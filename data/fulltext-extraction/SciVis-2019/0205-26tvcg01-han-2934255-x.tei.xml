<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TSR-TVD: Temporal Super-Resolution for Time-Varying Data Analysis and Visualization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">University of Notre Dame</orgName>
								<orgName type="institution" key="instit2">Notre Dame</orgName>
								<address>
									<postCode>46556</postCode>
									<region>IN</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoli</forename><surname>Wang</surname></persName>
							<email>chaoli.wang@nd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">University of Notre Dame</orgName>
								<orgName type="institution" key="instit2">Notre Dame</orgName>
								<address>
									<postCode>46556</postCode>
									<region>IN</region>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">University of Notre Dame</orgName>
								<orgName type="institution" key="instit2">Notre Dame</orgName>
								<address>
									<postCode>46556</postCode>
									<region>IN</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">â€¢</forename><forename type="middle">J</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">University of Notre Dame</orgName>
								<orgName type="institution" key="instit2">Notre Dame</orgName>
								<address>
									<postCode>46556</postCode>
									<region>IN</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TSR-TVD: Temporal Super-Resolution for Time-Varying Data Analysis and Visualization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TVCG.2019.2934255</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Time-varying data visualization</term>
					<term>super-resolution</term>
					<term>deep learning</term>
					<term>recurrent generative network</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Fig. 1. TSR-TVD learns the generation of intermediate volumes from a given pair of volumes. The network includes a generator G which consists of the predicting and blending modules and a discriminator D which distinguishes the synthesized volumes from the ground truth (GT) volumes. Once learned, the network can perform both same-variable and different-variable inferences using G.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In many fields of science, scientists run large-scale scientific simulations and produce time-varying multivariate data on a daily basis. In this paper, we focus on augmenting the temporal resolution of these data as scientists often simulate a long temporal sequence with many variables involved but could only afford to store a very limited number of time steps (e.g., every hundredth time step) for post-hoc analysis. Our goal is to augment these reduced simulation data during post-processing by generating the refined temporal resolution to enable more accurate investigation of dynamic spatiotemporal features of the underlying data. That is, given a low-resolution volume sequence of, for example, 100 time steps, we aim to generate the high-resolution volume sequence of, for example, 500 time steps, while keeping the spatial resolution intact.</p><p>Producing temporally refined volume sequences for time-varying data poses two main challenges. The first challenge is that the dynamic change of volume data over time is typically non-linear. Conventional approaches employ standard linear interpolation (LERP) to generate intermediate volumes. These interpolations are only based on local information around the interpolated position, and therefore may not capture the complex evolution and non-linear changes of volumes. The second challenge is how to take visual quality into consideration. Applying a typical recurrence-based neural network design to our problem would only measure the voxelwise distance, which may not lead to high-quality rendering results even though the PSNR is high. For example, Zhang et al. <ref type="bibr" target="#b60">[61]</ref> pointed out that only using a pixelwise loss function to train a neural network could generate images with noise and artifacts.</p><p>To address these challenges, we present TSR-TVD, a novel deep learning framework for producing temporal super-resolution (TSR) from time-varying data (TVD). We leverage the recurrent generative network (RGN), a combination of the recurrent neural network (RNN) and generative adversarial network (GAN), to achieve TSR. This is because RNN and GAN can learn the temporal and spatial relationship among different volumes non-uniformly and non-locally, thus interpolating the intermediate volumes with high quality. Inspired by the sequence learning (e.g., weather forecasting and machine translation) and image generation (e.g., frame interpolation and video prediction) techniques, our solution consists of a generator and a discriminator for producing temporally coherent TSR of a sequence of volumes using adversarial learning. TSR-TVD takes the sampled volumes as input and generates the intermediate volumes. We train TSR-TVD by optimizing the loss function that includes adversarial loss, volumetric loss, and feature loss. Our TSR-TVD handles multivariate data as well where the trained network from one variable is applied to generate TSR for another variable. To demonstrate the effectiveness of TSR-TVD, we show quantitative and qualitative results with several timevarying multivariate data sets of different characteristics. We compare TSR-TVD against the widely-used LERP and solutions based on RNN or CNN only. We show that our method achieves a better quality than LERP in terms of PSNR at the data level and the best quality in terms of SSIM at the image level.</p><p>The contributions of this paper are as follows. First, our work is the first that applies RGN (a combination of RNN and GAN) for generating TSR of a volume sequence. Second, we apply ConvLSTM layers for capturing spatiotemporal relationships and propose a new voxel shuffle layer for accelerating the training process. Third, we design a new architecture for the TSR task, which supports not only same-variable inference but also different-variable inference. Fourth, we investigate several hyperparameter settings and analyze how they impact the performance of TSR-TVD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Time-varying multivariate data analysis and visualization is a key topic in scientific visualization. We refer interested readers to the survey paper <ref type="bibr" target="#b23">[24]</ref> for an overview. Existing works on time-varying data analysis and visualization focus on efficient organization and rendering <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b58">59]</ref>, transfer function specification <ref type="bibr" target="#b19">[20]</ref>, illustrationinspired <ref type="bibr" target="#b22">[23]</ref> and importance-driven <ref type="bibr" target="#b53">[54]</ref> techniques. For time-varying multivariate data, researchers have investigated query-driven visualization <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b8">9]</ref>, exploration of variable correlation <ref type="bibr" target="#b42">[43]</ref>, variable grouping <ref type="bibr" target="#b3">[4]</ref>, and information flow between variables <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b28">29]</ref>, as well as navigation interfaces <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b47">48]</ref>. In the following, we restrict our attention to related work on temporal super-resolution for video, relevant GAN and RNN techniques, and deep learning for scientific visualization.</p><p>TSR for video. Deep learning has achieved great success in generating TSR for video. For instance, Niklaus et al. <ref type="bibr" target="#b34">[35]</ref> introduced a convolutional neural network (CNN) for frame interpolation where the network learns a kernel through the input frames and then applies the learned kernel to generate the missing frames. Nguyen et al. <ref type="bibr" target="#b33">[34]</ref> proposed a deep linear embedding model to interpolate the intermediate frames. They transformed each frame into a feature space, then linearly interpolated the intermediate frames in the feature space, and finally recovered the interpolated features to the corresponding frames. Jiang et al. <ref type="bibr" target="#b20">[21]</ref> established a CNN to estimate the forward and backward optical flows via two given frames. They then wrapped these optical flows into the frames to generate arbitrary in-between frames.</p><p>Our work differs from the above works. First, we leverage RNN to capture the temporal relationship rather than using CNN, since RNN can learn the potential pattern across time. Second, unlike image generation tasks where researchers can use a pre-trained image classification model to improve the visual quality of the synthesized results, there is no such model for volumes. Third, many deep learning models compute the optical flow to generate intermediate frames but these models need either a lot of labeled optical flow data for training or an additional subnetwork to calculate the optical flow in an unsupervised way. Therefore, We apply GAN to ensure the high quality of the synthesized volumes. There are works that use RNN or GAN for similar purposes <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b5">6]</ref>, but not at the same time as our work does.</p><p>Our approach does not require a lot of labeled data or additional modules, which reduces its complexity and training time. We incorporate ConvLSTM to achieve temporal coherence. Based on this mechanism, TSR-TVD uses all information from time steps i, i + 1, â€¢â€¢â€¢ , i + j âˆ’ 1 to interpolate time step i + j.</p><p>GAN and RNN techniques. Introduced by Goodfellow et al. <ref type="bibr" target="#b9">[10]</ref>, a GAN includes two networks: a generator G and a discriminator D. G tries to synthesize data samples from noise or observation to fool D, while D aims to distinguish the data generated by G from the real samples. GAN has been applied to image translation <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b54">55]</ref>, style transfer <ref type="bibr" target="#b21">[22]</ref>, image inpainting <ref type="bibr" target="#b36">[37]</ref>, and image super-resolution <ref type="bibr" target="#b27">[28]</ref> tasks. Due to the problem of unstable training, different techniques have been considered during training, such as L 2 loss <ref type="bibr" target="#b36">[37]</ref>, gradient regularization <ref type="bibr" target="#b31">[32]</ref>, and separate learning rates for G and D <ref type="bibr" target="#b41">[42]</ref>. An RNN accepts a sequence as input and builds the temporal relationships through the internal states. One of the most frequently used RNN architectures is long short-term memory (LSTM) <ref type="bibr" target="#b14">[15]</ref>, which has been applied to video colorization <ref type="bibr" target="#b26">[27]</ref> and video caption <ref type="bibr" target="#b50">[51]</ref>. Due to the problem of gradient vanishing and explosion, different techniques have been applied in RNN training, such as stacked LSTMs <ref type="bibr" target="#b50">[51]</ref> and gradient clipping <ref type="bibr" target="#b35">[36]</ref>.</p><p>Deep learning for scientific visualization. There is a growing body of works that apply deep learning to solve scientific visualization problems. Tzeng et al. <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50]</ref> pioneered the use of artificial neural networks for classifying 3D volumetric data sets. Ma <ref type="bibr" target="#b30">[31]</ref> pointed out the use of neural networks as a promising direction for visualization research. With the explosive growth of modern deep learning techniques, researchers have recently started to explore the capabilities of deep neural network (DNN) to address various problems.</p><p>For volume visualization, Zhou et al. <ref type="bibr" target="#b61">[62]</ref> presented a CNN-based solution for volume upscaling which better preserves structural details and volume quality than linear upscaling. Raji et al. <ref type="bibr" target="#b38">[39]</ref> leveraged CNNs to iteratively refine a transfer function, aiming to match the visual features in the rendered image of a similar volume data set with the one in the target image. Cheng et al. <ref type="bibr" target="#b6">[7]</ref> presented a deep-learning-assisted solution which depicts and explores complex structures that are difficult to capture using conventional approaches. Berger et al. <ref type="bibr" target="#b2">[3]</ref> designed a GAN to compute a model from a large collection of volume-rendering images conditioned on viewpoints and transfer functions. Shi and Tao <ref type="bibr" target="#b44">[45]</ref> proposed a CNN-based viewpoint estimation method that achieves good performance on images rendered with different transfer functions and rendering parameters. Xie et al. <ref type="bibr" target="#b59">[60]</ref> designed a temporally coherent approach to generate spatial super-resolution volumes where temporal coherence is guaranteed through a temporal discriminator. Weiss et al. <ref type="bibr" target="#b56">[57]</ref> presented an image-space solution that learns to upscale a sampled representation of geometric properties of an isosurface at low resolution to a higher resolution. They considered temporal variations by adding a frame-toframe motion loss to achieve improved temporal coherence.</p><p>For flow visualization, Hong et al. <ref type="bibr" target="#b15">[16]</ref> used LSTM to learn and predict data access patterns in particle tracing in order to hide the I/O latency in distributed and parallel flow visualization. Wiewel et al. <ref type="bibr" target="#b57">[58]</ref> took a LSTM-based approach to predict dense 3D+time functions of physics system using an autoencoder framework. Kim and GÃ¼nther <ref type="bibr" target="#b24">[25]</ref> combined filter and feature extraction in an end-to-end manner using CNN for robust reference frame extraction from unsteady 2D vector fields. Han et al. <ref type="bibr" target="#b11">[12]</ref> proposed a deep learning method for vector field reconstruction that takes the streamlines traced from the original vector fields as input and applies a two-stage process to reconstruct high-quality vector fields. Han et al. <ref type="bibr" target="#b10">[11]</ref> designed an autoencoder to learn the features of flow lines or surfaces in the latent space and performed dimensionality reduction and interactive clustering for representative selection.</p><p>Although generating a spatial high-resolution volume from a lowresolution one has been studied <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b59">60]</ref>, to our best knowledge, in the context of time-varying data analysis and visualization, no work has been done that generates a temporal high-resolution volume sequence from a low-resolution one, which is the focus of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OUR RECURRENT GENERATIVE APPROACH</head><p>We define the TSR problem as follows. Given a pair of volumes</p><formula xml:id="formula_0">(V i ,V i+k ) from time steps i and i + k (where k &gt; 1), we seek a function Ï† that satisfies Ï† (V i ,V i+k ) â‰ˆ V, where V = {V i+1 ,V i+2 , â€¢â€¢â€¢ ,V i+kâˆ’1 } are the intermediate volumes between V i and V i+k .</formula><p>We propose a novel recurrent generative network (RGN) which is a combination of RNN and GAN. The RGN includes a generator G and a discriminator D. G uses two modules to estimate the function Ï† , as sketched in <ref type="figure">Figure 1</ref>. The first module, the predicting module (Ï† PREDICT ), is a volume prediction network that produces a forward prediction</p><formula xml:id="formula_1">V F through V i and a backward prediction V B through V i+k , respectively. Namely, V F = Ï† F PREDICT (V i ),<label>(1)</label></formula><formula xml:id="formula_2">V B = Ï† B PREDICT (V i+k ),<label>(2)</label></formula><formula xml:id="formula_3">Ï† PREDICT = {Ï† F PREDICT , Ï† B PREDICT }.<label>(3)</label></formula><p>The second module, the blending module (Ï† BLEND ), takes V i , V i+k , and the corresponding pair of volumes from V F and V B that share the same time step (for clarity, <ref type="figure">Figure 1</ref> only illustrates the blending flow connections with time step i + 1) as input, and blends them into a volume that represents the final predictioná¹¼, i.e.,</p><formula xml:id="formula_4">V = Ï† BLEND (V i ,V i+k , V F , V B ).<label>(4)</label></formula><p>As we use the down-sampling and up-sampling framework to reconstruct intermediate volumes, detailed features may be lost during down-sampling which cannot be recovered perfectly during upsampling. Adding V i and V i+k into the blending will help to eliminate noise generated from the predictions. The discriminator D dis-tinguishesá¹¼ from V. That is, given different inputs to D, D outputs a score to indicate the realness of the input. Ideally, D(V) â‰ˆ 1 and D(á¹¼) â‰ˆ 0. In this regard, D can be treated as a binary classifier. The score from D can guide G in synthesizing high-quality volumes since the goal of G is to fool D so that D cannot distinguishá¹¼ as fake volumes.</p><p>In the following, we describe the details of TSR-TVD, including the definition of the loss function and the architectures of G and D. The training algorithm and optimization details can be found in Section 1 in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Loss Function</head><p>Notations. Let us denote</p><formula xml:id="formula_5">V T = {(V 1 ,V k 1 ), (V k 1 ,V k 2 ), â€¢â€¢â€¢ , (V k nâˆ’1 ,V k n )}</formula><p>as a set of input volume pairs for our TSR-TVD framework, and</p><formula xml:id="formula_6">V I = {{V 2 , â€¢â€¢â€¢ ,V k 1 âˆ’1 }, {V k 1 +1 , â€¢â€¢â€¢ ,V k 2 âˆ’1 }, â€¢â€¢â€¢ , {V k nâˆ’1 +1 , â€¢â€¢â€¢ ,V k n âˆ’1 }}</formula><p>as the ground truth (GT) intermediate volumes that we aim to interpolate. Let Î¸ G and Î¸ D be the learnable parameters in G and D, respectively, and K be the maximal interpolation step (i.e., k i+1 âˆ’ k</p><formula xml:id="formula_7">i K + 1, i âˆˆ [0, n âˆ’ 1]).</formula><p>Adversarial loss. Following the definition in GAN <ref type="bibr" target="#b9">[10]</ref>, we define the adversarial loss for G and D as follows</p><formula xml:id="formula_8">min Î¸ G L G = E V âˆˆV T log D(G(V )) ,<label>(5)</label></formula><formula xml:id="formula_9">min Î¸ D L D = 1 2 E V âˆˆV I log D(V ) + 1 2 E V âˆˆV T log 1 âˆ’ D(G(V )) ,<label>(6)</label></formula><p>where E[â€¢] denotes the expectation operation. The idea behind this formulation is that we let G and D compete with each other so that G can produce synthesized volumes which fully fool D as the training goes, while the goal of D is to distinguish the synthesized volumes as fake and return this information to G to force G to be more powerful until D and G achieve a balance. Through this process, G can learn how to synthesize high-quality volumes that are highly close to the GT. This competition idea behind adversarial loss encourages a perceptual solution rather than a PSNR or SSIM orientated solution by minimizing voxelwise loss, such as L 1 loss. Note that using the adversarial loss alone would lead to an unstable training for G and D. Therefore, we further consider volumetric loss and feature loss, which are described next.</p><p>Volumetric loss. Since the adversarial loss only considers perceptual results, we add a volumetric loss in L G , which means that the task of G is to not only fool D but also get close to the GT output in the L 2 sense, while the job of D remains unchanged. In addition, the training stability can also be improved through adding the volumetric loss <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b18">19]</ref>. Therefore, we utilize the L 2 distance as a part of our loss function for G</p><formula xml:id="formula_10">L V = E V âˆˆV T ,V âˆˆV I ||G(V ) âˆ’V || 2 ,<label>(7)</label></formula><p>where || â€¢ || 2 denotes the L 2 norm. Feature loss. Zhang et al. <ref type="bibr" target="#b60">[61]</ref> suggested that features extracted from DNNs can be used as a metric to evaluate the output of generative models. Following this guideline, we also compute the feature difference between V I and G(V T ) based on D. This feature loss enforces G to produce similar features between G(V T ) and V I at different scales. Specifically, the features are extracted from every convolutional (Conv) layers of D except the final Conv layer and G can try to minimize these intermediate representations between the real and synthesized volumes. We denote the feature representation extracted from the kth Conv layer as F k . Then the feature loss is defined as</p><formula xml:id="formula_11">L F = E V âˆˆV T ,V âˆˆV I Nâˆ’1 âˆ‘ k=1 1 N k ||F k (G(V )) âˆ’ F k (V )|| 2 ,<label>(8)</label></formula><p>where N is the total number of Conv layers in D and N k denotes the number of elements in the kth Conv layer. This feature loss is related to the VGG loss <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b54">55]</ref> which has achieved impressive results in image super-resolution and style transfer tasks. Taking all three losses into consideration, we define the final loss function for G as min</p><formula xml:id="formula_12">Î¸ G L G = Î» 1 E V âˆˆV T D(G(V )) âˆ’ 1 2 + Î» 2 L V + Î» 3 L F ,<label>(9)</label></formula><p>where Î» 1 , Î» 2 , and Î» 3 are hyperparameters, which control the relative importance of these three losses.  Since a traditional residual block cannot change the resolution of the input <ref type="bibr" target="#b13">[14]</ref>, we propose an advanced residual block (still referred to as the residual block for the rest of paper), which allows to downscale or upscale its input. This treatment brings several benefits to training TSR-TVD. First, it captures multiscale features. Second, it prevents the network from gradient vanishing. Third, it allows us to build a deeper neural network to enhance performance.</p><p>The core of the feature learning component is four residual blocks. Each residual block contains two parts (P 1 and P 2 ): P 1 consists of four Conv layers followed by spectral normalization (SN) <ref type="bibr" target="#b31">[32]</ref> (which normalizes the parameters in the Conv layer to stabilize the training) and ReLU <ref type="bibr" target="#b32">[33]</ref>, and P 2 contains one Conv layer followed by SN and ReLU. These two parts are bridged by skip connection <ref type="bibr" target="#b40">[41]</ref>, as shown in <ref type="figure" target="#fig_0">Figure 2</ref> (c). We set the kernel size for the first residual block to 5 Ã— 5 Ã— 5 and for the last three residual blocks to 3 Ã— 3 Ã— 3. We set the stride to two, which means that the resolution of the input is halved in each residual block. We set the feature maps in these four residual blocks to 16, 32, 64, and 64, respectively.</p><p>For the temporal component, we apply ConvLSTM <ref type="bibr" target="#b45">[46]</ref> to transfer the spatial features into spatiotemporal features so that TSR-TVD can predict the next volumes via the previous volumes. Compared with the traditional LSTM <ref type="bibr" target="#b14">[15]</ref>, the weight-sharing mechanism in convolution allows us to use fewer parameters to train ConvLSTM, which saves much memory and speeds up the training process. There are three states in ConvLSTM: input state, forget state, and output state. These three states determine whether or not we should let new input in (input state), delete the information because the input is not important (forget state), or let the input impact the output at the current time step (output state). Taking the data (x t ), previous hidden state (h tâˆ’1 ), and memory state (c tâˆ’1 ) as input, ConvLSTM is defined as follows   The upscaling component takes the spatiotemporal features from ConvLSTM as input and outputs a synthesized volume. A common approach to recover resolution from max-pooling or Conv layers is to use deconvolutional (DeConv) layers. However, this approach brings two problems: high computational cost and unnecessary zero padding. For example, if we need to upscale a feature of size [L,W, H] with a factor f , the DeConv operation will first expand the feature to a reso-</p><formula xml:id="formula_13">f t = Ïƒ (W x f x t + W h f h tâˆ’1 + b f ),<label>(10)</label></formula><formula xml:id="formula_14">i t = Ïƒ (W xi x t + W hi h tâˆ’1 + b i ),<label>(11)</label></formula><formula xml:id="formula_15">o t = Ïƒ (W xo x t + W ho h tâˆ’1 + b o ),<label>(12)</label></formula><formula xml:id="formula_16">c t = tanh(W xc x t + W hc h tâˆ’1 + b c ),<label>(13)</label></formula><formula xml:id="formula_17">c t = i t c t + f t c tâˆ’1 ,<label>(14)</label></formula><formula xml:id="formula_18">h t = o t tanh(c t ),<label>(15)</label></formula><formula xml:id="formula_19">where i t , f t , c t , o t , h</formula><formula xml:id="formula_20">(W x f , W h f , b f ), (W xi , W hi , b i ), (W xo , W</formula><formula xml:id="formula_21">lution of [ f L + S âˆ’ 1, fW + S âˆ’ 1, f H + S âˆ’ 1] through zero padding,</formula><p>where S is the kernel size, then apply Conv operations to produce the output of size [ f L, fW, f H]. To address these problems, we propose an effective sub-voxel Conv layer, which we call the voxel shuffle layer, for upscaling. The definition is as follows  <ref type="figure" target="#fig_3">Figure 3 (b)</ref>. We apply the same architecture used in the feature learning component to the upscaling component. The difference is that we add the voxel shuffle layer after the last SN layer in P 1 and P 2 . As for hyperparameter setting, the kernel size is set to 3 Ã— 3 Ã— 3 in the first three residual blocks and 5 Ã— 5 Ã— 5 in the last residual block. We set the upscaling factor to 2 for each voxel shuffle layer and the feature maps in these four residual blocks to 64, 32, 16, and 1, respectively. Note that we apply tanh(â€¢) after the final residual block.</p><formula xml:id="formula_22">O = S (W I + b),<label>(16)</label></formula><p>Through the prediction module, we obtain the forward prediction V F i and backward prediction V B i . The blending module </p><formula xml:id="formula_23">accepts V k i , V k i+1 , V F i ,</formula><formula xml:id="formula_24">V i = w i V k i + (1 âˆ’ w i )V k i+1 + 1 2 (V F i + V B i ),<label>(17)</label></formula><p>where w i is the weight which controls the importance of V k i and V k i+1 . Discriminator. To discriminate real volumes from synthesized ones, we train a discriminator network D. The architecture is shown in <ref type="figure" target="#fig_0">Figure 2 (b)</ref>. Following the guidelines in Radford et al. <ref type="bibr" target="#b37">[38]</ref> and Miyato et al. <ref type="bibr" target="#b31">[32]</ref>, we use several Conv and SN layers with leaky ReLU activation (Î± = 0.2), and avoid pooling layers throughout the network. D includes five Conv layers with 4 Ã— 4 Ã— 4 kernel size, which contains 64, 128, 256, 512, and 1 feature maps, respectively. Strided convolutions are used to reduce the volume resolution at each Conv layer except the last one. The stride is set to two at the first four Conv layers. At the last Conv layer, it produces an output of size 1 Ã— 1 Ã— 1 and we do not apply the activation function. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULTS AND DISCUSSION 4.1 Data Sets and Network Training</head><p>We experimented with TSR-TVD using the data sets listed in <ref type="table">Table 1</ref>. The climate data set comes from the simulations of the Earth's climate in the dynamical core model intercomparison project (DCMIP) <ref type="bibr" target="#b0">[1]</ref>. We acquired two ensemble runs generated by different models (camfv and fim). In these models, the volumes are fairly static in the early time steps and later on two turbulent branches appear. The combustion data set comes from direct numerical simulation of temporally evolving turbulent non-premixed flames where combustion reactions occur within the two layers. These layers are initially thin planar layers and then evolve into complex structures as they interact with the surrounding turbulence. The simulation generates multiple variables and we used three of them: heat release (HR), stoichiometric mixture fraction (MF), and OH mass fraction (YOH). The ionization data set is made available through the IEEE Visualization 2008 Contest. The simulation is concerned with 3D radiation hydrodynamical calculations of ionization front instabilities for studying a variety of phenomena in interstellar medium such as the formation of stars. The simulation generates multiple variables and we used two of them: He mass abundance <ref type="bibr">(He)</ref> and He+ mass abundance (He+). The solar plume data set comes from a simulation that aims to investigate the role of the solar plume plays in the transport of the heat, momentum, and magnetic field in the sun. The simulation generates a velocity vector field and we computed the velocity magnitude for our use. The supernova data set comes from a simulation that models the dynamics of exploding stars, which reveal instability in the shock wave blasts, imparting rotation to the newborn neutron stars in their cores. The simulation generates the entropy (E) scalar field and a velocity vector field for which we computed the velocity magnitude (VM) for our use. Finally, the vortex data set has been widely used in feature extraction and tracking. The data set comes from a pseudo-spectral simulation of vortex structures. We used the vorticity magnitude scalar variable.</p><p>A single NVIDIA TITAN Xp 1080 GPU was used for training. For each epoch, we randomly cropped four subvolumes with size 64 Ã— 64 Ã— 64 from a volume pair (V k i ,V k i+1 ). This cropping mechanism can speed up the training process and reduce the memory requirement. We scaled the range of inputs in V to [âˆ’1, 1] and that of output volumes to [âˆ’1, 1] (because the value range for the output of the final activation function tanh(â€¢) is [âˆ’1, 1]). For optimization, we initialized parameters in TSR-TVD using those suggested by He et al. <ref type="bibr" target="#b12">[13]</ref> and applied the Adam optimizer <ref type="bibr" target="#b25">[26]</ref> to update the parameters. We set one training sample per minibatch. We set different learning rates for G and D in order to reduce the training time and stabilize the training process <ref type="bibr" target="#b41">[42]</ref>. The learning rates for G and D are 10 âˆ’4 and 4 Ã— 10 âˆ’4 , respectively. Î² 1 = 0.0, Î² 2 = 0.999. Î» 1 = 10 âˆ’3 , Î» 2 = 1, and Î» 3 = 5 Ã— 10 âˆ’2 . We found that if Î» 1 is larger than Î» 2 and Î» 3 , TSR-TVD will fail to generate synthesized volumes that are similar to the GT volumes. This is because TSR-TVD will pay more attention to adversarial loss rather than volumetric and feature losses and the goal of adversarial loss is to synthesize novel volumes rather than volumes close to the GT volumes. We set Îµ to 10 âˆ’4 in SN, and n G and n D to 1 and 2, respectively (refer to Section 1 in the Appendix). During training, we set the maximal interpolation step K to 3 because a large K would lead to gradient vanishing in ConvLSTM and prevent TSR-TVD from finding the globally optimal solution. However, during inference, we can increase K to interpolate more immediate time steps since the gradient computation is not required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Due to the page limit, we are not able to show TSR-TVD results for multiple time steps in the paper. These results can be found in the accompanying video, which shows visualization results over the entire sequence and highlights the better quality of synthesized volumes generated using TSR-TVD over LERP, RNN, and CNN. Unless otherwise stated, all visualization results presented in the paper for volumes synthesized by TSR-TVD are the inferred results (i.e., the network does not see these volumes during training). These inferred results are from a volume subsequence that is far away from the training data, and within the subsequence, we select the time step that is farthest away from the GT volumes at the two end time steps (i.e., we show the worst possible TSR-TVD results). All visualizations for the same data set use the same setting for lighting, viewing, and transfer function (for volume rendering). For network analysis, please refer to Section 2 in the Appendix. Evaluation metrics. We utilize the peak signal-to-noise ratio (PSNR) to evaluate the quality of synthesized volumes at the data level. PSNR is defined as</p><formula xml:id="formula_25">PSNR(V, V ) = 20 log 10 I(V) âˆ’ 10 log 10 MSE(V, V ),<label>(18)</label></formula><p>where V and V are the original and synthesized volumes, I(V) is the difference between the maximum and minimum values of V, and <ref type="figure">)</ref> is the mean squared error between V and V . We apply the structural similarity index (SSIM) <ref type="bibr" target="#b55">[56]</ref> to evaluate the quality of rendered images at the image level. SSIM is defined as</p><formula xml:id="formula_26">MSE(V, V</formula><formula xml:id="formula_27">SSIM(I, I ) = (2Î¼ I Î¼ I + c 1 )(2Ïƒ I,I + c 2 ) (Î¼ 2 I + Î¼ 2 I + c 1 )(Ïƒ 2 I + Ïƒ 2 I + c 2 ) ,<label>(19)</label></formula><p>where I and I are sub-images from the rendered images of V and V , Î¼ I and Î¼ I are the average values of I and I , Ïƒ 2 I and Ïƒ 2 I are the variances of I and I , Ïƒ I,I is the covariance of I and I , and c 1 and c 2 are two small constants for stabilizing the division with the denominator.</p><p>To quantify the similarity between two isosurfaces extracted, respectively, from the synthesized and GT volumes, we compute the mutual information of their corresponding distance fields <ref type="bibr" target="#b4">[5]</ref>. Mutual information is computed by constructing a joint histogram of the two distance fields, where each entry (i, j) in the joint histogram contains the number of voxels that fall into bins i and j in the first and second distance fields, respectively. The larger the isosurface similarity (IS), the more similar the two surfaces are.</p><p>Qualitative and quantitative analysis. In <ref type="figure" target="#fig_5">Figure 4</ref>, we compare the rendering results of the synthesized volumes generated by LERP, RNN, and TSR-TVD. To train an RNN, we still use the architecture of TSR-TVD but eliminate the discriminator. To train a CNN, we use the same TSR-TVD architecture without ConvLSTM as the built-in temporal coherence predictor. For easy comparison, we calculate pixelwise differences (the Euclidean distances) of images generated from the synthesized and original volumes in the CIELUV color space. We map the noticeable pixel differences (with Î” 6.0) to nonwhite colors (clamping differences larger than 255) and display the difference image at the corner. The GT is provided for a fair comparison. For the combustion (MF) data set, RNN, CNN, and TSR-TVD can generate more details compared with LERP, for example, the small red component at the left corner. But the rendering result from TSR-TVD has fewer artifacts than the result from RNN or CNN. For the solar plume data set, all four methods generate similar results, but TSR-TVD is closest to the GT. It is clear that TSR-TVD leads to the best visual quality for the supernova (VM) data set while LERP is the worst and RNN or CNNgenerates many artifacts on the right side. For the vortex data set, LERP generates the worst result while RNN, CNN, and TSR-TVD yield similar results. Upon close examination, we can see that TSR-TVD yields the closest result at the bottom-left corner. In <ref type="figure" target="#fig_6">Figure 5</ref>, we compare the rendering results of the synthesized volumes generated by LERP and TSR-TVD through different-variable inference. For different-variable inference, we use a variable X of a data set for training and use another variable Y of the same data set for inference (X â†’ Y ). For MF â†’ HR of the combustion data set, TSR-TVD produces high-quality and detailed visual results in the purple part at the top-left corner and in the green and yellow parts at the middle-right corner while LERP yields less accurate results. For MF â†’ YOH of the combustion data set, TSR-TVD generates more accurate rendering result compared with LERP. For example, the purple part at the top-right corner and the green part at the bottom-right corner are closer to the GT. As for He â†’ He+ of the ionization data set, it is clear that TSR-TVD gives high-quality visual results at the middle and bottom layers while LERP leads to color shifting due to content change (even though the same transfer function is used to render the GT and synthesized volumes). It is obvious that TSR-TVD generates a better visual result for VM â†’ E of the supernova data set. TSR-TVD produces more accurate details in the orange, navy blue, and cyan parts. LERP, however, cannot faithfully recover these parts as we can clearly see color shifting due to content change.</p><p>In <ref type="figure">Figure 6</ref>, we compare TSR-TVD results against LERP and RNN results at the data (PSNR) and image (SSIM) levels. At the data level, we can see that RNN achieves the highest PSNR values. This is because RNN is a PSNR-oriented solution while TSR-TVD is constrained by not only the volumetric loss (PSNR-oriented loss) but also adversarial (perception-oriented loss) and feature losses, which could lead to lower PSNR values. For these four data sets, the PSNR curves follow a similar trend: PSNR values peak at both ends of the volume subsequence where the GT time steps are available and fall steadily as we move toward the time steps in the middle of the interval. We also see lower PSNR values across the three methods for the supernova (E) data set. This is because the supernova exhibits a fast-pacing rotational behavior which is more difficult to capture compared with other behaviors exhibited by the other two data sets. At the image level, TSR-TVD yields the highest SSIM values. It is a clear winner for the combustion (HR), combustion (MF), and supernova (E) data sets. TSR-TVD produces average SSIM values of âˆ¼ 0.72, âˆ¼ 0.72, âˆ¼ 0.65, respectively, but LERP produces average SSIM values of âˆ¼ 0.62, âˆ¼ 0.70, âˆ¼ 0.60, respectively. For the vortex data set, TSR-TVD still slightly outperforms LERP and RNN. In <ref type="table">Table 2</ref>, we report the average PSNR and SSIM values over the entire volume sequence for LERP, RNN, CNN, and TSR-TVD. Again, RNN performs the best in terms of PSNR while TSR-TVD performs the best in terms of SSIM.</p><p>In <ref type="figure">Figure 7</ref>, we show the variants of TSR-TVD using the combustion (YOH) data set. We can see that only using forward or backward prediction can still generate the synthesized volumes reasonably well but the results lack fine details. For example, there is a closed light cyan part in the GT volume at the middle-right corner, which is captured in the full mode in <ref type="figure">Figure 7</ref> (d) but missed in both <ref type="figure">Figure 7</ref> (a) and (b). Moreover, we find that without adding the original volumes into the blending module, the rendering result leads to obvious noise, as shown in <ref type="figure">Figure 7 (c)</ref>. This is because we use a traditional down-sampling and up-sampling framework to reconstruct intermediate volumes. In the down-sampling phase, TSR-TVD will compress the volumes and lose some information, however, in the up-sampling phase, the lost information cannot be perfectly recovered. This kind of information loss leads to the inferior quality of rendering images.</p><p>In <ref type="figure" target="#fig_8">Figures 8 and 9</ref>, we compare the isosurface rendering results of the synthesized volumes generated by LERP, TSR-TVD without blending original volumes, and TSR-TVD using the supernova (E) and combustion (HR) data sets. For each data set, the value range is normalized to [âˆ’1, 1] and we pick two time steps and two isovalues to show the isosurfaces. For the supernova (E) data set, we can observe that for v = 0, the isosurface generated by TSR-TVD includes more details (e.g., the bottom-right corner for time step 39 and the top-right corner for time step 55). For v = 0.176, TSR-TVD produces a higherquality isosurface since LERP totally fails to construct the isosurface close to the GT at time step 39 (we can see that the surface is severely shifted in the value space). Adding backward and forward predictions along with the use of voxel-wise volumetric loss, TSR-TVD is able to largely mitigate this and produces a surface very close to the GT. For the combustion (HR) data set, TSR-TVD can still generate closer isosurfaces than LERP, such as the bottom-left corner (v = 0.255) and the top-right corner (v = 0.569). For both data sets, it is clear that the full mode of TSR-TVD generates better results than those without adding the original volumes into the blending module. In addition, we report in <ref type="table">Table 3</ref> the average IS values over the entire volume sequence for these two data sets. The quantitative results also confirm that TSR-TVD leads to isosurfaces of better quality than LERP.</p><p>Failure case. As shown in <ref type="figure">Figure 10</ref>, both LERP and TSR-TVD cannot interpolate the intermediate time steps well for the climate (fim) data set. We can see that voxel values are shifted in both results. For example, the green parts in the GT become yellow and the blue parts almost vanish in the results of LERP and TSR-TVD. This is due to the limitation of TSR-TVD in estimating the difference between data distributions of neighboring time steps. Refer to Section 3 in the Appendix for further discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussion</head><p>As a deep learning technique, TSR-TVD takes a considerable amount of time for training. In <ref type="figure" target="#fig_0">Figure 2</ref> in the Appendix, we report the training time curves for selected data sets under different hyperparameter settings. With 40% of training samples and subvolume size of 64 Ã— 64 Ã— 64, it takes nearly six hours to train the combustion data set (100 epochs) and half a day to train the supernova data set (200 epochs). The training time forms a nearly linear relationship with the increase in the number of training samples or the subvolume size. We note that the actual size of the volume (i.e., the spatial dimension of the data) is not a critical limiting factor for network training because we apply the cropping mechanism in TSR-TVD training. Therefore, the training time mainly depends on the (cropped) subvolume size rather than the volume size. The inference time depends on the number of interpolated time steps and volume resolution. Due to the limited GPU memory, we infer individual subvolumes to form the entire volume. The time is between one hour (vortex) to one day (combustion). Refer to Section 3 in the Appendix for further discussion. Our current TSR-TVD framework has the following limitations. First, our solution does not consider the input transfer function or the visualization process. We simply treat the input 3D volumes as numerical data for producing the temporally resolved results. In terms of visualization, we use 1D transfer functions that map scalar values to color and opac- ity. The volume renderer also implements lighting calculation. Nevertheless, qualitative and quantitative evaluations show the overall advantage of TSR-TVD over LERP and RNN. While incorporating the transfer function, especially the opacity transfer function, may help boost the performances at the image level, it comes at the price of making the training process dependent on the input transfer function, which would demand the training from scratch whenever the transfer function changes. Second, TSR-TVD can only infer the intermediate volumes at any integer time steps rather than arbitrary time steps. We would further study how to infer arbitrary in-between volumes from two given volumes through disentangled representation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17]</ref>. Third, TSR-TVD considers temporal coherence through the recurrent module. A better way is to incorporate temporal coherence into loss function design. We will investigate temporal and cycle losses that take the coherence of neighboring time steps into consideration to achieve better temporal coherence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS AND FUTURE WORK</head><p>We have presented TSR-TVD, a deep learning solution for generating temporal super-resolution of time-varying data. Given a volume pair as input and its intermediate volumes as the GT, we leverage an RGN to make forward and backward predictions and train TSR-TVD. The trained network is then able to generate temporally resolved volume sequences for the rest of time series via same-variable inference or different-variable inference. Compared with LERP, RNN, and CNN, TSR-TVD yields synthesized intermediate volumes of better visual quality. TSR-TVD is part of our research effort toward what we call data augmentation for scientific visualization. Data augmentation in this context refers to the addition of spatial, temporal, and variable details to reduced data by incorporating information derived from internal and external sources. Besides temporal super-resolution (TSR), we would also consider spatial super-resolution (SSR) for time-varying data. Our eventual goal is to achieve spatiotemporal super-resolution (STSR) by producing volume sequences with greater spatial and temporal resolutions and details. The ability to upscale time-varying data in both spatial and temporal dimensions is critical for large-scale scientific simulations and applications. As scientists often have to save their simulation data sparsely due to the limited storage, our research will provide a promising alternative for them to make better decisions depending on the nature of the simulations and the characteristics of the data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Network architecture of (a) the predicting module in generator G and (b) discriminator D. (c) An example of the residual block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3. 2</head><label>2</label><figDesc>Network Architecture Generator. As sketched in Figure 1, the generator G contains two modules: a predicting module and a blending module. The predicting module is composed of three components: a feature learning component, a temporal component with multiple ConvLSTM layers, and an upscaling component, as sketched in Figure 2 (a). The feature learning component extracts feature representations from the volumes, the temporal component bridges the spatial and temporal information among different volumes, while the upscaling component recovers the volumes from the spatiotemporal features. The blending module takes all outputs from the predicting module as input and generates the final synthesized volumes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>t are the input, forget, memory, output, and hidden states at the tth step in ConvLSTM, respectively. Ïƒ (â€¢), tanh(â€¢), , and represent the logistic sigmoid activation function, hyperbolic tangent activation function, Conv operation, and elementwise multiplication, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3</head><label>3</label><figDesc>ho , b o ), and (W xc , W hc , b c ) are learnable parameters in ConvLSTM. By default, we set h 0 = 0 and c 0 = 0. Note that ConvLSTM does not change the resolution of the input. An example of the ConvLSTM is shown in Figure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>(a) An example of the ConvLSTM. (b) An example of the voxel shuffle layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Comparison of volume rendering results of same-variable inference. Top to bottom: combustion (MF), solar plume, supernova (VM), and vortex. For (a) to (d), the difference image with respect to the corresponding GT is shown at the corner.Table 3. Comparison of average IS values at selected isovalues. LERP TSR-TVD data set (variable)v = 0 v = 0.176 v = 0 v = v = 0.255 v = 0.569 v = 0.255 v = 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>(a) combustion (MF â†’ HR) (b) combustion (MF â†’ YOH) (c) ionization (He â†’ He+) (d) supernova (VM â†’ E) Comparison of volume rendering results of different-variable inference. Displayed here are combustion (HR), combustion (YOH), ionization (He+), and supernova (E). Top to bottom: LERP, TSR-TVD, and GT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Comparison of PSNR of synthesized volumes (top row) and SSIM of rendered images (bottom row) using LERP, RNN, and TSR-TVD. The projection views for computing SSIM are shown in the respective volume-rendering images in the paper. We only report PSNR and SSIM values for a volume subsequence where the GT time steps are available at both ends. The results for other subsequences follow a similar trend.(a) w/o fwd prediction (b) w/o bwd prediction (c) w/o original volumes (d) full model (e) GT Comparison of volume rendering results of same variable inference with the variants of TSR-TVD using the combustion (YOH) data set. Displayed here are the cropped left-half of the rendering images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Comparison of isosurface rendering results of same-variable inference using the supernova (E) data set. First row: time steps 39 with v = 0. Second row: time steps 39 with v = 0.176. Third row: time steps 55 with v = 0. Last row: for time steps 55 with v = 0.176. The two-end GT time steps are 37 and 41 for time step 39, and 53 and 57 for time step 55.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .Fig. 10 .</head><label>910</label><figDesc>(a) LERP (b) w/o ori volumes (c) TSR-TVD (d) GT (e) left-end GT (f) right-end GT Comparison of isosurface rendering results of same-variable inference using the combustion (HR) data set. First row: time steps 97 with v = 0.255. Second row: time steps 97 with v = 0.569. Third row: time steps 114 with v = 0.255. Last row: for time steps 114 with v = 0.569. The two-end GT time steps are 95 and 99 for time step 97, and 112 and 116 for time step 114. (a) LERP (b) TSR-TVD (c) GT Comparison of volume rendering results of same-variable inference using the climate (fim) data set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Manuscript received 31 Mar. 2019; accepted 1 Aug. 2019. Date of publication 16 Aug. 2019; date of current version 20 Oct. 2019. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org, and reference the Digital Object Identifier below. Digital Object Identifier no. 10.1109/TVCG.2019.2934255</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>where I and O are the input and output, respectively, W and b are the learnable parameters, and S is a periodic shuffle operation that rearranges the elements of a [C f 3 , L,W, H] tensor to a tensor of size [C, f L, fW, f H] (C denotes the number of channels). An example of this operation is illustrated in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>The dimensions and training epochs of each data set. Comparison of average PSNR and SSIM values.</figDesc><table><row><cell cols="2">data set (variable)</cell><cell cols="4">dimension (x Ã— y Ã— z Ã— t) epochs</cell></row><row><cell cols="2">climate (cam-fv)</cell><cell cols="2">360 Ã— 181 Ã— 30 Ã— 31</cell><cell>100</cell><cell></cell></row><row><cell cols="2">climate (fim)</cell><cell cols="2">360 Ã— 181 Ã— 30 Ã— 31</cell><cell>100</cell><cell></cell></row><row><cell cols="2">combustion (HR)</cell><cell cols="2">480 Ã— 720 Ã— 120 Ã— 121</cell><cell>100</cell><cell></cell></row><row><cell cols="2">combustion (MF)</cell><cell cols="2">480 Ã— 720 Ã— 120 Ã— 121</cell><cell>100</cell><cell></cell></row><row><cell cols="4">combustion (YOH) 480 Ã— 720 Ã— 120 Ã— 121</cell><cell>100</cell><cell></cell></row><row><cell cols="2">ionization (He)</cell><cell cols="2">600 Ã— 248 Ã— 248 Ã— 100</cell><cell>100</cell><cell></cell></row><row><cell cols="2">ionization (He+)</cell><cell cols="2">600 Ã— 248 Ã— 248 Ã— 100</cell><cell>100</cell><cell></cell></row><row><cell cols="2">solar plume</cell><cell cols="2">126 Ã— 126 Ã— 512 Ã— 28</cell><cell>200</cell><cell></cell></row><row><cell cols="2">supernova (E)</cell><cell cols="2">256 Ã— 256 Ã— 256 Ã— 60</cell><cell>200</cell><cell></cell></row><row><cell cols="2">supernova (VM)</cell><cell cols="2">256 Ã— 256 Ã— 256 Ã— 60</cell><cell>200</cell><cell></cell></row><row><cell>vortex</cell><cell></cell><cell cols="2">128 Ã— 128 Ã— 128 Ã— 90</cell><cell>200</cell><cell></cell></row><row><cell></cell><cell></cell><cell>PSNR (dB)</cell><cell></cell><cell cols="2">SSIM</cell></row><row><cell cols="3">data set (variable) LERP RNN CNN TSR</cell><cell cols="4">LERP RNN CNN TSR</cell></row><row><cell>combustion (HR)</cell><cell>25.61</cell><cell cols="2">26.13 25.72 25.81 0.66</cell><cell>0.70</cell><cell>0.69</cell><cell>0.72</cell></row><row><cell cols="2">combustion (MF) 25.12</cell><cell cols="2">25.86 25.43 25.62 0.71</cell><cell>0.73</cell><cell>0.73</cell><cell>0.74</cell></row><row><cell>supernova (E)</cell><cell>22.34</cell><cell cols="2">24.31 23.81 23.74 0.61</cell><cell>0.64</cell><cell>0.63</cell><cell>0.66</cell></row><row><cell>vortex</cell><cell>26.62</cell><cell cols="2">27.42 26.85 26.90 0.73</cell><cell>0.75</cell><cell>0.75</cell><cell>0.75</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research was supported in part by the U.S. National Science Foundation through grants IIS-1455886, CNS-1629914, and DUE-1833129, and the NVIDIA GPU Grant Program. The authors would like to thank the anonymous reviewers for their insightful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The 2012 dynamical core model intercomparison project (DCMIP)</title>
		<ptr target="https://earthsystemcog.org/projects/dcmip-2012/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A tri-space visualization interface for analyzing time-varying multivariate volume data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Akiba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Eurographics -IEEE VGTC Symposium on Visualization</title>
		<meeting>Eurographics -IEEE VGTC Symposium on Visualization</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="115" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A generative model for volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1636" to="1650" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An informationaware framework for exploring multivariate data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Woodring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2683" to="2692" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Isosurface similarity maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bruckner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>MÃ¶ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="773" to="782" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep video generation, prediction and completion of human action sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="374" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep-learning-assisted volume visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cardone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Krokos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Subramaniam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varshney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1378" to="1391" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised learning of disentangled representations from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Birodkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4414" to="4423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Visualizing temporal patterns in large multivariate data using textual pattern matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Glatter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1467" to="1474" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">FlowNet: A deep learning framework for clustering and selection of streamlines and stream surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Accepted</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Flow field reduction via reconstructing vector data from 3D streamlines using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="54" to="67" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Access pattern learning with long shortterm memory for parallel particle tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Pacific Visualization Symposium</title>
		<meeting>IEEE Pacific Visualization Symposium</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="76" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to decompose and disentangle representations for video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-T</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="515" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Multi-scale video frame-synthesis network with transitive consistency loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02874</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A study of transfer function generation for time-varying volume data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Jankun-Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Eurographics -IEEE TCVG Workshop on Volume Graphics</title>
		<meeting>Eurographics -IEEE TCVG Workshop on Volume Graphics</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="51" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Super SloMo: High quality estimation of multiple intermediate frames for video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9000" to="9008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Illustration-inspired techniques for visualizing time-varying data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rheingans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization Conference</title>
		<meeting>IEEE Visualization Conference</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="679" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visualization and visual analysis of multifaceted scientific data: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kehrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="495" to="513" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Robust reference frame extraction from unsteady 2D vector fields with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>GÃ¼nther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference for Learning Representations</title>
		<meeting>International Conference for Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning blind video temporal consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="170" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>HuszÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Association analysis for visual exploration of multivariate scientific data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="955" to="964" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Video frame synthesis using deep voxel flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4473" to="4481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Machine learning to boost the next generation of visualization technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="6" to="9" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference for Learning Representations</title>
		<meeting>International Conference for Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted Boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Video frame interpolation by plug-and-play deep locally linear embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01462</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="670" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Machine Learning</title>
		<meeting>IEEE International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Photo-guided exploration of volume data features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sisneros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Messmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Eurographics Symposium on Parallel Graphics and Visualization</title>
		<meeting>Eurographics Symposium on Parallel Graphics and Visualization</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="31" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On Wasserstein two-sample testing and related families of nonparametric tests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramdas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trillos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="47" to="62" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<meeting>International Conference on Medical Image Computing and Computer-Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Stabilizing training of generative adversarial networks through regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multifield-Graphs: An approach to visualizing correlations in multifield scalar data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Theisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="917" to="924" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A fast volume rendering algorithm for time-varying fields using a time-space partitioning (TSP) tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization Conference</title>
		<meeting>IEEE Visualization Conference</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="371" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">CNNs based viewpoint estimation for volume visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<idno>27:1-27:22</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Convolutional LSTM network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Query-driven visualization of large data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stockinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shalf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Bethel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization Conference</title>
		<meeting>IEEE Visualization Conference</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="167" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Exploring time-varying multivariate volume data using matrix of isosurface similarity maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Imre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1236" to="1245" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A novel interface for higherdimensional classification of volume data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-Y</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Lum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization Conference</title>
		<meeting>IEEE Visualization Conference</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="505" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">An intelligent system approach to higher-dimensional classification of volume data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-Y</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Lum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="284" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Sequence to sequence-video to text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4534" to="4542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">A framework for rendering large time-varying data using wavelet-based time-space partitioning (WTSP) tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
		<idno>OSU-CISRC-1/04-TR05</idno>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer and Information Science, The Ohio State University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Analyzing information transfer in time-varying multivariate data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Grout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Pacific Visualization Symposium</title>
		<meeting>IEEE Pacific Visualization Symposium</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="99" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Importance-driven time-varying data visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1547" to="1554" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8798" to="8807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Volumetric isosurface rendering with deep learning-based super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thuerey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Westermann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06520</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Latent-space physics: Towards learning the temporal evolution of fluid flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wiewel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Becher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thuerey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="71" to="82" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">High dimensional direct rendering of time-varying volumetric data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Woodring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization Conference</title>
		<meeting>IEEE Visualization Conference</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">tempoGAN: A temporally coherent, volumetric GAN for super-resolution fluid flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thuerey</surname></persName>
		</author>
		<idno>95:1-95:15</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Volume upscaling with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Computer Graphics International</title>
		<meeting>Computer Graphics International</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1" to="38" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
