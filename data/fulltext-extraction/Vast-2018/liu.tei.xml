<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Analyzing the Noise Robustness of Deep Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="laboratory">TNList Lab, State Key Lab for Intell. Tech. Sys</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixia</forename><surname>Liu</surname></persName>
							<email>shixia@mail.tsinghua.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
							<email>suhangss@mail.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="laboratory">TNList Lab, State Key Lab for Intell. Tech. Sys</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Comp. Sci.Tech</orgName>
								<orgName type="laboratory">TNList Lab, State Key Lab for Intell. Tech. Sys</orgName>
								<orgName type="institution" key="instit1">CBICR Center</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelei</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="laboratory">TNList Lab, State Key Lab for Intell. Tech. Sys</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Comp. Sci.Tech</orgName>
								<orgName type="laboratory">TNList Lab, State Key Lab for Intell. Tech. Sys</orgName>
								<orgName type="institution" key="instit1">CBICR Center</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">S</forename><surname>Liu</surname></persName>
						</author>
						<title level="a" type="main">Analyzing the Noise Robustness of Deep Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep neural networks</term>
					<term>robustness</term>
					<term>adversarial examples</term>
					<term>back propagation</term>
					<term>multi-level visualization</term>
				</keywords>
			</textClass>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>IA IB <ref type="figure">Figure 1</ref>: Explaining the misclassification of adversarial panda images. The root cause is that the CNN cannot detect panda's ears in the adversarial examples (F C ), which leads to the failure of detecting a panda's face (F D ). As a result, these adversarial examples are misclassified: (a) input images; (b) datapath visualization at the layer and feature map levels; (c) neuron visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ABSTRACT</head><p>Deep neural networks (DNNs) are vulnerable to maliciously generated adversarial examples. These examples are intentionally designed by making imperceptible perturbations and often mislead a DNN into making an incorrect prediction. This phenomenon means that there is significant risk in applying DNNs to safety-critical applications, such as driverless cars. To address this issue, we present a visual analytics approach to explain the primary cause of the wrong predictions introduced by adversarial examples. The key is to analyze the datapaths of the adversarial examples and compare them with those of the normal examples. A datapath is a group of critical neurons and their connections. To this end, we formulate the datapath extraction as a subset selection problem and approximately solve it based on back-propagation. A multi-level visualization consisting of a segmented DAG (layer level), an Euler diagram (feature map level), and a heat map (neuron level), has been designed to help experts investigate datapaths from the high-level layers to the detailed neuron activations. Two case studies are conducted that demonstrate the promise of our approach in support of explaining the working mechanism of adversarial examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep neural networks (DNNs) have evolved to become state-ofthe-art in a torrent of artificial intelligence applications, such as image classification and language translation <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60]</ref>. However, researchers have recently found that DNNs are generally vulnerable to maliciously generated adversarial examples, which are intentionally designed to mislead a DNN into making incorrect predictions <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b62">63]</ref>. For example, an attacker can modify an image of a panda (I A in <ref type="figure">Fig. 1</ref>) slightly, even imperceptibly to human eyes, and the generated adversarial example (I B in <ref type="figure">Fig. 1</ref>) is able to mislead a state-of-the-art DNN <ref type="bibr" target="#b20">[21]</ref> to classify it as something else entirely (e.g., a monkey), because the DNN detects a monkey's face in the top right corner of the adversarial example <ref type="figure" target="#fig_11">(Fig. 11A</ref>). This phenomenon brings high risk in applying DNNs to safety-and security-critical applications, such as driverless cars, facial recognition ATMs, and Face ID security on mobile phones <ref type="bibr" target="#b0">[1]</ref>. Hence, there is a growing need to understand the inner workings of adversarial examples and identify the root cause of the incorrect predictions.</p><p>There are two technical challenges to understanding and analyzing adversarial examples, which are derived from the discussions with machine learning experts (Sec. 3) and previous research on adversarial examples <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37]</ref>. The first challenge is how to extract the datapath for adversarial examples. A datapath includes the critical neurons and their connections that are responsible for the predictions of the examples <ref type="figure" target="#fig_2">(Fig. 2 (a)</ref>). Disclosing the datapath will help experts design more targeted defense approaches. However, in a DNN, the neurons have complex interactions with each other <ref type="bibr" target="#b5">[6]</ref>. Thus, it is technically demanding to disentangle these neurons from the whole network and thus form the datapath. The second challenge is how to effectively illustrate the inner workings of adversarial examples based on the extracted datapaths. A state-of-the-art DNN IEEE Conference on Visual Analytics Science and Technology (VAST) 21-26 October 2018, Berlin, Germany 978-1-5386-6861-0/18/$31.00 ©2018 IEEE usually contains hundreds of layers, with millions of neurons in each layer <ref type="bibr" target="#b20">[21]</ref>. Thus, an extracted datapath potentially contains millions of neurons and even more connections. Directly visualizing all the neurons and the corresponding connections in a datapath will lead to excessive visual clutter.</p><p>To tackle these challenges, we have developed a visual analytics tool, AEVis, to explain the root cause of the wrong predictions introduced by adversarial examples. The key is to effectively extract and understand the datapath of adversarial examples. We formulate the datapath extraction as a subset selection problem, which is NPcomplete <ref type="bibr" target="#b11">[12]</ref>. To analyze the adversarial examples in large DNNs, we approximate the subset selection problem as an easy-to-solve quadratic optimization by Taylor decomposition <ref type="bibr" target="#b44">[45]</ref>, and solve the quadratic optimization using back-propagation <ref type="bibr" target="#b7">[8]</ref>. Based on the extracted datapaths, we design a multi-level visualization that enables experts to effectively explore and compare datapaths from the high-level layers to the detailed neuron activations. In particular, at the layer-level, we design a segmented directed acyclic graph (DAG) visualization to provide an overview of the datapaths <ref type="figure">(Fig. 1  (b)</ref>). As shown in <ref type="figure">Fig. 1 (c)</ref>, the detailed neuron activations are presented as heat maps that are familiar to machine learning experts (neuron level). Between the layer level visualization and neuron level, we have added a feature map level because a layer may contain millions of critical neurons. A DNN, especially a convolutional neural network (CNN), organizes neurons in feature maps, each of which is a set of neurons sharing the same weight and thus detecting the same feature. This inherent property enables the features to be recognized regardless of their position in the input (e.g., an image) and thus improves the generalization of DNNs <ref type="bibr" target="#b16">[17]</ref>. At the feature map level, we employ an Euler diagram to illustrate and compare critical feature maps belonging to different datapaths. Two case studies are conducted to demonstrate that our approach can better explain the working mechanism of both white-box and black-box adversarial examples.</p><p>The key technical contributions of this paper are:</p><p>• A visual analytics tool that explains the primary cause of the wrong predictions introduced by adversarial examples. • A datapath extraction method that discloses critical neurons and their connections that are responsible for a prediction. • A multi-level datapath visualization that enables experts to examine and compare datapaths, from the high-level layers to the detailed neuron activations. In this paper, we focus on analyzing adversarial examples generated for CNNs on the task of image classification, because currently most adversarial example generation approaches focus on attacking CNNs on the image classification task <ref type="bibr" target="#b0">[1]</ref> . Besides CNNs, AEVis can be directly used to analyze other types of deep models, such as multilayer perceptrons (MLPs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Visual Analytics for Explainable Deep Learning</head><p>A number of visual analytics approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b56">57]</ref> have been developed to illustrate the working mechanism of DNNs. A comprehensive survey on exploring the space of interpretability interfaces was presented by Olah et al. <ref type="bibr" target="#b37">[38]</ref>. Most recent approaches on explainable deep learning can be categorized into two groups: network-centric <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b56">57]</ref> and example-centric approaches <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Network-centric approaches focus on illustrating the network structure of a DNN. Tzeng et al. employed a DAG visualization to illustrate the neurons and their connections. In particular, each neuron is represented by a node and their connections are represented by edges. Their method can illustrate the structure of a small neural network, but suffers from severe visual clutter when visualizing state-of-the-art DNNs. To solve this problem, Liu et al. <ref type="bibr" target="#b28">[29]</ref> developed a scalable visual analytics tool, CNNVis, based on clus-tering techniques. It helps machine learning experts to diagnose a failed training process. Wongsuphasawat et al. <ref type="bibr" target="#b56">[57]</ref> developed a tool with a scalable graph visualization (TensorFlow Graph Visualizer) to present the dataflow graph of a DNN. To produce a legible graph visualization, they apply a set of graph transformations that converts the low-level dataflow graph to the high-level structure of a DNN. The aforementioned approaches facilitate experts in better understanding the network structure, but they are less capable of explaining the predictions of individual examples. For example, the TensorFlow Graph Visualizer developed by Wongsuphasawat et al. <ref type="bibr" target="#b56">[57]</ref> does not extract and disclose the datapath of a set of examples, which is critical for identifying the root cause of the misclassification produced by adversarial examples.</p><p>There are several recent attempts to explain how DNNs make predictions for examples (example-centric approaches). A widely used approach is to feed a set of examples into a DNN, and visualize the internal activations produced by the examples. For example, Hartley et al. <ref type="bibr" target="#b19">[20]</ref> developed an interactive node-link visualization to show the activations in a DNN. Although this method is able to illustrate detailed activations on feature maps, it suffers from severe visual clutter when dealing with large CNNs. To solve this problem, Kahng et al. <ref type="bibr" target="#b23">[24]</ref> developed ActiVis to interpret large-scale DNNs and their results. They employed a multiple coordinated visualization to facilitate experts in comparing activations among examples and reveal the causes for misclassification. Although ActiVis can show the causes for misclassification to some extent, it cannot be directly used to illustrate the primary causes of the wrong prediction caused by adversarial examples as it heavily relies on expert knowledge to select which layer to examine. In addition, we argue that purely relying on activations in one layer will result in misleading results (Sec. 6.1). To solve this problem, we propose combining activations and gradients for selecting critical neurons at different layers, which are connected to form a datapath for the adversarial examples of interest. In addition, we integrate a DAG visualization with dot plots, which provide guided visual hints to help experts select the layer of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Adversarial Attacks on Deep Learning</head><p>Adversarial attacks are a new research focus of DNNs <ref type="bibr" target="#b0">[1]</ref>. Existing efforts mainly focus on the generation of adversarial examples <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b36">37]</ref> and how to defend against adversarial examples <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b62">63]</ref>.</p><p>The key of generating an adversarial example is to find a very small perturbation that can mislead DNNs into misclassification. Recently, researchers have proposed a variety of approaches for finding such perturbations, including the Fast Gradient Sign Method <ref type="bibr" target="#b17">[18]</ref>, universal adversarial perturbation <ref type="bibr" target="#b33">[34]</ref>, and DeepFool <ref type="bibr" target="#b34">[35]</ref>.</p><p>The generation of adversarial examples has inspired researchers to develop several methods to defend against adversarial attacks. A natural defense is training a DNN using adversarial examples (adversarial training) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b52">53]</ref>. Although adversarial training can defend the training adversarial examples, Moosavi-Dezfooli et al. <ref type="bibr" target="#b33">[34]</ref> discovered that new types of adversarial examples can be generated to attack DNNs that have been trained in this way. To tackle this issue, a more effective strategy is to change the network structure to improve the defense against unseen adversarial examples. Adding regularization to the corresponding layer(s) is one of the most commonly used methods for this. The typical regularization methods include input gradient regularization <ref type="bibr" target="#b43">[44]</ref> and layer-wise Lipschitz regularization <ref type="bibr" target="#b10">[11]</ref>. However, due to the limited understanding of the working mechanism of adversarial examples, the above defense strategies are often very heavy, which usually leads to performance degradation for large, complex datasets such as ImageNet <ref type="bibr" target="#b45">[46]</ref>. To solve this problem, we have developed a visual analytics tool to explain the primary cause of the wrong predictions introduced by adversarial examples. Based on a few vulnerable neurons identified by AEVis ( <ref type="figure">Fig. 1A)</ref>, machine learning experts can design more targeted and effective defense strategies.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE DESIGN OF AEVIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivation</head><p>The development of AEVis is collaborated with the machine learning team that won the first place in the NIPS 2017 non-targeted adversarial attack and targeted adversarial attack competitions, which aim at attacking CNNs <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b50">51]</ref>. Despite their promising results, the experts found that the research process was inefficient and inconvenient, especially the explanation of the model outputs. In their research process, a central step is explaining misclassification introduced by adversarial examples. Understanding why an error has been made helps the experts detect the weakness of the model and further design a more effective attacking/defending approach. To this end, they desire to understand the roles of the neurons and their connections for prediction. Because there are millions of neurons in a CNN, examining all neurons and their connections is prohibitive.</p><p>In the prediction of a set of examples, the experts usually extract and examine the critical neurons and their connections, which are referred to as datapaths in their field. To extract datapaths, the experts often treat the most activated neurons as the critical neurons <ref type="bibr" target="#b61">[62]</ref>. However, they are not satisfied with the current activation-based approach because it may result in misleading results. For instance, considering an image with highly recognizable secondary objects, which are mixed with the main object in the image. The activations of the neurons that detect the secondary objects are also large, however, the experts are not interested in them because these neurons are often irrelevant to the prediction of the main object. Currently, the experts have to rely on their knowledge to manually ignore these neurons in the analysis process.</p><p>After extracting datapaths, the experts examine them to understand their roles for prediction. Currently, they utilize discrepancy maps <ref type="bibr" target="#b63">[64]</ref>, heat maps <ref type="bibr" target="#b61">[62]</ref>, and weight visualization <ref type="bibr" target="#b17">[18]</ref> to understand the role of the datapaths. Although these methods can help the experts at the neuron level, they commented that there lacked an effective exploration mechanism enabling them to investigate the extracted datapaths from high-level layers to individual neurons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Requirement Analysis</head><p>To collect the requirements of our tool, we follow the humancentered design process <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b31">32]</ref>, which involves two experts (E 1 and E 2 ) from the winning team of the NIPS 2017 competition. The design process consists of several iterations. In each iteration, we present the developed prototype to the experts, probe further requirements, and modify our tool accordingly. We have identified the following high-level requirements in this process. Among these requirements, R2 and R3 are two initial requirements, while R1 and R4 are gradually identified in the development. R1 -Extracting the datapath for a set of examples of interest. Both experts expressed the need for extracting the datapath of an example, which serves as the basis for analyzing why an adversarial example is misclassified. In a CNN, different neurons learn to detect different features <ref type="bibr" target="#b61">[62]</ref>. Thus, the roles of the neurons are different for the prediction of an example. E 1 said that analyzing the datapath can greatly save experts' effort because they are able to focus on the critical neurons instead of examining all neurons. Besides the datapath for individual examples, E 1 emphasized the need for extracting the common datapath for a set of examples of the same class. He commented that the datapath of one example sometimes is not representative for the image class. For example, given an image of a panda's face, the extracted datapath will probably not include the neuron detecting the body of a panda, which is also a very important feature to classify a panda. R2 -Providing an overview of the datapath. In a large CNN, a datapath often contains millions of neurons and connections. Directly presenting all neurons in a datapath will induce severe visual clutter. Thus, it is necessary to provide experts an overview of a datapath. E 1 commented, "I cannot examine all the neurons in a datapath because there are too many of them. In the examining process, I often start by selecting an important layer based on my knowledge, and examine the neurons in that layer to analyze the learned features and the activations of these neurons. The problem of this method is when dealing with a new architecture, I may not know which layer to start with. Thus, I have to examine a bunch of layers, which is very tedious." Thus, it is necessary to provide the experts an overview of the datapath with visual guidance to facilitates experts in selecting the layer of interest. The requirement of providing an overview of a CNN aligns well with previous research <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b56">57]</ref>. R3 -Exploring a datapath at the detailed levels. Although the overview of a datapath facilitates experts in finding the layer of interest, it is not enough to diagnose the root cause of the wrong prediction introduced by an adversarial example. The experts said that they wanted to link the overview of a datapath with detailed neuron activations. This linkage helps them identify the most important neurons that lead to the misclassification. Since a layer may contain millions of critical neurons, the experts also desired a medium level between the layer level and neuron level. For CNNs, the experts recommended to group neurons into feature maps. E 2 said that, "Neurons in a feature map learn to detect the same feature. Grouping them is very common in our research." Previous research also indicates that visual analytics for deep learning benefits from multi-level visualization <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b27">28]</ref>. R4 -Comparing multiple datapaths. An adversarial example is often generated by slightly perturbing the pixel values of a normal image. Accordingly, a normal image and the corresponding adversarial example are nearly the same in the input space. However, their prediction results are different. The experts are interested in how they diverge to different predictions. For example, E 2 commented, "I want to know whether there are some critical 'diverging points' for the prediction difference or it is accumulated gradually layer by layer through the network." To this end, E 2 desired to compare the datapaths of normal examples and adversarial examples. Inspired by E 2 , E 1 added that it was interesting to compare the datapath of an adversarial example (e.g., a panda image that is misclassified as a monkey) with that of the images from the predicted class (e.g., normal images containing monkeys). Such comparison helps them understand how these very different images "merge" into the same prediction (e.g., the monkey). The need of visual comparison is consistent with the findings of previous research <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b29">30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">System Overview</head><p>Driven by the requirements collected from the experts, we have developed a visual analytics tool, AEVis, to illustrate the root cause of the robustness issues caused by adversarial examples. This tool consists of the following two parts.</p><p>• A datapath extraction module that extracts the critical neurons and their connections for a set of selected examples (R1). • A datapath visualization module that provides an overall understanding of the datapath of interest (R2), illustrates datapaths in multiple levels (R3), and facilitates experts to visually compare several datapaths (R4). As shown in <ref type="figure" target="#fig_2">Fig. 2</ref>, AEVis takes a trained CNN and the examples to be analyzed as the input. The examples include both normal examples ( <ref type="figure" target="#fig_2">Fig. 2A</ref>) and adversarial examples <ref type="figure" target="#fig_2">(Fig. 2B</ref>). Given the examples and the CNN, the datapath extraction module extracts the critical feature maps and their connections that are responsible for the predictions of the examples <ref type="figure" target="#fig_2">(Fig. 2 (a)</ref>). The extracted datapaths are then fed into the datapath visualization module <ref type="figure" target="#fig_2">(Fig. 2 (b)</ref>), which supports the navigation and comparison of the datapaths from the high-level layers to the detailed neuron activations.</p><p>A typical analysis workflow of AEVis is shown in <ref type="figure">Fig. 1</ref>. An expert explores the image list ( <ref type="figure">Fig. 1 (a)</ref>) and selects several groups of images for further understanding and analysis. After clicking on the 'Analyze' button, AEVis first provides an overview of the datapaths at the layer level ( <ref type="figure">Fig. 1 (b)</ref>). At the layer level, each rectangle represents a layer group and a dot plot is combined with each layer group to illustrate the similarities between/among the extracted datapaths of the layers in each layer group. By examining the dot plots, the expert is able to detect layers of interest and further examines the feature maps in the layer (L C and L D in <ref type="figure">Fig. 1</ref>). In each layer, the Euler-diagram-based design helps the expert focus on the share/unique feature maps of several datapaths. Aided by this design and the color coding (e.g., activations) of feature maps, the expert can select a feature map of interest (F C and F D in <ref type="figure">Fig. 1)</ref>, and explore the detailed neuron activations as well as the learned features of the neurons in this feature map ( <ref type="figure">Fig. 1 (c)</ref>). It facilitates him/her in finding the root cause (e.g., a set of neurons) for the misclassification of the adversarial examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DATAPATH EXTRACTION 4.1 Motivation and Problem Formulation</head><p>Extracting the datapath that are responsible for the prediction of a group of examples is the basis for analyzing why an adversarial example is misclassified (R1). The key challenge is to identify the critical neurons as selecting the corresponding connections to form the datapath is straightforward. Next we discuss how to extract the critical neurons for an individual example and then extend our approach to selecting critical neurons for a set of examples.</p><p>A commonly used method is to treat the most activated neurons as the critical neurons <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b23">24]</ref>. However, this method may result in misleading results, especially when a highly recognizable secondary object is mixed with the main object in an image. For example, <ref type="figure" target="#fig_3">Fig. 3</ref> shows the top 2 critical neurons found by such activationbased approach for a sheepdog image ( <ref type="figure" target="#fig_3">Fig. 3 (a)</ref>, employed network: ResNet-101 <ref type="bibr" target="#b20">[21]</ref>, image label: Shetland sheepdog). We can see that the second highly activated neuron learns to detect the head of a dog ( <ref type="figure" target="#fig_3">Fig. 3 (b)</ref>), which is indeed critical for classifying a sheepdog. However, the most critical neuron learns to detect a ball ( <ref type="figure" target="#fig_3">Fig. 3 (b)</ref>), which is not an important feature for classifying a sheepdog. Such a misleading result roots in that the neurons have complex interactions with each other and the activations of the neurons are processed by a highly nonlinear function to generate the final prediction. Thus, highly activated neurons may not be the critical neurons for a prediction. Such critical neurons are the neurons that highly contributed to the final prediction. In other word, by only combining the contributions of the critical neurons, the prediction of an example will not be changed. To this end, we aim at selecting a minimized subset of neurons, which keep the original prediction. Accordingly, we formulate critical neurons extraction as a subset selection problem:</p><formula xml:id="formula_0">N opt = arg min N s ⊆N (p(x) − p(x; N s )) 2 + λ |N s | 2 .<label>(1)</label></formula><p>The first term is to keep the original prediction and the second term ensures to select a minimized subset of neurons. Specifically, N is the set of the neurons in a CNN, N s is a subset of neurons N, N opt is the calculated critical neurons, p(x) is the prediction of example x, and p(x; N s ) is the prediction if we only consider the neuron subset N s . To measure the difference between two predictions, we adopt the widely used L2-norm. |N s | is the size of N s and λ is used to balance the two terms. A larger λ indicates a tendency to select a smaller subset of neurons. As discussed in Sec. 3.2, extracting the common datapath for a set of examples instead of an individual example can improve the representativeness of the extracted datapath (R1). A natural extension of Eq. (1) is minimizing the sum of the difference in the prediction of the example set X:</p><formula xml:id="formula_1">N opt = arg min N s ⊆N ∑ x k ∈X (p(x k ) − p(x k ; N s )) 2 + λ |N s | 2 .<label>(2)</label></formula><p>Next, we discuss how to effectively solve this problem. For simplicity, we take Eq. (1) (critical neurons for one example) as an example to illustrate the solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Solution to Subset Selection</head><p>Directly solving the subset selection problem (Eq. (1)) is timeconsuming because: (1) it is an NP-complete problem and (2) the search space is prohibitively large due to the large number of neurons in a CNN. We therefore combine a divide-and-conquer approach with a quadratic optimization to reduce the search space and find a more accurate approximation. In particular, we develop a divideand-conquer-based search space reduction method by splitting the subset selection problem into a series of separate subproblems and grouping the neurons into feature maps. As each subproblem is still NP-complete, we then employ the quadratic approximation to more accurately approximate each NP-complete subproblem as an easy-to-solve quadratic optimization by Taylor decomposition <ref type="bibr" target="#b44">[45]</ref>, and solve it based on back-propagation <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Search space reduction</head><p>A CNN is traditionally represented as a directed acyclic graph (DAG), where each node represents a neuron and each edge denotes a connection between nodes. A widely-used approach to accelerate DAG-related algorithms is processing the nodes layer by layer <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b49">50]</ref>. Inspired by these algorithms, we split the original subset selection problem (Eq. (2)) into a set of subproblems. Each selects the critical neurons in one layer:</p><formula xml:id="formula_2">N i opt = arg min (p(x) − p(x; N i s ∪ N −i )) 2 + λ i |N i s | 2 , where N i s ⊆ N i , N i</formula><p>is the set of the neurons in layer i, and N −i is the set of all other neurons in the CNN except the ones in layer i. After solving all the subproblems, we aggregate all the sub-solutions N i opt into the final critical neuron set N opt = i N i opt . Although dividing the original problem into a set of subproblems can largely reduce the search space, the search space of each subproblem is still large because a layer may contain more than one million neurons. Thus, we group neurons into a set of feature maps to further reduce the search space. In a CNN, neurons in a feature map share the same weights, and thus learn to detect the same feature. Utilizing this characteristics, we formulate the feature map selection problem as</p><formula xml:id="formula_3">F i opt = arg min F i s ⊆F i (p(x) − p(x; F i s ∪ F −i )) 2 + λ i |F i s | 2 ,<label>(3)</label></formula><p>where F is the set of feature maps in a CNN, F i is the set of the feature maps in layer i, F i s is a subset of F i , and F −i is the set of all other feature maps in the CNN except the ones in layer i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Quadratic Approximation</head><p>Although we have reduced the search space from millions of dimensions (neurons in a layer) to thousands of dimensions (feature maps in a layer), it is still time-consuming to solve Eq. (3) because it is an NP-complete discrete optimization problem. To tackle this issue, we transform the discrete optimization into a continuous optimization problem. In particular, we reformulate Eq. (3) as:</p><formula xml:id="formula_4">z i opt = arg min (p(x) − p(x; z i )) 2 + λ i (∑ j z i j ) 2 , where z i = [z i 1 , ..., z i n ] and z i j ∈ {0, 1}</formula><p>is an indicator to represent whether the j-th feature map in layer i is critical. If the feature map is critical, z i j = 1, otherwise, z i j = 0. Inspired by spectral clustering <ref type="bibr" target="#b35">[36]</ref>, we approximate the discrete optimization with a continuous optimization by removing the discreteness condition z i j ∈ {0, 1} and allowing z i j to take a value in [0, 1]:</p><formula xml:id="formula_5">z i opt = arg min z i j ∈[0,1] (p(x) − p(x; z i )) 2 + λ i ( ∑ j z i j ) 2 ,<label>(4)</label></formula><p>Eq. (4) can be solved using gradient-based numerical optimization approaches such as the BFGS (Broyden−Fletcher−Goldfarb−Shanno) algorithm <ref type="bibr" target="#b57">[58]</ref>. The gradient ∂ p/∂ z i j is calculated by back-propagation <ref type="bibr" target="#b7">[8]</ref>. However, this method is computationally expensive because the gradient-based optimization is an iterative process where we have to calculate the gradients ∂ p/∂ z i j at each iteration. According to the calculation process of back-propagation, the deeper a CNN is, the longer it takes to compute the gradients.</p><p>To tackle this issue, we approximate Eq. (4) as a quadratic optimization where we calculate the gradients only once. Since p(x) = p(a 1 , ..., a n ) and p(x; z i ) = p(z i j a 1 , ..., z n j a n ), we rewrite Eq. <ref type="formula" target="#formula_5">4</ref>as a multivariate function. Here a j is the activation vector of the j-th feature map produced by example x. We then the Taylor decomposition <ref type="bibr" target="#b44">[45]</ref> </p><formula xml:id="formula_6">is to decompose p(x) − p(x; z i ) into a linear form: p(x) − p(x; z i ) ≈ ∑ j (1 − z i j )a j • ∂ p ∂ a i j a , where a = [a i 1 , ..., a i n ]</formula><p>and x • y represents dot product between vectors x and y. By substituting the above decomposition into Eq. <ref type="formula" target="#formula_5">4</ref>, we obtain a quadratic optimization:</p><formula xml:id="formula_7">z i opt = arg min z i j ∈[0,1] z i (Q i + λ i I)(z i ) T − 2qq i • z i ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_8">q i = [a i 1 • ∂ p ∂ a i 1 a , • • • , a i n • ∂ p ∂ a i n a</formula><p>], q is the sum of q i , Q = (q i ) T q i is a n × n matrix, and I is a n × n identity matrix. Solving Eq. (5) only needs to evaluate the gradients once. We use the BFGS algorithm to solve Eq. <ref type="bibr" target="#b4">(5)</ref>. To control the scale of the two terms, the parameter λ i is set to 0.1/|F i | 2 , where |F i | is the number of the feature maps in layer i.</p><p>In the same way, we obtain a solution for selecting critical feature maps for a set of examples (Eq. (2)). Compared with Eq. (5), there are two differences: (1) replacing Q i by ∑ Q i k and (2) replacing qq i by ∑ q k q i k , where k is the index of the example in the example set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DATAPATH VISUALIZATION</head><p>An extracted datapath usually contains millions of neurons and even more connections, which prohibits experts from efficiently examining the datapath layer by layer. To help experts effectively explore the extracted datapaths, we design a multi-level datapath visualization, which enables experts to analyze and compare datapaths from high-level layers to detailed neuron activations (R2, R3, R4).</p><p>Based on our discussions with the experts, we visualize datapaths on three levels: the layer level, the feature map level, and the neuron level. At each level, we (1) calculate the layout of the items (layers, feature maps, neurons) to reveal the relationships among them; and (2) provide visual hints to guide experts in finding the item(s) of interest and zooming in to a more-detailed level (e.g., from the layer level to the feature map level). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Layer-level Visualization</head><p>The layer-level visualization provides an overview of the extracted datapath, and guides experts in selecting a layer to examine (R2). Layout. At the layer level, we focus on illustrating how layers are connected. Initially, we employ the widely-used TensorFlow Graph Visualizer <ref type="bibr" target="#b56">[57]</ref>. In particular, we formulate a CNN as a DAG (directed acyclic graph), where each layer is a node and the connections between layers are edges. Based on this formulation, a node-link diagram with a vertical layout is employed to visualize the layers and their connections. Then the DAG layout algorithm in <ref type="bibr" target="#b49">[50]</ref> is employed to calculate the position of each layer. To handle large CNNs, a hierarchy of the layers is built, where each leaf node is a layer and each non-leaf node is a layer group <ref type="bibr" target="#b56">[57]</ref>.</p><p>The experts were overall satisfied with this design, but after they tried the prototype they commented that they often need to zoom in to the lower levels of the layer hierarchy in order to analyze the root cause of a misclassification. It is tedious for them to zoom in for each adversarial example. In addition, when they zoom in to the lower levels, the visualization often suffers from a long skinny strip with a very high aspect ratio (e.g., <ref type="figure" target="#fig_8">Fig. 8 (a)</ref> in <ref type="bibr" target="#b56">[57]</ref>). This phenomenon is worse when they analyze state-of-the-art CNNs, such as ResNet <ref type="bibr" target="#b20">[21]</ref> with 50-200 layers.</p><p>To solve the above problems, we combine a treecut algorithm and a segmented DAG visualization to save experts' efforts and generate a layout with a better aspect ratio, respectively. Treecut. To save experts' efforts of zooming in, we use a treecut algorithm <ref type="bibr" target="#b12">[13]</ref> to select an initial set of layers (around 50 layers) from the layer hierarchy. In this algorithm, the DOI measures the datapath difference between two sets of images (e.g., adversarial examples and normal examples). Segmentation. Inspired by the segmented timeline <ref type="bibr" target="#b9">[10]</ref>, we propose transposing the vertical layout into a horizontal layout, segmenting the initial DAG into several parts, and visualizing the segmented parts from top to bottom. The experts commented that this horizontal design resembles a calendar and thus is familiar to them. As shown in <ref type="figure" target="#fig_4">Fig. 4 (a)</ref>, our segmented DAG visualization effectively illustrates the connections among dozens of layers with good aspect ratios.</p><p>The key challenge of segmenting a DAG is to decide where to segment it. We formulate the segmentation problem as a "printing neatly" problem <ref type="bibr" target="#b11">[12]</ref>, in which we aim to minimize the cost function that sums up the empty space at the end of each line while ensuring that no word is off screen. This optimization method is suitable for a CNN with a chain structure, such as VGG net <ref type="bibr" target="#b46">[47]</ref>. However, state-of-the-art CNNs (e.g., ResNet <ref type="bibr" target="#b20">[21]</ref>, and DenseNet <ref type="bibr" target="#b21">[22]</ref>) often contain basic building blocks, whose layers can split and merge (e.g., <ref type="figure" target="#fig_4">Fig. 4A</ref>). These building blocks are often connected with others as a chain. Directly minimizing the above cost function may cut the basic building blocks apart, which hinders the understanding of the network structure ( <ref type="figure" target="#fig_4">Fig. 4 (b)</ref>). To solve this problem, we add a regularization term to the cost function to penalize a segmentation scheme that cuts a building block into two parts:</p><formula xml:id="formula_9">min e i ≥0 k−1 ∑ i=1 e i + λ c i ,<label>(6)</label></formula><p>where k is the number of segments, e i is the empty space at the end of segment i, c i represents whether a building block is cut by segment i, and λ is used to balance the two terms. In AEVis, experts can interactively modify λ . Eq. (6) can be efficiently solved using dynamic programming. As shown in <ref type="figure" target="#fig_4">Fig. 4</ref>, if we only consider the empty-space objective, building block A will be cut into two parts <ref type="figure" target="#fig_4">(Fig. 4 (b)</ref>). Adding the regularization term can avoid unnecessary cuts by balancing between the small empty space and the protection of the building blocks ( <ref type="figure" target="#fig_4">Fig. 4 (a)</ref>). Visual hint. Showing how the layers are connected is not enough in guiding experts to find the layer of interest. Thus, for each node (a layer or layer group) in the segmented DAG, we provide a visual statistical description of the datapath(s) in that layer (group). Because there is limited space for each node and a layer group usually contains dozens of layers, we employ a dot plot as the visual hint, which is a compact visualization and widely used for relatively small datasets <ref type="bibr" target="#b55">[56]</ref>. In particular, in a dot plot, each dot represents a high-level statistics of a layer <ref type="figure" target="#fig_5">(Fig. 5)</ref>. The position of a dot on the x-axis denotes the value of the high-level statistics of the datapath(s) in that layer, such as the activation similarity between two datapaths (the datapath for adversarial examples and the one for normal examples). In particular, the activation similarity is calculated as ∑ sim(I i A , I i N )/N, where sim(., .) is the widely-used cosine similarity, I i A , I i N is a pair of adversarial and normal examples, and N is the number of such pairs in the examples. Other high-level statistics include the averaged activations of an example set on the corresponding datapath in that layer and the topological similarity between two datapaths (measured by Jaccard similarity). Examining the dot plots node by node helps experts detect the "diverging point," where a normal image and its corresponding adversarial example diverge into different predictions <ref type="figure" target="#fig_10">(Fig. 10)</ref>. Experts are able to examine the feature maps in the layer group of interest <ref type="figure">(Fig. 1)</ref> or expand it to examine child layers or layer groups <ref type="figure" target="#fig_4">(Fig. 4 (a)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Feature-map-level Visualization</head><p>When an expert detects a layer of interest, he/she then zooms in to the layer to examine the critical feature maps in that layer. To preserve the analysis context, we visualize the feature maps in the selected layer as the focus and other layers are shown as context. <ref type="figure">(Fig. 1 (b)</ref>). Layout. The belonging relationships between the feature maps and the extracted datapaths in one layer, including the unique feature maps of each datapath and the shared ones between/among them, are very useful for understanding the roles of these feature maps. As finding unique/share elements based on their set (datapath) membership is an important task tackled by the set visualization <ref type="bibr" target="#b3">[4]</ref>, we then formulate the feature map layout problem as a set visualization problem. Among various set visualization techniques, such as the Euler diagram, the line-based techniques (e.g., LineSets <ref type="bibr" target="#b2">[3]</ref>), and the matrix-based techniques (e.g., ConSet <ref type="bibr" target="#b24">[25]</ref>), we decide to employ on the Compact Rectangular Euler Diagram <ref type="bibr" target="#b41">[42]</ref> (ComED) because:</p><p>• It can well depict set relations and thus disclose the unique/share feature maps <ref type="bibr" target="#b3">[4]</ref>;</p><p>• Machine learning experts are familiar with Euler diagrams and they often use them to understand the set relationships; • The number of feature maps in each datapath is clearly conveyed in the Euler diagram, which is important for the analysis. As in ComED <ref type="bibr" target="#b41">[42]</ref>, we split the datapaths in the layer of interest by their intersection relations. This produces a hierarchy, which is visualized by non-overlapping rectangles <ref type="figure">(Fig. 1L C )</ref>. Then the split parts of datapaths are connected with lines, which can be shown on demand. Compared with ComED, we make two modifications to reduce visual clutter and better facilitate experts' visual comparison:</p><p>• Utilize K-Means <ref type="bibr" target="#b7">[8]</ref> to cluster the feature maps by their activations to reduce the visual clutter caused by a large number of feature maps <ref type="figure" target="#fig_6">(Fig. 6</ref>). Experts can interactively modify the parameters of the clustering (e.g., the number of the clusters k) to reduce the parameter sensitivity. • Use the Treemap layout instead of the graph layout because the graph layout result has an irregular boundary, which is not suitable for juxtaposing multiple layers to compare the datpaths in them ( <ref type="figure">Fig. 1L</ref> C and L D ). Visual hint.</p><p>Although the Eulerdiagram-based design reveals the shared/unique feature maps very well, it does not significantly help to efficiently select an individual feature map cluster of interest for further examination. To this end, for each cluster, we illustrate its average importance (z i j in Eq. <ref type="formula" target="#formula_5">4</ref>, Sec. 4) or average activation, because experts usually start their analysis from the most critical or most highly activated feature maps. The averaged importance/activation of a feature map cluster is encoded by the color of the feature map cluster <ref type="figure" target="#fig_6">(Fig. 6)</ref>. In addition to importance/activation, we also encode the size of a feature map cluster because it is a basic information for a cluster. We employ a series of stacked rectangles to represent a feature map cluster <ref type="figure" target="#fig_6">(Fig. 6)</ref>, and use the number of stacked rectangles to encode the size of a cluster. To avoid the visual clutter caused by a large number of stacked rectangles, the number of rectangles N R is proportional to the log of the cluster size N C : N R = log 10 (N C ) + 1. For the sake of consistency, a cluster with only one feature map in it is shown as a single rectangle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Neuron-level Visualization</head><p>When an expert finds a feature map of interest, AEVis allows him/her to examine the neurons in that feature map. As there are hundreds or even thousands of neurons in a feature map and dozens of images are often analyzed simultaneously, we cannot show all the activations in place due to the limited space. Thus, we add a neuron panel <ref type="figure">(Fig. 1  (c)</ref>) to show the neurons. To preserve the visual link <ref type="bibr" target="#b47">[48]</ref> of the selected feature map and the corresponding neurons, we add the same label to the feature map and the neurons <ref type="figure" target="#fig_7">(Fig. 7A)</ref>. Layout. In a feature map, the neurons are naturally organized on a grid. The position of a neuron is determined by the position of the image patch that influences the activation of this neuron <ref type="bibr" target="#b16">[17]</ref>. Visual hint. Following previous research <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b28">29]</ref>, we employ the learned features of neurons and the activations of the neurons to help expert understand the roles of the neurons for the prediction.</p><p>The activation of a neuron is represented by its color (red: negative activation; green: positive activation). Combining the color coding and the grid-based layout of neurons, experts can detect which part of the image highly activates the neurons in the feature map. For example, in <ref type="figure" target="#fig_7">Fig. 7</ref>, we can find the neurons on the top right corner are highly activated, which corresponds to the panda head in the image.</p><p>To visualize the learned features of neurons, we first try the method used in previous papers on explainable deep learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>This method selects the image patches that highly activate a neuron to represent its learned feature. However, we discovered that when handling very deep CNNs (e.g., ResNet101 <ref type="bibr" target="#b20">[21]</ref>), this method cannot illustrate the exact region in each image patch that highly activates a feature map neuron, especially for the neurons in top layers. This is because the activations of neurons in top layers of a very deep CNN are influenced by a large image patch. For example, the neurons in more than at layer 10 and deeper are influenced by all the pixels in an image in ResNet101. Thus, we employ the discrepancy map <ref type="bibr" target="#b63">[64]</ref> to highlight which region in the image actually highly activates a neuron and treat this region as the learned feature of this neuron. To calculate the discrepancy map for a neuron, Zhou et al. <ref type="bibr" target="#b63">[64]</ref> occluded a very small patch of an image (8 pixels× 8 pixels), and calculated the new activation of the neuron produced by the occluded image. If the activation changes a great deal, the small patch is marked as important. This process iterates many times, and for each iteration, a different patch is checked. After this process, all the important small patches of the image remain unchanged and other pixels are deemphasized by lowering their lightness. For example, in <ref type="figure" target="#fig_7">Fig. 7</ref>, by examining the discrepancy maps, we found that the neurons in the selected feature map learned to detect a panda head. Without detecting the important region, we may draw the wrong conclusion that the neurons learned to detect a whole panda. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATION</head><p>As part of our evaluation, we first performed a qualitative evaluation to demonstrate the effectiveness of the datapath extraction algorithm. Then, two case studies were conducted to illustrate how AEVis helps the experts E 1 and E 2 analyze both white-box and black-box adversarial examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Qualitative Analysis of Datapath Extraction</head><p>Top critical feature maps  Datapath extraction for a single example. <ref type="figure" target="#fig_8">Fig. 8</ref> shows the top 5 critical feature maps extracted by our approach for the same image in <ref type="figure" target="#fig_3">Fig. 3</ref>. Feature maps 1,2,3 and 5 ( <ref type="figure" target="#fig_8">Fig. 8 (b)</ref>) learn to detect the features of dog ears, head, etc. These features are indeed important to classify a Shetland sheepdog. Compared with the activationbased approach, our approach is able to ignore the irrelevant feature map that learns to detect a ball <ref type="figure" target="#fig_3">(Fig.3 (b)</ref>). Besides these easy-tounderstand feature maps, the learned feature of feature map 4 <ref type="figure" target="#fig_8">(Fig 8  (b)</ref>) is difficult to understand at first glance. To understand what this feature map actually learns, we calculate a set of discrepancy maps <ref type="figure" target="#fig_8">(Fig 8 (b)</ref>). A common property of these discrepancy maps is that they have white stripes with black strokes. Thus, we conclude that this feature map learns to detect such a feature. However, it is still unclear why this feature map is considered critical in classifying a sheepdog. To answer this question, we loaded more sheepdog images. We found that a distinctive feature of a sheepdog was a white strip between its two dark-colored eyes. This feature was ignored by us at first because it is not obvious in the original example ( <ref type="figure" target="#fig_8">Fig. 8 (a)</ref>). To verify our assumption, we further visualized the activations on the feature map ( <ref type="figure" target="#fig_8">Fig. 8 (c)</ref>) and found that this feature map was indeed highly activated by the white strip in the images of sheepdogs. Our experts were impressed with this finding, because the datapath extraction algorithm not only finds a commonsensical feature map, but also finds unexpected feature map(s).</p><p>Comparison between the extracted datapaths for one example and a set of examples. As shown in <ref type="figure" target="#fig_9">Fig. 9</ref>, we compared the extracted datapath for an image with only one panda face <ref type="figure" target="#fig_9">(Fig. 9A)</ref> with the one for a set of images that contain both panda faces and whole pandas <ref type="figure" target="#fig_9">(Fig. 9B)</ref>.</p><p>We found that the most critical feature maps extracted for an image with only one panda face contain the feature maps that detect a panda face <ref type="figure" target="#fig_9">(Fig. 9C and D)</ref> and important features on a panda face, such as eyes and ears <ref type="figure" target="#fig_9">(Fig. 9E)</ref>. The neuron in the blue rectangle <ref type="figure" target="#fig_9">(Fig. 9E</ref>) was unexpected at first glance. After examining more discrepancy maps for that feature map and the activation heat map <ref type="figure" target="#fig_9">(Fig. 9F)</ref>, we discovered that the feature map learned to detect black dots. Such a feature is a recognizable attribute of a panda face.</p><p>Just like the datapath of a panda's face, the datapath extracted for a set of panda images includes the feature maps for detecting a panda face <ref type="figure" target="#fig_9">(Fig. 9G and H)</ref>. In addition to these feature maps, the most critical feature maps include a feature map for detecting the black-and-white body of a panda <ref type="figure" target="#fig_9">(Fig. 9I)</ref>, which is also an important feature for classifying a panda. We visualized the activations on this feature map to verify that the feature map indeed learns to detect a panda body <ref type="figure" target="#fig_9">(Fig. 9J)</ref>.</p><p>This finding echoes requirement R1 in Sec. 3.2, i.e., that the extracted critical feature maps for one example may not be representative for a given set of images with the same class label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Case Study</head><p>There are two main types of adversarial attacks: white-box attack and a black-box attack <ref type="bibr" target="#b0">[1]</ref>. The white-box attack means that the attacker has a full knowledge of the target model, including the parameters, architecture, training method, and even the training data. While the Black-box attack assumes that the attacker knows nothing about the target model, which can be used to evaluate the transferability of adversarial examples. Next, we demonstrate how AEVis helps experts E 1 and E 2 analyze both white-box and black-box adversarial examples. In both case studies, we utilized the dataset that is from the NIPS 2017 non-targeted adversarial attack and targeted adversarial attack competitions <ref type="bibr" target="#b50">[51]</ref>, because the experts are familiar with this dataset. It contains 1,000 images (299 pixel × 299 pixel) We employed the white-box non-targeted attacking method developed by the winning machine learning group <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> to generate one adversarial example for each image in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Analyzing White-Box Adversarial Examples</head><p>Model. One of the state-of-the-art CNNs for image classification, ResNet101 <ref type="bibr" target="#b20">[21]</ref>, is employed as the target model. It contains 101 layers. We used the pre-trained model from TensorFlow <ref type="bibr" target="#b18">[19]</ref>. It achieves a high accuracy (96.3%) on the normal examples, and a very low accuracy (0.9%) on the generated adversarial examples. Analysis process. To start the analysis, we calculated an adversarial score for each image <ref type="bibr" target="#b38">[39]</ref>. A high score means the image is most probably to be an adversarial example. The expert E 1 then focused on the most uncertain images with medium adversarial scores. After examining these uncertain adversarial examples, E 1 selected one for further investigation (I B in <ref type="figure">Fig. 1</ref>). It contains a panda head but is misclassified as a guenon monkey. To understand the root cause of this misclassification, he wanted to compare its datapath with the one of the corresponding normal example (I A in <ref type="figure">Fig. 1</ref>).</p><p>To improve the representativeness of the extracted datapath for the normal example (Sec. 6.1, <ref type="figure" target="#fig_9">Fig. 9</ref>), he added 10 more normal panda images as well as the corresponding adversarial examples, to the images of interest (R1). Each added adversarial example is misclassified as a guenon monkey. Accordingly, E 1 split these images into two sets. The first set (adversarial group) contained 11 adversarial examples. The second set of images (normal group) contained 11 normal examples. Then he extracted and visualized the datapaths for these two sets of images. The datapath overview is shown in <ref type="figure" target="#fig_10">Fig. 10</ref>. The dot plots encoded the activation similarity between these two datapaths. The expert E 1 found that at the bottom layer of the network <ref type="figure" target="#fig_10">(Fig. 10L A )</ref>, the activation similarity is almost 1.0 (the dot is on the right). This similarity remained to be 1.0 until layer L B in <ref type="figure" target="#fig_10">Fig. 10</ref>. After layer L B , layer L C appeared as a "diverging point", where the activation similarity largely decreased (R4). In the following layers, the activation similarity continued decreasing to 0 (L D , L E , L F ), which resulted in the misclassification.</p><p>To analyze which feature map is critical for the divergence, the expert E 1 expanded the diverging point L C <ref type="figure">(Fig. 1)</ref>. In addition, he set the color coding of each feature map as the activation difference between two sets of examples (activation difference = activations of normal images − activations of adversarial examples). A large activation difference indicates that the corresponding feature map detects its learned feature in the normal images but did not detect such a feature in the adversarial examples. Because the feature map F C in <ref type="figure">Fig. 1</ref> shows the largest activation difference, he then checked its neurons. By examining the learned feature of the neurons <ref type="figure">(Fig. 1A)</ref>, he discovered that the neurons learned to detect a black patch that resembles a panda's ear ( <ref type="figure">Fig. 1B and C)</ref>. Such a feature is critical for detecting a panda's face, which does not appear in the adversarial example. To further investigate the influence of this feature map, E 1 continued to expand the next layer (layer L D ). He found that there was a large activation difference on the feature map for detecting a panda's face (F D in <ref type="figure">Fig. 1</ref>). This indicates that the corresponding feature map, F D , fails to detect a panda's face, which is a direct influence of the large difference on F C . By the same analysis, E 1 found that in the layer that detected the highest-level features (L E ), there was also a large activation difference on the feature map for detecting a panda's face. As the target CNN cannot detect a panda's face in highest-level features, the CNN failed to classify the adversarial example as a panda image.</p><p>The above analysis explains why the CNN failed to classify the adversarial example as a panda, but cannot explain why the adversarial example is classified as a monkey. Thus, E 1 compared the adversarial example with the corresponding example and a set of normal monkey images <ref type="figure" target="#fig_11">(Fig. 11 (a)</ref>). Inspired by the above analysis, E 1 directly expanded layer L C (diverging point), and examined its feature maps <ref type="figure" target="#fig_11">(Fig. 11 (b)</ref>). After checking the activations of the adversarial example on the "monkey's datapath", the expert found that the activations were the largest on the feature map for detecting the face of a monkey <ref type="figure" target="#fig_11">(Fig. 11 (c)</ref>  CNN was unexpected because there was no monkey face in the adversarial example at first glance. Thus, E 1 examined the detailed neuron activations on this feature map and discovered that the high activations appeared in the top right corner of the neurons, which corresponded to the top right corner of the image ( <ref type="figure" target="#fig_11">Fig. 11 (e)</ref>). E 1 did not understand why the CNN detected a monkey face at that part of the image. By carefully examining the adversarial example and the learned features of the neurons, he finally figured out the reason: there is a dark strip with a bright strip on each side <ref type="figure" target="#fig_11">(Fig. 11A</ref>). It is a recognizable attribute for the face of a guenon monkey, which made the CNN mispredict the adversarial example as a monkey. Another case is shown in <ref type="figure" target="#fig_2">Fig. 12</ref>, where a cannon image is misclassified as a racket (probability: 0.997). The reasons behind the misclassification are two-fold. First, the CNN recognizes a large wheel in the normal image ( <ref type="figure" target="#fig_2">Fig. 12A</ref>) but cannot recognize a large wheel in the adversarial image ( <ref type="figure" target="#fig_2">Fig. 12B)</ref>, while a large wheel is a recognizable attribute of a cannon <ref type="figure" target="#fig_2">(Fig. 12 (b)</ref>). Second, E 1 found that the CNN recognizes the head of a racket <ref type="figure" target="#fig_2">(Fig. 12C)</ref>, which is connected to the throat of a racket <ref type="figure" target="#fig_2">(Fig. 12D</ref>). These two reasons lead to the misclassification. E 1 commented, "There may be different subtle causes that lead to the misclassification of different adversarial examples. The value of AEVis is that it helps me find such causes quickly and effectively. I can easily integrate my knowledge into the analysis process by leveraging the provided interactive visualizations. I also see a great opportunity to use AEVis in analyzing more adversarial examples and summarize the major causes of the misclassification. This probably benefits future research on robust deep learning."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Analyzing Black-box Adversarial Examples</head><p>Model. We used the VGG-16 <ref type="bibr" target="#b46">[47]</ref> network to analyze black-box attacks because our employed attacking method has the knowledge of a set of state-of-the-art CNNs, such as ResNet <ref type="bibr" target="#b20">[21]</ref> and the Inception network <ref type="bibr" target="#b51">[52]</ref>. Thus, we adopted a traditional CNN (VGG) to analyze black-box attacks. We utilized the pre-trained VGG-16 network from TensorFlow <ref type="bibr" target="#b18">[19]</ref>. It achieved a 84.8% accuracy on the normal examples, and a 22.4% accuracy on the adversarial examples. Analysis process. The expert E 2 continued to explore the analyzed adversarial example, and tested it on the VGG network. He found that it was misclassified as a beagle (a type of dog, <ref type="figure" target="#fig_3">Fig. 13 (a)</ref>). To check how the prediction was made, E 2 extracted and compared the datapaths for normal beagle images and the adversarial panda image. He found that the there was no obvious "merging point" in VGG-16, because these images do not truly "merge" due to the high prediction score of beagle images (0.75−1.0) and the relatively low prediction score of the adversarial example (0.46). Thus, E 2 relied on his knowledge to select the layer that detected the highest-level features <ref type="figure" target="#fig_3">(Fig. 13 (b)</ref>). By setting the color coding of feature maps as the activation of the adversarial example, E 2 found a large activation appeared on a feature map in shared feature maps between the beagle's and adversarial panda's datapath <ref type="figure" target="#fig_3">(Fig. 13A)</ref>. It is a potential cause that leads to the misclassification. Thus, he examined the learned feature of the neurons in this feature map <ref type="figure" target="#fig_3">(Fig. 13 (c)</ref>), and found that they learned to detect the black nose of a beagle (a black patch, <ref type="figure" target="#fig_3">Fig. 13B</ref>). This feature is a recognizable attribute for a beagle. To understand why the adversarial example caused a large activation on this feature map, he further examined the activation heat map, and found that the neurons in the top right corner are highly activated <ref type="figure" target="#fig_3">(Fig. 13C</ref>). It indicates that there is such a feature in the corresponding part of the image. In particular, that part of the images is the black ear of the panda, which is also a black patch <ref type="figure" target="#fig_3">(Fig. 13C</ref>). This feature misled the VGG-16 network to detect a black nose in the adversarial panda image, which then led to the misclassification. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION</head><p>AEVis can better disclose the inner workings of adversarial examples and help discover the vulnerable neurons that lead to incorrect predictions. However, it also comes with several limitations, which may shed light on future research directions. Scalability. We have demonstrated that AEVis is able to analyze a state-of-the-art CNN (ResNet101), which has 101 layers and is much deeper than traditional CNNs (e.g., VGG-Net). More recently, researchers have developed many deeper CNNs with thousands of layers <ref type="bibr" target="#b20">[21]</ref>. When handling such deep DNNs, if an expert zooms in to low levels of the layer hierarchy, the layers of interest cannot fit in one screen, even with the help of our segmented DAG. To alleviate this issue, we can employ a mini-map to help the expert track the current viewpoint, which has been proven effective in TensorFlow Graph Visualizer <ref type="bibr" target="#b56">[57]</ref>. The dot plot is another factor that hinders AEVis from analyzing CNNs with thousands of layers. This is because a layer group may contain hundreds of layers, and the height of the resulting dot plot will be too large, which is a waste of screen space. To solve this problem, we can use non-linear dot plots <ref type="bibr" target="#b42">[43]</ref> to improve the scalability of AEVis.</p><p>Currently, we utilize a Euler-diagram-based design to illustrate the overlapping relationship between/among datapaths. Such a design is suitable for comparing several datapaths <ref type="bibr" target="#b3">[4]</ref>. Although researchers found that approximately four objects can be tracked in a visual comparison <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b60">61]</ref>, experts may have special needs of comparing a lot of datapaths. To fulfill these needs, we can leverage more scalable set visualization techniques, such as PowerSet <ref type="bibr" target="#b4">[5]</ref>. Generalization. AEVis aims at analyzing the adversarial examples of CNNs because most research on adversarial attacks focuses on generating adversarial images for CNNs.</p><p>In addition to attacking CNNs, there are several initial attempts to attack other types of DNNs <ref type="bibr" target="#b0">[1]</ref>, such as multilayer perceptron (MLP), recurrent neural networks (RNNs), autoencoders (AEs), and deep generative models (DGMs). Among these models, AEVis can be directly used to analyze MLPs by treating each neuron as a feature map that contains one neuron. For other types of DNNs, we need to develop suitable datapath extraction and visualization methods. For example, Ming et al. <ref type="bibr" target="#b32">[33]</ref> demonstrated that some neurons in an RNN were critical for predicting the sentiment of a sentence, such as the neurons for detecting positive/negative words. Such neurons and their connections form a datapath for an RNN. Thus, by extracting and visualizing datapaths, AEVis can be extended to analyze the root cause of adversarial examples for these types of DNNs. For example, to visualize the datapath of RNNs, we can first unfold the architecture of an RNN to a DAG <ref type="bibr" target="#b16">[17]</ref>, and then employ a DAG layout algorithm to calculate the position of each unfolded layer.</p><p>In addition to images, researchers try to generate adversarial examples for other types of data <ref type="bibr" target="#b0">[1]</ref>, such as adversarial documents and adversarial videos. To generalize AEVis to other types of data, we need to change the visual hint for neurons (discrepancy map and activation heat map) according to the target data type. For example, when analyzing adversarial documents, we can use a word cloud to represent the "learned feature" of a neuron in an RNN <ref type="bibr" target="#b32">[33]</ref>. In the word cloud, we select the keywords that activate the neuron.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>We have presented a robustness-motivated visual analytics approach that helps experts understand the inner workings of adversarial examples and diagnoses the root cause of incorrect predictions introduced by the adversarial examples. The major feature of this approach is that it centers on the concept of datapaths to tightly combine datapath extraction and visualization. Two case studies were conducted with two machine learning experts to demonstrate the effectiveness and usefulness of our approach in analyzing both white-box and black-box adversarial examples.</p><p>One interesting avenue for future research is to monitor the online testing process, detect potentially adversarial examples, and remove them from any further processing. The key is to design a set of streaming visualizations that can incrementally integrate the incoming log data with existing data. We would also like to continue working with the machine learning experts to conduct several field experiments, with the aim of designing more targeted and effective defense solutions based on the discovered root cause. Another important direction is to analyze the robustness of other types of DNNs, such as RNNs and DGMs. For these types of DNNs, exciting research topics include more efficient datapath extraction algorithms and suitable visualizations for different types of DNNs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>AEVis contains two modules: (a) a datapath extraction module and (b) a datapath visualization module that illustrates datapaths in multiple levels: layer level, feature map level, and neuron level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>A misleading result of the activation-based datapath extraction approach: (a) the input image; (b) the top 2 critical neurons found by the activation-based approach. The learned feature is computed by the discrepancy maps<ref type="bibr" target="#b63">[64]</ref> and the activation heat map show the activations on the corresponding feature map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Segmented DAG visualization: (a) showing 49 layers of ResNet101 simultaneously; (b) the segmentation result calculated by only considering the empty space at the end of each segment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>A dot plot as the visual hint for layer selection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>The visual hints for a feature map cluster. FM is short for feature map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Neuron-level visualization: (a) the learned feature is shown as discrepancy maps; (b) activations are shown as a heat map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Critical feature maps extracted by our approach for a sheepdog image: (a) the input image; (b) top 5 the critical feature maps; (c) activation heat maps of the neurons in feature map 4 on two examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Comparison between the extracted datapath for one example and a set of examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>The datapath overview illustrating the activation differences between the datapaths of the adversarial examples and normal examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Explanation of why the adversarial panda image is misclassified as a monkey. The CNN erroneously detects a monkey's face: (a) input images; (b) feature maps; (c) neurons; (d) learned feature; and (e) adversarial example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>and (d)). This behavior of the Explanation of why the adversarial cannon image is misclassified as a racket: (a) the adversarial example image; (b) normal examples from the training set; (c) the learned features and activations of neurons in the layer L E in Fig. 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 :</head><label>13</label><figDesc>The explanation of why the adversarial panda image was misclassified as a beagle (dog) by the VGG-16 network: (a) input images; (b) feature maps; and (c) neurons.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Threat of adversarial attacks on deep learning in computer vision: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00553</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Task-driven comparison of topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="320" to="329" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Design study of linesets, a novel set visualization technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Riche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Czerwinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2259" to="2267" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Visualizing sets and set-typed data: State-of-the-art and future challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alsallakh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Micallef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Aigner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Miksch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rodgers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Conference on Visualization</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Powerset: A comprehensive visualization of set intersections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alsallakh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="361" to="370" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Do convolutional neural networks learn class hierarchy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bilal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="152" to="162" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Overview: The design, adoption, and analysis of a visual document mining tool for investigative journalists</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brehmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ingram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munzner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2271" to="2280" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Timelines revisited: A design space and considerations for expressive storytelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brehmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">H</forename><surname>Riche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munzner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2151" to="2164" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Parseval networks: Improving robustness to adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="854" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Introduction to algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Cormen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">How hierarchical topics evolve in large text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2281" to="2290" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Non targeted adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<ptr target="https://github.com/dongyp13/Non-Targeted-Adversarial-Attacks" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Boosting adversarial attacks with momentum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06081</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arxiv preprint. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Considerations for visualizing comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="413" to="423" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Deep learning</title>
		<imprint>
			<publisher>MIT press Cambridge</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Google</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tensorflow</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An interactive node-link visualization of convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISVC</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="867" to="877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The spatial resolution of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Intriligator</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cavanagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="171" to="216" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ActiVis: Visual exploration of industry-scale deep neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kalro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H P</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Visualizing set concordance with permutation matrices and fan diagrams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interacting with Computers</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="630" to="643" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deeptracker: Visualizing the training process of convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint/>
	</monogr>
	<note>To appear in</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Analyzing the training processes of deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="87" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Towards better analysis of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="100" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A survey on information visualization: recent advances and challenges. The Visual Computer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1373" to="1393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Storyflow: Tracking the evolution of stories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2436" to="2445" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Human-centered approaches in geovisualization design: Investigating multiple methods through a long-term case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lloyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dykes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2498" to="2507" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Understanding hidden memories of recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>In IEEE VAST</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Universal adversarial perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="86" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">DeepFool: A simple and accurate method to fool deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2574" to="2582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="427" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The building blocks of interpretability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Satyanarayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mordvintsev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00633</idno>
		<title level="m">Towards robust detection of adversarial examples</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">DeepEyes: Progressive visual analytics for designing deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pezzotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Höllt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>Lelieveldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eisemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vilanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="108" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Visualizing the hidden activity of artificial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Fadel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Falcao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Telea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="101" to="110" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Untangling euler diagrams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">H</forename><surname>Riche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dwyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1090" to="1099" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Nonlinear dot plots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="616" to="625" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09404</idno>
		<title level="m">Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Principles of Mathematical Analysis</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="1964" />
			<publisher>McGraw-hill</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Context-preserving visual links</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Waldner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Streit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schmalstieg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2249" to="2258" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">LSTMVis: A tool for visual analysis of hidden state dynamics in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="667" to="676" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Methods for visual understanding of hierarchical system structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Toda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="125" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">I P</forename><surname>Systems</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<title level="m">Intriguing properties of neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Opening the black box -data driven visualization of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Y</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE VIS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="383" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Topicpanorama: A full picture of relevant topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2508" to="2521" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Dot plots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wilkinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="276" to="281" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Visualizing dataflow graphs of deep learning models in tensorflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wongsuphasawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Smilkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Numerical optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Springer Science</publisher>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Forvizor: Visualizing spatio-temporal team formations in soccer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hui Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A semantic-based method for visualizing large image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Multielement visual tracking: Attention and perceptual organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yantis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="295" to="340" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Improving the robustness of deep neural networks via stability training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4480" to="4488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Object detectors emerge in deep scene CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
