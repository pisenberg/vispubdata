<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">iForest: Interpreting Random Forests via Visual Analytics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Dik</roleName><forename type="first">Yanhong</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lun</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Cui</surname></persName>
						</author>
						<title level="a" type="main">iForest: Interpreting Random Forests via Visual Analytics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Interpretable Machine Learning</term>
					<term>Random Forests</term>
					<term>Random Forest Visualization</term>
					<term>Visual Analytics</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Figure 1. Using iForest to interpret random forests with Titanic dataset: (A) a Data Overview displaying an overview of how random forests classify data; (B) a Feature View depicting the relationships between features and predictions from various perspectives; (C) a Decision Path View revealing the underlying working mechanisms by enabling users to audit and compare different decision paths. iForest allows users to interpret random forests from various perspectives. For example, users can compare the negative decision paths (c1) against the positive ones (c2) to examine the most significant reasons for generating different results.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Random forests, an ensemble machine learning model that consists of many independent decision trees, are widely adopted for classification and regression tasks. Each tree in a random forest model is trained independently on a random subset of the training data and a random subset of features. Then, given a testing data input, random forests generate the final prediction by feeding the input to all decision trees and summarizing their results. Random forests have been extensively studied in the machine learning and data mining areas for many years <ref type="bibr" target="#b8">[10]</ref> and proved to perform well in many domains, such as biomedical engineering <ref type="bibr" target="#b15">[17]</ref> and traffic planning <ref type="bibr" target="#b23">[25]</ref>. One exhaustive study <ref type="bibr" target="#b18">[20]</ref> evaluates the performances of 179 classifiers stemming from different families on the UCI classification database, and shows that random forest classifiers overall outperform other classifier families, such as neural networks and support vector machines (SVM).</p><p>Despite their impressive prediction performance, one critical issue for random forests is interpretability. Although this issue is shared by most machine learning models, it is especially severe for random forests. According to Brieman et al. <ref type="bibr" target="#b9">[11]</ref>, "random forests are A+ predictors on performance" but "rate an F on interpretability". Thus, random forests are usually considered as black boxes <ref type="bibr" target="#b42">[44]</ref>, and the "Fgrade" interpretability has prevented the model from being adopted in some domains that have little or zero tolerance of errors, such as financial lending, criminal justice, and medical diagnosis. As any machine learning models may provide erroneous predictions <ref type="bibr" target="#b13">[15]</ref>, data scientists in these domains need to understand how a particular prediction is reached and examine whether the model works properly. For instance, in medical diagnosis settings, data scientists usually develop machine learning models to assist doctors in making treatment decisions. However, a doctor can barely trust the model without understanding how it generates the prediction, as blind trust can be catastrophic for patients <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b45">47]</ref>. Similarly, in the financial domain, failed loan applicants usually want to know the exact reasons why their loan applications have been rejected.</p><p>One popular approach to interpret classification models is to observe the relationships between features and predictions using methods like partial dependence plots (PDPs) <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b20">22]</ref>. Although these methods can illustrate how feature values affect predictions, they fall short when it comes to random forests. In a random forest model, each tree represents a self-consistent prediction strategy, which can easily be interpreted using PDPs. However, a random forest model often contains tens or hundreds of independent decision trees. Since the same feature is likely being treated differently in different trees, forest-level analysis helps users better understand which range of feature values is strongly related to which predictions. However, it is difficult to extend the existing methods to provide an overview of a large number of trees.</p><p>Apart from revealing feature-prediction relationships, uncovering the underlying working mechanism is another critical perspective for interpreting machine learning models <ref type="bibr" target="#b32">[34]</ref>, including random forests. This helps users examine whether the trained model works properly, and hence gain confidence in the generated predictions. To fully understand the working mechanism of random forest models, users should be able to audit a prediction and track the decision process, so that they can determine if the prediction is reliable based on their domain knowledge <ref type="bibr" target="#b56">[58]</ref>. For a single decision tree, users can easily audit its working mechanism by tracing the root-to-leaf decision paths. However, users need to compare hundreds of different decision paths for a particular prediction in random forests, which is a time-consuming and labor-intensive process.</p><p>Another important strategy for interpreting machine learning models is case-based reasoning. Based on the idea of solving problems by analogy <ref type="bibr" target="#b32">[34]</ref>, case-based reasoning facilitates users to determine their confidence in the prediction of a given data input by providing similar data examples as references <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b26">28]</ref>. Popular case-based reasoning methods used in general machine learning models are usually based on model proximity <ref type="bibr" target="#b12">[14]</ref> or feature similarity <ref type="bibr" target="#b55">[57]</ref>. Unfortunately, neither of them incorporates the fact that random forests generate predictions through partitioning the training data. Since each partition corresponds to a path in the forest, similarities between decision paths may provide valuable insights to help users understand why a prediction is made. However, existing reasoning methods fail to provide data examples.</p><p>The above challenges hinder data scientists from various domains in using random forests to solve their problems due to the lack of interpretability. To tackle these challenges, we develop iForest, an interactive visualization system, to help users interpret random forests from different perspectives. First, we build a Feature View to illustrate the relationships between input features and outcome predictions. To uncover the underlying working mechanism of random forests, we propose a novel design that summarizes several decision paths based on feature appearances and ranges, which allows users to explore and understand the partition logics of these paths. We also support the case-based reasoning for random forest models and predictions from the perspective of both feature similarity and decision path similarity. Specifically, our contributions are summarized as follows:</p><p>• An interactive visualization system, iForest, that assists users in interpreting random forest models and predictions. • A novel pixel-based bar chart design that summarizes the features and corresponding ranges in multiple decision paths to uncover the underlying working mechanism of random forests. • Two usage scenarios and a qualitative user study that demonstrate the usefulness and effectiveness of iForest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Random Forest Construction</head><p>Input:</p><formula xml:id="formula_0">X = {x 1 , x 2 , ..., x N }, F = { f 1 , f 2 , ..., f M }, Y = {y 1 , y 2 , ..., y N } Output: RF = {T 1 , .</formula><p>.., T K } for k= 1 to K do Draw a bootstrap sample D of size N by sampling N times with replacement from the training data X and initializing a decision tree T k with one root node.</p><p>repeat the following steps recursively for each terminal node v of tree T k . 1. Select m features randomly from F, where m &lt; M.</p><p>2. Pick the best splitting feature f v from the m features.</p><p>3. Find the best splitting threshold θ v for f v and split v into two child nodes.</p><p>until the number of nodes in tree T k reaches a certain threshold. end for Formally, given the input data X = {x 1 , ..., x N } where N is the size of training data, we consider x i = {x 1 i , ..., x M i } as the input vector of the ith data item with M features F = { f 1 , ..., f M }. In this work, we focus on binary classifiers and consider the data label Y = {y 1 , ..., y N } as binary categorical variables where y i ∈ {0, 1} is the label of the ith data item. A random forest model RF = {T 1 , ..., T K } can be built using Algorithm 1 in which T k represents a decision tree and K is the total number of trees. To assist the discussions in the rest sections, we introduce the following terms.</p><p>Split Point: For each node v in a decision tree, the split point s v refers to a feature f v and a threshold θ v that are used to split node v into two child nodes. We pick the split point s v that partitions the N v training samples into left child v L and right child v R to maximize the decrease of node impurity. In our paper, we measure the node impurity using Gini impurity <ref type="bibr" target="#b22">[24]</ref>.</p><p>Decision Path: Given an input data item, each decision tree will identify a root-to-leaf path (decision path) accordingly, which leads to the prediction made by the tree. Assuming a decision path p contains</p><formula xml:id="formula_1">H non-leaf nodes V (p) = {v 1 , .., v h , .., v H },</formula><p>it can be represented as:</p><formula xml:id="formula_2">p = {(x f v 1 θ v 1 ), (x f v 2 θ v 2 ), ..., (x f v H θ v H )}<label>(1)</label></formula><p>where f v h is the feature, θ v h is the corresponding threshold, and ∈ {" ≤ ", " &gt; "} represents the boolean condition on each node v h .</p><p>Prediction Score: For one data instance x i , we define the prediction score of the k th decision tree T k as T k (x i ) ∈ [0, 1]. A prediction score that is close to 0 or 1 indicates a high prediction confidence while a prediction score close to the threshold represents a low prediction confidence. The predicted label σ (T k (x i ), θ ) ∈ {0, 1} can be then obtained by setting certain predicted score threshold θ ∈ [0, 1]. If the score is larger than θ (by default 0.5), the predicted label is positive, otherwise negative. A random forest generates the final prediction by either taking the majority vote of {σ (T k (x i ), θ )|1 ≤ k ≤ K} or averaging the prediction scores of all its internal trees, i.e., σ (∑ K k=1 T k (x i )/K, θ ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RELATED WORK 3.1 Random Forest Interpretation</head><p>Random Forest interpretation methods can mainly be divided into 3 groups: feature analysis, model reduction, and case-based reasoning.</p><p>Feature Analysis. One simple but effective approach in this category is to calculate feature importance <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b22">24]</ref>, which assigns each feature a score to indicate its impact on resulting predictions. The feature importance is often evaluated based on either the Mean Decrease Accuracy (MDA) <ref type="bibr" target="#b8">[10]</ref> or the Mean Decrease Impurity (MDI) <ref type="bibr" target="#b11">[13]</ref>. MDA is a model-agnostic method that randomly permutes the values of a feature, and then measures how much the prediction accuracy decreases. Unlike MDA that can be used for any model, MDI <ref type="bibr" target="#b35">[37]</ref> is specialized for tree-based models. It calculates the average decreased impurity (e.g., Gini impurity <ref type="bibr" target="#b22">[24]</ref>), which determines the feature and its split point for each node in a decision tree <ref type="bibr" target="#b35">[37]</ref>. Both MDA and MDI can be either applied globally on an entire dataset to reveal the overall feature importance <ref type="bibr" target="#b8">[10]</ref> or calculated on a single prediction for a more detailed examination <ref type="bibr" target="#b41">[43,</ref><ref type="bibr" target="#b42">44]</ref>. Although they have been proven effective by various studies <ref type="bibr" target="#b35">[37]</ref>, MDI is more popular in random forest analysis <ref type="bibr" target="#b22">[24]</ref>. Therefore, we choose MDI as our feature importance measure in this paper. Apart from feature importance, another powerful tool for illustrating the relationships between features and predictions is partial dependence plots (PDPs) <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b20">22]</ref>, which depict how feature value changes affect predictions. Typically, a PDP is visualized as a line chart, where the x-axis represents feature values and the y-axis shows prediction probabilities. Prospector et al. <ref type="bibr" target="#b28">[30]</ref> recently introduces another heatmap-based design to represent PDPs, in which the color encodes prediction probabilities. This design is more space-efficient and allows the comparison of multiple features at the same time. However, one major drawback of the above methods is that they all ignore the split points in tree nodes. These thresholds depict random forests' partition criteria for each feature and can provide insights into which ranges of features are regarded critical by the trained model for predictions. We encode the feature threshold information which enables users to perceive these important feature thresholds.</p><p>Model Reduction. It is a post-hoc interpretation method that learns a surrogate model to approximate the original complex ensemble model. Surrogate models are often simple and interpretable, such as decision trees <ref type="bibr" target="#b46">[48,</ref><ref type="bibr" target="#b58">60]</ref>, decision rules <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b21">23]</ref>, and decision sets <ref type="bibr" target="#b30">[32]</ref>. For example, Schetinin et al. <ref type="bibr" target="#b46">[48]</ref> propose a method that evaluates the uncertainty of all trees in a random forest model and then select the one that has the highest confidence and accuracy as the surrogate model. However, in this approach, the structure of the surrogate tree depends on the model parameters, which means it can still be too complex for humans to interpret. Apart from surrogate trees, some other work also learns other models, such as decision rules <ref type="bibr" target="#b14">[16]</ref>, which may have simpler structures. Though model-reduction methods provide a simplified overview of the working mechanism of random forests, when users need to examine a specific prediction, these methods are usually not accurate enough to reflect the model's actual behavior as they summarize the structures and properties of all the decision trees. When considering a specific prediction, only the decision paths that are decided by the input data item are related to the working logic. Compared to model reduction-based methods that approximate all the trees' structure and property information, showing decision paths for a particular prediction can accurately and flexibly reflect the working logic of random forests. Thus, we summarize the decision paths to uncover the underlying working mechanisms of random forests instead of model reduction-based methods.</p><p>Case-based Reasoning. This methodology relies on the intuition that a new problem can be solved by summarizing the solutions of similar problems. This aligns with the analogical decision making process of humans <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b38">40]</ref>, which has been widely adopted in various real-world applications, such as the medical <ref type="bibr" target="#b6">[8]</ref> and the financial <ref type="bibr" target="#b31">[33]</ref> domains. In the random forests scenario, case-based reasoning is often used to justify the reliability of predictions <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b32">34]</ref>. Given a data item and its prediction, training data that are most similar to the input, along with their predictions, are collected first. Then, by comparing their feature and prediction differences, users can better understand and judge whether this prediction is reliable based on their knowledge. Common measures, such as Euclidean distance or Cosine distance in the feature space, are often adopted to compare different data items. In addition, specialized measures, such as the prediction score <ref type="bibr" target="#b12">[14]</ref> or the number of common leaf nodes reached <ref type="bibr" target="#b49">[51]</ref>, can also be used as distance functions for calculating data similarities in random forests. These measures examine data similarity from the model's perspective, which helps inspect whether the trained model ignores some features or assign high weights on other features. Since the two types of measures evaluate the similarity from different perspectives, we utilize both of them in our system for users to better understand and judge a prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Tree-based Model Visualization</head><p>In this section, we review existing work on decision tree and tree ensemble visualizations.</p><p>Decision trees can be visualized using different visualization techniques, such as node-link diagram <ref type="bibr" target="#b39">[41,</ref><ref type="bibr" target="#b54">56,</ref><ref type="bibr" target="#b57">59]</ref>, icicle plot <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b52">54]</ref>, and Treemap <ref type="bibr" target="#b36">[38,</ref><ref type="bibr" target="#b47">49]</ref>. As a natural representation of tree structures, node-link diagrams are widely adopted in visualizing decision trees. In addition to tree structures, BaobabView <ref type="bibr" target="#b51">[53]</ref> further encodes more information of models, such as the split point in each node and the training data volume that passes through each branch. Although nodelink diagrams can clearly depict tree structures, they may not scale well when trees become deep and complex. A more space-efficient way to visualize decision trees is icicle plots <ref type="bibr" target="#b29">[31]</ref>, in which the nodes are encoded by multiple stacked bars. The length of each bar is proportional to the volume of data which passes through the corresponding node. To further encode the label distribution of each node, Ankerst et al. <ref type="bibr" target="#b5">[7]</ref> adopt a pixel-based design that uses pixels to represent data items and pixel colors to represent the data labels. This pixel-based design enables users to evaluate whether each node can successfully partition data with different labels. Muhlbacher et al. <ref type="bibr" target="#b36">[38]</ref> also use the pixel-based design on Treemap to help users estimate the complexity and performance of a decision tree. Although existing methods are effective in visualizing decision trees, they are not designed for random forests. In random forests, it is difficult or even impossible for humans to understand and compare the structures and properties of all decision trees. For each decision tree, only one decision path is related to a specific data item and showing the entire tree structures may confuse users as it includes too much unrelated information. In this paper, we choose to visualize the decision paths that are used in a specific prediction instead of the whole tree structures to illustrate the working mechanism of random forests and reduce users' mental burden.</p><p>Apart from visualizing individual decision trees, many methods also attempt to visualize different perspectives of tree-ensemble models. One perspective is the relationships between data and decision trees. Breiman and Wald <ref type="bibr" target="#b10">[12]</ref> use random forest proximity as the similarity measure and adopt Multi-dimensional Scaling (MDS) to visualize training data, so that users can intuitively observe data clusters and outliers identified by the random forest model. Ploński et al. <ref type="bibr" target="#b43">[45]</ref> use Self-Organizing Map (SOM) instead of MDS to achieve higher accuracy. Another perspective is the relationships between features and decision trees. Urbanek et al. <ref type="bibr" target="#b50">[52]</ref> adopt a matrix-based visualization that illustrates the feature importances on each tree, which enables users to compare feature similarities in a fine granularity. One drawback of these methods is that they still consider random forests as a black box and ignore the structures and properties of decision paths that reflect the model's working mechanism. Analyzing the similarities between decision paths can answer many questions that are critical for prediction interpretation. For example, among all the decision paths for a data item, how many of them generate positive predictions and what are the prediction scores of these decision paths? To overcome this drawback, in addition to exploring the data similarity and feature correlations, we visualize different properties of decision paths to reveal how random forests generate predictions. Liu et al. <ref type="bibr" target="#b33">[35]</ref> propose an interactive visual diagnosis tool to analyze the performance of boosting trees, which are a tree ensemble model similar to random forests. They also design a temporal confusion matrix that visualizes the class confusions of all trees to help users analyze and diagnose boosting tree models. However, they do not help users understand how the model makes predictions and inspect a specific prediction. In our work, we focus more on understanding how random forests make predictions and observing prediction quality rather than model diagnosis such as comparing the effects of different model parameter settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DESIGN GOALS</head><p>Based on a thorough literature review of 35 papers collected from the machine learning, visualization and human-computer interaction fields, we distilled the following design goals to guide the system development. Further details are provided in supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G1: Reveal the relationships between features and predictions.</head><p>To make random forests transparent, users first need to understand what the model has learned in general and be able to evaluate the model's predictions <ref type="bibr" target="#b32">[34]</ref>. In training stages, random forests basically learn the mappings between input features and outcome predictions. Thus, these mappings reflect model behaviors and can facilitate users in understanding the characteristics of random forest models. For example, users may want to know which features are considered important by models and measure their influences (positive or negative) on predictions. Users can then understand whether a small change in feature values can alter the prediction <ref type="bibr" target="#b28">[30]</ref>. From the features' influences on predictions, users can further infer the feature correlation and remove unnecessary features. Thus, revealing the relationships between features and predictions is beneficial in interpreting random forests.</p><p>G2: Uncover the underlying working mechanisms. Opening the black box of random forests not only requires revealing the relationships between features and predictions, but also needs to uncover the underlying working mechanisms <ref type="bibr" target="#b32">[34]</ref>. Users should be able to audit the decision process of a prediction and make sure they agree before making a decision <ref type="bibr" target="#b7">[9]</ref>. For example, doctors may need to understand the model's prediction on a specific patient to evaluate whether this prediction is appropriate so that they can take further actions. This helps users examine whether the model works properly and understand why a specific prediction has been reached <ref type="bibr" target="#b56">[58]</ref>. For random forests, the overall working logic can be described by the structures and attributes of individual decision trees. For example, the split point of each tree node depicts the feature threshold of different predictions. Analyzing the root-to-leaf decision paths can help answer many questions in random forest interpretation. For instance, what are the similarities among the decision paths that generate the same prediction? What are the major differences for two decision paths to generate different results? For each decision path, what is the prediction and how certain is the prediction? By summarizing the structure and attributes of different decision trees, we aim to uncover the underlying working mechanisms of random forests.</p><p>G3: Provide case-based reasoning. Case-based reasoning is a crucial part of the most effective strategies in decision making scenarios. <ref type="bibr" target="#b26">[28]</ref>. It relies on the idea that a new problem can be solved based on the summarized solution of similar problems <ref type="bibr" target="#b27">[29]</ref>. These similar problems can serve as a scaffold for understanding and solving a new problem. This helps provide a holistic picture of random forests such as observing the model performance, examining which types of data the model tends to predict incorrectly, or identifying feature importance and impact. Similarly, when interpreting random forests, users can evaluate the prediction of a new case by comparing it with similar examples from the training data <ref type="bibr" target="#b12">[14]</ref>. However, providing case-based reasoning for random forests is more complex than other models, since the random forests can have various similarity metrics. For example, the similarity metric for random forests can be calculated based not only on feature value distance, but also on how many common leaf nodes are reached <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b49">51]</ref>. Thus, the similar cases of a testing case can vary according to different similarity metrics. Providing different sets of similar cases is beneficial as it helps users evaluate the prediction from various perspectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ANALYTICAL TASKS</head><p>To fulfill the design goals, we have distilled the following design tasks.</p><p>T1: Encode feature importance and partial dependence information. According to the definition, feature importance reflects the depth of a feature in decision trees and the amount of data partitioned by the feature threshold. Thus, it is a popular measure to help users understand which features are decisive to predictions <ref type="bibr" target="#b22">[24]</ref>. However, such information is not sufficient to reveal how feature values and predications are correlated (G1), because feature importances cannot demonstrate how feature values affect final predictions. To address this issue, the partial dependence information should also be examined, since it demonstrates how predictions respond to feature changes. Through the partial dependence information, users can judge whether changing a feature value slightly would strongly affect the prediction so that they can evaluate the robustness of the model or a specific prediction. Therefore, the system should encode both the feature importance and the partial dependence information of features.</p><p>T2: Encode the split point distribution of each feature. As discussed above, the split point distribution is another piece of critical information to reveal the relationships between input features and output predictions (G1). A feature range with dense split points suggests the prediction is sensitive to values in this range (i.e., this range has a high impact on predictions). On the other hand, if a feature range only has a few and sparse split points, this may indicate that the feature has little influence on predictions. Therefore, visualizing the density information helps reveal different ranges' effects on predictions.</p><p>T3: Encode the prediction results and summarizing the similarities of decision paths. To understand how a prediction is generated from random forests (G2), users often need to dig into all decision paths that jointly produce the final prediction. Showing the diversity of interim predictions helps users evaluate the uncertainty of the final prediction. In addition, the similarities of decision paths can provide a good overview of their relationships. For example, users may want to know which groups of decision paths produce positive results and which decision paths have similar structures in general. Hence, to deepen the understanding of the working mechanism of random forests, it is important to expose how interim predictions are generated and the final consensus is reached when given a data input.</p><p>T4: Review structures of decision paths. Each decision path has a unique structure, including the path length, the features appeared on the path, the order of the appeared features, and the split threshold in each node. Although two decision paths may yield the same prediction, their structures are likely to be very different. These structures can provide deeper insights into the underlying working mechanisms of random forests (G2) <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b32">34]</ref>. For example, users may be interested in whether there are any common feature thresholds among the decision paths that generate the same prediction. Users may also want to understand what the major differences between the paths that lead to opposite predictions. Thus, our system should summarize the structures of decision paths and enable users to examine their differences.</p><p>T5: Identify training data clusters and outliers. To support casebased reasoning (G3), users should be able to identify clusters and outliers from the training data. This can provide concrete examples for users to analyze the behaviors of models. For examples, users can observe whether certain data clusters are likely to have the same prediction result. Some data instances with high prediction confidences may also be incorrectly predicted by models. In addition, data outliers may indicate data noises, which can be removed from the training dataset to improve model performances. We aim to provide users with an overview of the training data and enable them to compare ground truth labels and prediction results.</p><p>T6: Encode training data value distribution. This distribution is useful for both feature analysis (G1) and case-based reasoning (G3). On one hand, it can reveal the relationships between features and predictions in a finer granularity (G1). For example, comparing the predictions of data in different feature ranges can help users analyze which feature ranges have stronger impact to predictions. On the other hand, the training data value distribution can also serve as evidence when observing partial dependence information and split point distribution. For example, if a numerical feature range has a low split point density, users need analyze whether this is because the data are sparse or this range of the feature is not considered important by the model based on the feature value distribution. Thus, encoding the distribution is useful for both feature analysis (G1) and case-based reasoning (G3).</p><p>T7: Support interactive model inspection. All three design goals listed above require our system to provide interactive model inspection. During the inspection process, users may want to examine why a group of data is incorrectly predicted. For example, they may examine the feature value distributions of the data group to check which features are responsible for the errors (G1). Sometimes users are more interested in exploring a single prediction interactively, rather than understanding the random forest model as a whole. When inspecting a single prediction, users may want to compare several deci-sion paths for a deeper understanding of the model's working mechanism (G2), or examine the predictions of similar examples in the training data (G3). Therefore, the system should support users to interactively inspect the model and individual predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">SYSTEM OVERVIEW</head><p>Motivated by the above design goals and tasks, we designed iForest, an interactive visual analytics system, to allow users to interpret random forest models. Our system consists of three major components: data processing, model building, and visual analysis. In the first module, we clean the raw datasets, conduct feature engineering, and store the processed data. The model building module trains a random forest model based on the processed data and calculates the similarity matrix for training data. These two modules are developed using Python. Specifically, we use scikit-learn <ref type="bibr" target="#b2">[4]</ref> to build random forests and leverage a Python web framework, Flask [2], to build the back-end. The visual analysis module, which is the front-end of our system, consists of three major views to support different design tasks with rich interactions. We utilize D3 <ref type="bibr" target="#b0">[1]</ref> in this visual analysis module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">VISUAL DESIGN</head><p>As shown in <ref type="figure">Fig. 1</ref>, iForest contains three major views: 1) the Data Overview which enables users to identify clusters and outliers from the training data and summarizes the model's performance on these data; 2) the Feature View which depicts the relationships between features and predictions from various perspectives; 3) and the Decision Path View which aims to help users inspect a single prediction by summarizing and comparing all the corresponding decision paths. We also provide a Control Panel to let users customize their own testing data and feed them into iForest for understanding and evaluating the predictions. A rich set of interactions is also provided to link these views together to allow a dynamic exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Data Overview</head><p>The Data Overview allows users to explore training data and their prediction results, so that users can have concrete examples to understand what the model has learned and how the model performs on the dataset. Common multi-dimensional data visualization techniques include parallel coordinate plots (PCP), scatterplot matrices (SPM), and dimension reduction techniques. PCP and SPM can accurately display multi-dimensional data and reveal dimension correlation while they are not space efficient when the number of dimensions is large. In contrast, dimension reduction methods have good scalability but one drawback is that the distances between points in the projected space may not accurately represent the distances in the original space. As we design the Data Overview mainly for users to obtain an overall picture on data clusters and outliers (T5), we select dimension reduction methods to display the training data for a better scalability. Then, we draw a confusion matrix to help users observe the model's performance on these data and enable users to filter certain data subsets (T7). Finally, we display a data table that allows users to easily browse the whole training dataset.</p><p>This view may have the visual clutter problem, which is a common drawback for many dimension reduction-based visualizations. To alleviate this issue, we adopt an overlap removal algorithm <ref type="bibr" target="#b16">[18]</ref> to increase the distances between overlapping circles. We further support zooming and panning to help users focus on a specific region in the view.</p><p>The confusion matrix is illustrated as four horizontal rectangles at the top. The rectangle width encodes the size of the corresponding category, while the color and texture represent different prediction categories. For example, true positives are illustrated in deep blue, and false positives are represented in light blue with stroke texture ( <ref type="figure">Fig. 1A)</ref>. Similarly, true negatives and false negatives are visualized in red. Compared with the traditional 2 × 2 matrix-based design, our design is more space-efficient and enables users to quickly compare the sizes of different prediction categories. Users can click on a rectangle to filter a category of data instances. The opacity of the corresponding circles below is set to zero, while we keep the strokes of them to provide contextual information. In the lower part, actual feature values are shown in a table to allow users to quickly browse and select a data instance for inspection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Feature View</head><p>Since random forest models are a mapping from input features and output predictions, revealing the relationships between features and predictions can help analyze what models have learned from training data and facilitate the understanding of models. For example, users may want to know which features are most important for a model and how feature values affect predictions (T1). In addition, the split point distribution of each feature (T2) and the training data value distribution (T6) also reflect the relationships between features and predictions. Therefore, we design the Feature View to reveal such information and facilitate users' understanding of random forest models.</p><p>Visual Encoding. The Feature View visualizes feature information as a list <ref type="figure" target="#fig_0">(Fig. 2)</ref>, in which each row represents a feature. Features are sorted based on the Gini importance (Sec. 2). The design of each feature row contains two parts as shown in <ref type="figure" target="#fig_0">Fig. 2a</ref>. The upper part displays the partial dependence information and split point distribution, while the bottom part shows the training data distribution. These two parts share an x-axis, which represents the current feature values in ascending order of Gini importance (from left to right).</p><p>In the upper part, we use a line chart to represent the partial dependence plots and a pixel-based bar chart to encode the split point distribution. For a feature f m ∈ F = { f 1 , f 2 , ..., f M }, let C = F − { f } to be the complement set of f , the partial dependence is calculated as:</p><formula xml:id="formula_3">PDP f m (α) = 1 N N ∑ i=1 RF(x C i , x m i = α)<label>(2)</label></formula><p>where N denotes the size of training data, function RF(x) represents random forest models that take the training data as input and output the prediction in the form of probability. The partial dependence calculates the average value of prediction f of all the training data, while x C i is fixed but setting the value of feature m to α for each data item. The y-axis represents the predictions of the model and the values that range from 0 to 1 are in ascending order from bottom to top. Since we focus on binary classification in this paper, we set the threshold to 0.5 so that the prediction is positive when value is larger than 0.5 and negative otherwise. Although we support both numerical and ordinal data, the line chart has different visualization formats for these two types of features <ref type="figure" target="#fig_0">(Fig. 2b)</ref>. For numerical data, the partial dependence information is illustrated as a continuous line chart; for ordinal data, it is displayed as discrete horizontal lines. To help users better evaluate predictions, we use a gray dashed horizontal line to highlight the threshold (0.5 by default). Thus, if the curve is above the dashed line, it indicates that the prediction is positive, otherwise negative. To allow users explore the effect of features on the predictions at different scales, we support switching between two scales of y-axis. The default scale of y-axis is ranging from 0 to 1, which is useful for users to directly evaluate which features have large effect on the prediction. Another scale of y-axis ranges from the minimum to the maximum prediction of each particular feature, which allows users to examine the relations between a specific feature and the predictions in details. In addition, we draw a pixel-based bar chart at the top of the line chart to represent the split point density in which the bar height encodes the number of split points at this position. In the bottom part, we use the bar charts to represent the feature value distributions and the inverted y-axis represents the feature value, where the feature value increases from top to bottom. The length of each bar represents the number of data items with the corresponding feature value. Design Alternatives. When designing the Feature View, we considered several alternatives. We considered heatmap and line charts in designing the partial dependence plots. We chose line charts over heatmap, because users may feel difficult to perceive small differences in heatmap <ref type="bibr" target="#b37">[39]</ref>. In order to maintain a consistent design, we chose histogram to obtain the coherent visualization style. In addition, we have considered to overlay the partial dependence plot on the value and split point distributions. Though this design is space-efficient, we abandoned it for several reasons. First, the design would cause occlusion problems that users may have difficulty in perceiving the partial dependence information and the size of training data in the same feature value range. In addition, the overlapping of histograms and the plots may lead to interpretation errors. Therefore, based on the above reasons, we select our design as the final design choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Decision Path View</head><p>Users usually need to flexibly audit the decision process of a prediction before making final decisions (T7). When auditing a prediction, the interim predictions and the corresponding prediction scores (T3) are an important aspect to examine. Moreover, each interim prediction has a unique decision path, thus summarizing and comparing the structures of various decision paths can provide deeper insights into the underlying working mechanisms (T4). Thus, this view is designed to help users audit, summarize, and compare decision paths. The Decision Path View <ref type="figure">(Fig. 1C)</ref> is a list in which all decision paths of a prediction is represented as a row. Specifically, each row contains three major visual components: 1) the Decision Path Projection that provides an overview of all decision paths in a single prediction based on their similarities, 2) the Feature Summary that summarizes the critical feature ranges of decision paths, and 3) the Decision Path Flow that provides the detailed information of decision paths layer by layer. Decision Path Projection. Since a single prediction in random forests is generated based on the results of many individual decision paths, we aim to provide users with an overview of these decision paths (T3). Similar to the Data Overview, we use t-SNE to project all the decision paths onto a 2D plane <ref type="figure" target="#fig_2">(Fig. 3a)</ref> as circles so that users can easily observe their similarities. The pairwise distances of decision paths can be calculated according to the feature ranges. Specifically, for a root-to-leaf decision path p (Eq. 1) of a data instance x = {x 1 , x 2 , ..., x M }, a feature may occur multiple times in the decision path. Let the set of features used in path p be F p ⊆ F = { f 1 , f 2 , ..., f M }. For each f n ∈ F p , we can merge the ranges of f n on the path to [θ</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Decision Path Distance</head><formula xml:id="formula_4">Input: Decision Paths p i , p j , Feature list F = { f 1 , f 2 , ..., f M } Output: Distance dist i j for each feature f m in F = { f 1 , f 2 , ..., f M } do if f m not in F p i and f m not in F p j then continue else if f m not in F p i or f m not in F p j then dist i j = dist i j + 1 f eature cnt = f eature cnt + 1 else dist i j = 1 2 (|θ f m ,l p i − θ f m ,l p j | + |θ f m ,u p i − θ f m ,u</formula><formula xml:id="formula_5">f n ,l p , θ f n ,u p ], where θ f n ,l p = Max{θ t h | f t h = f n , = " &gt; ",t h ∈ t(p)}, and θ f n ,u p = Min{θ t h | f t h = f n , = " ≤ ",t h ∈ t(p)}.</formula><p>To generate the t-SNE embedding, we normalize the feature range to [0, 1]. The normalized feature range of the path can be written as:</p><formula xml:id="formula_6">p = {[θ f n ,l p , θ f n ,u p ] | f n ∈ F p }<label>(3)</label></formula><p>We can calculate the pairwise decision path distance in Algorithm 2. We also considered including feature orders when measuring decision path distances. The advantage of including feature orders is that the calculated distance can better represent the dissimilarities between decision paths with less information loss. However, this approach may position two decision paths with similar features apart from each other due to the order effect. As the Decision Path Projection mainly aims at providing an overview of decision path similarity to guide users for further exploration, we exclude feature orders when calculating decision path distances to better cluster the paths that share a common feature set. Users can further compare the feature orders of different decision paths using the Decision Path Flow component.</p><p>As in the Data Overview, we use a red-to-blue divergent color scheme to encode the prediction score, from negative to positive. On top of the Decision Path Projection, we also draw a divergent bar and use the bar width to indicate the number of decision paths with positive or negative labels. User can click on a circle or draw a lasso <ref type="figure" target="#fig_2">(Fig. 3f)</ref> to add the corresponding decision paths to the Feature Summary and Decision Path Flow for further exploration.</p><p>Feature Summary. For the decision paths that have been selected in the Decision Path Projection, their features and corresponding ranges are summarized in Feature Summary. Specifically, each feature occurred in the selected paths is represented using a Feature Cell in a list format <ref type="figure" target="#fig_2">(Fig. 3b)</ref>. Users can scroll the list when the number of features listed is large. Feature Cell is a pixel-based bar chart design <ref type="figure" target="#fig_2">(Fig. 3d</ref>) that summarizes the feature ranges of multiple decision paths on feature f . We use the same categorical color scheme of the Feature View to represent different features. The x-axis represents feature values in ascending order from left to right, and the y-axis encodes the number of decision paths in which their feature ranges covers this value. We also draw a vertical gray bar to encode the current data instance's value x f . From the height of the bar chart, we can estimate which ranges of the feature are considered critical.</p><p>Decision Path Flow. The Decision Path Flow aims at revealing the structures and properties of multiple decision paths at the layer level. This enables users to examine the orders of the features appeared in different decision paths, which is critical in measuring feature importance. As shown in <ref type="figure" target="#fig_2">Fig. 3c</ref>, each column represents a layer, in which the layer depth increases from left to right. Similar to the Feature Summary, we use Feature Cells to summarize features and the corresponding ranges of nodes from the same layer. This helps users examine how the feature ranges of different decision paths evolve from the root layer to the leaf layer. If a layer contains leaf nodes, we append a pie chart in the corresponding column, where the red sector represents negative labels and the blue sector represents positive labels <ref type="figure" target="#fig_2">(Fig. 3e)</ref>. The pie chart radius encodes the number of decision paths that have leaf nodes at this layer and the sector angle encodes the ratio of the paths of the corresponding label. We draw curves to connect features from different layers. The curve width encodes the number of decision paths that have the corresponding feature pair in adjacent layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">EVALUATION</head><p>In this section, we describe two usage scenarios and a qualitative user study that demonstrate the effectiveness of iForest. The dataset descriptions and user study materials, such as task description, questionnaires, and study results, can be found in our supplemental materials.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Usage Scenario I</head><p>In this usage scenario, we describe Daniel, a machine learning beginner, utilizes iForest to investigate which groups of people were likely to survive in the Titanic shipwreck and how random forests make the corresponding predictions. The label to predict is Survival, a binary variable in which survival is marked as positive. Daniel first performs feature engineering on the training data <ref type="bibr" target="#b3">[5]</ref> which contains 891 passengers, to combine similar or redundant features together for model simplicity and robustness. He generates six features, namely, Sex, Title, Fare, PClass, Age, and Family Size, to train a random forest model. Daniel loads the trained model into iForest and he first examines the Feature View to investigate which features are more critical for survival. As Sex ranks top in the Feature View <ref type="figure">(Fig. 1B)</ref>, Daniel recognizes that Sex is the most important feature related to predictions (T1). From the partial dependence plots, he also observes that passengers tend to survive when Sex = 0 (female) but not to survive when Sex = 1 (male). This suggests that the model's learned pattern aligns with the fact that female passengers indeed are more likely to survive in the Titanic shipwreck <ref type="bibr" target="#b25">[27]</ref>.</p><p>Apart from categorical features like Sex, Daniel likes to further understand the relationships between prediction labels and numerical features, such as Fare and Age. Comparing Fare with Age, Fare's split points are mainly located at the beginning of the xaxis <ref type="figure">(Fig. 1b 1 )</ref> while the split points of Age are distributed more evenly <ref type="figure">(Fig. 1b</ref> 2 ) (T2). This suggests when examining Fare, the random forest model mainly checks if the values are smaller than a certain threshold. In contrast, there are no significant thresholds for predictions when examining Age. To examine the relationships between features and predictions, Daniel switches the y-axis of partial dependence plot from global scale to local scale. He finds that there is a steep curve in Fare's partial dependence plot around the value 25 <ref type="figure" target="#fig_2">(Fig. 1b 3 )</ref>. To discover the differences between the passengers divided by the threshold, Daniel hovers on the corresponding bar charts to highlight and compare the feature value distribution of the passengers in different Fare ranges (T7). He identifies that passengers with Fare &gt; 25 are usually from the first class and are more likely to survive than passengers with Fare &lt; 25 from the lower classes.</p><p>After examining the feature-prediction relationships learned by the random forest model, Daniel explores the Data Overview to analyze whether the random forest model fits the training data well and captures the characteristics of the training data. Daniel identifies several clusters <ref type="figure">(Fig. 1A)</ref> with different colors (T5). For example, cluster a 2 and a 3 are both encoded with dark red color, which indicates these passengers are predicted as non-survival by the model with high confidences. By hovering over the circles and observing their feature values, Daniel finds that these passengers are mainly male from the third class. Apart from cluster a 2 , Daniel also identifies six outliers <ref type="figure">(Fig. 1a 1 )</ref> with light yellow color (T5). He discovers that these outliers are also third class male passengers but their Fares are 56. To examine whether the model produces correct predictions on these six passengers, Daniel filters out True Negatives and discovers that these six passengers are False Negatives, which indicates that they survived.</p><p>To analyze the reasons why the model generates predictions on these passengers with a low confidence, Daniel clicks one of the six circles to add this passenger (ID = 664) to the Decision Path View for further examination. <ref type="figure" target="#fig_0">Fig. 1c 2</ref> shows that, apart from the many red circles that represent the decision paths with negative predictions, there are also some blue circles that represent positive predictions in the Decision Path Projection (T3). To explore the working mechanism differences between the positive and negative predictions of this passenger, Daniel first draws a lasso around the red circles to explore the decision paths with negative predictions <ref type="figure">(Fig. 1c 1 )</ref>. He also adds this passenger's account data copy to the Decision Path View and draws a lasso around the blue circles <ref type="figure" target="#fig_0">(Fig. 1c 2 )</ref> for comparison. Comparing the Feature Summary of the positive and negative decision paths, Daniel finds that the negative decision paths examine Sex and Class <ref type="figure" target="#fig_2">(Fig. 1c 3 )</ref> while the positive decision paths check Fare, in which this passenger has abnormal large values (T4). In addition, Daniel identifies that most of the negative decision paths use 3 as the threshold for feature Family Size. However, for the decision paths with positive predictions, the Family Size threshold is 1 <ref type="figure" target="#fig_4">(Fig. 1c 4 )</ref>. This suggests that in addition to feature types, these two groups of decision paths may even have different thresholds of the same feature in predictions (T4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Usage Scenario II</head><p>We use the German Credit Data <ref type="bibr" target="#b1">[3]</ref> to demonstrate this usage scenario. This dataset contains 1000 bank accounts with 9 features, each having a prediction label of good credit (positive) or bad credit (negative). Some of the features are ordinal (e.g., Account Balance's range is <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">4]</ref>, in which larger number indicates larger amount of balance). In this usage scenario, we describe Emma, a data scientist working at a bank to help her manager in analyzing whether a loan applicant has a good credit. She utilizes a random forest model built on this dataset to predict the credit status.</p><p>Emma first uploads a loan applicant's information to iForest and observes that the model predicts that the applicant has a bad credit. From the Decision Path View's prediction label distribution <ref type="figure" target="#fig_4">(Fig.4a)</ref>, she notices that the number of positive and negative decisions is similar, which indicates this is likely to be a marginal case. To ensure the prediction is reliable, Emma examines the Decision Path Projection in the Decision Path View to check the result and the corresponding confidence of each decision path. From the color saturations of circles, she observes that though negative decision paths outnumber positive decision paths, several positive decision paths have high confidence ( <ref type="figure" target="#fig_4">Fig. 4b</ref>) (T3). To examine why these positive decision paths achieve high confidence, she draws a lasso around these circles to examine them in detail. From the Feature Summary in the Decision Path View, she observes that the highly adopted ranges in these decision paths are Account Balance ∈ <ref type="bibr">[3.5, 4]</ref>, Duration of Credit ≥ 30, and Length of Current Employment ≥ 3.5 (T4). Since the corresponding three feature values of the applicant belong to these three feature ranges, these decision paths generate positive predictions with high  confidence. Then, Emma needs to investigate why other decision paths produce negative results so that she can better evaluate the strengths and weaknesses of this application. To find the strongest reason behind the rejection, Emma draws a lasso around the circles with dark red color <ref type="figure" target="#fig_4">(Fig. 4c)</ref>, which indicates that these negative decision paths have high confidence. She observes that the important ranges for rejecting the application are Account Balance ∈ [2. <ref type="bibr" target="#b3">5,</ref><ref type="bibr" target="#b2">4]</ref>, Credit Amount ≥ 8k, and Value Savings / Stocks ≤ 2.5. It is strange that Account Balance ∈ [2.5, 4] is similar to the feature range used in those positive decision paths of high confidence. In order to understand why a feature range is critical in both positive and negative decision paths with high prediction confidence, Emma further examines the Decision Path Flow to check the details of path structures. She observes that though Account Balance occurs in most of the decision paths, it appears mostly in the first and second layers while Credit Amount ≥ 8k appears mostly in leaf layers <ref type="figure" target="#fig_4">(Fig. 4d</ref>). To investigate these two ranges regarding case-based reasoning, Emma switches to the Feature View to examine account distributions (T6). From the feature distribution histograms, Emma identifies that a great number of accounts ( <ref type="figure" target="#fig_4">Fig. 4e)</ref> have an Account Balance within [2. <ref type="bibr" target="#b3">5,</ref><ref type="bibr" target="#b2">4]</ref>, which indicates large balance amounts. Meanwhile, there are only a few people whose Credit Amount are larger than 8k <ref type="figure" target="#fig_4">(Fig. 4f</ref>). Furthermore, by hovering over the histogram bars ranging from [2. <ref type="bibr" target="#b3">5,</ref><ref type="bibr" target="#b2">4]</ref> for Account Balance, she observes that many blue circles are highlighted in the Data Overview <ref type="figure" target="#fig_4">(Fig. 4g)</ref>, which indicates many accounts have good credit (T5). However, many accounts are in bad credits with Credit Amount larger than 8k.</p><p>Linking this observation to the decision paths in the Decision Path View, Emma realizes that Account Balance ∈ [2. <ref type="bibr" target="#b3">5,</ref><ref type="bibr" target="#b2">4]</ref> can separate the dataset into two coarse-grained large groups based on labels. However, for the data in the group of Account Balance ∈ [2. <ref type="bibr" target="#b3">5,</ref><ref type="bibr" target="#b2">4]</ref>, the Credit Amount ≥ 8k actually is the main reason for these decision paths generating negative prediction results with high confidence as it partitions the data in a finer granularity. As Credit Amount is a very critical factor in approving loan application according to the bank policy, Emma decides to suggest rejecting this application to avoid risk.</p><p>To provide explanations and suggestions, Emma decides to investigate what actions can be taken to change this loan application from negative to positive. She first examines the local partial dependence plots of features in the Feature View (T1) to inspect how the predictions are affected by the feature values. As Credit Amount is a strong factor, she first examines the column representing Credit Amount. As shown in <ref type="figure" target="#fig_4">Fig. 4h</ref>, from the local partial dependence plot, when the value of Credit Amount decreases from the current value 10, 144 to 8, 000, the prediction score keeps increasing. In the Feature View, Emma drags the circular label mark of Credit Amount, which rep-resents the current value of Credit Amount, to inspect the impact of Credit Amount on the prediction so that she can suggest a feasible value to the applicant (T7). When Emma reduces the Credit Amount value, she observes the minimum change to turn the prediction into positive is by setting Credit Amount to 8, 035. Therefore, based on the above analysis, Emma reports to reject this application and suggests reducing the Credit Amount.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Qualitative User Study</head><p>A formal comparison between iForest and a baseline visualization system is not applicable because existing techniques mainly focus on a single decision tree or the diagnosis of tree-ensemble models, which are different from the goal of iForest. In addition, random forest interpretation includes a series of complex tasks that may require users to navigate between the data, feature, and model perspectives, and link them together to understand how random forests generate certain predictions, which is not a simple yes or no question. Therefore, we choose to conduct a qualitative user study instead of a controlled quantitative experiments.</p><p>Design and Procedure. From a local university and an industry research lab, we recruited 10 participants (7 males, aged 21 to 28 years (mean = 24.4, SD = 2.5)) including undergraduate students, graduate students, and research scientists. Four of them have knowledge in information visualization; two of them have experience in machine learning but not familiar with random forests. The entire study took approximately 45 minutes to finish on average.</p><p>We began each study with a tutorial on random forests and an introduction to iForest's interface. The participants were asked to think aloud. When introducing iForest, we used the Titanic dataset to familiarize the participants with iForest. Then, we encouraged the participants to freely explore our system and try different interactions.</p><p>In the formal study, we adopted the German Credit dataset for users to complete tasks to avoid the memorization effects. For each task, we recorded the task completion time and the problems encountered by the participants. After finishing all the ten tasks, we asked the participants to complete a questionnaire with 12 questions to evaluate the effectiveness and aesthetics of iForest. Each question was designed using a 7-point Likert scale from strongly disagree (1) to strongly agree <ref type="bibr" target="#b5">(7)</ref>. We also conducted a post-study interview to collect user feedback on iForest, such as which parts need to be improved.</p><p>Results and Discussion. According to our design goals (Sec. 4), we designed 10 tasks to cover different perspectives of random forest interpretation. Tasks 1-3 pertain to case-based reasoning for gaining a holistic picture of random forests. Tasks 4-10 pertain to auditing a prediction to uncover the underlying working mechanisms and exploring the relationships between features and predictions. Successfully completing all the tasks requires the participants to utilize all the views in iForest. In general, the participants had completed the tasks successfully within a short period of time. However, two tasks (Task 9 and Task 10) took longer time than other tasks. This may be because that Task 9 requires the participants to examine different Feature Cells for summarization and Task 10 requires participants to manually tweak feature values. For the questionnaire, the majority of participants declared that the interactions are easy (6.3), the system is visually pleasing (6.5) and useful for interpret random forest models (6.7).</p><p>In the post-study interview session, most participants valued the effectiveness of iForest in helping them understand random forests and how a specific prediction is made. Particularly, they appreciated the usefulness of the Decision Path View and the Feature View. For the Decision Path View, a participant remarked that the feature ranges "can help me examine why a point is false positive. It is because some of its feature values are close to the decision thresholds." For the Feature View, one participant commented that "the Feature View is very intuitive, I can see all the feature and their distributions. It is easy to understand which features are important for decision making." Besides the positive feedbacks, the participants also provided many suggestions on improving iForest. For the Feature Summary of the Decision Path View, some participants mentioned that they need time to understand the meaning of the visual encodings. This indicates that for some people, the learning curve of iForest might be steep.</p><p>Apart from the visual design, most participants valued that iForest can be helpful for random forest interpretation in many domains. Some participants commented that "it can be useful for me to sell my used car at a good price. I can see which features are important for the prices and which parts I should fix." Some other participants also appreciated the usefulness of iForest for education and learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">DISCUSSION</head><p>Generalization. Although iForest mainly focuses on interpreting random forests, it can be applied to other tree-based ensemble models, such as boosting trees <ref type="bibr" target="#b20">[22]</ref>. These ensemble models all consist of many decision trees and generate final predictions by summarizing the output of all internal trees. Unlike random forests, in which decision trees are independently trained, decision trees in boosting tree models are trained in a sequential order. One potential enhancement to support boosting trees in iForest is encoding the temporal information. For example, we can enable users to switch the color encoding of the scatter plot so that they can compare decision paths from different time steps and understand how boosting trees evolve along time.</p><p>In addition, iForest can be easily extended to support multi-class classification and regression tasks. For classification tasks, one potential improvement is to apply the one-vs-rest strategy. For example, we can label one class as the positive class and all other classes as negative classes. For regression tasks, we can change the confusion matrix in the Data Overview to a violin diagram or bar charts to represent the prediction error distribution. Then, users can brush to select a specific group of data items for further examination.</p><p>Lastly, although the focus on iForest is to interpret random forests, it can also be extended to diagnose and debug random forests. For example, we can visualize and compare the feature importances on different trees to help feature engineering. From the evaluation perspective, we can further enable users to switch between different model evaluation measures, such as ROC-AUC <ref type="bibr" target="#b17">[19]</ref> or training loss <ref type="bibr" target="#b44">[46]</ref>.</p><p>Scalability. Similar to many visualization systems, one critical issue we considered when designing iForest is scalability.</p><p>In the Data Overview, circles may overlap when the number is large. To address this issue, we first adopt an overlap removal algorithm to enlarge the distances between overlapped circles. We also support users to focus on a specific region of circles via interactions, such as panning and zooming. In general, we support hundreds to thousands of data items in the Data Overview. When the data size becomes even larger, we can sample the data similar to Liu et al.'s approach <ref type="bibr" target="#b33">[35]</ref>.</p><p>In the Feature View, we use categorical colors to represent different features. Since humans may have problems in perceiving more than ten categorical colors <ref type="bibr" target="#b53">[55]</ref> at the same time, iForest colors the top ten features with largest importance. Other less important features are colored in gray. We support to visualize six to ten features in the same window depending on the display size, which is usually sufficient since previous research indicates that human has a limited visual capacity that around three to seven objects <ref type="bibr" target="#b48">[50]</ref>. When the number of features increases, users can use a scroll bar to examine them and we do not limit the number of total features in datasets.</p><p>In the Decision Path View, iForest can support hundreds of paths from different trees in the projection, which is usually adequate for training a good random forest model <ref type="bibr" target="#b40">[42]</ref>. When the number of decision paths increases, we use similar approaches adopted by the Data Overview to alleviate the clutter problems. Depending on different screen ratios, the Feature Summary and Decision Path Flow components generally support to visualize four consecutive decision nodes in a screen <ref type="figure">(Fig. 1C)</ref>. Users can scroll these two components to explore longer decision paths and we do not limit the maximum decision path length. Besides, as we use curves to link the nodes from different layers, these curves may have overlapping problems. However, this view mainly focuses on exploring a few similar paths. Thus, the edge number between each layer is limited, which makes the view less cluttered. In case more decision paths are investigated, users can view their feature summaries instead of the detailed layer information.</p><p>We conducted our evaluation using a MacBook Pro with 2.9GHz Intel Core i9 CPU with 16GB memory as the web server and Chrome (version 63) as the browser. The system can perform interactively after the model training stage, which may take a few minutes for the datasets used in the usage scenarios.</p><p>Targeted users. Our targeted users are the data scientists from various domains who apply random forests in solving their problems and demand interpretable predictions. We assume that they have basic knowledge on random forests but encounter difficulty in interpreting them. Note that the machine learning experts who have rich knowledge in machine learning may also benefit from using iForest in understanding the predictions made by random forests. Though they may know how random forests are constructed, the variety and complexity of the many trees still hinder them from understanding what a trained model has learned and how the decisions are made in a prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">CONCLUSION</head><p>We have presented iForest, an interactive visualization system that helps users interpret random forest models from various perspectives. The system reveals relations between input features and output predictions, hence enabling users to flexibly tweak feature values to monitor prediction changes. It also helps users audit the decision process of predictions to explore the underlying working mechanisms. Our evaluation results show that iForest can effectively assist users in understanding random forest models and their predictions.</p><p>iForest has several promising directions for future research. First, instead of feature importance and partial dependence information, we aim to further analyze feature correlations from the model perspective so that users can conduct feature engineering to improve model performance. Second, we plan to conduct more comprehensive user studies to further evaluate the effectiveness of iForest, such as recruiting more participants, conducting a quantitative study and comparing with other interpretation tools. In addition, we like to improve the scalability of our system for larger dataset, such as sampling the most representative data items instead of random sampling in the Data Overview.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Feature View's visual encoding for (a) numerical and (b) categorical features. The upper parts show the split point density and partial dependence information and the lower parts illustrate the feature value distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>p j |) f eature cnt = f eature cnt + 1 end if end for dist i j = dist i j / f eature cnt</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Visual components of the Decision Path View. (a) The Decision Path Projection provides an overview of decision path similarities. (b) The Feature Summary shows the summarized feature ranges for multiple selected decision paths. (c) The Decision Path Flow visualizes the detailed structures and feature ranges layer by layer. (d) The Feature Cell depicts the summarized feature range. (e) The pie chart displays the label distribution on the leaf node. (f) Lasso can be drawn around multiple circles to examine the details of the corresponding decision paths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Auditing a prediction with German Credit Data using iForest: (a) the prediction distribution of all decision paths; (b) the positive decision paths with high confidence; (c) the negative decision paths with high confidence; (d) the summarized decision rule from Account Balance ∈ [2.5, 4] to Credit Amount ≥ 8k; (e) the number of accounts with Account Balance = 4; (f) the number of accounts with Credit Amount ≥ 8k; (g) the accounts with Account Balance = 4; (h) the local partial dependence plot of Credit Amount ∈ [8000, 10144].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>• X. Zhao and D. Lee are with the Hong Kong University of Science and Technology. E-mail: {xzhaoag, dlee}@ust.hk • Y. Wu is with Visa Research. E-mail: yanwu@visa.com • W. Cui is with Microsoft Research Asia and is the corresponding author.</figDesc><table /><note>E-mail: weiwei.cui@microsoft.com Manuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication xx xxx. 201x; date of current version xx xxx. 201x. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org. Digital Object Identifier: xx.xxxx/TVCG.201x.xxxxxxx</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">ACKNOWLEDGMENTS</head><p>The authors would like to thank Prof. Huamin Qu from HKUST and Dr. Enrico Bertini from NYU for their constructive suggestions of revising the paper and the anonymous reviewers for their valuable comments. This research was supported in part by Hong Kong Themebased Research Scheme grant T41-709/17N.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://d3js.org/" />
		<title level="m">D3</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>German</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Data</surname></persName>
		</author>
		<ptr target="https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scikit-Learn</surname></persName>
		</author>
		<ptr target="http://scikit-learn.org/stable/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Titanic</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/c/titanic" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Visual data mining with pixel-oriented visualization techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ankerst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the SIGKDD Workshop on Visual Data Mining</title>
		<meeting>of the SIGKDD Workshop on Visual Data Mining</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards an effective cooperation of the user and the computer for classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ankerst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>of the International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="179" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Case-based reasoning in the health sciences: What&apos;s next?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bichindaritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Marling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence in Medicine</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="135" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Human-centric justification of machine learning predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Biran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Joint Conference on Artificial Intelligence</title>
		<meeting>of the International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1461" to="1467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Random forests. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Statistical modeling: The two cultures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="199" to="231" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Looking inside the black box</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wald Lecture II</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
		<respStmt>
			<orgName>Department of Statistics, California University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Manual on setting up, using, and understanding random forests v3.1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
		<respStmt>
			<orgName>Statistics Department Univ. of California Berkeley</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Casebased explanation of non-case-based learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kangarloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dionisio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the AMIA Symposium</title>
		<meeting>of the AMIA Symposium</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page">212</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>of the International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1721" to="1730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5456</idno>
		<title level="m">Interpreting tree ensembles with intrees</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gene selection and classification of microarray data using random forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Díaz-Uriarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>De Andres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast node overlap removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dwyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Marriott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Stuckey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Graph Drawing</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="153" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An introduction to roc analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fawcett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="861" to="874" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Do we need hundreds of classifiers to solve real world classification problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fernández-Delgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cernadas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Barro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amorim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3133" to="3181" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Greedy function approximation: a gradient boosting machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="1189" to="1232" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The elements of statistical learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Springer Series in Statistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Interpreting extracted rules from ensemble of trees: Application to computer-aided diagnosis of breast mri</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gallego-Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08288</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Variable selection using random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Genuer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tuleau-Malot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="2225" to="2236" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning driver behavior models from traffic observations for decision making and planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gindele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brechtel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dillmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Transportation Systems Magazine</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="79" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A survey of methods for explaining black box models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guidotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Monreale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Turini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pedreschi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Giannotti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01933</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Social class and survival on the ss titanic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Science &amp; Medicine</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="687" to="690" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The bayesian case model: A generative approach for case-based reasoning and prototype classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1952" to="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Case-based reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kolodner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Interacting with predictions: Visual inspection of black-box machine learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the CHI Conference on Human Factors in Computing Systems</title>
		<meeting>of the CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5686" to="5697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Icicle plots: Better displays for hierarchical clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Kruskal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Landwehr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="162" to="168" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lakkaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01154</idno>
		<title level="m">Interpretable &amp; explorable approximations of black box models</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Ranking-order case-based reasoning for financial distress prediction. Knowledge-Based systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="868" to="878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03490</idno>
		<title level="m">The mythos of model interpretability</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Visual diagnosis of tree boosting methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="163" to="173" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Design and evaluation of visualization support to facilitate decision trees classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Salvendy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Humancomputer Studies</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="95" to="110" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Understanding variable importances in forests of randomized trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Louppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wehenkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sutera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Geurts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="431" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Treepod: Sensitivity-aware selection of pareto-optimal decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mühlbacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Linhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Möller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Piringer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="174" to="183" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Visualization analysis and design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munzner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Human problem solving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Simon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1972" />
			<publisher>Prentice-Hall</publisher>
			<biblScope unit="volume">104</biblScope>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Interactive visualization in mining large decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shimodaira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Asia Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="345" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">How many trees in a random forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Oshiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Baranauskas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Machine Learning and Data Mining in Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="154" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Interpreting random forest classification models using a feature contribution method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Palczewska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Palczewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neagu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Integration of Reusable Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="193" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Interpreting random forest classification models using a feature contribution method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Palczewska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Palczewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neagu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Integration of Reusable Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="193" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Visualizing random forest with selforganising map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Płoński</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Soft Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="63" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Global refinement of random forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="723" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">why should I trust you?&quot;: Explaining the predictions of any classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>of the International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Confident interpretation of bayesian decision tree ensembles for clinical applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Schetinin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Fieldsend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Partridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Coats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Krzanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Everson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hernandez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Information Technology in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="312" to="319" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Tree visualization with tree-maps: 2-d space-filling approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="92" to="99" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Short-term storage capacity for visual objects depends on expertise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Sørensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kyllingsbaek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Psychologica</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="158" to="163" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Tree space prototypes: Another look at making tree ensembles interpretable</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">F</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hooker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Wells</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07115</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Exploring statistical forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Urbanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the Joint Statistical Meeting</title>
		<meeting>eeding of the Joint Statistical Meeting</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Baobabview: Interactive construction and analysis of decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Van Den Elzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Van Wijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Visual Analytics Science and Technology</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Concept tree based clustering visualization with shaded similarity matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gasser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conference on Data Mining</title>
		<meeting>of the IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="697" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Information visualization: perception for design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ware</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Interactive machine learning: letting users build classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ware</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Studies</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="281" to="292" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Case-based reasoning: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Knowledge Engineering Review</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="327" to="354" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.01870</idno>
		<title level="m">Challenges for transparency</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Vdm-rs: A visual data mining system for exploring and classifying remotely sensed images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gruenwald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gertz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Geosciences</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1827" to="1836" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Interpreting models via single tree approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hooker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09036</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
