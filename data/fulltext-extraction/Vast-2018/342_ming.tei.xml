<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RuleMatrix: Visualizing and Understanding Classifiers with Rules</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Ming</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Huamin</forename><surname>Qu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Enrico</forename><surname>Bertini</surname></persName>
						</author>
						<title level="a" type="main">RuleMatrix: Visualizing and Understanding Classifiers with Rules</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-explainable machine learning</term>
					<term>rule visualization</term>
					<term>visual analytics</term>
				</keywords>
			</textClass>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Understanding the behavior of a trained neural network using the explanatory visual interface of RuleMatrix. The user uses the control panel (A) to specify the detail information to visualize (e.g., dataset, level of detail, rule filters). The rule-based representation is visualized as a matrix (B), where each row represents a rule, and each column is a feature used in the rules. The user can also filter the data or use a customized input in the data filter (C) and navigate the filtered dataset in the data table (D).</p><p>Abstract-With the growing adoption of machine learning techniques, there is a surge of research interest towards making machine learning systems more transparent and interpretable. Various visualizations have been developed to help model developers understand, diagnose, and refine machine learning models. However, a large number of potential but neglected users are the domain experts with little knowledge of machine learning but are expected to work with machine learning systems. In this paper, we present an interactive visualization technique to help users with little expertise in machine learning to understand, explore and validate predictive models. By viewing the model as a black box, we extract a standardized rule-based knowledge representation from its input-output behavior. Then, we design RuleMatrix, a matrix-based visualization of rules to help users navigate and verify the rules and the black-box model. We evaluate the effectiveness of RuleMatrix via two use cases and a usability study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In this paper, we propose an interactive visualization technique for understanding and inspecting machine learning models. By constructing a rule-based interface from a given black box classifier, our method allows visual inspection of the reasoning logic of the model, as well as systematic exploration of the data used to train the model. With the recent advances in machine learning, there is increasing need for transparent and interpretable machine learning models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b31">32]</ref>. To avoid ambiguity, in this paper we define interpretability of a machine learning model as the ability to provide explanation for the reasoning of its prediction so that human users can understand. Interpretability is a crucial requirement for machine learning models in applications where human users are expected to sufficiently understand and trust them. The need for interpretable machine learning has been addressed in medicine, finance, security <ref type="bibr" target="#b17">[18]</ref> and many other domains where ethical treatment of data is required <ref type="bibr" target="#b12">[13]</ref>. In a health care example given by Caruana et al. <ref type="bibr" target="#b7">[8]</ref>, logistic regression was chosen over neural networks due to interpretability concerns. Though the neural network achieved a significant higher receiver operating characteristic (ROC) score than the logistic regression, domain experts felt that it was too risky to deploy the neural network for decision making with real patients because of its lack of transparency. On the other hand, with logistic regression, though less accurate, the fitted parameters have relatively clearer meanings, which can facilitate the discovery of problematic patterns in the dataset.</p><p>In the machine learning literature, trade-offs are often made between performance (e.g., accuracy) and interpretability. Models that are considered interpretable, such as logistic regression, k-nearest neighbors, and decision trees, often perform worse than models that are difficult to interpret, such as neural networks, support vector machines, and random forests. In scenarios where interpretability is required, the use of models with high performance is largely limited. There are two common strategies to strike a balance between performance and interpretability in machine learning. The first develops model simplification techniques (e.g., decision tree simplification <ref type="bibr" target="#b28">[29]</ref>) that generate a sparser model without much performance degradation. The second aims to improve the performance by designing models with commonlyrecognized interpretable structures (e.g., the linear relationships used by Generalized Additive Models (GAM) <ref type="bibr" target="#b7">[8]</ref> and decision rules employed by Bayesian Rule Lists <ref type="bibr" target="#b18">[19]</ref>). However, simplification techniques are applicable to a certain type of model, which impedes their popularization. The newly emerged interpretable models, on the other hand, rarely retain a state-of-the-art performance along with interpretability.</p><p>Instead of struggling with the trade-offs, in this paper we explore the idea of introducing an extra explanatory interface between the human and the model to provide interpretability. The interface is created in two steps. For a trained classification model, we first extract a rule list that approximates the original one using model induction. As a second step, we develop a visual interface to augment interpretability by enabling interactive exploration of details of the decision logic. The visual interface is crucial for numerous reasons. Though rule-based models are commonly considered to be interpretable, their interpretability is largely weakened when the model contains too many rules, or the composition of a rule is too complex. In addition, it is hard to identify how well the rules approximate the original model. The visual interface also enables the possibility to inspect the behavior of the model under a production environment, where the operators may not possess much knowledge about the underlying model. In summary, the main contribution of this paper is a visual technique that helps domain experts understand and inspect classification models using rule-based explanation. We present two use cases and a user study to demonstrate the effectiveness of the proposed method. We also contribute a model induction algorithm that generates a rule list for any given classification model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Recent research has explored promising directions to make machine learning models explainable. By associating semantic information with a learned deep neural networks, researchers created visualizations that can explain the learned features of the model <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b47">48]</ref>. In another direction, a variety of algorithms has been developed to directly learn more interpretable and structured models, including generalized additive models <ref type="bibr" target="#b5">[6]</ref> and decision rule lists <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b46">47]</ref>. Most related to our work, model-agnostic induction techniques <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b31">32]</ref> have been used to generate explanations for any given machine learning model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model Induction</head><p>Model induction is a technique that infers an approximate and interpretable model from any machine learning model. The inferred model can be a linear classifier <ref type="bibr" target="#b31">[32]</ref>, a decision tree <ref type="bibr" target="#b8">[9]</ref>, or a rule set <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b27">28]</ref>. It has been increasingly applied to create human-comprehensible proxy models that help users make sense of the behavior of complex models, such as artificial neural networks and support vector machines (SVMs). One most desirable advantage of model induction is that it provides interpretability by treating any complex model as a black box without compromising the performance.</p><p>There are mainly three types of methods to derive approximate models (often as rule sets) as summarized in related surveys <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, namely, decompositional, pedagogical and eclectic. Decompositional methods extract a simplified representation from specialized structures of a given model, e.g., the weights of a neural network, or the support vectors of an SVM, and thus only work for certain types of models. Pedagogical methods are often model-agnostic, and learn a model that approximates the input-output behavior of the original one. Eclectic methods either combine the previous two, or have distinct differences from them. In this paper, we adopt pedagogical methods to obtain rule-based approximations due to their simplicity and generalizability.</p><p>However, as the complexity of the original model increases, model induction would also face trade-offs. We either learn a small and comprehensible model that fails to approximate the original model well, or we learn a well-approximated but large model (e.g., a decision tree with over 100 nodes) that can be hardly recognized as "easy-tounderstand". In our work, we utilize visualization techniques to boost the interpretability while maintaining a good approximation quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Visualization for Model Analysis</head><p>Visualization has been increasingly used to support the understanding, diagnosis and refinement of machine learning models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b21">22]</ref>. In pioneering work by Tzeng and Ma <ref type="bibr" target="#b41">[42]</ref>, a node-linked visualization is used to understand and analyze a trained neural network's behavior in classifying volume and text data.</p><p>More recently, a number of visual analytics methods have been developed to support the analysis of complex deep neural networks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40]</ref>. Liu et al. <ref type="bibr" target="#b20">[21]</ref> used a hybrid visualization that embedded debugging information into the node-link diagram to help diagnose convolutional neural networks (CNNs). Alsallakh et al. <ref type="bibr" target="#b4">[5]</ref> stepped further to examine whether CNNs learn hierarchical representations from image data. Rauber et al. <ref type="bibr" target="#b29">[30]</ref> and Pezzotti et al. <ref type="bibr" target="#b26">[27]</ref> applied projection techniques to investigate the hidden activities of deep neural networks. Ming et al. <ref type="bibr" target="#b24">[25]</ref> developed a visual analytics method based on co-clustering to understand the hidden memories of recurrent neural networks (RNNs) in natural language processing tasks. Strobelt et al. <ref type="bibr" target="#b39">[40]</ref> utilized parallel coordinates to help researchers validate hypotheses about the hidden state dynamics of RNNs. Sacha et al. <ref type="bibr" target="#b34">[35]</ref> introduced a human-centered visual analytics framework to incorporate human knowledge in the machine learning process.</p><p>In the meantime, there are concrete demands in the industry to apply visualization to assist the development of machine learning systems <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b45">46]</ref>. Kahng et al. <ref type="bibr" target="#b15">[16]</ref> developed ActiVis, a visual system to support the exploration of industrial deep learning models in Facebook. Wongsuphasawat et al. <ref type="bibr" target="#b45">[46]</ref> presented the TensorFlow Graph Visualizer, an integrated visualization tool to help developers understand the complex structure of different machine learning architectures.</p><p>These methods have addressed the need for better visualization tools for machine learning researchers and developers. However, little attention has been paid to help domain experts (e.g., doctors and analysts) who have little or no knowledge of machine learning or deep learning to understand and exploit this powerful technology. Krause et al. <ref type="bibr" target="#b16">[17]</ref> developed an overview-feature-item workflow to help explain machine learning models to domain experts operating a hospital. Such nonexperts in machine learning are the major target users of our solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Visualization of Rule-based Representations</head><p>Rule-based models are composed of logical representations, that is, IF-THEN-ELSE statements which are pervasively used in programming languages. Typical representations of rule-based models include decision tables <ref type="bibr" target="#b43">[44]</ref>, decision trees <ref type="bibr" target="#b6">[7]</ref>, and rule sets or decision lists <ref type="bibr" target="#b33">[34]</ref>. Among these representations, trees are hierarchical data that have been studied abundantly in visualization research. A gallery of tree visualization can be found on treevis.net <ref type="bibr" target="#b35">[36]</ref>. Most related to our work, BaobabView <ref type="bibr" target="#b42">[43]</ref> uses a node-link data flow diagram to visualize the logic of decision trees, which inspired our design of data flow visualization in rule lists.</p><p>However, there is little research on how visualization can help analyze decision tables and rule lists. The lack of interest in visualizing decision tables and rule lists is partially due to the fact that they are not naturally graphical representations as trees. There is also no consensus that trees are the best visual representations for understanding rule-based models. A comprehensive empirical study conducted by Huysmans et al. <ref type="bibr" target="#b14">[15]</ref> found that decision tables are the most effective representations, while other studies <ref type="bibr" target="#b1">[2]</ref> disagrees. In a later position paper <ref type="bibr" target="#b11">[12]</ref>, Freitas summarized a few good properties rules and tables possess that trees do not. Also, all previous studies used pure texts to present rules. In our study, we provide a graphical representation of rule lists as an alternative for navigating and exploring proxy models.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A RULE-BASED EXPLANATION PIPELINE</head><p>In this section, we introduce our pipeline for creating a rule-based visual interface that helps domain experts understand, explore, and validate the behavior of a machine learning model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Goals and Target Users</head><p>In visualization research, most existing work for interpreting machine learning models focuses on helping model developers understand, diagnose and refine models. In this paper, we target our method at a large number of potential but neglected users -the experts in various domains that are impacted by the emerging machine learning techniques (e.g., health care, finance, security, and policymakers). With the increasing adoption of machine learning in these domains, however, experts may only have little knowledge of machine learning algorithms but would like to or are required to use them to assist in their decision making. The primary goal of these potential users, unlike model developers, is to fully understand how a model behaves so that they can better use it and work with it. Before they can fully adopt a model, adequate trust about how the model generally behaves need to be established. Once a model is learned and deployed, they would still need to verify its predictions in case of failure. Specifically, our goal is to help the domain experts answer the following questions:</p><p>Q1 What knowledge has the model learned? A trained machine learning model can be seen as an extracted representation of knowledge from the data. We propose to present a unified and understandable form of learned knowledge for any given model as rules (i.e., IF-THEN statements). Here each piece of knowledge consists of two parts: the antecedent (IF) and the consequent (THEN). In this way, users can focus on understanding the learned knowledge itself without extra burden of dealing with different representations.</p><p>Q2 How certain is the model for each piece of knowledge? There are two types of certainty that we should consider: the confidence (the probability that a rule is true according to the model) and the support (the amount of data in support of a rule). Low confidence means that the rule fails to approximate the model, while a low support indicates that there is little evidence for the rule to be true. These are important metrics that help users decide whether to accept or reject the learned knowledge.</p><p>Q3 What knowledge does the model utilize to make a prediction? This is the same question as "Why does the model predict x as y". Unlike the previous two questions, this question is about verifying the model's prediction on a single instance or a subset of instances, instead of understanding the model in general. This is crucial when users prefer to verify the reasons for a model's prediction than to blindly trust it. For example, a doctor would want to understand the reasons of an automatic diagnosis before making a final decision. Domain experts may have knowledge and theories that are originated in years of research and study, which current machine learning models fail to utilize.</p><p>Q4 When and where is the model likely to fail? This question arises when a model does not perform well on out-of-sample data. A rule that the model is confident about may not be generalizable in the production. Though undesirable, it is not rare that a model gives a highly confident but wrong prediction. Thus, we need to provide guidance on when and where the model is likely to fail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Rule-based Explanation</head><p>What are explanations of a machine learning model? In existing literature, explanations can take different forms. One widely accepted form of explanation in the machine learning community is the gradients of the input <ref type="bibr" target="#b37">[38]</ref>, which is often used to analyze deep neural networks. Recently, Ribeiro et al. <ref type="bibr" target="#b31">[32]</ref> and Krause et al. <ref type="bibr" target="#b16">[17]</ref> defined an explanation of the model's prediction as a set of features that are salient for predicting an input. Explanations can also be produced via analogy, that is, explaining the model's prediction of an instance by providing the predictions of similar instances. These explanations, however, can only be used to explain the model locally for a single instance.</p><p>In this paper, we present a new type of explanation that utilizes rules to explain machine learning models globally (Q1). A rule-based explanation of a model's prediction Y of a set of instances X is a set of IF-THEN decision rules. For example, a model predicts that today it will rain. A human explanation might be: it will rain because my knees hurt. The underlying rule format of the explanation would be: IF knees hurt = True THEN rain = 0.9. Such explanations with implicit rules occur throughout daily life, and are analogous to the inductive reasoning process that we use every day.</p><p>It should be also noted that there exist different variants of rule-based models. For example, rules can be mutually-exclusive or inclusive (i.e., an instance can fire multiple rules), conjunctive (AND) or disjunctive (OR), and standard or oblique (i.e., contain composite features). Though mutually-exclusive rule sets do not require conflict resolution, the complexity of a single rule is usually much larger than that in an inclusive rule set. In our implementation, we use the representation of an ordered list of inclusive rules (e.g., Bayesian Rule Lists <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b46">47]</ref>). When performing inference, each rule is queried in order and will only fire if all its previous rules are not satisfied. This allows fast queries and bypasses the complex conflicts resolution mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The Pipeline</head><p>Our pipeline for creating rule-based visual explanations consists of the three steps ( Rule Induction. Given a model F that we want to explain, the first step is to extract a rule list R that can explain it. There are multiple choices of algorithms as discussed in Sect. 2.1. In this step we adopt the common pedagogical learning settings. The original model is treated as a teacher, and the student model is trained using the data "labeled" by the teacher. That is, we use the predictions of the teacher model as labels instead of the real labels. The algorithm is described in detail in Section 4.</p><p>Filtering. After extracting a rule list approximation of the original model, we will have a semi-understandable explanation. The rule list is understandable in the sense that each rule is human-readable. However, the length of the list can grow too long (e.g., a few hundreds) to be practically understandable. Thus we adopt a step of filtering to obtain a more compact and informative list of rules.</p><p>Visualization. The simplest way to present a list of rules is just to show a list of textual descriptions. However, there are a few drawbacks associated with purely textual representations. First, it is difficult to identify the importance and certainty of each extracted rule (Q2). Second, it is difficult to perform verification of the model's prediction if the length of the list is long or the number of features is large. This is because the features used in each rule may be different and not aligned <ref type="bibr" target="#b14">[15]</ref>, which results in a waste of time in aligning features in input and features used in a rule.</p><p>As a solution, we develop RuleMatrix, a matrix-based representation of rules, to help users understand, explore and validate the knowledge learned by the original model. The details of the filtering and visual interface are discussed in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RULE INDUCTION</head><p>In this section, we present the algorithm for extracting rule lists from trained classifiers. The algorithm takes a trained model and a training set X as input, and produces a rule list that approximates the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Algorithm</head><p>We view the task of extracting a rule list as a problem of model induction. Given a classifier F , the target of the algorithm is a rule list R that approximates F . As a performance metric, we define the fidelity of the approximate rule list R as its accuracy with the true labels as the output of F :</p><formula xml:id="formula_0">f idelity(R)X = 1 |X | x∈X [F (x) = R(x)],<label>(1)</label></formula><p>where [F (x) = R(x)] evaluates to 1 if F (x) = R(x) and 0 otherwise. The task can be also viewed as an optimization problem, where we are maximizing the fidelity of the rule list. Unlike common machine learning problems, we have access to the original model F , which can be used as an omniscient oracle that we can ask for the labels of new data. Our algorithm highlights the use of the oracle. The algorithm contains four steps (Algorithm 1). First, we model the distribution of the provided training data X . We use a joint distribution estimation that can handle both discrete and continuous features simultaneously. Second, we sample a number of data X sample from the joint distribution. The number of samples is a customizable parameter and can be larger than the amount of original training data. Third, the original model F is used to label the sampled X sample . In the final step, we use the sampled data X sample and the labels Y sample to train a rule list. There are a few choices <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b46">47]</ref> for the training algorithm.</p><p>Input: model F , training data X , rule learning algorithm TRAIN Parameters: parameter n sample , feature set S Output:</p><formula xml:id="formula_1">A rule list R that approximates F 1 M ← ESTIMATEDISTRIBUTION(X , S); 2 Draw samples X sample ← SAMPLE(M , n samples ); 3</formula><p>Get the labels of X sample using: Y sample ← F (X sample ); The distribution estimation and sampling steps are inspired by TrePan <ref type="bibr" target="#b8">[9]</ref>, a tree induction algorithm that recursively extracts a decision tree from a neural network. The sampling is mainly needed for two reasons. First, since the goal is to extract a rule list that approximates the given model, the rule list should also be able to approximate the model's behavior on input that has not been seen before. The sampling helps generate unforeseen data. Second, when the training data is limited, the sampling step creates sufficient training samples, which helps achieve a good fidelity for the extracted rule list. Next, we introduce the details of the algorithm. Initialize a counter Counter : </p><formula xml:id="formula_2">x disc → 0; 4 for x (i) disc in X disc do 5 Counter[x (i) disc ] ← Counter[x (i) disc ] + 1 6 end 7 for x (i) disc in Counter do 8 p x (i) disc ← Counter[x (i) disc ]/</formula><formula xml:id="formula_3">X = {x (i) } N i=1 with N instances, where each x (i) ∈ R k is a k dimensional vector.</formula><p>Without losing generality, we assume the k features are mixed with d discrete features</p><formula xml:id="formula_4">x disc = (x1, ..., x d ) and (k − d) continuous features xcon = (x d+1 , ..., x k )</formula><p>. Using Bayes' Theorem, we can write the joint distribution of the mixed discrete and continuous random variables as:</p><formula xml:id="formula_5">f (x) =f (x disc , xcon) =P r(x disc )f (xcon | x disc ).<label>(2)</label></formula><p>The first term is the probability mass function of the discrete random variables, and the second term is the conditional density function of the continuous random variables given the values of the discrete variables. Next we discuss the two terms separately. We assume that the discrete features x disc follow categorical distributions. The probability of each combination of x disc can be estimated using its frequency in the training data (Algorithm 2, lines 3-9):</p><formula xml:id="formula_6">P r(x disc = x disc ) =px disc = N i=1 [x (i) disc = x disc ] N ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_7">[x (i) disc = x disc ] evaluates to 1 if x (i) disc =</formula><p>x disc , and 0 otherwise. We use multivariate density estimation with Gaussian kernel to model continuous features xcon (Algorithm 2, line 10-13). Since we are interested in the conditional distribution, we can write the conditional density estimation as:</p><formula xml:id="formula_8">f (xcon | x disc ) = 1 |S| x∈S exp{− 1 2 (xcon − xcon) T H −1 (xcon − xcon)} (2π) c 2 |H| 1 2 ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_9">S = {x | x disc = x disc ,</formula><p>x ∈ X } is a subset of training data that has the same discrete values as x disc , and c = (k − d) is the number of the continuous features. Here H is the bandwidth matrix, and also the covariance matrix for the kernel function. The problem left is how to choose the bandwidth matrix H. There are a few methods for estimating the optimal choice of H, such as smoothed cross validation and plug-in. For simplicity, we adopt Silverman's rule-of-thumb <ref type="bibr" target="#b36">[37]</ref>:  where σi is the standard deviation of feature i.</p><formula xml:id="formula_10">√ Hii = ( c + 2 4 n) − 1 c+4 σi Hij = 0, i = j,<label>(5)</label></formula><p>Once we have built a model of the distribution, M , we can easily create X sample . The question left is how to choose a proper number of samples, which will be discussed in Sect. 4.2.</p><p>Rule List. In the last step, a training algorithm TRAIN is needed to learn a rule list from (X sample , Y sample ). There exist various algorithms that can construct a list of rules from training data <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47]</ref>. Both of the algorithms proposed by Marchand and Sokolova <ref type="bibr" target="#b22">[23]</ref> and Fawcett <ref type="bibr" target="#b10">[11]</ref> follow a greedy construction mechanism and do not offer a good performance. In the implementation, we adopt the Scalable Bayesian Rule List (SBRL) algorithm proposed by Yang et al. <ref type="bibr" target="#b46">[47]</ref>. This algorithm models the rule list using a Bayesian framework and allows users to specify priors related to the length of the list and the complexity of each rule. This is useful for our task, since we can have controls on the complexity of the extracted rule list. This algorithm also has the advantage that it can be more naturally extended to support multi-class classification (i.e., by switching the output distribution from binomial to multinomial), which supports a more generalizable solution. Readers can refer to the paper by Yang et al. <ref type="bibr" target="#b46">[47]</ref> for more details.</p><p>Note that the algorithm requires a preprocessing step to discretize the input and pre-mine a candidate rule sets for the algorithm to choose from. In our implementation, we use the minimum description length (MDL) discretization <ref type="bibr" target="#b32">[33]</ref> to discretize continuous features, and use the FP-Growth item set mining algorithm <ref type="bibr" target="#b13">[14]</ref> to get the candidate rule sets. Other discretization and rule mining methods can also be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiments</head><p>To study the effect of sample size and evaluate the performance of the proposed rule induction algorithm, we test our induction algorithm on several publicly available datasets from the UCI Machine Learning Repository <ref type="bibr" target="#b9">[10]</ref> and a few popular models that are commonly regarded as hard to interpret.</p><p>Sampling Rate. First, we study the effect of sampling rate (i.e., number of samples / number of training data) using three datasets, Abalone, Bank Marketing and Pima Indian Diabetes (Pima). Abalone contains the physical measurements of 4177 abalones originally labeled with their rings (representing their ages). Since our current implementation only supports classification, we replace the number of rings with four simplified and balanced labels, i.e., rings &lt; 9, 9 ≤ rings &lt; 12, 12 ≤ rings &lt; 15, and 15 &lt; rings, with 1407, 1810, 596, and 364 instances respectively. Bank Marketing and Pima are binary classifications. All three datasets are randomly partitioned into a 75% training set and a 25% test set. We train a neural network with four hidden layers and 50 neurons per layer on the training set. Then we test the algorithm on the neural network with six sampling rates growing exponentially: 0.25, 0.5, 1.0, 2.0, 4.0, and 8.0. We run each setting 10 times and compute the fidelity on the test set.</p><p>As shown in <ref type="figure" target="#fig_5">Fig. 3</ref>, with all three datasets, the fidelity of extracted rule lists generally increases as the sampling rate grows. However, the complexity of the rule lists also increases dramatically (which is also a reason for an additional visual interface). Here there is a trade-off between the fidelity and interpretability of the extracted rule list. Considering that interpretability is our major goal, we adopt the following strategy for choosing sampling rate: start from a small sampling rate (1.0), and gradually increase the sampling rate until we get a good fidelity or the length of the rule list exceeds an acceptable threshold (e.g., 60).</p><p>Fidelity. To verify that the proposed rule induction algorithm is able to produce a good approximation of a given model, we benchmark the algorithm on a set of datasets with two different classifiers, neural networks and support vector machines. The datasets we use include: Breast Cancer Wisconsin (Diagnostics), Iris, Wine, Abalone (four-class classification), Bank Marketing, Pima Indian Diabetes and Adult.</p><p>We test the algorithm on neural networks with one, two, and four hidden layers, and support vector machines with nonlinear Radial Basis Function (RBF) kernel. We use the implementation of these models in the scikit-learn package <ref type="bibr" target="#b25">[26]</ref>. We use a sampling rate of 2.0 for the Adult dataset, and a sampling rate of 4.0 for the rest. As shown in <ref type="table" target="#tab_5">Table 1</ref>, the rule induction algorithm can generate rule lists that approximate a model with acceptable fidelity on the selected datasets. The fidelity is over 90% on most datasets except for Pima and Abalone.</p><p>Speed. The time for creating a list of 40 rules from 7,000 samples with 20 features can take up to 3 minutes on a PC (the time varies under different parameters). The estimation and sampling step take less than one second, and the major bottleneck lies in the FP-Growth (less than 10 seconds) and SBRL (more than 2 minutes) algorithms. We restrict the discussion of this issue in this paper due to page limits. The material necessary for reproduce the results is available at http: //rulematrix.github.io.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RULEMATRIX: THE VISUAL INTERFACE</head><p>This section presents the design and implementation of the visual interface for helping users understand, navigate and inspect the learned knowledge of classifiers. As shown in <ref type="figure">Fig. 1</ref>, the interface contains a control panel (A), a main visualization (B), a data filter panel (C) and a data table (D). In this section, we mainly present the main visualization, RuleMatrix, and the interactions supported by the other views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Visualization Design</head><p>RuleMatrix <ref type="figure">(Fig. 4)</ref> consists of three visual components: the rule matrix, the data flow, and the support view. The rule matrix visualizes the content of a rule list in a matrix-based design. The data flow shows how data flows through the list using a Sankey diagram. The support view supports the understanding and analysis of the original model that we aim to explain. Detail distribution  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Rule Matrix</head><p>The major visual component of the interface is the matrix-based visualization of rules. A decision rule is a logical statement consisting of two parts: the antecedent (IF) and the consequent (THEN). Here we restrict the antecedent to be a conjunction (AND) of clauses, where each clause is a condition on an input feature (e.g., 3 &lt; x1 AND x2 &lt; 4). This restriction eases users' cognitive burden of discriminating different logical operations. The output of each rule is a probability distribution over possible classes, representing the probability of an instance satisfying the antecedent belongs to each class. The simplest way to present a rule is to write it down as a logical expression, which is ubiquitous in programing languages. However, we found textual representations difficult to navigate when the length of the list is too large. The problem with textual representations is that the input features are not presented in the same order in each rule. Thus, it is difficult for users to search a rule with certain condition or compare the conditions used in different rules. This problem has also been identified by Huysmans et al. <ref type="bibr" target="#b14">[15]</ref>, To address this issue and help users understand and navigate the rule list (Q1), we present the rules in a matrix format. As shown in <ref type="figure">Fig. 4B</ref>, each row in the matrix represents the antecedent of a decision rule, and each column represents an input feature. If the antecedent of a decision rule i contains a clause using feature xj, then a compact representation <ref type="figure">(Fig. 4-1 )</ref> of the clause is shown in the corresponding cell (i, j). In this layout, the order of the features is fixed, which helps users visually search and compare rules by features. The length of the bar underneath a feature name encodes the frequency with which the feature occurs in the decision rules. The features are also sorted according to their importance scores, which is computed by the number of instances that a feature has been used to discriminate. The advantage of the matrix representation is that it allows users to verify and compare different rules quickly. This also allows easier verification and evaluation of the model's predictions (Q3).</p><p>Visualizing Conditions. In the antecedent of rule i, a clause that uses feature j (e.g., 0 ≤ xj &lt; 3) is visualized as a gray and translucent box in cell (i, j), where the covered range represents the interval in the clause (i.e., [0, 3)). In each cell (i, j), a compact view of the data distribution of feature j is also presented (inspired by the idea of sparklines <ref type="bibr" target="#b40">[41]</ref>). For continuous features, the distributions are visu-alized as histograms. For discrete features, bar charts are used. The part of data that satisfies the clause is also highlighted with a higher opacity. This combination of the compact view of data distribution and the range constraint helps users quickly grasp the properties of different clauses in a rule (Q1), i.e., the tightness or width of the interval and the number of instances that satisfy the clause.</p><p>Visualizing Outputs. As discussed above, the output of a rule is a probability distribution. At the end of each row, we present the output of the rule as a colored number, with color representing the output label of the rule, and the number showing the probability of the label. A vertically stacked bar is positioned next to the number to show the detailed probability of each label. Using this design, users are able to quickly identify the output label of the rule by the color, and learn the actual probability of the label from the number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Data Flow</head><p>To provide users with an overall sense of how the input data is classified by different rules, a waterfall-like Sankey diagram <ref type="figure">(Fig. 4A)</ref> is presented to the left of the rule matrix. The main vertical flow represents the data that remains unclassified. Each time the main flow "encounters" a rule (represented by a horizontal bar), a horizontal flow representing the data satisfying the rule forks from the main vertical flow. The widths of the flows represent the quantities of the data. The colors encode the labels of the data. That is, if a flow contains data with multiple labels, the flow is divided into multiple parallel sub-flows, whose widths are proportional to the quantities of different labels. The data flow helps the user maintain a proper mental model of the ordered decision rule list. The rules are ordered, and the success of a rule has the implication that previous rules are not satisfied. The user can identify the amount of data satisfying a rule through the width of the flow, which helps the user decide to trust or reject the rule (Q2). The design of the data flow is inspired by the node-link layout used in BaobabView <ref type="bibr" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Support View</head><p>The support view is designed to support the understanding and analysis of the performance of the original model. Note that there are two types of errors that we are interested in: the error between the rule and the model (fidelity), and the error between the model and the real data (accuracy). When the error between a rule and the model is high, users should be notified that the rule may not be a well-extracted "knowledge". When the error between the original model and the real data is high, the users should be notified that the model's prediction should not be fully trusted (Q4). In the support view, we provide for each rule a set of two performance visualizations <ref type="figure">(Fig. 4C</ref>), fidelity and evidence to help users analyze these two types of errors.</p><p>Fidelity. We use a simple glyph that contains a number (0 to 100) to present the fidelity (Equation 1) of the rule on the subset of data satisfying the rule. The value of fidelity represents how accurately the rule represents the original model on this subset. The higher the fidelity, the more reliable the rule is in representing the original model. The number is circled by an arc, whose angle also encodes the number. As shown in <ref type="figure" target="#fig_1">Fig. 4-2</ref> , the glyph can be colored green (high), yellow (medium), red (low) according to the level of fidelity. In the current implementation, the fidelity levels are set to above 80% (high), 50% (medium) to 80%, and below 50% (low), respectively.</p><p>Evidence. The second performance visualization shows the evidence of the original model on the real data (users can switch between training or test set). To support comprehensive analysis of the error distribution, we adopt a compact and simplified variant of Squares <ref type="bibr" target="#b30">[31]</ref>. As shown in Design 1 in <ref type="figure" target="#fig_5">Fig. 4-3</ref> , we use horizontally stacked boxes to present the predictions of the model. The color encodes the predicted class by the original model. The width of a box encodes the amount of data with a certain type of prediction. We use striped boxes to represent erroneous predictions. That is, a blue striped box ( ) represents data that is wrongly classified as class blue and has real labels different from class blue. During the development of this interface, we have experimented with an alternative design which had the same color coding, as shown in Design 2 in <ref type="figure" target="#fig_5">Fig. 4-3</ref> . In this alternative design, the data is divided into horizontally stacked boxes according to the true labels. Then we partition each box vertically into two parts: the upper one representing correct predictions and the lower one representing the wrong predictions (striped boxes). The lower part is further partitioned into multiple parts according to the predicted labels. However, during our informal pilot studies with two graduate students, the Design 2 was found to be "confusing" and "distracting". Though Design 1 fails to present the real labels of the wrong predictions, it is more concise and can be directly used to answer whether a model is likely to fail (Q4).</p><p>The advantage of the compact performance visualization is that it presents an intuitive error visualization within a small space. We can easily identify the amount of instances classified as a label or quantify the mistakes by searching for the boxes with the corresponding coding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Interactions</head><p>RuleMatrix supports three types of interactions: filtering the rules, which is used to reduce cognitive burden by reducing the number of rules to show; filtering the data, which is used to explore the relation between the data and the rules; and details on demand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Filtering the Rules</head><p>The filtering of rules helps relieve the scalability issue and reduce the cognitive load when the extracted rule list is too long. This occurs when we have a complex model (e.g., a neural net with multiple layers, or an SVM with nonlinear kernel), or a complex data set. In order to learn a rule list that well approximates the model, the complexity of the rule list inevitably grows. In our implementation, we provide two types of filters: filter by support and filter by confidence. The former filters the rules that have little support, which are seldom fired and are not salient. The latter filters the rules that have low confidence, which are not significant in discriminating different classes. In our implementation, filtered rules are grouped into collapsed "rules" so that users can keep track of them. Users can also expand the collapsed rules to see them in full details. By adjusting rule filters, users are allowed to explore a list of over 100 rules with no major cognitive burden.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Filtering the Data</head><p>The data filtering function is needed to support two scenarios. First, data filtering allows users to apply the divide and conquer strategy to understand the model's behavior, i.e., only focus on the model's behavior on the data one is interested in. Second, by filtering, users can identify the data entries in the data table <ref type="figure">(Fig. 1D)</ref> that support specific rules. This boosts users' trust in both the system and the model. During our experiments, we found that data filters can greatly reduce the number of rules shown when combined with rule filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Details on Demand</head><p>To provide a clean and concise interface, we hide the details that users can view on demand. Users can request details in two ways: interacting with the RuleMatrix directly or modifying the settings in the control panel. In the RuleMatrix, users can check the actual text description of a clause by hovering on the corresponding cell. To view the details about the data distribution, users can click on a cell, which expand the cell and show a stream plot (continuous feature) or a stacked bar charts (categorical feature) of the distribution <ref type="figure">(Fig. 4B)</ref>. The choice of stream plot for continuous features is due to its ability in preventing color discontinuities <ref type="bibr" target="#b42">[43]</ref>. A vertical ruler that follows the mouse is displayed to help align and compare the intervals of the clauses using the same feature across multiple rules. Users can see the actual amount of data by hovering over the evidence bars or certain parts of the data flow. Users can view the conditional distribution or hide the striped error boxes by modifying the settings in the control panel. Here the conditional distribution of feature xj at rule i denotes the distribution given that all previous rules are not satisfied, that is, the distribution of the data that is left unclassified until rule i.</p><p>The rule filtering functions are provided in the control panel <ref type="figure">(Fig. 1A)</ref>, and the data filtering functions are provided in the data filter <ref type="figure">(Fig. 1C)</ref>. Users are also allowed to customize an input and request the system to present the prediction of the original model and highlight the satisfied rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATION</head><p>We present a usage scenario, a use case, and a user study to demonstrate how our method effectively helps users understand the behavior of a classifier. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Usage Scenario: Understanding a Cancer Classifier</head><p>We first present a hypothetical scenario to show how RuleMatrix helps people understand the knowledge learned by a machine learning model. Mary, a medical student is learning about breast cancer and is interested in identifying cancer cells by the features measured from biopsy specimens. She is also eager to know whether the popular machine learning algorithms can learn to classify cancer cells accurately. She downloads a pre-trained neural network and the Breast Cancer Wisconsin dataset from the Internet. The dataset contains cytological characteristics of 699 breast fine-needle aspirates. Each of the cytological characteristics are graded from 1 to 10 (lower is closer to begin) at the time of sample collection. The accuracies of the model on training and test set are 97.3% and 97.1% respectively. She want to know what knowledge the model has learned (Q1). Understanding the rules. Mary uses our pipeline and extracts a list of 12 rules from the neural network. The visualization is presented to Mary. She quickly goes through the list and notices that rule 6 to rule 12 have little support from the training data (Q2). Then she adjust the minimum evidence in the rule filter ( <ref type="figure">Fig. 1A)</ref> to 0.014 to collapse the last 7 rules <ref type="figure" target="#fig_8">(Fig. 5</ref>). She then finds that the first rule outputs malignant with a high probability (0.99) and a high fidelity (0.99). She looks into the rule matrix and learns that if the marginal adhesion score is larger than 5, the model will very likely predict malignancy. This aligns with her knowledge that the loss of adhesion is a strong sign of cancer cells. Then she checks rule 3, which has the largest support from the dataset. The rule shows that if the bland chromatin (the texture of nucleus) is smaller or equal than 1, the cell should be benign. She finds this rule interesting since it indicates that one can quickly identify benign cells in the examination by checking if the nucleus is coarse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Use Case: Improving Diabetes Classification</head><p>In this use case, we the Pima Indian Diabetes Dataset (PIDD) <ref type="bibr" target="#b38">[39]</ref> to demonstrate how RuleMatrix can lead to performance improvements. The dataset contains diagnostic measurements of 768 female patients aged from 21 to 81, of Pima Indian heritage. The task is to classify negative patients (healthy) and positive patients (has diabetes). Each data instance contains eight features: the number of previous pregnancies, plasma glucose, blood pressure, skin thickness, insulin, body mass index (BMI), and diabetes pedigree function (DPF). DPF is a function measuring a patient's probability of getting diabetes based on the history of the patient's ancestors. The dataset is randomly partitioned into 75% training set and 25% test set. The distribution of the labels in the training set and test set are 366 negatives / 210 positives and 134 negatives / 58 positives respectively.</p><p>In the beginning, we trained a neural network of 2 layers with 20 neurons in each layer. The l-2 normalization factor was determined as 1.0 via 3-fold cross-validations. We ran the training 10 times and received an average accuracy of 72.4% on the test data. The best neural network had an accuracy of 74.0% on the test set. We ran the proposed rule-based explanation pipeline and extracted a list of 22 decision rules from a trained network. The rule list is visualized with the training data and a rule filter of minimum evidence of 0.02 <ref type="figure">(Fig. 6A)</ref>. From the header "evidence", we can see that the neural network achieves an overall accuracy of 79% on the training set.</p><p>Understanding the Rules (Q1, Q2). Then we navigated the extracted rules using the RuleMatrix with the training set. We noticed that there was no dominant rules with large supports, except for rule 4 and the last default rule, which have relatively longer bars in the "evidence" column, indicating a larger support. This reflects that the dataset is in a difficult domain and it is not easy to accurately predict whether one has diabetes or not. Rule 1 <ref type="figure">(Fig. 6-1</ref> ) has only one condition, 176 &lt; plasma glucose, which means that a patient with high plasma glucose is very likely to have diabetes. This agrees with our common knowledge in diabetes. Then we noticed that the outputs of rules 2 to 5 were all negative with probabilities above 0.98. Thanks to the aligned layout of features, we derived an overall sense that the patients younger than 32 ( <ref type="figure" target="#fig_1">Fig. 6-2</ref> ) and a BMI less than 36.5 are not likely to have diabetes. After going through the rest of the list, we concluded that patients with high plasma glucose and high BMI are more likely to have diabetes, and young patients are less likely to have diabetes in general.</p><p>Understanding the Errors (Q4). After navigating the rules, we were mostly interested in the type of patients that have diabetes but are wrongly classified as negative by the neural network. The false negative errors are undesirable in this domain since they may delay the treatment of a real patient and cause higher risks. Based on our findings concluded from rules 2 to 5, we decided to focus on the patients older than 32, that is, those with higher risk. We also filtered the patients with low or high plasma glucose (lower than 108 or higher than 137), because most of them are correctly classified as negative or positive by the model. As a result of the filtering, the accuracy of the model on the remaining data (74 instances) immediately dropped to 62%. From the resulting rules, we then further filtered patients with a BMI lower than 27, who are unlikely to have diabetes, and the patients with a DPF higher than 1.18, who are very likely to have the disease. After the filtering <ref type="figure">(Fig. 6B)</ref>, the accuracy of the model on the resulting subset of 62 patients dropped to only 56%. From <ref type="figure">Fig. 6C</ref>, we found a large portion of blue striped boxes ( ), denoting patients that have diabetes but were wrongly classified as healthy. This validated our suspicion that the patients with no obvious indicators are difficult to classify.</p><p>Improving the Performance. Based on the understanding of the error, a simple idea appeared to be worth trying: can we improve the accuracy of the model by oversampling the difficult subset? We experimented by oversampling this subset by half the amount to get 31 new training data, and trained new neural networks with the same hyper-parameters. To determine whether the change led to an actual improvement, we ran the training and sampling 10 times. The mean accuracy of 10 runs reached 75.5% on the test set, with a standard deviation of 2.1%. The best model had a performance of 78.6%, which was significantly better than the original best model (74.0%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">User Study</head><p>We conducted a quantitative experiment to evaluate the effectiveness of RuleMatrix in helping users understand the behavior of machine learning models. Our target is to investigate whether users can understand the interactive matrix-based representation of rules, and whether users can understand the behavior of a given model via the rule-based explanations. We asked participants to perform relevant tasks to benchmark the effectiveness of RuleMatrix, and asked for subjective feedback to understand users' preferences and directions for improvements. Study Design. We recruited nine participants, ages 22 to 30. Six were current graduate students majoring in computer science, three had experience in research projects related to machine learning, and none of them had prior experiences in model induction.</p><p>The study was organized into three steps. First, each participant was presented with a 15 minutes tutorial and was given 5 minutes to navigate and explore the interface. Second, participants were asked to perform a list of tasks using RuleMatrix. Finally, participants were asked to answer five subjective questions related to the general usability of the interface and suggestions for improvements. We used the Iris dataset and an SVM as the to-be-explained model during the tutorial. In the formal study, we used the Pima Indian Diabetes dataset, and used RuleMatrix to explain a neural network with two hidden layers with 20 neurons per layer. The extracted rule list contained 20 rules, each containing a conjunction of 1, 2, or 3 clauses).</p><p>Tasks. Six tasks <ref type="table" target="#tab_7">(Table 2)</ref> were created to validate participants' ability to answer the questions (Q1 -Q4) using RuleMatrix. For each task, we created two different questions with the same format (e.g., multiple-choice questions). That is, each participant was asked to perform 12 tasks. Questions of T1 to T5 were multiple-choice questions with one correct answer and four choices. T6(a) was also multiplechoice question while T6(b) asked the participants to enter a number.</p><p>Results. The average time that the participants took to complete all the 12 tasks in the formal study was 14' 43" (std: 2' 26"). Accuracy of the performed tasks is summarized in <ref type="table" target="#tab_7">Table 2</ref>. All the participants performed the required tasks fluently and correctly most of the time. This suggests validation of the basic usability of our method. However, we observed that participants took extra time in completing T2, which required the search and comparisons of multiple rules and multiple features. Three also complained that it was easy to get the wrong message from the textual representations provided in the choices in T1 and T2 (i.e., mistake 29 &lt; x from x &lt; 29), and they had to double check to make sure that the clauses they identified in the visualization indeed matched the texts. We examined the answer sheets and found the errors of T1 are all of this type. This affirmed to us that text is not as intuitive as graphics in representing intervals in our context.</p><p>Feedback. We gathered feedback through subjective questionnaires after the formal study. Most participants felt that the supported interactions (expand, highlight and filter) are very "useful and intuitive". The detailed information provided by the data flow and support view was also regarded as "helpful and just what they need". One participant liked how he could "locate my hypotheses in the rules and understand how the model reacts, whether it is right or wrong, and how much observations in the dataset supports the hypotheses". However, one participant had trouble understanding that there is only conjunctive relation between multiple clauses in a rule. Two participants suggested that a rule searching function would also be useful in validating hypotheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION AND CONCLUSIONS</head><p>In this work, we presented a technique for understanding classification models using rule-based explanations. We preliminarily validated the effectiveness of the rule induction algorithm on a set of benchmark datasets, and the effectiveness of the visual interface, RuleMatrix, through two use cases and a user study.</p><p>Potential Usage Scenarios. We anticipate the application of our method in domains where explainable intelligence is needed. Doctors can better utilize machine learning techniques for diagnosis and treatments with clear explanations. Banks can use efficient automatic credit approval systems while still being able to provide explanations to the applicants. Data scientists can better explain their findings when they need to present the results to no-experts.</p><p>Scalability of the Visualization. Though the current implementation of the RuleMatrix can visualize rule lists with over 100 rules with over 30 features, the readability and understandability have only been validated on rule lists with less than 60 rules and 20 features. It is unclear whether users can still get an overall understanding of the model from such a complex list of rules. In addition, we used a qualitative color scheme to encode different classes. Though the effectiveness is limited to datasets with a limit number of classes, we assume that the method will be effective in most cases, since most classification tasks have fewer than 10 classes. It is also interesting to see if the proposed interface can be extended to support regression models by changing the qualitative color scheme to sequential color schemes.</p><p>Scalability of the Rule Induction Method. An intrinsic limitation of the rule induction algorithm results from the trade-off between the fidelity and complexity (interpretability) of the generated rule list. Depending on the complexity of the model and the domain, the algorithm would require a list containing hundreds of rules to approximate the model with an acceptable fidelity. The interpretability of rules also depends on the meaningfulness of the input features. This also limits the usage of our method in domains such as image classification or speech recognition. Another limitation is the current unavailability of efficient learning algorithms for rule lists. The SBRL algorithm takes about 30 minutes to generate a rule list from 200,000 samples and 14 features on server with 2.2GHz Intel Xeon. Its performance does not generalize well to datasets with an arbitrary number of classes.</p><p>Future Work. One limitation of the presented work is that the method has not been fully validated with real experts in specific domains (e.g., health care). We expect to specialize the proposed method to meet the needs of specific domain problems (e.g., cancer diagnosis, or credit approvals) based on future collaborations with domain experts. Another interesting direction would be to systematically study the advantages and disadvantages of different knowledge representations (e.g., decision trees and rule sets) when considering human understandability. In other words, would people feel more comfortable with hierarchical representations (trees) or flat representations (lists) under different scenarios (e.g., verifying a prediction or understanding a complete model)?</p><p>We regard this work as a preliminary and exploratory step towards explainable machine learning and plan to further extend and validate the idea of interpretability via inductive rules.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3 Fig. 1 .</head><label>31</label><figDesc>Fig. 1. Understanding the behavior of a trained neural network using the explanatory visual interface of RuleMatrix. The user uses the control panel (A) to specify the detail information to visualize (e.g., dataset, level of detail, rule filters). The rule-based representation is visualized as a matrix (B), where each row represents a rule, and each column is a feature used in the rules. The user can also filter the data or use a customized input in the data filter (C) and navigate the filtered dataset in the data table (D).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The pipeline for creating a rule-based explanation interface. The rule induction step (1) takes (A) the training data and (B) the model to be explained as input, and produces (C) a rule list that approximates the original model. Then the rule list is filtered (2) according to user-specified thresholds of support and confidence. The rule list is visualized as RuleMatrix<ref type="bibr" target="#b2">(3)</ref> to help users navigate and analyze the rules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 )</head><label>2</label><figDesc>: 1. Rule Induction, 2. Filtering, and 3. Visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4</head><label></label><figDesc>Rule list R ← TRAIN(X sample , Y sample );5 return R; Algorithm 1: Rule Induction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 .</head><label>3</label><figDesc>The performance of the algorithm under different sampling rates. The x-axis shows the logarithms of the sampling rates. The blue, orange, and green lines show the average fidelities and average lengths of the extracted rule lists on the Abalone, Bank Marketing and Pima datasets for 10 runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>s t r a d iu s ( 4 ) m e a n c o n c a v e p o in ts ( 2 ) w o r s t c o n c a v it y ( 1 ) s m o o th n e s s e r r o r ( 1 ) m e a n te x tu r e ( 1 ) w o r s t s y m m e tr y ( 1 ) w o r s t te x tu r e (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>1 2 3 Fig. 4 .</head><label>134</label><figDesc>The visualization design. A: The data flow visualizes the data that satisfies a rule as a flow into the rule, providing an overall sense of the order of the rules. B: The rule matrix presents each rule as a row and each feature as a column. The clauses are visualized as glyphs in the corresponding cells. Users can click to expand a glyph to see the details of the distribution and the interval of the clause. C: The support view shows the fidelity of the rule for the provided data, and the evidence of the model's predictions and errors under a certain rule. 1 -3 : The designs of the glyph, the fidelity visualization, and the evidence visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 5 .</head><label>5</label><figDesc>o rm it y o f C e ll S h a p e (1 ) S in g le E p it h e li a l C e ll S iz e (1 ) Using the RuleMatrix to understand a neural network trained on the Breast Cancer Wisconsin (Original) dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>22 GFig. 6 .</head><label>226</label><figDesc>y M a s s In d e x (5 ) A g e (6 ) D ia b e te s P e d ig re e F u n c ti o n (2 ) P re g n a n c ie s (3 ) In s u li n (1 ) S k in T h ic k n e s s (1 ) lu c o s e (2 ) A g e (1 ) B o d y M a s s In d e x (3 ) P re g n a n c ie s (3 ) S k in T h ic k n e s s (1 ) O u tp u t (P r) F id e li ty (7 7 /1 0 0 ) E v id e n c e (A c c : 0 .The use case of understanding a neural network trained on Pima Indian Diabetes dataset. A: The initial visualization of the list of 22 extracted rules, with an overall fidelity of 91%. The neural network has an accuracy of 79% on the training data. B: The applied data filter. The ranges of the features are highlighted with light blue. C: The visualization of the rule list with the filtered data. The accuracy of the original model drops to only 56%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>• Yao Ming is with Hong Kong University of Science and Technology. E-mail: ymingaa@ust.hk • Huamin Qu is with Hong Kong University of Science and Technology.</figDesc><table /><note>E-mail: huamin@cse.ust.hk.• Enrico Bertini is with New York University. Email: enrico.bertini@nyu.edu</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Input: training data X , feature set S Output: The distribution estimation M Divide the features S into discrete features S disc and continuous features Scon;Partition X to X disc and Xcon according to S disc and Scon; /* Estimate the categorical distribution p */</figDesc><table><row><cell>3</cell></row></table><note>12</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>The first step is to build a model M that estimates the distribution of the training set</figDesc><table><row><cell></cell><cell></cell><cell>|X |;</cell></row><row><cell>9</cell><cell>end</cell><cell></cell></row><row><cell></cell><cell cols="2">/* Estimate conditional density f</cell><cell>*/</cell></row><row><cell>10</cell><cell cols="2">Estimate the bandwidth matrix H from Xcon;</cell></row><row><cell>11</cell><cell cols="2">for x (i) disc in Counter do</cell></row><row><cell>12</cell><cell>disc f x (i)</cell><cell>← DENSITYESTIMATION(Xcon, H);</cell></row><row><cell>13</cell><cell>end</cell><cell></cell></row><row><cell>14</cell><cell cols="2">return M = (p, f );</cell></row><row><cell></cell><cell cols="2">Algorithm 2: Estimate Distribution</cell></row><row><cell></cell><cell cols="2">Distribution Estimation.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 .</head><label>1</label><figDesc>The fidelities of the rule list generated by the algorithm from a neural network and an SVM. The table reports the mean and standard deviation (with parenthesis) in percentage of the fidelity of 10 runs for each setting.Breast Cancer 95.5 (1.4) 94.5 (1.5) 95.0 (2.0) 95.9 (1.4) Wine 93.1 (2.3) 94.0 (2.4) 94.0 (3.7) 91.3 (3.5) Iris 96.3 (1.7) 97.9 (2.6) 94.7 (3.1) 97.4 (2.0)</figDesc><table><row><cell>Dataset</cell><cell>NN-1</cell><cell>NN-2</cell><cell>NN-4</cell><cell>SVM</cell></row><row><cell cols="5">Pima 89.6 (2.0) 89.9 (1.2) 89.5 (1.7) 91.8 (1.5)</cell></row><row><cell cols="5">Abalone 88.5 (0.9) 88.6 (0.7) 86.8 (0.5) 90.1 (0.8)</cell></row><row><cell cols="5">Bank Marketing 96.4 (0.8) 92.1 (1.0) 89.1 (1.3) 97.0 (0.7)</cell></row><row><cell cols="5">Adult 95.0 (0.2) 94.8 (0.4) 93.2 (0.3) 96.7 (0.3)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 .</head><label>2</label><figDesc>The experiment tasks and results. The results are summarized as the number of correct answers / total number of questions. Which of the textual descriptions best describe rule i?16/18T2 Q1 Which of the rules exists in the extract rule lists? 18/18 T3 Q2 Which of the highlighted rules is most reliable in repre-</figDesc><table><row><cell>Goal Question</cell><cell>Result</cell></row><row><cell>T1 Q1 senting the original model?</cell><cell>17/18</cell></row><row><cell>T4 Q2 Which of the highlighted rules has the largest support?</cell><cell>17/18</cell></row><row><cell>T5 Q4 Under which of the four highlighted rules, the original</cell><cell>17/18</cell></row><row><cell>model is most likely to give wrong predictions?</cell><cell></cell></row><row><cell>T6 Q3 For a given data (presented in texts),</cell><cell></cell></row><row><cell>(a) what would the original model most likely to predict?</cell><cell>18/18</cell></row><row><cell>(b) which rule do you utilize to perform the prediction?</cell><cell>17/18</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was partially supported by the 973 National Basic Research Program of China (2014CB340304) and the Defense Advanced Research Projects Agency (DARPA) D3M program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Trends and trajectories for explainable, accountable and intelligible systems: An hci research agenda</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Y</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kankanhalli</surname></persName>
		</author>
		<idno type="DOI">10.1145/3173574.3174156</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI Conference on Human Factors in Computing Systems</title>
		<meeting>CHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">User-oriented assessment of classification model understandability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Allahyari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lavesson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th Conf. Artificial Inelligence</title>
		<meeting>11th Conf. Artificial Inelligence</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Survey and critique of techniques for extracting rules from trained artificial neural networks. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Diederich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Tickle</surname></persName>
		</author>
		<idno type="DOI">10.1016/0950-7051(96)81920-4</idno>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="81920" to="81924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rule extraction from neural networks -a comparative study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Augasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kathirvalavakumar</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICPRIME.2012.6208380</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Pattern Recognition, Informatics and Medical Engineering</title>
		<meeting>Int. Conf. Pattern Recognition, Informatics and Medical Engineering</meeting>
		<imprint>
			<date type="published" when="2012-03" />
			<biblScope unit="page" from="404" to="408" />
		</imprint>
	</monogr>
	<note>PRIME-2012</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Do convolutional neural networks learn class hierarchy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bilal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2017.2744683</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="152" to="162" />
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ensemble classification based on generalized additive models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D</forename><surname>Bock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Coussement</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">V</forename><surname>Poel</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.csda.2009.12.013</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics &amp; Data Analysis</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1535" to="1546" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Classification and regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Olshen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Elhadad</surname></persName>
		</author>
		<idno type="DOI">10.1145/2783258.2788613</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. 21th ACM SIGKDD Int. Conf. Knowledge Discovery and Data Mining, KDD &apos;15</title>
		<meeting>21th ACM SIGKDD Int. Conf. Knowledge Discovery and Data Mining, KDD &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1721" to="1730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Extracting tree-structured representations of trained networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Shavlik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th Int. Conf. Neural Information Processing Systems, NIPS&apos;95</title>
		<meeting>8th Int. Conf. Neural Information essing Systems, NIPS&apos;95<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="24" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dheeru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">Karra</forename><surname>Taniskidou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Prie: a system for generating rulelists to maximize roc performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fawcett</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10618-008-0089-y</idno>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="224" />
			<date type="published" when="2008-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Comprehensible classification models: A position paper</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Freitas</surname></persName>
		</author>
		<idno type="DOI">10.1145/2594473.2594475</idno>
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explor. Newsl</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2014-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">European union regulations on algorithmic decision-making and a &quot;right to explanation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Flaxman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="50" to="57" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mining frequent patterns without candidate generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<idno type="DOI">10.1145/335191.335372</idno>
		<idno>doi: 10.1145/ 335191.335372</idno>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMOD Record</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2000-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An empirical evaluation of the comprehensibility of decision table, tree and rule based predictive models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huysmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dejaeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanthienen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Baesens</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.dss.2010.12.003</idno>
	</analytic>
	<monogr>
		<title level="j">Decision Support Systems</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="154" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Activis: Visual exploration of industry-scale deep neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kalro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2017.2744718</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A workflow for visual diagnostics of binary classifiers using instance-level explanations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Swartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aphinyanaphongs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bertini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Visual Analytics Science and Technology (VAST)</title>
		<meeting>Visual Analytics Science and Technology (VAST)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Martic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Krakovna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Everitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lefrancq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Orseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Legg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09883</idno>
		<title level="m">AI safety gridworlds</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Letham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Mccormick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Madigan</surname></persName>
		</author>
		<idno type="DOI">10.1214/15-AOAS848</idno>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1350" to="1371" />
			<date type="published" when="2015-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deeptracker: Visualizing the training process of convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Towards better analysis of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2016.2598831</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="100" />
			<date type="published" when="2017-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards better analysis of machine learning models: A visual analytics perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Informatics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="48" to="56" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning with decision lists of datadependent features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sokolova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="427" to="451" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Decompositional rule extraction from support vector machines by active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Baesens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">V</forename><surname>Gestel</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2008.131</idno>
		<idno>doi: 10. 1109/TKDE.2008.131</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="178" to="191" />
			<date type="published" when="2009-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Understanding hidden memories of recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Visual Analytics Science and Technology (VAST)</title>
		<meeting>Visual Analytics Science and Technology (VAST)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scikitlearn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deepeyes: Progressive visual analytics for designing deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pezzotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hllt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P F</forename><surname>Lelieveldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eisemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vilanova</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2017.2744358</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="108" />
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Generating production rules from decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th Int. Conf. Artificial Intelligence, IJCAI&apos;87</title>
		<meeting>10th Int. Conf. Artificial Intelligence, IJCAI&apos;87<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1987" />
			<biblScope unit="page" from="304" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Simplifying decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0020-7373(87)80053-6</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Studies</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="80053" to="80059" />
			<date type="published" when="1987-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visualizing the hidden activity of artificial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Fadel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Falco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Telea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="101" to="110" />
			<date type="published" when="2017-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Squares: Supporting interactive performance analysis for multiclass classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2016.2598828</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="70" />
			<date type="published" when="2017-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Why should I trust you?&quot;: Explaining the predictions of any classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939672.2939778</idno>
		<idno>doi: 10. 1145/2939672.2939778</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd ACM SIGKDD, KDD &apos;16</title>
		<meeting>22nd ACM SIGKDD, KDD &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Modeling by shortest data description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
		<idno type="DOI">10.1016/0005-1098(78)90005-5</idno>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="465" to="471" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning decision lists</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Rivest</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1022607331053</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1987-11" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="229" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">What you see is what you can change: Humancentered machine learning by interactive visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sacha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sedlmair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peltonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>North</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2017.01.105</idno>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">268</biblScope>
			<biblScope unit="issue">C</biblScope>
			<biblScope unit="page" from="164" to="175" />
			<date type="published" when="2017-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Treevis.net: A tree visualization reference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Schulz</surname></persName>
		</author>
		<idno type="DOI">10.1109/MCG.2011.103</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="11" to="15" />
			<date type="published" when="2011-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Density estimation for statistics and data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Silverman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>CRC press</publisher>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learning Representations (ICLR) Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Using the adap learning algorithm to forecast the onset of diabetes mellitus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Everhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Knowler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Johannes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Symp</title>
		<meeting>Annu. Symp</meeting>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="page">261</biblScope>
		</imprint>
	</monogr>
	<note>Computer Application in Medical Care</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Lstmvis: A tool for visual analysis of hidden state dynamics in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2017.2744158</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="667" to="676" />
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Beautiful Evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Tufte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graphis Pr</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="46" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Opening the black box-data driven visualization of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-Y</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Visualization</title>
		<meeting>Visualization</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="383" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">BaobabView: Interactive construction and analysis of decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Van Den Elzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Van Wijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Visual Analytics Science and Technology (VAST)</title>
		<meeting>Visual Analytics Science and Technology (VAST)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011-10" />
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">From decision tables to expert system shells</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanthienen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data &amp; Knowledge Engineering</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="265" to="282" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Falling Rule Lists</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th Int. Conf. Artificial Intelligence and Statistics</title>
		<meeting>18th Int. Conf. Artificial Intelligence and Statistics<address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1013" to="1022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Visualizing dataflow graphs of deep learning models in tensorflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wongsuphasawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Smilkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Man</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Vigas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2017.2744878</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Scalable Bayesian rule lists</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seltzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 34th Int. Conf. Machine Learning (ICML)</title>
		<meeting>34th Int. Conf. Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
