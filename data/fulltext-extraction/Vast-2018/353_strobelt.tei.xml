<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SEQ2SEQ-VIS : A Visual Debugging Tool for Sequence-to-Sequence Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Behrisch</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Perer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
						</author>
						<title level="a" type="main">SEQ2SEQ-VIS : A Visual Debugging Tool for Sequence-to-Sequence Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Explainable AI</term>
					<term>Visual Debugging</term>
					<term>Visual Analytics</term>
					<term>Machine Learning</term>
					<term>Deep Learning</term>
					<term>NLP</term>
				</keywords>
			</textClass>
			<abstract>
				<p>our tool helps to find errors in seq2seq models using visual analysis methods. decoder (red highlight) but is not part of the language dictionary. When investigating the encoder neighborhoods (right), the user sees that &quot;seq2seq&quot; is close to other unknown words &quot; unk &quot;. The buttons enable user interactions for deeper analysis.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>unser werkzeug hil , fehler in &lt;unk&gt; modellen zu finden , die mit visuellen analysen &lt;unk&gt; .</p><p>unser werkzeug hil , fehler in &lt;unk&gt; modellen zu finden , die mit visuellen analysen der .</p><p>unsere instrument dabei dabei fehlern zu der modelle anzuwenden entdecken mit mit mittels visueller &lt;unk&gt; von &lt;unk&gt; das tool hilfreich dazu abwei chungen bei den &lt;unk&gt; einzusetzen suchen die indem visuell der belegen &lt;unk&gt; werden unserem hilfsmittel ist zu &lt;unk&gt; für form , mit verschaffen mittels mittels visuellen vi suali si erung auswertung geprägt zu so for example , there was one study that was done in a population of &lt;unk&gt; jews in new york city . in this manner , the world bank has now &lt;unk&gt; 30,000 project activities in &lt;unk&gt; countries , and donors are using a common platform to map all their projects . now compassion , when it enters the news , too o en comes in the form of &lt;unk&gt; feature pieces or &lt;unk&gt; about heroic people you could never be like or happy endings or examples of self-sacrifice that would seem to be too good to be true most of the time and they caught a couple of my guys who had hidden cameras in &lt;unk&gt; bags . and with these keys , they may have been able to get inside &lt;unk&gt; &amp;apos;s systems , to see and hear everything , and maybe even infect some of them .</p><formula xml:id="formula_0">wir</formula><p>on that table you can see 48 hours &amp;apos; worth of &lt;unk&gt; goods from passengers entering in to the united states . so these are consumers organizing , &lt;unk&gt; their resources to &lt;unk&gt; companies to do good . this is not the story of how you get shelf space at &lt;unk&gt; marcus . tony in chicago has been taking on growing experiments , like lots of other window farmers , and he &amp;apos;s been able to get his strawberries to fruit for nine months of the year in &lt;unk&gt; conditions by simply changing out the organic nutrients . and the important point about this is that it &amp;apos;s the earliest study in &lt;unk&gt; in mathematics . so the first time i worked with colors was by making these &lt;unk&gt; of &lt;unk&gt; &lt;unk&gt; .</p><p>who is going to allow a bunch of little girls , dressed up -&amp;quot; &amp;quot; -to come inside a jail and dance with their &lt;unk&gt; in &lt;unk&gt; suits ? &amp;quot; our tool helps to find errors in seq2seq models using visual analysis 3/29/2018 S2S Attention http://localhost:8080/client/index.html?in=our%20tool%20helps%20to%20find%20errors%20in%20%20seq2seq%20models%20using%20visual%20analysis%20methods%20.</p><p>Start entering some encoder sentence (enter triggers request)...</p><p>our tool helps to find errors in seq2seq models using visual analysis methods . In the translation view (left), the source sequence "our tool helps to find errors in seq2seq models using visual analysis methods." is translated into a German sentence. The word "seq2seq" has correct attention between encoder and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep learning approaches based on neural networks have shown significant performance improvements on many artificial intelligence tasks. However, the complex structure of these networks often makes it difficult to provide explanations for their predictions. Attention-based sequence-to-sequence models (seq2seq) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b49">49]</ref>, also known as encoderdecoder models, are representative of this trend. Seq2seq models have shown state-of-the-art performance in a broad range of applications such as machine translation, natural language generation, image captioning, and summarization. Recent results show that these models exhibit human-level performance in machine translation for certain important domains <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b51">51]</ref>. Seq2seq models are powerful because they provide an effective supervised approach for processing and predicting sequences without requiring manual specification of the relationships between source and target sequences. Using a single model, these systems learn to do reordering, transformation, compression, or expansion of a source sequence to an output target sequence. These modifications are performed using a large internal state representation first encodes and then decodes the source sequence. With enough data, these models provide a general purpose mechanism for learning to predict sequences.</p><p>While the impact of seq2seq models has been clear, the added complexity and uncertainty of deep learning based models raises issues. These models act as black-boxes during prediction, making it difficult to track the source of mistakes. The high-dimensional internal representations make it difficult to analyze the model as it transforms the data. While this property is shared across deep learning, mistakes involving language are often very apparent to human readers. For instance, a widely publicized incident resulted from a seq2seq translation system mistakenly translating "good morning" into "attack them" leading to a wrongful arrest <ref type="bibr" target="#b12">[12]</ref>. Common but worrying failures in seq2seq models include machine translation systems greatly mistranslating a sentence, image captioning systems yielding an incorrect caption, or speech recognition systems producing an incorrect transcript.</p><p>Ideally, model developers would understand and trust the results of their systems, but currently, this goal is out of reach. In the meantime, the visual analytics community can contribute to this crucial challenge of better surfacing the mistakes of seq2seq systems in a general and reproducible way. We propose SEQ2SEQ-VIS , a visual analytics tool that satisfies this criteria by providing support for the following three goals:</p><p>• Examine Model Decisions: SEQ2SEQ-VIS allows users to understand, describe, and externalize model errors for each stage of the seq2seq pipeline.</p><p>• Connect Decisions to Samples: SEQ2SEQ-VIS describes the origin of a seq2seq model's decisions by relating internal states to relevant training samples.</p><p>• Test Alternative Decisions: SEQ2SEQ-VIS facilitates model interventions by making it easy to manipulate of model internals and conduct "what if" explorations.</p><p>The full system is shown in <ref type="figure" target="#fig_1">Figure 1</ref> (or larger in <ref type="figure" target="#fig_8">Fig. 7</ref>). It integrates visualizations for the components of the model <ref type="figure" target="#fig_1">(Fig 1 left)</ref> with internal representations from specific examples <ref type="figure" target="#fig_1">(Fig 1 middle)</ref> and nearestneighbor lookups over a large offline corpus of precomputed examples <ref type="figure" target="#fig_1">(Fig 1 right)</ref>.</p><p>We begin in Sect. 2 by introducing important background and notation to formalize our overall goal of seq2seq model debuggers. In Sect. 3 we present a guiding example illustrating how a typical model understanding-and debugging session looks like for an analyst. The subsequent "Goals and Task" Section 4 enumerates the goals and procedures for building a seq2seq debugger. Based on these guidelines, Sect. 5 and Sect. 6 introduce our visual and implementation design choices. Sect. 7 highlights in three further real-world use cases for how SEQ2SEQ-VIS guides the user through the visual analysis process. Sect. 8 puts these contributions in the context of related work for this research domain and Sect. 9 presents future work and reflections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SEQUENCE-TO-SEQUENCE MODELS AND ATTENTION</head><p>We begin with a formal taxonomy of seq2seq models that will inform our visual analytics approach. Throughout this work, for brevity and clarity, we will consider the running example of automatic translation from one language to another. We use the sequence notation x 1:S to represent an S-word sentence in a source language, and y 1:T to represent a T -word sentence in a target language. Seq2seq models perform translation in a left-to-right manner, one target word at a time, until a special stop token is generated, which ends the translation.</p><p>We break down the translation process of seq2seq models into five stages: (S1) encode the source sentence, (S2) decode the current target words, (S3) attend to the encoded source, (S4) predict the next target word, and (S5) search for the best complete translation. Note that some systems use a slightly different order, but most adhere roughly to this setup. <ref type="figure">Fig. 2</ref> provides a structural overview of these five stages. Encoder (S1): Encoding uses a deep neural network to convert a sequence of source words x 1:S into a sequence of vectors x 1:S . Each vector in the sequence x s roughly represents one word x s but also takes into account the surrounding words, both preceding and succeeding, that may determine its contextual meaning. This encoding is typically done using a recurrent neural network (RNN) or a long short-term memory network (LSTM), however recently non-RNN-based methods such as convolutional neural networks (CNN) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> and Transformer <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b50">50]</ref> have also been employed. Our approach supports all types of encoding methods. Decoder (S2): The decoding process is analogous to encoding, and takes the sequence of previously generated target words y 1:t and converts them to a sequence of latent vectors y 1:t . Each vector represents the state of the sentence up to and including word y t . This provides a similar contextual representation as in the encoder, but is only based on previous words. Upon producing a new word, the prediction is used as input to the decoder. Attention (S3): The attention component matches encoder hidden states and decoder hidden states. For each y t we consider which encoder states x s are relevant to the next prediction. In some similar language pairs, like French and Spanish, the words often align in order, e.g., the fourth French word matches the fourth Spanish word. However for languages such as English and Chinese, the matching might be quite far away. Instead of using absolute position, attention compares the word representations to find which source position to translate. Attention forms a distribution based on the dot product between vectors x s • y t . We call this value a s,t , and it indicates how closely the source and target positions match. Prediction (S4): The prediction step produces a multi-class distribution over all the words of the target language -words that are more Five stages in translating a source to target sequence: (S1) encoding the source sequence into latent vectors, (S2) decoding to generate target latent vectors, (S3) attend between encoder and decoder, (S4) predict word probabilities at each time step, and (S5) search for a best complete translation (beam search).</p><p>likely to come next have higher probability. This problem takes two factors into account: the current decoder state y t and the encoder states weighted by attention, known as the context vector coming from S3. These two are combined to predict a distribution over the next word p(y t+1 |x 1:S , y 1:t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Search (S5):</head><p>To actually produce a translation, these previous steps are combined into a search procedure. Beam search is a variant of standard tree search that aims to efficiently explore the space of translations. The deep learning component of Seq2seq models predicts the probability of all next words, given a prefix. While one could simply take the highest probability word at each time step, it is possible that this choice will lead down a bad path (for instance, first picking the word "an" and then wanting a word starting with a consonant). Beam search instead pursues several possible hypothesis translations each time step. It does so by building a tree comprising the top Khypothesis translations. At each point, all next words are generated for each. Of these, only the most likely K are preserved. Once all K beams have terminated by generating the stop token, the final prediction is the translation with the highest score. Each stage of the process is crucial for effective translation, and it is hard to separate them. However, the model does preserve some separations of concerns. The decoder (S2) and encoder (S1) primarily work with their respective language, and manage the change in hidden representations over time. Attention (S3) provides a link between the two representations and connects them during training. Prediction (S4) combines the current decoder state with the information moving through the attention. Finally, search (S5) combines these with a global score table. These five stages provide the foundation for our visual analytics system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MOTIVATING CASE STUDY: DEBUGGING TRANSLATION</head><p>To motivate the need for our contributions, we present a representative case study. Further case studies are discussed in Sect. 7. This case study involves a model trainer (see <ref type="bibr" target="#b48">[48]</ref>) who is building a German-to-English translation model (our model is trained on the small-sized IWSLT '14 dataset <ref type="bibr" target="#b31">[31]</ref>).</p><p>The user begins by seeing that a specific example was mistranslated in a production setting. She finds the source sentence: Die längsten Reisen fangen an, wenn es auf den Straßen dunkel wird. <ref type="bibr" target="#b0">1</ref> This sentence should have been translated to: The longest journeys begin, when it gets dark in the streets. She notices that the model produces the mistranslation: The longest journey begins, when it gets to the streets. <ref type="figure" target="#fig_6">Fig. 5</ref>(E/D) shows the tokenized input sentence in blue and the corresponding translation of the model in yellow (on the top). The user observes that the model does not translate the word dunkel into dark.</p><p>This mistake exemplifies several goals that motivated the development of Seq2Seq-Vis. The user would like to examine the system's decisions, connect to training examples, and test possible changes. As described in Sect. 2, these goals apply to all five model stages: encoder, decoder, attention, prediction, and search. Hypothesis: Encoder (S1) Error? Seq2Seq-Vis lets the user examine similar encoder states for any example. Throughout, we will use the term neighborhood to refer to the twenty closest states in vector space from training data. SEQ2SEQ-VIS displays the nearest neighbor sentences for a specific encoder state as red highlights in a list of training set examples. <ref type="figure" target="#fig_4">Fig. 3</ref> shows that the nearest neighbors for dunkel match similar uses of the word. The majority seem to express variations of dunkel. The few exceptions, e.g., db, are artifacts that can motivate corrections of the training data or trigger further investigation. Overall, the encoder seems to perform well in this case. Hypothesis: Decoder (S2) Error? Similarly, the user can apply SEQ2SEQ-VIS to investigate the neighborhood of decoder states produced at times t and t + 1 <ref type="figure" target="#fig_5">(Fig. 4)</ref>. In addition to the neighbor list, it gives a projection view that depicts all decoder states for the current translation and all their neighbors in a 2D plane. The analyst observes that the decoder states produced by gets and streets are in proximity and share neighbors. Since these states are indicative for the next word we can switch the highlight one text position to the right (+1) and observe that the decoder states at gets and streets support producing dark, darker, or darkness. Thus, the decoder state does not seem very likely as the cause of the error. <ref type="bibr" target="#b0">1</ref> The closing quote of the book 'Kant' from German author Jörg Fauser, who is attributed as being a forerunner of German underground literature. Hypothesis: Attention (S3) Error? Since both encoder and decoder are working, another possible issue is that the attention may not focus on the corresponding source token dunkel. The previous hypothesis testing revealed that well-supported positions for adding dark are after gets or streets. This matches human intuition, as we can imagine the following sentences being valid translations: The longest travels begin when it gets dark in the streets. or The longest travels begin when it gets to the streets turning dark. In <ref type="figure" target="#fig_6">Fig. 5</ref>(S3) our analyst can observe that the highlighted connection following get to the correct next word dunkel is very strong. The connection width indicates that the attention weight is very high with the correct word. Therefore, the user can assume that the attention is well set for predicting dark in this position. The hypothesis for error in S3 can be rejected with high probability. Hypothesis: Prediction (S4) Error? The combination of decoder state and attention is used to compute the probability of the next word. It may be that an error occurs in this decision, leading to a poor probability of the word dark. The tool shows the most likely next words and their probabilities in <ref type="figure" target="#fig_5">Fig. 5(S4)</ref>. Here, our analyst can see that the model mistakenly assigns a higher probability to to than dark. However, both options are very close in probability, indicating that the model is quite uncertain and almost equally split between the two choices. These local mistakes should be automatically fixed by the beam search, because the correct choice dark leads to a globally more likely sentence. Hypothesis: Search (S5) Error? Having eliminated all other possible issues, the problem is likely to be a search error. The user can investigate the entire beam search tree in <ref type="figure" target="#fig_6">Fig. 5</ref>(S5), which shows the top K considered options at each prediction step. In this case, the analyst finds that dark is never considered within the search. Since the previous test showed that to is only minimally more likely than dark, a larger K would probably have lead to the model considering dark as the next best option. We therefore conclude that this local bottleneck of a too narrow beam search is the most likely error case. The analyst has identified a search error, where the approximations made by beam search cut off the better global option in favor of a worse local choice. Exploring Solutions. When observing the K-best predictions for the position of to, the analyst sees that dark and to are close in probability ( <ref type="figure" target="#fig_5">Fig. 5(S4)</ref>). To investigate whether the model would produce the correct answer if it had considered dark, SEQ2SEQ-VIS allows the user to evaluate a case-specific fix. The analyst can test this counterfactual, what would have happened if she had forced the translation to use dark at this critical position? By clicking on dark she can produce this probe (shown in <ref type="figure">Fig. 6</ref>), which yields the correct translation. The user can now describe the most likely cause of error (search error) and a local fix to the problem (forced search to include dark). The analyst can now add this case to a list of well-described bugs for the model and later consider a global fix. wir vergrößern das blickfeld , wir zoomen raus , durch eine nukleare pore , welche der zugang zu dem teil , der die dna beherbergt , ist und nukleus genannt wird . der &lt;unk&gt; ist dunkel auf dem einen und hell auf dem anderen bild . P <ref type="figure">Fig. 6</ref>. Testing a fix -by clicking on the correct word dark in the predicted top-K, the beam search is forced on a specific path (P) which leads to the correct prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">GOALS AND TASKS</head><p>We now step back from this specific instance and consider a common deployment cycle for a deep learning model such as seq2seq. First, a model is trained on a task with a possibly new data set, and then evaluated with a standard metric. The model performs well in aggregate and the stakeholders decide to deploy it. However, for a certain subset of examples there exist non-trivial failures. These may be noticed by users, or, in the case of translation, by post-editors who correct the output of the system. While the model itself is still useful, these examples might be significantly problematic as to cause alarm.</p><p>Although these failures can occur in any system, this issue was much less problematic in previous generations of AI systems. For instance when using rule-based techniques, a user can explore the provenance of a decision through rules activated for a given output. If there is a mistake in the system, an analyst can 1) identify which rule misfired, 2) see which previous examples motivated the inclusion of the rule, and 3) experiment with alternative instances to confirm this behavior.</p><p>Ideally, a system could provide both functionalities: the high performance of deep learning with the ability to interactively spot issues and explore alternatives. However, the current architecture of most neural networks makes it more challenging to examine decisions of the model and locate problematic cases. Our work tackles the following challenges and domain goals for seq2seq models analogous to the three steps in rule-based systems: Goal G1 -Examine Model Decisions: It is first important to examine the model's decision chain in order to track down the error's root cause. As mentioned in Sect. 2, seq2seq models make decision through several stages. While it has proven difficult to provide robust examination in general-purpose neural networks, there has been success for specific decision components. For example, the attention stage (S3) has proven specifically useful for inspection <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b52">52]</ref>. Our first goal is to develop interactive visual interfaces that help users understand the model's components, their relationships, and pinpoint sources of error. Goal G2 -Connect Decisions to Samples from Training Data: Once a model makes a particular decision, a user should be able to trace what factors influenced this decision. While it is difficult to provide specific reasoning about the many factors that led to a decision in a trained model, we hope to provide other means of analysis. In particular, we consider the approach of mapping example states to those from previous runs of the model. For instance, the training data defines the world view of a model and therefore influences its learned decisions <ref type="bibr" target="#b20">[20]</ref>. The goal is to utilize (past) samples from training data as a proxy to better understand the decision made on the example in question. Goal G3 -Test Alternative Decisions: Ultimately, though, the goal of the user is to improve the model's performance and robustness. While the current state-of-the art for diagnosing and improving deep neural network models is still in an early stage <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b47">47]</ref>, our goal is to allow the user to test specific interventions. We aim to let the user investigate causal effects of changing parts of the model the let users ask what if specific intermittent outputs of a model changed.</p><p>Our motivating case study (Sect. 3) follows these goals: First, the user defines five hypotheses for causes of error and tests them by examining the model's decisions (G1). Some of these decisions (for S1, S2) are represented in the model only as latent high-dimensional vectors. To make these parts tangible for the user, she connects them to representative neighbors from the training data (G2). Finally, by probing alternatives in the beam search (G3) she finds a temporary alternative that helps her to formulate a better solution.</p><p>We use these goals to compile a set of visualization and interaction tasks for Seq2Seq-Vis. The mapping of these tasks to goals is indicated by square brackets: Task T1 -Create common visual encodings of all five model stages to allow a user to examine the learned connections between these modules.</p><p>[G1] Task T2 -Visualize state progression of latent vector sequences over time to allow for high-level view of the learned representations.</p><p>[G1] Task T3 -Explore generated latent vectors and their nearest neighbors by querying a large database of training examples to facilitate error identification and training adjustment.</p><p>[G2] Task T4 -Generate sensible alternative decisions for different stages of the model and compare them to ease model exploration and compare possible corrections. [G1, G3] Task T5 -Create a general and coherent interface to utilize a similar front-end for many sequence-to-sequence problems such as translation, summary, and generation. [G1,G2,G3]</p><p>In the following section, we will match these tasks and goals to design decisions for SEQ2SEQ-VIS .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DESIGN OF Seq2Seq-Vis</head><p>Seq2Seq-Vis is the result of an iterative design process and discussions between experts in machine learning and visualization. In regular meetings we evaluated a series of low-fidelity prototypes and tested them for usability. The design presented in this section combines the prevailing ideas into a comprehensive tool.</p><p>Seq2Seq-Vis is composed of two main views facilitating different modes of analysis: In the upper part, the translation view provides a visual encoding for each of the model stages and fosters understanding and comparison tasks. In the lower part, the neighborhood view enables deep analysis based on neighborhoods of training data. <ref type="figure" target="#fig_8">Fig. 7</ref> shows the complete tool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Translation View</head><p>In the translation view <ref type="figure" target="#fig_8">(Fig. 7a)</ref>, each functional stage of the seq2seq model is mapped to a visual encoding (T1, T2, G1). We generalize and extend encodings from Olah &amp; Carter <ref type="bibr" target="#b36">[36]</ref> and Le et al. <ref type="bibr" target="#b23">[23]</ref>.</p><p>http://localhost:8080/client/index.html?in=wir%20wollen%20heute%20mal%20richtig%20spass%20haben%20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1/3</head><p>Start entering some encoder sentence (enter triggers request)...</p><p>wir wollen heute mal richtig spass haben . In Attention Vis <ref type="figure" target="#fig_8">(Fig. 7c)</ref>, the encoder words are shown in blue, the decoder words in yellow, and the attention is shown through weighted bipartite connections. To reduce visual clutter the attention graph is pruned. For each decoder step all edges are excluded that fall into the lower quartile of the attention probability distribution.</p><p>Right below the yellow decoder words, the top K predictions (S4 of model) for each time step are shown ( <ref type="figure" target="#fig_8">Fig. 7d)</ref>. Each possible prediction encodes information about its probability in the underlying bar chart, as well as an indication if it was chosen for the final output (yellow highlight).</p><p>In the bottom part of the translation view, a tree visualization shows the hypotheses from the beam search stage <ref type="figure" target="#fig_8">(Fig. 7e)</ref>. The most probable hypothesis, which results in the final translation sentence, is highlighted. Several interactions can be triggered from the translation view, which will be explained in Sect. 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Neighborhood View</head><p>The neighborhood view <ref type="figure" target="#fig_8">(Fig. 7b</ref>) takes a novel approach to look at model decisions in the context of finding similar examples (T2, T3, G1, G2). As discussed in Sect. 2, seq2seq models produce high-dimensional vectors at each stage, e.g., encoder states, decoder states, or context states. It is difficult to interpret these vectors directly. However, we can estimate their meaning by looking at examples that produces similar vectors. To enable this comparison, we precompute the hidden states of a large set of example sentences (we use 50k sentences from the training set). For each state produced by the model on a given example, SEQ2SEQ-VIS searches for nearest neighbors from this large subset of precomputed states. These nearest neighbors are input to the state trajectories ( <ref type="figure" target="#fig_8">Fig. 7g</ref>) and to the neighbor list <ref type="figure" target="#fig_8">(Fig. 7h)</ref>.</p><p>The state trajectories show the changing internal hidden state of the model with the goal of facilitating task T2. This view encodes the dynamics of a model as a continuous trajectory. First, the set for all states and their closest neighbors are projected using a non-linear algorithm, such as t-SNE <ref type="bibr" target="#b30">[30]</ref>, non-metric MDS <ref type="bibr" target="#b22">[22]</ref>, or a custom projection (see Sect. 7). This gives a 2D positioning for each vector. We use these positions to represent each encoder/decoder sequence as a trace connecting its vectors. See <ref type="figure" target="#fig_8">Fig. 7g</ref> for an example of a trace representing the encoder states for wir wollen heute mal richtig spass haben.</p><p>In the projection, the nearest neighbors to each vector are shown as nearby dots. When hovering over a vector from the input, the related nearest neighbor counterparts are highlighted and a temporary red line connects them. For vectors with many connections (high centrality), we reduce visual clutter by computing a concave hull for all related neighbors and highlight the related dots within the hull. Furthermore, we set the radius of each neighbor dot to be dependent on how many original states refer to it. E.g., if three states from a decoder sequence have one common neighbor, the neighbor's radius is set to ∼ 2.5 (we use a r(x) = √ 2x mapping with x being number of common neighbors). The state trajectories can be quite long. To ease understanding, we render a view showing states in their local neighborhood as a series of trajectory pictograms <ref type="figure" target="#fig_8">(Fig. 7f)</ref>. Each little window is a cut-out from the projection view, derived from applying a regular grid on top of the projection plane. Each pictogram shows only the cut-out region in which the respective vector can be found.</p><p>Clicking on any projected vector will show the neighbor list on the right side of the view. The neighbor list shows the actual sentences corresponding to the neighbor points, grounding these vectors in particular words and their contexts. Specifically, the neighbor list shows all the nearest neighbors for the selected point with the original sequence pair. The source or target position in the sequence that matches is highlighted in red. The user can facet the list by filtering only to show source (blue) or target (yellow) sequences. She can also offset the text highlight by −1 or +1 to see alignment for preceding or succeeding word positions (see <ref type="figure" target="#fig_5">Fig. 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Global Encodings and Comparison Mode</head><p>Seq2Seq-Vis uses visual encodings that are homogenous across all views to create a coherent experience and ease the tool's learning curve for model architects and trainers (T5). First, a consistent color scheme allows the user to identify the stage and origin of data (encoder -blue, decoder -yellow, pivot -green, compare -violet). Furthermore, every visual element with round corners is clickable and leads to a specific action. Across all views, hovering highlights related entities in red.</p><p>Additionally, the tool has a global comparison mode (T4, G1, G3). As soon as we generate a comparison sample from one of several triggers (Sect. 5.4), all views switch to a mode that allows comparison between examples. Attention Vis, Trajectory Pictograms, and State Projector display a superimposed layer of visual marks labeled with a comparison color (violet) different from the pivot color (green). To ease understanding, we disable all triggers in the comparison view. However, by providing an option to swap pivot and compare roles (arrow button), we allow a consistent exploration flow from one sample to the next. The only exception is the Beam Search Tree, which is only shown for the pivot sample to save visual space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Interacting With Examples</head><p>A major focus of Seq2Seq-Vis is interactive comparison between different sources and targets (T4, G1, G3). We consider two different modes of interactions to produce comparison samples or to modify the pivot: model-focused and language-focused changes. Model-focused interactions let the user (model architect) produce examples that the model believes are similar to the current pivot to test small, reasonable variations for the different model stages. Language-focused interactions enable the user (model trainer) to produce examples focused on the language task and observe model behavior.</p><p>For the model-focused interactions, we utilize a different variant of neighborhoods. To replace a word with a slightly different, but interpretable substitute, we search for neighbors from the model's word vectors. The user can trigger the substitution process by clicking on the word to be replaced. As a result, a word cloud projecting the closest words w.r.t. their vector embedding in a 2D plane is shown. A click on one of the words in the cloud replaces the original and triggers a new translation in comparison mode.</p><p>Another model-focused interaction is to modify the model directly, for instance, by altering attention weights (S3 in model). For this step, the user can switch to attention modification and select a target word for which attention should be modified. By repeatedly clicking on encoder words, she gives more weights to these encoder words. <ref type="figure">Fig. 8</ref> shows how the attention can be modified for an example. After hitting apply attn, the attention is applied for this position, overwriting the original attention distribution.</p><p>For language-focused interactions, the user can specify direct changes to either the source or the target. The user can trigger the changes by using the manual compare button and enter a new source or a new target sentence. When the source is changed, a new full translation is triggered. If the target is changed, a prefix decode is triggered that constrains the search on a predefined path along the words entered and continues regular beam search beyond.</p><p>Alternatively, the user can select the word from the top K predictions <ref type="figure" target="#fig_8">(Fig. 7d</ref>) that seems to be the best next word. By clicking on one of these words, a prefix decode is triggered as described above and shown in <ref type="figure">Fig. 6</ref>. Initiating either of these interactions switches Seq2Seq-Vis into comparison mode. As a core analysis method, comparison allows to derive insights about model mechanics (model-focused probing) or how well the model solves the task (language-focused testing).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Design Iterations</head><p>We considered several different variants for both main views of the system. For the translation view, we considered incorporating more state information directly into the encoding. <ref type="figure">Fig. 9</ref> shows iterations for adding per-word information around encoder and decoder. Similar to LSTMVis <ref type="bibr" target="#b48">[48]</ref>, the hidden state line charts show progression along encoder and decoder hidden states <ref type="figure">(Fig. 9a)</ref>. Domain scientists rejected this as too noisy for the given domain goals. In a later iteration the visualization experts proposed to indicate the closeness of the nearest neighbors with a simple histogram-like encoding <ref type="figure">(Fig. 9b)</ref>. This information did not help to formulate hypotheses. In addition, it did not reveal a lot of variance (see abundance of similar small gray boxes). The next design focused on incorporating language features rather than latent vectors. It showed for each time step of the decoder the top K predicted words being produced as if there was only the top beam evaluated until then. Finally, we decided to use the stronger visual variable length to encode the probability values <ref type="figure" target="#fig_8">(Fig. 7d)</ref>.</p><p>In the neighborhood view, the trajectory pictograms are a result of a series of visual iterations around combining the linear nature of sequences with keeping some spatial information describing vector proximity. We divide the state trajectory view into cutout regions forming a regular grid. Using a regular grid limits the variants of basic pictograms to a small and recognizable number. Alternative ideas were to center the cutout area around each state or to use the bounding box of the sequence state and all its neighbors as area. Both alternatives created highly variant multiples that introduced visual noise. For the regular grid, choosing the right grid size is important and the current static solution of applying a 3x3 grid will be replaced by a non-linear function of number of displayed words to allow for scalability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">IMPLEMENTATION</head><p>Seq2Seq-Vis allows for querying and interaction with a live system. To facilitate this, it uses tight integration of a seq2seq model with the visual client. We based the interface between both parts on a REST API, and we used OpenNMT <ref type="bibr" target="#b18">[18]</ref> for the underlying model framework. We extended the core OpenNMT-py distribution to allow easy access to latent vectors, the search beams, and the attention values. Furthermore, we added non-trivial model-diagnostic modifications for translation requests to allow prefix decoding and to apply user-specific attention. We plan to distribute Seq2Seq-Vis as the default visualization mode for OpenNMT.</p><p>To allow fast nearest neighbor searches, Python scripts extract the hidden state and context values from the model for points in a large subset of the training data. These states are saved in HDF5 files and indexed utilizing the Faiss <ref type="bibr" target="#b14">[14]</ref> library to allow fast lookups for closest dot products between vectors. For TSNE and MDS projections we use the SciKit Learn package <ref type="bibr" target="#b40">[40]</ref> for Python.</p><p>The model framework and the index work within a Python Flask server to deliver content via a REST interface to the client. The client is written in Typescript. Most visualization components are using the d3js library. Source code, a demo instance, and a descriptive webpage are available at http://seq2seq-vis.io.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">USE CASES</head><p>We demonstrate the application of Seq2Seq-Vis and how it helps to generate insights using examples from a toy date conversion problem, abstractive summarization, and machine translation (Sect. 3). Date Conversion. Seq2seq models can be difficult to build and debug even for simple problems. A common test case used to check whether a model is implemented correctly is to learn a well-specified deterministic task. Here we consider the use case of converting various date formats to the unified format YEAR-MONTH-DAY. For example, the source March 25, 2000 should be converted to the target 2000-03-25. While this problem is much simpler than language translation, it tests the different components of the system. Specifically, the encoder (S1) must learn to identify different months, the attention (S3) must learn to reorder between the source and the target, and the decoder (S2) must express the source word in a numeric format.</p><p>SEQ2SEQ-VIS provides tools for examining these different stages of the model. <ref type="figure" target="#fig_1">Figure 10</ref> shows an example, where the user, following Goal 3, employs a comparison between two different translations, one starting with March and the other with May. These two translations are nearly identical, except one yields the month 3 and the other 5. Following Goal 1, the user might want to examine the models decisions. The upper translation view provides a way to compare between the attention on the two inputs. The red highlighted connections indicate that the first sentence attention focuses on r c wheres the second focuses on y. These characters are used by the model to distinguish the two months since it cannot use M a. The user can also observe how the encoder learns to use these letters. The trajectory view compares the encoder states of sentence 1 and sentence 2. Here we use a custom projection, where the y-axis is the relative position of a word in a sentence and the x-axis is a 1-d projection of the vector. This reveals that the two trajectories are similar before and after these characters, but diverge significantly around r and c. Finally, following Goal 2, the user  <ref type="table">2  0  0  0  -0  3  -2  1   2  0  0  0  -0  3  -2  1   3  1  1  1  &lt;/s&gt;  1  6  &lt;/s&gt;  1  5   0  9  2  2  0  3  1  8  3  2   1  -3  9  4  5  2  4  0  6   &lt;/s&gt; &lt;s&gt;  5  5  8  2  8  6  6  9   pivot   change:   word attn   compare:   sentence   swap:   M  a  y  _  2  1  ,  _  2  0  0   2  0  0  0  -0  5  -2  1   2  0  0  0  -0  5  -2  1   3  1  1  1  0   6  &lt;/s&gt;  1  5   0  9  2  2  &lt;/s&gt;  5  7  8  3  2   1  -3  9  4  7  1  7  0  6   &lt;/s&gt; &lt;s&gt;  5  5  8  6  9  4  6</ref>   <ref type="table">M  a  r  c  h  _  2  1  ,  _  2  0  0  0   2  0  0  0  -0  3  -2  1   2  0  0  0  -0  3  -2  1   3  1  1  1  &lt;/s&gt;  1  6  &lt;/s&gt;  1  5   0  9  2  2  0  3  1  8  3  2   1  -3  9   5  2  4  0  6   &lt;/s&gt; &lt;s&gt;  5  5  8  2  8  6   9   pivot   change:   word attn   compare:   sentence   swap:   M  a  y  _  2  1  ,  _  2  0  0  0   2  0  0  0  -0  5  -2  1   2  0  0  0  -0  5  -2  1   3  1  1  1  0  1  6  &lt;/s&gt;  1  5   0  9  2  2  &lt;/s&gt;  5  7  8  3  2   1  -3  9  4  7  1  7  0  6   &lt;/s&gt; &lt;s&gt;  5  5  8  6  9  4  6</ref>  can connect these decisions back to the training data. On the right, she can see the nearest neighbors around the letter a in M a y (highlighted in red). Interestingly, the set of nearest neighbors is almost equally split between examples of M a y and M a r c h, indicating that at this stage of decoding the model is preserving its uncertainty between the two months.</p><p>Abstractive Summarization. For our second use case we apply the tool to a summarization problem. Recently, researchers have developed methods for abstractive text summarization that learn how to produce a shorter summarized version of a text passage. Seq2seq models are commonly used in this framework <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b45">45]</ref>. In abstractive summarization, the target passage may not contain the same phrasing as the original. Instead, the model learns to paraphrase and alter the wording in the process of summarization. Studying how paraphrasing happens in seq2seq systems is a core research question in this area. Rush et al. <ref type="bibr" target="#b44">[44]</ref> describe a system using the Gigaword data set (3.8M sentences). They study the example source sentence russian defense minister ivanov called sunday for the creation of a joint front for combating global terrorism to produce a summary russia calls for joint front against terrorism. Here russia compresses the phrase russian defense minister ivanov and against paraphrases for combating.</p><p>To replicate this use case we consider a user analyzing this sentence. In particular, he is interested in understanding how the model selects the length and the level of abstraction. He can analyze this in the context of Goal 3, testing alternatives predictions of the model, in particular targeting Stage 4. As discussed in Sect 5, SEQ2SEQ-VIS shows the top K predictions at each time step. When the user clicks on a prediction, the system will produce a sentence that incorporates this prediction. Each choice is "locked" so that further alterations can be made. <ref type="figure" target="#fig_1">Fig. 11</ref> shows the source input to this model. We can see four different summarizations that the model produces based on different word choices. Interestingly, specific local choices do have a significant impact on length, ranging from five to thirteen words. Switching from for to on leads the decoder to insert an additional phrase on world leaders to maintain grammaticality. While the model outputs the top choice, all other choices have relatively high probabilities. This observation has motivated research into adding constraints to the prediction at each time step. Consequently, we have added methods for constraining length and prediction into the underlying seq2seq system to produce different outputs. Machine Translation. Finally, we consider a more in-depth use case of a real-world machine translation system using a complete model trained on WMT '14 (3.96M examples) to translate from German to English. This use case considers a holistic view of how an expert might go about understanding the decisions of the system. <ref type="figure" target="#fig_1">Figure 12</ref> shows an example source input and its translation. Here the user has input a source sentence, translated it, and activated the neighbor view to consider the decoder states. She is interested in better understanding each stage of the model at this point. This sentence is interesting as there is significant reordering that must occur to translate from the original German to English. For instance, the subject he is at the beginning of the clause, but must interact with the verb gesprochen at the end of the German sentence.</p><p>We consider Goals 1 and 2 applied to this example, with the intent of analyzing the encoder, decoder, attention, and prediction (S1-S4). First we look at the attention. Normally, this stage focuses on the word it is translating (er), but researchers have noted that neural models often look ahead to the next word in this process <ref type="bibr" target="#b19">[19]</ref>. We can see branches going from he to potential next steps (e.g., von or gesprochen). We can further view this process in the decoder trajectory shown below, where he and spoke are placed near each other in the path. Hovering over the vector he highlights it globally in the tool. Furthermore, if we click on he, we can link this state to other examples in our data (Goal 2). On the right we can see these related examples, with the next word (+1) highlighted. We find that the decoder is representing not just the information for the current word, but also anticipating the translation of the verb sprechen in various forms.</p><p>In this case we are seeing the model behaving correctly to produce Sie sprach von einer vernich@@ tenden Anklage im Zusammenhang mit den Sicherheits@@ über@@ prü@@ fungen .</p><p>&lt;s&gt; She spoke of the dam@@ ning indic@@ tment on safety checks . &lt;/s&gt; Herr Kre@@ iss@@ l @-@ Dör@@ f@@ ler beispielsweise hat in Englisch von long term financing for long term projects gesprochen . Das ist eine verständliche Forderung .</p><p>&lt;s&gt; For example , Mr Kre@@ iss@@ l @-@ Dör@@ f@@ ler spoke in English about &amp;apos; long @-@ term capital for long @-@ term projects &amp;apos; -this is an obvious issue . &lt;/s&gt; Der Abgeordnete sprach auch von der modernen Technologie . Herr Präsident , der Kommissionspräsident war nich letzte Woche über die Regierungskonferenz sprach .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt;s&gt; Mr President , the President of the Commission w honest when he talked last week about the Intergover &lt;/s&gt;</head><p>Vergan@@ gene Woche sprach ich auf der äußerst in der Mitglieder der Parlamentarischen Versammlung de Abgeordnete dieses Parlaments teil@@ nahmen .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt;s&gt; I spoke last week at the extremely interesting s organised between members of the NATO parliamentar Members of this Parliament . &lt;/s&gt;</head><p>Sie sprach von einer vernich@@ tenden Anklage im Sicherheits@@ über@@ prü@@ fung en .</p><p>&lt;s&gt; She spoke of the dam@@ ning indic@@ tmen Sie sprach von einer skandal@@ ösen Miß@@ achtu  <ref type="figure" target="#fig_1">Fig. 12</ref>. Use case language translation using WMT'14 data. The attention graph (top) shows how attention for the target word he is not only focused on the decoder counterpart er but also on the following words, even to the far away verb gesprochen (spoke). The state trajectory (bottom left) for the decoder states reveals how close he and spoke are. The neighborhood list indicates that the model sets the stage for predicting spoke as next word. a good translation. However, the tool can also be useful when there are issues with the system. One common issue in under-trained or under-parameterized seq2seq models is to repeatedly generate the same phrase. <ref type="figure" target="#fig_1">Figure 13</ref> shows an example of this happening. The model repeats the phrase in Stuttgart in Stuttgart. We can easily see in the pictogram view that the decoder model has produced a loop, ending up in nearly the same position even after seeing the next word. As a short-term fix, the tool's prefix decoding can get around this issue. It remains an interesting research question to prevent this type of cycle from occurring in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">RELATED WORK</head><p>Various methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b34">34]</ref> have been proposed to generate explanations for deep learning model predictions. Understanding them still remains a difficult task. To better address the specific issues of our users, we narrow the target audience for our proposed tool. Following the classifications by Strobelt et al. <ref type="bibr" target="#b48">[48]</ref> and Hohman et al. <ref type="bibr" target="#b13">[13]</ref>, our tool aims at model developers who have at least a conceptual understanding of how the model works. This is opposed to end users, who are agnostic to the technique used to arrive at a specific result. Following Hohman et al., analysis itself can broadly be divided into global model analysis and instance-based analysis. In global model analysis, the most commonly seen methods are visualizations of the internal structure of trained deep learning models. Instance-based analysis may be coupled with interactive experimentation with the goal of understanding a particular prediction using the local information around only one input <ref type="bibr" target="#b38">[38]</ref>.</p><p>Global Model Analysis Most recent work focuses on visualizing hidden representations of convolutional neural networks (CNNs) <ref type="bibr" target="#b24">[24]</ref> for computer vision applications. Techniques for visualizing CNNs include showing neural activity in the convolutional layers as overlay over the image <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b23">23]</ref> and directly showing the images that maximize the activity <ref type="bibr" target="#b46">[46]</ref>. Zeiler and Fergus <ref type="bibr" target="#b54">[54]</ref> use deconvolutional networks to explore the layers of a CNN. This approach is widely used to generate explanations of models, for example, by Yosinski et al. <ref type="bibr" target="#b53">[53]</ref>. <ref type="figure" target="#fig_1">Fig. 13</ref>. An under-trained English-German model. Repetition repetition is a commonly observed phenomenon in under-trained or under-parametrized models. Here the trajectory pictograms show that for the repetition in Stuttgart in Stuttgart the decoder states alternate in the same region before being able to break apart.</p><p>A similar line of work has focused on visualizing recurrent neural networks (RNNs) and other sequence models. Preliminary work by Karpathy et al. <ref type="bibr" target="#b17">[17]</ref> uses static visualizations to understand hidden states in language models. They demonstrate that selected cells can model clear events such as open parentheses and the start of URLs. Strobelt et al. <ref type="bibr" target="#b48">[48]</ref> introduce LSTMVis, an interactive tool that allows users to understand activation patterns of combinations of hidden states. LSTMVis shows the neighborhood of activations within the training data as an approach of making sense of the complex interactions in a context.</p><p>Similar to our approach, Kahng et al. <ref type="bibr" target="#b16">[16]</ref> propose using the model structure as the entry point into the analysis. In their approach, they try to understand connections between misclassified examples and hidden states of parts of the network by showing activation pattern differences between correct and false examples from the training data. Ming et al. <ref type="bibr" target="#b32">[32]</ref> propose RNNVis, a tool that uses word clouds instead of full contexts or sentences to show typical words that appear for activation patterns. Our approach to show embeddings of a whole phrase is similar to that of Johnson et al. <ref type="bibr" target="#b15">[15]</ref>. They use three-dimensional tSNE in order to visualize progressions of context vectors. Novel in our approach are different types of progressions as well as the connection and embedding with neighborhoods.</p><p>An alternative to visualizing what a model has learned is visualizing how it is learning. RNNbow by Cashman et al. <ref type="bibr" target="#b4">[5]</ref> shows the gradient flow during backpropagation training in RNNs to visualize how the network is learning.</p><p>Instance-Based Analysis Instance-based analysis is commonly used to understand local decision boundaries and relevant features for a particular input. For example, Olah et al. <ref type="bibr" target="#b37">[37]</ref> extend methods that compute activations for image classification to build an interactive system that assesses specific images. They show that not only the learned filters of a CNN matter, but also their magnitudes. The same type of analysis can be used to answer counter-factual "what if" questions to understand the robustness of a model to pertubations. Nguyen et al. <ref type="bibr" target="#b35">[35]</ref> show that small perturbations to inputs of an image classifier can drastically change the output. Interactive visualization tools such as Picasso <ref type="bibr" target="#b11">[11]</ref> can manipulate and occlude parts of an image as input to an image classifier. Krause et al. <ref type="bibr" target="#b21">[21]</ref> use partial dependence diagnostics to explain how features affect the global predictions, while users can interactively tweak feature values and see how the prediction responds to instances of interest.</p><p>There is an intrinsic difficulty in perturbing inputs of models that operate on text. While adding noise to an image can be achieved by manipulating the continuous pixel values, noise for categorical text is less well defined. However, there is a rich literature for methods that compute relevant inputs for specific predictions, for example by computing local decision boundaries or using gradient-based saliency <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b55">55]</ref>. Most of these methods focus on classification problems in which only one output exists. Ruckle et al. <ref type="bibr" target="#b43">[43]</ref> address this issue and extend saliency methods to work with multiple outputs in a question-answering system. As an alternative to saliency-methods, Ding et al. <ref type="bibr" target="#b5">[6]</ref> use a layer-wise relevance propagation technique <ref type="bibr" target="#b1">[2]</ref> to understand relevance of input with regard to an output in sequenceto-sentence models. Yet another approach to understand predictions within text-based models is to find the minimum input that still yields the same prediction <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b28">28]</ref>. None of the previous methods use our approach of using nearest neighbors of word embeddings to compare small perturbations of RNNs.</p><p>One commonality among all these approaches is that they treat the model as a black box that generates a prediction. In contrast, we are assuming that our users have an understanding of the different parts of a sequence-to-sequence model. Therefore, we can use more indepth analysis, such as interactive manipulations of input, output, and attention. Our beam search and attention manipulations follow the approach by Lee et al. <ref type="bibr" target="#b25">[25]</ref> who show a basic prototype to manipulate these parts of a model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSIONS AND FUTURE WORK</head><p>Seq2Seq-Vis is a tool to facilitate deep exploration of all stages of a seq2seq model. We apply our set of goals to deep learning models that are traditionally difficult to interpret. To our knowledge, our tool is the first of its kind to combine insights about model mechanics (translation view) with insights about model semantics (neighborhood view), while allowing for "what if"-style counterfactual changes of the model's internals.</p><p>Being an open source project, we see future work in evaluating the longitudinal feedback from real-world users for suggested improvements. Two months after release, we already observed some initial quantitative and qualitative feedback. Currently, more then 5,500 page views have been recorded and 156 users liked (starred) the project on Github. The most requested new feature is integration of the tool with other ML frameworks.</p><p>There are many avenues for future work on the algorithmic and visualization side. Improving the projection techniques to better respect the linear order of sequences would be helpful. The tool could be extended to different sequence types, including audio, images, and video. Supporting these different data types requires non-trivial expansion of visual encoding for input and output. A prerequisite for future work targeting different models and frameworks is that model architects implement open models with hooks for observation and modification of model internals. We hope that SEQ2SEQ-VIS will inspire novel visual and algorithmic methods to fix models without retraining them entirely.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>werkzeuge helfen es etwas auf die anhand für geben und um von &lt;unk&gt; analyse zu lernen</head><label></label><figDesc>world , satellites and warning systems are saving lives in &lt;unk&gt; areas such as bangladesh . these are the two pictures taken of garment factories in &lt;unk&gt; province and garment factories in india . i would love to talk about my astronomy , but i suspect that the number of people who are interested in &lt;unk&gt; transfer in &lt;unk&gt; atmospheres and polarization of light in jupiter &amp;apos;s upper atmosphere are the number of people who &amp;apos;d fit in a bus shelter . if a neutrino had a brain , which it evolved in &lt;unk&gt; ancestors , it would say that rocks really do consist of empty space . i would love to talk about my astronomy , but i suspect that the number of people who are interested in &lt;unk&gt; transfer in &lt;unk&gt; atmospheres and polarization of light in jupiter &amp;apos;s upper atmosphere are the number of people who &amp;apos;d fit in a bus shelter . most of those individuals had spent most of their lives in &lt;unk&gt; hospitals . this is a long time ago . that &amp;apos;s year by year . this comes from our friends at &lt;unk&gt; i &amp;apos;ll get an esl class in &lt;unk&gt; learning &amp;quot; it &amp;apos;s raining , it &amp;apos;s pouring . &amp;quot; you send one blessed email to whomever you &amp;apos;re thinking of at &lt;unk&gt; she &lt;unk&gt; for jobs down in &lt;unk&gt; province in the south . science columnist lee &lt;unk&gt; describes a remarkable project at &lt;unk&gt; divide , antarctica , where a hardy team are drilling into &lt;unk&gt; ice to extract vital data on our changing climate . that technology will be used on &lt;unk&gt; animals . i work in &lt;unk&gt; homes , largely .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>our tool helps to find errors in seq2seq models using visual analysis methods . unser werkzeug hil , fehler in &lt;unk&gt; modellen zu finden mittels visueller analysen . unser werkzeug hil , fehler in &lt;unk&gt; modellen zu finden , visueller analysen . unsere instrument dabei dabei fehlern zu der modelle anzuwenden entdecken mit der &lt;unk&gt; von das tool hilfreich dazu abwei chungen bei den &lt;unk&gt; einzusetzen suchen die visuellen auswertung , unserem hilfsmittel ist zu &lt;unk&gt; für form , mit verschaffen mittels des analyse der wir werkzeuge helfen es etwas auf die anhand für geben und &lt;unk&gt; darstellungen desFig. 1 .</head><label>1</label><figDesc>Example of Seq2Seq-Vis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>•</head><label></label><figDesc>(*) indicates equal contribution • H. Strobelt, A. Perer are with IBM Research and MIT-IBM Watson AI Lab. • S. Gehrmann, A. Rush are with the Harvard NLP group. • M. Behrisch and H. Pfister are with the Harvard Visual Computing group. Manuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication xx xxx. 201x; date of current version xx xxx. 201x. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org. Digital Object Identifier: xx.xxxx/TVCG.201x.xxxxxxx</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Fig. 2. Five stages in translating a source to target sequence: (S1) encoding the source sequence into latent vectors, (S2) decoding to generate target latent vectors, (S3) attend between encoder and decoder, (S4) predict word probabilities at each time step, and (S5) search for a best complete translation (beam search).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>die längsten reisen fangen an , wenn es auf den straßen dunkel wird . the longest travel begins when it gets to the streets . the longest travel when when it &amp;apos;s to the streets . and oldest trips will if they gets dark a roads in so tallest journeys begins , the becomes buried shore road of well russians travels begin as there grows into heaven street , you icons journey start in this comes inFig. 3 .</head><label>3</label><figDesc>/localhost:8080/client/index.html?in=die%20l%C3%A4ngsten%20reisen%20fangen%20an%20,%20wenn%20es%20auf%20den%20stra%C3%9Fen%20dunkel%20wird%20. Start entering some encoder sentence (enter triggers request)... die längsten reisen fangen an , wenn es auf den straßen dunkel wird . ein dunkles etwas vor einem dunklen himmel . also gehen sie tief in die minen , um eine stille der umwelt zu finden , die es sie hören lässt , wenn ein dunkles &lt;unk&gt; ihren detektor tri . aber auch , wenn das schwarze loch von außen dunkel ist , ist es in seinem inneren nicht dunkel , denn alles licht der galaxis könnte hinter uns einfallen . das gebetbuch ist dunkel auf beiden bildern und kommt dunkel heraus . &amp;#91; &amp;quot; gelb &amp;quot; &amp;#93; db : rot . publikum : gelb . &amp;#91; &amp;quot; blau &amp;quot; &amp;#93; db : gelb . außerdem verursacht das &lt;unk&gt; im wagen das , was wir eine &lt;unk&gt; nennen , wodurch es dunkler wird . &amp;#91; &amp;quot; pferd &amp;quot; &amp;#93; db : gelb . publikum : gelb . unser bewusstsein über diese sache wird extrem hell und lebha , und alles andere wird wie dunkel . das ist mcmurdo selbst . ungefähr 1.000 menschen arbeiten im &lt;unk&gt; hier , und ca. 200 im winter , wenn es sechs monate lang völlig dunkel ist . wenn wir also die form dieser &lt;unk&gt; wüssten , sollten wir imstande sein , diese merkmale zu berechnen , die menge dunkler materie zu berechnen . aber es gab zeugen ; überlebende im dunkel . ich lebe kreise von licht und dunkel . das gebetbuch ist dunkel auf beiden bildern und kommt dunkel heraus . ein dunkles etwas vor einem dunklen himmel . also gehen sie tief in die minen , um eine stille der umwelt zu finden , die es sie hören lässt , wenn ein dunkles &lt;unk&gt; ihren detektor tri . aber auch , wenn das schwarze loch von außen dunkel ist , ist es in seinem inneren nicht dunkel , denn alles licht der galaxis könnte hinter uns einfallen . das gebetbuch ist dunkel auf beiden bildern und kommt dunkel heraus . &amp;#91; &amp;quot; gelb &amp;quot; &amp;#93; db : rot . publikum : gelb . &amp;#91; &amp;quot; blau &amp;quot; &amp;#93; db : gelb . außerdem verursacht das &lt;unk&gt; im wagen das , was wir eine &lt;unk&gt; nennen , wodurch es dunkler wird . &amp;#91; &amp;quot; pferd &amp;quot; &amp;#93; db : gelb . publikum : gelb . unser bewusstsein über diese sache wird extrem hell und lebha , und alles andere wird wie dunkel . das ist mcmurdo selbst . ungefähr 1.000 menschen arbeiten im &lt;unk&gt; hier , und ca. 200 im winter , wenn es sechs monate lang völlig dunkel ist . wenn wir also die form dieser &lt;unk&gt; wüssten , sollten wir imstande sein , diese merkmale zu berechnen , die menge dunkler materie zu berechnen . aber es gab zeugen ; überlebende im dunkel . ich lebe kreise von licht und dunkel .das gebetbuch ist dunkel auf beiden bildern und kommt dunkel heraus . Hypothesis: Encoder (S1) Error -nearest neighbors of encoder state for dunkel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>http://localhost:8080/client/index.html?in=die%20l%C3%A4ngsten%20reisen%20fangen%20an%20,%20wenn%20es%20auf%20den%20stra%C3%9Fen%20dunkel%20wird%20.show: edges nodes gets streets show: src tgt highlight: -1 0 +1 &lt;s&gt; this means we all benefit when another country gets rich . &lt;/s&gt; &lt;s&gt; the prayer book is dark in both images and it comes out dark . &lt;/s&gt; &lt;s&gt; now black holes are dark against a dark sky . &lt;/s&gt; &lt;s&gt; furthermore , the roof of the car is causing what we call a shadow cloud inside the car which is making it darker . &lt;/s&gt; &lt;s&gt; the prayer book is dark in both images and it comes out dark . &lt;/s&gt; &lt;s&gt; i live cycles of light and darkness . &lt;/s&gt; &lt;s&gt; this is mcmurdo itself . about a thousand people work here in summer , and about 200 in winter when it &amp;apos;s completely dark for six months . &lt;/s&gt; &lt;s&gt; this is a tumor : dark , gray , ominous mass growing inside a brain . &lt;/s&gt; &lt;s&gt; but even though the black hole is dark from the outside , it &amp;apos;s not dark on the inside , because all of the light from the galaxy can fall in behind us . &lt;/s&gt; &lt;s&gt; &lt;unk&gt; adapted with layers of fat . sea lions got sleek . &lt;/s&gt; &lt;s&gt; the archimedes text is dark in one image and bright in another . &lt;/s&gt; &lt;s&gt; then things get tense . &lt;/s&gt; &lt;s&gt; under those conditions , the foxo protein in blue has gone into the nucleus --that little compartment there in the middle of the cell --and it &amp;apos;s sitting down on a gene binding to it . &lt;/s&gt;&lt;s&gt; and it was very peculiar , because it was dark out , but she was Hypothesis: Decoder (S2) Error -nearest neighbors of decoder state for gets and streets, which are close in projection space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>://localhost:8080/client/index.html?in=die%20l%C3%A4ngsten%20reisen%20fangen%20an%20,%20wenn%20es%20auf%20den%20stra%C3%9Fen%20dunkel%20wird%20.Start entering some encoder sentence (enter triggers request)... die längsten reisen fangen an , wenn es auf den straßen dunkel wird . tallest journeys begins , the becomes buried shore road of well russians travels begin as there grows into heaven street , Hypotheses: Attention (S3), Prediction (S4), or Beam Search (S5) Error -encoder words and decoder words (E/D), Attention (S3), top k predictions for each time step in decoder (S4), and beam search tree (S5)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>3/ 27</head><label>27</label><figDesc>/2018 S2S Attention http://localhost:8080/client/index.html?in=die%20l%C3%A4ngsten%20reisen%20fangen%20an%20,%20wenn%20es%20auf%20den%20stra%C3%9Fen%20dunkel%20wird%20. Start entering some encoder sentence (enter triggers request)... die längsten reisen fangen an , wenn es auf den straßen dunkel wird . Enc words: Attention: topK: die längsten reisen fangen an , wenn es auf den straßen dunkel wird . the longest travel begins when it gets dark in the streets . the longest travel when when it &amp;apos;s to in the streets . and oldest trips will if they gets dark on roads roads of so tallest journeys begins , the becomes buried . streets street ? well russians travels begin as there grows into to tgt highlight: -1 0 +1 schwarze löcher sind ein dunkles etwas vor einem dunklen himmel . also gehen sie tief in die minen , um eine stille der umwelt zu finden , die es sie hören lässt , wenn ein dunkles &lt;unk&gt; ihren detektor tri . aber auch , wenn das schwarze loch von außen dunkel ist , ist es in seinem inneren nicht dunkel , denn alles licht der galaxis könnte hinter uns einfallen . das gebetbuch ist dunkel auf beiden bildern und kommt dunkel heraus . &amp;#91; &amp;quot; gelb &amp;quot; &amp;#93; db : rot . publikum : gelb . &amp;#91; &amp;quot; blau &amp;quot; &amp;#93; db : gelb . außerdem verursacht das &lt;unk&gt; im wagen das , was wir eine &lt;unk&gt; nennen , wodurch es dunkler wird . &amp;#91; &amp;quot; pferd &amp;quot; &amp;#93; db : gelb . publikum : gelb . unser bewusstsein über diese sache wird extrem hell und lebha , und alles andere wird wie dunkel . das ist mcmurdo selbst . ungefähr 1.000 menschen arbeiten im &lt;unk&gt; hier , und ca. 200 im winter , wenn es sechs monate lang völlig dunkel ist . wenn wir also die form dieser &lt;unk&gt; wüssten , sollten wir imstande sein , diese merkmale zu berechnen , die menge dunkler materie zu berechnen . aber es gab zeugen ; überlebende im dunkel . ich lebe kreise von licht und dunkel . das gebetbuch ist dunkel auf beiden bildern und kommt dunkel heraus . das war sehr merkwürdig , denn draussen war es dunkel , aber hinter ihr war fluoreszierendes licht und sie benahm sich sehr wie auf einer bühne , und ich konnte nicht erkennen , warum sie es tat . als erstes muss man beachten , dass es gegenden auf dieser welt gibt , die wegen mangelnder aufmerksamkeit im dunkeln stehen . und so haben wir entdeckt , dass es eine unendliche &lt;unk&gt; an gehäkelten hyperbolischen wesen gibt . es gibt eine gruppe in deutschland die beginnen augen zu konstruieren damit blinde hell und dunkel sehen können .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>really get fun enjoyed with to and &amp;apos;ve that really some funny here &amp;aposwir wollen heute morgen mal richtig spass haben . we want to have really fun this morning . we want to have really fun this morning . so &amp;apos;re really be fun enjoy that tomorrow , and &amp;apos;d a get a enjoyed in day in now really that really some funny here next with this &amp;apos;ve some do quite enjoyable with evening toFig. 7 .</head><label>7</label><figDesc>ich möchte ihnen heute morgen ein paar geschichten erzählen und über ein anderes afrika sprechen . &lt;s&gt; what i want to do this morning is share with you a couple of stories and talk about a different africa . &lt;/s&gt; ich möchte heute morgen ein wenig darüber sprechen , was passiert , wenn wir uns von design in richtung eines design-thinking bewegen . &lt;s&gt; i &amp;apos;d like to talk a little bit this morning about what happens if we move from design to design thinking . &lt;/s&gt; über diese beiden aspekte werde ich heute morgen etwas berichten . &lt;s&gt; and i &amp;apos;m going to say a few words about each one this morning . &lt;/s&gt; mein name ist ursus wehrli , und ich möchte ihnen heute morgen gerne von meinem projekt , kunst aufräumen , erzählen . &lt;s&gt; my name is ursus wehrli , and i would like to talk to you this morning about my project , tidying up art . &lt;/s&gt; eine neue theorie ist jetzt , und ihr habt sie bereits heute morgen von dr. insel gehört , dass psychische erkrankungen störungen der neuralen verbindungen sind , die einfluss auf gefühle , laune und &lt;unk&gt; haben . &lt;s&gt; now , an emerging view that you also heard about from dr. insel this morning , is that psychiatric disorders are actually disturbances of neural circuits that mediate emotion , mood and affect . &lt;/s&gt; alle 30 sekunden stirbt irgendwo auf der welt ein kind an malaria und paul levy sprach heute morgen über die metapher von der &lt;unk&gt; , die in den vereinigten staaten abstürzt . left: State Trajecories (h) right: Neighbor List Overview of Seq2Seq-Vis: The two main views (a) Translation View and (b) Neighborhood View facilitate different modes of analysis. Translation View provides (c) visualizations for attention, (d) the top k word predictions for each time step, and (e) the beam search tree. The Neighborhood View goes deeper into what the model has learned by providing (f,g) a projection of state trajectories and (h) a list of nearest neighbors for a specific model state.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>1 1 1 Fig. 8 .Fig. 9 .</head><label>11189</label><figDesc>/localhost:8080/client/index.html?in=N%20o%20v%20e%20m%20b%20e%20r%20_%200%209%20,%20_%201%209%208%209 1/Start entering some encoder sentence (enter triggers request)... /localhost:8080/client/index.html?in=N%20o%20v%20e%20m%20b%20e%20r%20_%200%209%20,%20_%201%209%208%209 1/Start entering some encoder sentence (enter triggers request)... /localhost:8080/client/index.html?in=N%20o%20v%20e%20m%20b%20e%20r%20_%200%209%20,%20_%201%209%208%209 1/Start entering some encoder sentence (enter triggers request)... To re-direct attention in Seq2Seq-Vis, the user first observes a split of attention between the input 8 and 9 for converting the last digits of a year in a date conversion model. She can (a) select attention mode, (b) select the decoder word, (c) click on the preferred encoder word, (d) apply the attention change, and (e) see the models reaction. Design variants for additional token information: (a) progression of hidden states, (b) density of neighborhoods, or (c) top K predictions as heatmap.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Start entering some encoder sentence (enter triggers request)...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Start entering some encoder sentence (enter triggers request)...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 10 .</head><label>10</label><figDesc>Comparing translations for a date conversion model. The input sequences March 21, 2000 and May 21, 2000 are only different by some letters. The attention (top) for predicting the correct months 3 and 5 is focused on this difference (y vs. rc). The trajectory view (bottom left) shows this difference along the progression of encoder states. The neighborhood list (bottom right) indicates that after input of M a the model is still undecided.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 11 .</head><label>11</label><figDesc>Use case of abstractive summarization. The input sentence russian defense minister ivanov called sunday for the creation of a joint front for combating global terrorism can be summarized in different ways. The yellow boxes indicate alternative translations for different prefix decode settings. Top: the unconstrained abstraction; middle: changing prediction from for to on leads to automatic insertion of on world leaders to stay grammatically correct; bottom left: changing the first word from russian to moscow or russia compresses the sentence even more while retaining its meaning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>http://localhost:8080/client/index.html?in=In%20Berlin%20hat%20er%20von%20der%20Erweiterung%20und%20der%20Verfassung%20zu%20gleicher%20Zeit%20gesprochen%20 der Kommissionspräsident war nicht ganz ehrlich , als er letzte Woche über die Regierungskonferenz sprach . &lt;s&gt; Mr President , the President of the Commission was not being quite honest when he talked last week about the Intergovernmental Conference . &lt;/s&gt; Er sprach nur von griechischen Staats@@ bürgern , also Einwohnern von Griechenland . &lt;s&gt; He simply said &amp;quot; of Greek citizens &amp;quot; , in other words people who are resident in Greece . &lt;/s&gt; Vor wenigen Augen@@ blicken sprach Herr V@@ at@@ anen von Temperaturen , die nicht nur unter 20 Grad minus , sondern unter 40 Grad minus liegen . &lt;s&gt; A moment ago , Mr V@@ at@@ anen spoke to us of lower temperatures , not of 20 degrees below zero , but of 40 degrees below zero . &lt;/s&gt; Er hat von Demokratie , Rechtsstaatlichkeit und Minderheiten@@ schutz gesprochen als drei Grund@@ elementen der Erwartungen , die die Europäische Union , die Kommission , Rat und auch Parlament an die Türkei haben und formulieren . &lt;s&gt; He has spoken of democracy , the rule of law and the protection of minorities as three basic elements in the expectations which the European Union , the Commission , the Council and the European Parliament have of Turkey and which they have formulated . &lt;/s&gt; Die Kommissarin sprach von der Bedeutung nationaler Regionen und der Staaten in der Kommunikations@@ politik . &lt;s&gt; The Commissioner has mentioned the importance of national regions , as well as the states , in its communication policy . &lt;/s&gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>&lt;s&gt;</head><label></label><figDesc>She spoke of the scandal of abuse of safety . &lt;/ Nach außen hin scheinen diese beiden V orschläge b änderungen einzuführen , um die Freizügigkeit in Europ jüngsten Urteile , von denen Frau Ber@@ ger in ihren ei Bemerkungen gesprochen hat , umzusetzen . &lt;s&gt; On the face of it these two proposals appear to b procedural changes to facilitate freedom of movement give e ect to the recent court cases Mrs Ber@@ ger ref remarks . &lt;/s&gt; Zweitens erwähnte er die Nach@@ wahlen . &lt;s&gt; Secondly , he also mentioned their by @-@ ele Gestern -einige von Ihnen haben das bereits komm@ Wirtscha s@@ ausschuß von der Verbesserung der wir in Europa . &lt;s&gt; Yesterday , as some of you have noted , the Com Monetary A airs referred to the improvement in the e Europe . &lt;/s&gt; Der Abgeordnete sprach auch von der modernen Tec &lt;s&gt; The honourable Member also mentioned mode Die Kommissarin sprach von der Bedeutung nationa Staaten in der Kommunikations@@ politik .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>&lt;s&gt; The honourable Member also mentioned modern technology . &lt;/s&gt;</head><label></label><figDesc></figDesc><table><row><cell>In</cell><cell>Berlin</cell><cell>,</cell><cell>he</cell><cell>spoke</cell><cell>of</cell><cell>enlargement</cell><cell>and</cell><cell>the</cell><cell cols="2">Constitution</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>same</cell><cell>tim</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>same</cell><cell>tim</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>the</cell><cell>Co</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>same</cell><cell>tim</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>same</cell><cell>tim</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>same</cell><cell>tim</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>time</cell><cell>.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>time</cell><cell>.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>decoder</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>In</cell><cell>Berlin</cell><cell></cell><cell>,</cell><cell>he</cell><cell>spoke</cell><cell></cell><cell>of</cell><cell>enlargement</cell><cell>a</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">show: edges nodes</cell><cell></cell><cell></cell><cell></cell><cell cols="2">show: src</cell><cell>tgt highlight: -1 0 +1</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Enc words: Attention: topK:</p><p>In Berlin hat er von der Erweiterung und der Verfassung zu gleicher Zeit gesprochen .</p><p>In Berlin , he spoke of enlargement and the Constitution at the same time .</p><p>In Berlin , he spoke of enlargement and the Constitution at the same time . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>He</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A causal framework for explaining the predictions of black-box sequence-to-sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Alvarez-Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09-09" />
			<biblScope unit="page" from="412" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Klauschen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">130140</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Explanation and justification in machine learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Biran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI-17 Workshop on Explainable AI (XAI)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rnnbow: Visualizing learning via backpropagation gradients in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cashman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mosca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Visual Analytics for Deep Learning (VADL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Visualizing and understanding neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1150" to="1159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Visualizing higherlayer features of a deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Montreal</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A Convolutional Encoder Model for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<title level="m">Convolutional Sequence to Sequence Learning</title>
		<imprint>
			<date type="published" when="2017-05" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Hassan</forename><surname>Awadalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chowdhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Achieving human parity on automatic chinese to english news translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Menezes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Picasso: A modular framework for visualizing the learning process of neural network image classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Open Research Software</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Facebook translates &apos;good morning&apos; into &apos;attack them&apos;, leading to arrest. The Guardian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hern</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pienta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06889</idno>
		<title level="m">Visual analytics in deep learning: An interrogative survey for the next frontiers</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08734</idno>
		<title level="m">Billion-scale similarity search with gpus</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Google&apos;s multilingual neural machine translation system: enabling zero-shot translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04558</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Activis: Visual exploration of industry-scale deep neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kalro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H P</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Visualizing and understanding recurrent networks. ICLR Workshops</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">OpenNMT: Open-Source Toolkit for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Six challenges for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Knowles</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03872</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Understanding black-box predictions via influence functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04730</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Interacting with predictions: Visual inspection of black-box machine learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2016 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5686" to="5697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Nonmetric multidimensional scaling: a numerical method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="115" to="129" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Building high-level features using large scale unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>0010</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Interactive visualization and manipulation of attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="121" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rationalizing neural predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01" />
			<biblScope unit="page" from="107" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Visualizing and Understanding Neural Models in NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<meeting><address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note>NAACL</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Understanding neural networks through representation erasure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08220</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goodrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10198</idno>
		<title level="m">Generating wikipedia by summarizing long sequences</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Wit3: Web inventory of transcribed and translated talks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Christian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marcello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of European Association for Machine Translation</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="261" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10777</idno>
		<title level="m">Understanding hidden memories of recurrent neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Abstractive text summarization using sequence-to-sequence rnns and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.06023</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">How do humans understand explanations from machine learning systems? an evaluation of the human-interpretability of explanation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00682</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="427" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Attention and augmented recurrent neural networks. Distill</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carter</surname></persName>
		</author>
		<idno type="DOI">10.23915/distill.00001</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The building blocks of interpretability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Satyanarayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mordvintsev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Conceptvector: text visual analytics via interactive lexicon building using word embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Diakopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="361" to="370" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.04304</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Scikitlearn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Why should i trust you?: Explaining the predictions of any classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03717</idno>
		<title level="m">Right for the right reasons: Training differentiable models by constraining their explanations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">End-to-end non-factoid question answering with an interactive visualization of neural attention weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rücklé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2017, System Demonstrations</title>
		<meeting>ACL 2017, System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="19" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00685</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Get to the point: Summarization with pointer-generator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04368</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<title level="m">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Smilkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03788</idno>
		<title level="m">Direct-manipulation visualization of deep networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Lstmvis: A tool for visual analysis of hidden state dynamics in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="667" to="676" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06579</idno>
		<title level="m">Understanding neural networks through deep visualization</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Visualizing and Understanding Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10590-153</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">8689</biblScope>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Visualizing deep neural network decisions: Prediction difference analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Zintgraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
