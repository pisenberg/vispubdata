<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DQNViz: A Visual Analytics Approach to Understand Deep Q-Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junpeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Gou</surname></persName>
							<email>ligou@visa.com</email>
							<affiliation key="aff0">
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Han-Wei</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
							<email>haoyang@visa.com</email>
							<affiliation key="aff0">
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DQNViz: A Visual Analytics Approach to Understand Deep Q-Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Deep Q-Network (DQN), as one type of deep reinforcement learning model, targets to train an intelligent agent that acquires optimal actions while interacting with an environment. The model is well known for its ability to surpass professional human players across many Atari 2600 games. Despite the superhuman performance, in-depth understanding of the model and interpreting the sophisticated behaviors of the DQN agent remain to be challenging tasks, due to the long-time model training process and the large number of experiences dynamically generated by the agent. In this work, we propose DQNViz, a visual analytics system to expose details of the blind training process in four levels, and enable users to dive into the large experience space of the agent for comprehensive analysis. As an initial attempt in visualizing DQN models, our work focuses more on Atari games with a simple action space, most notably the Breakout game. From our visual analytics of the agent's experiences, we extract useful action/reward patterns that help to interpret the model and control the training. Through multiple case studies conducted together with deep learning experts, we demonstrate that DQNViz can effectively help domain experts to understand, diagnose, and potentially improve DQN models.</p><p>Index Terms-Deep Q-Network (DQN), reinforcement learning, model interpretation, visual analytics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recently, a reinforcement learning (RL) agent trained by Google Deep-Mind <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref> was able to play different Atari 2600 games <ref type="bibr" target="#b7">[8]</ref> and achieved superhuman level performance. More surprisingly, the superhuman level performance was achieved by taking only the game screens and game rewards as input, which makes a big step towards artificial general intelligence <ref type="bibr" target="#b13">[14]</ref>. The model that empowers the RL agent with such capabilities is named Deep Q-Network (DQN <ref type="bibr" target="#b32">[33]</ref>), which is a deep convolutional neural network. Taking the Breakout game as an example <ref type="figure" target="#fig_1">(Figure 2</ref>, left), the goal of the agent is to get the maximum reward by firing the ball to hit the bricks, and catching the ball with the paddle to avoid life loss. This is a typical RL problem <ref type="figure" target="#fig_1">(Figure 2</ref>, right), which trains an agent to interact with an environment (the game) and strives to achieve the maximum reward by following certain strategies. Through iterative trainings, the agent becomes more intelligent to interact with the environment to achieve high rewards. Despite the promising results, training DQN models usually requires in-depth know-how knowledge due to the following reasons. First, different from supervised/unsupervised learnings that learn from a predefined set of data instances, reinforcement learnings learn from the agent's experiences, which are dynamically generated over time. This requires dynamic summarizations of the experiences to achieve a good understanding of the training data. Second, interpreting the behavior of a DQN agent is also challenging. For example, when the agent moves the paddle left, what does the agent really see? Is this an intentional move or it is just a random choice? These questions are important to understand the agent, but cannot be directly answered by model statistics captured from conventional approaches. Finally, DQN models take a certain amount of random inputs during training (e.g., randomly moving the paddle in the Breakout game). The random inputs give the agent more flexibilities to explore the unknown part of the environment, but also prevent the agent from fully exploiting its intelligence. Therefore, a proper random rate is crucial to the training.</p><p>In recent years, we have witnessed the success of many visual analytics approaches to understand deep neural networks. These approaches cover supervised (e.g. CNNVis <ref type="bibr" target="#b26">[27]</ref>) and unsupervised (e.g. GAN-Viz <ref type="bibr" target="#b49">[50]</ref>) deep learning models and are able to expose the models with multiple levels of details. However, to date, few visual analytics works have been reported for deep RL models. Additionally, visualization has played an increasingly important role in model diagnosis and improvement. For example, Liu et al. <ref type="bibr" target="#b25">[26]</ref> showed how visualization can help in diagnosing training failures of deep generative models by disclosing how different neurons interact with each other. Through visualization, Bilal et al. <ref type="bibr" target="#b9">[10]</ref> demonstrated the effects of the class hierarchy in convolutional neural networks and they successfully improved the models by considering the hierarchy. We believe similar attempts of diagnosing and improving models are also promising for deep RL models.</p><p>In this work, we propose DQNViz, a visual analytics system to understand, diagnose, and potentially improve DQN models. DQNViz helps domain experts understand the experiences of a DQN agent at four different levels through visualization. The agent's experiences are not only the inputs for next training stages, but also the outputs from previous training stages. Therefore, they decide what the agent will learn next, and also reflect what the agent has learned previously. By thoroughly studying those experiences with domain experts using DQN-Viz, we have identified several typical action and reward patterns, which are very useful in understanding the behavior of the agent, evaluating the quality of the model, and improving the performance of the training. For the challenge of understanding the agent's mind when performing an action, we dive into the structure of DQN and use guided backpropagations <ref type="bibr" target="#b45">[46]</ref> to expose the features that different convolutional filters extracted. To sum up, the contributions of this work include: • We present a visual analytics system, DQNViz, which helps to understand DQN models by revealing the models' details in four levels: overall training level, epoch level, episode level, and segment level. • We propose a visual design for event sequence data generated from DQN models. The design can effectively reveal the movement patterns of an agent, and synchronize multiple types of event sequences. • Through comprehensive studies with domain experts using DQNViz, we propose an improvement in controlling random actions in DQN models, which is directly resulted from our visual analytics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>DQN Model and Model Challenges. Reinforcement learning (RL) aims to generate an autonomous agent interacting with an environment to learn optimal actions through trial-and-error. Researchers have developed three main approaches to address RL problems: value function based approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b51">52]</ref>, policy based approaches <ref type="bibr" target="#b22">[23]</ref>, and actor-critic approaches <ref type="bibr" target="#b23">[24]</ref>. Different approaches have their respective merits and frailties, which have been thoroughly discussed in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. Our work focuses on DQN <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>, a value function based approach, to present an initial effort in probing RL problems with a visual analytics approach. DQN learns a Q-value function <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b51">52]</ref> for a given state-action pair with deep neural networks to handle the large number of input states (e.g., playing Atari games). We explain it with details in Section 3.</p><p>Recently, there are several important extensions of DQN models. Wang et al. <ref type="bibr" target="#b50">[51]</ref> proposed dueling networks to learn a value function for states and an advantage function associated with the states, and combined them to estimate the value function for an action. Double DQN <ref type="bibr" target="#b16">[17]</ref> tackles the over-estimation problem by using double estimators. Another important extension is the prioritized experience replay <ref type="bibr" target="#b40">[41]</ref> that samples important experiences more frequently. Other extensions to reduce variability and instability are also proposed <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>Despite these advances, there are several challenges to understand and improve DQNs. The first one is to understand the long-time training process. A DQN infers the optimal policy by enormous trial-and-error interactions with the environment, and it usually takes days/weeks to train the model <ref type="bibr" target="#b11">[12]</ref>. It is not easy to effectively track and assess the training progress, and have intervention at the early stages. Secondly, it is not clear what strategies an agent learns and how the agent picks them up. The agent generates a huge space of actions and states with strong temporal correlations during the learning. It is helpful yet challenging to extract the dominant action-state patterns and understand how the patterns impact the training. Thirdly, it is a non-trivial task to incorporate domain experts' feedback into the training. For example, if experts observe some good/bad strategies an agent learned, they do not have a tool to directly apply such findings to the training. Finally, similar to other deep learning models, it needs numerous experiments to understand and tune the hyper-parameters of DQNs. One example is to understand the trade-off between explorations and exploitations <ref type="bibr" target="#b12">[13]</ref>, which is controlled by the exploration rate <ref type="bibr" target="#b34">[35]</ref>. In this work, we try to shed some light on these challenges through a visual analytics attempt.</p><p>Deep Neural Networks (DNN) Visualization. The visualization community has witnessed a wide variety of visual analytics works for DNN in recent years <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28]</ref>. According to the taxonomy in machine learning <ref type="bibr" target="#b38">[39]</ref>, those works can generally be categorized into works for supervised and unsupervised DNN. Example visualization works for supervised DNN include: CNNVis <ref type="bibr" target="#b26">[27]</ref>, RNNVis <ref type="bibr" target="#b30">[31]</ref>, LSTMVis <ref type="bibr" target="#b46">[47]</ref>, ActiVis <ref type="bibr" target="#b21">[22]</ref>, Blocks <ref type="bibr" target="#b9">[10]</ref>, DeepEyes <ref type="bibr" target="#b36">[37]</ref>, etc. For unsupervised DNN, DGMTracker <ref type="bibr" target="#b25">[26]</ref> and GANViz <ref type="bibr" target="#b49">[50]</ref> are typical examples.</p><p>One part missing from the above taxonomy <ref type="bibr" target="#b38">[39]</ref> is the reinforcement learning <ref type="bibr" target="#b4">[5]</ref>, and few visual analytics works have been reported for this part. Specifically, for DQN, the visualization is limited to using t-SNE <ref type="bibr" target="#b29">[30]</ref> to lay out activations from the last hidden layer of the model, as presented in the original DQN paper <ref type="bibr" target="#b32">[33]</ref>. Although the result is effective in providing a structured overview of the large amount of input states, it is not interactive and only limited information is presented. The effectiveness of this preliminary visualization has also demonstrated the strong need of a comprehensive visual analytics solution.</p><p>Event Sequence Data Visualization. To explore the action space over time for reinforcement learnings, a time-oriented sequence visualization is of our interest. A large number of event sequence data visualization works can be found from literature <ref type="bibr" target="#b41">[42]</ref>, and here we focus on two categories: flow-based and matrix-based approaches according to <ref type="bibr" target="#b15">[16]</ref>. The flow-based approaches use a time-line metaphor to list a sequence of events and extend them along one dimension (e.g. time). Multiple sequences usually share the same extending dimension and thus can be synchronized accordingly. Example visualization works in this group include: LifeLines <ref type="bibr" target="#b37">[38]</ref>, LifeFlow <ref type="bibr" target="#b52">[53]</ref>, CloudLines <ref type="bibr" target="#b24">[25]</ref>, EventFlow <ref type="bibr" target="#b33">[34]</ref>, DecisionFlow <ref type="bibr" target="#b14">[15]</ref>, etc. The matrix-based approaches, such as MatrixFlow <ref type="bibr" target="#b35">[36]</ref> and MatrixWave <ref type="bibr" target="#b54">[55]</ref>, can effectively aggregate events and present them with compact matrices to avoid visual clutters. The combination of both types of approaches has also been proposed recently, e.g., EgoLines <ref type="bibr" target="#b53">[54]</ref>, EventThread <ref type="bibr" target="#b15">[16]</ref>. We also focus on event sequence data in this paper, and our objective is to visualize multiple types of event sequences and enable users to synchronize and analyze them simultaneously. On the one hand, we use multiple types of statistical charts to quantitatively summarize the event sequences. On the other hand, we propose a new visual design that can qualitatively reflect the behavior patterns of a DQN agent and synchronize different types of event sequences on-demand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BACKGROUND ON DEEP Q-NETWORKS (DQN)</head><p>DQN <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>, as one type of reinforcement learning, aims to train an intelligent agent that can interact with an environment to achieve a desired goal. Taking the Breakout game as an example <ref type="figure" target="#fig_1">(Figure 2</ref>, left), the environment is the game itself, and it responds to any action (e.g. moving left) from an agent (the trained player) by returning the state of the game (e.g. paddle position) and rewards. With the updated state and achieved reward, the agent makes a decision and takes a new action for next step. This iterative interaction between the agent and environment <ref type="figure" target="#fig_1">(Figure 2</ref>, right) continues until the environment returns a terminal state (i.e. game-over), and the process generates a sequence of states, actions, and rewards, denoted as:</p><formula xml:id="formula_0">s 0 , a 0 , r 1 , s 1 , a 1 , r 2 , ..., r n , s n .</formula><p>The desired goal is maximizing the total reward achieved by the agent.</p><p>How to maximize the total reward? The total reward for one game episode (i.e., from game-start to game-over) is:</p><formula xml:id="formula_1">R=r 1 + r 2 + ... + r n .</formula><p>Suppose we are at time t, to achieve the maximum total reward, the agent needs to carefully choose actions onwards to maximize its future rewards: R t =r t + r t+1 + ... + r n . To accommodate the uncertainty introduced by the stochastic environment, a discount factor, γ ∈ [0, 1], is usually used to penalize future rewards. Therefore, R t =r t + γr t+1 + γ 2 r t+2 ... + γ n−t r n =r t + γR t+1 , i.e., the maximum reward from time t onwards equals the reward achieved at t plus the maximum discounted future reward. Q-learning <ref type="bibr" target="#b51">[52]</ref> defines the maximum future reward as a function of the current state and the action taken in the state, i.e., Q(s, a)=r+γmax a Q(s , a ), and s /a is the state/action after s/a. <ref type="formula">1</ref>This equation is well known as the Bellman equation <ref type="bibr" target="#b8">[9]</ref>. The problem of maximizing the total reward is to solve this equation now, which can be conducted through traditional dynamic programming algorithms. Why DQN is needed? One problem in solving the Bellman equation is the algorithm complexity, especially when the number of states is large. In Breakout, the states should reflect the position, direction, speed of the ball and the paddle, the remaining bricks, etc. To capture such information, RL experts use four consecutive game screens as one state, which contains both static (e.g. paddle position) and dynamic (e.g. ball trajectory) information. As a result, each state has 84×84×4 dimensions (each screen is a gray scale image of resolution 84×84), and the total number of states is 256 84×84×4 . Solving the Bellman equation with input in this scale is intractable. DQN, which approximates Q(s, a) through a deep neural network, emerges to be a promising solution.</p><p>How does DQN work? The core component of DQN is a Q-network that takes screen states as input and outputs the q-value (expected reward) for individual actions. One popular implementation of the Qnetwork is using a deep convolutional neural network (explained later in <ref type="figure">Figure 6</ref>, left), which shows strong capabilities for image inputs <ref type="bibr" target="#b43">[44]</ref>. The DQN framework consists of 4 major stages <ref type="figure" target="#fig_3">(Figure 3</ref> • The Act stage is handled by the environment (an Atari game emulator, we used ALE <ref type="bibr" target="#b7">[8]</ref> in this work). It takes the predicted action as input and outputs the next game screen, the resulted reward, and whether the game terminates or not. The new screen will be pushed into the State Buffer (a circular queue storing the latest four screens) and constitute a new state with the three previous screens, which is the input for the next Predict stage. • The Observe stage updates the Experience Replay memory (ER, a circular queue with a large number of experiences) by pushing a new tuple (of the predicted action, the reward of the action, the new screen, and the terminal value) as an experience into the ER.</p><p>• The Learn stage updates the Q-network by minimizing the following loss at iteration i <ref type="bibr" target="#b32">[33]</ref>, i.e., the mean square error between q and q t :</p><formula xml:id="formula_2">L i (θ i ) = E (s,a,r,s )∼ER r + γmax a Q(s , a ; θ − i ) − Q(s, a; θ i ) 2 (2)</formula><p>where (s, a, r, s ) are random samples from the ER and θ − i are the parameters of the Q-network used to generate the target q at iteration i, i.e., q t =r+γmax a Q(s , a ; θ − i ). To stabilize q t during training, θ − i are updated much less frequently (every C steps) by copying from θ i <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref> (C=1000 in <ref type="figure" target="#fig_3">Figure 3</ref> (left) as well as our model training process). Exploration and Exploitation Dilemma <ref type="bibr" target="#b12">[13]</ref>. In the Predict stage, actions are not always from the Q-network. A certain percentage of the actions are randomly generated. The reason is that we should not only exploit the intelligence of the agent to predict actions, but also explore the unknown environment with random actions. Usually, the ratio between exploration and exploitation is dynamically updated in the training (i.e., the value of ε in Equation 3 decays over time). However, choosing a proper value for this ratio (to handle the trade-off between exploration and exploitation) is still a very challenging problem.</p><p>predicted action= random action, with probability ε argmax a Q(s, a; θ i ), with probability 1−ε (3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">REQUIREMENT ANALYSIS AND APPROACH OVERVIEW 4.1 Design Requirements</head><p>We maintained weekly meetings with three domain experts in deep learning for more than two months to distill their requirements of a desired visual analytics system for DQN. All the experts have 3+ years experience in deep learning and 5∼10 years experience in machine learning. Through iterative discussions and refinements, we finally identified the following three main themes of requirements: R1: Providing in-depth model statistics over a training. Having an overview of the training process is a fundamental requirement of the experts. In particular, they are interested in the following questions:</p><p>• R1.1: How does the training process evolve, in terms of common statistical summaries (e.g., the rewards per episode, the model loss)? • R1.2: What are the distributions of actions and rewards, and how do the distributions evolve over time? For example, will the action distribution become stable (i.e., a roughly fixed ratio among different actions in an epoch) in later training stages? • R1.3: Can the overview reflect some statistics of the agent's action/movement/reward behaviors? For example, are there any desired patterns that happen more often than others over time? R2: Revealing the agent's behavior patterns encoded in the experience data. Demonstrating the action/movement/reward patterns of the agent is a strong need from the experts <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>  • R3.3: Comparing filters when processing the same experience segment at different stages. This requirement aims to reveal if the agent treats the same experience segment differently in different epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Approach Overview</head><p>Figure 3 (right) shows an overview of our approach to understand DQN with DQNViz. First, we train the DQN model and collect two types of data during the training: (1) the agent's experiences, which are heterogeneous time-varying sequences (Section 5);</p><p>(2) the model losses and network parameters, which are used to assess the model quality and read the agent's mind at different training stages. A pre-processing on the experience data is performed to derive useful summary statistics, which include the average reward, average q values, etc. Second, the DQNViz system takes the two types of data and the derived statistics as input, and presents them to domain experts (Section 6). Aligned with the design requirements, the components of DQNViz are organized into three modules: (R1) model statistics, (R2) behavior patterns, and (R3) segment analysis, which are implemented through four visualization views. The four views, following a top-down exploration flow, present the collected data at four levels of details: overall training level (Statistics view), epoch-level (Epoch view), episode-level (Trajectory view), and segment-level (Segment view). Lastly, we demonstrate several case studies in which the knowledge learned from DQNViz has helped domain experts to diagnose and improve a DQN model (Section 7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">TRAINING PROCESS AND DATA COLLECTION</head><p>We focus on the Breakout game to present DQNViz in this work, as it is one of the most frequently tested/used games in previous works, and a DQN agent can achieve superhuman performance on it.</p><p>In Breakout, the agent has five lives in each game episode. Life loss happens when the agent fails to catch the ball with the paddle. The game terminates if the agent loses all five lives. There are four possible actions: no-operation (noop), firing the ball (fire), moving left (left), and moving right (right). The agent receives rewards of 1, 4 and 7 when the ball hits bricks in the bottom two rows, middle two rows, and top two rows respectively. Otherwise, the reward is 0. On the top of the game scene, two numerical values indicate the current reward and the number of lives left (e.g., they are 36 and 2 in <ref type="figure" target="#fig_1">Figure 2</ref>, left).</p><p>We trained the DQN model from <ref type="bibr" target="#b0">[1]</ref> for 200 epochs. Each epoch contains 250,000 training steps and 25,000 testing steps. The testing part (the blue paths in <ref type="figure" target="#fig_3">Figure 3</ref>, left) does not update the model parameters, and thus is used to assess the model quality. At each testing step, we collected the following eight types of data: 1. action: a value of 0, 1, 2 or 3 representing noop, fire, right, and left. 2. reward: a value of 0, 1, 4 or 7 for the reward from an action. 3. screen: an array of 84×84 values in the range of [0, 255]. 4. life: a value in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref> for the number of lives the agent has currently. 5. terminal: a boolean value indicating if an episode ends or not. 6. random: a boolean value indicating if an action is a random one. 7. q: the predicted q (a floating-point value) for the current step. 8. q t : the target q value, i.e. q t (see Section 3), for the current step.</p><p>At the training stage, the random rate ε starts with 1, decays to 0.1 in one million steps (i.e., 4 training epochs), and keeps to be 0.1 to the end. For testing, ε is always 0.05. During data collection, if an action is a random one, we still use the DQN to derive its q and q t value, though the action to be executed will be the randomly generated one.</p><p>There is an inherent hierarchy in the collected experiences. A step that composed by the eight types of data is an atomic unit. A segment is a consecutive sequence of steps in an episode with a customized length. An episode includes all steps from a game-start to the corresponding game-end (five lives). A testing epoch contains 25,000 steps. In summary, the relationship for them is: step⊆segment⊆episode⊆epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">VISUAL ANALYTICS SYSTEM: DQNViz</head><p>Following the requirements (Section 4.1), we design and develop DQN-Viz with four coordinated views, which present the data collected from a DQN model at four different levels in a top-down order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Statistics View: Training Process Overview</head><p>The Statistics view shows the overall training statistics of a DQN model with line charts and stacked area charts. Those charts present the entire DQN training process over time, along the horizontal axis.</p><p>The line charts (revealing the trend of different summary statistics over the training) are presented as small-multiples <ref type="bibr" target="#b48">[49]</ref> (R1.1, R1.3). As shown in <ref type="figure" target="#fig_0">Figure 1a</ref>, the five line charts track five summary statistics (from left to right): average rewards per episode, number of games per epoch, average q value, loss value, and number of bouncing patterns. Users are able to plug other self-defined statistics into this view, such as maximum reward per episode, number of digging patterns, etc.</p><p>The two stacked area charts demonstrate the distribution of actions and rewards over time (R1.2). The evolution of action/reward distributions provides evidence to assess the model quality. For example, by looking at the distribution of reward 1, 4, and 7 in <ref type="figure" target="#fig_0">Figure 1</ref>-a2, one can infer that the model training is progressing towards the correct direction, as the high rewards of 4 and 7 take increasingly more portions over the total reward. To the rightmost, the even distribution of reward 1, 4, and 7 reflects that the agent can hit roughly the same number of bricks from different rows, indicating a good performance of the agent.</p><p>All charts in this view are coordinated. When users hover one chart, a gray dashed line will show up in the current chart, as well as other charts, and a pop-up tooltip in each individual chart will show the corresponding information, as shown in <ref type="figure" target="#fig_0">Figure 1a</ref> (the mouse is in the stacked area chart for the reward distribution). Meanwhile, the hovering event will trigger the update in the Epoch view (presented next).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Epoch View: Epoch-Level Overview</head><p>The Epoch view presents the summary statistics of the selected epoch with a combined visualization of a pie chart and a stacked bar chart, as shown in <ref type="figure" target="#fig_0">Figure 1b (R1.2, R2.1</ref>). The pie chart shows the action/reward distribution of all steps in the current epoch; whereas the stacked bar chart presents the action/reward distribution of each individual episode in the epoch. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>-b2, there are 20 episodes in the current epoch (one stacked bar for one episode), and the stacked bars are sorted decreasingly from left to right to help users quickly identify the episode with the maximum number of steps/rewards.</p><p>The two types of charts are linked with each other via user interactions. For example, when hovering over the white sector of the pie chart (representing noop actions), the noop portion of all stacked bars will be highlighted, as the area of the sector is the sum of all white regions from the stacked bars. By default, the distributions of actions and rewards are presented in this view, as these two are of the most interest. But, users can also plug in other variables (e.g., the step distribution in different lives of the agent) by modifying the configuration file of DQNViz.</p><p>All views of DQNViz share the same color mapping. For example, the red color always represents the fire action; and the purple color always indicates 4-point reward. Therefore, the pie charts (with text annotation in different sectors) also serve as the legends for DQNViz.  <ref type="figure">Fig. 4</ref>. Different designs for the event sequence data from DQN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Trajectory View: Episode-Level Exploration</head><p>The Trajectory view aims to provide an overview of all steps in one epoch, and reveal the action/reward patterns in all episodes of the epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Visual Design for DQN Event Sequence Data</head><p>The data collected from one episode is essentially an event sequence (also called as a trajectory by experts <ref type="bibr" target="#b11">[12]</ref>). We, therefore, start the design of this view with event sequence visualization solutions, and several key design iterations are briefly discussed as follows.</p><p>Our first design presents one episode with a line of circles representing different types of actions, as shown in <ref type="figure">Figure 4a</ref>. The color white, red, blue, and green represent noop, fire, right, and left respectively. The circles with the black stroke represent the actions with a reward. Many previous works (e.g. <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b53">54]</ref>) have adopted this type of design, as it is straightforward and easy to understand. However, the design cannot reflect the agent's movement patterns and it is also not scalable. To overcome these limitations, we tried to merge consecutive circles representing the same actions as one line <ref type="figure">(Figure 4b</ref>), which can effectively reveal the repeat of different actions. We also explored the spiral layout to address the scalability issue <ref type="figure">(Figure 4c</ref>). The spiral layout can present one entire episode on the screen, but compactly arranging all episodes (in one epoch) with varying lengths becomes a problem.</p><p>To reveal the movement patterns of the agent (R2.2), we visualize the displacement of the paddle to the right boundary over time <ref type="figure">(Figure 4d)</ref>. The design is based on our observation that the moving behavior of the agent is visually reflected by the position of the paddle. For example, the oscillation in the first half of <ref type="figure">Figure 4d</ref> indicates that the agent keeps switching between left and right to adjust the position of the paddle. Also, this design is scalable. It allows us to flexibly compress the curve horizontally, like a spring, to get an entire view of a long episode (R2.1). One limitation is that the paddle positions, though reflecting the agent's movement patterns, cannot accurately reflect the action taken by the agent. For example, in the right half of <ref type="figure">Figure 4d</ref>, the paddle stays at the leftmost position (the top side). The action that the agent is taking now can be noop, fire (fire does not change the paddle position), or left (the paddle is blocked by the left boundary and cannot go further). To address this issue, we overlay the action circles/lines onto the curve. From the visualization <ref type="figure">(Figure 4e</ref>), we found that the agent takes three types of actions (i.e., left, noop, and fire) in the right half of <ref type="figure">Figure 4d</ref>, and most of the time, the agent is repeating the left action.</p><p>Lastly, this design can synchronize other types of data with the action data (R2.3). As shown in <ref type="figure">Figure 4e</ref>, some actions are highlighted with background bars in cyan, indicating they are random actions. Bars with color light green, purple, and orange encode the reward of 1, 4, and 7 respectively <ref type="figure" target="#fig_0">(Figure 4f, 1-c5</ref>). We also design glyphs for actions with a life loss, as shown in the rightmost of <ref type="figure">Figure 4f</ref>. This glyph is a gray bar with 0∼4 dark red rectangles inside, indicating the number of remaining lives after the life loss action. The terminal information is also encoded in this glyph (gray bars with 0 dark red rectangle). The q and q t values are presented as transparent area charts with green and blue color respectively in the background <ref type="figure">(Figure 4g, 4e)</ref>. When users click the action circles/lines, a video clip <ref type="figure">(Figure 4h</ref>) will pop up and show the screen data (a sequence of screens). The two vertical yellow bars on the two sides of the video are the progress bars. We found they are very useful in reflecting the progress of static videos, e.g., when the agent is repeating the noop action. The color bar on the bottom shows the predicted action. It changes from frame to frame when the video is playing. To avoid visual clutters in the Trajectory view, users can show/hide the eight types of data on demand through a set of check-box widgets <ref type="figure" target="#fig_0">(Figure 1-c2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Segment Clustering and Pattern Mining</head><p>It is not a trivial task to visually identify the common patterns from the large number of actions in one epoch. To address this issue, we adopted two techniques of segment clustering (R2.1) and pattern mining (R2.2).</p><p>To cluster segments, we first cut the episodes in one epoch into many smaller segments and cluster them using hierarchical clustering <ref type="bibr" target="#b17">[18]</ref>. A segment is expressed by a sequence of values, indicating the paddle positions. The segment length is set to 100 by default, but it can be adjusted on-demand from the configuration file of DQNViz. To better quantify the similarity between segments, we used the dynamic time warping algorithm (DTW) <ref type="bibr" target="#b39">[40]</ref>. Specifically, for a pair of segments (i.e. two temporal sequences), the DTW algorithm can find the best temporal alignment between them and derive a more comprehensive similarity score. Applying DTW to all pairs of segments will derive a similarity matrix for all segments in one epoch, which is the input of the clustering algorithm. When clicking the "Tree" button in <ref type="figure" target="#fig_0">Figure 1</ref>-c1, a dendrogram visualization of the clustering results will show up <ref type="figure" target="#fig_5">(Figure 5a)</ref>. Selecting different branches of the dendrogram will highlight different clusters of segments in the Trajectory view ( <ref type="figure" target="#fig_5">Figure 5b)</ref>. We found some typical movement patterns of the agent while exploring the clustering results. By defining those patterns and mining them in other epochs, we can provide more insight into the agent's behaviors. The regular expression <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b47">48]</ref> is used to define a pattern as it is simple and flexible. For example, an action sequence can be expressed as a string (of 0, 1, 2, and 3) and a predefined regular expression can be used to search on the string to find when and where a particular pattern happens. <ref type="table" target="#tab_1">Table 1</ref> presents two example movement patterns, i.e., repeating and hesitating (frequently switching between left and right). the two 1s, 4s, and 7s are where the ball hits the bottom, middle and top two rows of bricks, the 0s in between are the round trip of the ball between the paddle and bricks. bouncing (70+){5,} hitting top 2 rows for at least 5 times.</p><p>Reward patterns can be defined similarly. For example, we found that the agent becomes very smart in later training stages, and it always tries to dig a tunnel through the bricks, so that the ball can bounce between the top boundary and the top two rows to achieve 7-point rewards. The digging and bouncing pattern can be defined using regular expressions, as shown in <ref type="table" target="#tab_1">Table 1</ref>. We can also visually verify the patterns from the bars in the Trajectory view <ref type="figure" target="#fig_0">(Figure 1-c5)</ref>. It is worth mentioning that the regular expression for each pattern can be relaxed. For example, the digging pattern in <ref type="table" target="#tab_1">Table 1</ref> can be relaxed to 10+40+40+70+.</p><p>Tracking patterns is an effective way to provide insight into the evolution of the agent's behaviors. For example, the decreasing of repeating indicates the agent became more flexible in switching among actions. The increasing of digging reflects the agent obtained the trick of digging tunnels. The number of a pattern can be defined as a metric (R1.3) and plugged into the Statistics view for overview <ref type="figure" target="#fig_0">(Figure 1-a1)</ref>. First, given a segment, we find the state that is maximumly activated by each of the 160 filters. Specifically, for each filter in each layer, we first apply forward propagations on all the input states of the segment (Algorithm 1, line 5) to get the state (max state in line 7) that is maximumly activated by the filter. Then, using the activation of this state (max activation in line 8), we perform guided back-propagations to generate a saliency map (map in line 9) for the state. The saliency map will have the same size with the input state (i.e., 84×84×4), and the pixel values in the map indicate how strong the corresponding pixels of the input state have been activated by this filter (the back-propagation computes the gradient of the maximum activation on the input state, details can be found in <ref type="bibr" target="#b45">[46]</ref>). Finally, we blend the input state with its corresponding saliency map (blend in line 10). The blending image can expose which region of the input state has been seen by the current filter (like an eye of the agent). For example, <ref type="figure" target="#fig_9">Figure 8a</ref> shows the blending result of the second screen of a state with its corresponding saliency map. We can see that the filter extracts the ball from the screen. The bar charts view <ref type="figure">(Figure 6a</ref>) shows the size of features that individual convolutional filters extracted from the input states. Each row is a bar chart representing one segment (four rows are in the view in <ref type="figure">Figure 6a</ref>). Each bar in each row represents a filter from the DQN, and the height of the bar indicates the size of the feature that the filter extracted (i.e., the number of activated pixels in the corresponding saliency map, see Algorithm 1). The color red, green, and blue indicate the filter is from the first, second, and third layer respectively. Different rows represent the filters for different source segments selected by users, and the corresponding filters are linked together across rows for comparisons (R3.2, R3.3). Clicking the "Sort Filters" button in the header of this view will sort the bars based on their height. Users can focus on filters in layer 1, 2, 3 or all of them for analysis by interacting with the widgets in the header (currently all filters are in analysis). The row with the pink background (the 4th row) is the segment in selection and currently analyzed in the other two sub-views. Clicking on different rows will update contents of the other two sub-views.</p><p>The PCA view <ref type="figure">(Figure 6b</ref>) presents how the filters can capture similar or dissimilar features from the screen states (R3.1). It projects the 160 convolutional filters of the selected row based on their saliency map, i.e., reducing the 84×84×4 dimensional saliency maps to 2D using PCA. Each circle in this view represents one filter, and the color red, green, and blue indicate the filter is from the first, second, and third layer. The size of circles encodes the size of features extracted by the filters. Moreover, the circles in this view are coordinated with the bars of the selected row in the bar charts view. Clicking any bars/circles will pop up a four-frame video showing the blending result of the input state and the corresponding saliency map. <ref type="figure" target="#fig_9">Figure 8a</ref> shows the second frame of the pop-up video when clicking the 8th filter in the second layer (position indicated in <ref type="figure">Figure 6b</ref>). Semantic zoom (the user interfaces in the bottom right of this view) is also enabled to reduce visual clutters. This interaction can enlarge the view while maintaining the size of circles, which will mitigate the overlap of the circles by increasing the distances among them.</p><p>The four screens <ref type="figure">(Figure 6c)</ref> show the aggregated results of states from a selected segment. For example, the top-left screen is the result of averaging the first screen from all input states of the selected segment. We also introduce some interactions to help users easily observe which part of the screen is seen by the filters. For example, when users select different convolutional filters from the bar charts view (via brushing) or from the PCA view (via lasso selection), the union of the corresponding saliency maps will be highlighted on the aggregated screens. In <ref type="figure">Figure 6c</ref>, the two selected clusters of filters <ref type="figure">(Figure 6b</ref>, circles in the green and red lasso) capture the features that the ball is digging the left and right corner of the bricks respectively (R3.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CASE STUDIES</head><p>We worked with the same three domain experts (E1, E2, and E3) in the design stage of DQNViz on several case studies. Here, we present two of them: one emphasizes how DQNViz can help the experts understand the DQN training process; the other shows how the experts use DQNViz to diagnose and improve the exploration rate of the DQN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Unveiling the model training evolution</head><p>Overall training statistics. The experts all confirmed that the training was successful based on observations from the Statistics view in <ref type="figure" target="#fig_0">Figure 1a</ref>. First, both average reward and average q value are increasing, indicating the agent expected and was able to receive higher and higher rewards. Meanwhile, the decreasing number of games (nr games) shows the agent could survive longer in individual episodes (R1.1). Second, the experts observed that the agent became more intelligent and strategic to receive higher points as the training evolves (R1.3). For example, in <ref type="figure" target="#fig_0">Figure 1</ref>-a1, the agent adopted more bouncing patterns that can collect high points along with the training. Also, the reward distribution in <ref type="figure" target="#fig_0">Figure 1</ref>-a2 echoes the same observation as the agent obtained an increasing amount of 4 and 7 points over time (R1.2).</p><p>One interesting observation pointed by one expert is that the action distribution is very diverse even in the later training stages (R1.2). However, the agent can still achieve high rewards with diverse action distributions. This indicates that by merely looking at the action distribution, it is difficult to understand why and how the agent achieved high rewards. It also calls for the investigation of detailed action patterns, which is conducted using the Trajectory view later on by the experts.</p><p>The experts also spotted some abnormal epochs. For example, the reward distribution in epoch 37 did not follow the general trend in the stacked area chart. This observation led the experts to select epoch 37 and drill down to explore its details in the Epoch view.</p><p>Epoch statistics. From the statistics shown in the Epoch view, the experts suspected that the agent repetitively moved the paddle left and right in epoch 37, but those moves were mostly useless. As shown in <ref type="figure" target="#fig_8">Figure 7a</ref>, the left and right actions take 31% and 47% of the total 25,000 steps in this epoch (R1.2). Meanwhile, it can be imagined that the agent mostly hit bricks in the bottom two rows as the achieved rewards were mostly 1-point reward <ref type="figure" target="#fig_8">(Figure 7b</ref>). From the stacked bar charts in <ref type="figure" target="#fig_8">Figure 7a</ref> and 7b, the experts observed that most episodes in this epoch lasted for less than 0.6k steps (much shorter than normal episodes) and achieved less than 5 points. Therefore, the large number of left and right did not contribute much to a better performance (R2.1).</p><p>Action/Reward patterns in episodes. Drilling down to the agent's movement patterns in the Trajectory view, the experts confirmed that the large number of left and right actions in epoch 37 were mostly useless. As shown in <ref type="figure" target="#fig_8">Figure 7c</ref>, the agent repeated a lot of hesitating and repeating patterns, which contributed nothing to achieving rewards. By highlighting the random actions (cyan bars in <ref type="figure" target="#fig_8">Figure 7c</ref>), the experts also learned that random actions play an important role in terminating the hesitating and repeating patterns. Moreover, terminating those patterns may need multiple random actions (e.g., the two random actions in <ref type="figure" target="#fig_8">Figure 7</ref>-c1 are not sufficient to terminate the hesitating pattern).</p><p>The experts were also interested to know the agent's behaviors in normal epochs. Among the many epochs whose statistics follow the general trend, the experts randomly selected epoch 120 to explore. A few hesitating and repeating patterns can still be found in this epoch as shown in <ref type="figure" target="#fig_0">Figure 1-c3 and 1-c7</ref>. Moreover, many digging and bouncing patterns were found in this normal epoch. From the exploration of the hierarchical clustering result <ref type="figure" target="#fig_5">(Figure 5</ref>), the experts found many similar movement patterns shown in <ref type="figure" target="#fig_0">Figure 1-c4 and 1-c6</ref>. By zooming into <ref type="figure" target="#fig_0">Figure 1-c4</ref>, visualizing the reward data, and replaying the video clips, the experts found that the agent was digging a tunnel through the bricks and a bouncing pattern appeared right after the digging pattern, as shown in <ref type="figure" target="#fig_0">Figure 1-c5 (R2.2, R2.3</ref>). In the digging pattern, the agent periodically moved the paddle to catch the ball and received 1-point, 4point, and 7-point rewards with regular step intervals. In the successive bouncing pattern, the agent received 7-point rewards very frequently.</p><p>It is also obvious that the q value kept increasing in digging, but immediately started decreasing after the tunnel was created. The experts interpreted this observation as follows. During digging, the agent can see the progress of the tunnel (from input states), and the expected reward keeps increasing as the tunnel will potentially result in bouncing (i.e., keep getting high rewards). However, when the bouncing starts, the bricks (especially in the top two rows) keep being destroyed, therefore, the expected future reward starts decreasing. The experts also found that when bouncing happens, the paddle mostly stays at the leftmost position, which is "quite phenomenal". This deepens their understanding on the successful playing strategies of the agent and they would like to conduct further theoretical investigations on this.</p><p>Segment-level investigations. Intrigued by digging and bouncing patterns, the experts were curious about how the agent can observe states and make the action predictions. This motivated them to select some segments of interest and analyze them in the Segment view (R3.1). From there, they found that filters from higher convolutional layers usually have stronger activations and capture larger and more diverse features from the input states. For example, to further investigate the digging pattern in <ref type="figure" target="#fig_0">Figure 1</ref>-c4, the experts selected that segment (step 263∼485 of episode 12 in epoch 120) into the Segment view (the fourth row in <ref type="figure">Figure 6a</ref>). By exploring it, they found that: (1) the height of bars representing filters from layer 1 (red), 2 (green), and 3 (blue) shows an increasing trend ( <ref type="figure">Figure 6a)</ref>; (2) the circles representing filters from three layers show an inner-to-outer layout in the PCA view <ref type="figure">(Figure 6b)</ref>.</p><p>By examining the digging segment, the experts also learned that the agent dug tunnels from both sides of the bricks and they identified what filters captured the digging on different sides. Specifically, the two groups of filters in the green and red lasso of <ref type="figure">Figure 6b</ref> are filters that captured the digging on the left and right of the scene <ref type="figure">(Figure 6c</ref>). From the aggregated saliency map of all filters in <ref type="figure" target="#fig_9">Figure 8d</ref>, it can be seen that the agent moves the paddle between the left boundary and the middle of the scene to catch the ball and dig tunnels from both sides.</p><p>Compare segments from the same epoch (R3.2). To have more understandings on the functionality of different filters, the experts also compared the above digging segment with other segments in this epoch. For example, one expert selected another segment shown in <ref type="figure" target="#fig_0">Figure 1-c8</ref>, where the agent moved the paddle all the way to the left then to the right. <ref type="figure" target="#fig_0">Figure 1d</ref> shows the fourth average screen and the aggregated saliency map from all filters for this segment. It is clear that the agent tracked the moving path of the ball, i.e., A-B-C-D-E-F, and it moved the paddle all the way to the left then to the right to catch the ball in B and F. By comparing the filters in these two segments, the experts found that certain filters behave similarly, e.g., filter 16 from the third convolutional layer traces the ball in both segments. However, some filters also have dramatically different functions, e.g., filter 23 in the second layer stares at the top-left tunnel in the digging segment; while it traces the ball in the other segment. This result provides evidence that filters in the same stage may not always have the same functions.</p><p>Track a segment over time (R3.3). The experts also wanted to understand how filters evolved over time by comparing the same segment across epochs. As shown in the top three rows in <ref type="figure">Figure 6a</ref>, one expert tracked the segment in <ref type="figure" target="#fig_0">Figure 1</ref>-c8 in epoch 1, 10 and 120. The blended saliency maps are shown in <ref type="figure" target="#fig_9">Figure 8b</ref>, 8c, and 1d (the saliency maps from all filters on the fourth average screen). From them, the expert understood that the agent did not have a clear vision on the input states in early stages <ref type="figure" target="#fig_9">(Figure 8b)</ref>, and it gradually developed its attention on important parts, e.g. the moving path of the ball <ref type="figure" target="#fig_0">(Figure 8c, 1d</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Optimizing the Exploration Rate (Random Actions)</head><p>One expert was very interested in the random actions, especially after he saw that random actions can terminate bad movement patterns in <ref type="figure" target="#fig_8">Figure 7c</ref>. Here, we describe the experiments that we worked with him to diagnose and improve the use of random actions using DQNViz.</p><p>Experiment 1: No random action. The expert first hypothesized that random actions are not necessary after the model is well trained. The logic behind this is that an action predicted by a well-trained agent should be better than a randomly generated one. To test this, the expert set the exploration rate (ε in Equation 3) to 0, after 200 training epochs, and used the agent to play the Breakout for 25,000 steps (a testing epoch) to see if anything will go wrong (by default ε=0.05 in testing).</p><p>The result of the 25,000 steps in the Trajectory view is shown in <ref type="figure" target="#fig_10">Figure 9a</ref>. The expert had two observations: (1) there is only one episode in the 25,000 steps, and the episode is very long; (2) the agent keeps repeating the noop action in roughly 60% of the episode.</p><p>In detail, the single and long episode can roughly be cut into three phases as labeled in <ref type="figure" target="#fig_10">Figure 9a</ref>. In phase I <ref type="figure" target="#fig_0">(Figure 9-a1)</ref>, the agent played very well in the first ∼1,080 steps, and this phase ended with a life loss. In phase II, the agent kept repeating noop for ∼15, 000 steps (i.e., trapped by the environment). By looking at the screen data <ref type="figure" target="#fig_10">(Figure 9c</ref>) at the position indicated in <ref type="figure" target="#fig_10">Figure 9</ref>-a2, the expert understood that the paddle stays around the middle of the scene and the ball is not in the scene (as there is no fire action). This indicates that the agent does not know that he needs to fire the ball at the beginning of a game, but just keeps waiting for the ball. The expert was very surprised about the agent's movement pattern in phase III, i.e., how did the agent get out of the trap without the help of random actions? After checking the screen data at the position indicated by <ref type="figure" target="#fig_10">Figure 9</ref>-a3, he realized that the game has crashed actually, as the numbers for reward and life disappear and the entire scene becomes lighter <ref type="figure" target="#fig_10">(Figure 9d</ref>).</p><p>Experiment 2: Random actions on demand. From Experiment 1, the expert learned that random actions are necessary. Next, he was wondering if one can control the initiation of random actions when they are really needed. The expert hypothesized that random actions are needed when the agent keeps repeating the same patterns but gets no reward (e.g. hesitating in <ref type="figure" target="#fig_8">Figure 7c</ref>, and noop in <ref type="figure" target="#fig_10">Figure 9</ref>-a2).</p><p>Experiment 2 tested this hypothesis with a pattern detection (PD) algorithm, which can be explained as follows. First, a buffer that stores the latest 20 steps is maintained. At each step of the game, if the agent received rewards in the latest 20 steps, no random action is needed. However, if the agent did not receive any reward, but kept repeating the same action/pattern in those steps (detected using regular expressions, see Section 6.3.2), a random action would be introduced. As observed before, a repeating pattern usually has a basic repeating unit which is very short, e.g., the basic unit of the hesitating pattern in <ref type="figure">Figure 4e</ref> is left-left-right-right and the pattern length is 4. Experiment 2 checks pattern length from 2∼7, and a random action is introduced if a pattern has been repeated for 3 times. For example, if the latest three actions are 230 (right-left-noop) and this pattern can be found 3 times in the latest 9 steps, then the next action will be a random action. <ref type="figure" target="#fig_10">Figure 9b</ref> shows the result of applying the PD algorithm to DQN. Similarly, only one episode is generated in the 25,000 steps, and the episode can be cut into three similar phases. The expert first noticed that the PD algorithm worked well in terminating the repeating of one action in phase I (before ∼1,800 steps). For example, in <ref type="figure" target="#fig_10">Figure 9</ref>-b1, the noop action has been repeated for 20 times, and the agent introduced several random actions and got out of the repeating of noop finally. However, the expert also observed that the agent was trapped by the environment again in phase II. After zooming into this phase, the expert found that the agent kept repeating a long pattern with the length of around 50 steps <ref type="figure" target="#fig_1">(Figure 9-b2)</ref>. By replaying the game, as shown in <ref type="figure" target="#fig_10">Figure 9e</ref>, he realized that the agent kept moving the paddle between point A and D to catch the ball, and the ball repeated the loop A-B-C-D-C-B-A. No random action was introduced, as the length of the repeating pattern exceeded the threshold (i.e., 7) in the PD algorithm.</p><p>The game crashed again in phase III. However, by exploring different segments in this phase, we could still see that the PD algorithm worked well in introducing random actions (e.g., <ref type="figure" target="#fig_10">Figure 9</ref>-b3, b4, b5).</p><p>Experiment 3: Improved random actions. With the lessons learned in Experiment 2, the expert applied the following changes to the PD algorithm: (1) changing the maximum pattern length from 7 to 50; (2) increasing the buffer size from 20 to 100; (3) introducing a random action if a pattern repeats twice. With these changes, the agent was able to play the game very well and no longer trapped by the environment. In 25,000 steps, the agent played 12 episodes and received 5,223 total rewards. The number of random actions in those steps is 501, which is much less than 1,250 (i.e., 5% of 25,000 in the original setting). To quantitatively evaluate this improvement, the expert compared the PD algorithm with other two baselines using ε=0.05 and ε=0.02, as shown in <ref type="table" target="#tab_2">Table 2</ref> (results are averaged over 10 runs). As we can see, the PD algorithm worked better than the other two random-only methods. Compared to the method of ε=0.05, the PD algorithm introduced less random actions, but achieved ∼700 more rewards in 25,000 steps. Also, it led to fewer life losses, as the number of episodes is less than the baseline using ε=0.05. Compared to the method of ε=0.02, the PD algorithm obtained much higher total rewards in 25,000 steps, though the number of random actions was similar. This comparison further verifies that the PD algorithm can effectively control random actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Feedback from Domain Experts</head><p>We collected the experts' feedback via in-depth interviews after the case study sessions. Overall, all experts believed that the tool is "extremely helpful to have actionable insight into the (model) evolution", and "has great potentials to comprehend and improve deep RL models".</p><p>In terms of aiding their understanding of the model, all experts agreed that the overall training statistics are the basic need as they "use them in their daily model building practice". The overall trend and distribution of DQN-specific metrics (e.g., the number of bouncing pattern) really "provide a glimpse into the evolution details" and "would jump-start their diagnosing towards the hurdle of model training".</p><p>All experts believed the most useful component is the Trajectory view, where they "could explore, observe, and extract meaningful patterns" and "ponder how the agent developed its playing strategies". All experts spent the most time on this view to explore useful patterns that could help them comprehend and potentially improve the model. "It is also quite entertaining to look at the video play-back. This level info is absent in other tools.", commented by E3. Another example is the three experiments on random actions. Towards the end of that study, E1 concluded that "the visualization had significantly improved his understanding about random actions". He explained that the final choice of "the (pattern) length to be 50 is not random". In fact, that value should be (or close to be) the upper bound for the number of steps that the ball needed for a round trip (between the paddle and bricks).</p><p>The experts expressed that the Segment view is "very fun to play with" and appreciated this view "provides many details about how the agent parses the game screens". E1 and E2 both made a connection between this view and the "attention mechanism" <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b44">45]</ref> by commenting "it is very enlightening to know layers have attentions over different parts of the screens". E1 even proposed a hypothesis that "filter activation in a complicated network may correspond to different playing strategies".</p><p>Additionally, the experts also mentioned several desirable features and suggested some improvements. E2 was first confused with the visual encoding of paddle positions in the Trajectory view, though later on she understood it with some explanations. She felt "(there is) a disconnection between the horizontal location in the game interface and the vertical position encoding", and "some labels (at the vertical axis of the Trajectory view) may be helpful". For the Segment view, two experts thought "there is a high learning curve", and "it is especially true for the layout of filters". "Showing network structures and highlighting where those filters come from may help", suggested by E1. The three experts also shared their concerns on the generalization of DQNViz. E3 pondered "how this tool can visualize games with a larger action space" or "even real games, like AlphaGO <ref type="bibr" target="#b42">[43]</ref>". These comments are very useful in guiding our further improvements of DQNViz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">DISCUSSION, LIMITATIONS AND FUTURE WORK</head><p>Dimensionality Reduction. The Segment view uses a dimensionality reduction algorithm to project the high-dimensional saliency maps (resulted from individual filters) to 2D for visualization and interaction. The use of the PCA algorithm came out in one of our discussions with the domain experts and it turns out that the algorithm is simple, sufficient for our objective, and involves few parameters to tune with. However, it also suffers from many drawbacks as shown in previous works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b20">21]</ref>, and we do believe that other dimensionality reduction algorithms can be used here as alternative solutions. Most notably, we have tested the t-SNE algorithm <ref type="bibr" target="#b29">[30]</ref> and demonstrated some results of it with different parameter settings in our supplementary material. Both algorithms lay out filters of higher layers in outer locations, indicating higher layer filters usually capture more diverse features. Generalization. DQNViz can be applied to several other Atari games that involve simple movements, e.g., Pong and Space-Invader. However, as a preliminary prototype, it is not readily applicable to all Atari games, especially the ones with sophisticated scenes and large action spaces, like Montezuma's Revenge. This limitation motivates us to generalize DQNViz from several directions in the future. The first direction is to adapt the Trajectory view to games with a larger action space. Currently, this view can only capture movement patterns in 1D (i.e., moving horizontally or vertically). Enabling the view to capture 2D movement patterns is our first planned extension. In detail, we can divide the possible actions into three categories: horizontal movement actions, vertical movement actions, and other actions. Multiple Trajectory views can be used to visualize actions in different categories, and coordinated interactions can be used to connect those views to retrieve screen states and observe action patterns. Second, we believe visual analytics can help the diagnoses and improvements of DQN far more than optimizing the exploration rate. Our next attempt targets on prioritizing the agent's experiences <ref type="bibr" target="#b40">[41]</ref> through visualization to accelerate the training. Lastly, we also want to explore if any components of DQNViz or the designs in our work can be reused to analyze other RL models. For example, the four-level exploration framework may be directly applicable to other RL models, though the details in each level will have many differences. The idea of looking from the agent's eyes through guided back-propagations could also be reused in other deep RL models.</p><p>Scalability. One potential challenge with DQNViz is its scalability, including large parameter settings, games with a very long training process, and so on. As to address the large parameter settings, some components of DQNViz should be improved to accommodate them. For example, the Trajectory view currently presents all actions in one epoch containing 25,000 steps. However, if the number of steps in an epoch is very large, the Trajectory view will have to aggregate and smooth those steps, and use semantic zoom to help users explore the sequence patterns. Similarly, the scalability problem may also occur when extending DQNViz to other games with a very long training process. For example, in certain games, the length of individual game episodes may be too long to be presented in the Trajectory view. In those cases, we can first cut a long episode into many very short segments and use the most frequently appeared action in each segment to represent the segment (i.e. binning and voting). In short, intelligent data aggregations, effective uses of the visualization space, and friendly user interactions always deserve more considerations in addressing the scalability problem.</p><p>More Future Works. We are also interested in exploiting the power of regular expressions in pattern mining, and more user-defined regular expressions may be used to explore the agent's experiences. It may also be possible to extract certain patterns using automatic data mining algorithms. Additionally, our system analyzes DQN models off-line currently (i.e., after training). Enabling domain experts to directly interact with the model training process (i.e., guiding the agent to learn specific behaviors during training) is an important and interesting direction for us to investigate in the future. Lastly, the current version of DQNViz targets to serve domain experts with certain knowledge on DQN models. Simplifying the complex interface and generalizing the domain-specific components of DQNViz to extend the tool to common users are also potential research directions that worth to be explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSION</head><p>In this work, we present DQNViz, a visual analytics system that helps to understand, diagnose, and potentially improve DQN models. The system reveals the large experience space of a DQN agent with four levels of details: overall training level, epoch-level, episode-level, and segment-level. From our thorough studies on the agent's experiences, we have identified typical action/movement/reward patterns of the agent, and those patterns have helped in controlling the random actions of the DQN. The insightful findings we demonstrated, the improvements we were able to achieve, and the positive feedback from deep learning experts validate the effectiveness and usefulness of DQNViz.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>DQNViz: (a) the Statistics view presents the overall training statistics with line charts and stacked area charts; (b) the Epoch view shows epoch-level statistics with pie charts and stacked bar charts; (c) the Trajectory view reveals the movement and reward patterns of the DQN agent in different episodes; (d) the Segment view reveals what the agent really sees from a selected segment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The Breakout game and the reinforcement learning problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>, left): • The Predict stage (conducted by the Q-network with its current parameters θ i ) takes the latest state (4 screens) as input and outputs the predicted reward for individual actions. The action with the maximum reward, i.e., argmax a Q(s, a; θ i ), is the predicted action; and the maximum reward is the predicted q(uality) value, i.e., q=max a Q(s, a; θ i ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Left: the four stages of DQN: Predict, Act, Observe, and Learn; right: the overview of our framework to analyze and improve DQN models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Clustering segments (segment length is 100) in epoch 120.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 . 6 . 4 Algorithm 1 6 : 7 : 8 :</head><label>6641678</label><figDesc>Left: the DQN structure; right: Segment View: (a) bar charts view, (b) PCA view with lasso selections, (c) average screens with saliency maps. Segment View: Segment-Level Interpretation6.4.1 See through the agent's eyesThe Segment view targets to reveal what the agent really sees and how it gains such vision in the screen states from a trajectory segment. To achieve this, we dive into the DQN structure<ref type="bibr" target="#b32">[33]</ref> (Figure 6, left) and reveal what have been extracted by the neural network (R3.1). The input of the network is a state of size 84×84×4 (in green), and the output is a vector of four values (in yellow) representing the predicted rewards for the four actions. Among the four values, the maximum one is the predicted q, and its index is the predicted action. Between the input and output are 3 convolutional and 2 fully connected layers.The filters in the convolutional layers are the basic computational units that extract features from the input states. We focus on them to interpret what the agent sees. The four numbers in each gray rectangle ofFigure 6(left) represent the number of filters, the width and height of each filter, and the number of channels. There are 32, 64, and 64 filters in the first, second, and third convolutional layer (160 in total). Picking out the maximumly activated state (max state) from a segment (screens), and generating the corresponding saliency map (map) of the state for each convolutional filter in each layer.1: screens = [s 1 , s 2 , ..., s n ] // input: a segment of n screens 2: states = [{s 1 , s 2 , s 3 , s 4 }, ..., {s n−3 , s n−2 , s n−1 , s n }] // n-3 states 3: for i = 0; i &lt; layers.length; i + + do 4: for j = 0; j &lt; layers[i]. f ilters.length; j + + do 5: activations = DQN. f prop(layers[i]. f ilters[ j], states) max idx = argmax(activations) max state = states[max idx] max activation = activations[max idx] 9: map = DQN.b prop(layers[i]. f ilters[ j], max activation) 10: out put[i][ j] = blend(max state, map) // algorithm output 11: end for 12: end for Algorithm 1 shows how we visualize what features each filter extracted from an input state. The algorithm includes three main steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>6. 4 . 2</head><label>42</label><figDesc>Analysis components with the agent's eyes The Segment view enables users to analyze the 160 filters along with the 160 states they have maximumly activated in three sub-views (Figure 6, right): a parallel bar charts view, a principal component analysis (PCA) view, and a view showing the average state of the input segment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>The Epoch (a, b) and Trajectory (c) view of Epoch 37.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>(a) Blending a state with its saliency map. (b,c,d) what the agent sees from the segment in the 1st, 2nd, and 4th row of Figure 6a.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 .</head><label>9</label><figDesc>(a, b) Result of Experiment 1, 2; (c, d) game screen at a2, a3; (e) ball trajectory for segment b2, the ball follows the loop A-B-C-D-C-B-A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, given that few existing tools are readily applicable for this purpose. • R2.1: Revealing the overall action/movement/reward patterns from a large number of steps. Facing with a large number of experiences, the experts need an effective overview to guide their pattern explorations. • R2.2: Efficiently detect/extract patterns of interest to understand the agent's behavior. It is nearly impossible to scrutinize the numerous data sequences to spot all interesting patterns (merely with visualization). A mechanism of pattern detection/extraction is desirable. • R2.3: Being able to present other types of data on-demand to facilitate comprehensive reasoning. The q, q t values, random actions, etc., are important context information when analyzing the agent's behaviors. Users should be able to bring them up flexibly. R3: Empowering segment analysis and comparisons by looking through the agent's eyes. This requirement enables users to dive into the architecture of DQN, to analyze how the network works.</figDesc><table><row><cell>screen t screen t-1 screen t-2 screen t-3 84x84x4 State Buffer Predict scalar value 4-tuple 84x84 screen DQN stage sharing path for training &amp; testing path for training only ( * , * ; * )</cell><cell>action</cell><cell>Act Handled by Atari</cell><cell>84x84x1 screen t+1 reward terminal one experience Observe Replay Memory Experience exp 1M … exp 3 exp 2 exp 1 Learn every 4 steps reward terminal action screen</cell><cell>s screen n screen n-1 screen n-2 screen n-3 ( , ; * ) copy every loss 1000 steps</cell><cell>s' screen n+1 screen n screen n-1 screen n-2 ( , , ′; * . )</cell><cell>DQNViz</cell><cell>DQN summary statistics epoch R1: model statistics Data Statistics View</cell><cell>Predict Epoch View R2: behavior patterns Act Observe agent's experiences Trajectory View episode R3: segment analysis Learn network parameters Segment View segment</cell></row></table><note>• R3.1: Revealing what important features are captured and how they are captured by the agent. Specifically, the experts are curious about the functionalities of each convolutional filter in terms of extracting features and making action predictions for experience segments. • R3.2: Comparing convolutional filters when processing different experience segments at the same training stage. For example, domain experts are interested in if the same filter always extracts the same feature when handling different segments in the same epoch.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Formalizing action/reward patterns with regular expressions.</figDesc><table><row><cell>Pattern</cell><cell>Regular Exp.</cell><cell>Explanation</cell></row><row><cell cols="2">repeating 0{30,}</cell><cell>repeating noop (0) for at least 30 times.</cell></row></table><note>hesitating (20*30*){5,} switching left (2) and right (3) for at least 5 times. There might be multiple noop actions between the left and right.digging 10+10+40+ 40+70+70+</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Statistics of random actions per epoch (averaged over 10 runs).</figDesc><table><row><cell></cell><cell>steps</cell><cell>episodes</cell><cell>total rewards</cell><cell>random actions</cell></row><row><cell>ε=0.05 (5%)</cell><cell>25,000</cell><cell>16.6</cell><cell>4198.6</cell><cell>1269.4</cell></row><row><cell>PD Algorithm</cell><cell>25,000</cell><cell>11.4</cell><cell>4899.2</cell><cell>503</cell></row><row><cell>ε=0.02 (2%)</cell><cell>25,000</cell><cell>9.9</cell><cell>3780.8</cell><cell>492.1</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank Wei Zhang, Yan Zheng, and Dean Galland from Visa Research for their insightful comments, valuable feedback, and great helps. This work was supported in part by US Department of Energy Los Alamos National Laboratory contract 47145, UT-Battelle LLC contract 4000159447, NSF grants IIS-1250752, IIS-1065025, and US Department of Energy grants DE-SC0007444, DE-DC0012495, program manager Lucy Nowell.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Github Simple Dqn</surname></persName>
		</author>
		<idno>2018-02-08</idno>
		<ptr target="https://github.com/tambetm/simple_dqn.Ac-cessed" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Foundations of Computer Science (Chapter 10: Patterns, Automata, and Regular Expressions)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Aho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Ullman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<publisher>Computer Science Press, Inc</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning with averaged target DQN. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Anschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Baram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shimkin</surname></persName>
		</author>
		<idno>abs/1611.01929</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A brief survey of deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Arulkumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Deisenroth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brundage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Bharath</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1708.05866" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning: A brief survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Arulkumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Deisenroth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brundage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Bharath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="26" to="38" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Visualizing distortions and recovering topology in continuous projection techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aupetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">7-9</biblScope>
			<biblScope unit="page" from="1304" to="1330" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A multidimensional brush for scatterplot data analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aupetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heulot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-D</forename><surname>Fekete</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual Analytics Science and Technology (VAST), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="221" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The arcade learning environment: An evaluation platform for general agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Naddaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res.(JAIR)</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="253" to="279" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Dynamic Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Bellman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Dover Publications, Incorporated</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Do convolutional neural networks learn class hierarchy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bilal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="152" to="162" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Visual analytics for explainable deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<idno>doi: 10. 1109/MCG.2018.042731661</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="84" to="92" />
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning from human preferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Martic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4302" to="4310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>François-Lavet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fonteneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ernst</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.02011</idno>
		<title level="m">How to discount deep reinforcement learning: Towards new dynamic strategies. NIPS Deep Reinforcement Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goertzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pennachin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial general intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2007" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Decisionflow: Visual analytics for highdimensional temporal event sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gotz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stavropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1783" to="1792" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Eventthread: Visual summarization and stage analysis of event sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gotz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="56" to="65" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Double q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">V</forename><surname>Hasselt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta</editor>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="2613" to="2621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The elements of statistical learning: data mining, inference and prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning to play in a day: Faster deep reinforcement learning by optimality tightening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<idno>abs/1611.01606</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Modayil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.02298</idno>
		<title level="m">Rainbow: Combining improvements in deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Proxilens: Interactive exploration of high-dimensional data using projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heulot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-D</forename><surname>Fekete</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aupetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VAMP: EuroVis Workshop on Visual Analytics using Multidimensional Projections. The Eurographics Association</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Activis: Visual exploration of industry-scale deep neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kalro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H P</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Policy search for motor primitives in robotics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kober</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou</editor>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On actor-critic algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Konda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Control Optim</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1143" to="1166" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cloudlines: Compact display of event episodes in multiple time-series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krstajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bertini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2432" to="2439" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Analyzing the training processes of deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="87" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Towards better analysis of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="100" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards better analysis of machine learning models: A visual analytics perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Informatics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="56" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Understanding hidden memories of recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ming</surname></persName>
			<affiliation>
				<orgName type="collaboration">VAST</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cao</surname></persName>
			<affiliation>
				<orgName type="collaboration">VAST</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
			<affiliation>
				<orgName type="collaboration">VAST</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
			<affiliation>
				<orgName type="collaboration">VAST</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
			<affiliation>
				<orgName type="collaboration">VAST</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
			<affiliation>
				<orgName type="collaboration">VAST</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
			<affiliation>
				<orgName type="collaboration">VAST</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual Analytics Science and Technology</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Playing atari with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page">529</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Temporal event sequence simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Plaisant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2227" to="2236" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep exploration via bootstrapped dqn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4026" to="4034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Matrixflow: temporal network visual analytics to track symptom evolution during disease progression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AMIA annual symposium proceedings</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2012</biblScope>
			<biblScope unit="page">716</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deepeyes: Progressive visual analytics for designing deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pezzotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Höllt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>Lelieveldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eisemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vilanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="108" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Lifelines: visualizing personal histories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Plaisant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Milash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Widoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI conference on Human factors in computing systems</title>
		<meeting>the SIGCHI conference on Human factors in computing systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="221" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A survey of machine learning for big data processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Advances in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">67</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Toward accurate dynamic time warping in linear time and space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="561" to="580" />
		</imprint>
	</monogr>
	<note>Intelligent Data Analysis</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Prioritized experience replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<meeting><address><addrLine>Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Visualization of linear time-oriented data: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Catarci</surname></persName>
		</author>
		<idno>doi: 10 .1109/WISE.2000.882407</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First International Conference on Web Information Systems Engineering</title>
		<meeting>the First International Conference on Web Information Systems Engineering</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="310" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Deep attention recurrent q-network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sorokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seleznev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ignateva</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Striving for simplicity: The all convolutional net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (workshop track)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Lstmvis: A tool for visual analysis of hidden state dynamics in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="667" to="676" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Programming techniques: Regular expression search algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="419" to="422" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Beautiful Evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Tufte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Graphics Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Ganviz: A visual analytics approach to understand the adversarial game</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
		<idno>doi: 10. 1109/TVCG.2018.2816223</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1905" to="1917" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Dueling network architectures for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N. De</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1995" to="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="279" to="292" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Lifeflow: visualizing an overview of event sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wongsuphasawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Guerra Gómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Plaisant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Taieb-Maimon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI conference on human factors in computing systems</title>
		<meeting>the SIGCHI conference on human factors in computing systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1747" to="1756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Egocentric analysis of dynamic networks with egolines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Glueck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chevalier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2016 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<biblScope unit="page" from="5003" to="5014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Matrixwave: Visual comparison of event sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dontcheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems</title>
		<meeting>the 33rd Annual ACM Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="259" to="268" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
