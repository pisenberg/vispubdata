<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Identification of Temporally Varying Areas of Interest in Long-Duration Eye-Tracking Data Sets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithiviraj</forename><forename type="middle">K</forename><surname>Muthumanickam</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Vrotsou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aida</forename><surname>Nordman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Johansson</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Cooper</surname></persName>
						</author>
						<title level="a" type="main">Identification of Temporally Varying Areas of Interest in Long-Duration Eye-Tracking Data Sets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Fig. 1: Finding patterns of spatial visual attention in long-duration eye-tracking data (here ∼90 minutes) suffers from visual clutter as shown in (a). We subdivide the eye-tracking data into overlapping time windows and perform clustering within each to identify areas of spatial visual attention. A cluster merging approach is then used to create a hierarchy of cluster levels. This enables the analyst to explore the changing patterns of attention which may exist over several time windows, disappearing and reappearing as the task progresses. (b) results of clustering the eye-tracking points into 1 minute time windows with 50% overlap at level 0. (c) and (d) merging the clusters into 8. and 32.5 minute time windows at levels 3 and 6, respectively. In (f) and (e) the clusters of levels 0 and 3 are redrawn but coloured according to the clusters in level 6. This allows the analyst to identify recurrent AoIs and observe the dynamic shape of each.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Eye-tracking experiments typically focus upon tasks which have a duration of no more than a few minutes or seconds. The data acquired can, therefore, be readily explored to identify patterns of subjects' attention using simple static visual representations, such as heat maps, and temporal change can be observed using a simple animation. When dealing with longer duration experiments, however, extracting behaviour is made much more complex due to clutter in the raw data itself, and identifying patterns of change made near impossible due to the many potential time frames which must be compared. The main motivation of this work is, therefore, to develop techniques that support the analyst in the exploration of tasks performed by subjects in very long duration (hours) eye-tracking experiments, for which standard methods have proven too difficult and take too long to be practical. These qualities are present in many long-duration monitoring tasks such as examining the attention patterns of car drivers, the monitoring of large and complex systems (e.g. industrial processes, power distribution), traffic flows, and even the behaviour of audiences attending theatre or sporting events. The authors have come to this problem due to the desire to conduct experiments in the field of air traffic control (ATC), where controllers behaviour must be observed for a period of 1-2 hours, and their patterns of behaviour will change not only due to changes in the traffic flow but also as a consequence of changes in prevailing conditions, such as weather or unexpected events (such as in-flight emergencies or communication failure).</p><p>The problem presents a set of features which differ from most eyetracking studies. First, the data is of long duration, presenting problems of cluttering and with the identification of changes in behaviour over time. Second, while the subject's focus is driven by objects in the display, the subject does not specifically watch the objects. Indeed, in the case of an air traffic controller (ATCO), they are trained to never focus on any single object for a significant length of time. Instead, they develop a sense of 'situational awareness' and maintain this by looking at regions of the display at suitable times to update their understanding of the situation. This is similarly true for the driver of a vehicle. Third, the processes being monitored are driven by more than a single simple display, but may also involve attention paid to features outside of the display such as, in the case of a driver or tower ATCO, looking out of the windows or looking at other support information such as paper documents, other information displays or system controls. Consequently, the behaviour of the subject may not be captured by analysing what lies in the display and must be identified directly from the eye-tracking data, and so the focus of the subject's attention over time. The problem also presents an additional aspect in that the subject is involved in a live and long-duration task, where they are controlling the events. For example, an ATCO is directing the aircraft under their control and so, where there exists more than one way to resolve a possible issue during their work, the subsequent behaviour of the system under observation may diverge between two sessions, even when the initial conditions of the system are identical. What behaviour of the subjects leads to a different opinion about the best solution is something that may be captured by comparison between the subjects' separate behaviours over time. These problems raise a number of design goals for the work we present here, each to meet a specific need in the analysis:</p><p>G1 An algorithmic process for the identification of temporally varying areas of interest in long-duration eye-tracking experiments, where the aim is to understand how the visual attention of a subject evolves over time. The process could be fully automatic or controlled by the analyst as appropriate.</p><p>G2 A system for interactive visual analysis providing a summary view of the identified AoIs over the entire duration of the data.</p><p>The system should support the identification of important characteristics of the subject's behaviour and the identification of changes in behaviour at potentially unanticipated points.</p><p>G3 Identification of similarities and differences between individual subjects' behaviour despite variation in their actions leading to differences in the progress of the task.</p><p>This work addresses the goals presented above by using a combination of clustering of the raw data, temporal merging of the clusters to produce a hierarchy of AoIs and their display in an interactive Space-Time Cube (STC) <ref type="bibr" target="#b22">[23]</ref> representation. The STC representation allows the analyst to examine the temporal evolution of AoIs as they appear, disappear and recur over time in long-duration experiments. A visual comparison can also be performed to identify any substantial variation in the behaviour of a set of subjects undertaking the same experiment. The main contributions of our approach are the following:</p><p>• A time-dependent clustering and cluster merging method to identify spatial AoIs that appear, disappear and change over time.</p><p>• A visual representation of long-duration visual attention behaviour through the display of changes in the identified AoIs over time.</p><p>• Flexible navigation of the hierarchy of identified AoIs using information from higher levels to aid the analyst in tracking recurrent behaviour.</p><p>• A scalable approach that can handle very long-duration eyetracking data through parallelization of the data analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Visual attention of subjects is commonly studied using eye gaze data collected through eye-tracking experiments. Visual attention can be broadly classified into two types: bottom-up and top-down attention <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39]</ref>. Bottom-up attention refers to involuntary attention, where a salient stimulus can attract attention even though the subject had no intentions to attend to it. Top-down attention refers to voluntary allocation of attention, where subjects focus their attention to: (1) a small well-defined region of space in the scene (top down spatial attention) referred to as an Area of Interest (AoI), or (2) a moving or otherwise distinct object in the scene (top down feature attention) referred to as an Object of Interest (OoI). The similarity in top-down and bottom-up approaches is that, although the reason for visual attention may be different, the attended stimuli receive preferential processing and hence there will be an accumulation of eye gaze points around them in the collected data. Identifying and labelling these accumulated eye gaze points across a scene and analyzing them can help identify eye movement patterns, cognitive load and subjects' behaviours.</p><p>An important distinction to make between the two types of top-down attention (AoIs vs OoIs) lies in the questions they aim to answer. The identification and study of spatial AoIs aims to reveal how a subject inspects and monitors a scene which may contain moving or distinct objects. This contrasts with OoIs identification which aims to reveal what distinct or moving objects in the scene are monitored by the subject. Readers can refer to <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40]</ref> for methods that identify OoIs in eye tracking data. Though OoIs can capture the attention shift between distinct objects in the scene, their associated spatial context is lost along with their temporal evolution (in case of moving objects). In our work, we are interested in identifying spatial AoIs in long duration eye tracking datasets. In order to identify the context that attracted the spatial attention, we perform an in-context visual analysis of AoIs by overlaying them over the video capture of eye gaze data. In the following, we discuss the present state of the art on various methods for identification and visual analysis of AoIs and their disadvantages in dealing with long-duration eye-tracking data sets are also highlighted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Identification of AoIs</head><p>Areas of interest change over time with respect to their size and position during long-duration eye-tracking experiments. AoIs of a first time subject looking at a certain stimulus can be different from an experienced subject looking at the same stimulus. Hence, there is a time evolving nature to AoIs and labelling them can be useful in understanding how a particular subject is paying attention over a stimulus. In general, manual identification and labelling of AoIs is a difficult task <ref type="bibr" target="#b11">[12]</ref> and several approaches are studied for automating this process. We classify identification and labelling of AoIs in eye-tracking data into two main categories: 1) methods using manually predefined grids, and 2) automated methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Manually predefined AoIs</head><p>A common approach to the identification and labelling of AoIs is to divide the scene into predefined grids, or a set of manually defined areas, and then assign appropriate labels to eye data points falling within them. The resolution of the grid in this case determines the accuracy and the number of identified AoIs <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20]</ref>. The disadvantage of predefined static AoIs is that they can fail to capture the temporal evolution of the subjects' attention over time, particularly when longduration experiments are involved. Consequently, such methods, do not adhere to our design goals G1 and G2. In these cases, it becomes a necessity to adopt other methods for identifying and labelling the AoIs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Automated methods</head><p>Several approaches for automatic identification of AoIs have been suggested that are mainly based on clustering of the raw eye-tracking data. As discussed earlier, AoIs are often varying over time and hence clustering which takes time into account needs to be applied to understand gaze movement strategies <ref type="bibr" target="#b5">[6]</ref>. The research available on this is extensive but not well suited to long-duration data. In fact, the identification of time evolving AoIs in stimuli that contain either active or passive content is ranked as important future work in a recent survey <ref type="bibr" target="#b7">[8]</ref>.</p><p>Traditional approaches dealing with analytics of eye-tracking data start with eye movement classification algorithms to differentiate between fixations and saccades. They involve threshold and bayesian Methods such as Mean shift <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b41">42]</ref> and Gaussian Mixture Models <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b43">44]</ref> have been used to perform clustering on the whole eye-tracking data set to identify AoIs. These methods do not satisfy any of our design goals because they do not consider the changing behaviour of an observer over time. Therefore, they are not suitable for direct application on long-duration data sets as when the amount of eye gaze data increases, the local AoIs get distorted or saturated resulting in very few clusters (AoIs) that were accumulated over time.</p><p>Methods that consider eye-tracking scan paths as trajectories have also been used for analysing eye-tracking data and identifying AoIs <ref type="bibr" target="#b15">[16]</ref>. The work of Andrienko et al. <ref type="bibr" target="#b2">[3]</ref> provides a thorough overview of visual analytics approaches from Geographic Information Science (GIS) with their merits and drawbacks for analyzing eye movement data. We tried a trajectory clustering algorithm <ref type="bibr" target="#b3">[4]</ref> that considers both 2D spatial information of the gaze points and the time stamp value to find time evolving AoIs. We found, however, that using such an approach did not produce satisfying results primarily due to the nature of the eye gaze trajectories and also due to the long-duration of eye-tracking recordings. Unlike the trajectories of moving objects, eye gaze points are not spatially continuous over time, since eye gaze data commonly involve frequent attention switching over the stimulus. This is particularly true in the case of air traffic controllers' visual scanning patterns. Smooth pursuits are an exception as they generate a continuous trajectory of data when a subject is continuously tracking a moving object. However, our goal (as stated in G1) is to identify how attention to spatial AoIs evolves over time and not to identify and track actual moving objects in the scene (OoIs). Furthermore, the percentage of smooth pursuits in the entire eye-tracking data set depends on the nature of eye-tracking task. In the specific case of air traffic control, smooth pursuits are considered a dangerous behaviour because they can lead to a loss of local situational awareness. Overall, the methods described above are not suitable for direct application to long-duration experiments because they are mainly focusing on extracting AoIs from the entire eye gaze data set projected onto a 2D space, which becomes problematic as the data increase and local AoI information gets lost.</p><p>While the curse of multi-dimensional data is its data sparsity in higher dimensions, we are faced with high accumulation of data in 2D over time that distorts the information. We exemplify this by applying Mean Shift clustering <ref type="bibr" target="#b13">[14]</ref> on a 4.5 minute time interval of a data set composed of 2D eye gaze points collected at 60Hz, as shown in <ref type="figure" target="#fig_0">Figures 2(a)</ref> and (b). In the example, high (0.15) and low quantile (0.01) values were used leading to time varying AoIs getting labelled at either a very high or very low granular level, respectively. We tested with various quantile values in between but were unable to find any that produced results comparable with our approach, where we build AoIs from the smallest time windows to larger ones by merging the AoI clusters based on their strength of intersection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Visual analysis of eye-tracking data</head><p>Eye tracking data is collected in the form of gaze position and time stamp. Visualization of time oriented data is discussed extensively in <ref type="bibr" target="#b1">[2]</ref>. Blascheck et al. <ref type="bibr" target="#b6">[7]</ref> categorizes visualization techniques for eye tracking data into 2D and 3D, animated and static, spatial, temporal and spatio-temporal, in-context and not in-context, etc.</p><p>2D Heat maps, or attention maps, is one of the simplest visualization technique, where areas with differing eye fixation density can be mapped to a colour scale. Such maps can be constructed in realtime <ref type="bibr" target="#b17">[18]</ref> and are used to aggregate the fixation data over the stimulus and mark out areas with high accumulation of eye gaze points. Methods presented in <ref type="bibr" target="#b20">[21]</ref> and <ref type="bibr" target="#b47">[48]</ref> quantify the similarity of eye fixation points and the degree of coverage of fixations over a visual stimulus. While they can provide information about the distribution of eye fixation density over the stimulus, they fail to display any temporal characteristics of the identified AoIs. Another disadvantage is that they can easily saturate the entire stimulus with their colour scale, when long-duration eye-tracking experiments are involved. As we are dealing with long duration eye tracking datasets, small multiples based visual representation can suffer from lack of screen space while animation based visualization techniques are not suitable due to difficulties in preserving the viewers mental map <ref type="bibr" target="#b1">[2]</ref>. This makes such methods not applicable to our problem as per our design goal G2.</p><p>Space-Time Cube representations can capture variation over space and time in the same view which is why they have been used in several studies <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b32">33]</ref>. In case of eye-tracking data, they can display the time evolving nature of AoIs, their temporal duration, underlying stimuli (dynamic/static), and aggregated information from multiple subjects <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b30">31]</ref>. Due to these characteristics of the STC, we use it along with 2D spatial projection of time slices from STC. Kurzhals et al. <ref type="bibr" target="#b27">[28]</ref> propose an approach that combines the use of STC with synchronized scarfplots and timeline views in order to identify and represent AoIs/OoIs over time. The main difference between this work and our approach is that they focus on the identification of objects over time in the scene, while we are interested in identifying interesting spatial AoIs over time (as per G1). Due to this, the use of scarfplots and timelines is inappropriate for us because these representations lack the spatial reference which is central to our work. Moreover, previous approaches focus upon tasks which have duration of no more than few minutes, while our method deals with eye gaze recordings in the order of hours.</p><p>In general, many pure visual representations such as AoI trees, AoI rivers, graphs, scarfplots <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b44">45]</ref> fail in combining both the spatial and temporal characteristics of the AoIs and OoIs. Furthermore, they often suffer from visual clutter, when dealing with longduration eye-tracking data. In order to overcome these shortcomings, a semi-automatic analyst driven data mining approach for interactive exploration of AoI sequences over time was proposed in <ref type="bibr" target="#b35">[36]</ref>. They proposed to adapt algorithms used for exploring sequences in a large event-based data <ref type="bibr" target="#b45">[46]</ref>. Along with data mining and analyst driven interactive exploration of AoIs, only those AoIs of interest to the analyst can be visualized over the stimulus, thereby reducing visual clutter Step 3</p><p>Step 2</p><p>Step 1 <ref type="figure">Fig. 3</ref>: Process pipeline for identification of temporally varying AoIs in long-duration eye-tracking data sets. As a first step of the above proposed visual analytics model, our aim in this work is to identify temporally varying spatial AoIs in long-duration eye-tracking data sets (G1), which can then be further explored using data mining approaches, as described in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OUR METHOD</head><p>The automated approaches for the extraction of AoIs discussed in Section 2.1.2 are not suitable for direct application to long-duration eye-tracking experiments due to the high accumulation of data in 2D over time which distorts the local AoI information. Our proposed method for identifying temporally varying AoIs is composed of the following 3 steps which are described in detail in subsections 3.2-3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Divide the long-duration eye-tracking data into overlapping time</head><p>windows and run a clustering algorithm (Section 3.1) independently on each time window using parallel threads (Section 3.2).</p><p>2. Merge clusters from neighbouring time windows in a hierarchical manner, based on the strength of overlap between neighbouring clusters. Repeat the merging to the desired level of hierarchy (Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">From the desired level of hierarchy, propagate cluster labels downwards (Section 3.4).</head><p>Implementation and performance details of the complete system are described in Subsection 3.6. The process pipeline of our method is shown in <ref type="figure">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Zahn's clustering approach</head><p>Having explored a variety of algorithms, both the clustering and merging steps in our approach have been implemented using Zahn's Minimum Spanning Tree (MST) based clustering algorithm <ref type="bibr" target="#b48">[49]</ref>. Before we present details of each step of our method, we briefly describe this graph-based clustering algorithm. Consider a data set of points S in a metric space with a distance function ρ(x, y). The data set S implicitly defines a weighted undirected graph G obtained by associating the cost ρ(x, y) with each edge (x, y), for any two points x, y ∈ S. In <ref type="bibr" target="#b48">[49]</ref>, it is shown how the MST for graph G can be used to capture the cluster structure in S. The intuition behind the algorithm is to detect clusters in the data set by deleting those edges from the MST that have a significantly higher cost than the other edges. These edges are called inconsistent edges.The connected sub-graphs of G obtained after the removal of inconsistent edges form the clusters.</p><p>To determine the inconsistency of the edges, Zahn proposes several methods. The following was used in our work. Assume . An edge (x, y) is inconsistent if this ratio is above a given threshold f &gt; 0, called the inconsistency factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Step1: clustering raw data points</head><p>In the first step of our proposed method, we pre-process the data for any null values in the eye gaze co-ordinates and replace it with a predefined spatial point in order to retain the information on when the subject moved away from the field of view of the eye tracker. The raw data points are then divided into overlapping time windows and clustering is applied to the points in each time window. The size of the time windows and amount of overlap is chosen by the analyst depending on the nature of the dataset. Following this, we use the Minimum Spanning Tree based clustering algorithm proposed in <ref type="bibr" target="#b48">[49]</ref> to cluster the points within each time window. One important advantage of this method is that it can handle very large data sets, since clustering is performed independently in each time window and hence can be parallelized. Previously, MST based techniques <ref type="bibr" target="#b40">[41]</ref> were used to identify fixation and saccades in eye-tracking data due to their degree of control and flexibility. However, their computational cost was high, which we overcome in our method through the use of parallelization for the cluster computation. The nodes of our weighted undirected graph are eye gaze points and the initial graph is obtained by computing the Delaunay triangulation (DT) <ref type="bibr" target="#b18">[19]</ref> of the eye-tracking points, as shown in <ref type="figure" target="#fig_3">Figure 4</ref>(a). The cost associated with an edge (x, y) corresponds to the Euclidean-distance between the nodes x and y. Note that an MST of the weighted undirected graph, implicitly defined by a set of points in a metric space with a distance function, is a sub-graph of DT. An MST is then computed (figure 4(b)) and inconsistent edges detected and removed using the method described in Section 3.1. The resulting clusters correspond to the connected sub-graphs. The output of the first step of our method is the set of cluster polygons <ref type="figure" target="#fig_3">(Figure 4</ref>(c)) corresponding to these connected sub-graphs.</p><p>The results of this step correspond to the first and most detailed level of granularity of the identified AoIs and will, in the following, be referred to as level 0 of our approach. The clustering results can be displayed in a space-time cube <ref type="bibr" target="#b22">[23]</ref>, as shown in <ref type="figure">Figure 1</ref>(b). The STC's horizontal plane corresponds to spatial coordinates, in this case spatial positions of eye gaze points, and the vertical axis corresponds to time going upward. We choose the Zahn's method <ref type="bibr" target="#b48">[49]</ref> for our clustering step because of the degree of control, flexibility <ref type="bibr" target="#b40">[41]</ref> and for uniformity, since the same process is used in the second step of our method. Other clustering algorithms could, if appropriate, be used in this first step to cluster the raw data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Step2: cluster merging AoIs</head><p>A drawback with performing clustering over time windows, as outlined earlier, is that spatial visual attention over a stimulus evolves over time and hence a visual attention cluster (AoI) can spread across adjacent time windows. This would eventually lead to different cluster labels, as the clustering in a single time window is independent of the rest. We overcome this problem by combining the clusters from a set of neighbouring time windows based on the strength of local overlap of the cluster polygons. This merging process can be carried out in a hierarchical manner which creates a spatio-temporal hierarchy, as shown in <ref type="figure" target="#fig_4">Figure 5</ref>. This hierarchy makes it possible to effectively assign meaningful labels to the identified AoIs over time, even if they are significantly separated in time. The merging process is performed in two steps and is repeated for each adjacent pair of time windows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Compute an adjacency matrix of cluster polygons based on a cost</head><p>function between cluster polygons.</p><p>2. Compute an MST from the adjacency matrix and apply the Zahn clustering algorithm to find cluster polygons with maximum intersection. The nodes of the MST represent the cluster polygons, while an edge between two cluster polygons x and y is associated with the cost between x and y. Computations at each time window in a level are independent of each other and hence can be done in parallel.</p><p>There are two main advantages to the proposed merging step. First it enables us to find local overlapping AoIs without clustering of the entire data. Second, the process is parallelizable and so re-computing the merging process at selected nodes in the hierarchy can be done in real time and under the control of the analyst.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Adjacency matrix of intersection costs</head><p>We compute an adjacency matrix to establish the neighbourhood relations between cluster polygons from two consecutive time windows. The adjacency matrix, C, is a symmetric N × N matrix where N is the total number of cluster polygons from two consecutive time windows. The entries of the adjacency matrix range from 0.0 to 1.0. The value C(i, j) represents how strongly the polygons i and j are connected with each other, as shown in equation 1. If there is a high overlap between two cluster polygons (see <ref type="figure">Figure 6</ref>(a),(b)) then there is a minimal cost and vice-versa. The entry C(i, j) is 0.0, if there is no intersection between polygons i and j. Otherwise, the value of the cost function is in the range [ε, 1.0], where the value of epsilon ε corresponds to two polygons with full intersection.</p><formula xml:id="formula_0">C(X,Y ) =          0 , X ∩Y = / 0 min area(X −Y ) area (X) , area(Y − X) area (Y ) , X ∩Y = / 0 ∧ X −Y = 0 ∧ Y − X = 0 ε , X −Y = 0 ∨ Y − X = 0<label>(1)</label></formula><p>An additional parameter for temporal decay can be included in the cost function to control the merging of two cluster polygons based on their temporal proximity. For our example data, however, the chosen cost function (Equation 1) works well without any temporal decay parameter. We perform N 2 computations for finding intersections between polygons. While this is a costly operation its computation time in our example data is negligible because the number of polygons in each time window is small and decreases further at higher levels. Spatial data structures such as kd-trees could be used to accelerate intersection tests between polygons if their numbers are large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">MST-based clustering</head><p>There may be multiple intersections between cluster polygons (see <ref type="figure" target="#fig_6">Figure 8</ref>(a)) within a local spatial area due to the volatility of eye movements. The aim is to find maximum connectivity/intersection among the polygons. Minimum spanning trees can be effectively used to solve such combinatorial problems (see, for example, <ref type="figure">Figure 7)</ref>. They are already used to remove intersections among nodes in a graph layout algorithm <ref type="bibr" target="#b36">[37]</ref>. We construct an MST from the adjacency matrix computed from the previous step. Each node in the MST represents a cluster polygon and an edge represents the intersection between the polygons. The weight of an edge or edge length, corresponds to the cost of intersection between two polygons. The higher the cost the longer the edge between the two nodes. Thus the set of interconnected polygons with maximum intersection are closely connected in the MST, while connections with high cost are separated by longer edge lengths. We use Zahn's method <ref type="bibr" target="#b48">[49]</ref>, described in Section 3.1, to find clusters of intersecting polygons. Finding inconsistent edges is performed in the same manner as in Step 1 of our approach (Section 3.2). Thus, the deletion of inconsistent edges leads to MST sub-graphs that correspond to AoI clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Step 3: label propagation</head><p>The spatio-temporal hierarchy of levels shown in <ref type="figure" target="#fig_4">Figure 5</ref> can be explored by the analyst to view the temporally evolving AoIs. The analyst can select a particular level of granularity from which assigned AoI labels (and colours) can be propagated downwards to shorter time windows, as shown in <ref type="figure">Figures 1(e</ref>) &amp; (f). This helps to retain the temporal detail of the identified AoIs but assign a more coarse labelling to them which better reveals the subject's attention concentrations and shifts. This feature will be crucial when extracting AoI sequences from the data. These sequences can then be used to automatically identify visual scanning patterns through data mining algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Method parameters</head><p>The parameters used in our method are described below. Size of time window, t ws : The size of the time window is application specific and depends on the nature of the data. In general, it is advisable to use small duration time windows to avoid accumulation of data over time and let the merging process at higher levels control the granularity of the cluster polygons/AoIs. Since there is frequent attention switching in our example data set we set this parameter to a low value of one minute to avoid saturation of local AoIs.</p><p>Percentage of overlap, t ov : Overlapping time windows are necessary to prevent AoIs in neighbouring time windows from being assigned different cluster labels. Increasing the overlap percentage increases the number of time windows at level 0 and vice-versa. This value is application specific and we tried with several values for our example data set and found an overlap of 50% to be suitable.</p><p>Neighbourhood depth, mst d : Inconsistent edge detection requires the computation of the ratio r xy , for each edge (x, y), by exploring the edge's neighbourhood to a depth of mst d . A high value for the neighbourhood depth would lead to few inconsistent edges and, consequently, a small number of clusters. The value should be heuristically determined based on the nature of the data set.</p><p>Inconsistency factor, f : The inconsistency factor value is also application specific. In our approach, the ratio r xy is computed for every edge, (x, y), of the MST and the values are displayed in a line plot to help the analyst to select an appropriate value to control the granularity of the clusters, as shown in sub-figures 8(b) and (c).</p><p>Minimum overlap threshold for cost function, c min : A threshold for minimum overlap between two AoIs is included in the merging step. This allows the analyst to control the strength of the connection between two AoIs in order for them to be considered as one.</p><p>Minimum number of points in a cluster polygon, cluster points : The clustering of raw eye-tracking data discards the saccade points based on a minimum point threshold. Since the maximum duration of a saccade is 80ms <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b42">43]</ref> and the refresh rate of the eye tracker used to collect our example data is 60 Hz, the number of points constituting a saccade can be ∼5. In our algorithm, we safely initialize the minimum point threshold to be more than twice the saccade threshold 12. This value is set based on the refresh rate of the eye tracker. <ref type="figure">Fig. 7</ref>: Merging different polygons based on maximum connectivity among them is solved using minimum spanning trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>+ =</head><p>Minimum area of cluster polygon, cluster area : We can set a threshold to decide a minimum area for the cluster polygon to be considered an AoI. This value is application specific and can be controlled by the analyst in order to be able to remove low surface area clusters, by treating them as noise.</p><p>In our method some parameters can be set to an initial value based on the nature of the application. Size of time window, percentage of overlap, neighbourhood depth are set once for our example data set taken from an air traffic control experiment. Parameters such as minimum overlap threshold, minimum number of points and minimum area of cluster polygon are used to filter the AoIs based on the requirements of the analyst. Inconsistency factor is used to control the granularity of the identified clusters, where increasing or decreasing the value can lead to an increased or decreased number of clusters (see <ref type="figure" target="#fig_6">Figure 8</ref>). The analyst is required to choose this value to control the localization of visual attention. For our example data set from the domain of ATC, we provide an initial value to control the granularity, but the analyst can change it in real time to meet their needs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Implementation details and performance</head><p>Visual display of the eye-tracking data and cluster polygons, together with video playback of the stimulus were implemented in Qt and OpenGL. All computations were implemented in Python and multithreaded where appropriate. We tested our approach with nine different data sets collected from ATCOs monitoring a radar screen. The data was recorded at 60 Hz which generated data sets of 270 k-320 k samples. We tested the performance on an iMac with a 3.5GHz Intel Core i7 processor, 16GB RAM and an NVIDIA GeForce GTX 775M graphics card with 2 GB memory. The computation time for our example data sets was between 33 to 52 seconds for calculation of the Delaunay triangulation, MST construction, computing cluster polygons and identification of inconsistent edges.</p><p>If N is the number of gaze points in a data set, and k is the initial number of time windows then the Delaunay triangulation, used in step 1 of our method, can be computed in O(N log N) time and produces O(N) edges. Computing the ratio r xy for each edge requires O(N 2 ) time. Consequently Zahn's clustering algorithm also requires O(N 2 ) time. However, the computation time can be reduced by pre-computing and storing the neighbourhood MST of the raw eye data points. Clustering the raw data points is the most time consuming step in our method, since Delaunay triangulation followed by Zahn's clustering algorithm are applied to each time window. However, this process can be parallelized over time windows. Calculating the distance between the nodes for cluster merging (step 2), requires O(N log N) time. The height of the hierarchy built in step 3 is logarithmic on k and so each level in the hierarchy has O(k) time windows, each of which involves only a few identified AoIs and so is completed in milliseconds for our data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">APPLICATION INTERFACE</head><p>Our application interface (see <ref type="figure" target="#fig_7">Figure 9)</ref> provides an in-context visual display of the eye gaze data overlaid on the captured video of the stimulus and allows the analyst to browse the hierarchy obtained through the cluster merging, as shown in <ref type="figure" target="#fig_4">Figure 5</ref>. Analysts can also modify the results of clustering to their desired level of granularity in real-time.</p><p>In-context visualization. The 2D context visualization window <ref type="figure" target="#fig_7">(Figure 9(b)</ref>) displays captured video of the stimulus of the experiment in the background and can be overlaid with raw eye gaze points and/or the identified temporally varying AoIs. This view gives a contextual spatial overview of the AoIs on the screen and aids the analyst in understanding the stimulus's events that attracted the visual attention. To capture the temporal aspects of the data, the main context window is supplemented with a 3D Space-Time Cube <ref type="figure" target="#fig_7">(Figure 9(a)</ref>) that displays eye points and/or temporally varying AoIs spread over time. In this view, the horizontal plane corresponds to the 2D stimulus, while time is represented on the vertical axis, increasing from bottom to top.</p><p>Data exploration. An interactive control panel composed of traditional widgets <ref type="figure" target="#fig_7">(Figure 9(c)</ref>) is available for the analyst to browse through the data. Analysts can browse the time windows at any level of the hierarchy and also navigate across the multiple levels of hierarchy as shown in <ref type="figure" target="#fig_4">Figure 5</ref>. By moving up and down the multiple levels of the hierarchy an analyst can identify the maximum level of data saturation that creates only a few big clusters which cover the entire area of the stimulus. At this point, the analyst can traverse down the hierarchy to their desired level of cluster saturation. They can browse through the time windows at that level of the hierarchy to find temporally evolving AoIs. Cluster polygons from the currently selected time window are displayed in the context visualization window, as shown in <ref type="figure" target="#fig_7">Figure 9(b)</ref>. Furthermore, the control panel allows the analyst to select AoI colours from a particular level and have them propagated down to lower levels of the hierarchy. Four sliders are available to browse the data at desired time intervals, where the first two sliders are used to set a time interval, the third to position the time interval and the fourth to browse within that time interval. Based on the interaction of the fourth slider, the background of the Context window is updated by image frames from the video capture of the stimulus.</p><p>Tuning clustering parameters. Among the parameters described in Section 3.5, the inconsistency factor, f , controls the granularity of the AoI clusters, as shown in <ref type="figure" target="#fig_6">Figure 8</ref>. An interactive 2D plot <ref type="figure" target="#fig_7">(Figure 9(d)</ref>) is used to display the ratio r xy to help the analyst to interactively set the inconsistency factor used for finding inconsistent edges, as described in Section 3.2. This plot allows analysts to set a maximum threshold by moving a horizontal line up and down on the plot, as shown in <ref type="figure" target="#fig_7">Figure 9(d)</ref>. We can browse through the time windows at any desired level of the hierarchy (see <ref type="figure" target="#fig_4">Figure 5</ref>) and, once the threshold is set, the cluster merging process is re-run for the corresponding time window and all the higher levels of the time window hierarchy are updated accordingly. Parameters such as minimum overlap threshold of two cluster polygons, (c min ), minimum number of points in a cluster polygon, (cluster points ), and the minimum area of a cluster polygon, (cluster area ), all set through the Control Panel <ref type="figure" target="#fig_7">(Figure 9(c)</ref>), provide high flexibility to analysts for setting the properties of valid spatial AoIs, depending on the application domain. The Control panel also allows the analyst to set the size of the time windows (t ws ), the percentage of overlap between consecutive time windows (t ov ), and the neighbourhood depth (mst d ) used to determine the inconsistent edges.</p><p>All the views in the interface are linked and any changes or selections made by the analyst in the control panel will be reflected in all windows. Upon choosing a time window, data corresponding to it will be displayed in all views. Eye data points and/or identified temporally varying AoIs, belonging to the selected time window, will be shown in the main visualization panel and the STC. The visibility of these can be interactively toggled by the analyst through the control panel. and is updated as the analyst moves through the temporal data using the sliders available in (c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">APPLICATION EXAMPLE: SIMULATION OF REAL AIR TRAF-FIC CONTROL SCENARIO</head><p>In this Section we demonstrate the results of applying our method to data collected from two operational air traffic controllers (ATCOs) conducting a training session on a simulation of flights into a medium-sized airport. ATCOs are required to do this type of training during their entire professional career. The performed training session was concerned with control of an air traffic sector through a radar screen with which the ATCO could interact via a mouse. After finishing each scenario they, together with an instructor, go through the scenario to discuss how the ATCO performed. Both ATCOs in this example conducted the same training scenario, which allowed us to compare their behaviour. This particular training session lasted for around 85 minutes, which is common for these types of training sessions. Training scenarios can be constructed with purely normal traffic, usually for trainee controllers, and with unusual events for more experienced controllers. Unusual events can include prearranged proximity events, failure of pilots to obey instructions, radar and radio failures, unidentified aircraft entering the airspace, unusual weather conditions. Tower simulations, which involve large screen displays to provide a panorama view of the airfield, can include issues with ground vehicles, pilot errors while taxiing or sudden poor visibility due to fog or very heavy cloud. Understanding how the ATCOs behave when dealing with these scenarios, their focus of attention, and to which features they may not be paying sufficient attention, is of great potential benefit to the trainer. Data collection. While each ATCO was monitoring the radar screen <ref type="figure" target="#fig_9">(Figure 11</ref>), their eye movements were collected using a Tobii <ref type="bibr" target="#b0">[1]</ref> eye tracker at a rate of 60Hz in an ideal laboratory environment. The radar screen content itself was also captured to video to make it possible to overlay the eye-tracking data on the dynamically changing radar display. ATCOs continuously switch their attention across different moving aircraft and update various flight settings such as heading or altitude either by voice commands to human 'pilots' in another room, who are controlling the simulation or by a simple computer interface within the radar screen as in our case.</p><p>Identification of AoIs. The collected data contains 3D gaze points as temporal positions on the radar screen (x, y,t). We applied our method to the entire training session of ∼85 minutes. As per our Step1 (see 3.2), the initial clustering of raw data points is performed using 1 minute time windows with a 50% overlap, as described in <ref type="figure" target="#fig_4">Figure 5</ref>. This window size was chosen since any action by an ATCO (assessing a flight, comparing it with the trajectory of another flight, or making an adjustment to a flight), should require less than 30 seconds. The clustering of the 320k samples is thus broken down into around 170 time windows, each of around 3600 samples. This generates our granularity level 0 in the hierarchy of merged clusters. The level 0 clustering removes very small clusters of eye points meaning that saccade infor-mation is removed leading to a quite small number of well defined clusters for each window at granularity level 0. Since the cluster computation in each time window is performed in parallel, the process is more efficient (see Section 3.6) in terms of processing time. The merging process, Step2 (see 3.3) was then applied for every pair of adjacent time windows leading to granularity level 1 containing 85 time windows of 1.5 minutes duration each. This merging process is repeated to level 2 with 43 time windows of 2.5 minutes duration each and so on to the highest level, in this case level 8, containing a single time window with the results of cluster merging for the whole data set. Initially, default values as described in Section 3.5 were used for clustering parameters. We browsed through level 0 of the hierarchy using our interactive control panel <ref type="figure" target="#fig_7">(Figure 9(c)</ref>) to view the results. We tested various values for the inconsistency factor using the inconsistency factor plot <ref type="figure" target="#fig_7">(Figure 9</ref>(d)) as described in <ref type="figure" target="#fig_6">Figure 8</ref> before settling upon a value of f = 3 for level 0 (eye data points). Among all the collected data sets that were used to test our method, we found this value to be sufficient to identify clusters with a high density of gaze points. For all higher levels we found f = 10 to be suitable, but in some cases where we felt the need for refinement of the clustering results, we updated the value of f using our plot, as shown in <ref type="figure" target="#fig_6">Figure 8</ref>(b) &amp; (c). The inconsistency factor relates to the type of data being investigated, hence it is regarded as an analyst-controlled parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Air traffic Controller A</head><p>We begin our investigation by browsing the lower levels of the hierarchy of identified AoIs. <ref type="figure" target="#fig_8">Figure 10</ref>(a) shows one time window at level 5, corresponding to the first 16 minutes of the recorded data, together with the identified AoIs for ATCO-A. Based on our in-context visualization, we identify that these AoIs correspond to the attention of ATCO-A on aircraft moving along two routes, from the top and bottom left of the display towards the centre of the screen. We quickly identify interesting but expected results such as that the ATCO frequently focusses on the entry points, where new flights appear on their display. This is to identify the new flight and to understand its characteristics. Typically less attention is paid to each flight as they continue towards the airport unless a flight's heading needs adjustment to avoid a proximity event. Significantly more attention is paid when the flights approach the glide path to the runway. We could confirm these findings from the first 16 minutes of the recorded video that showed aircraft approaching the airport via two different routes on the radar screen. Image frames from the captured video are displayed in the background of the 2D context window and on the base of the STC and controlled using a slider in the control panel. During these 16 minutes, the attention of ATCO-A switched between the two routes approaching the airport at the centre of screen but less attention was focussed on the airport since no aircraft had yet reached the glide path entry point. For the next 16 minutes, from Clearly the behaviour of ATCO-A has changed and their focus is more sparse across the display while that of ATCO-B remains similarly dense. (e) and (f) show the AoIs identified in the entire data set at the highest level in the hierarchy and the difference in behaviour is clear in sub-figure (f) where few, large AoIs remain. The STC representations (g) and (h), with colours propagated down from level 8 to level 4, make this difference in behaviour clear, as well as showing that there is a change in behaviour by both ATCOs roughly a third of the way through the scenario, in this case due to an abrupt influx of aircraft increasing the traffic density within the ATC sector.</p><p>16:30-32:30, similar AoI patterns were repeated when new incoming aircraft entered the display but more attention is now focussed on the airport at the centre of the display since flights have now begun arriving at the glide path. This can be seen in <ref type="figure" target="#fig_8">Figure 10</ref>(c). The interactive control panel facilitates the identification of these interesting (though expected) recurring patterns by browsing through our hierarchy. From these results we observe that our AoI identification approach identifies temporally varying AoIs which would have been otherwise lost when using traditional methods as discussed earlier (see <ref type="figure" target="#fig_0">Figure 2)</ref>.</p><p>At higher levels we observe a large "blob" at the centre of the radar display, corresponding to the airport, together with a more dense coverage of the display by the identified AoIs. Interestingly, ATCO-A exhibits a scanning strategy where their AoIs are typically fixed at certain geographical locations and there exists a clear recurring pattern of following the aircraft at different time intervals. At the topmost level of the identified AoIs we found an interesting pattern where there were clear regions on the screen that received repeated attention over the course of the entire training session (85 minutes). In this data set, the highest level (level 8) portrayed a good overview of the AoIs for the entire duration of the training session (see <ref type="figure" target="#fig_8">Figure 10</ref>(e)) and hence according to Step3 (see 3.4), the cluster labels were propagated down using the control panel from level 8 to level 4 as shown in <ref type="figure" target="#fig_8">Figure 10</ref>(g).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Air Traffic Controller B</head><p>The same training scenario was performed by ATCO-B. We begin our investigation by moving through the lower levels and, at the same time, analysed the recordings in a similar manner as for ATCO-A. For the first 16 minutes of the data recording, the attention of ATCO-B was switching between the two routes of the aircraft along their paths toward the airport at the centre of the screen. ATCO-B spent the next 16 minutes monitoring further incoming aircraft, as shown in <ref type="figure" target="#fig_8">Figure 10</ref>(b) &amp; (d). In the second of those example images, (d), the density of AoIs along the flight paths is clearly much higher, however, than for ATCO-A. This indicates ATCO-B is looking at the incoming flights far more frequently in this second time period. At level 6 in the hierarchy, where the size of the time window is 32 minutes, the temporally varying AoIs became close to saturated. A condition which only worsens at higher levels, as shown at the highest level in the hierarchy in <ref type="figure" target="#fig_8">Figure 10(f)</ref>. At these higher levels the information is lost and, hence, when using the 2D display, we can be restricted to working within shorter time windows at the lower levels of the hierarchy. The STC, however, has no such restriction and we can clearly see that our earlier hypothesis, that ATCO-B was revisiting each aircraft more frequently than ATCO-A, is correct. When examining the <ref type="figure" target="#fig_8">Figures 10(g,h)</ref> it is plain that the very large AoIs seen at the higher level are due to frequent visits to the different flights. For example in the green AoI of ATCO-B <ref type="figure" target="#fig_8">(Figure 10</ref> (h)), the STC display allows us to see that the merged AoI is created by many parallel AoIs moving with the flights as they are visited repeatedly. The STC display for ATCO-A shows a more sparse observation of the flights under their control, which leads to the better defined AoIs at higher levels. These differences show that ATCO-B has a very different strategy for maintaining 'situational awareness' of the flights under their control. They rely on rapid reacquisition of information while ATCO-A, relies more on memory. This may indicate different characteristics between the two ATCOs such as level of confidence, experience or, simply, quality of memory for details.</p><p>It is interesting to note that the STCs for the two ATCOs show greater similarity in the first third of the scenario run-time, with comparable levels of sparsity. After that point, however, the STC for ATCO-A remains similar in sparsity at the periphery but with greater focus on the airport. ATCO-B, however, begins to show greater focus on all parts of the airspace. One possible cause is that the number of aircraft has increased at this point and ATCO-A has continued to cope well with this increased traffic density, while ATCO-B is under increased pressure and may be coping less well. In summary, the hierarchical structure of merged AoIs in our system has allowed us to easily explore these long duration eye tracking data sets and identify the differing characteristics of ATCO-A and ATCO-B. It has allowed us to examine these differences between their working strategies in context and even to identify what may be a weakness in one of the two ATCOs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>The problem of interactive exploration of long-duration eye-tracking data presents some unique challenges which have been addressed through the developments described in this paper. In the introduction a number of goals for the work were laid out. This section describes how these goals have been met and to what degree.</p><p>Goal G1 -Identification of temporally varying areas of interest. We process the raw eye-tracking data to determine the spatial areas which are of interest to the subject at different times throughout the duration of the lengthy session and to be able to classify and simplify those areas so that the analyst can understand how the patterns of the subject's attention change over time. Through the application of the hierarchical merging of low-level clusters derived directly from the data the analyst is able to see which areas are active at which points in the session. By selecting colour information from a higher level in the hierarchy the analyst is able to see those areas which, although sometimes active infrequently and at well-separated periods in time, are strongly spatially related and so relate to the same activity. Examples seen in our case of the air traffic control scenario are the entry points for the aircraft, the areas where aircraft are marshalled towards the runway and the points where flight trajectories typically intersect en route to the airport or to the exit points from the sector.</p><p>Goal G2 -Visual analysis. The aim of Goal G2 was to provide a clear summary view of the important characteristics of the subject's behaviour while carrying out the task in question. This has clearly been met in the STC representation of the identified Areas of Interest which allows an immediate grasp of the ATCO's activity throughout the session. This is clearly demonstrated in the case study where the two STCs showing the behaviour of the two ATCOs carrying out the training scenario are shown to be so distinctly different and where the analyst is then able to explore the temporal shifts between different patterns of behaviour and compare between the two. Using the interactive control panel, the analyst can browse through different levels of the hierarchy and through the time windows within a hierarchy level to identify differences in a subject's behaviour between different periods of time during the session. We can also explore and compare the subject's behaviour without prior knowledge of events which are expected to cause behavioural changes, making the exploration entirely data driven and allowing changes due to unanticipated events to be more easily identified. The system implemented permits exactly that as the display in the STC allows the analyst to identify shifts in subject behaviour and then examine the periods before and after those shifts in a 2D display that is overlaid on the background stimulus. The addition of simple interaction methods to clamp the display to shorter periods of time around the identified shifts in behaviour might be employed in more dense displays where clutter becomes more of an issue.</p><p>Goal G3 -Comparison of multiple subjects. The aim of this goal was to enable comparison between different subjects to identify similarities and differences in their behaviour when undertaking identical or similar tasks. This goal has been partially met as shown in the case study where the comparison of two sessions carried out by two separate subjects undertaking a similar task has enabled the identification of periods of similar and distinctly different behaviour. There is a need to build on this, however, with a common identification of areas of interest across the different sessions. This could be achieved by using the set of areas identified within one session as a reference and cross comparing with one or more other sessions to identify those AoIs which are very similar between the sessions and those which differ significantly. The area matching methods used in the work so far (see Section 3.3) may be sufficient to carry out this matching. A common labelling of the identified matching AoIs between the different sessions would then allow automated visual analysis of patterns of attention that are similar and different between multiple subjects. Methods from geographic movement data analysis as described in Andrienko et al. <ref type="bibr" target="#b2">[3]</ref> for eye tracking data analysis can also be tried to compare data from multiple subjects. This has not yet been implemented but has been left as future work in combination with other planned additions.</p><p>Scalability and generalization. All the computations applied within each time window of a particular level of the hierarchy (see <ref type="figure" target="#fig_4">Figure 5</ref>) are independent and so can be executed in parallel making this approach applicable to very long duration eye tracking datasets. To further improve the efficiency of the algorithm, the ratio r xy for every edge of the MST described in Section 3.1 could be pre-computed and stored for a particular depth, d, for each eye gaze dataset. The interactive control panel (see <ref type="figure" target="#fig_7">Figure 9</ref>(c)) can be used to modify the parameters for calculation of AoIs and hence the system is readily generalizable for eye tracking datasets from different applications.</p><p>Overall the results described show that the methods developed in this work provide a superior representation of the data to that provided by 2D representations such as heat maps which provide none of the temporal information which is so important in these long-duration experiments. The simplification through the AoI identification, together with the STC display, provides direct access to both spatial and temporal variation in the subject's attention. Also, as discussed earlier, the method developed in this work to identify spatial AoIs provides superior results when compared with a traditional clustering approach for identifying AoIs such as mean shift algorithm (see <ref type="figure" target="#fig_0">Figure 2</ref>). Our user interface facilitates modifying the clustering parameters in realtime to achieve the desired clustering results and the identified AoIs are displayed in an in-context visualization where image frames from the captured stimulus can be displayed as a background to the current selected time region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS AND FUTURE WORK</head><p>A new approach for computing temporally varying AoIs in longduration eye-tracking data sets has been presented. The method can be parallelized and hence can be applied to very long duration eye-tracking sessions. A temporal subdivision of large data sets into overlapping time windows is employed, together with a clustering algorithm used on each time window. A hierarchical cluster merging step then identifies temporally varying AoIs by combining the cluster polygons from neighbouring time windows based on maximum connectivity in both the spatial and temporal domain. Our hierarchical approach helps the analyst to identify a desired level of temporal subdivision so that the local AoIs are not lost due to saturation of the eye-tracking data. Our aim in the future is to be able to automatically extract temporal sequences of AoIs visited by test subjects during long-duration experiments. Data mining techniques can then be used to uncover patterns of subjects' attention hidden in the long temporal sequences of AoIs which would not otherwise be easily recognized. The method described in this paper is the first step in a larger analysis pipeline being developed. This analysis pipeline will include not only the possibility to mine visual scan strategies of single subjects but also the possibility to automatically mine and compare patterns of visual behaviour among a group of test subjects participating in an eye-tracking experiment. This latter capability brings forward the interesting research challenge of how to create a common AoI alphabet for the several eye-tracking study participants. We also plan as future work to evaluate the proposed visual analysis system with the assistance of air-traffic control instructors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>An example of identifying AoIs using a mean shift clustering and our own method on a 4.5 minute time interval of a data set composed of 2D eye gaze points collected at 60Hz. Mean shift applied with (a) high (0.15) and (b) low quantile (0.01) values results in either too little and too much separation in the identified AoIs, respectively. (c) Temporally varying AoIs identified using our method display more balanced results.learning based methods for eye movement classification. For example, Tobii<ref type="bibr" target="#b0">[1]</ref> employs the Velocity-Threshold Identification (I-VT) algorithm. Grouping the consecutive fixations into cluster hulls can generate AoIs over time. Since the classification algorithms are not completely accurate, if there is a misclassification of a saccade as a fixation point, it can lead to erroneous clusters. Also, clusters identified in this way within a short time duration can end up with different cluster labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>window Merge cluster polygons of adjacent pair of time windows to a desired level of hierarchy From the desired level propagate cluster labels downwards</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>N d (x), with d &gt; 0, denotes the set of graph edges in a path of length 0 &lt; i ≤ d from node x. In other words, N d (x) is the set of edges in the neighbourhood of node x within a depth of at most d. Consider also the average, µ xy , and the standard deviation, σ xy , for the edges in N d (x) ∪ N d (y), for a given d &gt; 0, and then determine the ratio r xy = |ρ(x,y)−µ xy | σ xy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>(a) Delaunay triangulation (b) Minimum spanning Tree (MST) (c) Delete inconsistent edges Clustering of eye data points using Zahn's method<ref type="bibr" target="#b48">[49]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>The eye-tracking data is divided into overlapping time windows for clustering/temporally varying AoI identification. Cluster merging is performed in a hierarchical manner to identify a proper sub-division of time that does not saturate the dynamically changing AoI information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>9 Fig. 6 :</head><label>96</label><figDesc>Cost function to compute the strength of connection between two cluster polygons. (a) 20% of the area of x remains to be merged with y and so the cost is .2. (b) &amp; (c) 30% and 50% of the area of y remains to be merged with x. (d) 90% of the area of x remains to be merged with y and so the cost is 0.9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Cluster polygons from adjacent pairs of time windows are merged based on maximum connectivity/intersections among them. (a) Adjacency matrix of cluster polygons are computed based on a cost function and the MST is computed from the adjacency matrix. (b) &amp; (c) Inconsistency factor plot for the current MST can be tuned interactively to adjust the granularity of the clusters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 :</head><label>9</label><figDesc>The main views of our application interface: (a) Space-Time Cube displaying temporal aspects of the data and AoIs, (b) main 2D visualization window showing the raw data and/or AoIs, (c) interactive control panel, (d) interactive 2D plot displaying the ratio r xy , for each edge (x, y). An Image frame from the video capture of the stimulus is displayed as a background in (b) and as the base of the space-time cube in (a),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 :</head><label>10</label><figDesc>Identification of AoIs for two ATCOs working on the same training scenario, lasting ∼85 minutes. (a) and (b) show the AoIs detected for the first 16:30 of the data for each ATCO at level 5 of our merged hierarchy, while (c) and (d) show the same result for the second 16 minutes of the data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 :</head><label>11</label><figDesc>Radar display showing flights with lead lines representing their heading. Additional information about each flight (type, speed, altitude and rate of climb/descent) is displayed in a box adjacent to the aircraft.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thankÅsa Svensson for providing the dataset. This work is supported by the Swedish Research Council, grant number 2013-4939 and partly by RESKILL project funded by the Swedish Transport Administration, Swedish Maritime Administration and the Swedish Air Navigation Service Provider LFV.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tobii studio users manual</title>
		<idno>Version 3.4.5</idno>
	</analytic>
	<monogr>
		<title level="m">Tobii AB</title>
		<imprint>
			<date type="published" when="2016-01" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Visualization of time-oriented data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Aigner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Miksch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tominski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Visual analytics methodology for eye movement studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2889" to="2898" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Detection, tracking, and visualization of spatial event clusters for real time monitoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rinzivillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-D</forename><surname>Betz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Science and Advanced Analytics (DSAA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A review of temporal data visualizations based on space-time cube operations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dragicevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Archambault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hurter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carpendale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics conference on visualization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Challenges and perspectives in big eye-movement data visual analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blascheck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raschke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Big Data Visual Analytics (BDVA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">State-of-the-art of visualization for eye tracking data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blascheck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurzhals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raschke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EuroVis</title>
		<meeting>EuroVis</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2014</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Visualization of eye tracking data: A taxonomy and survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blascheck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurzhals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raschke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
		<editor>CGF. Wiley Online Library</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">AoI hierarchies for visual exploration of fixation sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blascheck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurzhals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raschke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Strohmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Biennial ACM Symposium on Eye Tracking Research &amp; Applications</title>
		<meeting>the Ninth Biennial ACM Symposium on Eye Tracking Research &amp; Applications</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="111" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Visual task solution strategies in tree diagrams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Höferlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raschke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PacificVis</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A dynamic graph visualization perspective on eye movement data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raschke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blascheck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on ETRA</title>
		<meeting>the Symposium on ETRA</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="151" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Eye Tracking and Visualization: Foundations, Techniques, and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">AoI rivers for visualizing dynamic eye gaze frequencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="281" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mean shift: A robust approach toward feature space analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="603" to="619" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visual attention: bottom-up versus top-down</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Egeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yantis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current biology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="850" to="852" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Analysing eye-tracking data: From scanpaths and heatmaps to the dynamic visualisation of areas of interest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Drusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">STHESCA</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">205</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robbins. Scanpath comparison revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Duchowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Driver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jolaoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Ramey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 symposium on ETRA</title>
		<meeting>the 2010 symposium on ETRA</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Aggregate gaze visualization with real-time heatmaps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Duchowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Orero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on ETRA</title>
		<meeting>the Symposium on ETRA</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="13" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Handbook of discrete and computational geometry. chap. Voronoi Diagrams and Delaunay Triangulations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fortune</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>CRC Press, Inc</publisher>
			<biblScope unit="page" from="377" to="388" />
			<pubPlace>Boca Raton, FL, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scanpath clustering and aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">I</forename><surname>Helfman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 symposium on ETRA</title>
		<meeting>the 2010 symposium on ETRA</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="227" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Where people look when watching movies: Do all viewers look at the same place?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Peli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in biology and medicine</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="957" to="964" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Etgraph: A graph-based approach for visual analytics of eye-tracking data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bixler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>D'mello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">What about people in regional science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hägerstrand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Regional Science Association Papers</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="6" to="21" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">icomp: a tool for scanpath visualization and comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heminghous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Duchowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2006 Research posters</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page">186</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Eye tracking: A comprehensive guide to methods and measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Holmqvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nyström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Andersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dewhurst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jarodzka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>OUP</publisher>
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards automated comparison of eye-tracking recordings in dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Bukenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ungewiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wörner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schiefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Rosenstiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kasneci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EUVIP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Analysis of eye movements with eyetrace</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schievelbein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aufreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Rosenstiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kasneci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Biomedical Engineering Systems and Technologies</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="458" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ISeeCube: Visual analysis of gaze data for video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurzhals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Heimerl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on Eye Tracking Research and Applications</title>
		<meeting>the Symposium on Eye Tracking Research and Applications</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gaze stripes: Image-based visualization of eye tracking data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurzhals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hlawatsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Heimerl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1005" to="1014" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visual analytics for mobile eye tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurzhals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hlawatsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Seeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="301" to="310" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Space-time visual analytics of eye-tracking data for dynamic stimuli</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurzhals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2129" to="2138" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">AoI transition trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurzhals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Graphics Interface Conference</title>
		<meeting>the 41st Graphics Interface Conference</meeting>
		<imprint>
			<publisher>Canadian Information Processing Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visual exploration of eye movement data using the space-time-cube</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Çöltekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-J</forename><surname>Kraak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Geographic Information Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="295" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Data visualization saliency model: A tool for evaluating abstract data visualizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Matzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Haass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Divis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="563" to="573" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Clustering of gaze during dynamic scene viewing is predicted by motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Mital</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="24" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Supporting exploration of eye tracking data: Identifying changing behaviour over long durations. BELIV &apos;16</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Muthumanickam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Forsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vrotsou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cooper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ACM</publisher>
			<biblScope unit="page" from="70" to="77" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Node overlap removal by growing a tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nachmanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nocaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bereg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holroyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Graph Drawing and Network Visualization</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="33" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A computer vision system for attention mapping in slam based 3d models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Paletta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Santner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lodron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Thallinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mayer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1305.1163</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention are independent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Van Der Leij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">G</forename><surname>Sligte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Lamme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Scholte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of vision</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="16" to="16" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Algorithms for defining visual regions-ofinterest: Comparison with eye fixations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Privitera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">W</forename><surname>Stark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="970" to="982" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Identifying fixations and saccades in eye-tracking protocols</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Salvucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2000 symposium on Eye tracking research &amp; applications</title>
		<meeting>the 2000 symposium on Eye tracking research &amp; applications</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="71" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Robust clustering of eye movement recordings for quantification of visual interest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Decarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 symposium on ETRA</title>
		<meeting>the 2004 symposium on ETRA</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Bayesian identification of fixations, saccades, and smooth pursuits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kasneci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Biennial ACM Symposium on ETRA</title>
		<meeting>the Ninth Biennial ACM Symposium on ETRA<address><addrLine>Charleston, SC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-03-14" />
			<biblScope unit="page" from="163" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Determining comprehension and quality of tv programs using eye-gaze tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sawahata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Komine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hiruma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Itou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Issiki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1610" to="1626" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">eSeeTrack -visualizing sequential fixation patterns. Visualization and Computer Graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Swindells</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="953" to="962" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Activitree: Interactive visual exploration of sequences in event-based data using graph similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vrotsou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="945" to="952" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Exploratory visual sequence mining based on pattern-growth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vrotsou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nordman</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2018.2848247</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Eye movements of large populations: II. deriving regions of interest, coverage, and similarity using fixation maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Wooding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="518" to="528" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Graph-theoretical methods for detecting and describing gestalt clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Zahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on computers</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="68" to="86" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
