<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GAN Lab: Understanding Complex Deep Generative Models using Interactive Visual Experimentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsuk</forename><surname>Kahng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Thorat</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Polo</roleName><forename type="first">Duen</forename><surname>Horng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernanda</forename><forename type="middle">B</forename><surname>Chau</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Viégas</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wattenberg</surname></persName>
						</author>
						<title level="a" type="main">GAN Lab: Understanding Complex Deep Generative Models using Interactive Visual Experimentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep learning</term>
					<term>information visualization</term>
					<term>visual analytics</term>
					<term>generative adversarial networks</term>
					<term>machine learning</term>
					<term>interactive experimentation</term>
					<term>explorable explanations</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Fig. 1. With GAN Lab, users can interactively train Generative Adversarial Networks (GANs), and visually examine the model training process. In this example, a user has successfully used GAN Lab to train a GAN that generates 2D data points whose challenging distribution resembles a ring. A. The model overview graph summarizes a GAN model&apos;s structure as a graph, with nodes representing the generator and discriminator submodels, and the data that flow through the graph (e.g., fake samples produced by the generator). B. The layered distributions view helps users interpret the interplay between submodels through user-selected layers, such as the discriminator&apos;s classification heatmap, real samples, and fake samples produced by the generator.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recent success in deep learning has generated a huge amount of interest from practitioners and students, inspiring many to learn about  <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b43">44]</ref>. For example, Karpathy's popular interactive demo <ref type="bibr" target="#b16">[17]</ref> enables users to run convolutional neural nets and visualize neuron activations, inspiring researchers to develop more interactive tools for deep learning. Another notable example is Google's Tensor-Flow Playground <ref type="bibr" target="#b35">[36]</ref>, an interactive tool that visually represents a neural network model and allows users to interactively experiment with the model through direct manipulation; Google now uses it to educate their employees about deep learning <ref type="bibr" target="#b30">[31]</ref>.</p><p>The rise of GANs and their compelling uses. Most existing interactive tools, however, have been designed for simpler models. Meanwhile, modern deep learning models are becoming more complex. For ex-ample, Generated Adversarial Networks (GANs) <ref type="bibr" target="#b8">[9]</ref>, a class of deep learning models known for their remarkable ability to generate synthetic images that look like natural images, are difficult to train and for people to understand, even for experts. Since the first GAN publication by Goodfellow et al. <ref type="bibr" target="#b8">[9]</ref> in 2014, GANs have become one of the most popular machine learning research topics <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18]</ref>. GANs have achieved state-of-the-art performance in a variety of previously difficult tasks, such as synthesizing super-resolution images based on low-resolution copies, and performing image-to-image translation (e.g., converting sketches to realistic images) <ref type="bibr" target="#b7">[8]</ref>. Key challenges in designing learning tools for GANs. At the high level, a GAN internally combines two neural networks, called generator and discriminator, to play a game where the generator creates "fake" data and the discriminator guesses whether that data is real or fake (both types of data are mixed together). A perfect GAN is one that generates fake data that is virtually indistinguishable from real data. A user who wishes to learn about GANs needs to develop a mental model of not only what the two submodels do, but also how they affect each other in its training process. The crux in learning about GANs, therefore, originates from the iterative, dynamic, intricate interplay between these two submodels. Such complex interaction is challenging for novices to recognize, and sometimes even for experts to fully understand <ref type="bibr" target="#b31">[32]</ref>. Typical architecture diagrams for GANs (e.g., <ref type="figure" target="#fig_0">Fig. 2</ref>, commonly shown in learning materials) do not effectively help people develop the crucial mental models needed for understanding GANs.</p><p>Contributions. In this work, we contribute:</p><p>• GAN Lab, the first interactive tool designed for non-experts to learn and experiment with GAN models, a popular class of complex deep learning models, that overcomes multiple unique challenges for developing interactive tools for GANs (Sect. 4). • Novel interactive visualization design of GAN Lab <ref type="figure">(Fig. 1)</ref>, which tightly integrates a model overview graph that summarizes GAN's structure ( <ref type="figure">Fig. 1A)</ref> as a graph, selectively visualizing components that are crucial to the training process; and a layered distributions view <ref type="figure">(Fig. 1B</ref>) that helps users interpret the interplay between submodels through user-selected layers (Sect. 6). GAN Lab's visualization techniques work in tandem to help crystalize complex concepts in GANs. For example, GAN Lab visualizes the generator's data transformation, which turns input noise into fake samples, as a manifold <ref type="figure">(Fig. 1</ref>, big box with purple border). When the user hovers over it, GAN Lab animates the input-to-output transformation ( <ref type="figure">Fig. 3)</ref> to visualize how the input 2D space is folded and twisted by the generator to create the desired ring-like data distribution, helping users more easily understand the complex behavior of the generator. • New interactive experimentation features for learning complex deep learning models, such as step-by-step training at multiple levels of abstraction for understanding intricate training dynamics (Sect. 7). The user can also interact with the training process by directly manipulating GAN's hyperparameters.    <ref type="figure">3</ref>. In GAN Lab, the generator 's non-trivial data transformation is visualized as a manifold, which turns input noise (leftmost) into fake samples (rightmost). GAN Lab animates the input-to-output transformation to help users more easily understand this complex behavior.</p><p>for real-time interactive tools, or large number of concurrent user sessions through the web. We overcome such practical challenges in deploying interactive visualization for deep learning by using Tensor-Flow.js Core, 1 an in-browser GPU-accelerated deep learning library recently developed by Google; the second author is a lead developer of TensorFlow.js Core. Anyone can access GAN Lab using their web browsers without the need for installation or specialized backend. GAN Lab runs locally on the user's web browser, allowing us to easily scale up deployment for our tool to the public, significantly broadening people's access to tools for learning about GANs. The source code is available in https://github.com/poloclub/ganlab/.</p><p>• Usage scenarios that demonstrate how GAN Lab can help beginners learn key concepts and training workflow in GANs, and assist practitioners to interactively attain optimal hyperparameters for reaching challenging equilibrium between submodels (Sect. 8).</p><p>VIS's central role in AI. We believe in-browser interactive tools developed by our VIS community, like GAN Lab, will play critical roles in promoting people's understanding of deep learning, and raising their awareness of this exciting new technology. To the best of our knowledge, our work is the first tool designed for non-experts to learn and experiment with complex GAN models, different from recent work in visualization for deep learning <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b42">43]</ref> which primarily targets machine learning experts. Our work joins a growing body of research that aims to use interactive visualization to explain complex inner workings of modern machine learning techniques. Distill, a new interactive form of journal, is dedicated to achieving this exact goal <ref type="bibr" target="#b28">[29]</ref>. We hope our work will help inspire even more research and development of visualization tools that help people better understanding artificial intelligence technologies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND: GENERATIVE ADVERSARIAL NETWORKS</head><p>This section presents a brief introduction of Generated Adversarial Networks, which will help ground our discussion in this paper. Generative Adversarial Networks (GANs) <ref type="bibr" target="#b8">[9]</ref> are a new class of unsupervised generative deep learning models that model data distributions. It can be used for generating multi-dimensional data distributions (e.g., an image is a multi-dimensional data point, where each pixel is a dimension). The model takes real samples and random vectors (i.e., random noise) as inputs and transforms the random vectors into fake samples that mimic the real samples. Ideally, the distribution of the fake samples will be indistinguishable from the real samples. The architecture of GANs is composed of two neural networks, called generator and discriminator, and is often represented as an abstracted data-flow graph as in <ref type="figure" target="#fig_0">Fig. 2</ref>. The generator, G, takes a random noise vector, z, as input and transforms it into a fake sample, G(z) (i.e., a multi-dimensional vector); the discriminator, D, which is a binary classifier, takes either a real or fake sample, and determines whether it is real or fake (D(x) represents the probability that x is real rather than fake).</p><p>A GAN model is iteratively trained through a game between the discriminator and generator. In GAN, two cost functions exist: the one for the discriminator measures the probability of assigning the correct labels to both real and fake samples (i.e., the sum of D(x) and 1 −D(G(z))); the other for the generator measures that for fake samples only (i.e., 1 − D(G(z))). The goal of the discriminator is to maximize its cost, but the goal of the generator is to minimize its cost, which introduces conflicts (i.e., zero-sum). Therefore, it has to play a mini-max game to find the optimum. Goodfellow et al. <ref type="bibr" target="#b8">[9]</ref> used an interesting analogy to explain how it works, where we can view the generator as a counterfeiter who makes fake dollar bills, and the discriminator as the police. If the police can spot the fake bills, that means the counterfeiter is not "good enough," so the counterfeiter carefully revises the bills to make them more realistic. As the discriminator (police) differentiates between real and fake samples, the generator (counterfeiter) can glean useful information from the discriminator to revise its generation process so that it will generate more realistic samples in the next iteration. And to continue to receive such helpful information, the generator keeps providing its updated samples to the discriminator. This iterative interplay between the two players leads to generating realistic samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Visualization for Understanding Deep Learning</head><p>Researchers and practitioners have written articles and deployed explorable web-based demos to help people learn about concepts in deep learning. One of the popular examples is Chris Olah's series of essays, 2 explaining mathematical concepts behind deep learning using visualizations. One article explains how neural networks transform and manipulate manifolds <ref type="bibr" target="#b27">[28]</ref>. Another popular example is Andrej Karpathy's collection of web-based demos developed using ConvNetJS, <ref type="bibr" target="#b2">3</ref> a lightweight JavaScript library for deep learning. His MNIST demo <ref type="bibr" target="#b16">[17]</ref> dynamically visualizes intermediate results, such as neuron activation.</p><p>Olah's articles and Karpathy's demos have inspired many researchers to develop interactive visualizations for novices to easily understand deep learning techniques <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b35">36]</ref>. A notable example is TensorFlow Playground <ref type="bibr" target="#b35">[36]</ref>, an interactive visualization tool for non-experts to train simple neural net models. Google has integrated it into its internal machine learning course for educating its employees; the course is now available to the public <ref type="bibr" target="#b30">[31]</ref>. Distill, a new online interactive journal, has recently been created and it is dedicated to interactive explanation of machine learning <ref type="bibr" target="#b28">[29]</ref>. The journal features a growing number of articles with interactive visualization <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b41">42]</ref>. However, most existing visualizations focus on simpler models. Modern deep learning models are much more complex, and we will present and discuss unique design challenges that stem from such complexity (Sect. 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Algorithm Visualization &amp; Explorable Explanations</head><p>Even before the surge of interest in deep learning techniques, researchers had studied how to design interactive visualization to help learners better understand the dynamic behavior of algorithms <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35]</ref>. These tools often graphically represent data structures and allow students to execute programs in a step-by-step fashion <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b34">35]</ref>. While many of these tools target algorithms covered in undergraduate computer science curricula, some specialized tools exist for artificial intelligence <ref type="bibr" target="#b1">[2]</ref>. As deep learning models are a category of specialized algorithms, when we design GAN Lab, we draw inspiration from the principles and guidelines proposed in the aforementioned related domains <ref type="bibr" target="#b33">[34]</ref>.</p><p>As web has become a central medium for sharing ideas and documents, many interactive experimentation tools implemented in JavaScript have been viewed as "explorable explanations," an umbrella term coined by Bret Victor in 2011 <ref type="bibr" target="#b38">[39]</ref>. He advocated the use of interactive explanations with examples to help people better understand complex concepts by actively engaging in the learning process. Many interactive tools instantiate this idea, including the ones showcased on the popular website with the same name (Explorable Explanations 4 ). These tools aim to help people actively learn through playing and interactive experimentation. GAN Lab aligns with this research theme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Visual Analytics for Deep Learning Models &amp; Results</head><p>Over the past few years, many visual analytics tools for deep learning have been developed <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b40">41]</ref>, as surveyed in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23]</ref>. Most were designed for experts to analyze models and their results. For instance, TensorFlow Graph Visualizer <ref type="bibr" target="#b42">[43]</ref> visualizes model structures, to help researchers and engineers build mental models about them. Many other tools focus to visually summarize model results for interpreting how models respond to their datasets. For example, CNNVis <ref type="bibr" target="#b20">[21]</ref> was designed for inspecting CNN model results; LST-MVis <ref type="bibr" target="#b36">[37]</ref> and RNNVis <ref type="bibr" target="#b26">[27]</ref> were for RNN models. A few other tools allow users to diagnose models during training. For example, DeepEyes <ref type="bibr" target="#b29">[30]</ref> does so through t-SNE visualizations. Two visual analytics tools have been developed for GANs <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b40">41]</ref>. DGMTracker <ref type="bibr" target="#b19">[20]</ref> allows experts to diagnose and monitor the training process of generative models through visualization of time-series data on data-flow graphs. GANViz <ref type="bibr" target="#b40">[41]</ref> helps experts evaluate and interpret trained results through multiple views, including one showing the distributions of real and fake image samples, for a selected epoch, using t-SNE. Different from all existing tools designed to help experts analyze models and results that we summarized above, we focus on non-experts and learners, helping them build intuition of the internal mechanisms of models, through interactive experimentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DESIGN CHALLENGES FOR COMPLEX DEEP LEARNING MODELS</head><p>Our goal is to build an interactive, visual experimentation tool for users to better understand GANs, a complex deep learning model. To design GAN Lab, we identified four key design challenges unique to GANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C1. [MODEL]</head><p>Complex model structures with submodels. The structures of modern deep learning models (including GANs) are complex; they often incorporate multiple base neural networks or deep learning models as submodels. For example, a GAN combines two neural nets: generator and discriminator; an image captioning model often consists of both CNNs and RNNs for translation between images and text <ref type="bibr" target="#b39">[40]</ref>. Effective visualization of such models calls for new strategies different from those designed for conventional models. For example, it is crucial to find the appropriate levels of visual abstraction for the models, as visualizing all low-level details will overwhelm users. Special visual design may be needed to help users interpret the intricate interplay between submodels (e.g., discriminator and generator).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C2. [DATA]</head><p>High-dimensional datasets. As deep learning models often work with large, high-dimensional datasets, visualizing their distributions would quickly create many traditional challenges well-studied in information visualization research <ref type="bibr" target="#b21">[22]</ref>. While we may use techniques like dimensionality reduction to partially address such issues, this could introduce additional complexities to the systems, potentially distracting users from their main goal of understanding how deep learning models work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C3. [TRAINING PROCESS]</head><p>Many training iterations until convergence. Deep learning models are trained through many iterations (i.e., at least thousands), introducing nontrivial challenges for developing interactive tools. First of all, as it takes time to converge, the tools need to keep providing users with information during training (e.g., progress), and users may also want to provide feedback to models (e.g., by changing hyperparameters). In addition, one popular feature used in many experimentation tools is a stepby-step execution of systems <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b32">33]</ref>, however, the definition of steps becomes different in training of complex models, because the training process consists of many iterations and each iteration also consists of the training of multiple submodels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C4. [DEPLOYMENT]</head><p>Conventional deep learning frameworks illfitted for multi-user, web-based deployment. Training deep learning models conventionally requires significant computing resources. Most deep learning frameworks written in Python or C++, like TensorFlow <ref type="bibr" target="#b0">[1]</ref>, typically run on dedicated servers that utilize powerful hardware with GPU, to speed up the training process. However, even with a powerful backend, they cannot easily support a large number of concurrent user sessions through the web, because each session requires significant computation resources. When combined, even a small number of concurrent sessions can bog down a powerful server. Off-loading computation to the end user is a possible solution, but conventional deep learning frameworks are not designed to support low-latency computation needed for real-time interactive tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DESIGN GOALS</head><p>Based on the identified design challenges in the previous section, we distill the following main design goals for GAN Lab, a novel interactive visualization tool for learning and experimenting with GANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G1</head><p>. Visual abstraction of models and data flow. To give an overview of the structure of complex models, we aim to create a visual representation of a model by selectively choosing and grouping low-level operations (and intermediate data) into high-level components (C1). It helps users visually track how input data are transformed throughout the models. For users to clearly examine the internal model training process and data flow, we would use low-dimensional datasets (C2). (Sect. 6.1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G2</head><p>. Visual analysis of interplay between discriminator and generator. As GANs internally use two different neural nets, it is important for users to understand how they work together, to get a holistic picture of the overall training process (C1). In response, we would like to enable users to examine and compare the visualizations of the model components to understand they affect each other to accomplish the generation tasks. (Sect. 6.2)</p><p>G3. Dynamic experimentations through direct manipulation of hyperparameters. We aim to let users dynamically play and experiment with models. To help users quickly understand the roles of many hyperparameters and control them (C3), we would like to design interactive interfaces which users can easily locate and manipulate the options. The users' actions are directly applied to the model training process. (Sect. 7.1)</p><p>G4. Supporting step-by-step execution for learning the training process in detail. Since the training process of deep learning models consists of many iterations and each iteration also consists of several steps, the step-by-step execution of models can greatly help novices to understand the training process (C3). To address this needs, we aim to design multiple ways to execute models in a step-by-step fashion by decomposing the training process into steps at multiple levels of abstraction. (Sect. 7.2)</p><p>G5. Deployment using cross-platform lightweight web technologies. To develop a tool that is accessible from multiple users without a need to use specialized powerful backend (C4), we would like to use web browsers both for training models and visualizing results. (Sect. 7.3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">VISUALIZATION INTERFACE OF GAN LAB</head><p>This section describes GAN Lab's interface and visualization design. <ref type="figure" target="#fig_2">Fig. 4</ref> shows GAN Lab's interface, consisting of multiple views. Using the control panel on top, users can run models and control the speed of training, which we describe in detail in the next section (Sect. 7). This section primarily describes the other three views that visualize models and trained results: (A) model overview graph view on the left (Sect. 6.1); (B) layered distributions view in the middle (Sect. 6.2); (C) metrics view on the right (Sect. 6.3). In the figure, 2D real samples are drawn from two Gaussian distributions. The user's goal is to train the model so that it will generate a similar distribution, by transforming 2D Gaussian noise using a neural net with a single hidden layer. Color scheme. In our visualization, we color real data green and fake data purple. We do not use a more traditional green-red color scheme, as we do not want to associate fake data with a negative value. For visualizing the discriminator, we use blue, a color unrelated to the color scheme chosen for coloring data. For visualizing the generator, we again use the color purple because the generated points are the fake points the model sees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Model Overview Graph: Visualizing Model Structure and Data Flow</head><p>The model overview graph view ( <ref type="figure" target="#fig_2">Fig. 4 at A)</ref> visually represents a GAN model as a graph, by selectively grouping low-level operations into high-level components and presenting data flow among them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstraction of Model Architecture as Overview Graph</head><p>The model overview graph visually summarizes the architecture of a GAN model. Instead of presenting all low-level operations and intermediate data (i.e., output tensors), it selectively represents high-level components and important intermediate data as nodes. Specificallly, nodes of the graph include two main submodels (i.e., generator and discriminator) and several intermediate data (e.g., fake samples). Each submodel, which is a neural network, is represented as a large box, and six data nodes are visualized as small boxes. This decision is based on our observation of how people draw the architecture of GANs <ref type="bibr" target="#b5">[6]</ref> (like <ref type="figure" target="#fig_0">Fig. 2</ref>). Users are often familiar with the structure of the basic neural networks and more interested in the overall picture and interplay between the two submodels. When we determine the position of the nodes, we place input data nodes on the left side of the submodels and output nodes on the right (for forward data flow). Then we draw edges where forward data paths are drawn from left to right and backward data paths, representing backpropagation, are drawn as two large backward loops (one for the discriminator and the other for the generator).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualization of Nodes in Overview Graph</head><p>We visualize the current states of models within the nodes in the graph for users to understand and monitor the training process.</p><p>Using 2D datasets to promote comprehension. One challenge in visualizing this information arises from the difficulty of visualizing a large number of high-dimensional data points. To tackle this issue, we decided that we limit our GAN models to generate two-dimensional data samples, while GANs often work with high-dimensional image data. This decision is mainly for helping users easily interpret visualization and focus to understand the internal mechanisms of the models. As many researchers identified, when designing interactive tools, it is even more desirable to focus on simpler cases <ref type="bibr" target="#b33">[34]</ref>. Visualization of two-dimensional space is easier for people to understand how data are transformed by the models than that of higher-or one-dimensional spaces: 3D or larger requires dimensionality reduction techniques that add more complexity to users and hinders their understanding.</p><p>Below we describe how we visualize each node. We show a miniaturized copy of each node's visualization from <ref type="figure" target="#fig_2">Fig. 4</ref> for easier referencing.</p><p>Real samples are what a GAN would like to model. Each sample, a two-dimensional vector, is represented as a green dot, where its x and y position represents the values of its two-dimensional data point. In this example, two Gaussian distributions exist: on the upper-left, and on the right.</p><p>Random noise, an input to the generator, is a set of random samples. In GAN Lab, noise can be either 1D or 2D. If it is a 1D value, data points are positioned in a line; if a 2D vector (which is default), positioned in a square box, as shown in the small figure on the right.</p><p>Fake samples are output produced the generator by transforming the random noise. Like real samples, fake samples are also drawn as dots, but in purple. For a welltrained GAN, the generated distribution should look indistinguishable from the real samples' distribution.</p><p>Generator, a neural net model, is a transformation function, G : R 2 → R 2 , that maps a 2D data point (i.e., random noise, z) to another 2D data point (i.e., fake sample, G(z)). We visualize the transformed results as a 2D manifold <ref type="bibr" target="#b27">[28]</ref>, as in the figure on the right. To draw this manifold, we first create a square grid (e.g., 20x20) for the random noise (see <ref type="figure" target="#fig_3">Fig. 5, leftmost)</ref> where each cell represents a certain noise range (e.g., {z = (z 1 , z 2 ) | 0.85 ≤ z 1 &lt; 0.90 ∧ 0.10 ≤ z 2 &lt; 0.15)}). We color each cell in purple, encode its probability density with opacity (i.e., more opaque means more samples in the cell). The generator G transforms the random noise into fake samples by placing them in new locations. To determine the transformation for the grid cells, we feed each cell's four corners into the generator, which returns their transformed positions forming a quadrangle (e.g., G(0.85, 0.10) = (0.21, 0.75), G(0.85, 0.15) = (0.24, 0.71), ...). Thus, the whole grid, now consisting of irregular quadrangles, would look like a warped version of the original regular grid. The density of each (warped) cell has changed. We calculate its new density by dividing the original density value (in the input noise space) by the area of the quadrangle. Thus, a higher opacity means more samples in smaller space. Ideally, a very finegrained manifold will look almost the same as the visualization of the fake samples. Our visualization technique aligns with the continuous scatterplots idea <ref type="bibr" target="#b2">[3]</ref> that generalizes scatterplots to continuous data by computing the density of data samples in the scatterplot space. To help users better understand the transformation, we show an animation of the square grid transitioning into the warped version (see <ref type="figure" target="#fig_3">Fig. 5</ref>), when users mouse over the generator node in the overview graph. Discriminator is another neural net model, which is a binary classifier, that takes a sample as input and determines whether it is real or fake by producing its prediction score (values from 0 to 1). We visualize the discriminator using a 2D heatmap, as in TensorFlow Playground <ref type="bibr" target="#b35">[36]</ref>. The background colors of a grid cell encode the prediction values (darker green for higher values representing that samples in that region are likely real; darker purple for lower values indicating that samples are likely fake). As a GAN approaches the optimum, the colors become more gray (as in the above figure), indicating the discriminator cannot distinguish fake examples from the real ones. Predictions are outputs from the discriminator. We place real or fake samples at their original positions, but their fill colors now represent prediction scores determined by the discriminator. Darker green indicates it is likely a real sample; darker purple likely a fake sample. In this example, most samples are predicted as fake, except for the ones on the upper left. Gradients for generator are computed for each fake sample by backpropagating the generator's loss through the graph. This snapshot of gradients indicates that how each sample should move to, in order to decrease the loss value. As a gradient represents a vector, we visualize it as a line starting from the position of each sample, where length indicates strength.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Layered Distributions: Visual Analysis of Interplay between Discriminator and Generator</head><p>In complex models like GANs, it is a key to understanding relationships among several elements of the models. For example, users may want to check how the distribution of fake samples are similar to those of real samples. Although users can perform a side-by-side comparison of the two different nodes on the model overview graph, this task would be greatly improved when they are overlapped in the same coordinates.</p><p>To help visually analyzing relationships among multiple components, we create a layered distributions view <ref type="figure" target="#fig_2">(Fig. 4 at B</ref>) that presents a large canvas showing the visual representations of the nodes in the model overview graph as multiple layers. The layers can be turned on or off using toggle switches. We do not intend to visualize all layers, as it is overwhelming to users and it is much more effective to include only the useful information for particular tasks. The view currently supports six layers. All layers, except the one for the real samples' density contour, are magnified versions of the visual representations of the graph nodes we described in the previous subsection (Sect. 6.1). The layers are:</p><p>• Real samples (green dots)</p><p>• Real samples' density contour (see <ref type="figure" target="#fig_5">Fig. 7</ref> Useful combinations of layers. By selecting which visualizations to be included in the canvas, users can visually analyze the state of the models and the interplay between discriminator and generator, from multiple angles. We describe three example combinations that support multiple analysis tasks. First, <ref type="figure" target="#fig_4">Fig. 6</ref> illustrates that the discriminator may be visually interpreted by comparing the samples' positions with grid's background colors. Here, the discriminator is performing well, as most real and fake samples lie on its classification's green and purple regions, respectively. The second example in <ref type="figure" target="#fig_5">Fig. 7</ref> illustrates how users may visually evaluate how well the distribution of fake samples matches that of the real samples. It helps users to determine whether the two distributions are similar or not, which is the main goal of GANs. The last example in <ref type="figure" target="#fig_6">Fig. 8</ref> shows how the view can help users understand the interplay between discriminator and generator. Fake samples' gradient directions point to the classification's green regions, meaning that the generator leverages information from the discriminator to make fake samples less distinguishable from the real ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Metrics: Monitoring Performances</head><p>The metrics view <ref type="figure" target="#fig_2">(Fig. 4 at C)</ref> shows a number of line charts that track several metric values changing as the training promises. GAN Lab currently provides two classes of metrics. The first kind is the loss values of the discriminator and generator, which are helpful for evaluating submodels and comparing their strengths. The second kind of metrics is for evaluating how similar the distributions of real and fake samples are. GAN Lab provides Kullback-Leibler (KL) and Jensen-Shannon (JS) divergence values <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b37">38]</ref> by discretizing the 2D continuous space (via the grid). Formally, the KL divergence value is defined as KL(P real ||P fake ) = − ∑ i P real (i) log P fake (i) P real (i) , where P real (i) is the probability density of the real samples in the i-th cell, calculated by dividing the number of real samples in the i-th cell by the total number of real samples; P fake (i) is similarly defined for the fake examples. We decided to use these measures, among others, because they are some of the most commonly used approaches for comparing distributions and they do not incur heavy in-browser computation overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">INTERACTIVE EXPERIMENTATION</head><p>This section describes how users can interactively experiment with GAN models using GAN Lab.</p><p>Basic workflow. Clicking the play button, located on the top of the interface, starts running the training of a GAN model and dynamically updates the visualizations of intermediate results every n epochs (a.k.a., iterations). This helps users keep track of the model's training and examine how they evolve. Users can pause the training by clicking the pause button (the play button changes to pause button during training).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Direct Manipulation of Hyperparameters</head><p>GAN Lab is designed for users to directly manipulate model's training as easy as possible. When users click the editing icon on the right side of the label for the model overview graph view, several up/down buttons or dropdown menus, which controls the model's hyperparameters, are shown (see <ref type="figure" target="#fig_2">Fig. 4</ref>). Each item is located near its relevant submodel or data node for users to easily locate it. Users can directly change the values using the buttons or dropdown menus, and the user's actions (e.g., increasing learning rate) are immediately applied to the model training process, except for some of the submodel-specific options (e.g., number of hidden layers), and the effects of this change will be visualized, as the training further progresses. This would greatly help users understand how these hyperparameters affect the model training process.  <ref type="bibr" target="#b4">5</ref> • Noise dimension (e.g., 1D, 2D) and distribution type (e.g., uniform, Gaussian) GAN Lab also allows users to pick a distribution of real samples using the drop-down menu that currently implements five examples (e.g., ring). Users can also specify a new distribution by drawing one on a canvas with brush, as illustrated in <ref type="figure" target="#fig_7">Fig. 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Step-by-Step Model Training at Multiple Levels</head><p>GAN Lab supports step-by-step training at multiple levels of abstraction for understanding intricate training dynamics. The step-by-step execution of systems is one of the useful ways for learners to understand how they work <ref type="bibr" target="#b34">[35]</ref>, however, training of GANs consists of thousands of iterations and each iteration also consists of several steps (as illustrated in <ref type="figure" target="#fig_8">Fig. 10</ref>). To address this problem, we decompose the training process into steps in multiple levels: epoch-, submodel-, and component-level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1">Manual Step Execution in Epoch-Level</head><p>Users can train a model for only one epoch, by clicking a button once. This epoch-level step execution is designed to help users track the training process to see how models update to find the optimum state through iterations. To use this feature, a user first clicks the step icon on top, which will shows three buttons. The last button ("Both") represents the training for one epoch. We describe the other two buttons' usage next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2">Manual Step Execution in Submodel-Level</head><p>A single epoch consists of training of a discriminator and generator, as illustrated in <ref type="figure" target="#fig_8">Fig. 10</ref>. GAN Lab allows users to update only the discriminator or generator. The experimentation of training only one of the two submodels is effective for users to understand how they work differently. For example, clicking the button for the discriminator changes the background grid while preserving the positions of fake samples. On the other hand, clicking the discriminator button moves the fake samples while fixing the background grid. To use this feature, users click the step icon first, then the three buttons will be shown. The first button is for training the discriminator; the second button is for the generator; and the last button is for training both submodels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.3">Slow-Motion Mode in Component-Level</head><p>GAN Lab also provides the slow-motion mode, designed to help novices learn how each component of the model works to make updates within each epoch. It works differently from the manual step execution described in the two previous paragraphs. When users turn on this mode by clicking the icon on top during training, it slows down the speed of training. In addition, two similar lists of five steps are presented: one for updating the discriminator and the other for the generator, as depicted in <ref type="figure">Fig. 11</ref>. The five steps include (1) running the generator; (2) running the discriminator; (3) computing discriminator or generator loss; (4) computing gradients; and (5) updating the discriminator or generator. For every few seconds, it moves to the next step highlighting the corresponding model components with textual descriptions. For example, each of the five steps for the discriminator is highlighted one after another. At the same time, the whole training loop for the discriminator is also highlighted (i.e., edges colored by blue). Once the five steps are completed, it proceeds to the training of the generator, highlighting the training loop for the generator (i.e., purple edges) and executing its five steps. By following these training paths, users can learn how every component is used in training GANs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Browser-based Implementation for Deployment</head><p>GAN Lab is an open-source, web-based visualization tool. Anyone can access it using their modern web browsers without the need for installation or specialized backend. The demo is currently available at https://poloclub.github.io/ganlab/.</p><p>The tool is implemented in HTML and TypeScript (a typed version of JavaScript) with a few open-source JavaScript libraries: Ten-sorFlow.js 6 is used for training and running models, which we will elaborate in detail in the next paragraph; Polymer 7 is used for building web applications; and D3.js 8 is used to visualize the model overview graph and layered distributions. The source code is available in https://github.com/poloclub/ganlab/.</p><p>Using TensorFlow.js for model building and training. GAN Lab runs locally on user's web browsers by using TensorFlow.js Core (formerly known as deeplearn.js), an in-browser GPU-accelerated deep learning library, developed by Google. The TensorFlow.js library uses WebGL to efficiently perform computation on browsers, required for training deep learning models. Not only does it enable rapid experimentation of the models, but also allows us to easily scale up deployment for the public. While most other implementations of GANs that use Python or other server-side languages would backfire when multiple users train models concurrently, our GAN models are trained in JavaScript, which means that that the models and their visualizations run locally on web browsers, enabling us to significantly broaden people's access to GAN Lab for learning about GANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">USAGE SCENARIOS</head><p>This section describes two example usage scenarios for GAN Lab, demonstrating how it may promote user learning of GANs. The scenarios highlight: (1) how beginners may learn key concepts for GANs by experimenting with the tool's visualizations and interactive features (Sect. 8.1); (2) how the tool may help practitioners discover advanced inner-workings of GANs, and how it can assist them to interactively attain optimal hyperparameters for reaching equilibrium between submodels (Sect. 8.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Beginners Learning Concepts and Training Procedure</head><p>Consider Alice, a data scientist at a technology company, who has basic knowledge about machine learning. Recently, she has started to learn about deep learning, and a few of the introductory articles she has been reading mention GANs. Excited about their potential, she wishes to use GAN Lab to interactively learn GANs.</p><p>Becoming familiar with basic workflow. When Alice launches GAN Lab in her web browser, she sees the model overview graph, which looks like a GAN architecture diagram that she has seen in her articles. By default, real samples are drawn from a 2D distribution that resembles a line. She clicks the play button on the tool bar. During the training, the movement of the fake samples in the layered distribution view attracts her attention. They keep moving towards the real samples.</p><p>Using the slow-motion mode for tracking the training procedure. Alice is aware that discriminator and generator take turns to train, but she is unsure of what that means. To see how training progresses, Alice clicks the slow-motion icon (Sect. 7.2.3) to enter the <ref type="figure">Fig. 11</ref>. The slow-motion mode slowly executes the model training process in a component level, in a step-by-step fashion. The steps are grouped into two lists, one for discriminator and the other for generator, each consisting of five steps. <ref type="figure" target="#fig_0">Fig. 12</ref>. Experimenting with manual step execution, to understand the interplay between discriminator and generator.</p><p>slow-motion training mode, which slows down the speed of training, and presents two lists of training steps, one for the discriminator, and another for the generator (see <ref type="figure">Fig. 11</ref>). She notices that in for every epoch, the discriminator is trained first, then the generator follows. The two models' training sequences seem very similar, but she discovers several key differences. For example, she is able to find that while discriminator's loss is computed by using both real and fake samples, only fake samples are used when computing the generator's loss.</p><p>Understanding the different roles of discriminator and generator with the manual step execution. While the slow-motion mode has helped her better understand the steps of the training process, Alice wonders how the discriminator and generator play a "game" to generate data distributions. To analyze the different effects for the discriminator and the generator, she would like to experiment with the two submodels using the manual step-by-step execution feature. She clicks the button (Sect. 7.2.1) to update the generator. Her initial clicks cause the fake samples to move towards the real samples, but as she clicks a few more times, the fake samples "overshoot," no longer matching real samples' distribution ( <ref type="figure" target="#fig_0">Fig. 12, top row)</ref>. She now realizes that the fake samples have moved towards regions where the colors of background grid cells are green, not directly towards the real samples. This leads Alice to hypothesize that training the discriminator is necessary for the generator to produce better fake samples. So, she switches to only training the discriminator, which does not reposition the fake samples, but the grid colors update <ref type="figure" target="#fig_0">(Fig. 12, second row)</ref> to correct a decision boundary that separates the real and fake samples. She believes that this new boundary helps guide the fake samples towards desirable regions where the real samples are located. This experiment helps her realize that updating both submodels is important for generation of better fake samples. Now she clicks the buttons for updating the discriminator and generator alternatively, which successfully creates a fake distribution that matches the real distribution. That is, the discriminator cannot distinguish between real and fake samples. <ref type="figure" target="#fig_0">(Fig. 12</ref>, last row).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Practitioners Experimenting with Hyperparameters</head><p>One of GAN Lab's key features is the interactive, dynamic training of GANs. Experimentation using GAN Lab could provide valuable practical experience in training GAN models even to experts. Consider Bob, a machine learning engineer at a technology company.</p><p>Guiding models to find the optimum. Bob launches GAN Lab and starts the training process. Fake samples quickly move towards real samples. However, as the training progresses, he notices that the fake samples oscillate around the real samples. Based on his previous experience, he believes this indicates that the learning rates may be set too high. He first decreases the value for the discriminator by using the dropdown menu, but the amount of oscillation becomes more severe. By checking the interface, he quickly realizes that there are two learning rates in GANs, so he reverts its value and decreases the generator's learning rate. After a few more iterations, the oscillation subsides and the distribution of the fake samples almost matches that for the real samples. This experimentation helps him understand the importance in balancing the power between the discriminator and generator.</p><p>Understanding equilibrium between discriminator and generator. Bob wonders what would happen if he perturbs the equilibrium between the discriminator and generator. That is, what if either submodel overpowers its complement. Looking into the model overview graph, he finds that some other hyperparameters also come in matched pairs, such as the number of training loops, one for the discriminator and the other for the generator. Originally, both numbers are set to 1 (i.e., the submodels run one training epoch in alternate sequence). Bob decides to increase discriminator's loop count 3 (i.e., 3 discriminator epochs, followed by 1 generator epoch, followed, and repeat). To his surprise, this "unbalanced" epoch setting (3 vs. 1) causes GAN to converge faster. Comparing this "unbalanced" setting with the original "balanced" (1 vs. 1) setting, Bob starts to understand that a more powerful discriminator can indeed accelerate training, because a stronger discriminator leads to stronger gradients for the generator, which in turns more quickly move the fake samples towards the real distribution, thus faster training convergence.</p><p>Exploring mode collapse. Bob would like to train a GAN to work with more complex data distributions. He picks one distribution that consists of three disjoint dense regions. He increases the number of layers for both the generator and discriminator, then clicks the play button. After a few seconds, all fake samples seem to have disappeared, as he can only see real samples. He temporarily hides the real samples (by toggling their visibility), thinking that they may be covering the fake samples. Then, he realizes that all fake samples have collapsed into a single point (as shown in <ref type="figure" target="#fig_9">Fig. 13</ref>). He does not know why this happens, and wonders if it is due to his hyperparameter choices. So he experiments with several other sets of hyperparameters, and observes the pattern that this happens more often when the generators and discriminators are set to use more layers and neurons. He consults the literature for possible causes, and learns that this is in fact a wellknown problem in GANs, called mode collapse, whose exact cause is still an active research topic <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26]</ref>. Bob's observation through GAN Lab motivates him to study new variants of GANs, which may overcome this problem <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">INFORMED DESIGN THROUGH ITERATIONS</head><p>The current design of GAN Lab is the result of 11 months of investigation and development through many iterations. Below we share two key lessons learned from our experience.</p><p>The model overview graph is a crucial and effective feature that helps users develop mental models for GANs. Our early design ( <ref type="figure" target="#fig_2">Fig. 14)</ref> did not include the overview graph. Instead, it displayed a long list of hyperparameters. While that design had all the necessary features for training GANs interactively, pilot users, including machine learning experts, commented that the tool was difficult to use and to interpret. The main reason is that, without an overview, users had to develop mental models for GANs (in their heads) to keep track of how the larger number of hyperparameters map to the different model components. This finding prompted us to add the model overview graph, inspired from common architecture diagrams for GANs, which helps users build mental models for the training process of GANs <ref type="bibr" target="#b23">[24]</ref>.</p><p>Animating the generator's transformation <ref type="figure" target="#fig_3">(Fig. 5</ref>) was helpful in helping users interpret the manifold visualization. Our early version only showed the transformed manifold (e.g., <ref type="figure" target="#fig_3">Fig. 5, rightmost)</ref>. However, many users were puzzled by what they saw because, the manifold could be so severely distorted that they could not tell what its original shape was (a uniform 2D grid), thus they could not make the connection to realize that the manifold visualization was indeed representing the generator's output. We though about adding text to the interface to explain the manifold, but as GAN Lab is intended to be used as a standalone tool, we would like to keep the visual design compact, and we wanted to include textual descriptions only when necessary. Thus, we came up with the idea of visually explaining the transformation as an animated transition, which was immediately clear to all users. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">LIMITATIONS AND FUTURE WORK</head><p>Transferring user knowledge to higher dimensions. Our main decision to use 2D datasets is to promote comprehension <ref type="bibr" target="#b33">[34]</ref>. Through our tool, with 2D datasets, users can gain important knowledge about the overall training process of GANs, and specific details, such as how model components interact over time, how data flow through components, and how losses are recomputed to iteratively update components. These important concepts and knowledge are transferable to practical use cases of GANs where higher dimensional data are used (e.g., images). However, it remains an open research problem whether certain behaviors (e.g., mode collapse) that users may observe when experimenting with 2D datasets would be easily reproducible in higher dimensional datasets, where the larger number of parameters would lead to more-complex interactions and less-predictable results. We plan to conduct studies to develop deeper understanding of how and when such correspondence or mismatch may occur.</p><p>Supporting image data. To extend GAN Lab to support image data, some modifications and optimizations will be needed. Training on image data is often time consuming. To speed this up, pre-trained models may be provided to users so they can skip the earlier training steps. As for visual design, projection methods (e.g., t-SNE) may be used to replace some views in GAN Lab to visualize the distribution of generated image samples <ref type="bibr" target="#b40">[41]</ref>.</p><p>Speed and scalability. GAN Lab leverages TensorFlow.js to accelerate GAN training for browser-based deployment. For models with many parameters, this can be time consuming. In the short term, we believe rapid advances in JavaScript and hardware will shorten this by a good amount. A longer-term challenge to overcome is browsers' inability to render visualization and perform computation at the same time (i.e., single-threaded). Developers need to strike a good balance in planning and interleaving these actions, to maximize model computation speed and visual responsiveness.</p><p>Supporting more GAN variants. While GAN Lab currently implements a few different loss functions, other GAN variants exist <ref type="bibr" target="#b11">[12]</ref>. Through open-sourcing GAN Lab, we look forward to seeing the community to build on GAN Lab to implement more variants, enabling users to interactively and visually compare them, easing the challenges in evaluating GANs <ref type="bibr" target="#b7">[8]</ref>. Some variants may require minor design changes of the interface (e.g., adding new nodes to overview graph).</p><p>In-depth evaluation of educational benefits. Longitudinal studies of GAN Lab will help us better understand how it helps with learning of GANs. It would be particularly valuable to investigate how different types of users (e.g., students, practitioners, and researchers) would benefit from the tool.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>A graphical schematic representation of a GAN's architecture commonly used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.</head><label></label><figDesc>Fig. 3. In GAN Lab, the generator 's non-trivial data transformation is visualized as a manifold, which turns input noise (leftmost) into fake samples (rightmost). GAN Lab animates the input-to-output transformation to help users more easily understand this complex behavior.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>The GAN Lab interface integrates multiple views: A. The model overview graph summarizes a GAN model's structure as a graph, with nodes representing the submodels, and the data that flow through the graph; B. The layered distributions view overlays magnified versions of the graph's component visualizations, to help users more easily compare and understand their relationships; C. The metrics view presents line charts that track metric values over the training process. Users start the model training by clicking the play button on menu bar. The three views are dynamically updated, as training progresses. In this example, real samples are drawn from two Gaussian distributions, and the generator, consisting of a single hidden layer with 14 neurons, has created samples whose distribution is quite similar to that of the real samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Visualization of generator's transformation. When users mouse over the generator node, an animation of the square grid transitioning into a warped version is played.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>The discriminator's performance can be interpreted through the layered distributions view, a composite visualization composed of 3 layers selected by the user: Real samples, Fake samples, and Discriminator's classification. Here, the discriminator is performing well, since most real samples lies on its classification surface's green region (and fake samples on purple region).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Evaluating how well the distribution of fake samples matches that of real samples by turning on real samples' density contour and fake samples in the layered distributions view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Example of understanding the interplay between discriminator and generator using the layered distributions view. Fake samples' movement directions are indicated by the generator's gradients (pink lines), based on those samples' current locations and the discriminator's current classification surface (visualized by background colors).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Users can create real samples by drawing their distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Training typically involves of thousands of epochs (iterations). Each epoch includes training both discriminator and generator. GAN Lab supports step-by-step model training at different abstraction levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 13 .</head><label>13</label><figDesc>Mode collapse, a common problem in GANs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 14 .</head><label>14</label><figDesc>Early design of GAN Lab did not include a model overview graph that helps users develop mental models for GANs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>• MinsukKahng  and Duen Horng (Polo) Chau are with Georgia Institute of Technology. E-mail: {kahng | polo}@gatech.edu. • Nikhil Thorat, Fernanda B. Viégas, and Martin Wattenberg are with Google Brain. E-mail: {nsthorat | viegas | wattenberg}@google.com. TVCG.201x.xxxxxxx this technology. Visual and interactive approaches have successfully been used to describe concepts and underlying mechanisms in deep learning</figDesc><table /><note>Manuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication xx xxx. 201x; date of current version xx xxx. 201x. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org. Digital Object Identifier: xx.xxxx/</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">TensorFlow.js (https://js.tensorflow.org) was formerly deeplearn.js.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Colah's blog, http://colah.github.io 3 ConvNetJS, https://cs.stanford.edu/people/karpathy/convnetjs/ 4 Explorable Explanations, http://explorabl.es/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">In training of GANs, for every epoch, the discriminator and generator are trained by turns. Goodfellow et al.<ref type="bibr" target="#b8">[9]</ref> suggested that the discriminator can be updated k more times in practice, and GAN Lab enables to adjust this k value.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">TensorFlow.js, https://js.tensorflow.org/ 7 Polymer, https://www.polymer-project.org/ 8 D3.js, https://d3js.org/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Shan Carter, Daniel Smilkov, Google Big Picture Group and People + AI Research (PAIR), Georgia Tech Visualization Lab, and the anonymous reviewers for their feedback. This work was supported in part by NSF grants IIS-1563816, CNS-1704701, and TWC-1526254.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<title level="m">TensorFlow: Large-scale machine learning on heterogeneous distributed systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Designing CIspace: Pedagogy and usability in a learning environment for AI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Arksey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Conati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mackworth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maclaren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Poole</surname></persName>
		</author>
		<idno type="DOI">10.1145/1067445.1067495</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Annual SIGCSE Conference on Innovation and Technology in Computer Science Education</title>
		<meeting>the 10th Annual SIGCSE Conference on Innovation and Technology in Computer Science Education</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="178" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Continuous scatterplots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bachthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2008.119</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1428" to="1435" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Do convolutional neural networks learn class hierarchy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bilal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2017.2744683</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="152" to="162" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Using artificial intelligence to augment human intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nielsen</surname></persName>
		</author>
		<idno type="DOI">10.23915/distill.00009</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative adversarial networks: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Arulkumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Bharath</surname></persName>
		</author>
		<idno type="DOI">10.1109/MSP.2017.2765202</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="65" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Why momentum really works</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<idno type="DOI">10.23915/distill.00006</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.00160</idno>
		<title level="m">NIPS 2016 tutorial: Generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Online python tutor: embeddable web-based program visualization for cs education</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.1145/2445196.2445368</idno>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the 44th ACM Technical Symposium on Computer Science Education</title>
		<meeting>eeding of the 44th ACM Technical Symposium on Computer Science Education</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="579" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An interactive node-link visualization of convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-27857-5_77</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Symposium on Visual Computing</title>
		<meeting>the 11th International Symposium on Visual Computing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="867" to="877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The GAN Zoo: A list of all named GANs! https:// deephunt.in/the-gan-zoo-79597dc8c347</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hindupur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2018" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visual analytics in deep learning: An interrogative survey for the next frontiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pienta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2018.2843369</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">What you see is what you code: A &quot;live&quot; algorithm development and visualization environment for novice learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Hundhausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Brown</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jvlc.2006.03.002</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Languages &amp; Computing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="47" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A meta-study of algorithm visualization effectiveness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Hundhausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Stasko</surname></persName>
		</author>
		<idno type="DOI">10.1006/jvlc.2002.0237</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Languages &amp; Computing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="259" to="290" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ActiVis: Visual exploration of industry-scale deep neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kalro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2017.2744718</idno>
		<idno>doi: 10. 1109/TVCG.2017.2744718</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<ptr target="https://cs.stanford.edu/people/karpathy/convnetjs/demo/mnist.html.Accessed" />
		<title level="m">ConvNetJS MNIST demo</title>
		<imprint>
			<biblScope unit="page" from="2018" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Answer to &quot;what are some recent and potentially upcoming breakthroughs in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno>2016. Ac- cessed: 2018-03-31</idno>
		<ptr target="http://qr.ae/TU1FeA" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Divergence measures based on the shannon entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1109/18.61115</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="151" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Analyzing the training processes of deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2017.2744938</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="87" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Towards better analysis of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2016.2598831</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="100" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visualizing high-dimensional data: Advances in the past decade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maljovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-T</forename><surname>Bremer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pascucci</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2016.2640960</idno>
		<idno>doi: 10. 1109/TVCG.2016.2640960</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1249" to="1268" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards better analysis of machine learning models: A visual analytics perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.visinf.2017.01.006</idno>
	</analytic>
	<monogr>
		<title level="j">Visual Informatics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="56" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mental models, visual reasoning and interaction in information visualization: A top-down perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2010.177</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Thank Graphics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="999" to="1008" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Smolley</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.304</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2813" to="2821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unrolled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Understanding hidden memories of recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Visual Analytics Science and Technology</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<ptr target="http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/" />
	</analytic>
	<monogr>
		<title level="m">Neural networks, manifolds, and topology</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2018" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Research debt. Distill</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carter</surname></persName>
		</author>
		<idno type="DOI">10.23915/distill.00005</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">DeepEyes: Progressive visual analytics for designing deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pezzotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Höllt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>Lelieveldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eisemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vilanova</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2017.2744358</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="108" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Machine learning crash course</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenberg</surname></persName>
		</author>
		<ptr target="https://developers.googleblog.com/2018/03/machine-learning-crash-course.html" />
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2018" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improved techniques for training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Effective features of algorithm visualizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Saraiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Shaffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Mccrickard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>North</surname></persName>
		</author>
		<idno type="DOI">10.1145/971300.971432</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th SIGCSE Technical Symposium on Computer Science Education, SIGCSE &apos;04</title>
		<meeting>the 35th SIGCSE Technical Symposium on Computer Science Education, SIGCSE &apos;04</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="382" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Interactive visualization for the active learning classroom</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brown</surname></persName>
		</author>
		<idno type="DOI">10.1145/1227504.1227384</idno>
		<idno>doi: 10. 1145/1227504.1227384</idno>
	</analytic>
	<monogr>
		<title level="j">ACM SIGCSE Bulletin</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="208" to="212" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Algorithm visualization: The state of the field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Shaffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J D</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Akbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Edwards</surname></persName>
		</author>
		<idno type="DOI">10.1145/1821996.1821997</idno>
		<idno>doi: 10. 1145/1821996.1821997</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computing Education (TOCE)</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Directmanipulation visualization of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Smilkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Viegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Visualization for Deep Learning at the 33rd International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">LSTMVis: A tool for visual analysis of hidden state dynamics in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2017.2744158</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="667" to="676" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A note on the evaluation of generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Explorable explanations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Victor</surname></persName>
		</author>
		<ptr target="http://worrydream.com/ExplorableExplanations/" />
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2018" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298935</idno>
		<idno>doi: 10. 1109/CVPR.2015.7298935</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">GANViz: A visual analytics approach to understand the adversarial game</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2018.2816223</idno>
		<idno>doi: 10. 1109/TVCG.2018.2816223</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1905" to="1917" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">How to use t-SNE effectively. Distill</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vigas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="DOI">10.23915/distill.00002</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Visualizing dataflow graphs of deep learning models in TensorFlow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wongsuphasawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Smilkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2017.2744878</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Understanding neural networks through deep visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning Workshop at the 31st International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
