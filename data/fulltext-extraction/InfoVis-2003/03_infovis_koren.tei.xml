<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visualization of Labeled Data Using Linear Transformations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
							<email>yehuda@wisdom.weizmann.ac.il</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liran</forename><surname>Carmel</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Applied Mathematics</orgName>
								<orgName type="institution">The Weizmann Institute of Science</orgName>
								<address>
									<settlement>Rehovot</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">IEEE Symposium on Information Visualization</orgName>
								<address>
									<addrLine>October 19-21</addrLine>
									<postCode>2003, 2003</postCode>
									<settlement>Seattle</settlement>
									<region>Washington</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<addrLine>9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Visualization of Labeled Data Using Linear Transformations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T18:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Optimization-Constrained optimization, Global optimization</term>
					<term>H.2.8 [DATABASE MANAGEMENT]: Database Applications-Data mining</term>
					<term>H.5.0 [INFORMATION INTERFACES AND PRE-SENTATION]: General-</term>
					<term>I.5.2 [PATTERN RECOGNITION]: Design Methodology-Pattern analysis visualization, dimensionality-reduction, projection, principal component analysis, Fisher&apos;s linear discriminant analysis, eigenprojection, classification</term>
				</keywords>
			</textClass>
			<abstract>
				<p>We present a novel family of data-driven linear transformations, aimed at visualizing multivariate data in a low-dimensional space in a way that optimally preserves the structure of the data. The well-studied PCA and Fisher&apos;s LDA are shown to be special members in this family of transformations, and we demonstrate how to generalize these two methods such as to enhance their performance. Furthermore, our technique is the only one, to the best of our knowledge, that reflects in the resulting embedding both the data coordinates and pairwise similarities and/or dissimilarities between the data elements. Even more so, when information on the clustering (labeling) decomposition of the data is known, this information can be integrated in the linear transformation, resulting in embeddings that clearly show the separation between the clusters, as well as their intra-structure. All this makes our technique very flexible and powerful, and lets us cope with kinds of data that other techniques fail to describe properly.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One of the most important aspects of exploratory data analysis is data visualization, which aims at revealing structure and unexpected relationships in large datasets, by presenting them as human accessible drawings.</p><p>An important family of visualization tools comprises methods for achieving a low-dimensional embedding (for short, an embedding) of multivariate data. This is, mapping the data to points in a low-dimensional space (mostly 2-D or 3-D) in a way that captures certain structured components of the data. There are numer-ous techniques in this family, including principal component analysis <ref type="bibr" target="#b2">[Everitt and Dunn 1991;</ref><ref type="bibr" target="#b8">Webb 2002]</ref>, multidimensional scaling <ref type="bibr" target="#b7">[Schiffman et al. 1981]</ref>, eigenprojection <ref type="bibr" target="#b3">[Hall 1970;</ref><ref type="bibr" target="#b4">Koren et al. 2002;</ref><ref type="bibr" target="#b5">Koren 2003</ref>], and force-directed placement <ref type="bibr" target="#b1">[Davidson et al. 2001;</ref><ref type="bibr" target="#b6">Morrison et al. 2002]</ref>.</p><p>We are particularly interested in the sub-family of methods that use linear transformations to map the high-dimensional data into a low-dimensional space. This way, each low-dimensional axis is some linear combination of the original axes. Linear mappings are certainly more limited than their nonlinear counterparts, but on the other hand, they possess several significant advantages:</p><p>1. The embedding is reliable in the sense that it is guaranteed to show genuine properties of the data. In contrast, the relation to the original data is less clear for nonlinear embeddings. 2. The embedding axes are meaningful as they are linear combinations of the original axes. These combinations can even sometimes induce interesting domain-specific interpretations. 3. Using the already computed linear transformation, new data elements can be easily added to the embedding without having to recalculate it. 4. In general, the computational complexity of linear transformation methods is very low, both in time and in space.</p><p>Labeled data is a collection of elements that are partitioned into disjoint clusters by some external source, normally either a clustering algorithm or from a domain specific knowledge. Visualizing such data requires special effort since, besides the desire to convey the overall structure, we would also like to reflect the inter-cluster and intra-cluster relationships. Motivated by visualizing labeled data, we introduce in this paper several new linear transformations, specifically designed to show the different clusters as separate as possible, as well as to preserve intra-cluster structure. The resulting embedding can be very instructive in validating the results of a clustering algorithm or in revealing interesting structures like:</p><p>• Which clusters are well separated and which are similar?</p><p>• Which clusters are dense and which are heterogeneous?</p><p>• What is the shape of the clusters (elongated or spherical)?</p><p>• Which data coordinates account for the decomposition to clusters?</p><p>Our visualization technique proves powerful also in its robustness towards outliers in the data. Moreover, the well studied PCA and Fisher's Linear Discriminant Analysis are shown to be special cases of our more general linear transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Basic Notions</head><p>Throughout the paper, we always assume n data elements in an m dimensional space arranged row-wise in the n × m coordinates matrix X, with X iα being the coordinate α of element i. For convenience, but without any loss of generality, we assume that the coordinates are centered, i.e. each column of X has a zero mean -for every 1 α m: ∑ n i=1 X iα = 0. This can always be achieved by a harmless translation of the data.</p><p>A p dimensional embedding of the data is frequently defined by p direction vectors v 1 ,...,v p ∈ R m , so that the α-th coordinates of the embedded data are the entries of the vector Xv α ∈ R n . Consequently, we shall call the vectors Xv 1 ,...,Xv p the embedding coordinates. In most applications p 3, but here we will not specify p so as to keep the theory general.</p><p>We denote the pairwise Euclidean distances between the elements (in the original space) by dist i j , so that dist i j = ∑ m α=1 (X iα − X jα ) 2 . When referring to the pairwise distances in a p-dimensional embedding of the data, we shall add the superscript</p><formula xml:id="formula_0">p: dist p i j = ∑ p α=1 ((Xv α ) i − (Xv α ) j ) 2 .</formula><p>Another key magnitude that describes relations between the data elements is the Laplacian, which is an n × n symmetric matrix, characterized by having zero sum to all its rows (or columns) and being positive-semidefinite. Therefore, all diagonal entries are nonnegative, whereas some non-diagonal entries are non-positive. The usefulness of the Laplacian stems from the fact that the quadratic form associated with it is just a weighted sum of all pairwise squared distances:</p><p>Lemma 2.1 Let L be an n × n Laplacian, and let x ∈ R n . Then</p><formula xml:id="formula_1">x T Lx = ∑ i&lt; j −L i j (x i − x j ) 2 . Similarly, for p vectors Xv 1 ,...,Xv p ∈ R n we have: p ∑ α=1 (Xv α ) T LXv α = ∑ i&lt; j −L i j • p ∑ α=1 (Xv α ) i − (Xv α ) j 2 = = ∑ i&lt; j −L i j • dist p i j 2 .</formula><p>The proof of this lemma is direct. Another technical lemma to be used intensively throughout the paper is (here, δ i j is the Kronecker delta defined as δ i j = 1 for i = j and δ i j = 0 otherwise):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 2.2 Given a symmetric matrix A and a positive definite matrix B, denote by</head><formula xml:id="formula_2">v 1 ,...,v p the p highest generalized eigenvec- tors of (A, B), with corresponding eigenvalues λ 1 ••• λ p (i.e., Av α = λ α Bv α ). Then, v 1 ,...,v p</formula><p>are an optimal solution of the constrained maximization problem:</p><formula xml:id="formula_3">max v 1 ,...,v p p ∑ α=1 (v α ) T Av α subject to: (v α ) T Bv β = δ αβ , α, β = 1,... p. (1)</formula><p>The solution of the corresponding minimization problem are the p lowest generalized eigenvectors of (A, B).</p><p>The proof, which is somewhat tedious, will be given elsewhere. Note that since B is positive definite it can be decomposed into: B = CC T . Thus, the generalized eigenequation Av = λ Bv, can be reduced to the symmetric eigenequation C −1 AC −T u = λ u, using the substitution v = C −T u.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A Generalized Projection Scheme</head><p>An important and fundamental family of linear transformations are projections, which geometrically project the data onto some lowdimensional space. In algebraic terms, projections are characterized by having all the direction vectors orthonormal, i.e.:</p><formula xml:id="formula_4">(v α ) T v β = δ αβ , α, β = 1,..., p<label>(2)</label></formula><p>Probably the most widely used projection is principal component analysis (PCA). PCA projects (possibly) correlated variables into a (a possibly lower number of) uncorrelated variables called principal components. The first principal component accounts for as much of the variability in the data as possible, and each succeeding component accounts for as much of the remaining variability as possible. By using only the first few principal components, PCA makes it possible to reduce the number of significant dimensions of the data, while maintaining the maximum possible variance thereof. See <ref type="bibr" target="#b2">[Everitt and Dunn 1991]</ref> for a comprehensive discussion of PCA.</p><p>Technically, PCA defines the orthonormal direction vectors v 1 ,...,v p , as the p highest unit eigenvectors of the m × m covariance matrix 1 n X T X. While the common explanation for PCA is as the best variancepreserving projection, we would like to derive PCA using a different, yet related motivation. This will enable us later to suggest significant generalizations of PCA.</p><p>In the following proposition we show that PCA computes the pdimensional projection that maximizes the sum of all squared pairwise distances between the projected elements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 3.1 PCA computes the p-dimensional projection that maximizes</head><formula xml:id="formula_5">∑ i&lt; j dist p i j 2 .</formula><p>( <ref type="formula">3)</ref>This proposition implies intimate relationships between PCA and multidimensional scaling (MDS). MDS is a name for a collection of techniques that, given pairwise distances, produce a p-D layout of the data that maximally preserves the pairwise distances. MDS is inherently different from linear transformations like PCA, since it does not use coordinates of the data. Still, PCA and MDS share, in a sense, similar objectives: Clearly dist p i j dist i j for any p-dimensional projection and any two elements i, j. Thus, for any projection:</p><formula xml:id="formula_6">∑ i&lt; j dist p i j 2 ∑ i&lt; j dist i j 2 .</formula><p>In a lossless projection all pairwise distances are preserved and we obtain an equality in the last inequality. Thus, proposition 3.1 shows that PCA computes the projection that maximizes the preservation of pairwise distances, similarly to what MDS strives to achieve. Before proving proposition 3.1, we define the n × n unit Laplacian, denoted by L u , as L u i j = δ i j • n − 1. The unit Laplacian satisfies the following lemma:</p><p>Lemma 3.1 The matrix X T L u X is identical to the covariance matrix up to multiplication by a positive constant.</p><p>Proof We shall examine two corresponding entries of the matrices:</p><formula xml:id="formula_7">(X T L u X) αβ = n ∑ i, j=1 L i j x iα x jβ = n ∑ i, j=1 (n • δ i j − 1)x iα x jβ = = n ∑ i=1 n • x iα x iβ − n ∑ i, j=1 x iα x jβ = n(X T X) αβ − n ∑ i=1 x iα • n ∑ j=1 x jβ = = n(X T X) αβ</formula><p>The last equation stems from the fact that the coordinates are centered. Hence, X T L u X is obtained by multiplying the covariance matrix by n 2 . Now we prove proposition 3.1.</p><p>Proof Recall that the p-dimensional projection is</p><formula xml:id="formula_8">Xv 1 ,...,Xv p .</formula><p>Use lemma 2.1 to obtain:</p><formula xml:id="formula_9">∑ i&lt; j dist p i j 2 = ∑ p α=1 (Xv α ) T L u (Xv α ) = ∑ p α=1 (v α ) T X T L u Xv α .</formula><p>Hence, a projection maximizing (3) can be formally posed as the solution of:</p><formula xml:id="formula_10">max v 1 ,...,v p p ∑ α=1 (v α ) T X T L u Xv α subject to: (v α ) T v β = δ αβ , α, β = 1,... p. (4)</formula><p>Using lemma 2.2 the solution of (4) is obtained by taking v 1 ,...,v p to be the p highest eigenvectors of the matrix X T L u X. Thus, by Lemma 3.1, these are also the p highest eigenvectors of the covariance matrix (multiplication of a matrix by a positive constant does not change the eigenvectors or their order). Consequently the solution of (4) is achieved exactly by the p first principal components.</p><p>Formulating PCA as in Proposition 3.1 easily lends itself to many interesting generalizations. In the strict PCA we are using a uniform Laplacian, meaning that we maximize an unweighted sum of the squared distances. However, we may prefer to weight this sum for achieving various purposes, such as enlarging the distance between certain elements.</p><p>We can formulate this idea by introducing symmetric pairwise dissimilarities d i j , such that d i j measures how important it is for us to place elements i and j further apart. Consequently, we shall modify (3) by seeking for the projection that maximizes:</p><formula xml:id="formula_11">∑ i&lt; j d i j dist p i j 2 .<label>(5)</label></formula><p>We now define the associated n × n Laplacian L d as:</p><formula xml:id="formula_12">L d i j = ∑ n j=1 d i j i = j −d i j i = j ,</formula><p>which provides the desired projection, as given in the following proposition:</p><formula xml:id="formula_13">Proposition 3.2 The p-dimensional projection that maximizes ∑ i&lt; j d i j dist p i j 2</formula><p>is obtained by taking the direction vectors to be the p highest eigenvectors of the matrix</p><formula xml:id="formula_14">X T L d X.</formula><p>Replacing L u by L d , the proof is identical to the proof of proposition 3.1. Note that X T L d X is an m × m matrix. Since m is usually much smaller than n, the eigenvector computation is very fast.</p><p>When would we like to apply such a weighted version of PCA? Well, there may be many occasions. In some cases we are given an external knowledge about dissimilarity relationships between the elements. Hence we may wish to incorporate such additional information in our projection. It is possible to do so by taking the weight d i j to be the dissimilarity between i and j. This way, we prefer projections that separate elements that are known to be dissimilar. In Subsection 3.2 we give an example of such a case. Another use of weighted PCA is discussed in detail in the following subsection. We propose a specific choice of the dissimilarities that results in a projection, which we call normalized PCA, that is far more robust compared to the plain PCA. The PCA projection is fooled by the outliers, unlike the normalized PCA projection that maintains much of the structure of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Normalized PCA</head><p>As we have shown in Proposition 3.1, PCA strives to preserve squared pairwise distances. The fact that the distances are squared gives a substantial preference to the preservation of the larger pairwise distances, frequently at the expense of preserving the shorter distances. In many cases, this preference impairs the results of PCA. One of the most undesirable consequences of this behavior is the dramatic sensitivity of PCA to outliers (extreme observations that are well separated from the remainder of the data), which are common in realistic datasets. Since pairwise distances involving outliers are significantly larger than the other pairwise distances, PCA emphasizes these outlying distances, thus skewing the projection from the desired one. We illustrate this phenomenon in <ref type="figure" target="#fig_0">Fig.  1</ref>, in which we present a 2-D dataset, comprised of a bulk of 50 normally-distributed points as well as two outlying points. As can be seen in the figure, the 1-D projection computed by PCA projects the data in a direction that emphasizes the outliers while hiding almost all the structure of the bulky region. We propose a weighted PCA scheme that achieves impressive robustness towards outliers, by normalizing squared pairwise distances in a way that reduces the dominance of the large distances. Specifically, we choose the weights in (5) as:</p><formula xml:id="formula_15">d i j = 1 dist i j .</formula><p>The resulting projections are well balanced, aiming at preserving both large and small pairwise distances. We have found this method, which we call normalized PCA, to be superior to the bare PCA, especially when the data contain outliers. For example, refer again to <ref type="figure" target="#fig_0">Fig. 1</ref> where the 1-D projection achieved by normalized PCA is demonstrated to preserve much better the overall structure of the data set. The PCA projection merges the clusters, while the weighted PCA projection keeps them much apart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Projecting labeled data</head><p>When the data is labeled (i.e., clustered), a projection is often required to reflect the discrimination between the clusters. PCA may fail to accomplish this, no matter how easy the task is, as it is an unsupervised technique not designed to handle such cases. It is just that directions that maximize the scatter of the data, might not be as adequate to discriminate between clusters. Fortunately, our generalization to PCA can straightforwardly address labeled data. Assume we are given some dissimilarity values, which might be those used in normalized PCA or simply uniform constants. Then, we may put an emphasis on clusters separation by decaying the weights of intra-cluster pairs, using some decay factor 0 t 1, so that the new weights are:</p><formula xml:id="formula_16">d labeled i j = t • d i j i and j have the same label d i j otherwise</formula><p>Typically, we take t = 0, which means that we are not interested at all in separating elements within the same cluster. We give an example in <ref type="figure" target="#fig_1">Fig. 2</ref>, where a 2-D dataset comprising two normally-distributed clusters (200 points each) is shown, together with two 1-D projections. As is well demonstrated, the 1-D PCA projection completely merges the clustering, whereas by setting all the intra-cluster dissimilarities to zero, we obtain a 1-D projection that clearly captures the clustering decomposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">General Linear Transformation</head><p>So far we limited ourselves to projections, which are only a specific subfamily of linear-transformations. Recall that projection is achieved as long as the direction vectors are orthonormal. In several cases it would be better to relax this orthonormality restriction and allow other kinds of linear transformations, which are not strict projections. In order to introduce a formal framework of such transformations, let us first dwell upon the implications of the orthonormality:</p><p>• For every 1 α p, the direction vectors are normalized: v α 2 = 1. This determines the scale of the embedding and is of utmost importance. If we have not restricted the norm of the direction vectors, we could maximize (5) by arbitrarily scaling up the embedding, regardless of its merits. This, of course, makes no sense at all.</p><p>• For α = β , the direction vectors are orthogonal:</p><formula xml:id="formula_17">(v α ) T v β = 0.</formula><p>This prevents a situation where two direction vectors are equal or very similar, resulting in "wasted", uninformative direction vectors.</p><p>Orthonormality, then, is very important for obtaining proper embeddings. Hence, we cannot just remove this constraint without proposing a suitable replacement. Here, we suggest to relax the orthonormality constraint by posing the more general constraint, enforcing the direction vectors to be B-orthonormal for some properly chosen m × m matrix B, so:</p><formula xml:id="formula_18">(v α ) T Bv β = δ αβ , α, β = 1,... p .</formula><p>This way we significantly increase the freedom in choosing a linear transformation, yet still maintaining a version of the two aforementioned orthonormality implications. Needless to say, taking B as the identity matrix restores the original orthonormality constraint. In the rest of this section, we describe several suitable choices of the matrix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Achieving orthonormal embedding coordinates</head><p>A reasonable requirement from the embedding is that its coordinates would be orthonormal. As the columns of X are centered, so are the embedding coordinates. Orthogonality constraint on the latter, thus, means that they are uncorrelated (two centered vectors are uncorrelated when they are orthogonal) and consequently each axis conveys a new information that does not exist in the rest of the axes.</p><p>To sharpen the distinction, unlike projection methods, which are characterized by orthonormal direction vectors, here we require the embedding coordinates to be orthonormal. Formally, recall that given direction vectors v 1 ,...,v p ∈ R m , the embedding coordinates are Xv 1 ,...,Xv p ∈ R n . Hence we require that for each α, β , (Xv α ) T Xv β = δ αβ . This is, (v α ) T X T Xv β = δ αβ , meaning that the direction vectors are required to be X T X orthonormal.</p><p>To summarize, a suitable choice for the matrix B, which ensures uncorrelation between the embedding coordinates, is taking B = X T X. Using lemmas 2.1 and 2.2, we immediately obtain that the solution of</p><formula xml:id="formula_19">max v 1 ,...,v p ∑ i&lt; j d i j dist p i j 2 (6) subject to: (v α ) T X T Xv β = δ αβ , α, β = 1,... p<label>(7)</label></formula><p>are the p highest generalized eigenvectors of (</p><formula xml:id="formula_20">X T L d X, X T X).</formula><p>The X T X orthonormality constraint (7) implies (Xv α ) T Xv α = 1, which means that the variance of each of the embedding coordinates is 1. Intuitively, this imposes a compromise between the desire to maximize the weighted sum of the squared pairwise distances, and the desire to keep the scatter of the data fixed. As a consequence, this method is perfectly suitable for dealing with labeled data, where intra-cluster dissimilarities have been decayed. In this case we expect highly dissimilar elements (belonging to different clusters) to be placed distantly to maximize (6). On the other hand, elements of the same cluster have (almost) no influence on (6) so they are placed closely to satisfy the constraint (7). Interestingly, this method is a generalization of the known Fisher's Linear Discriminant Analysis that will be discussed in Subsection 4.4.</p><p>A demonstration of such an embedding is given in <ref type="figure" target="#fig_4">Fig.  3</ref>.</p><p>The dataset comprises hand-written digits taken from  <ref type="table">4  4 4   3  3 3 3   3   3  3   3  3   3   3  3  3   3   3   3   3   3  3   3   3   3  3   3  3  3   3   3   3   3  3   3   3   33  3   3  3  3  1</ref>   <ref type="table">4   4   3  3  3  3   3  3   3   3   3   3   3  3   3   3   3   3   3   3   3   3  3   3   3 3  3   3  3   3   3   3   3  3   3   3   3   3   3   3   3</ref>  www.cs.toronto.edu/~roweis/data.html. This dataset contains 39 samples per digit, where a sample is a 20 × 16 bitmap, resulting in 320(= 20 × 16) binary coordinates. In (a) we show a 2-D embedding of this 320-D dataset using the method we formerly described here. Inter-cluster dissimilarities were simply set to 1, whereas intra-cluster dissimilarities were set to 0, yielding a good separation between clusters. For comparison, we show in (b) the uninformative embedding obtained using PCA projection.</p><p>Another demonstration of the method is the Colas data taken from <ref type="bibr" target="#b7">[Schiffman et al. 1981]</ref>. This data was collected in an experiment where tastes of ten colas were compared by a human panel. Subjects were asked to perform two tasks: rate each individual cola with regard to 13 flavor descriptors and to rank the level of dissimilarity between each pair of colas. At the end of the day, the resulting dataset contains ten elements in 13-D space, as well as pairwise dissimilarities. We provide three 2-D embeddings of this dataset in <ref type="figure" target="#fig_5">Fig. 4</ref>. In (a) we show the embedding computed by PCA. This embedding reflects well the coordinates alone, but cannot account for the dissimilarities between the colas. An embedding of the data using eigenprojection is shown in (b). This embedding accounts for the given pairwise dissimilarities but completely ignores the coor-dinates. In (c), we use our method with the weights d i j being the given dissimilarities, utilizing thereby all available information: coordinates and dissimilarities. Comparison of this embedding to the former two shows a clear resemblance to the nonlinear eigenprojection embedding, validating our success in incorporating the dissimilarities into the final result. Unlike the nonlinear eigenprojection, here the embedding axes are interpretable linear combinations of the original descriptors, indicating which characteristics (data coordinates) influence the way people sense different colas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Working with similarities</head><p>Constraining the direction vectors to be X T X orthonormal makes it feasible to use another approach based on similarities.</p><p>We define pairwise similarities s i j that measure how important it is to place elements i and j close to each other. In analogy with (6) and <ref type="formula" target="#formula_19">7</ref>, we can now define the complementary minimization problem:</p><formula xml:id="formula_21">min v 1 ,...,v p ∑ i&lt; j s i j dist p i j 2 subject to: (v α ) T X T Xv β = δ αβ , α, β = 1,... p (8)</formula><p>Here we strive to shorten the distance between highly similar elements. It is important to keep in mind that when working with similarities the fixed variance constraint (achieved by the X T X orthonormality) is essential. Otherwise, we might minimize (8) by projecting the data along an uninteresting direction where they have almost no variability.</p><p>By defining the Laplacian L s as:</p><formula xml:id="formula_22">L s i j = ∑ n j=1 s i j i = j −s i j i = j</formula><p>and using lemmas 2.1 and 2.2, we can show that (8) is solved by the p lowest generalized eigenvectors of (X T L s X, X T X). Similarity values appear frequently in data analysis. Two simple ways of extracting them from the coordinates are by using decreasing functions of the distances or by computing correlation coefficients. Sometimes it is beneficial to neglect low similarity values by setting them to zero. The resulting Laplacian will be sparse, having non-zero entries only between close elements. In this case, it is sometimes advisable to set all these non-zero entries to the value 1, thus getting a binary similarity matrix.</p><p>The similarity-based approach can also be used for labeled data. Here, we have to decay all the similarities between elements from different clusters, using some decay factor 0 t 1:</p><formula xml:id="formula_23">s labeled i j = s i j i and j have the same label t • s i j otherwise</formula><p>Typically, we set t = 0, meaning that we do not want the embedding to reflect any proximity relations between elements from different clusters.</p><p>We can not give a conclusive advice on whether to prefer working with similarities or with dissimilarities. In general, it depends on which kind of relationships is easier to be measured on the specific data.</p><p>An example where working with similarities is convenient is the odors dataset shown in <ref type="figure" target="#fig_6">Fig. 5</ref>. The dataset comprises 30 volatile odorous pure chemicals that were chosen to represent a broad range of chemical properties. The odor-emission of each sample was measured using an electronic nose, resulting in a 16-D vector representing that sample. In total, we have performed 300 measurements to yield a dataset of 300 elements in 16-D that are partitioned into 30 clusters. In a separate work <ref type="bibr" target="#b0">[Carmel et al. 2003</ref>], we have developed a technique to derive from the raw data pairwise similarity values between any two samples.</p><p>In 5(a) we show a 2-D embedding of this dataset using our method, where inter-cluster similarities were set to zero. In general, the clusters, which are color-coded in the figure, are well separated. For comparison, we show in (b) the projection of this dataset using PCA, in which case the separation between the clusters is much less significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Inter-cluster repulsion and intra-cluster attraction</head><p>We have shown in Lemma 3.1 that the matrices X T L u X and X T X are identical up to multiplication by a positive constant. Consequently, in problems (6) and (8) we can replace the matrix X T X with X T L u X without altering the solution. This suggests a generalization of these problems by using a general Laplacian, L d , rather than L u , in the constraint. Consequently, if we are given pairwise similarities s i j , as well as pairwise dissimilarities d i j , we may formulate the following minimization problem:</p><formula xml:id="formula_24">min v 1 ,...,v p ∑ i&lt; j s i j dist p i j 2 subject to: (v α ) T X T L d Xv β = δ αβ , α, β = 1,... p (9)</formula><p>The solution of this problem is given by the p lowest generalized eigenvectors of (X T L s X, X T L d X).</p><p>In order to gain some intuition on (9), we shall rewrite it in the equivalent form:</p><formula xml:id="formula_25">min v 1 ,...,v p ∑ i&lt; j s i j dist p i j 2 ∑ i&lt; j d i j dist p i j 2 subject to: (v α ) T X T L d Xv β = C • δ αβ α, β = 1,... p ,<label>(10)</label></formula><p>where C is an arbitrary scaling constant. </p><formula xml:id="formula_26">Since(v α ) T X T L d Xv α = ∑ n i&lt; j d i j • ((Xv α ) i − (Xv α ) j )</formula><p>2 , the last constraint states that the weighted sum of squared distances should be uniform along all axes. It is straightforward to show that a solution of (9) is also a solution of (10).</p><p>Minimizing the target function of problem (10) is achieved by both minimizing the distances between highly similar elements (to minimize the numerator) and maximizing the distances between highly dissimilar elements (to maximize the denominator). When the data is labeled we decay inter-cluster similarities and intracluster dissimilarities, usually setting them to zero. Consequently, in problem (10) we strive to minimize the weighted sum of intracluster squared distances while maximizing the weighted sum of inter-cluster squared distances.</p><p>Similarly, we can generalize the maximization problem (6), and obtain the following problem:</p><formula xml:id="formula_27">max v 1 ,...,v p ∑ i&lt; j d i j dist p i j 2 ∑ i&lt; j s i j dist p i j 2 subject to: (v α ) T X T L s Xv β = C • δ αβ α, β = 1,... p (11)</formula><p>Here, the solution is given by the p highest generalized eigenvectors of (</p><formula xml:id="formula_28">X T L d X, X T L s X).</formula><p>Problems (9)-(11) allow for more degrees of freedom than the previous methods discussed in this paper. They let us use pairwise weights not only in the target function that has to be maximized/minimized but also in the orthonormality constraint. Therefore, they are very suitable for labeled data, as they can induce "attraction" between elements of the same cluster, and "repulsion" between elements of different clusters. An important application of these methods is a robust form of Fisher's Linear Discriminant Analysis, to which we now turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Normalized LDA</head><p>A classical and well-known approach for achieving a linear transformation that separates clusters is Fishers's Linear Discriminant Analysis (LDA) and its generalizations. For details refer to, e.g., <ref type="bibr" target="#b2">[Everitt and Dunn 1991;</ref><ref type="bibr" target="#b8">Webb 2002]</ref>. Briefly, the objective of this method is to find the best linear combinations of the data coordinates so as to maximize the inter-cluster variance and minimize the intra-cluster variance.</p><p>Interestingly, it can be shown that problem (6) (and of course (11)) is a generalization of LDA, as we can choose the dissimilarities such that (6) coincides with LDA, by taking: Here, we denote the number of elements in cluster c by n c . More details will be given in the extended version of this paper.</p><formula xml:id="formula_29">d i j = 1 n 2 − 1 n•</formula><p>The original and most common use of LDA is for classification, rather than for visualization. When used for visualization it suffers from two drawbacks. First, a simple maximization of the intercluster variance is sensitive to outliers and therefore LDA prefers showing few remotely located clusters while masking closer clusters. For example, in <ref type="figure">Fig. 6</ref> we show a 2-D dataset decomposed into 10 clusters, each of which contains 100 elements. Two of the clusters are placed distantly from the rest. As can be seen in the figure, the LDA 1-D projection of the data shows clearly the two "outlying" clusters, but completely masks the other eight clusters, which might be the more fundamental portion of the data.</p><p>The second, more severe problem is that by trying to minimize the variance of a cluster we completely ignore its shape and size.</p><p>No matter if the cluster is dense or heterogeneous, or if the cluster is elongated or spherical, LDA strives to embed it as a small sphere. This may be good for classification, but prevents a reliable visual assessment of the cluster properties. For a concrete example, consider <ref type="figure">Fig. 7</ref>. This figure shows a 2-D dataset containing two normally-distributed clusters, each of which contains 200 elements. One cluster is symmetric having the same variance along both axes, whereas the other cluster is elliptic, and its variance along the x-axis is 10 times larger than the variance along the y-axis. The 1-D projection of LDA makes the two clusters look the same, striving to diminish the intra-scatter of each of the clusters. This way, the heterogeneity of the elliptic cluster cannot be discerned.</p><p>The two aforementioned shortcomings of LDA can be addressed by an appropriate choice of the pairwise similarities/dissimilarities in our weighted methods. Here we would like suggest a particular weighting scheme, which we call normalized LDA.</p><p>Similarly to our proof of proposition 3.1, it can be shown that LDA strives to maximize the ratio between inter-cluster pairwise squared distances and intra-cluster pairwise squared distances. Consequently, it is mainly concerned with the larger pairwise distances. This explains why in <ref type="figure">Fig. 7</ref> it preferred a projection that closely places the distant points of the elliptic cluster. This also explains why LDA prefers separating remotely located clusters.</p><p>To remedy these shortcomings we suggest to compute the embeddings by optimizing problem (11) with appropriately chosen normalization weights that reduce the dominance of large distances. This is achieved by setting the similarities and dissimilarities as follows: The normalized LDA is far more robust with respect to a few outlying clusters, corresponding to large distances from the rest of the data. Such distances will have smaller impact as their weights are reduced. This is beautifully demonstrated in <ref type="figure">Fig. 6</ref> where the 1-D projection of the normalized LDA captures well the eight clusters in a row, reflecting the main trend in the data. Similarly, it is not very important for normalized LDA to place distant points of the same cluster in close proximity, as their respective weights are small. This can be seen in the normalized LDA 1-D projection in <ref type="figure">Fig. 7</ref>, where the different structure of the clusters is preserved, without ruining their separation.</p><p>LDA can produce at most k − 1 embedding axes, where k is the number of clusters. On the other hand, normalized LDA can produce m different embedding axes, regardless of the number of clusters (recall that m is the dimensionality of the data). This is yet another advantage of normalized LDA over LDA, which is particularly important in the frequently encountered two clusters problems. In these cases LDA can produce only one-dimensional embedding, while normalized LDA can produce a higher dimensional embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Relation to Eigenprojection</head><p>Interestingly, the methods surveyed here have intimate relationships with the non-linear eigenprojection visualization technique <ref type="bibr" target="#b3">[Hall 1970;</ref><ref type="bibr" target="#b4">Koren et al. 2002;</ref><ref type="bibr" target="#b5">Koren 2003</ref>]. There, we use only pairwise relationships without utilizing the coordinates themselves.</p><p>In fact, our methods reduce to the eigenprojection when we discard the coordinates matrix X, by setting it to the n × n identity matrix. To make this apparent, substitute X = I in problem (6) (or (5)) and see that the embedding coordinates would be the p highest eigenvectors of L d . Similarly, when substituting X = I in (8) the  <ref type="figure">Figure 6</ref>: Two 1-D projections of 2-D data composed of ten clusters, two of them are outliers. The LDA projection, striving to maximize the inter-cluster variance, emphasizes only the outlying clusters. However, the normalized LDA separates those eight clusters that form the main trend of the data.</p><p>embedding coordinates would be the p lowest eigenvectors of L s . Such nonlinear embeddings are exactly the results of the eigenprojection method.</p><p>We have chosen to derive our methods as a generalization of PCA (or LDA), but we could equally well present them as a way to use the eigenprojection for computing linear transformations of the coordinates. By this, we introduce an interesting link between two seemingly unrelated approaches: PCA and eigenprojection.</p><p>The eigenprojection involves eigenvector computation of an n × n matrix, whereas our methods solve m × m eigen-equations. Typically m is much smaller than n. Therefore, an alternative viewpoint of our methods is as an approximation of eigenprojection that vastly accelerates the computation speed. This approximation computes the optimal embedding only in the m dimensional subspace spanned by the data coordinates, instead of optimizing it in the full n-dimensional space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We propose a family of novel linear transformations to achieve low dim2ensional embedding of multivariate data. These transformations have a significant advantage over other techniques in their ability to simultaneously account for many properties of the data such as coordinates, pairwise similarities, pairwise dissimilarities, and their clustering decomposition. Therefore, we exhaust all kinds of available information so as to make an instructive and reliable visualization. In fact, the derivation of these transformations integrates two apparently very different approaches: those that are coordinates-based and those that are pairwise-weights-based. This reveals interesting inter-relationships between the linear PCA and LDA and the nonlinear eigenprojection and MDS.</p><p>Our methods contain PCA and LDA as special cases, but offer more powerful variants that can better visualize the data. Such two interesting variants, which address several shortcomings of PCA and LDA, are normalized PCA and normalized LDA. One of their advantages is an improved ability to handle outliers in the data.</p><p>All formulations lead to optimal solutions that can be directly  <ref type="figure">Figure 7</ref>: Two 1-D projections of 2-D data composed of two clusters of very different shapes. The LDA projection, striving to diminish the intra-cluster variance, produces very similar projections for both clusters. However, the normalized LDA succeeds in showing the different intra-structure of the clusters.</p><p>computed by eigenvector decomposition of m × m matrices, where m is the dimensionality of the data. This is also the case in PCA and LDA. However, the power of our formulations lies in the fact that these m × m matrices are derived by matrix multiplications that involve an n × n Laplacian matrix, where n is the number of data elements (typically, n &gt;&gt; m). Therefore, we fine-tune the m × m matrix by appropriately altering the n × n entries of the Laplacian. Consequently, the pairwise relationships between data elements are directly reflected in the m × m matrix. One of the most important properties of our methods is that they can adequately address labeled data by capturing well the intercluster structure of the data, as well as the intra-cluster shapes. This is naturally highly beneficial when we are interested in data exploration.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Two 1-D projections of an originally 2-D data that contain two outliers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Two 1-D projections of 2-D data that contain two clusters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Hand-written digits dataset containing 390 samples in 320-D. (a) Result of our method with good separation of clusters. (b) Result of PCA with poor cluster discrimination</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Three embeddings of the colas dataset. Each is characterized by 13 coordinates reflecting its flavor as assessed by human subjects. These subjects also produced pairwise dissimilarities between the different colas. (a) A PCA projection of the dataset accounting only for the coordinates. (b) Nonlinear embedding by eigenprojection, accounting only for the dissimilarities. (c) A linear transformation by our method, taking into account both coordinates and dissimilarities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Odors dataset containing 300 measurements classified into 30 clusters; color-coding shows the classification. (a) The result of our method, clearly exhibiting sharp separation between the clusters. (b) The result of PCA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>n c i and j are both in cluster c 1 n 2 i and j are in different clusters ,</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Visualizing and classifying odors using a similarity matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th International Symposium on Olfaction and Electronic Nose (ISOEN&apos;02)</title>
		<meeting>9th International Symposium on Olfaction and Electronic Nose (ISOEN&apos;02)<address><addrLine>Aracne</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="141" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cluster stability and the use of noise in interpretation of clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Wylie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Boyack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Information Visualization (InfoVis&apos;01)</title>
		<meeting>IEEE Information Visualization (InfoVis&apos;01)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Applied Multivariate Data Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Everitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dunn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<pubPlace>Arnold</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">n r-dimensional quadratic placement algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="219" to="229" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ace: A fast multiscale eigenvectors computation for drawing huge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Information Visualization (InfoVis&apos;02)</title>
		<meeting>IEEE Information Visualization (InfoVis&apos;02)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On spectral graph drawing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th International Computing and Combinatorics Conference (COCOON&apos;03)</title>
		<meeting>9th International Computing and Combinatorics Conference (COCOON&apos;03)</meeting>
		<imprint>
			<publisher>Sringer-Verlag</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">2697</biblScope>
			<biblScope unit="page" from="496" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hybrid layout algorithm for sub-quadratic multidimensional scaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chalmers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Information Visualization (InfoVis&apos;02)</title>
		<meeting>IEEE Information Visualization (InfoVis&apos;02)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="152" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schiffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">W</forename><surname>Young</surname></persName>
		</author>
		<title level="m">Introduction to Multidimensional Scaling: Theory, Methods and Application</title>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Statistical Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Webb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>John Wiley and Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
