<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mapping Nominal Values to Numbers for Effective Visualization *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geraldine</forename><forename type="middle">E</forename><surname>Rosario</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Worcester Polytechnic Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elke</forename><forename type="middle">A</forename><surname>Rundensteiner</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Worcester Polytechnic Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">C</forename><surname>Brown</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Worcester Polytechnic Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">O</forename><surname>Ward</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Worcester Polytechnic Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Mapping Nominal Values to Numbers for Effective Visualization *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T18:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>G.3 [Mathematics of Computing]: Probability and Statistics-Contingency Table Analysis; I.5.3 [Pattern Recognition]: Clustering-Similarity Measures D.2.12 [Software Engineering]: Interoperability-Data Mapping E.4 [Data]: Coding and Information Theory-Data Compaction and Compression nominal data</term>
					<term>visualization</term>
					<term>dimension reduction</term>
					<term>correspondence analysis</term>
					<term>quantification</term>
					<term>clustering</term>
					<term>classing</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Data sets with a large number of nominal variables, some with high cardinality, are becoming increasingly common and need to be explored. Unfortunately, most existing visual exploration displays are designed to handle numeric variables only. When importing data sets with nominal values into such visualization tools, most solutions to date are rather simplistic. Often, techniques that map nominal values to numbers do not assign order or spacing among the values in a manner that conveys semantic relationships. Moreover, displays designed for nominal variables usually cannot handle high cardinality variables well. This paper addresses the problem of how to display nominal variables in general-purpose visual exploration tools designed for numeric variables. Specifically, we investigate (1) how to assign order and spacing among the nominal values, and (2) how to reduce the number of distinct values to display. We propose that nominal variables be pre-processed using a Distance-Quantification-Classing (DQC) approach before being imported into a visual exploration tool. In the Distance Step, we identify a set of independent dimensions that can be used to calculate the distance between nominal values. In the Quantification Step, we use the independent dimensions and the distance information to assign order and spacing among the nominal values. In the Classing Step, we use results from the previous steps to determine which values within a variable are similar to each other and thus can be grouped together. Each step in the DQC approach can be accomplished by a variety of techniques. We extended the XmdvTool package to incorporate this approach. We evaluated our approach on several data sets using a variety of evaluation measures.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Nominal (or categorical) variables are variables whose values do not have a natural ordering or distance. High cardinality nominal variables (i.e., those with a large number of distinct values) are common in real-world data sets. Examples of high cardinality nominal variables include product codes and species names.</p><p>Visualization provides an efficient and interactive way of exploring high dimensional data <ref type="bibr" target="#b19">[Ward 1994</ref>]. Unfortunately, nominal variables, especially high cardinality nominal variables, pose a serious challenge for data visualization tool developers. Difficulties arise due to several reasons.</p><p>First, visualization methods specifically designed for nominal data are not as commonly used as those designed for numeric data <ref type="bibr" target="#b6">[Friendly 1999</ref>]. Possible reasons include: (1) They tend to be more special-purpose (e.g., Mosaic Displays <ref type="bibr" target="#b6">[Friendly 1999</ref>] are designed for discovering associations whereas Parallel Coordinates <ref type="bibr" target="#b9">[Inselberg and Dimsdale 1990]</ref>, which are for numeric variables, can be used for exploring outliers, clusters, and associations). <ref type="formula">2</ref>Methods such as the Fourfold Display <ref type="bibr" target="#b6">[Friendly 1999</ref>] cannot handle multiple nominal variables. <ref type="formula">3</ref>Methods such as the Mosaic Display cannot handle high cardinality variables well. (4) Most methods are not readily available in common visualization software <ref type="bibr" target="#b6">[Friendly 1999]</ref>.</p><p>Second, most visualization software packages only provide displays that are designed for numeric variables. Reasons for this include: (1) Data sets have traditionally contained only numeric data.</p><p>(2) Numeric displays are more general-purpose. <ref type="formula">3</ref>The inherent order and spacing among numeric values makes it natural to convey notions such as magnitude and similarity.</p><p>One way to display nominal variables using numeric displays is to map the nominal values to numbers, i.e., assigning order and spacing to the nominal values. Display methods such as Parallel Coordinates <ref type="figure">(Figure 1</ref>) require both order and spacing among values. But care must be taken. Blindly casting nominal values into numeric displays may introduce artificial patterns and cause errors in the interpretation of the visualization. Existing nominalto-numeric mapping techniques do not always assign both order and spacing to the values. For example, <ref type="bibr" target="#b13">[Ma and Hellerstein 1999]</ref>'s technique only assigns order to the nominal values, but not spacing. As a motivating example of the need for order and spacing, refer to Figures 1 and 2 which both display the quality, color and size information of 6550 objects (from a synthetic data set). <ref type="figure">Figure 1</ref> gives an example of a display where nominal values were assigned order and spacing using our DQC approach, whereas <ref type="figure">Figure 2</ref> shows alphabetical ordering and uniform spacing of the nominal values. <ref type="figure">Figure 1</ref> reveals that blue and purple objects have similar underlying distributions for quality and size. Such information is difficult to extract from <ref type="figure">Figure 2</ref>.</p><p>This paper addresses the problem of how to display data sets with a large number of nominal variables, some with high cardinality, in visual exploration tools designed for numeric variables. Specifically, we address two sub-problems:</p><p>• How do we map nominal values to numbers such that we effectively assign order and distance among the values? Order is used to position values along an axis, where the adjacency of values suggests similarity. Distance is used to space the values along that axis. The amount of spacing suggests the degree of similarity among values, making it easier to spot clusters as well as outliers.</p><p>• When a variable has many values, how do we group similar values together to reduce the number of distinct values to dis-IEEE Symposium on Information <ref type="bibr">Visualization 2003</ref><ref type="bibr">, October 19-21, 2003</ref>, Seattle, Washington, USA 0-7803-8154-8/03/$17.00 ©2003 IEEE  play? Reducing the cardinality is needed for displays such as Dimensional <ref type="bibr">Stacking [LeBlanc et al. 1990]</ref> and Trellis Displays <ref type="bibr" target="#b2">[Becker et al. 1997]</ref> which are limited by the number of values they can display.</p><p>We also want our solution to have the following features: datadriven (not relying on domain knowledge), multivariate (using the relationship of a nominal variable with several other variables to decide the ordering, spacing and classing of the values), scalable (can work with a large number of variables with high cardinality using limited memory), distance-preserving (the distance between two nominal values in nominal space is preserved in numeric space), association-preserving (nominal variables that are highly associated in nominal space are also highly correlated in numeric space), and accessible (readily available to data analysts). To our knowledge, no solution exists that has all these features (this is further discussed in Section 2).</p><p>To solve this problem, we propose that nominal variables be pre-processed using a Distance-Quantification-Classing (DQC) approach before being imported into visual exploration tools designed for numeric variables. In the Distance Step, we transform the data and search for a set of independent dimensions that can be used to calculate the distance between nominal values. This distance is based on each value's distribution across several other nominal variables. In the Quantification Step, we assign order and spacing among the nominal values based on the distance information. In the Classing Step, we determine which values within a variable are similar to each other and thus can be grouped together. Each of these three steps can be accomplished by more than one technique as we will show in Sections 4 to 6.</p><p>We implemented the DQC approach in XmdvTool, a publicdomain visualization package developed at WPI <ref type="bibr" target="#b20">[XmdvTool Home Page 2003</ref>]. For the Distance Step, we implemented and evaluated two alternatives -the well-established technique of Multiple Correspondence Analysis (MCA) <ref type="bibr" target="#b8">[Greenacre 1993</ref>] from Statistics and our own Focused Correspondence Analysis (FCA) which we describe in this paper. FCA is our proposed alternative to MCA when memory is limited. For the Quantification Step, we used a modification of the Optimal Scaling technique <ref type="bibr" target="#b8">[Greenacre 1993</ref>] to also make it work for data sets with perfectly associated variables. For the Classing Step, we used a Hierarchical Clustering algorithm <ref type="bibr" target="#b10">[Johnson and Wichern 1988]</ref> so we can perform multivariate classing (using information from several variables to guide the classing).</p><p>To test our ideas, we pre-processed several data sets using the DQC approach and used numeric displays such as Parallel Coordinates to evaluate the usefulness of the quantified versions of the nominal variables. We compared MCA, FCA and arbitrary quantification using a wide range of evaluation measures such as time, memory, quality of quantification, quality of classing, and quality of visual display.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Visualizing Nominal Variables: Several approaches to visualizing nominal variables exist. One can use displays that are specifically designed for nominal variables: sieve diagrams <ref type="bibr" target="#b6">[Friendly 1999</ref>], mosaic displays <ref type="bibr" target="#b6">[Friendly 1999</ref>], Correspondence Analysis maps <ref type="bibr" target="#b8">[Greenacre 1993</ref>], fourfold displays <ref type="bibr" target="#b6">[Friendly 1999</ref>], treemaps <ref type="bibr" target="#b11">[Kolatch and Weinstein 2001]</ref>, dimensional stacking <ref type="bibr" target="#b12">[LeBlanc et al. 1990]</ref> and CatTrees <ref type="bibr" target="#b11">[Kolatch and Weinstein 2001]</ref>. Unfortunately, these approaches are either special-purpose, not readily available in common data analysis software <ref type="bibr" target="#b6">[Friendly 1999</ref>], or cannot handle high cardinality nominal variables well.</p><p>Others have mapped nominal values to numbers using some ordering technique and equal spacing between values, and then displayed them using numeric displays. Ordering techniques range from arbitrary ordering (e.g., alphabetical order), ordering based on the value of another variable <ref type="bibr" target="#b19">[Ward 1994</ref>] (e.g., time), ordering based on domain expertise <ref type="bibr" target="#b13">[Ma and Hellerstein 1999]</ref>, to more intelligent ordering techniques (e.g., via natural clusters <ref type="bibr" target="#b13">[Ma and Hellerstein 1999]</ref>, using the spectral method <ref type="bibr" target="#b3">[Beygelzimer et al. 2001]</ref>). Unfortunately, arbitrary ordering often creates artificial patterns which can lead to wrong conclusions. Furthermore, equal spacing does not convey the degree of similarity between nominal values.</p><p>Correspondence Analysis: Several research efforts on Correspondence Analysis (CA) have provided ideas for our research. <ref type="bibr" target="#b5">[Friendly 1992</ref>] suggested using the coordinates from the first CA principal axis to order the values of nominal variables in mosaic displays to reveal the pattern of association. <ref type="bibr" target="#b8">[Greenacre 1993</ref>] proposed using the coordinates from the first CA principal axis as input to create a classing tree. In this tree, the nominal values are grouped together using reduction in inertia to represent loss of information. <ref type="bibr" target="#b8">[Greenacre 1993</ref>] also suggested the use of quantified versions of nominal variables as input to statistical techniques that require numeric variables such as regression. The SPSS Categories package uses CA to pre-process data for their Categorical Regression module and uses CA maps for visualizing nominal variables <ref type="bibr" target="#b14">[Meulman and Heiser 2000]</ref>. These uses of the coordinates of the first CA principal axis seem to be due to the theory of Optimal Scaling, that states that these coordinates provide an optimal numeric representation of the nominal values <ref type="bibr" target="#b8">[Greenacre 1993]</ref>. Unfortunately, when the nominal variable is perfectly associated with another nominal variable, such coordinates are not optimal, as we will show later. <ref type="bibr" target="#b16">[Milanese et al. 1996</ref>] used CA and clustering to group similar images and created a hierarchical tree for use in fast indexing into classes of images. This is similar to our approach in that we also use CA as a data reduction technique and use clustering to group similar nominal values together.</p><p>Classing: There are several approaches to grouping similar nominal values together. One could use expert knowledge but this can be tedious for high cardinality nominal variables. One could use information about the nominal variable itself (e.g., based on the frequency of occurrence of the values, the values can be grouped into popular, common or rare values). Or, one could use the relationship of the nominal variable with a target classification or regression variable [Micci-Barreca 2001] (e.g., group cities based on income level). But using only one specific variable to guide the classing (bivariate classing) may result in a classing that is believable only within the context of that specific variable (e.g., if we group cities based on income level alone, we may have to regroup cities if we want to visualize their relationship with land area). A better classing approach is to use several variables to guide the classing of a target variable (multivariate classing). One multivariate classing approach applies Clustering <ref type="bibr" target="#b10">[Johnson and Wichern 1988]</ref> on a data set where the records represent the nominal values and the variables contain summary information about each nominal value. We use this clustering approach for our Classing Step (Section 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Overview of Proposed Approach</head><p>Our proposed approach, the Distance-Quantification-Classing approach, consists of three steps <ref type="figure">(Figure 3</ref>). Each step can be accomplished by more than one technique. In this section, we describe the input, output and purpose of each step. In the succeeding sections, we discuss possible techniques for each step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3: DQC Approach</head><p>Step 1: Distance Step -Given a data set with nominal variables, one of which is the nominal variable to be quantified and classed. The purpose of this step is to create a table where the rows represent the values of the nominal variable and the columns represent information about the other variables in the data set. For this table to be useful for the Quantification and Classing steps, we should be able to calculate the distance between two nominal values from this table.</p><p>To explain this better, consider a data set that contains quality, color and size information for 6550 objects. Quality has three possible values -good, ok, bad; color has six values -blue, green, orange, purple, red, white; and size has ten values -'a' to 'j'. Suppose we want to analyze color (which we shall call our target variable) using quality and size (which we shall call our analysis variables). To analyze color, we look at the distribution of its values with respect to the analysis variables using a contingency or counts table <ref type="figure" target="#fig_2">(Figure 4</ref>). From the counts table, we can calculate row percentages ( <ref type="figure">Figure 5</ref>) and get a glimpse of which colors are similar to each other based on row profiles; <ref type="figure">Figure 5</ref> shows that blue and purple have similar row profiles. From the row percentage table, we may be tempted to calculate the distance between two rows using Euclidean Distance formula; however, there are two row percentage tables for color (color by quality and color by size). The technique to be used for this step must have a way to combine all the columns of all tables for color, extract new dimensions that are independent of each other, and transform the counts table into a table that uses the independent dimensions ( <ref type="figure" target="#fig_3">Figure 6</ref>). These independent dimensions would then be the basis of distance calculations needed in the succeeding steps. Using independent dimensions ensures that the distance calculation is not biased by groups of highly associated columns. This argument is similar to performing Principal Component Analysis prior to Cluster Analysis to ensure that the dimensions are independent of each other as required by the Euclidean Distance calculations <ref type="bibr" target="#b10">[Johnson and Wichern 1988]</ref>. Each row in the output table ( <ref type="figure" target="#fig_3">Figure 6</ref>) can be thought of as a point in p-dimensional space defined by the p independent dimensions.</p><p>Often, the number of analysis variables is large although several may be highly associated with each other. This suggests that the number of independent dimensions to keep in the output table <ref type="bibr">(Figure 6</ref>) can be reduced while still maintaining a high accuracy for the distance calculation. This Distance Step must also determine how many of the independent dimensions to keep.</p><p>This step is the most important step as it dictates the accuracy of the distance calculation needed in the Quantification and Classing Steps. It is also the most memory hungry and computationally intensive step as it involves transformations of the original (large) data sets and data reduction. Step 2: Quantification</p><p>Step -Given a table with rows representing the values of the target variable and columns representing independent dimensions extracted from the analysis variables <ref type="figure" target="#fig_3">(Figure 6</ref>), this step uses the distance information to assign order and spacing to the values of the target variable. The output is a nominalto-numeric mapping ( <ref type="figure">Figure 7</ref>). The goal of this step is to create that mapping in a way that is distance-preserving and associationpreserving. Step 3: Classing</p><p>Step -This step uses the distance information derived in the Distance Step to determine which values of the target variable are similar to each other and thus can be grouped together with minimal loss of information. Ideally, the output is a hierarchical classing tree showing which values can be grouped together successively and the information lost with each grouping <ref type="figure" target="#fig_4">(Figure 8</ref>). Note that the Quantification and Classing steps may or may not be dependent of each other, as suggested by the dashed line between them in <ref type="figure">Figure 3</ref>.</p><p>The DQC approach has several advantages. First, it is generalpurpose. It provides a pre-processing approach that is useful not only for visualization purposes but also for other techniques that cannot handle high-cardinality nominal variables (e.g., clustering algorithms, association rules) or can only handle numeric variables. Second, it provides a hierarchical classing tree which gives users the flexibility to decide how many value-groups to use in visual displays, depending on their specific analysis goals. Third, it enables multivariate quantification and classing (i.e., determining the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Distance Step</head><p>A well-known family of techniques from Statistics suitable for the Distance Step is the Correspondence Analysis (CA) family <ref type="bibr" target="#b8">[Greenacre 1993;</ref><ref type="bibr" target="#b17">SAS Institute Inc 2000;</ref><ref type="bibr" target="#b18">StatSoft Inc 2002]</ref>. CA has been reinvented under different names such as Dual Scaling, Optimal Scaling and Reciprocal Averaging. Its simplest version, called Simple Correspondence Analysis (SCA), is designed to analyze the relationship of two nominal variables. SCA takes as input a 2-way counts table <ref type="figure" target="#fig_2">(Figure 4</ref>). The rows of the counts table can be thought of as data points in a p-dimensional coordinate space defined by the p columns. As such, there is a distance between two data points. CA eliminates the dependencies among the columns by extracting a reduced set of new columns that are independent of each other, while still preserving all or most of the information about the differences between the rows. <ref type="figure" target="#fig_3">Figure 6</ref> shows an example output from CA. CA is similar to Principal Component Analysis (PCA) except that CA is for nominal variables while PCA is for numeric variables. Just like PCA, each successive independent dimension (called a principal axis) explains less and less of the overall information.</p><p>In its general form, CA can analyze n-way tables that contain some measure of correspondence between the rows and columns (not just counts). In this Distance Step, one can use any version of Correspondence Analysis, as long as it can analyze the relationship of more than two variables and it can provide as output the coordinates of the top independent dimensions for each value of the target nominal variable (as in <ref type="figure" target="#fig_3">Figure 6</ref>). In the following subsections, we describe two versions of CA suitable for the Distance Step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Multiple Correspondence Analysis</head><p>Multiple Correspondence Analysis (MCA) extends SCA to analyze more than two nominal variables <ref type="bibr" target="#b8">[Greenacre 1993;</ref><ref type="bibr" target="#b17">SAS Institute Inc 2000;</ref><ref type="bibr" target="#b18">StatSoft Inc 2002]</ref>. To perform MCA, simply create a Burt Table <ref type="figure" target="#fig_5">(Figure 9</ref>) and use that as input to SCA. If a counts table is a cross between two nominal variables, a Burt Table is a cross of all variables by all variables. If V is the total number of unique values across all variables, then the size of the Burt <ref type="table">Table is</ref> </p><formula xml:id="formula_0">V*V.</formula><p>The Burt <ref type="table">Table structure</ref> allows MCA to simultaneously analyze all variables. That is, for every target variable, it can build row profiles using information from all other variables. This simultaneous analysis is efficient in terms of processing time because certain calculations can be reused, though wasteful in memory. When the number of nominal variables to analyze is large and some have high cardinality, MCA could run out of memory, depending on how it is implemented.</p><p>The coordinates of the first principal axis from MCA follow an optimal scaling property <ref type="bibr" target="#b8">[Greenacre 1993</ref>]. This means that such coordinates represent a quantification of all nominal values in all variables. Note, however, that this quantification is sub-optimal when the target variable has a perfect 1-to-many or many-to-many association with another variable, as we show in Section 7.  <ref type="table">Table)</ref> Figure 10: Example FCA Input <ref type="table">Table (Compressed Burt Table)</ref> 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.2 Focused Correspondence Analysis</head><p>Due to the memory-intensive nature of MCA, we have designed an alternative solution, which we call Focused Correspondence Analysis (FCA), aimed at processing a large number of nominal variables, some possibly having high cardinality.</p><p>Unlike MCA which analyzes all variables simultaneously, FCA analyzes one variable at a time, making FCA less computationally efficient than MCA. The memory savings in FCA come from this key idea: instead of comparing value profiles across all other nominal variables, just compare value profiles across the set of nominal variables most associated (i.e., correlated) with the target variable. For example, to analyze one nominal variable color against its most associated variables, say quality and size, we use a compressed Burt table such as <ref type="figure">Figure 10</ref> as input to SCA. This table is a concatenation of counts tables of color*quality and color*size.</p><p>We now discuss why such a table would be a valid input for SCA. In Section 4, we mentioned that the basic version of SCA uses a counts table as input. In Section 4.1, we indicated that we can perform MCA by using a Burt Table as input to SCA. In general, SCA can use as input any table that has the following properties <ref type="bibr" target="#b7">[Greenacre 1984]:</ref> (1) the table must use the same physical units or measurements, and (2) the values in the table must be non-negative. If the input table does not meet these assumptions, the table must be transformed before performing SCA. The table in <ref type="figure">Figure 10</ref> follows these properties.</p><p>Two pre-processing steps are needed for FCA: (1) Measure the pairwise association between nominal variables, and (2) Determine the top k associated variables for each nominal variable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Measure the pairwise association between nominal variables</head><p>Given the counts table of two nominal variables, we can state how closely related the variables are with each other using measures of nominal association <ref type="bibr" target="#b0">[Agresti 1990</ref>]. These measures are analogous to measures of correlation between numeric variables. Several measures of nominal association exist. The choice depends on factors such as the size and shape of the counts table and the presence of low counts <ref type="bibr" target="#b0">[Agresti 1990</ref>]. For our purpose, we want a measure of association that is valid for counts tables that may be large, non-square and may contain low cell counts -all properties of counts tables from high cardinality variables. We also want a measure of association that has a bounded range of values, so it is easy to compare two values. One such measure is the Uncertainty Coefficient Asymmetric measure U(R|C) [SAS Institute Inc 2000]. U(R|C) gives the proportion of uncertainty in the row variable R that can be explained by the column variable C. If U(R|C) = 1, the value of the row variable can be known precisely given the value of the column variable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Determine top k associated variables for each nominal variable</head><p>For now, we select some k greater than 2, depending on the memory space available. Since there may be variables that are only weakly associated with other variables, we cannot use a threshold on the measure of association chosen in Section 4.2.1. By selecting k to be greater than 2, we ensure that we use at least one analysis variable for each target variable.</p><p>In summary, FCA has its own strengths and weaknesses. With FCA, memory usage is reduced and, in fact, controllable. Also, we empirically show in Section 7 that FCA provides better classing trees compared to MCA for some data sets. FCA however needs a longer run time compared to MCA. This is due to the one-at-a-time analysis as well as the need for pre-processing. In the context of visualization tools, intelligently mapping nominal values to numbers is a pre-processing step that can be run in batch mode. Hence, the run time may not be as important compared to memory space in some situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Reduce Number of Dimensions to Keep</head><p>The CA family of techniques uses forms of decomposition (e.g., Singular Value Decomposition, Eigenvalue Analysis) to extract the set of independent dimensions. By default, all forms of CA will keep all independent dimensions calculated <ref type="bibr" target="#b8">[Greenacre 1993]</ref> which, for high dimensional high cardinality data sets, require a lot of space. These independent dimensions are ordered by diminishing importance. Part of the CA output is the set of eigenvalues (principal inertia) that indicate the importance of each independent dimension. The first dimension, which is the most important dimension, will have the highest eigenvalue. We plot the eigenvalue by dimension number (called a Scree Plot) and find the 'elbow', the point at which the change in consecutive eigenvalues is small. We keep only the dimensions up to the 'elbow'. This is a common technique used in Factor Analysis <ref type="bibr" target="#b17">[SAS Institute Inc 2000]</ref>. This technique is independent of the particular version of CA we use for the Distance Step.</p><p>In summary, the MCA-based Distance Step algorithm is as follows:</p><p>1. BurtTable(rawdataMatrix) -&gt; burtMatrix 2. SCA(burtMatrix) -&gt; coordMatrix, evaluesVector 3. ReduceNumberDim(coordMatrix, evaluesVector) -&gt; coordMatrixSubset while the FCA-based Distance Step algorithm is as follows:</p><p>1. PairwiseAssociation(rawdataMatrix) -&gt; assocMatrix 2. Set k 3. FCATable(rawdataMatrix, k, assocMatrix) -&gt; fcaInputMatrix 4. SCA(fcaInputMatrix) -&gt; coordMatrix, evaluesVector 5. ReduceNumberDim(coordMatrix, evaluesVector) -&gt; coordMatrixSubset</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Quantification Step</head><p>Quantification is the process of assigning order and spacing to the nominal values. For this step, we want a technique that can take as input the independent dimensions from the Distance Step and produce a nominal-to-numeric mapping for each nominal variable.</p><p>As mentioned in Section 2, a popular technique used for quantification is based on the theory of Optimal Scaling <ref type="bibr" target="#b8">[Greenacre 1993</ref>].</p><p>Based on Optimal Scaling, we can use the coordinates from the first CA independent dimension as the quantified version of the nominal values. Unfortunately, when a nominal variable is perfectly associated with another variable (e.g., one-to-many association: one state has many zip codes, or many-to-many association: specific products are only sold in specific regions), we have found in our experiments that this technique fails (see Section 7).</p><p>Since we want our technique to work without the need for domain knowledge, we want it to automatically handle cases of perfect associations. Hence, we propose an adjustment to the Optimal Scaling approach: If the first n CA eigenvalues are 1.0, let scale i = ∑ n j=1 coordinate i, j where coordinate i, j is the coordinate of the jth independent dimension for row i. Else set scale i = coordinate i,1 (coordinate of the first independent dimension). Scale is the term used in Optimal Scaling for the quantified version of a nominal variable. In Section 7, we show that this proposed adjustment gives more effective results for cases with perfect association.</p><p>By using independent dimensions extracted via CA to create the quantified versions of nominal values, we have essentially defined the order and spacing of two nominal values to be a function of the chi-squared distance between them. Chi-squared distance is the distance function used in CA <ref type="bibr" target="#b8">[Greenacre 1993</ref>]. Chi-squared distance is the weighted Euclidean Distance between a row profile and the average (or expected) row profile. Put differently, the quantified version of a nominal value depends on how different its profile is from the average profile. This implies that even if the nominal variable has an underlying order (i.e., even if it is actually a discretized numeric variable), that order is not likely to be recreated in the quantified version.</p><p>An alternative to our modified optimal scaling is to use an algorithm similar to <ref type="bibr" target="#b1">[Ankerst et al. 1998</ref>]'s algorithm for rearranging dimensions for a visualization. We search for an ordering of the rows of <ref type="figure" target="#fig_3">Figure 6</ref> that minimizes the sum of the distances between all pairs of adjacent rows. This defines the order of the nominal values. The spacing between values can be defined using the distance between the row values. Our Optimal Scaling quantification is faster than this algorithm because Optimal Scaling directly uses output from CA at no extra cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Classing Step</head><p>Classing (or intra-dimension clustering) is the process of finding which values within a nominal variable are similar to each other and thus can be grouped together. For this step, we want a technique that can take as input a table with rows representing the values of the target variable and columns representing independent dimensions extracted from the analysis variables, and produce a hierarchical classing tree showing value groupings and the amount of information lost with each grouping (shown in <ref type="figure" target="#fig_4">Figure 8</ref>). One method for solving this is to apply a hierarchical clustering algorithm on the CA output table <ref type="figure" target="#fig_3">(Figure 6</ref>), where each value (row point) is weighted by its counts.</p><p>Classing is a data reduction technique, thus it results in loss of information. In this step, we also want to show the amount of information lost whenever two values are grouped together, and display this alongside the classing tree. To approximate the loss of information incurred in classing the nominal variable X, we follow four steps (inspired by <ref type="bibr" target="#b8">[Greenacre 1993]</ref>): (1) Determine the variable V with the highest association with X. (2) Create a contingency table between variables X and V. (3) Calculate the total table measure of association (e.g., Uncertainty Coefficient). <ref type="formula">4</ref>Starting from the bottom of the classing tree and going all the way to the top, for every pair of nodes merged together, calculate the loss of information incurred, defined by the cumulative percentage loss of information In f oLoss = 100 * (A( f ullTable) − </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experimental Evaluation</head><p>In this section, we compare the MCA-based implementation, FCAbased implementation and the common approach of arbitrary quantification (arbitrary ordering and uniform spacing) using a wide range of evaluation measures. We focus our evaluations on the Distance Step (MCA vs. FCA) because it is the most important step in the DQC approach. All implementations and evaluations were done within XmdvTool [XmdvTool Home Page 2003].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Setup</head><p>We used real as well as synthetic data sets, as listed in <ref type="figure" target="#fig_6">Figure 11</ref>. The real data sets used are popular benchmark data sets taken from <ref type="bibr" target="#b4">[Blake and Merz 1998]</ref>. We have used only the nominal variables for most of these data sets. The NOTPERF synthetic data set has three variables (quality, color, size) and is intended to simulate varying degrees of association. This is the data set used in all examples given in earlier sections. The PERF synthetic data set has three variables (region, country and product code) and is intended to simulate perfect associations (1-to-many: region-country, many-tomany: specific set of products are only sold in specific countries).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Quality of Visual Display</head><p>Intuitively, quantification A is better than quantification B if the visual display resulting from A allows the data analyst to confirm or discover (true) patterns in the data that are otherwise harder or impossible to learn using B. The quality of a visual display is more difficult to measure and quantify. One alternative is to conduct user studies and have subjects answer questions using data sets for which they have some domain knowledge. Example questions include: Based on your domain knowledge, are the values that are positioned close together for the most part similar to each other? Are the values that are positioned far from the rest of the other values for the most part that different? Are there fewer line crossings because of the ordering and spacing? Did you discover any new patterns (e.g., outliers, clusters, strength of association between two nominal variables)? In general, which quantification do you feel is better (easier to understand, more believable ordering and spacing)?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1">Automobile Data Set Case Study</head><p>We chose the Automobile Data Set because it is easy to interpret. <ref type="figure" target="#fig_7">Figures 12, 13</ref> and 14 display the quantified versions of selected variables in a Parallel Coordinates display. In Parallel Coordinates, each vertical line represents one variable, and each polyline cutting across the vertical axes represents one instance in the data set. Parallel Coordinates is one type of display that requires ordering and spacing of values and it can display several variables compactly. In these figures, we have ordered the variables such that the vertical axes of highly associated variables are adjacent to each other for easier interpretation.  The MCA-based display <ref type="figure" target="#fig_7">(Figure 12</ref>) and the FCA-based display ( <ref type="figure">Figure 13</ref>) present alternative notions of similarity among the values. Some results are similar (Peugot/Mercedes are positioned away from Honda/Mazda), some are different (the spacing between Convertible/Hardtop/Hatchback and Sedan/Wagon). But both MCA and FCA displays confirm our domain knowledge. Which is better depends on the user's preference. Also, both MCA and FCA-based displays have fewer line crossings than the Arbitrary Quantification display <ref type="figure" target="#fig_2">(Figure 14)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2">PERF Data Set Case Study</head><p>Figures 15 and 16 display the quantified versions of the variables in the PERF Data Set. Recall that the region-country pair has a 1-to-many association while the country-product code pair has a many-to-many association. These perfect associations are revealed in all CA-based quantifications but are hidden in the arbitrary quantification. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Memory Space and Processing Time</head><p>The most memory-intensive part of our implementation is the use of CA in the Distance Step, so we only focus on the memory needed there. Ignoring any specific memory optimization that may be employed by some CA implementations, in general, the MCA input table <ref type="figure" target="#fig_5">(Figure 9</ref>) requires (sum o f cardinality) 2 while the FCA input table <ref type="figure">(Figure 10</ref>) requires at most max cardinality * (sum o f cardinality − max cardinality) for each nominal variable to be processed. These formulas and the example tables show that MCA uses more memory than FCA. <ref type="figure" target="#fig_10">Figure 17</ref> shows the percentage of time the FCA-based approach runs longer than MCA-based using the formula 100 * (total time − MCA total time)/(MCA total time). For each MCA bar, we show the actual number of seconds that the MCA-based approach ran. So although the gap between FCA and MCA run times seems large, the actual run time of the FCA-based approach is still fast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Quality of Quantification</head><p>Intuitively, a given quantification is good if (a) instances that are close to each other in nominal space are also close together in quantified space, and (b) if two variables are highly associated with each other, we expect their quantified versions to also have high correlation measure.  <ref type="bibr" target="#b8">[Greenacre 1993</ref>] suggests the use of Average Squared Correlation to measure the quality of a quantification. Given the original dataset, replace each nominal variable V j with its quantified version Q j (i.e. scale). For each instance i, calculate score i = average(Q i j ) for all variables j. For each quantified variable Q j , calculate the correlation of Q j and score for the entire data set. Then calculate the average squared correlation = average((correlation(Q j , score)) 2 ) across all Q j . The higher the average squared correlation, the better the quantification. Intuitively, if two variables are highly associated with each other, we expect their quantified versions to also have a high correlation measure. If all nominal variables are highly associated with each other, then the score of each observation should be highly correlated with each individual quantified variable. This further implies that if two observations are close together in nominal space, then they would also be close together in quantified space; so the scores of these observations would be close to each other. <ref type="figure" target="#fig_4">Figure 18</ref> shows the Average Squared Correlation for MCAbased, FCA-based and arbitrary quantifications. It shows that both CA-based quantifications are better than arbitrary quantification. The figure also verifies the Optimal Scaling theory, namely, that the quantification based on the coordinates of the first MCA extracted dimension is optimal <ref type="bibr" target="#b8">[Greenacre 1993]</ref>. <ref type="figure" target="#fig_5">Figure 19</ref> shows how close the FCA scales are to the MCA scales. This figure uses boxplots to show, for the real data sets, the distribution of the correlation between MCA and FCA scales. These boxplots show the minimum and maximum values as well as the 25th, 50th and 75th percentile values of each set of correlation values. Correlation values close to 1.0 mean the FCA scales closely agree with the MCA scales. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Quality of Classing</head><p>Intuitively, classing A is better than classing B if, given a classing tree, the rate of information loss with each merging is slower. One way of calculating information loss is given in Section 6. <ref type="figure">Figure 20</ref> compares the rate of information loss of MCA compared to FCA for one variable. Each line shows the cumulative <ref type="figure" target="#fig_5">Figure 19</ref>: Correlation between MCA Scales and FCA Scales information loss incurred at each merging of nodes. The lower the line, the slower is the information loss, the better the classing. The gap between the lines (MCA cumulative loss minus FCA cumulative loss) can be calculated for all variables. Its distribution has been summarized in <ref type="figure" target="#fig_12">Figure 21</ref>. This plot shows that the FCA-based classing is better than MCA-based for some data sets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>In this paper, we proposed the Distance-Quantification-Classing (DQC) approach which enables the exploration of data sets containing nominal variables using visualization tools that have been designed exclusively for numeric variables. To make the approach accessible to data analysts, we implemented it in XmdvTool, a public-domain multivariate data visualization package. For our implementation, we used Multiple Correspondence Analysis (MCA) and our own Focused Correspondence Analysis (FCA) for the Distance Step, a modification of the Optimal Scaling formula for the Quantification Step, and Hierarchical Clustering for the Classing Step. We evaluated our approach in terms of memory space requirement, run time, quality of quantification, quality of classing, and quality of visual display. MCA-based and FCA-based quantifications are clearly better than the common practice of arbitrary quantification. In terms of the quality of classing and quantification, MCA seems to perform better than FCA but in terms of the quality of the visual displays, which one is better depends on the eye of the beholder. When memory space is limited, FCA provides a viable alternative to MCA for the Distance Step. The adjustment made to the quantification function to make it work for variables with perfect association improves upon the existing technique of taking only the coordinates of the top CA dimension. Producing classing trees further allows users to reduce the data for displays requiring low cardinality nominal variables.</p><p>The DQC approach is a general-purpose pre-processing step which can also be used for other techniques that require low cardinality nominal variables as input (e.g., such as clustering algorithms, association rules, neural networks), or require numeric variables as input (e.g., regression). Possible future work includes allowing the user to interactively modify the ordering, spacing and classing of the nominal values, conducting formal evaluations, and trying other alternatives for each step.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure</head><label></label><figDesc>Figure 1: Parallel Coordinates with FCA Quantification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure</head><label></label><figDesc>Figure 2: Parallel Coordinates with Arbitrary Quantification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Counts Table Figure 5: Row Percentage Table Showing Row Profiles</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Transformed Table withIndependent DimensionsFigure 7: Nominal-to-Numeric Mapping</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Classing Tree with Information Loss Measure distance between the values based on their profiles across several other variables) which we believe provides more robust results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>Example MCA InputTable (Burt</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 11 :</head><label>11</label><figDesc>Evaluation Data Sets A(a f terMerging))/A( f ullTable), where A(t) is the association measure for table t. An alternative measure of information loss is the R-squared measure that can be calculated with Cluster Analysis [SAS Institute Inc 2000].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 12 :</head><label>12</label><figDesc>Automobile Data, MCA-Based Quantification Figure 13: Automobile Data, FCA-Based Quantification</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 14 :</head><label>14</label><figDesc>Automobile Data, Arbitrary Quantification</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 15 :</head><label>15</label><figDesc>Perfect Association Data, FCA-Based QuantificationFigure 16: Perfect Association Data, Arbitrary Quantification</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 17 :</head><label>17</label><figDesc>Total Run Time of Entire DQC Approach</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 18 :</head><label>18</label><figDesc>Average Squared Correlation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>FigureFigure 21 :</head><label>21</label><figDesc>Distribution of the Difference in MCA and FCA Information Loss</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: We gratefully acknowledge our colleagues in the XmdvTool group at WPI for their contributions to this research, as well as to NSF and NSA for the funding for XMDV research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Categorical Data Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agresti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>John Wiley &amp; Sons</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Similarity clustering of dimensions for an enhanced visualization of multidimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ankerst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Berchtold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Symposium on Information Visualization, InfoVis&apos;98</title>
		<meeting>of IEEE Symposium on Information Visualization, InfoVis&apos;98</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="52" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Trellis graphics displays: A multidimensional data visualization tool for data mining. Knowledge Discovery and Data Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cleveland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page">97</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast ordering of large categorical datasets for better visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Perng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">UCI repository of machine learning databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Merz</surname></persName>
		</author>
		<ptr target="http://www.ics.uci.edu/∼mlearn/MLRepository.html" />
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mosaic displays for loglinear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Friendly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Statistical Graphics Section</title>
		<meeting>the Statistical Graphics Section</meeting>
		<imprint>
			<publisher>ASA</publisher>
			<date type="published" when="1992-08" />
			<biblScope unit="page" from="61" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Visualizing categorical data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Friendly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cognition and Survey Research</title>
		<editor>M. G. Sirken, D. J. Herrmann, S. Schechter, N. Schwarz, J. M. Tanur, and R. Tourangeau</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley &amp; Sons, Inc</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="319" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Theory and Applications of Correspondence Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Greenacre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
			<publisher>Academic Press</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Correspondence Analysis in Practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Greenacre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Academic Press</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Parallel coordinates: A tool for visualizing multidimensional geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Inselberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dimsdale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Visualization &apos;90</title>
		<meeting>of Visualization &apos;90</meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="361" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Applied Multivariate Statistical Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Wichern</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>Prentice Hall International, Inc</publisher>
		</imprint>
	</monogr>
	<note>second ed</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Cattrees: Dynamic visualization of categorical data using treemaps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kolatch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Weinstein</surname></persName>
		</author>
		<ptr target="http://www.cs.umd.edu/class/spring2001/cmsc838b/Project/KolatchWeinstein" />
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploring ndimensional databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leblanc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wittels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Visualization &apos;90</title>
		<meeting>of Visualization &apos;90</meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="230" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ordering categorical data to improve visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Hellerstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Information Visualization Symposium Late Breaking Hot Topics</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="15" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Meulman</surname></persName>
			<affiliation>
				<orgName type="collaboration">SPSS Categories</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Heiser</surname></persName>
			<affiliation>
				<orgName type="collaboration">SPSS Categories</orgName>
			</affiliation>
		</author>
		<idno>10.0</idno>
		<imprint>
			<date type="published" when="2000" />
			<publisher>SPSS Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A preprocessing scheme for high-cardinality categorical attributes in classification and prediction problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Micci-Barreca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explorations</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="27" to="32" />
			<date type="published" when="2001-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Correspondence analysis and hierarchical indexing for content-based image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Milanese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Squire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd IEEE Int. Conf. on Image Processing, ICIP&apos;96</title>
		<meeting>3rd IEEE Int. Conf. on Image essing, ICIP&apos;96</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="859" to="862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">SAS OnlineDoc Version 8 with PDF Files</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Inc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Electronic statistics textbook: Correspondence analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Statsoft Inc</surname></persName>
		</author>
		<ptr target="http://www.statsoftinc.com/textbook/stcoran.html" />
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Xmdvtool: Integrating multiple methods for visualizing multivariate data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Visualization &apos;94</title>
		<meeting>of Visualization &apos;94</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="326" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xmdvtool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Page</surname></persName>
		</author>
		<ptr target="http://davis.wpi.edu/˜xmdv" />
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
