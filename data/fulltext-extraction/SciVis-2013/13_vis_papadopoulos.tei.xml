<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Acuity-Driven Gigapixel Visualization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Charilaos</forename><surname>Papadopoulos</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE LoD</roleName><forename type="first">Arie</forename><forename type="middle">E</forename><surname>Kaufman</surname></persName>
						</author>
						<title level="a" type="main">Acuity-Driven Gigapixel Visualization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Gigapixel visualization</term>
					<term>visual acuity</term>
					<term>focus and context</term>
					<term>Reality Deck</term>
					<term>gigapixel display</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Fig. 1. Illustration of the operation of our acuity-driven gigapixel visualization framework. In the top row, the user (red circle) is standing in the middle of a large immersive visualization space (gray rectangle), examining a gigapixel resolution image. Notice in the middle column the selected LoDs, mapped to the HSV color space, and the tessellation of an F+C lens applied to the image (visible on the right column, please zoom in on PDF for detail). In the bottom row, the user approaches the front wall of the large-format display. The acuity-driven visualization system responds, rendering a higher LoD in the screens close to the user while reducing the detail in the side walls. Additionally, the tessellation of the F+C lens increases to provide better visual quality. Gigapixel imagery from</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recent advances in data acquisition, display and rendering technology have brought to the masses the ability to interact with extremely high resolution data. For instance, the Gigapan project <ref type="bibr" target="#b0">[2]</ref> allows any person with a web browser to explore images that span hundreds of gigapixels. While these vistas are composed over a period of time using offline stiching, research projects are yielding devices (such as parallel arrays of micro-cameras <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b22">24]</ref>) that can capture gigapixel images with a wide Field-of-View (FoV). Simultaneously, devices for visualizing such high resolution data are also expanding in size and fidelity. Large-format displays or Powerwalls are a traditional visualization platform for scientific and industrial applications. While the original Powerwall only spanned a few megapixels in size, modern systems of this kind, that maintain a planar form-factor, can expand into the hundreds of megapixels <ref type="bibr">[1]</ref>. In 2012, the Reality Deck <ref type="bibr" target="#b26">[28]</ref> broke the gigapixel resolution barrier for large-format displays by utilizing 416 LCD panels in an immersive setting, providing a 360 • horizontal FoV. Visually exploring data within such systems has a number of advantages. Such facilities have been shown to greatly improve performance in basic visualization tasks ( <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b43">45]</ref> and many others). These benefits largely stem from the ability of users to naturally navigate through the data by physically moving within the space of the system. It has been shown that, when given an option between virtual and physical navigation, users will generally prefer to move to different areas of the display rather than manipulate the virtual camera <ref type="bibr" target="#b4">[6]</ref>.</p><p>An obvious side effect of the physical navigation in front (or within) a high resolution immersive system is the fact that at any point in time, a user may be at an optimal distance from some display surfaces but at a suboptimal distance from others. This means that the user may not be able to perceive the full amount of detail delivered by all displays. For gigapixel data this implies a waste in terms of computational resources and network bandwidth as the visualization system has loaded and rendered a version of the data that can not be perceived by the user due to her physical location. When integrated over the area of a large immersive system and over time, this waste is significant and can actually hamper certain use-cases (for example dynamically updating data or video that would result in local caches being invalidated at every frame). We can also consider the case where an F+C lens is applied on the visualization. If the rendering system is oblivious to the user's position within the visualization space, it will strive for maximum visual fidelity in the lens application, when a lower quality approximation would have yielded the same effect to the user and provided performance gains.</p><p>In this paper we propose optimizing the visualization process of gigapixel data by smoothly degrading the quality of the visuals based on the user's physical position in relation to the display surfaces. We apply this concept to two different aspects of the visualization pipeline:</p><p>• Rendering of gigapixel images in an out-of-core fashion using virtual texturing.</p><p>• Application of displacement-based F+C zoom lenses (based on Carpendale's Elastic Presentation Framework <ref type="bibr" target="#b8">[10]</ref>) with enhanced visual quality through GPU-based tessellation.</p><p>We term our approach acuity-driven gigapixel visualization since the rendering optimizations are guided by the analytical formulation for visual acuity. We posit that, due to the physiological basis of this formulation (based on the spacing of photoreceptors on a person's retina), the visual quality degradation will not be noticeable to the majority users of the visualization system and will not affect performance in various visualization tasks. We test our assumption via a user study, carried out by deploying our visualization framework on the Reality Deck, a 1.5 gigapixel immersive display that offers a 360 • horizontal FoV and a 33 ′ × 19 ′ × 11 ′ workspace. Our quantitative results do not show a significant effect of the gigapixel rendering modality on search task performance. Meanwhile, qualitative comments from our users illustrate that our acuity-driven tessellation scheme is visually indistinguishable from the (much slower) naive pre-tessellation. Additionally, utilizing the tracking data gathered from our study (positions of users over time) we create a number of synthetic usage scenarios. Based on these, we conduct a performance evaluation of our method, showcasing tangible gains in terms of both image data transfers (a reduction of 70%) and frame rates during the application of high-quality F+C lenses (7.5fps compared to less than 2fps for naive pre-tessellation at similar image quality). Finally, we discuss insights gained from our user study and potential visualization scenarios enabled by our acuitydriven framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND RELATED WORK</head><p>We begin by providing an overview of related work for the two aspects of gigapixel visualization that we strive to optimize, handling of gigapixel-resolution images and F+C techniques. Additionally, we provide background on perceptually optimized rendering methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Handling Gigapixel Images</head><p>Gigapixel resolution images present unique challenges due to their size. Even though modern PCs provide a large amount of memory to applications (both on the CPU and GPU side), data-set resolutions often exceed available memory capacity and demand that out-of-core schemes be utilized. Most gigapixel content is generally broken up into constant resolution tiles (usually 256 × 256 or 512 × 512 pixels in size) for more performant management and indexing. Fitting very large images to a limited amount of graphics memory has been a topic of constant interest in the rendering field. Effectively it amounts to mapping a large M × N texture into a much smaller M Phys × N Phys GPU-resident physical texture. Early work by Tanner et al. at SGI introduced the Clipmap concept <ref type="bibr" target="#b35">[37]</ref>, the notion of only using the visible part of a mipmap <ref type="bibr" target="#b40">[42]</ref> pyramid while rendering. More recently and motivated by the increasing texture requirements of video games, the concept of virtual textures has been pursued in the rendering industry <ref type="bibr" target="#b24">[26]</ref>. In virtual texture implementations, a determination of the visible parts of the original texture is made (usually in a preliminary rendering pass at lower resolution, which is read from the GPU to the host and processed on the CPU) and the necessary tiles are paged into the physical texture in the background (without blocking the rendering process in order to allow for smooth interaction). At this stage, the pipeline also makes a determination of the appropriate LoD for each pixel (we cover this aspect in more detail in the next section). Recent graphics hardware is now shipping with support for Partially Resident Textures <ref type="bibr" target="#b1">[3]</ref>, effectively implementing virtual texturing on the GPU/driver level. The visualization community has also been investigating the rendering and manipulation of gigapixel resolution data. Powell et al. <ref type="bibr" target="#b31">[33]</ref> have presented a distributed framework for image operations on high resolution data from planetary imaging with on-demand paging of tiles. The Giga-stack framework, by Ponto et al. <ref type="bibr" target="#b30">[32]</ref>, suggests a rendering scheme in which tile state is determined using a "texture table" and tiles are loaded to the GPU memory on demand in a fashion similar to Virtual Texturing. This technique has been extended include large image collections <ref type="bibr" target="#b42">[44]</ref>. Kopf et al. <ref type="bibr" target="#b19">[21]</ref> have presented a system for capturing panoramic gigapixel images using an automated panning mount, preprocessing and rendering with a custom projection and tone mapping scheme to improve the visualization quality. Summa et al. <ref type="bibr" target="#b34">[36]</ref> have introduced an efficient framework for interactive editing of gigapixel imagery. By using a progressive Poisson solver and intelligent data accessing, it allows for interactively applying complex operators on data spanning hundreds of gigapixels.</p><p>Exploring gigapixel resolution images adds an additional layer of complexity on top of the existing challenges of 2D visualization. When visualizing on a limited resolution display, salient features of the data may not even create a screen-space impact. Ip and Varshney <ref type="bibr" target="#b18">[20]</ref> have presented a framework for saliency assisted navigation of gigapixel images that combines multi-scale computer vision techniques with user feedback to detect and visualize important regions in the data. Luan et al. <ref type="bibr" target="#b21">[23]</ref> have presented a system for inserting and intelligently rendering annotations to large images. Appert et al. <ref type="bibr" target="#b2">[4]</ref> have tackled the issue of quantization in F+C lens manipulation on high resolution data, meaning the disparity between visual precision and user interface accuracy in the magnification region. Han and Hoppe <ref type="bibr" target="#b16">[18]</ref> have introduced a system for the generation of a mipmap pyramid that ensures smooth transitions between coarse and fine imagery with different visual characteristics.</p><p>Our gigapixel rendering framework utilizes virtual texturing for out-of-core rendering. This design decision is motivated by the fact that this method makes no assumptions about the underlying geometry as the visibility and LoD determination is made through a rasterization-based preprocessing pass (rather than analytically). As a result, manipulations to the gigapixel image that translate to operations on the geometry can be transparently handled with correct texturing. Out-of-core rendering (and more specifically virtual texturing) is an active research topic but to our knowledge there is no significant work on its acuity-driven integration within a large visualization environment that affords significant physical navigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Focus and Context Techniques</head><p>Traditionally, large 2D datasets are explored using a series of panning and zooming motions. However, when zooming in to examine a ROI, the context (which is very often important in the decision making process) is lost. Various techniques can be combined with traditional exploration controls to enhance the user's ability to make sense of data. A classical approach, the overview-detail model <ref type="bibr" target="#b29">[31]</ref> would display the context region in a separate viewport of the visualization, however this has the side effect of disconnecting the focal region from the overall context. F+C techniques aim to merge the focus and context regions in the same visualization by providing in-place magnification of the ROI while applying a transformation/deformation to the context (that may or may not preserve some of its properties, e.g. shape, size, etc.) F+C techniques have been investigated for a large number of visualization modalities such as tree structures <ref type="bibr" target="#b25">[27]</ref>, graphs <ref type="bibr" target="#b14">[16]</ref>, networks <ref type="bibr" target="#b36">[38]</ref> and volume visualization <ref type="bibr" target="#b10">[12]</ref>. In this paper we focus on techniques that are applicable to 2D image datasets and in particular F+C techniques for in-situ magnification that require the deformation of the transition area to maintain the size of the context. Other F+C techniques, such as the DragMag <ref type="bibr" target="#b38">[40]</ref> will present the ROI in an offset inset and show its connectivity with the context region but will not actually illustrate the transition between focus and context.</p><p>Spence and Apperley <ref type="bibr" target="#b33">[35]</ref> have proposed the concept of a bifocal representation, effectively viewing the entirety of a data set while part of it is presented to the user in full detail. This concept sparked vast amounts of research in F+C such as the generalized fish-eye view concept <ref type="bibr" target="#b13">[15]</ref>. Early work by Biet et al. <ref type="bibr" target="#b6">[8]</ref> have introduced a seethrough interface for interacting with different types of lenses (not limited to magnification). Pietriga et al. introduced the Sigma Lens concept that incorporates transitions in the time domain and formalizes the F+C lens as a composition of several different functions <ref type="bibr" target="#b27">[29]</ref>. Additionally, they proposed a framework for representation-independent magnification based on the Sigma Lens <ref type="bibr" target="#b28">[30]</ref>. In their work, a lens is implemented as a two-pass rendering method (with the first pass rendering the context, while the second pass, at a higher resolution, dealing with the Region of Interest (ROI) and the transition region). In the second pass, the resulting pixels are redirected based on a displacement and compositing function. The pixel redirection component of a Sigma-lens can be implemented in a fragment shader. The focus rendering pass of a sigma lens assumes that the ROI and transition region information is readily available for rendering. Since most gigapixel resolution schemes work in an out-of-core fashion on the GPU (as we detail later on in this section) this may not always be the case, thus making the Sigma Lens somewhat unsuitable for our target application.</p><p>Another lens formulation is the Elastic Presentation Framework (EPF) of Carpendale et al. <ref type="bibr" target="#b8">[10]</ref>. EPF places the information to be visualized on a 2D plane and renders it using a 3D perspective camera model, defining different presentation variations by displacing the points on the plane. When very large magnification factors are used, occlusion becomes a problem as the enlarged focus area blocks visibility towards the distorted context. Further work by the same group <ref type="bibr" target="#b9">[11]</ref> deals with this problem. EPF-based F+C lenses can be implemented simply on the GPU as vertex shaders displacing texture mapped geometry based on an F+C lens function. The final image quality of the F+C lens application is then directly dependent on the tessellation of this geometry and as a result, the method is not directly applicable to gigapixel images. However, the simplicity of EPF and its direct application to the GPU led us to adopt it as the F+C formulation for this work, as we describe later on.</p><p>While the vast body of work into F+C provides users with a range of techniques for data exploration, the real-world problem of applying these techniques to extremely large image datasets while maintaining performance and rendering quality has not been tackled. In particular, we are motivated by the fact that, when visualized on a large format display, the transition regions of F+C lenses can display significant visual artifacting because the underling geometry of the image is not dense enough to capture the detail of the lens function. This shortcoming is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. For the remainder of this paper, when we utilize the term "F+C lens", we are referring to zoom lenses with a smooth transition region, as defined by Carpendale et al <ref type="bibr" target="#b8">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Perceptually Optimized Level-of-Detail</head><p>The spatial resolution of the human retina decreases as distance from the retina increases. This property has been been utilized by a number of researchers in order to improve rendering performance. For example, Guenter et al. <ref type="bibr" target="#b15">[17]</ref> have achieved a 5-fold acceleration in rendering speed by combining 3 image layers rendered at decreasing spatial resolutions with an efficient antialiasing algorithm. Previous approaches focused on specific rendering modalities. Levoy and Whitaker <ref type="bibr" target="#b20">[22]</ref> have introduced gaze-directed volume rendering. Most of the work on foveal rendering utilizes eye-tracking and is targeted at single displays and/or head-mounted systems. Watson et al. <ref type="bibr" target="#b39">[41]</ref> have combined head-tracking and eye-tracking to investigate the effect of reduced peripheral acuity in large displays. Some large display systems, such as that by Minakawa et al. <ref type="bibr" target="#b23">[25]</ref> are based on this principle. However, we are not aware of any work that drives LoD based on the formulation of visual acuity and its variation during the visual exploration process within an immersive system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ACUITY-DRIVEN GIGAPIXEL VISUALIZATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Visual Acuity</head><p>We begin by providing a brief overview of the fundamentals of visual acuity, as we construct our framework on them. The amount of visual detail that is perceivable from a display is dependent on two factors, the dot pitch of the display and the viewer's distance from it. The dot pitch P is approximately equal to W H with W being the display width and H being its horizontal resolution. Assuming that the user is looking at a single pixel from distance D, the visual angle on the user's retina covered by it is V = tan −1 ( P D ). For very small visual angles, this can be approximated as V ≈ tan(V ) = P D (small-angle approximation). Based on Snellen's definition, a person of 20/20 vision can discriminate two points that are 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>60</head><p>• apart <ref type="bibr" target="#b37">[39]</ref>. This notion is termed point acuity. Using this information, we can solve for the optimal viewing distance D opt of a display for an average user. For example, for a display of W = 23.53" and H = 2560px, we can calculate P = 0.00919 " px and a D opt of approximately 31.68". While this distance obviously varies from person to person and the calculations make some assumptions about the display pixel layout, it serves as an estimation for determining a point of "diminishing returns", past which the full resolution of a display becomes increasingly imperceivable by the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Acuity-driven LoD Selection</head><p>A virtual texture pipeline maintains several sub-sampled levels of the original texture data. When texture mapping a particular pixel, the system must determine which LoD is appropriate given the screen-space impact of the geometry. On a GPU virtual texture implementation, this determination is performed within a shader using local data, in a way that is similar to traditional texture mipmapping <ref type="bibr" target="#b40">[42]</ref>. Briefly, the virtual texture pipeline makes the determination based on the imageplane spatial derivatives of the texture coordinates <ref type="bibr">[u, v]</ref> :</p><formula xml:id="formula_0">[ ∂ u ∂ x , ∂ v ∂ x ] and [ ∂ u ∂ y , ∂ v ∂ y ]</formula><p>These spatial derivatives can be used to estimate the size d of the pixel projection into texture space, based on Heckbert's heuristic <ref type="bibr" target="#b17">[19]</ref>:</p><formula xml:id="formula_1">A = max([ ∂ u ∂ x , ∂ v ∂ x ] • [ ∂ u ∂ y , ∂ v ∂ y ])</formula><p>By estimating the subdivision level that reduces this texture space projection to approximately 1 texel, one can determine the appropriate MIP level (with 0 being the original, non-downsampled texture data):</p><formula xml:id="formula_2">m MIP = log 2 (A)</formula><p>This MIP level m is a factor of the display resolution (as lower resolution implies larger texture coordinate spatial derivatives) and the orientation of the geometry with respect to the virtual camera. It does not take into account the visual information delivered to the user, based on her spatial relationship to the display. For example, a 2048px × 2048px texture is mapped on a quadrilateral that is aligned precisely with the boundaries of an imaginary 2048px × 2048px resolution display of 20" diagonal. For this display, D opt = 23.74". Based on the standard virtual texture LoD determination, for this configuration m = 0 and the highest resolution is accessed for each pixel. For a user standing at a distance ≤ D opt the entirety of this detail is perceivable. However, as the user moves away from the display, the visual angle between individual pixels crosses the 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>60</head><p>• threshold and they (and their underlying texture detail) become increasingly indiscernible.</p><p>Our acuity-driven gigapixel visualization framework is based on the assumption that, as the visual angle of the pixel on the user's retina gets smaller, so does its effective projection within the texture space. More concretely, for a certain display configuration (with dot pitch P) we can define a visual angle V opt = P D opt . Now assume that the user is at distance D ′ from the display, then the visual angle of a pixel is</p><formula xml:id="formula_3">V ′ = P D ′ or V ′ = D opt D ′ V .</formula><p>We scale the texture-space projection A of each pixel</p><formula xml:id="formula_4">A ′ = D ′ D opt A.</formula><p>The intuition behind this is that as the user moves away from the screen and neighboring pixels begin to overlap on his retina, texture coverage from both pixels should be considered, thus the texture-space projection of the pixel should be expanded. Having already determined m MIP , we calculate a secondary mipmap level m acuity as follows:</p><formula xml:id="formula_5">1 2 m acuity = D opt D ′ ⇒ 2 m acuity = D ′ D opt ⇒ m acuity = log 2 ( D ′ D opt )</formula><p>It is worth noting that for D ′ ≤ D opt , m MIP should be selected (as there is no point in actually going "lower" in the LoD pyramid if the display/geometry arrangement can not deliver the detail). So actually:</p><formula xml:id="formula_6">m acuity = max(0, log 2 ( D ′ D opt ))</formula><p>Additionally, this calculation can be biased towards quality by ensuring that the finest acuity-driven LoD is selected at each time by setting:</p><formula xml:id="formula_7">m acuity = ⌊max(0, log 2 ( D ′ D opt )⌋</formula><p>Thus at distance D ′ the final LoD level m ′ is:</p><formula xml:id="formula_8">m ′ = m MIP + m acuity</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Acuity-driven Tessellation for F+C Lenses</head><p>Applying an F+C lens onto an image under the EPF framework is tantamount to displacing the proxy geometry of the image along its normal based on the lens function. Under a rasterization environment, this implies that between individual vertices of the underlying geometry, the lens function is linearly approximated rather than accurately sampled. By naively increasing the granularity of the geometry, the lens function can be captured more accurately but at a fixed performance overhead and without taking into account the user's position in relation to the visualization. Instead, we propose that the geometry can be adaptively tessellated by considering the following two conditions:</p><p>• The curvature of the F+C lens. For example, if the lens has a flat focal region, then tessellating its geometry would not yield a visible effect. On the other hand, it makes sense to more finely tessellate the transition region of the lens in an effort to avoid the visual artifacts seen in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>• The distance of the viewer from the physical display. Similarly motivated to the acuity-driven LoD selection, the tessellation can be set to a maximum available upper limit (user or hardware imposed) when the viewer is standing at a distance ≤ D opt and reduced as she distances herself from the display.</p><p>In our acuity-driven visualization framework we calculate an adaptive parametrization for the gigapixel image geometry based on the above conditions. This parametrization is comprised of two factors: a lens-based tessellation metric and a view-based tessellation metric, which we describe below. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lens-based Tessellation Metric</head><p>The goal of this method is to provide an adaptive tessellation metric that captures the curvature of the F+C lens function F lens (x, y), parametrized over the image-plane x, y. Effectively, the tessellation should be maximized in areas that demonstrate high curvature variation (e.g. the transition regions of the F+C lens) but in the flat regions it should remain minimal (maintaining the complexity of the base mesh). Additionally, since the tessellation metric is calculated on the GPU and inside a tessellation shader (that only has local knowledge of the geometry, meaning that it can only access the vertex information of the original, coarse, primitive), the metric calculation should be feasible without global knowledge of the structure of F lens (x, y).</p><p>Each edge e i of the underlying geometry is sampled uniformly k PerEdge times at regular intervals. At each sampling point v cur , we calculate the difference di f f of the gradient of F lens (x, y) between v cur and v 0 (the first vertex of the edge), effectively capturing the curvature variation along the edge. The length of the difference is normalized by the distance between the two vertices |v Displ 0 − v <ref type="bibr">Displ</ref> cur |. The resulting tessellation factor is the maximum of this metric across e i . The algorithmic process is outlined below: </p><formula xml:id="formula_9">Algorithm 1 For edge e i = [v 0 , v 1 ] : s lens = edgeLensTessellationFactor(v 0 , v 1 , k SamplesPerEdge ) v Displ 0 := v 0 + n v 0 * F lens (v 0 ) result := 0 dir = v 1 − v 0 for i = 1, i → k SamplesPerEdge do v cur := v 0 + i * step * dir v Displ cur := v cur + n v cur * F lens (v cur ) dist := |v Displ 0 − v Displ cur | di f f := ∇F lens (v cur ) − ∇F lens (v 0 ) if | di f f | dist &gt; max then result := | di f f |</formula><formula xml:id="formula_10">:= v i + n v i * F lens (v i )</formula><p>is the geometric displacement process that results in magnification under EPF. Algorithm 1 is schematically illustrated in <ref type="figure" target="#fig_1">Figure 3</ref> View-based Tessellation Practically, render-time tessellation of geometric primitives can be defined as the choice between a base mesh with subdivision s 0 and a maximum available tessellation s max . An additional constraint is imposed by the size of the projection of the geometry on to the screen. Intuitively, edges at a distance from the virtual camera position need only be tessellated densely enough so that the resulting sub-edges have a desirable screen-space footprint. We term this amount of tessellation s view . Of course, s view ≤ s max .</p><p>In our acuity-driven gigapixel visualization framework, s view represents the upper tessellation parametrization that is utilized when the user is at distance ≤ D opt from the display. We scale this tessellation factor based on the user's actual distance D ′ from the display in order to define s acuity :</p><formula xml:id="formula_11">s acuity = D opt max(D opt , D ′ ) * (s view − s 0 )</formula><p>The impact of the view-dependent tessellation component on the parametric error can be see in <ref type="figure">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Combined Metric</head><p>We can combine these two parametrization metrics in the following way to determine s f inal :</p><formula xml:id="formula_12">s f inal = s lens * s acuity + s 0</formula><p>Effectively, s acuity sets the upper tessellation bound for the parts of F lens (x, y) that maximize the response of s lens . For example, if the user is standing at D opt from the display (and is thus maximizing s acuity ), the focal region of a flat-top F+C lens would still remain coarsely tessellated as s lens would go to zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">IMPLEMENTATION</head><p>We integrated our acuity-driven LoD selection in a gigapixel visualization pipeline based on virtual texturing. Our system supports gigapixel data in tile form (usually 256px × 256px). Each image tile at full resolution is mapped onto a single quadrilateral. With this approach (rather than utilizing a single piece of proxy geometry for the entire gigapixel image), we can work around hardware-imposed GPU tessellation limits on individual geometric primitives. The polygon count for such an approach still remains low (e.g., a 4 gigapixel images requires approximately 61000 quadrilaterals). At the same time, it allows us to implement our tessellation technique entirely on the GPU. The virtual texture visibility determination pass is performed by rendering to an off-screen buffer at 25% the resolution of the display and streamed to the CPU for processing using a Pixel Buffer Object. Pages are loaded in order of "importance" (effectively tied to their screen-space coverage).</p><p>Our acuity-driven LoD selection scheme is implemented on the GPU. Since the LoD determination for virtual texturing happens on the per-pixel level, any shaders need to be able to calculate the position of each pixel within the physical space. Assuming that the canonical screen space position p css for each pixel is known, then its physical position p phys is:</p><formula xml:id="formula_13">p phys = p screencorner + (p css .</formula><p>xy * 0.5 + 0.5) * d screen with d screen being a two dimensional vector containing the display width and height and p screencorner being the physical position of the bottom left corner of the screen in the physical space. This calculation assumes that the display lays on the x − y plane of the real world coordinate system and can be easily generalized for multiple screens that are not co-planar (as is the case for the Reality Deck, our testbed facility). The value of p phys is calculated within the GLSL shaders that are involved in the visibility determination or rendering of the virtual texture. The screen position and size information as well as the user's head position are passed to the shaders though uniform variables. Our acuity-driven LoD calculation is combined with the MIP default, as described in Section 3.2. It is worth mentioning that in our implementation, we do not interpolate between adjacent LoD levels (m ′ is clamped to the most detailed level). Our acuity-driven tessellation scheme for F+C lenses is also entirely implementable on the GPU. Our implementation of the EPL framework uses vertex shaders for displacement of the underlying mesh. In particular, we utilize OpenGL tessellation shaders for performing runtime subdivision of the proxy geometry for our gigapixel image. We calculate s acuity for each edge of every quadrilateral of the base mesh via OpenGL geometry shaders. More specifically, we utilize s f inal as the per-edge tessellation factor of the Tessellation Control shader. F lens (x, y) can either be defined analytically within the shader program or it can be precomputed and stored in a lookup texture.</p><p>Our visualization pipeline supports distributed execution and synchronized rendering. We utilize the Equalizer framework <ref type="bibr" target="#b12">[14]</ref> for proliferating render state data, window creation and event loop handling.</p><p>Our framework can be used on a variety of visualization facilities, ranging from a single desktop to a fully immersive setup. For the purposes of this paper, we deployed our software on the Reality Deck <ref type="bibr" target="#b26">[28]</ref>, a 1.5 gigapixel immersive display. The Reality Deck is comprised of 416 commercial LCD panels with an individual resolution of 2560px × 1440px. The panels have been extensively customized to reduce the bezel area and remove distractions by rerouting the OSD and indicator LEDs to the rear. The panels are arranged into 4 walls, completely enclosing the visualization space. The "front" and "back" walls are 16 monitors across and 8 monitors tall while the side walls are 10 monitors wide (we use the terms "front" and "back" loosely as the facility has no explicit orientation). A section of 5 × 3 monitors of the back wall is mounted on a mechanized door system that enables access while preserving the immersive nature of the facility during operation. The total workspace of the facility (the area enclosed within the 4 walls) is approximately 33 ′ × 19 ′ and 11 ′ tall. The Reality Deck is driven by a cluster of 18 render nodes, with dual hex-core Intel Xeon CPUs, 48GB of RAM and 4 AMD FirePro V9800 GPUs. Each node is connected to 24 displays (6 per GPU), except two corner nodes that drive 16 displays instead. The cluster is interconnected with gigabit ethernet and Infiniband networking (although the latter is not used in the experiments described in this paper). Motion tracking is provided via a 24-camera OptiTrack system. For additional information on the Reality Deck, readers may visit www.realitydeck.org.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>In this section, we provide some results of our acuity-driven framework within the Reality Deck system. First, we illustrate the expected behavior of our acuity-driven LoD scheme. The displays of the Reality Deck have a resolution of 2560px × 1440px with a diagonal of approximately 27" for a D opt of approximately 31". Given that the Reality Deck is approximately 33 ′ in width and 19 ′ in depth, with the viewer in the very middle, we would expect to see a log 2 ( 33 ′ 2 /31") bias in the LoD for the very middle of the left and right walls. Similarly, we would expect a log 2 ( 19 ′ 2 /31") bias for the very middle of the front and back walls, which is added to the MIP-based value. Indeed, this effect materializes within the Reality Deck, as <ref type="figure">Figure 5</ref> shows.</p><p>Our acuity-driven tessellation algorithm experiences a linear behavior between distance from screen and resulting tessellation. Additionally, it accurately captures the underlying structure of the F+C lens that is applied to the gigapixel image, as seen in <ref type="figure" target="#fig_3">Figure 6</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATION</head><p>At its core, our gigapixel visualization framework aims to maximize overall system performance (by minimizing the overhead associated with transferring image data to the GPU and rendering high-fidelity F+C lenses). With regard to the LoD selection, it is crucial that our technique does not hamper the user's ability to visually explore the data efficiently. Consequently, we want to quantitatively compare our acuity-driven LoD scheme against a standard gigapixel visualization pipeline (with no LoD optimizations taking place, other than the usual MIP selection). More formally:</p><p>We hypothesize that the operation of our acuity-driven visualization framework will not hinder performance in search tasks on a large format display when compared to naive gigapixel rendering.</p><p>This hypothesis of equivalency is somewhat unconventional in the domain of visualization research, as most techniques are targeted at improving user performance versus some established baseline or state of the art. In our case, the benefit of our visualization framework lies in the improvements in system performance. The purpose of the qualitative evaluation is to quantify the effect of the acuity-driven LoD selection on visualization search tasks. A negative impact would indicate that utilizing this technique is unsafe as it would result in the user requiring additional time to complete visual search tasks or miss the intended targets altogether, both undesirable outcomes. Demonstrating the equivalency of our technique in terms of user performance to a baseline system will allow for its deployment without fear of negative impact on the visual exploration process, while simultaneously realizing the system performance benefits that we discuss later. In this section we describe the design and results of a user study aimed at testing this hypothesis. Additionally, we discuss some qualitative comments from our users pertaining to the image quality of the adaptive LoD for gigapixel images and the adaptive tessellation for F+C lenses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Study Design</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Apparatus</head><p>Our user study was conducted within the Reality Deck (the configuration of which was described in Section 6). Three columns of the rear wall remained inactive during the study. This reduced the effective resolution of the Reality Deck to approximately 1.4 gigapixels. We utilized a baseball cap with mounted IR retroreflective markers (defining a rigid body) for head tracking. The tracking volume of our Opti-Track system is large enough for reliable recognition close to the walls of the facility and past the D opt of the monitors. A joypad controller was utilized for scene manipulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>We recruited 10 graduate and undergraduate students to participate in our study. Average age was 26 (σ = 2.93). Seven participants were male and 3 were female. Two had prior experience with the Reality Deck and scientific visualization in general. However, the task (described in the following paragraph) did not require any domainspecific knowledge and all users were given the opportunity to familiarize themselves with the Reality Deck facility prior to commencing the study. Additionally, the exploration of the task data sets during the quantitative portion of the study was conducted purely through physical navigation, eliminating any potential bias due to unfamiliar user interfaces. Consequently, we posit that this discrepancy in user familiarity with the visualization domain did not significantly affect our quantitative evaluation. Finally, all participants reported average ( 20 20 ) vision either naturally or through corrective glasses or lenses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tasks and Procedure</head><p>We wanted to evaluate the impact of our acuity driven gigapixel visualization (termed ADGV) on the data exploration process and compare it against the baseline "standard" gigapixel visualization (SGV). This variable is termed technique (TECH). We designed a number of search tasks targeted at testing our hypothesis within our gigapixel resolution system. We utilized the High Resolution Imaging Science Experiment (http://hirise.lpl.arizona.edu/) data set to generate a gigapixel resolution image of Mars topography. The original data spans approximately 1.8 gigapixels and is texture-mapped to an ellipsoid that surrounds the display volume of the Reality Deck (the resolution of the visible part of the data was approximately 1.2 gigapixels). This vast landscape serves as the background for our search task. Within this backdrop, we hide 3 targets that the study subjects must locate. Our tasks are split into 3 difficulty levels (DIFF): easy, medium and hard (denoted by the suffixes -E, -M and -H). The differentiator between the levels is the size of the targets in relation to the data set. In the -E level, targets span 256px × 512px, in -M their size is 128px × 256px and in the -H condition they are reduced to 25px × 50px. <ref type="figure">Figure 7</ref> shows the data set used in the study and illustrates the scale of the targets in relation to the backdrop. The artwork used as a target in our study is available in the public domain. The targets were similar in hue and brightness to the background data set. For this task, we recorded Elapsed Time (ET) until discovery of all 3 targets. Furthermore, we captured positional information for the majority of the users for further analysis.</p><p>Additionally, we wished to evaluate the image quality provided by our F+C adaptive tessellation scheme. For this purpose, we utilize a second gigapixel photograph, this time from Dubai (available from http://www.gigapan.com/gigapans/75554). We placed an EPF-based F+C lens, aligned with the center of the view frustum and connected to the translation of the virtual camera. Subjects could translate along the x − y image plane using the left stick of the joypad controller (rotation was fixed) and could alternate between our acuity-guided tessellation (ADGV), a coarse version of the image geometry (NGV) and a 16-fold pre-tessellated version of the same mesh (PRE). They were also free to move within the visualization space. This evaluation was qualitative and subjects were asked to comment on their method of preference in terms of image quality.</p><p>Upon arriving at the study location, the users received a brief description of the Reality Deck and its function. The mechanized door was closed and the users were allowed to familiarize themselves with the space. At this stage, the screens of the Reality Deck showed a static splash image. After being queried for their demographic information, the subjects were shown a picture of the survey target and instructed to look for 3 such targets in each task. They were also asked to wear the <ref type="figure">Fig. 7</ref>. Cross-section of the HiRISE data set that was visible on the displays of the Reality Deck during the study. The resolution is approximately 85K × 14K pixels. The insets show progressively zoomed-in crops from the main view, illustrating the size of the targets used in our study in relation to the entire data set. The right-most inset shows three targets for the -E, -M and -H values of DIFF (respectively from left to right). The smallest target's dimensions are 25px × 50px. head tracking prop. This was a single-blind experiment and the subjects were not aware of the choice between ADGV or SGV (or even the fact that the LoD strategy was the between-subjects variable). Both the order of task difficulty (-E, -M or-H) and the order of method were determined using a Latin Square. For each difficulty, both methods were tested in succession and the user was queried on the quality of the last two visualizations in an effort to determine whether the presence of ADGV was noticeable. In summary, the design of the quantitative part study was 10 participants × 3 tasks × 2 methods = 60 interactions in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results and Discussion</head><p>We began by examining the effect of TECH on ET. Student's t-test for each DIFF (α = 0.05) did not show a significant effect (For -E, t(9) = 0.867, p = 0.408, for -M, t(9) = 0.568, p = 0.584 and for -H, t(9) = 1.110, p = 0.296). We performed between-TECH Two One Sided T-test (TOST) equivalence analysis for each DIFF with a threshold of 5% of mean ET but the evidence was not conclusive (for the sake of completeness, relevant t-statistics and p-values can be seen in <ref type="table">Table 1</ref>). We also evaluated the percentage difference in performance of ADGV by considering SGV as the baseline performance (in order to normalize the performance metric between subjects). We applied Student's t-test on this percentage metric, with an expected mean of 0. This approach also did not indicate a significant effect of TECH on ET (for -E t(9) = 1.45, p = 0.089, for -M t(9) = 0.492, p = 0.317 and for -H t(9) = 0.016, p = 0.494). While there is no evidence suggesting an effect of TECH on ET at any DIFF, we can not concretely claim that there is no statistically significant effect at our chosen threshold level.</p><p>We conducted post-hoc analysis on the positional tracking data captured during our user study. We focused, in particular, on the -H task as it would potentially require the most adjustment of the users' physical location in order to accommodate for the shifts in visual quality during the exploration process. We calculated d closest , the distance of the user from the closest screen at any point during the exploration process. A TOST equivalence test of d closest indicated that there is no statistically significant effect of TECH on d closest at p-values 0.013 (t(9) = 2.684) and 0.012(p(9) = 2.726) at 7% of the mean (approximately 5"). This leads to the conclusion that users did not have to significantly adjust their average distance from the display while exploring the data under ADGV.</p><p>Out of the 10 study participants, only 1 rated SGV as demonstrating superior image quality to ADGV during any of the tests (this user described herself as having experience with scientific visualization). Three users commented that under certain DIFF values, they actually found the image quality of ADGV to be superior (which is obviously impossible since SGV presents an upper bound to image quality). Under all other conditions, the subjects claimed to find no perceivable difference in image quality between ADGV and SGV. We interpret these findings as an indication that the effect of TECH on the perceived image quality is generally limited, at least under our test datasets.</p><p>Finally, we asked users to evaluate the image quality of F+C lens application. All study participants rated ADGV as being of equal visual quality to PRE. Interestingly, only 2 users (who identified themselves as having experience in scientific visualization) could easily pin-point the improvement in visual quality between ADGV and NGV (the smoother approximation of the F+C lens function). The rest of the subjects could detect a visual difference, but could not definitively say whetherADGV looked better than NGV. Some users characterized the ADGV visualization as "bulging" or "bowing out", a result of the image geometry following the smooth underlying function more closely. This finding raises some questions about the tangible effect of ADGV on the user experience for non-experts. <ref type="table">Table 1</ref>. Report of our TOST equivalence analysis between ADGV and SGV for each DIFF at a = 0.05. We did not observe an effect of TECH on ET for any value of DIFF. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DIFF</head><formula xml:id="formula_14">-E -M -H µ &gt; θ µ &lt; −θ µ &gt; θ µ &lt; −θ µ &gt; θ µ &lt; −θ t<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Performance</head><p>Utilizing the motion capture data acquired during our user study, we created 3 synthetic use cases under which we evaluated the performance of our framework. We focused on the -H difficulty level since it yielded the longest overall user sessions. Based on the tracking data, we created a heatmap of the users' positions within the visualization space of the Reality Deck (at a resolution of 160 × 100 cells). We then drew 1500 random samples from the non-zero heatmap locations and performed k-means clustering on them to yield 6 "hotspots" of user presence. Given this list of hotspots:</p><p>1. We randomly select p start from the list.</p><p>2. We also randomly select p next from the list.</p><p>3. We traverse the path p next − p start at a speed of 2.8mph at 120Hz 4. We remove p start from the list, set p start = p next and repeat from</p><p>Step 2 until the list is empty.</p><p>We opted for this synthetic data set creation instead of utilizing prerecorded data directly from our users in order to reduce the sensitivity of our benchmarks to the search patterns of any one particular study subject. In total, we created 3 data sets (A, B, C), with duration 145, 216 and 175 seconds. For technical reasons, we had to limit our benchmarking to the front, left and right display surfaces of the Reality Deck (a total of 13 nodes and approximately 1.268 gigapixels of display space).</p><p>Using these synthetic user sessions, we evaluated two performance metrics, one for each aspect of our technique:</p><p>• Data transfer overhead -the number of virtual texture pages required to fully texture map the surface of the Reality Deck facility under ADGV and SGV. Since these pages ultimately need to be transfered from some remote source (long term storage or otherwise) to the render nodes and then to the GPUs, their count serves as a good indicator of system performance under different values of TECH.</p><p>• Frame rate -our acuity-driven tessellation framework is evaluated by measuring the cluster-wide frame rate of the visualization application while displaying an F+C lens under ADGV and PRE (with an s pre = 16). For the sake of comparison, we also report the frame rate under NGV (which demonstrates significantly lower image quality).</p><p>The only differentiating factor between each benchmark was the visualization technique, allowing us to directly measure any performance gains over the baseline implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Transfer Overhead for Acuity-driven LoD Selection</head><p>To evaluate the impact of our technique on data transfer overhead, we measured the average number of pages per frame required to completely texture map the display surfaces of the Reality Deck facility. In order to simulate dynamic gigapixel data, we invalidated entirety of the page cache at each node every frame. As expected, under SGV, the number of pages required for proper texturing is almost fixed (≈ 18, 000, with very small variations occurring due to minute differences in the cluster-wide frame rate under each scenario). If one assumes an average tile size of 9kB (as is the case with our data), approximately 162 megabytes of data per frame are required for complete coverage of the three walls of the Reality Deck. That translates to 4.9 gigabytes per second worth of data transfer (with the system running at a hypothetical 30 frames per second). Conversely, when ADGV is active, under scenario A we observed a 70% reduction in page transfer overhead. Scenario B demonstrated a decrease of 68% and for scenario C the savings were 72%. To underline the significant reduction in data transfer overhead, under ADGV, assuming again a 30fps frame rate, the required bandwidth drops to approximately 1.5 gigabytes per second. 5fps. NGV exhibits artifacting since the coarse base mesh can not accurately capture the smooth variation of the lens function. This shortcoming manifests as sharp "bends" along straight lines in the image. ADGV delivers a much smoother visual result that is indistinguishable to PRE while maintaining a substantially higher frame rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Frame Rates for Acuity-driven Tessellation</head><p>Our acuity-driven tessellation scheme (ADGV) aims at bettering the rendering quality of F+C lenses while improving system performance.</p><p>To quantify this gain, we the average cluster-wide average frame-rate under the three synthetic user sessions, while applying ADGV on a single F+C lens. This results in a measurement of 7.5fps with an s max of 64. This is in contrast with the performance of naive pre-tessellation (PRE) with a fixed tessellation factor s pre = 16. Under this condition, the frame rate was under 2fps while the upper visual quality bound is actually lower when compared to ADGV (since s pre &lt; s max ). As an additional point of comparison, applying a lens directly on the base mesh (the NGV technique) provided a frame rate of 15fps at substantially reduced image quality. Overall, our technique provides image quality that is not distinguishable from naive pre-tessellation (according to the qualitative segment of our user study), at a substantially faster frame rate. These frame rates, along with captures from within the Reality Deck illustrating the improvements in visual quality, are summarized in <ref type="figure" target="#fig_4">Figure 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION AND FUTURE WORK</head><p>In this paper, we introduced a framework for acuity-driven gigapixel visualization. Our method utilizes the formulation for visual acuity to guide the LoD selection process for a virtual texturing pipeline. Using the same formulation, we can improve the visual quality of F+C lenses, applied to gigapixel images, through adaptive tessellation. We conducted a user-study in the Reality Deck, an immersive gigapixel display, which did not show an effect of the acuity driven LoD in search tasks of various difficulties. Also, we determined that users did not have to adjust their distance from the display during the visual exploration process. Qualitatively, our adaptive LoD approach was rated equal to "naive" gigapixel visualization by most users. Our adaptive tessellation for F+C lenses rated equal to a pre-tessellated version in terms of image quality. However, when compared against a coarse mesh, our scheme was rated superior only by experts. While laymen could detect a difference in the rendering, they could not concretely state which of the modalities sported the superior visual quality. Nevertheless, the acuity-driven LoD approach yielded very tangible benefits in terms of data transfer overhead (≈ 70% or 3.4 gigabytes per second at 30fps in our synthetic benchmarks) while the adaptive F+C lens tessellation showed significant frame rate improvements compared to naive pre-tessellation. It is worth noting that there exist a large body of work in the LOD field that utilizes perceptual metrics to drive the simplification of a dense mesh. For example, the approach of Williams et al. <ref type="bibr" target="#b41">[43]</ref> utilizes a Contrast Sensitivity Function (CSF) as a heuristic for traversing a multi-triangulation (MT) structure. Such approaches are similar to our proposed acuity-driven LoD framework at a fundamental level (since the CSF builds upon the same physiological basis as visual acuity). However, there are also a number of differentiators. These stem partly from a narrower problem scope (gigapixel images with EPF lenses versus generic geometry) and the fact that we target a large immersive facility. First of all, our method does not require the precomputation of an MT or other LOD structure, apart from the MIP pyramid (which is already present in most gigapixel rendering pipelines) and runs entirely on the GPU. Additionally, our method considers the position of the user in relation to each display in an immersive space and adapts as she moves, rather than assuming a fixed position (which might be necessary during the preprocessing step of an LOD algorithm). Finally, our scheme for F+C lenses works in a way that is opposite to most LOD approaches. Instead of attempting to simplify a complex mesh based on perceptual criteria to fit a polygon budget, our algorithm refines a coarse image mesh based on the physical distance of the user from the display while trying to approximate the underlying lens function. These differences make direct comparisons between our technique and work in the LOD field challenging. Still, there is valuable insight to be gained from this body of work. For example, the "texture stretch" metric by Sander et al. <ref type="bibr" target="#b32">[34]</ref> could be integrated with our adaptive tessellation scheme to minimize texture-space distortions during F+C lens application.</p><p>Looking forward, we would like to pursue a more thorough evaluation of our acuity-driven LoD selection that would include a larger sample size, different types of image data and domain-specific reasoning tasks. Secondly, we would like to expand our framework in order to support multi-user scenarios. Additionally, we wish to explore the potential for a predictive approach to LoD selection, accounting for the user's trajectory in the space. Also, we would like to investigate the applicability of perception-driven techniques within a large immersive system that affords significant freedom of movement to the user without permitting the deployment of bulky eye-trackers. Such endeavors would require the expansion of existing understanding of the correlation between head and eye movement (e.g. <ref type="bibr" target="#b5">[7]</ref>) in order to take into account the translation of the user within the visualization space. Finally, we want to investigate the impact of acuity-driven visualization on the feasibility of rendering dynamic data, such as gigapixel video, in real-time over a bandwidth-constrained network.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Without adaptive tessellation, the geometry that an image is mapped on may not be able to capture the smoothness of the F+C lens, particularly in the transition region.The red boxes annotate some of the visible rendering artifacts. Top left inset: Wireframe of the underlying geometry after lens application. The gigapixel photograph of Dubai used in this figure is located at http://www.gigapan.com/gigapans/75554.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Schematic representation of our lens-based tessellation factor calculation. Starting at vertex v 1 the algorithm calculates the difference between v 0 and other vertices along the edge at regular intervals. The inlet shows the resulting di f f for this invocation of the algorithm. In this example, di f f is defined using the the sampled F+C function gradients that show maximum divergence (in this case v 0 and v 2 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Comparison of the parametric error for applying an analytical Gaussian lens under different conditions. Each pixel has a [grey, blue, green, red] hue if the error is [&lt; 0.1,&lt; 0.5,&lt; 1.0,≥ 1.0] pixels respectively. (a) A low-density static mesh. (b) Our acuity-driven tessellation algorithm at a distance of approximately 9 ′ from the display. (c) Our proposed method at D opt = 31". At D opt , the parametric error for the majority of the lens is reduced to less than 0.5 pixels. The visible Moire pattern is a result of the parametric error varying along the new sub-edges, with it being minimal at the added vertices and slightly increasing along each edge. Results of the proposed acuity-driven LoD selection. Without acuity-driven visualization, the entire scene would be texture-mapped at LoD 0. However, using our acuity-driven gigapixel visualization framework, the system adaptively selects the LoD based on the viewer's position within the space. (a) Standing in the middle of the visualization space and based on the D opt value of the display (≈ 31"), the system selects LoD 4 for the visualization. (b) The user comes within D opt of the middle of the front wall, thus the LoD 0 is selected to offer the maximum amount of visual detail for that area. (c) The high detail area tracks the user as she moves to the front-left corner of the facility.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Off-axis rendering of 3 overlapping F+C lenses. Left column: Using a coarsely tessellated mesh. Right column: using our adaptive tessellation technique. Our method accurately captures the underlying lens function and eliminates rendering artifacts. The bottom row shows zoomed views of the areas annotated in red, illustrating the smooth deformation that is achievable with our method and the improvement in visual quality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Summary of the performance evaluation of our acuity-driven tessellation scheme for F+C lenses. (a) NGV -15fps. (b) PRE with s pre = 16 -&lt; 2fps. (c) ADGV with s max = 64 -7.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">ACKNOWLEDGMENTSThis work has been supported by the National Science Foundation, grants IIS0916235, IIP1069147 and CNS0959979.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gigapan</surname></persName>
		</author>
		<ptr target="www.gigapan.org" />
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m">AMD. Partially resident textures. www.amd.com</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">High-precision magnification lenses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Appert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pietriga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th International Conference on Human Factors in Computing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="273" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Effects of tiled high-resolution display on basic visualization and navigation tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>North</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CHI Extended Abstracts on Human Factors in Computing Systems</title>
		<imprint>
			<biblScope unit="page" from="1196" to="1199" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Realizing embodied interaction for visual analytics through large displays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>North</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="380" to="400" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Vestibulo-ocular function during co-ordinated head and eye movements to acquire visual targets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Physiology</title>
		<imprint>
			<biblScope unit="volume">287</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="127" to="147" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Toolglass and magic lenses: The see-through interface</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Bier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Derose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGGRAPH</title>
		<imprint>
			<biblScope unit="page" from="73" to="80" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multiscale gigapixel photography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gehm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kittle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Golish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">486</biblScope>
			<biblScope unit="issue">7403</biblScope>
			<biblScope unit="page" from="386" to="389" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A framework for unifying presentation space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S T</forename><surname>Carpendale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Montagnese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th Annual ACM Symposium on User Interface Software and Technology</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="61" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Achieving higher magnification in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carpendale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ligh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pattison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th Annual ACM Symposium on User Interface Software and Technology</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="71" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The volume in focus: Hardware-assisted focus and context effects for volume visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Brodlie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Phillips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Symposium on Applied Computing</title>
		<imprint>
			<biblScope unit="page" from="1231" to="1235" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gigapixel computational imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">S</forename><surname>Cossairt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Miau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computational Photography (ICCP)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Equalizer: A scalable parallel rendering framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eilemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="436" to="452" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<title level="m">Generalized fisheye views. SIGCHI Conference on Human Factors in Computing Systems</title>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="page" from="16" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Topological fisheye views for visualizing large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Gansner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>North</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="457" to="468" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Foveated 3d graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guenter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snyder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">164</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Optimizing continuity in multiscale imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="171" to="181" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Texture mapping polygons in perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Heckbert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
			<biblScope unit="page">13</biblScope>
		</imprint>
		<respStmt>
			<orgName>NYIT Computer Graphics Lab Technical Memo</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Saliency-assisted navigation of very large landscape images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Ip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varshney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1737" to="1746" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Capturing and viewing gigapixel images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Uyttendaele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Deussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gaze-directed volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Whitaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="217" to="223" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
		<title level="m">Annotating gigapixel images. ACM Symposium on User Interface Software and Technology</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="33" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Optical testing of the aware wide field 2-gigapixel multiscale camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mclaughlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gehm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Brady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Optics</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A multi-detailed spatial immersive display</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Minakawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Moriya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Takeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th International Conference on Artificial Reality and Tele-existence</title>
		<meeting>11th International Conference on Artificial Reality and Tele-existence</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="48" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Advanced virtual texture topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mittring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGGRAPH Games</title>
		<imprint>
			<biblScope unit="page" from="23" to="51" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Treejuxtaposer: scalable tree comparison using focus+context with guaranteed visibility</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munzner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Guimbretiere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tasiran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="453" to="462" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Building the Reality Deck</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Kaufman</surname></persName>
		</author>
		<ptr target="http://www.powerwall.mdx.ac.uk/" />
	</analytic>
	<monogr>
		<title level="m">POW-ERWALL: International Workshop on Interactive, Ultra-High-Resolution Displays, SIGCHI Conference on Human Factors in Computing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sigma lenses: Focus-context transitions combining space, time and translucence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pietriga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Appert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">26th Annual SIGCHI Conference on Human Factors in Computing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1343" to="1352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Representation-independent in-place magnification with sigma lenses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pietriga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Appert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="455" to="467" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Image-browser taxonomy and guidelines for designers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Plaisant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Software</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="21" to="32" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Giga-stack: A method for visualizing giga-pixel layered imagery on massively tiled displays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ponto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Doerr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kuester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="693" to="700" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A scalable image processing framework for gigapixel mars and other celestial body images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Aerospace Conference</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Texture mapping progressive meshes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gortler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="409" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Bifocal Display. The Interaction Design Foundation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Spence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Apperley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<pubPlace>Aarhus, Denmark</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Interactive editing of massive imagery made simple: turning atlanta into atlantis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Summa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Scorzelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-T</forename><surname>Bremer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pascucci</surname></persName>
		</author>
		<idno>7:1-7:13</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The clipmap: A virtual mipmap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Tanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Migdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="151" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hyper mochi sheet: A predictive focusing interface for navigating and editing nested networks through a multifocus distortion-oriented view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Toyoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shibayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCHI Conference on Human Factors in Computing Systems</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="504" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Information visualization: perception for design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ware</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">The dragmag image magnifier. Human Factors in Computing Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ware</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="407" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Managing level of detail through head-tracked peripheral degradation: a model and resulting design principles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Hodges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Symposium on Virtual Reality Software and Technology</title>
		<meeting>the ACM Symposium on Virtual Reality Software and Technology</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="59" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pyramidal parametrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Perceptually guided simplification of lit, textured meshes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luebke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kelley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schubert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Symposium on Interactive 3D Graphics</title>
		<meeting>the 2003 Symposium on Interactive 3D Graphics</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="113" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Visualization of high-resolution image collections on large tiled display walls</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yamaoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-U</forename><surname>Doerr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kuester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="498" to="505" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Beyond visual acuity: the perceptual scalability of information visualizations for large displays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Haciahmetoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>North</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
