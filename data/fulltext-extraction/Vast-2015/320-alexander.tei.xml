<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Task-Driven Comparison of Topic Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Alexander</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Michael</forename><surname>Gleicher</surname></persName>
						</author>
						<title level="a" type="main">Task-Driven Comparison of Topic Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TVCG.2015.2467618</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Text visualization, topic modeling</keywords>
			</textClass>
			<abstract>
				<p>Fig. 1: Buddy plots show consistency of document relationships across topic models by encoding similarity with respect to individual documents. In this figure, each row represents a document, with the rest of the corpus encoded as circular glyphs along the row. Distance from the row&apos;s document in one model is encoded using horizontal position, while distance in a second model is encoded using color. This combination of encodings lets us see similarities from two models within one row of glyphs. Deviations in similarity between the two models can be identified as breaks from a smooth gradient. Though the two models seem to correlate well with documents at either extreme (blue documents to the left, red documents to the right), we see dramatic shifts between different classifications for documents in between, identified by breaks in the blue-to-red gradient structure.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Probabilistic topic modeling is an increasingly popular method for performing text analysis in a wide variety of domains. This suite of algorithms extracts collections of words that tend to appear in the same documents in an attempt to discover semantic connections. The ability to find such connections within large corpora without needing to read through every document makes topic models attractive. Due to the size and complexity of the resulting models, visualization can be helpful for exploring these results. There have been many tools in recent years designed to expose a single model to the user. However, these tools rarely facilitate direct comparison between models.</p><p>The ability to compare topic models offers a number of useful insights. For example, such comparison can help pick which model to use within the parameter space, which can be vast. (To be clear, we are using the term "topic model" to describe an instance of output given by a topic modeling algorithm run on a corpus.) Other reasons for comparison include validating findings (i.e. making sure that a pattern appearing in one model also appears in another) and evaluating different modeling or pre-processing techniques. Typically, these comparisons are relegated to numerical metrics, reducing comparison to the optimization of a single number. While this is useful for being able to iterate through many models quickly, these metrics can be inconsistent and often fail to align with human judgments <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b29">30]</ref>. A single number also gives little insight into the significance of differences between models as it tends not to provide prescriptive recommendations beyond "this model ranks higher." Difference is only significant to the degree that it affects what the user will actually do with the model. Therefore, we need to support task-driven topic model comparison. Such comparison would be easier to interpret, especially for users familiar with their tasks but unfamiliar with machine learning concepts like held-out likelihood and mutual information. By framing differences between models in terms of the task being performed, we can allow the user to decide which differences are significant for their particular use case. This can lead to a more nuanced understanding of the models, building user trust in the process.</p><p>Task-driven comparison must address the wide variety of tasks for which topic models are used. These range from full-corpus summary, to comparisons between documents or groups of documents, to investigations of evolution over time. To account for this range, we com- pose a list of comparative tasks that correspond to the most frequent single-model tasks, which we group into the categories of understanding topics, understanding similarity, and understanding change. We outline cross-model comparisons to be performed for each of these categories and describe visual techniques for doing so. In selecting the techniques seemingly best suited for the comparison tasks, we use a number of pre-existing encodings as well as a novel encoding for document similarity. Together, these enable in-depth exploratory comparison of topic models.</p><p>Our main contributions in this paper are:</p><p>• A characterization of the problem of topic model comparison, and an argument for a task-driven solution</p><p>• A survey of comparison tasks that corresponds to the singlemodel tasks for which topic models are already used</p><p>• Visual analytics approaches to address these comparison tasks, including the application of standard visualizations as well as more novel designs including buddy plots, a novel way of looking at relative distances within a corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Topic modeling is a text processing technique that determines themes within a collection of texts using statistical analysis <ref type="bibr" target="#b5">[6]</ref>. While there is a broad and evolving range of available techniques, most produce results of a similar form: topics are represented by sets of commonly occurring words, allowing documents to be assigned to topics by considering the words they contain. Most topic modeling techniques are probabilistic, creating weighted assignments of words to topics. Model comparison is typically done either by hand or using statistical metrics. The former method-analyzing each model individually using whatever tools are available-is often resorted to in practice, but is inefficient for large and complex models. Instead, statistical metrics of a model's quality, such as predicted likelihood on a set of held out documents, provide more expedient methods of comparison. However, the variety of ways for calculating this likelihood tend to be inconsistent and inaccurate <ref type="bibr" target="#b31">[32]</ref>, often disagreeing with human judgments of the documents <ref type="bibr" target="#b8">[9]</ref>. Chang et al. <ref type="bibr" target="#b8">[9]</ref> offer an alternative method for computing model quality called "topic intrusion," which asks human subjects to pick out topics that have been artificially inserted into document distributions. Though this can be effective, it relies on having a large pool of participants to perform evaluation.</p><p>Rather than judging the quality of a model as a whole, some metrics instead judge individual topic quality. These metrics have used pointwise mutual information computed with an external corpus <ref type="bibr" target="#b28">[29]</ref> and document co-occurence within the original corpus <ref type="bibr" target="#b25">[26]</ref>. However, these may disproportionately favor certain modeling types <ref type="bibr" target="#b29">[30]</ref>. AlSumait et al. <ref type="bibr" target="#b2">[3]</ref> have developed a topic significance ranking that defines a topic distribution's quality as its distance from three "junk" distributions: uniform across the vocabulary, equivalent to the corpuswide distribution of words, and uniform across all documents in the corpus. While these are interesting and important qualities, the method sacrifices explanatory power for simplicity by combining them into a single numerical metric. These metrics have been shown to be inconsistent with each other and human judgment <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>Other metrics of topic quality take advantage of human input. "Word intrusion" metrics measure the consistency with which subjects can identify a fake topic word inserted into the list <ref type="bibr" target="#b8">[9]</ref>. Chuang et al. <ref type="bibr" target="#b10">[11]</ref> compare topics to expert-generated categories to filter them into classifications of junk, fused, missing, repeated, or resolved. However, though both of these methods are informative, they can again be impractical for those without access to a wide pool of participants.</p><p>Some visual approaches to model evaluation and comparison have been developed. Chuang et al. <ref type="bibr" target="#b10">[11]</ref> use matrix views to compare topics to ground-truth concepts generated by a group of experts. Though their method could be extensible to comparing two models, it is difficult to answer questions of why the models are different and which are better without the expert ground-truth. Crossno et al. <ref type="bibr" target="#b13">[14]</ref> use bipartite graphs and two-dimensional embeddings to compare models built with LDA and LSA. Though their visual techniques are informative and use encodings very similar to those we present for aligning topics, they do not let the user drill deeply into individual relationships between topics, nor do they address the wider variety of tasks for which topic models are used. Our encodings for topic alignment add finer detail about different levels of alignment in the form of Pareto bar charts, and allow direct comparison of the words included so as to help identify split and merged topics across models (see Section 4.1.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MOTIVATION</head><p>There is no one perfect model for any given corpus. Rather, there are many possible models that can be created, each with different strengths and weaknesses depending on the scope of the user's questions and the granularity of the patterns in which they are interested. In this section, we outline these decisions and tasks, and describe why we believe they require a different set of model comparisons than is currently available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The parameter space</head><p>The decisions that go into creating a topic model form a vast parameter space. The first is deciding which technique to use. Latent Dirichlet Analysis (LDA) <ref type="bibr" target="#b5">[6]</ref> is increasingly common, but other methods include latent semantic analysis (LSA) <ref type="bibr" target="#b15">[16]</ref> and non-negative matrix factorization (NMF) <ref type="bibr" target="#b35">[36]</ref>. Some versions incorporate expert knowledge <ref type="bibr" target="#b4">[5]</ref> or build hierarchical topic structures <ref type="bibr" target="#b18">[19]</ref>. These model types have been show to perform differently depending on the task <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>There are a number of technique-specific parameters that affect the specific instance of a model. The most obvious is the number of topics to extract, the optimum for which will be highly dependent on the corpus, but there are often many others. For instance, in LDA, the coarseness of the topic and document distributions are controlled by two hyperparameters, α and β . Although end-user topic modeling packages like MALLET often hide these parameters from the user <ref type="bibr" target="#b24">[25]</ref>, they can have a significant impact on topic legibility <ref type="bibr" target="#b30">[31]</ref>.</p><p>Though there has been some work visually exploring this hyperparameter space <ref type="bibr" target="#b10">[11]</ref>, there are also many parameters that go into the pre-processing steps of model creation. For example, the analyst may choose to filter out stopwords: words that are considered to be without semantic meaning but may still be common enough to dominate statistical analysis. Obvious examples include articles, pronouns, and other function words, but can also include proper nouns or domain-specific terminology. Though there are default lists built into many tools, the precise definition of a stopword largely depends on what the user is looking for. For example, for a journalist building a topic model on newspaper articles, proper nouns like "Obama" might be a semantically important to include in analysis, while a literature scholar might not want to associate two plays just because they both contain a character named "Antonio." Even treatment of function words can vary, as there are scholars interested in such words' ability to distinguish between groups or authors <ref type="bibr" target="#b20">[21]</ref>.</p><p>Another pre-processing step that is not always considered in evaluation is the practice of "chunking" documents. When working with longer documents, the concepts of context and co-occurrence can get a bit fuzzy. We might be able to assume that two words in a journal abstract are semantically related, but what about two words at either end of a novel? Or a chapter? Or a scene? A common way of managing this discrepancy is to cut larger documents into chunks-either semantically (e.g. chapter or scene) or numerically (e.g. by number of words). However, the size of these chunks is largely dependent on the level of granularity at which the user wants to explore patterns-again, on the particular task they have in mind.</p><p>There are many other possible parameters a user might want to compare (see <ref type="bibr">Section 5)</ref>. Though there are some objective performance differences, given the task specificity of these parameters, it is not enough to rely solely on statistics that return a number. We want to instead help the user build insight into these decisions by supporting comparison that is parameter agnostic, showing the effects of their choices on the tasks they want to perform. Understanding the tasks most often performed using topic models can help inform decisions within this parameter space. From our own experience and examination of literature, we believe that the majority of uses fall into these categories:</p><p>Understanding topics Topic models are often used as a tool for summarizing documents, pulling out the most important topics without having to read through everything. Connected to this task is understanding, summarizing, and naming individual topics. Though topics are often represented by their top three words <ref type="bibr" target="#b7">[8]</ref>, word clouds <ref type="bibr" target="#b34">[35]</ref>, or ordered lists <ref type="bibr" target="#b11">[12]</ref>, how best to convey them comprehensibly remains an open research question.</p><p>Understanding similarity Topic models can provide an unsupervised distance function for comparing documents, both globally (how do documents cluster across the corpus; how well do clusters match human categories) and locally (which documents are most similar to a particular document of interest). Using models as a metric for similarity can also be useful for identifying individual documents that are exemplary of a topic or trend, or else that are outliers from the rest of the corpus <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>Understanding change Seeing how corpora change over time is one of the most common uses of topic modeling. Temporally focused tasks include both looking for large trends and patterns as well as particular events <ref type="bibr" target="#b21">[22]</ref>, often in correlation with outside historical data. Visual analysis is particularly well-suited to address these sorts of tasks, and many tools incorporate some form of temporal river flow visualization for doing so <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>Each of these single-model tasks corresponds to a type of comparison that a user might make between models ( <ref type="table" target="#tab_2">Table 1</ref>). The primary comparison task involved in understanding topics is topic alignment: how well do the topics from the two models match? If they do not, why not, and what do those differences mean?</p><p>When considering the similarity of documents, topic models can be used to calculate distances between documents which can be compared. These distance comparisons can be made both globally (do the models agree upon how to group the documents) and locally (how do the document neighborhoods change from one model to another).</p><p>Finally, we refer to temporal comparisons between a collection of documents as timeline comparison. How well are the models aligned in time? Do they pick out similar important events? Do topics that share similar words also evolve in the same ways?</p><p>There is overlap between these comparative tasks, but the requirements of these tasks are sufficiently distinct to require different visual techniques to conduct effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">VISUALIZING COMPARISON</head><p>Most topic models can be represented as three matrices containing three different sets of probability distributions: a matrix associating documents and topics, a matrix associating topics and words, and a matrix associating documents and words. One could frame the problem of topic model comparison as simply one of matrix comparison. However, though matrix views can be useful, there are a number of reasons why they are insufficient by themselves. It is difficult for visualizations of these matrices to scale, with topics on the order of hundreds, documents on the order of hundreds or thousands, and words on the order of tens of thousands. Depending on the questions the user might ask, pure matrix views might not give much flexibility for exploring individual documents or derived relationships between smaller sets of documents and topics. Finally, these matrices do not connect directly to the tasks.</p><p>Given the importance of such relationships and metadata in the tasks for which models are used, we explore visual encodings that address them directly in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Understanding topics</head><p>Our predominant concern when considering topics across two models is topic alignment: which topics best match with one another, and how. Matching can mean either sharing the same documents or the same words. This comparison is important for tracking consistency and correspondence across models. In two closely aligned models, we would expect to see a one-to-one matching, but this is not always the case, either because of parameter differences or random variations.</p><p>There are a few common phenomena to look for when comparing topics across models:</p><p>• Matched topics -Some topics will be (close to) direct matches of one other, sharing distributions of both words and documents.</p><p>• Split/merged topics -A topic from one model will occasionally split into multiple topics in another model (or, multiple topics may merge, depending on the direction of comparison). This happens often with-but is not limited to-comparisons between models with different numbers of topics.</p><p>• Absent topics -Finally, some topics from one model will simply have no correlated counterpart in the other model.</p><p>Unfortunately, identifying these phenomena only gets us so far. Without a ground truth to compare against, the user must be able to drill into these splits and merges to understand whether they are semantic or random, and what to do about them. To this end, we must not only support topic alignment, but topic comparison as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Topic alignment</head><p>The alignment between two sets of topics is represented by a matrix of the distances between each topic in one set and the other. These distances can be computed as the vector distances of either the words in the topic or the documents containing the topic, by any appropriate vector distance transform (typically cosine distance). We convert these distance matrices to similarity matrices to facilitate matrix operations like re-ordering and sparsification. Given the topic similarity matrix, it is easy to identify each topic's corresponding best match as the maximal element of its corresponding row or column. However, more information is typically desirable. For example, in examining a match, it is useful to know how strong the match is (the topic similarity), whether the match is "clean" (i.e. is the best match clearly better than the alternatives), and whether the match is bi-directional (i.e. that the best matching topic doesn't have a better match for itself).</p><p>Since topics are categorical, we can re-order them to best expose the matching structure. While a number of potential algorithms are possible (see <ref type="bibr" target="#b26">[27]</ref> for a survey), we find that a simple sparse matrix diagonalizer is sufficient as we are generally concerned with only the primary diagonal structure (the few best matches for each). Our approach first sparsifies the similarity matrix, reducing the number of non-zero elements such that each row and column has a minimum (typically 3 or 5) of non-zero elements. We then use an asymmetric variant of the Cuthill-McKee algorithm (greedy breadth first search) to optimally order the rows and columns. This provides a matrix where the strongest matches appear close to the diagonal.</p><p>We provide a heatmap view of this re-ordered similarity matrix (see <ref type="figure" target="#fig_0">Figure 2</ref>). We also provide a standard "parallel coordinates" node-link view, showing links between similar topics. We use colors on the links to encode the match type: green for a bidirectional best match, purple for a one-way best match (with tapered lines indicating directionality), and less salient grays to encode other links. Weak matches can be filtered using a threshold, or omitted completely. As it is difficult to encode link strength on the lines, we augment the parallel coordinates with a small pareto bar-chart next to each topic's node. The bar chart shows the strength of the top (usually 5) matches, and color is used to encode rank (with similar values counted as the same) so that cases where there are ties are clearly visible. The stack of colored pareto bar charts allows for quick scanning to identify whether each topic has a strong best match, and whether this match is clearly the best. We also allow users to look at subsets of the topics-for example, letting them consider just the topics that are shared between two documents of interest. This can be helpful for understanding how and why individual document relationships change between models. An example of this sort of comparison is described in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Topic comparison</head><p>Once the user has identified instances of split, merged, or pseudomatched topics, the question becomes what to do about it. Is the split semantic or arbitrary? To find out, the user needs to see the words. The problem is that there are potentially hundreds or thousands of words in the topics, so simply reading through the lists is not feasible. One  <ref type="figure">Fig. 3</ref>: Two topics from one model (Model A) built on the works of Shakespeare split into 1000-word chunks (topics 2 A and 17 A , colored with blue-to-yellow and red-to-yellow ramps in descending order by frequency) to a topic from a non-chunked model built on the same texts (topic 20 from Model B, unique words colored in gray). The overlap of words within topic 20 B seem to possibly indicate a merged topic. Zooming in to see the actual words in the window to the right shows a semantic difference between 2 A and 17 A : 2 A seeming to be related to family while 17 A seems to be about wealth.</p><p>approach would be to limit the user to seeing only the top-most words in each model, but the other words do have an effect (as we see in Section 5.4), so we don't want to throw them out completely.</p><p>We use a color-field rank comparison visualization to allow the user to see patterns across the entire topic lengths while also being able to drill into the individual words (see <ref type="figure">Figure 3)</ref>. The visualization takes an object topic from one model and potentially multiple reference topics from the other model. For each reference topic, we create a color transfer function encoding words with colors from a continuous ramp based on their rank within the topic. (These ramps are interpolated from discrete ColorBrewer ramps <ref type="bibr" target="#b6">[7]</ref>.) We then use these reference ramps to show where each reference topic's words appear in the object topic, where they overlap, and where the object topic is unique.</p><p>This makes it easy to spot instances of split topics, as shown in <ref type="figure">Figure</ref> 3. To evaluate these splits, we have a moveable window allowing users to zoom into the individual words. For example, in <ref type="figure">Figure 3</ref>, the red topic's words seem to be associated with wealth while the blue topic seems more about family. This looks like a semantic split, so the user may favor the model containing the split topics rather than their merged version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Understanding similarity</head><p>Using a topic model to calculate similarities between documents provides insight into specific relationships within a corpus. Similarity can be both a local concept (which documents are most similar to a given reference) and a global one (how do documents cluster together). By treating document distributions as vectors, we can compute distance matrices to look at such similarities. Though cosine distance is the most frequently used, other distance metrics include KL-divergence <ref type="bibr" target="#b2">[3]</ref> and rescaled dot product <ref type="bibr" target="#b10">[11]</ref>.</p><p>In this section, we look at a novel method for viewing document similarities, as well as a method for comparing how those similarities can be used to cluster documents in different models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hamlet</head><p>Close in Model A Far in Model A (a) In this example, distance within a 25-topic model built on the works of Shakespeare is encoded as horizontal position (closer documents to the left, further to the right), while distance within a 50-topic model built on the same documents is encoded as color along a ramp going from blue (close) to red (far).   <ref type="figure" target="#fig_2">(Figure 4a</ref>). They can also be used in parallel <ref type="figure" target="#fig_1">(Figure 4b</ref>), using two axes to show precise movement between two models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comedy of Errors</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Buddy plots</head><p>A common way of looking at document distance is to place documents into a two-dimensional embedding <ref type="bibr" target="#b13">[14]</ref>. When looking at change in similarities between models, we can consider how documents move within the embedding. As distances in topic models are a product of dimensionality reduction, they have no semantic meaning. Therefore, apparently large changes in position from one embedding to the next may not correspond to any meaningful difference, and it is difficult to know which pairwise relationships are authentic as opposed to artifacts of the dimensionality reduction. One way of seeing corpus movement that sidesteps these challenges is to shift our vantage point from an overhead, all-inclusive one to that of a single document. To see how similarities have changed for this document, we can effectively hold it stationary across two models and observe how the rest of the corpus shifts around it. This shift in vantage point can give us local insight around the stationary document. Doing so for multiple documents (and potentially juxtaposing their respective vantage points together) can give us a more global sense of the changing similarities across the two models. We have created visualizations called buddy plots that give a view of the corpus from this shifted perspective.</p><p>Buddy plots are customizable and can map a variety of data to different encodings. They show topic similarity from the perspective of a single reference document. The rest of the documents (the reference's "buddies") are arranged linearly along a horizontal axis based on their distance from the reference document. When comparing models, we can combine the sets of distances in one of two ways:</p><p>Single buddy plots have only one axis, and by default encode one set of distances as horizontal position and the other as color along a diverging ramp <ref type="figure" target="#fig_2">(Figure 4a</ref>).</p><p>Parallel buddy plots have two axes, typically encoding each set of distances on its own axis and connecting them with parallelcoordinate-style edges <ref type="figure" target="#fig_1">(Figure 4b</ref>).</p><p>These two sets of encodings create a trade-off between scalability and accuracy. Single buddy plots fit more data into a tight space, and allow us to leverage perceptual gradient detection to make judgments about document movement. If the two sets of distances agree, the documents should form a smooth gradient from left to right. Departures from this gradient become pre-attentively identifiable <ref type="bibr" target="#b0">[1]</ref>, drawing the user's attention to outliers that can be inspected further or explicitly zoomed in on (see <ref type="figure">Figure 7)</ref>. Parallel buddy plots, on the other hand, give more detailed information about precisely which documents are moving where across the two models by giving each model its own axis and drawing explicit connections. By default, color remains consistent across both axes (though this is redundant with position) to facilitate easy tracking of movement and direct comparison. Globally, combining buddy plots gives a sense of how similarity changes across the corpus between the two models. With single buddy plots, deviations from the gradient can give a sense of the cross-model consistency <ref type="figure">(Figure 1</ref>). Patterns of parallel buddy plots can show directional or density trends from one model to the next <ref type="figure" target="#fig_2">(Figure 14)</ref>. Though single buddy plots are more space efficient, it is still difficult to fit an entire corpus onto the screen. As has been seen elsewhere <ref type="bibr" target="#b1">[2]</ref>, reordering the documents by task is a simple but effective way of bringing the most important items into view. We offer a variety of metrics by which to reorder documents based on what the user is most interested in, such as the average change in distance or rank for documents (i.e. how much a document moves with respect to the rest of the corpus).</p><p>While being able to see movement within the entire corpus can be informative, many tasks are only concerned with exploring the most similar documents. As such, we want to give users a sense of how these few documents-the reference document's local neighborhood-change from one model to the next. Neighborhoods can be thought as the closest documents either by distance (e.g. documents within .1 of the reference document) or by rank (e.g. the top 10 closest documents to the reference document). To measure change in neighborhoods, we devised a metric called pareto radii. Pareto radii indicate how much a document's neighborhood has spread out from one model to the next (for multiple definitions of neighborhood). Multiple radii can indicate increasing proportions of the neighborhood. Specifically, for distance neighborhoods, when looking for change from model A to B within the neighborhood of some reference document k, the pareto radius r dist <ref type="figure">(d, p)</ref> is the smallest radius around document k in model B that contains the proportion p of all documents within distance d of document k in model A. Pareto radii can also be defined for rank neighborhoods: when looking for change from model A to B within the neighborhood of some reference document k, the pareto radius r rank (n, p) is the smallest radius around document k in model B that AP880216-0094.txt</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>20% 40%</head><p>60% 80% 100% <ref type="figure">Fig. 5</ref>: Pareto radii are used to indicate how much a document's "neighborhood" spreads out from one model to another. Here, the gray gradient shows pareto radii in a trimmed model built on AP news documents that are needed to cover 20, 40, 60, 80, and 100 percent of the closest 20 documents to the reference document in the untrimmed version of the model.</p><p>contains the proportion p of the top n documents of document k in model A. These radii can be encoded using a gradient for increasing values of p as seen in <ref type="figure">Figure 5</ref>. Length of pareto radii can also be an effective encoding for sorting buddy plots, bringing those with the most or least movement within their local neighborhoods to the top for user inspection. Through flexibility in the encodings, buddy plots can be tuned to answer a variety of different questions (see <ref type="figure" target="#fig_3">Figure 6</ref>). In addition to distance, document color can be used to encode rank, change in distance or rank across the two models, or metadata about the documents (like genre or domain). Edge color in parallel buddy plots can be changed to highlight documents that move far, stay consistent, or move in a particular direction relative to the reference document. Also, the draw order of both documents and edges, like zooming, can help combat overdraw for corpora with many or tightly clustered documents. The default draw order keeps documents that were closer in one model on top, making it easier to pick out movement within a document's relative neighborhood (see <ref type="figure">Figure 7)</ref>. Other potential draw orders include rank within either model, as well as change in rank or distance. : Buddy plots allow researchers to explore comparisons by combining a variety of encodings and data. Though most data and encodings can be combined, we have a set of defaults that seem well suited to single and parallel buddy plots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Top circle color</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Comparison of clustering from models</head><p>The distance metrics provided by topic models are often used to cluster groups of related documents. Users concerned with such grouping tasks may want to compare models based on their performance at clustering documents, considering that different models may induce different groups. It is possible to use each model to cluster the documents, and then to compare the resulting clusterings using any number of clustering comparison tools <ref type="bibr" target="#b17">[18]</ref>. Such clustering comparison is no different from comparing two different clusterings created from the same model. This emphasizes a key point: since there are many pos- <ref type="figure">Fig. 7</ref>: Zooming in on buddy plots can help pick out individual documents. Also, by changing draw order, we can make sure interesting outliers are easily found. In this example, draw order is controlled by the amount which a document has moved from one model to the other. Though there are a large number of red documents overdrawing one another, we are easily able to pick out the blue documents, which were close to the row's reference but moved much further away. Clicking on one brings up its metadata, allowing users to compare its specific topics to those of the reference, as in Section 5.3. sible clusterings that may result form the same model, a difference between clusterings created by two different models might just be a matter of adjusting some of the clustering parameters. For understanding how differences in models affect clustering, we introduce a method that avoids considering a single clustering, but instead tries to help assess the intrinsic clustering ability of a model. Rather than performing clustering once, we repeat the process many times. This allows for the non-deterministic algorithm finding different solutions, as well for many different settings of clustering parameters. We then provide an assessment of how consistent this set of clusterings is, and allow the user to compare these assessments. We visualize this set with a matrix that counts for each possible pair of documents how often the documents appear in the same cluster. The intuition is that if a model is good for clustering, then documents should consistently either be in the same cluster or not, even as the parameters are changed. We display these matrices as heatmaps, ordering the rows using the Cuthill-McKee algorithm to best expose structure. An example comparing two models is shown in <ref type="figure" target="#fig_4">Figure 8</ref>. In this simple example, multiple runs of K-means clustering is run on each model. Because of the non-determinism of the starting points, the results are different on each trial. However, with the 10-topic model, the clusters are more consistent, suggesting that the boundaries in this model are clearer and less likely to simply be artifacts of the clustering process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Understanding change</head><p>Seeing how content changes over time is a very prevalent use of topic models. To this end, a common visualization technique is to create "river flow" style diagrams to display these changes <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b14">15]</ref>. Though there are many variations, such diagrams typically have individual colored bands for each topic. The width of these bands encode  the portion of the corpus represented by a particular topic at a point in time (with time represented as position along the horizontal axis). For a user interested in exploring such temporal patterns, it may be important to see whether different events or timepoints are highlighted differently by two models. To this end, we have created asymmetrical topic flow diagrams to compare trends over time across models (see <ref type="figure">Figure 9</ref>). Topic densities from the two models are plotted along a horizontal time axis, with one model's topics appearing above the axis and the other model's topics appearing below. This allows the user to look for symmetry (or asymmetry) in the trends and events being highlighted by the two models.</p><p>Especially for models built on the same documents, there may not be large changes to the global shape of the topics. However, the behavior of individual topics can exhibit interesting differences to lead a user to prefer one model over another. To explore such differences, we allow users to directly compare aligned topics across the two models. Brushing over an individual topic in one model will highlight its most aligned topics in the other model (using the same topic alignment method as described in Section 4.1.1). Clicking on the topic will filter away all other topics but the selected one and its closest matches.</p><p>This can provide an informative comparison between split/merged topics. Such topics may be split uniformly across time, or else might alternate, highlighting different events (see <ref type="figure">Figure 10</ref>). This distinction can help the user decide whether the split is a semantic one or a random artifact of the model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">USAGE SCENARIOS</head><p>In this section, we describe a number of usage scenarios that illustrate the capabilities of our comparison techniques on real data. Some of these comparisons fall into the traditional motivations for model evaluation (parameter optimization, etc.), while others would not be possible with a statistical metric.</p><p>These comparisons use models built on three separate corpora: the 36 plays of William Shakespeare, a collection of 1127 abstracts from select IEEE sponsored visualization conferences from 2007-2013 (including SciVis, InfoVis, VAST, BioVis, and PacificVis), and a collection of 2250 articles from the Associated Press <ref type="bibr" target="#b32">[33]</ref>. All models were built with MALLET <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Same parameters (Visualization Abstracts)</head><p>Topic models are inherently probabilistic, and there has been concern that some (like LDA) can be inconsistent across different runs with the same parameters <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24]</ref>. A researcher wanting to validate their findings in one model may want to confirm that there are no significant changes due to chance or hidden parameters, and that any existing changes do not affect the patterns in which they are most interested. We decided to investigate this phenomenon. To do so, we built two 25-topic models on our collection of visualization abstracts, using the default parameters for α and β . We found some inconsistencies, which might disproportionately affect different use cases.</p><p>Specifically, we saw change in both the topic alignment and document similarities. Though the differences in topics may fall within a reasonable allowance (see <ref type="figure" target="#fig_9">Figure 13a)</ref>, changes in document similarity tended to be much more dramatic (see <ref type="figure">Figure 11)</ref>. Though using MALLET's built in hyperparameter optimization improved the topic alignment (see <ref type="figure" target="#fig_1">Figure 13b</ref>), document neighborhoods remained inconsistent. This suggests that while LDA may work well for topic-based tasks like corpus summarization, it may not be as well suited for tasks that require consistent evaluation of similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Different numbers of topics (Shakespeare)</head><p>As mentioned in Section 2, the most important parameter for building a topic model is the number of topics. Too low of a number will result in overly broad topics or merges between unrelated topics, while too high of a number can create non-semantic splits within topics and hide larger trends. To observe these differences, we built models of 10 and 50 topics on the works of William Shakespeare.</p><p>The primary difference we saw between the two models was how they were clustering. <ref type="figure" target="#fig_4">Figure 8a</ref> shows a heatmap of the consistency of clustering across 20 runs of k-means upon the 10-topic model. We can see that with the minor exception of the cluster in the direct center (and in particular, Romeo and Juliet), the groups being formed are BioVis.2011.6094045.txt <ref type="figure">Fig. 11</ref>: This parallel buddy plot compares distances between two 25-topic models built using the same (default) parameters on a corpus of 1127 visualization abstracts. Color encodes distance in the first model for glyphs along both axes for consistency, and edge lines are ordered by greatest magnitude of change in distance. We can see that there has been a dramatic shift in similar documents across the two models.</p><p>AP880217-0012.txt  (a) This heatmap shows the alignment from two models generated using MAL-LET's default parameters. Though there is a strong diagonal, other strong matches may indicate split topics. (b) This heatmap shows two models that were also generated using the same parameters, but with MALLET's hyperparameter optimization. This seems to improve the topic alignment.  <ref type="figure" target="#fig_1">Figure 8b</ref>, on the other hand, exhibits much less consistency. Many documents appear with almost all of the others in a cluster in at least one of the runs. For a user hoping to explore the differences between these groups, the 10-topic model may give them cleaner distinctions to investigate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Document chunking (Shakespeare)</head><p>As described in Section 2, longer documents can muddy the concept of document co-occurrence. Should words still be considered to be co-occurring if they occur in different paragraphs, or pages, or chapters? Intuition would say that longer chunks will create more general models, but it can be hard to predict the effects a priori.</p><p>We built two models of 25 topics on the works of Shakespeare: one in which the documents were divided into 1000-word chunks (as recommended in <ref type="bibr" target="#b22">[23]</ref>) and stitched back together after being tagged by the model, and one in which the documents were treated as whole.</p><p>A quick glance at the buddy plots showed that there did not seem to be dramatic changes in document neighborhoods, as the blue-to-red gradients were relatively maintained. Expanding the rows out into parallel buddy plots exposed a consistent trend: though the order of document distances does not change much, documents seemed to grow much further apart in the non-chunked model (as seen in <ref type="figure" target="#fig_2">Figure 14)</ref>. This makes sense as the process of stitching small chunks back to-gether would tend to create document vectors that cover a wider range of topics. The fact that relative orders do not change much may suggest that we can trust relationships we find in the un-chunked (and correspondingly much faster to build) model.</p><p>There are a few salient red edges, however, showing that some documents do move dramatically against the grain. For one such example, in <ref type="figure" target="#fig_1">Figure 4b</ref>, Taming of the Shrew grows much closer to Comedy of Errors in the unchunked model, while everything else moves further away. Looking at the bipartite topic alignment of topics specifically contained within these two documents, we noticed that there is one topic that they share in the unchunked model that is best aligned with two topics in the chunked model that they distinctly do not share.</p><p>Drilling into the words shows that this is a semantic split (see <ref type="figure">Figure  3)</ref>, with the red topic dealing with wealth and the blue topic dealing more with family. Depending on the user's interest in these specific topics and documents, incorporating this split may be worth the added time and effort of the chunking process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Trimmed topic models (AP articles)</head><p>One of the problems with using topic models for text analysis is that they can be difficult to understand for a lay audience. Among the challenging concepts for users without a background in statistics or machine learning are the existence of overlapping topics (i.e. words in multiple topics), the distribution of low-frequency words, and the probabilistic nature of topic tags. In attempts to make topic models easier to understand, we have considered a variety of ways of simplifying the distributions created by algorithms like LDA.</p><p>One such method is to simply trim away words from the bottom of the topic distributions. This certainly makes the topics simpler, and reduces them to something more closely matching their visual representations to users, which tend to be lists or word clouds of the top most frequent words. We were interested in looking at how much such an operation would affect the relationships between documents within a corpus. If it had no effect, then perhaps trimming in this fashion would be a useful way of greatly simplifying topic representations.</p><p>We built a model of 50 topics on a corpus of articles from the Associated Press <ref type="bibr" target="#b32">[33]</ref>, trimmed the bottom proportions at a number of different increments, and looked for the changes in document relationships using buddy plots. There was rather dramatic movement from the untrimmed model to the trimmed versions. <ref type="figure">Figure 1</ref> shows a subset of single buddy plots comparing the untrimmed model to a model with topics trimmed down to the 50 most frequent words. Though we still see a semblance of a blue-to-red gradient, the darkest blues (the original document neighborhoods) are spread out dramatically. Some documents have orange and red documents bleeding far to the left. By drilling into a parallel view of one of these documents <ref type="figure" target="#fig_0">(Figure 12</ref>), we can see that the documents become intensely jumbled. to a model built on the documents treated as whole (the bottom lines). Red edges indicate documents that move closer in the un-chunked model, while blue edges indicate documents that move further away. From the color and slant of these edges, we see a trend of documents moving away from each document, with a few salient red exceptions.</p><p>That trimming away such dramatic portions of the topics would have such an effect is perhaps unsurprising. After all, the ability of topic models to semantically relate documents relies on its ability to find subtle patterns of co-occurrence that would likely escape a human observer. However, this emphasizes the importance of the full topic distributions, especially for documents that are low in high-frequency concepts and words. It can be easy to forget that a topic is more than just its top 10 words. This exploration makes us wary of representations that hide too much of the full size and complexity of topics. We hope to explore differences created by other methods of simplifying or sparsifying topic models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we have argued for the importance of explicit, taskdriven topic model comparison. We have offered visual techniques for making comparisons associated with the tasks of using topic models to understand topics, similarity, and change within text corpora. Among these techniques, we have introduced buddy plots, a novel visualization for viewing changes across two sets of distances by restricting our vantage point to that of a single reference document. Finally, we have described a number of use cases that exhibit the variety of comparisons that can be made with these methods.</p><p>We plan to combine these approaches into a fully deployed, interactive tool. There are a number of technical improvements that can be made to our algorithms (e.g. considering fuzzy matches in topic alignment). However, we also see a number of future directions for work in task-driven model comparison more generally. Though comparison between two models is important for understanding differences, such a restricted focus can also be a limitation for exploring the large decision space. We believe that there can be a benefit to combining pairwise comparison with higher-level overviews that incorporate more models. Tied to this, we would like to introduce better aggregation to combat the issue of scale, as our current implementations can break down with corpora in the thousands.</p><p>Though we believe that our three categories of understanding topics, similarity, and change cover the vast majority of topic model uses, there are a handful of other tasks we would like to support. For instance, there is a growing number of users who use topic models not to save them reading, but to direct their reading, augmenting individual words and passages with topic data <ref type="bibr" target="#b1">[2]</ref>. We would like to be able to enable comparison at this level, informing users of what different models show about the passages and flow of individual documents.</p><p>We also want to provide tools to allow users to connect differences observed between models to actionable insights upon how to further improve and tune their models. Some work has been done on actively tuning models by merging or breaking up topics <ref type="bibr" target="#b9">[10]</ref>, pulling documents apart or closer together <ref type="bibr" target="#b16">[17]</ref>, or directly intervening in the ongoing iterations of the algorithm <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b3">4]</ref>. It may be helpful to allow users to tune not just single models, but perhaps to pull the best parts of a variety of models together. We believe that the sorts of comparisons outlined in this paper would be useful in identifying which aspects to separate or combine.</p><p>Finally, we are interested in seeing these techniques applied to a wider variety of corpora and domains. We have only scratched the surface of potential comparisons to be made, and hope to use these methods to learn more about things like stopword usage, different modeling types, and models built on non-intersecting sets of documents.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Topic alignment between two models built on the works of William Shakespeare, one with 10 topics and one with 15 topics. On top, a heatmap of topic alignment indicates which topics from the two models are closely matched (dark orange indicating a close match, yellow indicating no match). Below, a bipartite visualization indicates matches of different strengths (green as a two-directional match, purple as a one-directional match, and gray as a weak match) and the bar charts next to each topic show the strength of the top five matches (with bar height encoding strength and color used to show rank so that ties are salient). Topics exhibiting multiple close matches (e.g. Topic 4 in the 10-topic model) may be instances of merged concepts to explore more closely.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( b )</head><label>b</label><figDesc>In this parallel buddy plot, distance in the 50-topic model is encoded along the top axis while distance in the 25-topic model is encoded along the bottom axis. (Color is consistently encoded on both top and bottom as distance in the 50-topic model to facilitate comparison.) Most documents are further from Comedy of Errors in the second model, with one easily identifiable exception (Taming of the Shrew).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Buddy plots encode the distances of corpus documents away from an individual reference document (labeled to the left). By using both position and color, buddy plots can combine multiple sets of distances within a single line</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6</head><label>6</label><figDesc>Fig. 6: Buddy plots allow researchers to explore comparisons by combining a variety of encodings and data. Though most data and encodings can be combined, we have a set of defaults that seem well suited to single and parallel buddy plots.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 :</head><label>8</label><figDesc>These heatmaps show the consistency of clusterings resulting from 20 runs of k-means on two models built from the works of William Shakespeare (k = 5). Darker brown squares indicate pairs of documents that consistently appear within the same cluster. Lighter yellow squares indicate pairs of documents that never appear within the same cluster. Here, the 10topic model seems to have more consistent clustering behavior.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 :Fig. 10 :</head><label>910</label><figDesc>Asymmetrical topic flow diagrams show how topics change over time within two models. The horizontal axis encodes time; width of bands above the axis indicate topic proportions from one model while those below the axis indicate proportions from the other model. Hovering over an individual topic highlights it in yellow and highlights any aligned topics from the other model as in Section 4.1.1 (green for a two-way match, purple for a one-way match).(a) Some splits happen uniformly across time (b) Some splits alternate with events Asymmetrical topic flows allow the user to select individual topics, filtering out others so as to compare them to their best aligned matches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 12 :</head><label>12</label><figDesc>This parallel buddy plot compares distances from a 50-topic model built on a corpus of Associated Press documents to distances from that same model when all but the top 50 most frequent words are stripped from each topic. (Color encodes distance in the first model and edges are drawn in increasing order of change in distance.) We see that the act of stripping words from the topics creates a very dramatic change in the document's relationships with the rest of the corpus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 13 :</head><label>13</label><figDesc>These topic alignment heatmaps show comparisons of two pairs of topic models (25 topics on a corpus of visualization abstracts) generated using the exact same parameters.quite consistent. The 50-topic model as shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 14 :</head><label>14</label><figDesc>Here, expanded parallel plots compare a model built on Shakespeare's works split into 1000-word chunks (the top lines)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>•</head><label></label><figDesc>Eric Alexander is with the University of Wisconsin-Madison. E-mail: ealexand@cs.wisc.edu. • Michael Gleicher is with the University of Wisconsin-Madison. E-mail: gleicher@cs.wisc.edu.</figDesc><table /><note>Manuscript received 31 Mar. 2015; accepted 1 Aug. 2015; date of publication xx Aug. 2015; date of current version 25 Oct. 2015. For information on obtaining reprints of this article, please send e-mail to: tvcg@computer.org.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Manuscript received 31 Mar. 2015; accepted 1 Aug. 2015; date of publication 20 Aug. 2015; date of current version 25 Oct. 2015. For information on obtaining reprints of this article, please send e-mail to: tvcg@computer.org. Digital Object Identifier no. 10.1109/TVCG.2015.2467618</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Mapping Single Model Tasks to Comparison Tasks</figDesc><table><row><cell>Single model task</cell><cell>Model comparison task</cell></row><row><cell>Understanding topics</cell><cell>Topic alignment</cell></row><row><cell>Understanding similarity</cell><cell>Distance comparison</cell></row><row><cell>Understanding change</cell><cell>Timeline comparison</cell></row><row><cell>3.2 The tasks</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported in part by NSF award IIS-1162037 and a grant from the Andrew W. Mellon Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sequence surveyor: Leveraging overview for scalable genomic alignment visualization. Visualization and Computer Graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Albers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dewey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2392" to="2401" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Serendip: Topic model-driven visual exploration of text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kohlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Valenza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Witmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual Analytics Science and Technology (VAST), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Topic significance ranking of lda generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Alsumait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Barbará</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gentle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Domeniconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="67" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Modeltracker: Redesigning performance analysis tools for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chickering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Suh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems</title>
		<meeting>the 33rd Annual ACM Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="337" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A framework for incorporating general domain knowledge into latent dirichlet allocation using first-order logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Andrzejewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI Proceedings-International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">1171</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Colorbrewer in print: A catalog of color schemes for maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Brewer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Hatchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Harrower</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cartography and Geographic Information Science</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Visualizing topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI on Weblogs and Social Media</title>
		<meeting>AAAI on Weblogs and Social Media</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reading tea leaves: How humans interpret topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="288" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Utopian: User-driven topic modeling based on interactive nonnegative matrix factorization. Visualization and Computer Graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1992" to="2001" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Topic model diagnostics: Assessing domain relevance via topical alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML-13)</title>
		<meeting>the 30th International Conference on Machine Learning (ICML-13)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="612" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Termite: visualization techniques for assessing textual topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advanced Visual Interfaces</title>
		<meeting>Advanced Visual Interfaces</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="74" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Interpretation and trust: Designing model-driven visualizations for text analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Human Factors in Computing Systems</title>
		<meeting>ACM Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="443" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Topicview: Visually comparing topic models of text collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Crossno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Shead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Dunlavy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tools with Artificial Intelligence (ICTAI), 2011 23rd IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="936" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Textflow: Towards better understanding of evolving topics in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2412" to="2421" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAsIs</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semantic interaction for sensemaking: inferring analytical reasoning for model steering. Visualization and Computer Graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Endert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fiaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>North</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2879" to="2888" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Coral: an integrated suite of visualizations for comparing clusterings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Filippova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gadani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kingsford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">276</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hierarchical topic models and the nested chinese restaurant process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ThemeRiver: Visualizing theme changes over time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Havre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hetzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nowell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Information Visualization</title>
		<meeting>IEEE Information Visualization</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The hundredth psalm to the tune of &quot;green sleeves&quot;: Digital approaches to shakespeare&apos;s language of genre. Shakespeare Quarterly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Witmore</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="357" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Et-lda: Joint topic modeling for aligning events and their twitter feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kambhampati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="59" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Macroanalysis: Digital methods and literary history</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Jockers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>University of Illinois Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">High-reproducibility and high-accuracy method for automated topic classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lancichinetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Sirer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Körding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A N</forename><surname>Amaral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review X</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">11007</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Mallet: A machine learning for language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Mccallum</surname></persName>
		</author>
		<ptr target="http://mallet.cs.umass.edu" />
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Optimizing semantic coherence in topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Talley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leenders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="262" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A comparison of vertex ordering algorithms for large graph visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lumsdaine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Asia-Pacific Symposium on Visualization</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007-02" />
			<biblScope unit="page" from="141" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Opening the black box: Strategies for increased user involvement in existing algorithm implementations. Visualization and Computer Graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Muhlbacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Piringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gratzl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sedlmair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Streit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1643" to="1652" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Evaluating topic models for digital libraries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Talley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th annual joint conference on Digital libraries</title>
		<meeting>the 10th annual joint conference on Digital libraries</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="215" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exploring topic coherence over many models and many topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kegelmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Andrzejewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Buttler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="952" to="961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Rethinking lda: Why priors matter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Minmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Evaluation methods for topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mimno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1105" to="1112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.3298</idno>
		<title level="m">Continuous time dynamic topic models</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Tiara: a visual exploratory text analytic system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Knowledge discovery and data mining</title>
		<meeting>ACM Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="153" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visual analysis of topic competition on social media. Visualization and Computer Graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Q</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2012" to="2021" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Document clustering based on non-negative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval</title>
		<meeting>the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="267" to="273" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
