<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Supporting Activity Recognition by Visual Analytics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Röhlig</surname></persName>
							<email>martin.roehlig@uni-rostock.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Luboschik</surname></persName>
							<email>martin.luboschik@uni-rostock.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Krüger</surname></persName>
							<email>frank.krueger2@uni-rostock.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kirste</surname></persName>
							<email>thomas.kirste@uni-rostock.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heidrun</forename><surname>Schumann</surname></persName>
							<email>heidrun.schumann@uni-rostock.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Rostock</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Bögl</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Alsallakh</surname></persName>
							<email>alsallakh@cvast.tuwien.ac.at</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Miksch</surname></persName>
							<email>miksch@cvast.tuwien.ac.at</email>
							<affiliation key="aff1">
								<orgName type="institution">Vienna University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">IEEE Conference on Visual Analytics Science and Technology</orgName>
								<address>
									<addrLine>2015 October 25-30</addrLine>
									<settlement>Chicago</settlement>
									<region>Il</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Supporting Activity Recognition by Visual Analytics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I</term>
					<term>3</term>
					<term>6 [Computing Methodologies]: Computer Graphics-Methodology and Techniques; H</term>
					<term>5</term>
					<term>2 [Information Interfaces and Presentation]: User Interfaces-Theory and methods</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Recognizing activities has become increasingly relevant in many application domains, such as security or ambient assisted living. To handle different scenarios, the underlying automated algorithms are configured using multiple input parameters. However, the influence and interplay of these parameters is often not clear, making exhaustive evaluations necessary. On this account, we propose a visual analytics approach to supporting users in understanding the complex relationships among parameters, recognized activities, and associated accuracies. First, representative parameter settings are determined. Then, the respective output is computed and statistically analyzed to assess parameters&apos; influence in general. Finally, visualizing the parameter settings along with the activities provides overview and allows to investigate the computed results in detail. Coordinated interaction helps to explore dependencies, compare different settings, and examine individual activities. By integrating automated, visual, and interactive means users can select parameter values that meet desired quality criteria. We demonstrate the application of our solution in a use case with realistic complexity, involving a study of human protagonists in daily living with respect to hundreds of parameter settings.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Decomposing multivariate time series into sequences of segments and assigning labels to these segments is a common task. A prominent example is the recognition and analysis of activities of human protagonists for ambient assisted living. The goal is to reconstruct activities from sensor-generated time series to allow researchers to study behaviors in a conceptual way.</p><p>Typically, automated algorithms are applied to compute and label the segments. Labels correspond to specific activities and segments denote associated occurrences and time periods. Although these algorithms are reasonably fast, they are limited in accuracy, particularly when exceptional patterns occur. Moreover, the algorithmic output is controlled by parameters, whose influence on the segmentation is hard to predict. For judging the determined activities reasonably, it is important to understand the interplay between input parameters, labeled segments, and recognition accuracies.</p><p>The traditional evaluation of activity recognition algorithms is commonly performed by investigating the results with statistical analysis methods, e.g., analysis of variance (ANOVA), and statistical plots, e.g., confusion matrices or interaction plots. This is successful for rather simple recognition strategies, but it barely covers all aspects of more complex approaches that require sophisticated parameter tuning. In our use case (Sect. 3), we apply such a complex approach to activity recognition based on Computational State Space Models (CSSMs) <ref type="bibr" target="#b11">[12]</ref>. CSSMs require extensive configuration of parameters, which is time-consuming and error-prone.</p><p>Visual analytics provides first approaches that already support users in visually inspecting the labeled segments, identifying specific patterns, and adjusting these results interactively <ref type="bibr" target="#b0">[1]</ref>. Open issues are how to support users in (i) exploring hundreds of different parameter settings, (ii) comparing their influence, and (iii) increasing the accuracy of the computed results.</p><p>With this paper, which builds upon ideas first presented as a poster at IEEE VIS'14 <ref type="bibr" target="#b17">[18]</ref>, we introduce a novel visual analytics approach to address these issues. Our contributions are:</p><p>Novel Visual-Interactive Design: We present an enhanced visual approach (Sect. 5) that provides a comprehensive overview of parameters, labeled segments, and accuracies in an interlinked fashion. Coordinated interaction enables users to explore complex relationships, to compare different configurations, and to inspect the various aspects of the data in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of Statistical and Visual Analysis: Statistical and</head><p>visual methods (Sect. 4 and 5) support the data analysis in different ways. Therefore, we compare our visual analytics approach with purely statistical analysis. We identify benefits and limitations (Sect. 6), and discuss how open issues can be tackled in future work (Sect. 7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Our work is related to visualizing segmented and labeled time series, and to visual analysis of parameter dependencies in general. Visualizing Segmented and Labeled Time Series Segmentation results are commonly visualized as sequences of colored stripes. Color encodes the stripe label, and size represents the corresponding time period. This representation has been used to show segmentation results along with the time series data to inspect industrial processes <ref type="bibr" target="#b0">[1]</ref>. Multiple segmentation results can be visualized to support comparison <ref type="bibr" target="#b20">[21]</ref>. Similarly, a sequence index plot can be used to analyze multiple state sequences <ref type="bibr" target="#b7">[8]</ref>, which is common in sequential data analysis <ref type="bibr" target="#b4">[5]</ref>. However, such plots become difficult to interpret for larger numbers of sequences. Generally, labeled time series can be interpreted as categorical data over time, for which various visualization approaches exist <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>Temporal categorical data can be analyzed using similarity measures <ref type="bibr" target="#b22">[23]</ref>. Such similarity metrics can also be useful for sorting and comparing parameter settings, labeled segments, and accuracies over time. Simple sorting methods are based on lexicographic, averaged Euclidean, and cluster-based techniques. A prominent class of cluster-based techniques are self organizing maps (SOM). SOMs can also be used to deal with categorical data <ref type="bibr" target="#b5">[6]</ref>.</p><p>Visual Analysis of Parameter Dependencies Segmentation and labeling algorithms are generally governed by a set of parameters. Visual analysis is a common means to study the parameters' influence <ref type="bibr" target="#b18">[19]</ref>. So-called global-to-local approaches are directly related to our work. They provide overviews of several pre-computed parameterizations and associated outcomes and allow for a detailed inspection on demand. However, only a few existing techniques address parameter-dependent time series data. An example is the <ref type="figure">Figure 1</ref>: Illustration of the activity recognition process. The activities of a human protagonist are measured via sensors, then reconstructed by an automated algorithm for each applied parameter setting, and finally represented via different labeled segments over time.</p><p>approach for comparing parameter-dependent time series within larger parameter spaces <ref type="bibr" target="#b12">[13]</ref>. However, the handling of categorical data has not been addressed so far.</p><p>Global-to-local techniques require the specification and computation of a large set of different parameterizations. Commonly, surrogate models are used to reduce calculation efforts <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20]</ref>. However, such models require that the data can be interpolated, which is not the case for our categorical time series. Other strategies for reducing parameterizations are for instance undirected optimizations <ref type="bibr" target="#b14">[15]</ref> or parameter space partitioning <ref type="bibr" target="#b3">[4]</ref>, but these techniques do not provide a thorough overview of the entire parameter space.</p><p>Parameter-dependent data classification is also relevant in image segmentation. There it is used to link parameters with the 2D segmentation result <ref type="bibr" target="#b15">[16]</ref> or with a ground-truth-based fitness function <ref type="bibr" target="#b19">[20]</ref>. The latter approach also incorporates accuracy measures for judging the classification. Similarly, in <ref type="bibr" target="#b2">[3]</ref> a ground truth is used to assess parameter-dependent multivariate predictions.</p><p>In conclusion, several individual techniques exist for visualizing either segmented and labeled time series or parameter dependencies. Yet, none of these techniques is directly suited for a combined analysis of parameters, labeled segments, and accuracies from activity recognition algorithms beyond statistical analysis methods. Therefore, we aim at developing an integrated approach that supports users in visually analyzing recognized activities over time, and that facilitates investigations of parameters' influence on the performance of the activity recognition. To achieve this goal, we (i) visualize multiple parameter settings along with labeled segments and associated accuracies, and (ii) provide coordinated interaction techniques to compare different aspects of the data and inspect details. Before we introduce our novel approach (Sect. 5), we first explain the examined activity recognition algorithm (Sect. 3) and the statistical analysis of its performance (Sect. 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ACTIVITY RECOGNITION AND USE CASE</head><p>Knowledge about current and possible future activities as well as eventually the final goals of individuals is the foundation for application domains such as security systems and assistive technologies. To this end, automated activity recognition algorithms are applied to assign activity labels to each time step of sensor observations. The basic idea is to abstract from the raw sensor data and to focus on detecting certain activities and reasoning about specific behaviors instead. However, to obtain adequate activity recognition results, the automated algorithms need to be configured and evaluated for different conditions and sensor systems. For judging the algorithms' performance in a given scenario, multiple algorithm runs with different parameter settings are necessary. Each algorithm run generates large data sets containing parameter-dependent information about the observed activities as segmented and labeled time series. <ref type="figure">Figure 1</ref> illustrates this general activity recognition process.</p><p>As a concrete example, we consider an activity recognition algo- rithm based on Computational State Space Models (CSSMs) <ref type="bibr" target="#b11">[12]</ref>. CSSMs are powerful tools for automatically segmenting and labeling time series data based on probabilistic graphical models. They can recognize activities even from noisy or ambiguous sensor measurements <ref type="bibr" target="#b11">[12]</ref>. Although we specifically address activity recognition based on CSSMs, our approach is generic enough to be applicable to other algorithms and problems as well. An example showing the reconstruction of drilling processes is given in Sect. 6. Studying and evaluating the performance of such sophisticated activity recognition algorithms is a difficult task. On the one hand, the algorithms' results are difficult to analyze and compare as they typically consist of long segmented and labeled time series with possibly multiple activities and thousands of time steps. On the other hand, the algorithms' results depend on the parameter configuration, where the impacts of individual parameters or their combinations are largely unknown. Our objective is to support machine learning experts in understanding the influence of parameters. The goal is to help them evaluate the algorithms' mode of operation and confirm or reject hypotheses about algorithm performance in realistic scenarios. Particularly, we aim at supporting experts in analyzing the parameter influence on the recognition performance (i) over time, (ii) of different protagonists, (iii) of all activities in general, and (iv) of individual activities.</p><p>The data for the evaluation of the examined CSSM was generated in an experiment with seven participants. Their activities included cooking a meal, setting the table, eating the meal, and finally cleaning the dishes (see <ref type="figure" target="#fig_0">Fig. 2</ref>). Each subject was instrumented with five inertial measurements units. Video logs were created and manually annotated to document the course of each experiment session and to obtain ground truth sequences for each subject.</p><p>A CSSM was created to recognize the subjects' activities by modeling the environment and possible actions <ref type="bibr" target="#b11">[12]</ref>. As a result, we obtained 99 grounded actions (e.g., take-knife-drawer) from 16 activities (e.g., take) with a state space of about 146M states. Five parameters were defined to influence the CSSM: To study the influence of the different parameters on activity recognition, we applied our CSSM to the sensor data. The domain experts identified 432 parameter settings to be examined. By applying the CSSM to the sensor data of all seven subjects we obtained 3,024 algorithm runs in total. For each run, the CSSM assigned one of the 16 activities to each time step in the sensor data. The overall duration of the experimental sessions varied per subject within the range of 160s to 320s (600 to 1,200 time steps at 3.75 Hz).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">STATISTICAL ANALYSIS OF RECOGNIZED ACTIVITIES</head><p>To study activity recognition algorithms, statistical analysis commonly comprises: (1) deriving measures to assess recognition performance, (2) displaying these measures as statistical plots in relation to tested parameters for generating hypotheses about parameter dependencies, and (3) statistical hypotheses testing <ref type="bibr" target="#b11">[12]</ref>.</p><p>A typical measure for recognition performance is accuracy, that is, the percentage of correctly labeled time steps. The statistical analysis starts with calculating accuracy for each parameter configuration. Correct and incorrect labels are determined through comparing the sequences of recognized labels with the ground truth. To evaluate the influence of single parameters and of multiple parameters on the accuracy, we applied a repeated measures analysis of variance (rANOVA) across all tested configurations. We obtained accuracy ratings for single activities and globally for all activities. <ref type="table">Table 1</ref> shows an excerpt of the statistical analysis. For single parameters (top), the results show that most parameters, except for parameter P Time , have highly significant influence (marked with ***) regarding the overall recognition accuracy as well as regarding individual activities. Particularly, the sensor model (P Sensor ) has by far the highest influence, as indicated by its F value, followed by parameter P Filter and P Dist . Similar ratings of these parameters can be observed for other subjects and activities in our experiment.</p><p>Relationships between multiple parameters (bottom) show significant influence as well. For example, the relationship between parameters P Filter and P Sensor exhibits strong influence. Yet, the influence also varies depending on the activities and the relationships between parameters. For instance, the relationship P Sensor : P Dist : P Time shows significant influence on the recognition accuracy of activity eat, but less influence on the general accuracy. Some higher-order relationships between parameters, such as P Filter : P Dist : P Weight : P Time , have no significant influence.</p><p>In summary, all single parameters as well as certain relationships between multiple parameters clearly influence the performance and thus need to be considered for activity recognition based on CSSMs. However, applying purely statistical analysis to study how these dependencies manifest would require additional investigations. For example, suitable measures have to be defined for judging parameter influence on each activity and its temporal behavior. Moreover, large amounts of statistical ratings and associated plots would need to be compared to evaluate the influence of each parameter setting individually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">VISUAL ANALYSIS OF RECOGNIZED ACTIVITIES</head><p>Visual analysis is another way to study the performance of activity recognition. Next, we present a novel visual analytics approach for this purpose. We start by summarizing the design requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Design Requirements</head><p>Discussing with domain experts and analyzing the statistics-based approach, we derived requirements for the visualization design.</p><p>Visualizing Labeled Segments (DR 1 ): The first step towards analyzing activity recognition is to inspect the recognized activities over time for one specific parameter configuration. The existence or absence of activities, their duration, and temporal sequence can help to decide about accepting or rejecting configurations. Thus, visualizing the labeled segments is a basic requirement. Relating Parameters to Labeled Segments (DR 2 ): To explore interrelationships between different parameters, multiple or even all parameter configurations need to be examined at once. By comparing the influence of parameter combinations, users can search for appropriate configurations with regard to a given task. Hence, the second requirement is visualizing all relevant parameter settings along with the algorithmic outcome. Relating Accuracies to Labeled Segments (DR 3 ): The results of the activity recognition are commonly compared to ground truth data to judge which parameter settings lead to adequate results. Deviations from the ground truth lead to reduced accuracies. Evaluating accuracy globally and individually for selected activities helps to understand how well certain parameter settings match the properties of the sensor data. Therefore, visually associating accuracy with the labeled segments has to be supported. Exploring Patterns and Relationships (DR 4 ): A visual analysis of parameters, labeled segments, and accuracies, requires exploring global and local temporal patterns. Global patterns typically appear as similar outputs across all configurations or along an entire time series. Relating global patterns to the parameter settings helps, for example, to exclude individual parameters with less influence. Local patterns on the other hand, depend on specific ranges of parameter values, exist for limited time spans, or occur only with certain sequences of activities. Relating local patterns to the parameter settings helps, for instance, to detect parameter values with undesired side effects. Identifying global and local patterns is vital for choosing suitable parameter settings as well as for reasoning about the performance of the activity recognition algorithms. Hence, besides visualizing the data, interactive exploration of patterns and relationships needs to be facilitated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Visual Encoding</head><p>Our main objective is the exploration of the parameters' influence on labeled segments and associated accuracies over time. For this purpose, we adapt the visual layout found in <ref type="bibr" target="#b12">[13]</ref>. We extend this design to (i) consider categorical data, (ii) take accuracy information into account, (iii) show aggregated information about segmentation results both per time step and per algorithm run, and (iv) integrate tailored interaction techniques to explore the data, parameter settings, and relationships. To this end, we provide a compact visual overview (see <ref type="figure" target="#fig_1">Fig. 3</ref>) to match the design requirements DR 1 . . . DR 4 . Encoding Labels We select the visual variable color to encode the different labels as it is well suited for representing categorical data <ref type="bibr" target="#b13">[14]</ref> and allows for a compact visual design (cf. <ref type="bibr" target="#b10">[11]</ref>). Other reasonable options are either needed to represent the temporal dimension, e.g., position, or would require more display space, e.g., area or length. To address scalability issues when dealing with larger numbers of labels, we select distinct colors from the L*a*b* color space and maximize the perceptual distance of neighboring labeled segments by permuting their color assignment (cf. <ref type="bibr" target="#b8">[9]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualizing Labeled Segments</head><p>The algorithmic outcome is visualized as sequences of colored stripes over time (see <ref type="figure" target="#fig_1">Fig. 3b</ref>). As aforementioned, we apply qualitative color palettes with every label assigned to a distinct color. Additionally, the preset color coding can be interactively adapted to match given color conventions.</p><p>Each row of colored stripes represents the segmentation and labeling results of one algorithm run. This compact encoding provides a good overview and facilitates the comparison of multiple segmented and labeled time series (DR 1 ).</p><p>Visualizing Parameters The parameter settings are shown aligned to each algorithm run ( <ref type="figure" target="#fig_1">Fig. 3a)</ref> and thus, the connection between parameter values and the algorithmic outcome is established (DR 2 ). Each row of the parameter display encodes one particular parameter setting and each column represents a single parameter. The parameter values are encoded using shades of gray. For each parameter individually we use distinct shades for categorical parameter values or a sequential scale for numerical parameter values. This way, the parameters can be clearly distinguished from the presentation of labeled segments.</p><p>Visualizing Accuracies To evaluate the activity recognition performance of each parameterization, we visualize accuracy information along with the labeled segments (DR 3 ). As a reference, the ground truth data is visualized as a single sequence of colored stripes on top of the labeled segments <ref type="figure" target="#fig_1">(Fig. 3c</ref>). To show deviations to this ground truth data, the user can switch between two additional encoding modes for the labeled segments. The first mode shows the accuracy of the labeled segments in a binary encoding: correct segmentations are colored green and incorrect are colored red <ref type="figure" target="#fig_1">(Fig. 3d)</ref>. The second mode provides a more detailed differentiation regarding incorrect results of one selected activity: true positives (TP) are colored green and false positives (FP) and false negatives (FN) are colored in different shades of red <ref type="figure" target="#fig_1">(Fig. 3e</ref>). This way, the color coding of both modes is visually consistent. Selecting another activity in the second mode results in a re-coloring of the labeled segments.</p><p>Compound Views of Labeled Segments The visual encoding discussed so far, allows for exploring individual segmentation and labeling results, associated accuracy information, and underlying parameter dependencies. To summarize the labeled segments in different ways, we introduce a horizontal and a vertical compound view. Both views provide a greater overview of multiple segmentation results at once and in this way facilitate the detection of local and global patterns. The horizontal compound view aggregates labels across all parameterizations for every point in time. It is located beneath the visualization of the labeled segments <ref type="figure" target="#fig_1">(Fig. 3f)</ref>. The vertically oriented compound view is positioned on the right of the labeled segments and sums up labels for the entire time series of each parameter setting <ref type="figure" target="#fig_1">(Fig. 3g</ref>). Both compound views encompass three separate components. The first component encodes predominant labels by calculating majorities over parameter settings and time <ref type="figure" target="#fig_1">(Fig. 3h)</ref>. The second component communicates the percentage of this labeling <ref type="figure" target="#fig_1">(Fig. 3i)</ref>. The percentage values are obtained by computing the ratio of dominant labels with respect to the overall distribution of labels. These values are then encoded using a sequential color scale, with high values indicated by dark colors. To clearly identify high and low percentages, we enlarge respective stripes outside of a certain percentage interval. The third component provides further details and encodes the ratio of each label using stacked bars <ref type="figure" target="#fig_1">(Fig. 3j)</ref>. Labels with very low ratio of segments are summarized into one neutral color to ensure visibility and to make the ratios comparable.</p><p>In this way, the three components of the compound views allow to easily trace predominant labels and accuracy conditions. For example, the horizontal compound view gives the user a sense for the stability of the label selection over time. Moreover, it facilitates an interactive data folding: labels, which are of high accuracy and stable across different parameterizations for long time periods, can be collapsed to focus the exploration on uncertain segments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Interactive Exploration</head><p>To support a comprehensive visual exploration we provide a rich set of tailored interaction techniques. These techniques are fundamental for studying patterns and relationships in changing scenarios and thus, particularly address design requirement DR 4 .</p><p>Navigation Our compact visual encoding provides overview for comparatively large amounts of labeled segments from different parameter settings. Yet, considering the size of the data in certain usage scenarios, showing all parameter values and time steps for each algorithm run can easily exceed the available screen space. Interactive zooming and panning enable users to overcome these limitations and view different subsets in greater detail. The parameter axis (vertical) and the time axis (horizontal), can be scaled and translated independently or in combination. During zooming and panning, all views are interlinked to preserve the relationships between parameters, labeled segments, and summaries in the compound views. Furthermore, each view can be moved and collapsed individually to improve screen space utilization.</p><p>Zooming and panning supports studying global patterns. Local patterns generally benefit from combining overview and detail displays. An interactive rectangular fish-eye lens <ref type="bibr" target="#b16">[17]</ref> shows selected subsets in detail while preserving their context regarding the whole data set. Lens position and magnification can be adjusted to focus on the information relevant for the task at hand.</p><p>Visual cues provide additional support. Scroll bars represent the location of the current view and two visual indicators inform the user whether the visualization is affected by over-plotting along either axis <ref type="figure" target="#fig_1">(Fig. 3k)</ref>. A red indicator signals that the perception of patterns might be impaired by over-plotting. Clicking an indicator switches to a more adequate zoom level.</p><p>Selecting and Highlighting Investigating patterns regarding selected labels or specific sequences in a data set of hundreds of parameter settings with thousands of labeled segments can be a tedious task. To support users, we extend our overview visualization with flexible selection and highlighting techniques.</p><p>A selection of labeled segments can be performed in several ways, including selecting (i) segments of labels, (ii) accuracy conditions, (iii) parameterizations, (iv) segments crossing a specific point in time, and (v) particular regions of interest by applying an interactive selection lens. Selections can be expanded or reduced using binary set operations, or be restricted to selected labels only. While selecting, the visual encoding is updated to dim deselected segments. All selection options are provided in a toolbar <ref type="figure" target="#fig_1">(Fig. 3m)</ref>.</p><p>Our selection techniques support both: (1) changing the selection to compare different parts of the data and (2) viewing the selection in different encoding modes to analyze the chosen subset in greater detail. <ref type="figure" target="#fig_2">Figure 4</ref> illustrates this functionality. The default activity view is shown in <ref type="figure" target="#fig_2">Figure 4a</ref>. In the example, a selection is initialized by clicking on a bar within the stacked bar chart of the horizontal compound view to select all contributing segments crossing the specified time step <ref type="figure" target="#fig_2">(Fig. 4b)</ref>. Afterwards, the selection is altered using the selection lens to include only segments of specific length <ref type="figure" target="#fig_2">(Fig. 4c)</ref>. Finally, by switching the encoding mode, the selected segments are associated with accuracy information <ref type="figure" target="#fig_2">(Fig. 4d)</ref>. This allows to examine true positively (green) and false positively (red) classified time steps. As we explain next, such selections can also assist in changing the arrangement of the visualization.</p><p>Sorting Visualizing parameter settings and labeled segments in a row-wise fashion enables analyzing their dependencies. However, the order of the rows is essential for discovering patterns across multiple configurations <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref>. To support the user in finding suitable orderings, we provide automated sorting based on (i) labeled segment, (ii) accuracy, and (iii) parameters.</p><p>Sorting based on labeled segments and accuracy helps to identify configurations with similar sequences, and to build hypotheses related to the associated parameter values. In turn, parameter sorting facilitates the interpretation of parameter influence, allowing us to test the hypotheses from the parameters' perspective.</p><p>Sorting multiple time series of labeled segments and parameter settings is rather challenging. Sorting labeled segments requires a similarity measure for sequences of categorical values that also considers their temporal context. Sorting parameter settings demands taking heterogeneous types of data into account, such as categorical and numerical parameter values with distinct scales and resolutions. To address such diverse data properties, we provide various metrics and algorithms for automated sorting. For example, we use modified edit distances in combination with self-organizing maps for sorting labeled segments over time. For parameters, we mainly employed lexicographic or averaged Euclidean based sorting algorithms. <ref type="figure" target="#fig_3">Fig. 5</ref> illustrates the impact of sorting on the patterns revealed by our visualization.</p><p>In addition to global sorting, users can interactively restrict the sorting to selected parts of the data. Cursors highlight the selected parts in the overview and visually link them to parameter settings and the time axis. Optionally, different sorting methods can be applied to different selections. Also, rows can be reordered manually.</p><p>By means of automatic and interactive sorting, users can analyze value distributions and temporal patterns. For example, global and local dependencies between parameters and labeled segments can be identified by testing different sortings on subsets of the data and looking for patterns across rows. Sorting according to accuracy information allows to examine configurations with similar performance over time and to relate them to the ground truth by switching between activity and accuracy encoding modes. All selections and sorting techniques are maintained in an interaction history. This allows users to directly undo or redo individual sorting steps and trace their overall analysis sessions.</p><p>Combining the visual design with the rich set of interaction techniques completes our visual analytics approach. Next, we apply this approach to analyze the data from our use case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results</head><p>We implemented a research prototype and applied it to analyze the data generated by the automated segmentation and labeling described in Sect. 3. To study the results, we conducted informal interviews with three experts from the visualization community. The experts had multiple years of experience in visual analysis of parameter spaces, multivariate time series, and probabilistic classifiers. Before each interview, we briefly explained the background of the data as well as goals and open questions regarding our use case. However, the results of the statistical analysis were concealed to prevent biasing interpretations of the shown data. After a short introduction of the research prototype, the visualization experts proceeded to explore and analyze the data. Assistance regarding the usage of specific features was provided and all comments and insights were documented by the interviewer. Our machine learning experts participated in the interviews as well. They were asked to reason directly about findings made by the visualization experts; some findings were reflected in subsequent discussions. Next, we summarize some of the findings and show respective visualizations.</p><p>Parameters' Influence on Accuracy <ref type="figure" target="#fig_4">Fig. 6</ref> illustrates dependencies between parameters and the performance of the activity recognition algorithm in general. In the example, the data of the sixth subject is shown with labeled segments encoded according to correct (green) and incorrect (red) classifications. First, the time series were sorted in ascending order regarding the amount of correctly classified time steps. Based on the resulting overview, a subsequent sorting was applied with respect to the parameter with the most obvious influence. At first glance, <ref type="figure" target="#fig_4">Fig. 6</ref> shows a global trend of low to high accuracy across algorithm runs from bottom to top <ref type="figure" target="#fig_4">(Fig. 6a</ref>) and a second global trend over time from left to right <ref type="figure" target="#fig_4">(Fig. 6b)</ref>. The first trend <ref type="figure" target="#fig_4">(Fig. 6a</ref>) reflects the dependency of the activity recognition results on the parameter P Sensor . The global temporal trend <ref type="figure" target="#fig_4">(Fig. 6b</ref>) reveals that the number of misclassifications increases over time for the majority of parameter settings. In fact, this effect was suspected by our domain experts, but prior to visualizing the data it was not known for certain.</p><p>A second observation was made regarding the influence of P Filter . Local gradients can be identified for P Filter and the ratio of misclassifications per configuration (stacked bars in the vertical compound view <ref type="figure" target="#fig_4">(Fig. 6c)</ref>). This indicates that some filter modes mostly result in higher accuracy (light-gray and white) compared to other modes (black and dark-gray). Additionally, P Dist shows an inverse local gradient for the labeled segments <ref type="figure" target="#fig_4">(Fig. 6d)</ref>. Hence, higher accuracies within this band only appear in connection with two modes (black or gray), but not with the third mode (white).</p><p>Triggered by these observations, our domain experts raised questions concerning the influence of P Weight on the accuracy. Hence, during one of the interviews a visualization expert sorted the visualization first with respect to P Weight and then by overall accuracy. Interestingly, subtle patterns emerged while comparing the predominant labels in the vertical compound view with each band of configurations per parameter value. Increasingly good classification results coincide with lower parameter values, but accuracy decreases with higher values. This dependency is due to the fact that too low and too high degrees of goal-directed action selection can impair the activity recognition model. Consequently, finer sampling parameter P Weight and testing respective parameter settings were noted for future improvements of the results.</p><p>Parameter Influence on Activities After analyzing the performance of the activity recognition in general, the analysis sessions proceeded to investigate parameter influence on individual labels. By relating accuracy to the ground truth, the visualization experts initially noticed accumulations of misclassifications around certain activities. An effect especially prevalent for the activities cook and eat across the data of different subjects <ref type="figure" target="#fig_4">(Fig. 6e)</ref>. Hence, these activities were explored in greater detail.</p><p>As an example, <ref type="figure" target="#fig_5">Figure 7</ref> depicts the data of the first subject with labeled segments encoded according to activities. All segments of cook and eat were selected and time series are sorted based on their similarity. From the overview, three distinct groups of time series emerge that are vertically aligned with different values of P Sensor . The first group <ref type="figure" target="#fig_5">(Fig. 7a</ref>) corresponds to the parameter value encoded in white and includes segments roughly equal to the ground truth. In contrast, numerous missing segments in the second group <ref type="figure" target="#fig_5">(Fig. 7b</ref>) indicate that the recognition of both activities fails with the gray colored parameter value. Finally, the black colored parameter value leads to the third group <ref type="figure" target="#fig_5">(Fig. 7c</ref>) of overlong segments which in case of activity eat even merge two separate instances in the ground truth. The vastly different results of these three groups hint at an overly coarse sampling of P Sensor . Hence, studying other values might deepen the understanding of the parameter's influence. For example, testing other sensor models could help to obtain adequate recognition results for both activities.</p><p>The second group of missing segments <ref type="figure" target="#fig_5">(Fig. 7b)</ref> was investigated further. Selecting and highlighting the segments of other labels made clear that the respective parameter value had produced noisy segmentation and labeling results. To determine which labels were confused, the visualization experts examined the stacked bars in the horizontal compound view <ref type="figure" target="#fig_5">(Fig. 7d)</ref>. Instead of eat, mainly the activity drink was recognized. Switching between accuracy encodings for each label and comparing false negatives of eat with false positives of the other labels confirmed these assumptions.</p><p>The results of the visual analysis suggest that parameter influence can be observed by global and local patterns in the computed activities and associated accuracy. Based on the visual analysis, the domain experts were able to answer open questions regarding dependencies between individual parameters values and the performance of the activity recognition. Several new hypotheses were generated on how the influence of single parameters and combinations of multiple parameters manifests. In addition, exceptional parameter settings and temporal influences were identified. All of the reported insights require further verification and validation, for example, by testing the parameter settings with different data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION AND LESSONS LEARNED</head><p>So far, we described the statistical analysis approach for evaluating recognition performance (Sect. 4) and our novel visual analytics approach (Sect. 5) in isolation. In this section, we consider both approaches in a broader discussion.</p><p>Participatory Design Our visual-interactive design is the result of a participatory design process in cooperation with a group of four machine learning experts. In this process we conducted several collaborative analysis sessions. The gathered feedback formed the basis for our visualization and interaction techniques. Most design decisions were made to address data properties and analysis tasks. Some visualization components were motivated by the machine learning experts, e.g., the compound views for showing label distributions and the amount of correct and incorrect segments.</p><p>Reflecting on the Findings Generally, statistical analysis as well as visual-interactive exploration support users in understanding the complex interplay between parameters, labeled segments, and accuracies. However, the findings obtained are rather different. While statistical analysis provides quantified assessments of specific aspects, findings from visual-interactive exploration are rather of qualitative nature. For example, the results of the statistical analysis allow to rate and compare the influence of single parameters and of multiple parameters <ref type="table">(Table 1</ref>). In addition, by testing every parameter configuration, certain accuracy characteristics can be traced precisely. But, this comes at the cost that relevant parameter settings have to be determined beforehand. In this regard, the analysis mainly supports the identification of particular dependencies and provides results with regard to predefined questions.</p><p>In contrast, visual-interactive exploration provides overview and details on demand, without the need to specify the relevant parameter settings in advance. This enables users to investigate global and local patterns as well as their temporal context, which in turn helps to explain results of the statistical analysis. As an example, the statistical results indicate significant influence of P Sensor on the accuracy <ref type="table">(Table 1</ref>). This influence can be observed visually as well <ref type="figure" target="#fig_4">(Fig. 6</ref>). On top of that, the visual analysis reveals variations in accuracy due to differently labeled segments of specific activities for each value of P Sensor <ref type="figure" target="#fig_5">(Fig. 7)</ref>. Thus, interactively studying the data leads to new hypotheses and supports reasoning about additional influences on the activity recognition's mode of operation.</p><p>Yet, complex relationships can be hard to discern visually. For instance, during the interview sessions mostly relationships between two parameters and rarely up to three parameters have been identified. Moreover, the experts commented that some of the perceived patterns were rather vague and that they were uncertain about their findings at times. Consequently, they asked for computation means to verify their hypotheses. This suggests that statistical and visual-interactive analysis should be used in combination. This way, global and local patterns can be communicated, hypotheses about dependencies can be generated, and interesting findings can be validated in an intertwined fashion.</p><p>Reflecting on the Analytic Process We analyzed the outcome of activity recognition to determine dependencies between parameters (input) and labeled segments (output). However, our current analysis (statistical and visual-interactive) is mainly an offline process after the recognition procedure. Hence, it can be considered outcome-oriented. Consequently, relevant inputs have to be either known in advance or have to be tested exhaustively.</p><p>Yet, choosing suitable parameter values to investigate their influence is challenging. Analyzing the inputs and outputs of activity recognition can support users in finding appropriate parameter samples. For instance, starting an evaluation by testing only subsets of parameter settings, allows to generate initial hypotheses. Domain experts can then reject parameters with insignificant influence in an early stage of the evaluation and instead focus an testing additional values of parameters which show strong effects. This enables decisions about the performance with regard to certain quality criteria, without the necessity of testing all parameter settings. This way, moving towards an online analysis of the data facilitates direct feedback and steering of the evaluation, which in turn helps in guiding users to desired results and fine-tuning algorithms to specific needs.</p><p>For example, the insights gained from the statistical and visualinteractive analysis in our use case led to further samplings of P Weight to better understand its influence and to improve previous results (see Sect. 5.4). Our domain experts identified the ability to visually explore different patterns and to generate hypotheses early to be a major advantage. But, they simultaneously asked for tightly interrelating such visual analysis methods into the concept phase and statistical evaluation phase of their algorithms.</p><p>In this context, other open issues from applying automated activity recognition remain, such as dealing with diverging data properties, different algorithms, or changing performance requirements. For instance, our use case focused on studying parameters' influence for one algorithm (CSSM) with a fixed set of parameter settings in a given application scenario. The applied statistical analysis methods were customized to fit the evaluation of this specific setup. Consequently, they are hard to generalize regarding other conditions or evaluation constraints. In contrast, our visual analytics approach is generic enough to be applied to other algorithms or data sets for generating insights and hypotheses. Though, validating and verifying these hypotheses currently needs to be done separately.</p><p>Other Application Scenarios While our use case focused on data from activity recognition, the proposed solutions are applicable in other fields as well. Generally promising is the analysis of parameter influence on time series segmentation and labeling. For instance, Alsallakh et al. <ref type="bibr" target="#b0">[1]</ref> present a scenario dealing with the reconstruction of drilling processes from sensor measurements using automated methods. Similarly to our use case, their scenario involves long term time series containing thousands of time steps. In case of an erroneous segmentation and labeling of this data, drilling specialists have to inspect the reconstructed processes, adjust parameters of automated algorithms, and compare according outputs. In this regard, our solution can be beneficial as it provides overview for multiple parameter settings and allows for exploring different properties of the complex data. Moreover, by analyzing parameters' influence with our solution, the specialists can find suitable parameterizations and adjust the algorithms to different drilling conditions.</p><p>An interesting aspect of such applications is that the ground truth is often not known in advance. Concerning this matter, our approach enables comparing different segmentation and labeling results and choosing configurations that meet desired quality criteria. For example, using the overview and compound views predominant labels can be traced over time while also communicating the stability of this label selection. On this basis, users can adjust the results interactively to compensate for uncertain segments or missing data (cf. <ref type="bibr" target="#b0">[1]</ref>). Once suitable labeled segments have been defined, they can be used as a reference, i.e., in place of a ground truth, to analyze other configurations in their context with our proposed solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION AND FUTURE WORK</head><p>In this paper we present a novel visual analytics approach for parameter-dependent activity recognition and compare our approach with statistical analysis methods. The statistical analysis of recognized activities enables rating the influence of parameters on the performance of used algorithms. Supported by our novel overview visualization of parameters, labeled segments, and accuracies, users can investigate the computed results and discover global and local patterns. Coordinated views and interactions help exploring and comparing different aspects of the data and facilitate hypothesis generation about associated dependencies. In conclusion, by providing extensive visual and interactive means, our approach supports users in understanding parameters' influence and in this way helps in increasing the accuracy of computed results.</p><p>To improve our approach, we plan further user studies and interviews with domain and visualization experts. Considering the lessons learned, we are interested in more tightly interrelating statistical and visual-interactive methods. For example, using direct visual feedback, users can generate hypotheses regarding analyzed parameter settings and trigger according statistical analysis to verify them. This allows to drill-down only into parameter subspaces that fit specific requirements, which reduces computational costs.</p><p>Additionally, our solution will be extended for comparing multiple data sets at once, such as recordings from several protagonists or different activity recognition algorithms. On the one hand, this allows to consider subjective and environmental influences when evaluating the performance of applied methods. On the other hand, taking the provenance of the data, i.e., changing algorithmic models, and their associated parameter spaces into account enables users to enhance their search to find an appropriate recognition algorithm with the best possible combination of parameters. Yet, this requires enhanced visual-interactive designs for dealing with the added complexity, e.g., dedicated aggregation methods to support scalability, but also for detecting and analyzing sophisticated relationships among several influencing factors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of our experiment. The human figure (a) shows, where the sensors are located. Three exemplary activities (b)cook, eat, and wash -are depicted in combination with their timeoriented and multivariate sensor observations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Schematic illustration of the visual analysis tool. For each parameter setting (a) three different modes are shown: activities as colorcoded stripes (b), deviations from the ground truth (c) for all activities (d), and for one selected activity (e). Two additional views represent three types of aggregated information per column (f) or per row (g): the dominant activity (h), the percentage of the dominant activity (i), and the percentages of all activities as stacked bars (j). A visual cue (k) denotes the presence (red) or absence (green) of over-plotting in rows and columns. Selecting and highlighting different parts of the data is enabled through a dedicated toolbar (m).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Selecting and highlighting labeled segments. The image sequence shows: (a) the default activity view, (b) a selection of segments for one activity (gray) crossing one time point, (c) a reduced selection for certain parameter settings, and (d) the same selection in accuracy encoding mode with true (green) and false (red) positives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Sorting labeled segments. The overviews show algorithm runs (a) in their default order and (b) sorted based on labeled segments. The sorting reveals dependencies between the occurrence of red segments and values of the first and third parameter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Visualizing correct and incorrect classifications. The overall accuracy mainly depends on (a) parameter P Sensor , (c) parameter P Filter , (d) parameter P Dist , and (b) decreases over time. In addition, misclassifications accumulate (e) around certain activities in the ground truth (top). All effects can be confirmed by looking at the vertical and horizontal compound views, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Visualizing activities cook and eat. The selection of all segments of both activities shows their dependence on the values of parameter P Sensor : (a) segments are similar to the ground truth (top), (b) segments are missing for both activities, and (c) overlong segments. Moreover, the gray colored parameter value (b) leads to noisy segments with different labels (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>&lt;0.001 *** 429.74 &lt;0.001 *** P Filter 386.97 &lt;0.001 *** 68.19 &lt;0.001 *** P Dist 166.71 &lt;0.001 *** 150.14 &lt;0.001 *** P Weight 33.31 &lt;0.001 *** 15.62 &lt;0.001 ***</figDesc><table><row><cell></cell><cell cols="2">Subject 6 (all)</cell><cell cols="2">Subject 1 (eat)</cell></row><row><cell>Single parameters</cell><cell>F</cell><cell>p</cell><cell>F</cell><cell>p</cell></row><row><cell cols="3">P Sensor 1496.16 P Time 9.96 0.003 **</cell><cell cols="2">6.34 0.014 *</cell></row><row><cell>Multiple parameters</cell><cell>F</cell><cell>p</cell><cell>F</cell><cell>p</cell></row><row><cell>P Filter : P Sensor</cell><cell cols="4">32.25 &lt;0.001 *** 26.07 &lt;0.001 ***</cell></row><row><cell>P Weight : P Time</cell><cell cols="2">3.30 0.011 *</cell><cell cols="2">5.83 &lt;0.001 ***</cell></row><row><cell>P Sensor : P Dist : P Time</cell><cell cols="2">0.95 0.442</cell><cell cols="2">12.45 &lt;0.001 ***</cell></row><row><cell>P Sensor : P Weight : P Time</cell><cell cols="2">3.82 &lt;0.001 ***</cell><cell cols="2">5.82 &lt;0.001 ***</cell></row><row><cell>P Filter : P Sensor : P Dist : P Time</cell><cell cols="2">7.83 &lt;0.001 ***</cell><cell cols="2">1.39 0.194</cell></row><row><cell>P Filter : P Dist : P Weight : P Time</cell><cell cols="2">1.05 0.426</cell><cell cols="2">1.92 0.016 *</cell></row><row><cell cols="5">Table 1: Exemplary results from statistically analyzing the perfor-</cell></row><row><cell cols="5">mance of the CSSM. Shown are quantitative ratings (F and p values)</cell></row><row><cell cols="5">and levels of significance (low: *, mid: **, high: ***) from applying a</cell></row><row><cell cols="5">rANOVA to assess the influence of single parameters (top) and the</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Sensor model (P Sensor ): This parameter determines the quality of</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>the sensor model and thereby the amount of information used</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>during activity recognition.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Filter mode (P Filter ): This parameter selects different filtering</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>modes for the processing of the sensor measurements.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Goal distance mode (P Dist ): This parameter offers different algo-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>rithms for a goal-directed selection of successive actions.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Goal distance weight (P Weight ): This parameter implements dif-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ferent degrees of goal-directedness when selecting actions.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Duration model (P Time ): This parameter offers different duration</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>models for the activities to be recognized.</cell></row></table><note>influence of relationships between multiple parameters (bottom). The presented ratings are calculated regarding the general accuracy of the sixth subject and the accuracy of activity eat of the first subject.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work was supported by the German Research Foundation (DFG) as part of VASSiB (SPP 1335), the DFG graduate research school 1424 MuSAMA, and by the Austrian Federal Ministry of Science, Research, and Economy via CVAST, a Laura Bassi Centre of Excellence (No. 822746).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A visual analytics approach to segmenting and labeling multivariate time series data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alsallakh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bögl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gschwandtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Miksch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Esmael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnaout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Thonhauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zöllner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Eu-roVA&apos;14</title>
		<meeting>of Eu-roVA&apos;14</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A framework for visualization and exploration of events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Beard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">R</forename><surname>Pettigrew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Visualization</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="151" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Uncertaintyaware exploration of continuous parameter spaces using multivariate prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Piringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Filzmoser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gröller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comp. Graph. Forum</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="911" to="920" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Paraglide: Interactive parameter space partitioning for computer simulations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bergner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sedlmair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Möller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Abdolyousefi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1499" to="1512" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sequence analysis with stata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brzinsky-Fay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stata J</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="435" to="460" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An extension of self-organizing maps to categorical data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Marques</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Progr. in Art. Intelligence</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="304" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Delocalized unsteady vortex region detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Peikert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sadlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alsallakh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gröller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of VMV&apos;08</title>
		<meeting>of VMV&apos;08</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="81" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Analyzing and visualizing state sequences in R with TraMineR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gabadinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ritschard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Studer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An automatic generation of schematic maps to display flight routes for air traffic controllers: Structure and color optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hurter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Serrurier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tabart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Vinot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AVI&apos;10</title>
		<meeting>of AVI&apos;10</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">LiveGantt: Interactively visualizing a large manufacturing schedule</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2329" to="2338" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Designing pixel-oriented visualization techniques: Theory and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="78" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Computational state space models for activity and intention recognition. a feasibility study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Krüger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nyolt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yordanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kirste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">109381</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Supporting the integrated visual analysis of input parameters and simulation trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luboschik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rybacki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Haack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Schulz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comp. &amp; Graph</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="37" to="47" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Automating the design of graphical presentations of relational information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mackinlay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>ACM TOG</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="110" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Design galleries: A general approach to setting parameters for computer graphics and animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andalman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Beardsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F F</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V</forename><surname>Mirtich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ruml</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ryall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Seims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGGRAPH&apos;97</title>
		<meeting>of SIGGRAPH&apos;97</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="389" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visualization of parameter space for image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Pretorius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-A</forename><surname>Bray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Ruddle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2402" to="2411" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">General rectangular fisheye views for 2d graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Rauschenbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jeschke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comp. &amp; Graph</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="609" to="617" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Analyzing parameter influence on time-series segmentation and labeling. Poster at IEEE VAST&apos;14</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Röhlig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luboschik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bögl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alsallakh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Miksch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visual parameter space analysis: A conceptual framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sedlmair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Heinzl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bruckner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Piringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Möller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2161" to="2170" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Tuner: Principled parameter finding for image segmentation algorithms using visual response surface exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Torsney-Weir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Möller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-C</forename><surname>Hege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Verbavatz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1892" to="1901" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient unsupervised temporal segmentation of human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vögele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krüger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SCA&apos;14</title>
		<meeting>of SCA&apos;14</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Temporal summaries: Supporting temporal categorical searching, aggregation and comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Plaisant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Spring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1049" to="1056" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Finding comparable temporal categorical records: A similarity measure with an interactive visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wongsuphasawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of VAST&apos;09</title>
		<meeting>of VAST&apos;09</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
