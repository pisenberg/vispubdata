<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VA 2 : A Visual Analytics Approach for // Evaluating Visual Analytics Applications</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanja</forename><surname>Blascheck</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>John</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuno</forename><surname>Kurzhals</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Koch</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Thomas</forename><surname>Ertl</surname></persName>
						</author>
						<title level="a" type="main">VA 2 : A Visual Analytics Approach for // Evaluating Visual Analytics Applications</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TVCG.2015.2467871</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>visual analytics</term>
					<term>qualitative evaluation</term>
					<term>thinking aloud</term>
					<term>interaction logs</term>
					<term>eye tracking</term>
					<term>time series data</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Fig. 1. Our interactive visualization approach for finding patterns in eye movements and interactions consists of a) an Area of Interest (AOI) Sequence Chart View for individual AOI timelines of participants, b) an Options panel, c) a Participant List View represented with a dendrogram, d) an AOI List View containing AOIs and histograms, e) a Legend, and f) a Pattern Editor to search for patterns. Abstract-Evaluation has become a fundamental part of visualization research and researchers have employed many approaches from the field of human-computer interaction like measures of task performance, thinking aloud protocols, and analysis of interaction logs. Recently, eye tracking has also become popular to analyze visual strategies of users in this context. This has added another modality and more data, which requires special visualization techniques to analyze this data. However, only few approaches exist that aim at an integrated analysis of multiple concurrent evaluation procedures. The variety, complexity, and sheer amount of such coupled multi-source data streams require a visual analytics approach. Our approach provides a highly interactive visualization environment to display and analyze thinking aloud, interaction, and eye movement data in close relation. Automatic pattern finding algorithms allow an efficient exploratory search and support the reasoning process to derive common eye-interaction-thinking patterns between participants. In addition, our tool equips researchers with mechanisms for searching and verifying expected usage patterns. We apply our approach to a user study involving a visual analytics application and we discuss insights gained from this joint analysis. We anticipate our approach to be applicable to other combinations of evaluation techniques and a broad class of visualization applications.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The evaluation of visualization techniques plays an important role to understand if users and how users perceive and interpret a new visualization approach. Especially for visual analytics (VA) it is essential to evaluate whether an approach conveys information, fosters insight, and support sense making <ref type="bibr" target="#b24">[25]</ref>. As a consequence, researchers developed sophisticated methods and measures over the years to evaluate numerous aspects of usability and comprehensibility <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref>. Typical examples are measures of completion times and accuracy to evaluate a participant's performance with a new visualization technique. The complexity of modern visualization applications increases, especially in the field of VA. Thus, researchers require a combination of multiple methods and measures to better understand sense making <ref type="bibr" target="#b23">[24]</ref> and insight generation <ref type="bibr" target="#b8">[9]</ref>.</p><p>Researchers have adapted common approaches from other research fields for an evaluation of visualization techniques, i.e., thinking aloud protocols, interaction logs, and eye tracking.</p><p>Thinking aloud protocols: Researchers often apply thinking aloud protocols <ref type="bibr" target="#b14">[15]</ref> to receive qualitative feedback from participants during the use of an application. Audio or written transcriptions concurrently log participants' verbalized thoughts and actions. Protocols are finally annotated and analyzed. A retrospective variant is an alternative where participants watch a video of their actions after a task and verbalize their thoughts then. Interaction Logs: Interaction logs provide spatial as well as temporal data about participants' input to an interface <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">30]</ref>. Mouse movements, mouse clicks, and key strokes are typical examples of information that an interaction log contains. Additionally, the logs contain information about visual components with which a user interacted. An analyst can then evaluate these log files using statistical as well as visual techniques. While statistical evaluation can provide useful information about frequencies of certain interactions, visual analysis provides more information about spatio-temporal patterns, i.e., sequences of interactions (e.g., Kang et al. <ref type="bibr" target="#b23">[24]</ref>).</p><p>Eye tracking: An analysis of recorded eye movements can provide insight into visual reading strategies of participants while using a VA system <ref type="bibr" target="#b27">[28]</ref>. Recorded eye tracking data contains spatial as well as temporal information about participants' fixations on a screen. For an analysis of this data, evaluators can apply statistical and visualization techniques. For example, Andrienko et al. <ref type="bibr" target="#b3">[4]</ref> applied VA approaches to evaluate eye tracking data recorded from other VA techniques.</p><p>Researcher usually analyze recorded data from these protocols separately. However, to fully understand test subjects' common strategies and cognitive processes behind the recorded data, a combined analysis of these data sources is beneficial. All of them provide a certain view of the interaction between a participant and an evaluated VA system. Typically, researchers measure task performance and completion times when evaluating a VA system. With this information, they can find out if participants carried out a task correctly. One important research question is to find out why some participants need significantly more time than others do. Thinking aloud protocols can provide information on when users gained important insights. However, a protocol usually contains no information about subconscious processes and may not be sufficient to answer this question. Interaction logs can help to see in which order users interacted with certain views. Additionally, eye movement data can aid in finding out what and where participants focused on (e.g., indicating indecision about the next step) before they proceeded with an interaction. As a result, a combined analysis of all these protocols can provide more detailed information about a participant's performance than just one method.</p><p>We apply a combined visual analysis of these different data categories to evaluate visualizations and VA systems. With our approach, we aim to support analysts in validating their hypotheses through pattern definition and automatic data analysis. These eye-interactionthinking patterns can be an indicator for cognitive processes during the sense making process <ref type="bibr" target="#b16">[17]</ref>. While Pirolli and Card <ref type="bibr" target="#b39">[40]</ref> describe sense making during intelligence analysis, their model is fit to describe many investigative or exploratory tasks with underspecified questions -at least on an abstract level. Our approach supports investigative tasks on evaluation data and shows some similarities to Pirolli and Card's model. VA approaches, with their various degrees of freedom, typically do not enforce a specific way of solving a task and often users can chose many different strategies to reach an analytic goal. As a consequence it is difficult to anticipate behavior of participants or to detect it immediately from combined evaluation data. Therefore, our approach embeds the foraging loop, covering subtasks for collecting, filtering, and preparing information. We collect and synchronize data, apply different filtering mechanisms, and offer various pattern generation options. In the sense making loop, an analyst can evaluate the data, build and test hypotheses, and derive conclusions from the data. Going through this process bottom-up supports a data-driven approach, performing it top-down is a hypothesis-driven procedure. Our approach supports both analysis processes. Hence, identification of patterns to derive common strategies of participants during the use of VA systems is the main focus of the presented approach. Thus, our work is a VA approach for evaluating VA applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Previous work discussed how visualization and VA <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b52">53]</ref> can be evaluated and how insight can be defined and measured <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b51">52]</ref>.</p><p>With our approach, we want to support analysts in discovering insights. We want to help them to test the flexibility of an approach by supporting the detection of different problem solving strategies <ref type="bibr" target="#b34">[35]</ref>. Approaches to explain user interaction with existing theories for human reasoning <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b41">42]</ref> can provide a foundation for one such evaluation. However, individual differences in VA applications still make it necessary to investigate them in detail.</p><p>Carpendale <ref type="bibr" target="#b7">[8]</ref> summarizes different quantitative and qualitative approaches for evaluating information visualizations. She argues that it takes a variety of methodologies to sufficiently analyze new approaches. Some challenges she and other researchers see <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b40">41]</ref> are that systems should be tested with real users, real tasks, and realistic datasets. Especially, using abstract user tasks during an evaluation is a challenge. Brehmer and Munzner <ref type="bibr" target="#b6">[7]</ref> define a multi-level typology for such abstract visualization tasks. Based on their typology a task should contain why a user executed it, what a task pertained to, and how a user executed a task. Thus, we base our approach on this concept and define one evaluation method for each of these three types. In our approach, an analyst can evaluate thinking aloud protocols to find out why a user executed a task. We evaluate interaction logs to investigate how a user executed a task and evaluate eye movement data to analyze what a task pertains to.</p><p>North <ref type="bibr" target="#b37">[38]</ref> proposes to define insights based on thinking aloud data. An analysis of thinking aloud data requires a classification of users' utterances into different categories. Saraiya et al. <ref type="bibr" target="#b44">[45]</ref> presented the most common categorization. The authors classify insights into: overview, pattern, group, or detail. Yi et al. <ref type="bibr" target="#b51">[52]</ref> adjusted this categorization into the categories: provide overview, adjust, detect pattern, and match mental model. Smuc et al. <ref type="bibr" target="#b46">[47]</ref> added insight categories for data and tool insights. Currently, our approach includes uncategorized thinking aloud data. We use thinking aloud information to verify hypotheses generated when using our approach rather than evaluating them explicitly.</p><p>Examples of interaction evaluation in VA systems are numerous. We discuss approaches that are similar to ours and depict differences. Lipford et al. <ref type="bibr" target="#b32">[33]</ref> and Dou et al. <ref type="bibr" target="#b13">[14]</ref> applied interaction logs and thinking aloud to discover reasoning processes from VA systems. Comparable to our approach, the authors visualize their data on parallel time lines. In contrast to our work, they only investigated single participants and did not include eye movement data. Moreover, they mention that they could not capture visual interests of a user and therefore missed relationships between data items even with thinking aloud data available.</p><p>Tory et al. <ref type="bibr" target="#b48">[49]</ref> applied eye tracking to multiple coordinated views. However, they did not compare multiple participant's eye-interactionthinking patterns. Crowe and Narayanan <ref type="bibr" target="#b12">[13]</ref> presented an approach called ETAS to show eye tracking and interaction logs from different interfaces. Although the authors applied a visually similar approach, they did not include thinking aloud data and did not support automatic pattern search to make comparisons between participants.</p><p>In general, our approach displays data on parallel timelines for individual areas of interest. Other authors <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b43">44]</ref> also utilized this approach to eye movement data. However, they only visualized eye movement data without its relation to different interactions and participants' verbalizations. Raschke et al. <ref type="bibr" target="#b43">[44]</ref> applied automatic comparison algorithms. In contrast, we provide an interactive visualization that combines thinking aloud, interaction, and eye movement data with an automatic data mining algorithm that performs comparisons on multivariate data.</p><p>To detect similar patterns in eye movement data, we use stringbased comparison algorithms that eyePatterns <ref type="bibr" target="#b49">[50]</ref> or ScanMatch <ref type="bibr" target="#b11">[12]</ref> also used. However, these applications only analyze eye movement data without interactions and depict results in an abstract, string-based representation that is hard to interpret. We integrated the underlying algorithms into a more intuitive visualization, which shows similarities and differences between participants better.</p><p>Our main contribution is a new VA approach for analyzing multivariate protocol data from multiple participants. In contrast to previous work, we combine an evaluation of three established evaluation <ref type="table">Table 1</ref>. The three main categories encode, manipulate, and introduce to classify interaction data in a visualization <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encode:</head><p>Initial encoding of data in a visualization. Manipulate: Modification of existing elements in a visualization. Select:</p><p>Demarcation of elements in a visualization. Navigate:</p><p>Alteration of the viewpoint. Arrange:</p><p>Spatial organization of elements in a view. Change:</p><p>Alteration of the visual encoding. Filter:</p><p>Exclude or include elements in the visualization. Aggregate:</p><p>Changing the granularity of elements in a visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduce:</head><p>Adding new elements to a visualization. Annotate:</p><p>Adding graphical or textual annotations to elements. Import:</p><p>Adding new elements to a visualization. Derive:</p><p>Compute new elements based on the existing elements of a visualization. Record:</p><p>Save or capture elements.</p><p>methods and support a data-driven as well as a hypothesis-driven analysis. With our approach, an analyst can perform exploratory search as well as automatic pattern analysis on data recorded from thinking aloud protocols, interaction logs, and eye movement data. This allows us to capture visual interests of users performing a task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DATA COLLECTION AND SYNCHRONIZATION</head><p>Our approach supports analysts in exploring and analyzing evaluation data from multiple data sources. We visualize thinking aloud, interaction, and eye movement data in relation to each other. The approach provides means to evaluate and verify hypotheses of participant behavior as well as to find and extract usage patterns between participant groups. We support this through hierarchical clustering and different automatic search methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Collection</head><p>Thinking aloud protocols, interaction logs, and eye movement data are three typical data sources for evaluating interactive visualization systems. We developed our approach keeping in mind that we want to synchronize these three data sources. For each category, we recorded a date and timestamp. Date specifies the system time, and timestamp indicates the elapsed time since a recording started. In our case, student assistants transcribed the audio recordings of thinking aloud data manually into written form. They assigned a timestamp to each statement to allow synchronization. An eye tracker records gaze points at a specific frequency. It then aggregates raw gaze points temporally and spatially into fixations. Fixations represent eye movements during which a participant focuses on a stimulus, whereas saccades are rapid eye movements between individual fixations. A sequence of fixations and saccades is a scanpath. Eye movement and interaction data has a screen position specifying where on a stimulus a participant looked or interacted.</p><p>We additionally assigned interaction categories to interaction data for a more holistic evaluation. Brehmer and Munzner <ref type="bibr" target="#b6">[7]</ref> defined eleven categories in their paper which <ref type="table">Table 1</ref> presents. We use these to map interaction data to categories. Furthermore, interaction data can have different discretizations: They are either continuous or discrete. A continuous interaction lasts for a certain time span whereas a discrete interaction happens instantaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data Synchronization</head><p>First, we have to synchronize all data collected during a study before we can analyze it. We use timestamps to synchronize thinking aloud, interaction, and eye movement data. We chose to visualize data on a sequential basis as well as on a temporal resolution, i.e., linearly ordered time <ref type="bibr" target="#b0">[1]</ref>. Showing sequences of eye movements and interaction data in combination independent of exact times allows an analyst to see patterns more easily.</p><p>The general procedure in our approach is to find all interactions taking place during a fixation. A fixation has a certain duration. Thus,  <ref type="figure">Fig. 2</ref>. We assign interactions to fixations depending on their timestamp (top). This can cause multiple interactions to be assigned to the same fixation. Thus, we created an overview visualization (middle) which represents fixations with their actual durations. An aggregated version of this visualization (bottom) may have duplicate fixations to represent all interactions. We add thinking aloud data on a separate timeline independent of the AOIs. They are synchronized with eye movement data based on their timestamp.</p><p>we use start and end timestamps of fixations to find all interactions having a timestamp in between them. For example, in <ref type="figure">Figure 2</ref> interaction I1 occurs during fixation F1.</p><p>However, this has some challenges, which have to be carefully considered. One problem when synchronizing interaction and eye movement data is that multiple interactions can take place during one fixation. For example, in <ref type="figure">Figure 2</ref> interactions I4 and I5 take place during fixation F8. When presenting the sequential order of data, we map each interaction to its corresponding eye movement. Thus, this may cause a duplication of eye movements in the visualization (cf. <ref type="figure">Figure 2</ref> bottom). To overcome the problem of data duplication and representing exact temporal data, we added a second visualization to our approach which displays time explicitly. A user can use this second visualization to get an overview of the data (cf. <ref type="figure">Figure 2</ref>, middle). Continuous interactions, i.e., scrolling, where multiple fixations may occur do not require a duplication. We represent those interactions with a connected line.</p><p>We use areas of interest (AOI), i.e., different views in a VA system, to relate eye movement and interaction data. In eye tracking research, each eye movement is assigned to one AOI. Applying semantic AOIs, performing a fixation clustering, or laying a grid on top of a view defines AOIs for one view or multiple views of a VA system <ref type="bibr" target="#b19">[20]</ref>. We chose AOIs to correlate fixations, as they contain a semantic meaning of a system. We additionally assign interaction data to AOIs. This supports an analyst in evaluating eye movements and interaction data together. The different visual states of the evaluated system are still available through recorded screen casts and can be replayed to inspect behavior of each test participant in detail.</p><p>The analysts have to define AOIs themselves. Our approach only supports rectangular AOIs and two points of a rectangle are enough to match eye movements and interaction data to these AOIs. We match fixations and interactions to AOIs by aligning the positions of fixations and the mouse pointer with the AOIs' covered areas.</p><p>A problem for relating eye movement and interaction data arises when defined AOIs do not fully cover the area of a stimulus. This can lead to periods in time when a participant is not looking at any AOI. However, if a participant is interacting within an AOI, we have an interaction without eye movement data. Adding an AOI for those areas on a stimulus not covered by other AOIs solves this issue. Nonetheless, we decided to specially mark these points in time rather than adding another AOI. <ref type="figure">Figure 2</ref> depicts this problem, where fixation F5 does not belong to an AOI. Yet, interaction I3 would happen during this fixation. Thus, we draw this interaction without a corresponding fixation on the AOI timeline.</p><p>Early work in analyzing eye and mouse data in correlation has shown that there is a tight connection between eye and mouse movements <ref type="bibr" target="#b10">[11]</ref>. Often the eyes move to an interesting region on a stimulus before the mouse follows for an interaction. Thus, synchronizing eye movements and interaction data requires considering this aspect. Since we map both interaction data and eye movements to AOIs, this is not a big problem. As eye movements within AOIs are usually longer, our approach maps interaction data correctly.</p><p>In our approach, a string represents eye movement and interaction data. Each AOI hit, i.e., matching of an eye movement or interaction to an AOI, is a part of this string. This string includes AOIs of eye movements, AOIs of interaction data, as well as interaction categories. This results in strings which are made up of 3-tuples (α, β , γ), where α, β are symbols of one alphabet and γ of a different alphabet. In each alphabet one symbol specifies areas on a stimulus which an AOI does not define. An example of such a tuple could be (1,2,A), where α and β are from the alphabet of integers and γ is from the alphabet of characters. Each integer would then represent one AOI and each character one interaction category. We use this string representation for generating regular expressions and calculating the Levenshtein distance (cf. Section 4.3).</p><p>We also synchronize thinking aloud data with interaction and eye movement data based on timestamps from audio recordings. Each verbal statement is assigned to a fixation where the timestamp lies within the fixations start and end time (cf. <ref type="figure">Figure 2</ref>). However, we do not assign verbalizations to AOIs. Rather, the visualization depicts them as additional information atop of AOIs. This approach does not consider that insights within thinking aloud protocols do not necessarily occur during or right after an eye movement or interaction <ref type="bibr" target="#b38">[39]</ref>. So far, we have not included this aspect in our approach. Since we solely use thinking aloud data to verify patterns, this has no negative effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">VISUAL DATA ANALYSIS</head><p>Our visual approach offers two timeline visualizations for verifying hypotheses and finding patterns visually. To further support an analyst, we provide different methods to search for patterns automatically. Additionally, we calculate similarities between participants and cluster them accordingly. An analysis in our approach can either be top-down, i.e., hypothesis-driven, or bottom-up, i.e., data driven.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Visual Data Representation</head><p>In order to compare eye movement and interaction data of participants our approach visualizes this data on an AOI Sequence Chart <ref type="bibr" target="#b19">[20]</ref>. For every participant our system depicts a separate AOI sequence chart where each AOI is shown as a separate timeline. This allows a comparison of individual participants. Each AOI hit, i.e., a match of a fixation or interaction to an AOI, is visualized on its corresponding timeline <ref type="bibr" target="#b19">[20]</ref>. We represent AOI hits for eye movements as well as interaction data interleaved. At the moment, we arrange AOIs vertically in the order they appeared. <ref type="figure">Figure 1 A)</ref> shows an AOI sequence chart. Gray rectangles connected by lines depict eye movements. Dotted lines display time spans where a participant focused on areas on a stimulus not included in any AOI. Colored circles depict interaction data. We mapped each interaction category to eleven separate colors retrieved from Color Brewer <ref type="bibr" target="#b17">[18]</ref> for qualitative data. This allows an analyst to see which interaction a participant used most often. In the example in <ref type="figure">Figure 3</ref>, interaction data shown as a purple circle are from the navigate category. Here, two interaction points are connected representing a continuous interaction. The blue circle representing an interaction from the select category is a discrete interaction and is not connected.</p><p>Combining eye movement and interaction data empowers an analyst to see which AOIs a participant interacted with and which AOIs a participant focused on during an interaction. For example, <ref type="figure">Figure 3</ref> shows an AOI sequence charts of two participants. In this example, participant 01 used a navigate interaction while looking at AOI 1, AOI 2, and again at AOI 1. Afterwards, the participant looked at AOI 3 and then back at AOI 1 selecting some information. An analyst could interpret this eye-interaction path as a participant searching for something in a VA system using a navigation interaction before selecting a found visual element. Representing AOI sequence charts for each participant individually reduces visual clutter and empowers an analyst to evaluate data of different participants separately. Nonetheless, one goal of our approach is to find common patterns between participants. Participant 02 at first glance seems to have a similar eye-interaction path. However, the participant first looked at AOI 2. To compare these two paths, we included scrolling, which allows an analyst to align two paths, cf. <ref type="figure">Figure 3</ref> on the right. The AOI sequence chart has a supplementary timeline at the top, which we labeled "Thinking Aloud", see <ref type="figure">Figure 3</ref>. Here, we added transcribed information from thinking aloud protocols. Thinking aloud data is shown independent of AOIs. An analyst can use this information to generate or verify hypotheses. Small speech bubbles mark utterances of a participant and they are aligned on the timeline based on timestamps of a participants' record. When a user hovers a speech bubble, detailed information about what a participant said is shown in a tooltip. Clicking on a speech bubble opens a separate window with a video of a participant started at the position of the selected timestamp. A representation with speech bubbles permits to see whether participants expressed their thoughts at a specific point in time or not. Tooltips are also shown on demand for eye movements and interaction data.</p><p>Although the order of eye movements and interaction data in the visualization depicts the temporal order of data, we are not able to see how long participants looked at specific AOIs. We encoded this information in a tooltip. However, for getting a visual overview this is not appropriate. Thus, we added a second Overview AOI Sequence Chart, which an analyst can use to retrieve this information. <ref type="figure">Figure 3</ref> at the bottom shows the same data visualized showing the complete data over time. This allows an analyst to evaluate specific parts in more detail. Additionally, an analyst gets an overview of how long each participant took to complete a task. For example, this overview shows that participant 01 was faster than participant 02.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pattern Editor</head><p>An analyst can use the AOI sequence chart to find patterns visually, for example, to compare different strategies of participants. However, if an analyst has found an interesting pattern in the data she might be interested in seeing all occurrences of this pattern in the data of all other participants. Doing this visually becomes a challenging task. Thus, our AOI sequence chart offers means for interacting with the displayed data. An analyst is able to select an interesting pattern in the visualization using a selection rectangle as shown in <ref type="figure">Figure 4</ref>. Our  <ref type="figure">Fig. 4</ref>. A blue selection rectangle shows a selection of a part in the AOI sequence chart (left side). This selection leads to an update of the pattern editor. An analyst may change the pattern editor and add wild cards (middle). This leads to a result, where parts in both timelines are highlighted after an analyst used a fuzzy search (right side).</p><p>approach automatically adds a selected pattern to the Pattern Editor <ref type="figure">(Figure 1 F)</ref> and uses it for searching. If our algorithm finds a search pattern, it highlights the pattern in all AOI sequence charts giving immediate feedback for whom and where the pattern is found.</p><p>The Pattern Editor depicts a stack of rectangles for each search element. Each rectangle represents one AOI of a search element, cf. <ref type="figure">Figure 4</ref> middle. As in the AOI sequence chart a gray rectangle depicts a selected AOI for eye movements. A color coded circle is shown for a selected AOI of interaction data. Using left and right mouse clicks an analyst is able to select a fixation or an interaction category respectively. If an analyst used a left mouse click, a context menu opens for a selection of an interaction category.</p><p>On the bottom of the pattern editor a table lists all found patterns. The first column of this table visually lists a search element, i.e., search string in <ref type="figure">Figure 1</ref> F. The second column lists all participants where a pattern is found. For each participant, a count shows how often a pattern is found for this participant, i.e., participant (count). The last column shows the sum over all participants' counts, i.e., overall count. Selecting a search element in the table highlights all appearances of it in the visualization allowing an analyst to see where a pattern is located. <ref type="figure">Figure 4</ref> shows how an analyst selected an interesting pattern in the visualization. The selected pattern is added to the pattern editor and an analyst changed the pattern to find more similar sequences. The analyst is interested in a pattern where participants first focused on AOI 1 starting to navigate in this AOI. During this navigation a participant can focus on different AOIs before selecting a visual element in AOI 1. This leads to a pattern where the user added a wild card between start and end of the navigation and a wild card between the end of the navigation and select interaction. This pattern retrieves a hit for both participant 01 and 02, which is then highlighted in the visualization. If a pattern occurs multiple times, it is highlighted each time the pattern is found. However, this might require an analyst to scroll through each AOI sequence chart separately to see all occurrences. To get a faster overview of where patterns are located we added a zoom functionality showing more or less detail. Additionally, an analyst can calculate an optimal zoom level showing the complete information at once. These options become especially useful when trying to detect all found patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Automatic Analysis</head><p>Our system offers different search algorithms. After an analyst created a search pattern in the pattern editor, she is able to first try to do an Exact Search. An exact search retrieves all occurrences of a pattern in all timelines of all selected participants. If an exact search pattern does not derive enough satisfying search results, a Fuzzy Search can be used. A fuzzy search does not return an exact pattern. Rather, it returns a pattern that is similar, for example, where a participant looked at another AOI in between. Adding a search element without specifying eye movement or interaction data creates a fuzzy search patterns with a wild card. In our example, an analyst would have to use a fuzzy search to find the pattern, since participant 01 focused on more AOIs than participant 02. This approach, selecting an interesting pattern in the AOI sequence chart and then using exact and fuzzy search, is one possible sequence when retrieving similar patterns in the data. This sequence corresponds more to a top-down approach.</p><p>If an analyst wants to do a data-driven bottom-up analysis, our approach offers alternative search options. One starting point is to calculate n-grams of participant strings. A n-gram retrieves all strings of a specified length n in a set of words. By calculating how often an n-gram is present for the same participant as well as in all participants, an analyst is able to find similar patterns. Using n-grams in our example would result in six 2-grams for participant 01 and 02. The table below the pattern editor displays all n-grams found. Selecting an n-gram highlights all appearances of it in the visualization.</p><p>A second option for a bottom-up analysis is to calculate All Common Substrings and the Longest Common Substring. This is slightly different from calculating n-grams as only patterns are retrieved that are present in all selected participants. We added the option to retrieve all common substrings, since any common substring is a possibility to search further rather than only looking at the longest common substring. The longest common substring is interesting as well to find the longest period in the data where participants behaved similarly.</p><p>A classical approach for comparing eye movement scanpaths is calculating the string edit distance or Levenshtein Distance <ref type="bibr" target="#b30">[31]</ref>. Privitera and Stark <ref type="bibr" target="#b42">[43]</ref> were the first to use Levenshtein distances to calculate similarities of scanpaths. Calculating the Levenshtein distances uses AOIs to represent scanpaths as strings. It computes the edit distance based on the minimum number of insert, delete, and substitute operations when transforming one string into another. If the number of transformations is small, scanpaths are more similar.</p><p>In the classical Levenshtein algorithm, transforming one string into another has cost functions: replace = 1, and delete and insert = 2. In our case we use different replace functions. If two symbols in two tuples are the same, a replace costs 1/3. If only one part is equal, the cost is 2/3, and if the complete string has to be replaced, the cost is 1. We chose this approach as we believe that when two participants are looking at the same AOI or using the same interaction in the same AOI, this is more similar than if they do completely different things.</p><p>Typically, calculating edit distances retrieves the best results if short sequences are compared. Thus, we added an option to constrain temporal information to a specified time span. For example, an analyst might be more interested in the beginning or end of a recording to find out if there are specific patterns during this selected time period. Thus, a user can select a time span which is used to calculate the Levenshtein distance and the AOI sequence chart only shows data during this time span.</p><p>Similar scanpaths are Clustered Hierarchically. Raschke et al. <ref type="bibr" target="#b43">[44]</ref> used such an approach for eye movement data, and we use the same approach to further simplify an analysis. We use a hierarchical agglomerative clustering approach. Our algorithm calculates the two most similar clusters based on Levenshtein distance. As a linking criterion of clusters we use single-linkage. The algorithm terminates if it reaches a pre-set dissimilarity or the maximal number of clusters.</p><p>A Dendrogram shows resulting clusters giving an overview about similar participants. Using the clusters, an analyst can evaluate participant groups of similar or different viewing behavior. <ref type="figure">Figure 1 (C)</ref> shows a list of participants after clustering. In this view, a user can select participants using a toggle button with a participant's identifier. The toggle button also contains a direct link to a screen-capture video recorded of participants during a study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">AOI List View</head><p>Additionally, our approach offers information about AOIs used in a study. <ref type="figure">Figure 1 (D)</ref> depicts the AOI List View. Names of AOIs are shown as well as options for changing points of the rectangle representing an AOI. Different histograms show information about frequencies of AOI visits and interactions as well as interaction categories. This allows an analyst to see which AOIs participants focused on and interacted with the most and how often each interaction category participants used. With this information an analyst may focus a pattern search on those AOIs, which participants used the most. Furthermore, we included a transition matrix for another possibility to start an analysis. A transition matrix displays the transition count, i.e., eye movement and interaction changes, between AOIs and allows an analyst to look for those AOIs, with a high transition count. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">USE CASE</head><p>In the following, we present one exemplary application of our approach on a user study for the VA system VarifocalReader <ref type="bibr" target="#b26">[27]</ref>. Our literature expert suggested real research tasks for our user study. We conducted a second user study with the VA system Word Cloud Explorer <ref type="bibr" target="#b18">[19]</ref> to show the generalizability of our approach. Due to space limitations, we only discuss some lessons learned from this second study's analysis in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">VarifocalReader</head><p>VarifocalReader offers a multi-layer visualization approach and supports analysts in exploring and understanding documents based on the inherent structure of a document, i.e., chapters, pages, and lines (layers). It provides several views to show different abstractions of a text, annotations appropriate to users needs, and results of search requests. A user can attach bar charts, pictograms, or word clouds to each layer as abstract views. Additionally, clicking on a word in a word cloud starts a search request for individual words. The hierarchical perspective on text documents and the navigation concept are based on the SmoothScroll approach <ref type="bibr" target="#b50">[51]</ref>. It enables an analyst to navigate through a visualization and to keep track of the current position across all layers. As previously mentioned in Section 3.1, we assign interaction data to categories and define our AOIs manually. We determined a separate AOI for each layer however, not for layer items (e.g., individual pages), as depicted in <ref type="figure" target="#fig_0">Figure 5</ref>. Furthermore, we assign possible interactions to the categories as defined by Brehmer and Munzner <ref type="bibr" target="#b6">[7]</ref>.</p><p>The show and hide method of our abstract views is a form of encode, because these tasks rely on how users transform data into a visual representation. In addition, we allocate a selection of word cloud terms, bar charts, or annotations to select, since those are based on the delimitation of visual elements and on differentiating selected from unselected visual elements. We assign the navigation concept including jump and scroll to the navigate category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Example User Study</head><p>For our user study, we prepared questions for two literary texts with the help of a literature expert. We use Emil Staiger's poetic "Grundbegriffe der Poetik" (Basic Terms of Poetics) and the English version of Homer's "Iliad". For both texts we designed a simple, a moderate, and a difficult task. Questions and goals of our study were manifold, since our literary scholars are interested in answering rather abstract questions. Challenging tasks are how authors characterize poets or introduce and discuss specific topics. For example, investigative questions are if Schiller's work "Wallenstein" is a typical example of dramatic poetry and which other works are referenced in this context.</p><p>The objective of our user study was to get insights about how analysts navigate with the hierarchical visualization. We investigated how users use visual abstractions as well as how helpful they are at the various layers and for different analysis tasks. Our intention was to find benefits and weaknesses of VarifocalReader for further development.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Environmental Conditions and Technical Setup</head><p>We performed the study in a laboratory isolated from external distractions. We recorded a screen capture with thinking aloud as a video. After recording, our student assistants transcribed the thinking aloud data into written form. The VarifocalReader itself logged interaction data and saved it as an MS Excel file. Interaction data files include information about date, timestamp, mouse coordinates, name and boundary shape of AOIs, event (e.g., click on a word cloud term), and description (e.g., a selected term in a word cloud, as depicted in <ref type="figure" target="#fig_0">Figure 5</ref> (B)). A Tobii T60 XL eye tracker recorded eye movement data with a sampling rate of 60 Hz, a 24-inch screen, and a resolution of 1920 × 1200 pixels. The Tobii Fixation Filter aggregated eye movement data (velocity threshold = 35 pixels/samples and distance threshold = 35 pixels) as Tobii <ref type="bibr" target="#b47">[48]</ref> recommended. To calibrate the Tobii T60 XL eye tracker we conducted a nine-point calibration. We exported raw data as tab-separated value (TSV) files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Participants</head><p>We conducted the study with 16 participants (twelve female, four male). The average age of participants was 23.5 years (min = 21, max = 33). Eleven of the participants were students from the humanities and five studied computational linguistics. Each participant successfully performed an Ishihara test and a Snellen chart to confirm that they were physically able to accomplish given tasks. The study took about 60 min, depending on the speed of participants. We compensated all participants with EUR 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Procedure</head><p>First participants had to fill out a questionnaire about their domestic information. Then, we gave participants a brief introduction to the VarifocalReader. Afterwards, each participant had to solve a simple, a moderate, and a difficult tasks and we switched the text after each task. The order of texts was counter-balanced between participants. Between tasks, we enforced short breaks to maintain concentration. Before each task, we performed a re-calibration of the eye tracker. After the last task, participants answered a system usability scale about the VarifocalReader approach and had the chance to give final remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysis of the Example User Study</head><p>Our approach supports analysts in exploring and analyzing collected data and permits a bottom-up as well as top-down processing of data. In the following, we present two use cases that demonstrate suitability of our approach for each analysis task. First, we illustrate that our analyst can find common patterns in order to generate hypotheses bottom-up using various methods. Second, we introduce a hypothesis-driven top-down analysis, based on the knowledge and experience of our literature expert.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bottom-Up Analysis</head><p>As a first step, the analyst explores and analyzes the overview AOI sequence chart. In addition, she considers the histograms in the AOI List View (cf. <ref type="figure">Figure 6 (A)</ref>). She notes that participants mainly focused on the subchapter and text AOI. Analyzing the interaction data histogram she finds out that most participants carried out their interactions on the chapter, subchapter, and page AOI. The histograms support her hypothesis that participants rarely used paragraphs since these text passages are too short for a deeper analysis. In the interaction category histogram she detects that participants used the navigate interaction the most. Thus, she focuses on this type of interaction category during the rest of her analysis.</p><p>As a next step, she switches to the normal AOI sequence chart and activates the clustering to find similar participants, as depicted in <ref type="figure">Figure 6</ref> (B). Our analyst uses the time span filter because she is especially interested in the start and end of the recorded data. She suspects that at the beginning and at the end of a task similar patterns are present. The hierarchically clustered dendrogram view helps her to get an initial overview of similar participants. She observes that the dendrogram clusters participant 10 and 14 together and she wants to find common patterns of both. Accordingly, she activates the <ref type="figure">Fig. 6</ref>. The histogram shows an analyst that participants focused on and interacted with chapters, subchapters, and text the most using mostly a navigate interaction (A). Hierarchically clustered dendrogram (B). Searching for the longest common substring (C) leads to an AOI sequence chart of two similar participants with highlighted common patterns (D). <ref type="figure">Fig. 7</ref>. The AOI sequence chart displays an overview of several participants after an n-gram selection. They all switched their focus from the subchapter AOI to the text AOI while using a navigate interaction on the chapter AOI.</p><p>longest common substring method and investigates common patterns, as shown in <ref type="figure">Figure 6</ref> (C). She finds several interesting common patterns, which could be a suitable starting point for a further analysis. Then, our analyst decides to select the longest common substring to further analyze these two participants. This highlights the pattern in the AOI sequence chart, as depicted in <ref type="figure">Figure 6</ref> (D). This pattern shows that both participants first navigated and focused on the page AOI and after a while moved their focus to the text area staying on the page area to navigate further.</p><p>In the following, she is interested whether other participants proceeded in a similar manner. To search for this pattern in the data of other participants, she tries an exact search. After our analyst detects that no one else has this exact pattern, she uses a fuzzy search and identifies that participant 09 has navigated in a similar way. To find out how others navigated, she searches for all 3-to 5-grams. When inspecting the search results, she realizes that many other participants interacted with VarifocalReader in a similar way. Most participants navigated on a higher layer (e.g., chapters) while moving their focus between the chapter or subchapter AOI and the text AOI, as depicted in <ref type="figure">Figure 7</ref>. This analysis confirms her previous assumption that especially at the end of a task similar patterns are present.</p><p>The data-driven analysis showed that our approach offers several methods to find common patterns in order to get insights and generate hypotheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Top-Down Analysis</head><p>To show suitability of our approach for analyzing data using a topdown process, we present an example based on a sample solution our literature expert (participant 00) performed with VarifocalReader.</p><p>Our analyst begins to verify her hypotheses where she suspects that each participant started to solve the task with an encode interaction followed by a select interaction on a chapter or subchapter AOI. To confirm this assumption, she makes various search requests using exact and fuzzy searches, as depicted in <ref type="figure">Figure 8 (A)</ref>. She determines, that all participants used this pattern at the beginning of their analysis, as represented in <ref type="figure">Figure 8 (B)</ref>. In addition, she has the hypothesis that a group of participants who took longer for the task have not used the VarifocalReader approach in an optimal way. To examine this hypothesis, she activates the overview AOI sequence chart to get an initial overview, as shown in <ref type="figure">Figure 8</ref> (C). She quickly identifies those participants who took longer than the expert participant. By exploring this group of participants, she finds out that they have fewer interactions than the rest. However, many and long eye movements on the text AOI, as depicted in the red rectangle in <ref type="figure">Figure 8 (C)</ref>, shows that they excessively read the text. Yet, she is surprised that participant 05 and 14 have a shorter completion time than our literature expert. She generates the assumption that they potentially chose a different starting point to solve the task. To verify her hypothesis, she begins to hover over the thinking aloud speech bubbles. Our analyst identifies that both participants started to search for another keyword of the question than our literature expert. She verifies this by looking at the first selection interaction on the timeline and the video of the different participants, as <ref type="figure">Figure 8</ref> (D) depicts. This procedure also led to the right solution, taking less time, albeit with a lucky guess.</p><p>The analysis illustrated that our approach supports analysts in confirming hypotheses and gaining insights through a hypothesis-driven procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERT REVIEW</head><p>Evaluating a new visualization approach is an important part in the visualization design life cycle. Nonetheless, evaluation can be time consuming and challenging, specifically if dealing with sophisticated, highly interactive approaches that do not enforce a stringent path to follow. We aim at supporting analysts in complex sense making tasks (similar to those Pirolli and Card <ref type="bibr" target="#b39">[40]</ref> described). We refrained from evaluating our evaluation approach with itself, since this would certainly hide potential issues and defects of our approach. Instead, we chose to evaluate our approach using an expert review with a heuristic evaluation. Heuristic evaluation is a low-cost and informal method involving experts judging the usability of a visualization system <ref type="bibr" target="#b36">[37]</ref>. Forsell and Johansson <ref type="bibr" target="#b15">[16]</ref> extracted ten heuristics that cover most usability problems in visualization. Therefore, in our expert review, we use those ten heuristics to analyze our approach.</p><p>We asked the experts to use these heuristics to inspect our approach while solving a typical analysis task. Their task was to find patterns in the data from the VarifocalReader study described in Section 5. We had three experts evaluating our approach. All experts have worked with eye tracking before. Two experts were from the visualization group and one from the human-computer interaction group of our institute.</p><p>Each expert analyzed our approach in two passes. The first pass was to familiarize themselves with our system. The second pass was to find patterns and judge our method using the ten heuristics. We encouraged the experts to note all details they encountered while using our approach and classify problems based on the heuristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Findings</head><p>Overall, all experts liked our approach and could use it without major problems. They noted some smaller usability issues. Expert 1 (E1) especially noted that the histograms were a good starting point to get an overview of the data and find outliers. He also stated that the dendrogram was useful for determining which participants were similar. Expert 2 (E2) found the n-grams useful. However, he also questioned scalability of the AOI sequence chart if more than ten AOIs are present. Additionally, he mentioned that our system should support an analyst more in finding interesting patterns (E2). He noted that this would require a definition of typical and generalized tasks to search for in eye-interaction data.</p><p>In the following, we discuss statements of the experts in relation to the ten heuristics.</p><p>Information coding: Our experts noted that coding of information was adequate, increment of details was chosen well (E1), and <ref type="figure">Fig. 8</ref>. To analyze data top-down an analyst first looks at all patterns where a select follows an encode (A). This retrieves results for all participants (B). If comparing completion times of all participants one notices that participant 05 and 14 were faster than the expert (participant 00) (c). Inspecting their thinking aloud protocols shows that they started their task with a different keyword (D).</p><p>mapping of data was plausible (E1, E2). Each data type has a clearly distinctive symbol and color coding was acceptable (E1, E2). Furthermore, representing eye movements in the background of interactions makes an analysis easier (E2). The coding of information leads to a good overview of data and a reduction of visual clutter (E3). However, the experts also mentioned some points for improvement. Although the overview AOI sequence chart shows temporal information of data, it could represent time more explicitly (E1, E2). Also some context information was missing (e.g., a reference to the stimulus may be helpful) (E2). Furthermore, highlighting which participant contains a search pattern could be helpful in faster recognizing them (E2). Global information about data, such as how long a participant needed to solve a task, or histograms for individual participants could be added, too (E2).</p><p>Minimal actions: The experts noted that the number of actions necessary to accomplish a task was good. There were no excessive actions (E1), offered functions were easily accessible (E3), and there were only a few dialogs, i.e., for the transition matrix and the video (E2, E3). The experts mentioned some points for improvement, for example, a history of all actions or evaluation provenance, to identify which patterns an analyst has found. Our experts also suggested to add additional options to create participant groups as well. Furthermore, recognizing a pattern in the visualization requires to zoom, which adds an extra action to this task.</p><p>Flexibility: Our experts appreciated the flexible use of our approach. It offers multiple ways of achieving a goal, without overwhelming the experts with many possibilities for interaction (E1). Possibilities for finding common patterns were extensive (E1), and the visualization was flexible and useful to accomplish a task (E2, E3).</p><p>Orientation and help: Controlling levels of detail, i.e., granularity of data through multiple interaction options, was chosen well (E2, E3). Different interactions allow panning and zooming to navigate through presented data (E1). Except for the pattern editor undo and redo functionalities were not necessary (E3).</p><p>Spatial organization: Spatially, our approach was organized and structured well leading to a clearly arranged system, which an analyst can efficiently use (E1, E2). Nonetheless, we could improve some parts. For example, in the AOI sequence chart, it was not clearly visible where a search pattern was located exactly (E1), here a mini map could be useful to mark found patterns. As an enhancement the system could integrate a video player instead of opening it in a separate window (E2, E3).</p><p>Consistency: All experts liked the consistency. Design, interaction with data, as well as use of visual elements and symbols were consistently used throughout our approach.</p><p>Recognition rather than recall: Altogether our experts considered the cognitive strain to be low, an analyst had to remember almost nothing. The legend was helpful in looking up color codes of interaction categories (E1). Yet, eleven colors were hard to remember and experts had to consult the legend from time to time (E2). A clear differentiation of eye movements as rectangles, interaction data as circles, and use of speech bubbles for thinking aloud data further reduced memorization of information (E1, E3). An advancement could be a visual encoding of participants for a better distinction (e.g., assigning unique visual fingerprints to each) (E2).</p><p>Prompting: Our approach does not offer many decision points and usually only one action was necessary to solve a task (e.g., finding all common strings) (E1). Thus, the experts mentioned that they did not get lost in our system (E2, E3).</p><p>Remove the extraneous: Different options exist to add and remove parts of data. An analyst can select participants as well as show or hide eye movement and interaction data (E1, E3). Nevertheless, thinking aloud data cannot be hidden (E1, E3).</p><p>Data set reduction: Experts thought, that we reduced data to a minimal amount of information required for an analysis (E2). Selecting and deselecting participants allows an analyst to filter (E3), and hovering over objects shows detailed information in tooltips. Additionally, we could show extra information at positions with whitespace (E2). However, we could summarize some data even more, for example, dotted lines connecting fixations in the same AOIs (E1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION AND FUTURE WORK</head><p>So far, we included thinking aloud data to depict participants' utterances by transcribing what they said without annotating or further analyzing it. One approach for annotating thinking aloud data is to use the categories presented by Saraiya <ref type="bibr" target="#b44">[45]</ref>, Yi <ref type="bibr" target="#b51">[52]</ref>, or Smuc <ref type="bibr" target="#b46">[47]</ref>. This would allow an evaluation of a VA system using insights participants gained. Analyzing insights together with eye movements and interaction data requires a synchronization beyond timestamps. We chose to synchronize the data through timestamps as a first step of our implementation. Nevertheless, a synchronization and visu-alization based on time intervals could improve this representation. The overview timeline temporally depicts only fixations with their real time intervals. In future work, we want to investigate how the visualization could represent thinking aloud and interaction data with their actual times. Although the eye-mind hypothesis <ref type="bibr" target="#b22">[23]</ref> states that participants focus on what they think about, insights can occur even after a while <ref type="bibr" target="#b38">[39]</ref>. Thus, synchronizing insights requires reflecting together with participants which actions led to an insight and label them accordingly <ref type="bibr" target="#b4">[5]</ref>. Additionally, we have to integrate a distinction of thinking aloud categories into the thinking aloud symbol. With an addition of insights we are able to define higher level patterns for analyzing behavior.</p><p>Our approach focuses on finding patterns in thinking aloud, interaction, and eye movement data. We present data in a sequential order to support an analyst in finding interesting patterns. Nevertheless, there are some possibilities for including more temporal aspects for a visual display of data. In the AOI sequence chart we could vary the size of rectangles for eye movements to show temporal aspects. Additionally, our approach so far includes discrete and continuous interactions. However, in some VA systems we also have to consider cases in which a user starts interactions at a specific point in time and the system does not show the result of the interaction until some later time. One possibility is to mark the start and end of these interactions and connect them with a dotted line to distinguish them from continuous interactions.</p><p>A further challenge in our approach is the dependency of interactions on AOIs. In VA systems interactions may also lead to changes in different views. For example, using brushing-and-linking techniques leads to changes in multiple views. Representing interaction data in association with an AOI where an interaction took place might not be sufficient. To analyze if participants were able to see which views an interaction influenced, the visualization has to mark all AOIs. However, a different identification should be used. For example, circles could have a different shape, i.e., a dotted circle outline or different color saturation. A challenge with this is to find out which interactions influence AOIs. Thus, an analyst has to know semantic information about AOIs. Yet, we act on the assumption that an analyst also has deeper knowledge about the VA system evaluated to define this semantic mapping of AOIs.</p><p>In our presented use case AOIs were static and not changing. Yet, in eye tracking research AOIs can also change or move. For example, a user might add, remove, or relocate a view. Handling dynamic AOIs, i.e., movable views or windows, has been a challenge in eye tracking research, and some approaches exist in dealing with them <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b28">29]</ref>. Dynamic AOIs may also lead to a large number of AOIs or to AOIs which are empty for a long period. If AOIs are empty for long times, an automatic time selection might be an option and if AOIs are not present, the visualization could fold their timeline together. In VA systems, AOIs might also be associated with a focus-and-context technique or a magic lens. This leads to further challenges, since a magic lens can be seen as an interaction and an AOI, for example, if context information is represented next to a lens. This also requires additional semantic information of AOIs.</p><p>With our approach, we aimed to address scalability on several levels. We tried to achieve a visually scalable solution by supporting zooming, scrolling, filtering, and aligning data in the visualization. However, in the shown examples we deal with a small number of AOIs. There can be situations where this number is higher. This would require more vertical space per participant, potentially hampering a comparison of different participants. A potential solution to such a problem could be a hierarchical arrangement of AOIs and providing means to collapse or expand parts of this hierarchy according to an evaluator's needs. Additionally, being able to rearrange AOIs in our evaluation approach would be a useful addition if too many AOIs are present and if the order of AOIs is not as clearly represented as in the case of the VarifocalReader. For example, in the VarifocalReader use case, an analyst may want to reorganize AOIs to have chapter, subchapter, and text AOIs next to each other, since most transitions are between these AOIs. In the case of the Word Cloud Explorer, order of AOIs is not obvious. It would require a calculation of an optimal ordering, (e.g., using the transition count of AOIs). Another possibility to assure scalability would be to use a matrix approach rather than separate timelines. A matrix has an advantage of representing data in a compact way. Yet, following transitions visually becomes more challenging.</p><p>So far, our approach still requires a lot of scrolling even for only short sequences and few participants. Some ideas on supporting analysts in finding patterns and thus reducing scrolling efforts would be to include more automatic sequence alignment techniques. For example, we could implement more algorithms for comparing sequences (e.g., Needleman-Wunsch <ref type="bibr" target="#b35">[36]</ref> or BLAST <ref type="bibr" target="#b1">[2]</ref>). There has also been some research in using Markov models <ref type="bibr" target="#b31">[32]</ref> or support vector machines <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b33">34]</ref> in eye tracking.</p><p>Merely analyzing interactions to understand an intent of an interaction and thus assess an interactive visualization is not sufficient <ref type="bibr" target="#b38">[39]</ref>. User goals and tasks should be part of an evaluation which requires to look at why a user executed a task, what the concern of a task is together with how a user executed it. Defining abstract tasks in the way Brehmer and Munzner <ref type="bibr" target="#b6">[7]</ref> did allows to analyze data on a higher level. We have information about why, how, and what tasks a user solved. Thinking aloud data defines why, interaction data how, and eye movement data what. Going one step further with our approach would be to suggest and search for such tasks. This would require a way of generating such tasks in the pattern editor. At the moment, the pattern editor only allows to define patterns based on AOIs, sequences, and interaction categories. For defining abstract tasks, an analyst should be able to define patterns independent of AOIs.</p><p>Our approach mainly aims at analyzing VA systems that support exploratory analyses. In addition to applying our evaluation method to the VarifocalReader, we conducted a short proof of concept with a second use case. We recorded data from participants while using the Word Cloud Explorer <ref type="bibr" target="#b18">[19]</ref>. The authors used a word cloud visualization method for the Word Cloud Explorer and provided several interactive text analysis features. Its intention is to analyze text or documents. Altogether, we define seven AOIs and interactions from the categories select, filter, change, and navigate. Most of the focus and interaction lay on the central word cloud. Users employed other interactions to manipulate the word cloud or get specific details about a document. We were able to apply our approach to data recorded for the Word Cloud Explorer without any adaptations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>Our primary contribution is a VA approach for evaluating VA applications in which we combine thinking aloud, interaction, and eye movement data. This combined analysis differs from traditional evaluations where an analyst evaluates only one data type at a time. A combination of these three data sources allows us to find new insights about the effectiveness of a typical VA application for Digital Humanities. For example, some users did not use the navigation much and read the whole text instead. Our approach enables an analyst to identify and search for patterns to verify hypotheses. The AOI sequence chart allows to search for patterns, depict these patterns in the visualization, and compare participants and participant groups with each other. An analyst has different options for finding patterns (e.g., searching for n-grams, finding the longest common substring, or searching for exact patterns). We presented the applicability of our approach with a use case from the VarifocalReader study. We discussed how our approach applies to other VA systems in a second use case using the Word Cloud Explorer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 5 .</head><label>5</label><figDesc>Homer's "Iliad" divided into layers showing chapters (A), subchapters (B) (with word clouds), pages (C) (with bar charts), paragraphs (D) (with bar charts and pictograms), and lines of text (E).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>•</head><label></label><figDesc>Kuno Kurzhals is with the Visualization Research Center, University of Stuttgart, Germany. E-mail: kuno.kurzhals@visus.uni-stuttgart.de.</figDesc><table /><note>• All other authors are with the Institute for Visualization and Interactive Systems (VIS), University of Stuttgart, Germany. E-mail: firstname.lastname@vis.uni-stuttgart.de. Manuscript received 31 Mar. 2015; accepted 1 Aug. 2015; date of publication xx Aug. 2015; date of current version 25 Oct. 2015. For information on obtaining reprints of this article, please send e-mail to: tvcg@computer.org.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Manuscript received 31 Mar. 2015; accepted 1 Aug. 2015; date of publication 20 Aug. 2015; date of current version 25 Oct. 2015. For information on obtaining reprints of this article, please send e-mail to: tvcg@computer.org. Digital Object Identifier no. 10.1109/TVCG.2015.2467871</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>The left side shows two AOI sequence charts for two participants. Interaction data represented in purple are from the navigate category and interaction in blue is from the encode category. Both timelines appear similar. Using the scroll functionality, an analyst can align the timelines to better show their similarity (right side). The overview timeline (bottom) shows exact durations of fixations within AOIs. The participants stayed for different durations inside each AOI.</figDesc><table><row><cell>Participant_ID_01</cell><cell>Participant_ID_01</cell></row><row><cell>Thinking Aloud</cell><cell>Thinking Aloud</cell></row><row><cell>AOI 1</cell><cell>AOI 1</cell></row><row><cell>AOI 2</cell><cell>AOI 2</cell></row><row><cell>AOI 3</cell><cell>AOI 3</cell></row><row><cell>Participant_ID_02</cell><cell>Participant_ID_02</cell></row><row><cell>Thinking Aloud</cell><cell>Thinking Aloud</cell></row><row><cell>AOI 1</cell><cell>AOI 1</cell></row><row><cell>AOI 2</cell><cell>AOI 2</cell></row><row><cell>AOI 3</cell><cell>AOI 3</cell></row><row><cell>Participant_ID_01</cell><cell></cell></row><row><cell>Thinking Aloud</cell><cell></cell></row><row><cell>AOI 1</cell><cell></cell></row><row><cell>AOI 2</cell><cell></cell></row><row><cell>AOI 3</cell><cell></cell></row><row><cell>Participant_ID_02</cell><cell></cell></row><row><cell>Thinking Aloud</cell><cell></cell></row><row><cell>AOI 1</cell><cell></cell></row><row><cell>AOI 2</cell><cell></cell></row><row><cell>AOI 3</cell><cell></cell></row><row><cell>Fig. 3.</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work has been partially funded by the German Federal Ministry of Education and Research (BMBF) as part of the 'ePoetics' project and the German Science Foundation (DFG) as part of the priority program (SPP) 1335 'Scalable Visual Analytics'.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Visualization of Time-Oriented Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Aigner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Miksch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tominski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Basic local alignment search tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Altschul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Molecular Biology</title>
		<imprint>
			<biblScope unit="volume">215</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="403" to="410" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Evaluating information visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Andrews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BELIV Workshop</title>
		<meeting>BELIV Workshop</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Visual analytics methodology for eye movement studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">V</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2889" to="2898" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards analyzing eye tracking data for evaluating interactive visualization systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blascheck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BELIV Workshop</title>
		<meeting>BELIV Workshop</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="70" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">State-of-the-Art of Visualization for Eye Tracking Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blascheck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurzhals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raschke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Euro-Vis -STARs</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="63" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A multi-level typology of abstract visualization tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brehmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munzner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2376" to="2385" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Evaluating information visualizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carpendale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Visualization</title>
		<editor>A. Kerren, J. T. Stasko, J.-D. Fekete, and C. North</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">4950</biblScope>
			<biblScope unit="page" from="19" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Defining insight for visual analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ziemkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ribarsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. CG&amp;A</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="14" to="17" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Empirical studies of information visualization: a meta-analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Studies</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="851" to="866" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">What can a mouse cursor tell us more?: Correlation of eye/mouse movements on web browsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI EA on Human Factors in Computing Systems</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="281" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ScanMatch: A novel method for comparing fixation sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cristino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Matht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Theeuwes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gilchrist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="692" to="700" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Comparing interfaces based on what users watch and do</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Crowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">H</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Symposium ETRA</title>
		<meeting>Symposium ETRA</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="29" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recovering reasoning processes from user interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stukes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ribarsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. CG&amp;A</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="52" to="61" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Protocol Analysis: Verbal Reports as Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Ericsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Simon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An heuristic set for evaluation in information visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Forsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johansson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Int. Conf. on AVI</title>
		<meeting>the Int. Conf. on AVI</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="199" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visual analytics for complex concepts using a human cognition model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ribarsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on VAST</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Colorbrewer.org: An online tool for selecting colour schemes for maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harrower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Brewer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Cartographic Journal</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="37" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Word cloud explorer: Text analytics based on word clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Heimerl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lohmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HICSS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1833" to="1842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Holmqvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nyström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Andersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dewhurst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jarodzka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<title level="m">Eye Tracking: A Comprehensive Guide to Methods and Measures</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cognitive modelling of a ship navigator based on protocol and eye-movement analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Itoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Le Travail Humain</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="127" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The state of the art in automating usability evaluation of user interfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Ivory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comp. S</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="470" to="516" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A theory of reading: From eye fixations to comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Just</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Carpenter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="329" to="354" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Evaluating visual analytics systems for investigative analysis: Deriving design principles from a case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gorg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on VAST</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="139" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Visual analytics: Definition, process, and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-D</forename><surname>Fekete</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Görg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kohlhammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Melançon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Using eye movements to recognize activities on cartographic maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kiefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Giannopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raubal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM SIGSPATIAL Int. Conf. on Advances in Geographic Information Systems</title>
		<meeting>the 21st ACM SIGSPATIAL Int. Conf. on Advances in Geographic Information Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="488" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Varifocalreader-indepth visual analysis of large text documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Worner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1723" to="1732" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Evaluating visual analytics with eye tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurzhals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BELIV Workshop</title>
		<meeting>BELIV Workshop</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="61" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">ISeeCube: Visual analysis of gaze data for video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurzhals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Heimerl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Symposium ETRA</title>
		<meeting>Symposium ETRA</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Research methods in humancomputer interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lazar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hochheiser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Binary codes capable of correcting deletions, insertions, and reversals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Levenshtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soviet Physics-Doklady</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="707" to="710" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning eye movement pattens for characterization of perceptual expertise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pelz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ovesdotter Alm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Haake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Symposium ETRA</title>
		<meeting>Symposium ETRA</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="393" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Helping users recall their reasoning process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stukes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on VAST</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A regression-based method for the prediction of the indecisiveness degree through eye movement patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lufimpu-Luviya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Merad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Drai-Zerbib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baccino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fertil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conference on ETSA</title>
		<meeting>Conference on ETSA</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="32" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Many roads lead to rome: Mapping users&apos; problem solving strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Smuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Risku</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BELIV Workshop</title>
		<meeting>BELIV Workshop</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="8" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A general method applicable to the search for similarities in the amino acid sequence of two proteins</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Needleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Wunsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Molecular Biology</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="443" to="453" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Usability inspection methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference Companion on Human Factors in Computing Systems</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="413" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Toward measuring visualization insight</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>North</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. CG&amp;A</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="6" to="9" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The science of interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Pike</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>O'connell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Visualization</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="263" to="274" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The sensemaking process and leverage points for analyst technology as identified through cognitive task analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pirolli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Card</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Int. Conf. on IA</title>
		<meeting>of Int. Conf. on IA</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="2" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The challenge of information visualization evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Plaisant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Working Conference on AVI</title>
		<meeting>of the Working Conference on AVI</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="109" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The user puzzle -explaining the interaction with visual analytics systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Smuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mayr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2908" to="2916" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Algorithms for defining visual regions-ofinterest: Comparison with eye fixations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Privitera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Stark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PA and MI</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="970" to="982" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A visual approach for scan path comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raschke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Herr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blascheck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schrauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Willmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Symposium ETRA</title>
		<meeting>Symposium ETRA</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="135" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">An insight-based methodology for evaluating bioinformatics visualizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Saraiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>North</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="443" to="456" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Strategies for evaluating information visualization tools: Multi-dimensional in-depth long-term case studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Plaisant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AVI Workshop on BEyond Time and Errors</title>
		<meeting>AVI Workshop on BEyond Time and Errors</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">To score or not to score? tripling insights for participatory design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Smuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lammarsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Aigner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Miksch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gartner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. CG&amp;A</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="29" to="38" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tobii</surname></persName>
		</author>
		<ptr target="http://www.tobii.com/en/eye-tracking-research/global/library/manuals" />
		<imprint>
			<date type="published" when="2015-03-27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Eyegaze analysis of displays with combined 2d and 3d views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Atkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-Z</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="519" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">eyePatterns: Software for identifying patterns and similarities across fixation sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Haake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Rozanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Karn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Symposium ETRA</title>
		<meeting>Symposium ETRA</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="149" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Smoothscroll: A multi-scale, multi-layer slider. Computer Vision, Imaging and Computer Graphics -Theory and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wörner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">274</biblScope>
			<biblScope unit="page" from="142" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Understanding and characterizing insights: How do people gain insights using information visualization?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Stasko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Jacko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BELIV Workshop</title>
		<meeting>BELIV Workshop</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Understanding visualization by understanding individual users</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ziemkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ottley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Crouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chauncey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. CG&amp;A</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="88" to="94" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
