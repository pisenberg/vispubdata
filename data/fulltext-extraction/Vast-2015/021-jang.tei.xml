<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MotionFlow: Visual Abstraction and Aggregation of Sequential Patterns in Human Motion Tracking Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujin</forename><surname>Jang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niklas</forename><surname>Elmqvist</surname></persName>
						</author>
						<title level="a" type="main">MotionFlow: Visual Abstraction and Aggregation of Sequential Patterns in Human Motion Tracking Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TVCG.2015.2468292</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Human motion visualization</term>
					<term>interactive clustering</term>
					<term>motion tracking data</term>
					<term>expert reviews</term>
					<term>user study</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Fig. 1. MotionFlow for pattern analysis of human motion data. (a) Pose tree: a simplified representation of multiple motion sequences aggregating the same transitions into a tree diagram. (b) A window dedicated to show a subtree structure based on a query. (c) Space-filling treemap [32] representation of the motion sequence data using slice-and-dice layout. (d) Node-link diagram of pose clusters (nodes) and transitions (links) between them. This view supports interactive partition-based pose clustering. (e) Multi-tab interface for storing unique motion patterns. (f) Animations of single or multiple selected human motions.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recent development of reliable and low-cost sensing technologies now enable accurate and convenient acquisition of human motion tracking data. Human motion tracking data typically consists of temporal sequences of human poses that are defined by a set of 3D joint positions (e.g., head, hands, elbows, and knees). These data support researchers in performing detailed and comprehensive analysis of a vari- ety of complex human motions. In particular, analyzing common and irregular motion patterns across subjects and environments can help researchers better understand human behavior in real-world situations. For example, comparative analysis of motion patterns may indicate differences between novice and professional dancers <ref type="bibr" target="#b1">[2]</ref>. In learning environments, studying gesture patterns of students can lead to deeper understanding of the role of gestures in the learning process <ref type="bibr" target="#b6">[7]</ref>. Similarly, the analysis of repetitive and irregular motion patterns can be used in a rehabilitation process to diagnose and correct patients' undesired movement <ref type="bibr" target="#b45">[46]</ref>. This style of analysis requires understanding motion sequence data, identifying main trends, and detecting anomalies by properly aggregating similar sequences into pattern groups.</p><p>However, such analysis is challenging for many reasons: (1) human motion data is multivariate with an almost infinite range of spatial and temporal variations; (2) motion sequences commonly consist of hundreds or thousands of consecutive frames; and (3) different individuals often exhibit small variations in their motions representing the same general patterns. While visual analytics has previously been applied to this problem-e.g. MotionExplorer <ref type="bibr" target="#b4">[5]</ref> and GestureAnalyzer <ref type="bibr" target="#b17">[18]</ref>these solutions are not sufficiently flexible, scalable, and visually compact to fully support motion pattern analysis.</p><p>To meet this need, we designed MotionFlow, an interactive visual analytics system that supports pattern analysis of sequential motion data. Compared to state-of-the-art systems <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18]</ref>, it supports efficient but effective visualization of multiple motion patterns, and reflects the context of data and human perception of pose similarity into pattern analysis. MotionFlow defines each motion sequence as a series of transitions between pose states. Then, it combines the motion sequences sharing the same pose transitions into a pose tree: a tree diagram where each node represents a specific pose state and edges define transitions between pose states. We adopt a flow visualization technique to provide an overview of the pose transition patterns in the pose tree. In this visualization, the frequency of pose transition is encoded using the thickness of edges. To define representative pose states, MotionFlow supports interactive partition-based pose clustering. In this approach, each pose cluster is represented by a poselet where poses included in the cluster are displayed inside of a tear-drop like glyph. Transitions between pose clusters are drawn in a directed cyclic graph where the user can explore pairwise similarity among clusters, internal cluster similarity, and transition frequency. Users can use such information along with their intuition and perception of pose similarity during clustering. The structure of graph is driven by the user by manipulating pose clusters through both local and global interaction techniques such as split, merge, lock of cluster nodes (local) and partition number control <ref type="bibr">(global)</ref>. Users can also select and investigate individual or group of motion data in patterns through a treemap and animation view. As a result of pattern analysis, MotionFlow generates an organized motion tracking database where similar motion sequences are aggregated into a set of patterns, and stored in multi-tab windows. In summary, the key contributions in this paper are as follows:</p><p>• An approach to visualize sequential motion patterns that preserves spatio-temporal information and supports visual analysis of transitions between pose states;</p><p>• A set of interaction techniques for visual abstraction, exploration, and organization of unstructured sequential motion data into a set of patterns; and</p><p>• Results of an expert review conducted on MotionFlow.</p><p>In this paper, we present MotionFlow in the context of gesture pattern studies, wherein comparative pattern analysis of natural human gestures is enabled. In order to understand the usefulness of our approach in practical analytics tasks, we evaluated MotionFlow with six domain experts, focusing on the role of visualization components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Below, we review topics related to the analysis of human motion tracking data, visualization of sequential data, and interactive clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Visual Analytics for Human Motion Tracking Data</head><p>So far, only a few visual analytics systems have been proposed to help researchers better understand and analyze human motion tracking data. To our best knowledge, Bernard et al. was the first to introduce such a system, MotionExplorer <ref type="bibr" target="#b4">[5]</ref>, which supports interactive search and retrieval of target motion sequences based on an interactive hierarchical clustering of motion tracking database. Although Mo-tionExplorer helps users generate and analyze meaningful subsets of a large database, it does not fully support aggregation of similar sequences into a set of pattern groups. To understand style variations in the retrieved sequences, this system requires manual exploration and comparison of multiple motion data. In this context, manual analysis can be challenging. The interactive clustering technique introduced in MotionExplorer is helpful in providing an overview of data structure in a tree diagram by aggregating similar pose data. However, the tree structure is purely decided by the pose similarity measure, and users are only able to control the global aggregation level (i.e., adjusting the number of clusters). Thus, the users cannot locally manipulate any aggregations that conflict with their perception of pose similarity. For example, there could be an unnecessary splitting of pose clusters during adjusting the global level control, while there also could be undesired merging of dissimilar pose clusters. Further, MotionExplorer employs a motion graph <ref type="bibr" target="#b19">[20]</ref> approach to visualize motion patterns. The motion graph is a node-link diagram where nodes represent static human poses while links define sequences of human poses. Although, it provides a simplified overview of multiple motion patterns, when the motion patterns being displayed become complex and diverse, it is hard to recognize complete transitions (i.e., order of node occurrence). In short, it does not provide a clear start and end to each sequence.</p><p>The closest existing work to MotionFlow is GestureAnalyzer <ref type="bibr" target="#b17">[18]</ref>, a visual analytics system for interactive aggregation of sequential motion data based on time-series similarity. However, similar to Mo-tionExplorer, GestureAnalyzer uses a fixed cluster structure, so there is no way to change the aggregation of motion sequences at a local level. This system provides an overview of multiple sequential motion data through small-multiple visualization. Since each motion data is represented by a single row, there could be redundant pose block across the entire visualization. Thus, this approach suffers from information overload when it tries to display large amount of sequential motions. Also, GestureAnalyzer does not allow users to define representative human poses that reflect their perception of pose similarity and representativeness for an effective overview of motion trends. In contrast, MotionFlow supports comparative pattern analysis of motion sequences, and allows for organizing them into meaningful pattern groups while leveraging human perception and domain knowledge. Furthermore, MotionFlow enables users to directly control aggregation through a set of direct manipulation techniques (e.g., splitting and merging clusters) when defining representative poses. Finally, Mo-tionFlow formulates sequential motion patterns into a state transition diagram in a tree form to eliminate redundant information, and provide more concise and simple overview of multiple sequential motions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sequential Pattern Visualization</head><p>A sequential pattern is commonly defined by a series of temporal events (e.g., A → B → C where A, B, and C stand for an event occurrence at a given time). There exists a vast body of work in the visualization of sequential patterns for different application areas. The most relevant approach to our system is a flow visualization. Here, different quantities of flows are represented by the width of edges that connect nodes where specific states of data is represented. Riehmann et al. <ref type="bibr" target="#b27">[28]</ref> introduced interactive Sankey diagrams to visualize different flow quantities through the size of edges. Similar techniques have been applied in visualizing various contexts of data such as social media <ref type="bibr" target="#b44">[45]</ref>, storytelling <ref type="bibr" target="#b21">[22]</ref>, text <ref type="bibr" target="#b10">[11]</ref>, and temporal events <ref type="bibr" target="#b42">[43]</ref>.</p><p>The tree diagram is one of the common approaches to visualize multiple pathways. Word trees <ref type="bibr" target="#b39">[40]</ref> visualize a series of words appearing in multiple sentences by spatially aggregating and locating them in a tree diagram, providing an effective way to search and explore specific path ways from a complex text-based database. Timeline trees <ref type="bibr" target="#b9">[10]</ref> provide an overview of multiple sequential transaction data through a hierarchical tree diagram and a timeline visualization. eSeeTrack <ref type="bibr" target="#b33">[34]</ref> takes a similar approach to visualize sequential patterns of eye fixations. LifeFlow <ref type="bibr" target="#b43">[44]</ref> provides an overview of multiple sequential patterns by aggregating the same pathway into a tree diagram.</p><p>While our visualization design is inspired by both tree diagrams and flow visualizations, we define each node with representative human pose states defined by users' perception and domain knowledge. To visualize multiple sequential motion patterns, MotionFlow aggregates the pose states into a tree structure, providing an effective but efficient overview of multiple sequential motion patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Visual Abstraction of Sequential Data</head><p>Motion tracking data is a series of multivariate, continuous human poses where infinite spatial and temporal variations can exist. To provide an overview of multiple sequential motions, we need to consider how to properly abstract and simplify such highly variable data. There exists work in simplifying sequential event data that is discrete and concurrent (e.g., medical event, transaction history). OutFlow <ref type="bibr" target="#b42">[43]</ref> applied a hierarchical clustering method to reduce the number of states in the flow visualization by combining similar event states based on a user-defined merging threshold. Monroe et al. <ref type="bibr" target="#b22">[23]</ref> introduced a simplification of sequential event data by replacing and aligning longer and complex event sequences with respect to a set of user-defined sub-sequences. Similarly, DecisionFlow <ref type="bibr" target="#b13">[14]</ref> provides an overview of high-dimensional sequential event data with several representative events. The approach provides an abstraction of a complex data set, and then lets the user progressively explore details of the sequence.</p><p>A state transition graph is an effective way to visualize multiple pathways of sequential events by considering temporal changes as state transitions <ref type="bibr" target="#b7">[8]</ref>. Ham et al. <ref type="bibr" target="#b35">[36]</ref> introduced a visualization for state transitions in a 3D space tree structure. Here, to reduce visual complexity, state nodes sharing the same ancestor nodes are clustered into a sub-tree structure. However, this approach does not provide users a direct control of state definitions as they are automatically defined. Pretorius et al. <ref type="bibr" target="#b25">[26]</ref> proposed a bar tree to abstract multivariate state transitions into 2D space. Blaas et al. <ref type="bibr" target="#b5">[6]</ref> introduced an approach to explore higher-order state (i.e., more than three states) transition patterns by aggregating transitions into spline bundles in a 2D graph layout. These approaches provide an overview and detail of specific transition patterns. However, they do not provide capability to compare multiple transition patterns based on time.</p><p>In this paper, we formalize pattern analysis of human motion into an interactive state transition visualization. Unlike existing work in state visualization, we provide the users direct manipulation of states to properly abstract complex and massive sequential motion into a manageable overview, then progressively define each transition pattern at a local level through an interactive clustering approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Interactive Data Clustering</head><p>Data clustering techniques cater to a wide number of research questions from a broad set of disciplines. Traditionally, clustering algorithms are considered an unsupervised machine learning where an unknown data set is clustered into a set of meaningful groups (see Berkhin et al. <ref type="bibr" target="#b3">[4]</ref> for a review). However, there has been considerable efforts to turn clustering algorithms into supervised learning by integrating users domain knowledge into the clustering process.</p><p>In the data mining community, researchers have introduced the concept of constrained clustering providing a role for the user in the clustering process <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>. In these efforts, users define pairwise cluster constraints such as "must be" and "must not be" in the same cluster. Balcan and Blum <ref type="bibr" target="#b2">[3]</ref> discussed interactive feedback in data clustering process. They described a pair of natural clustering processes-split and merge-and provide algorithmic requirements with bounded number of split and merge. However, these approaches do not support dynamic manipulation of the clustering process due to the lack of user interactions. Also, in addition to Balcan and Blum's local split/merge clustering metaphor, we also provide a global clustering control.</p><p>In visual analytics, clustering algorithms have been actively integrated with interactive visualizations to provide a control over the clustering process. Hierarchical clustering generates aggregation results in the form of tree diagrams, and most existing work allows users to steer the aggregation level by adjusting the depth of the tree structure <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b30">31]</ref>. gCLUSTO <ref type="bibr" target="#b26">[27]</ref> is a software package for testing various algorithms through a visual investigation. Ahmed et al. <ref type="bibr" target="#b0">[1]</ref> integrated partition-based clustering with multiple coordinate visualizations. Schultz et al. <ref type="bibr" target="#b29">[30]</ref> proposed an interactive spectral clustering approach to segment 3D imagery data. These approaches incorporate experts into data clustering processes, and require users to have background knowledge to properly control algorithmic parameters.</p><p>To our best knowledge, Hossain et al. <ref type="bibr" target="#b16">[17]</ref> proposed a concept most similar to our work, with interaction techniques that enable users with limited expertise in clustering algorithms to directly control the aggregation process through scatter and gather interactions. These interaction techniques support iterative modification of clustering results through a user-defined constraint matrix. However, such tabular input is less intuitive than direct cluster manipulation.</p><p>Our work focuses on providing an interactive clustering technique that supports users who have limited expertise in data mining to adeptly insert their domain knowledge and perception into the pose clustering process. To provide an effective interaction techniques in manipulating the clustering process, MotionFlow features both global and local cluster controls.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ANALYSIS OF HUMAN MOTION TRACKING DATA</head><p>The use-case driving the design of MotionFlow is gesture pattern studies in human-computer interaction (HCI). After reviewing this background below, we then identify requirements and rationale for the system design based on close collaboration with domain experts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Background: Gesture Pattern Studies</head><p>In HCI, a gesture is commonly defined by "a motion of the body that contains information" <ref type="bibr" target="#b20">[21]</ref>. Human gestures are actively and increasingly being used to support gesture-based interaction (e.g., mobile interaction <ref type="bibr" target="#b28">[29]</ref>, virtual 3D object design <ref type="bibr" target="#b36">[37]</ref>, and automobile interaction <ref type="bibr" target="#b24">[25]</ref>). In general, development of such interaction systems requires deep understanding of natural aspects in human gestures to increase naturalness and intuitiveness of the interactions, and reflecting this knowledge into gesture recognition <ref type="bibr" target="#b40">[41]</ref>. Thus, researchers in gestural interaction design must be experts in either one or both of HCI and pattern recognition. Such gesture interaction researchers are the target audience for our work in this paper.</p><p>To observe and understand natural patterns in human gestures, researchers commonly perform gesture pattern analysis through elicitation studies <ref type="bibr" target="#b41">[42]</ref>. Gesture elicitation studies collect natural and common trends in human gestures by categorizing similar gestures into a set of groups based on perception, background knowledge, and interaction contexts. Motion tracking data is central to this analysis, and researchers apply automated computational methods to aggregate gestures based on computational similarity <ref type="bibr" target="#b17">[18]</ref>. For example, when a researcher wants to study how people express gestures to control a virtual object on a big screen, he/she may capture natural gestures elicited from the candidate user groups. Through exploration and comparative analysis of gesture patterns, the researcher can answer questions such as, "What is the most common and similar gesture pattern expressed among the users?", or "How much variation is observed in the gesture patterns?". This kind of analysis requires annotation and grouping of similar gestures based on interaction context. Researchers also need to iteratively generate and modify gesture categorization criteria.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Method: Requirements Analysis</head><p>To collect requirements for a system based on the analysis procedures and tasks of gesture pattern studies, we collaborated with three HCI researchers with long experience in gestural interaction design, including one of co-authors of this paper. We had discussions with the collaborators every week, and demonstrated the initial design of Mo-tionFlow to gather their opinions and feedback. We also shared visual design of the most relevant existing system, GestureAnalyzer <ref type="bibr" target="#b17">[18]</ref> and MotionExplorer <ref type="bibr" target="#b4">[5]</ref>, so that the collaborators are able to understand limitations of the system, and reflect them in generating requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Design Requirements and Rationale</head><p>During the requirements analysis process, we identified a list of analytical requirements that MotionFlow should support in order to scaffold pattern analysis of human gestures in HCI research: R1 Visualizing multiple motion data sequences to support exploration and understanding of gesture motion patterns;</p><p>R2 Selecting interesting gesture patterns for detailed analysis;</p><p>R3 Comparing gestures to identify similar and dissimilar gestures;</p><p>R4 Supporting users to reflect their domain knowledge and interaction contexts into aggregating similar gesture data; and R5 Generating pattern analysis results in a form so that findings and insights can be shared with other researchers and domains. In particular, all domain experts expressed the need to have a global overview of multiple gesture data, and then be able to decide which portion of data should be further explored and analyzed (R1, R2, R3). Although tools such as GestureAnalyzer <ref type="bibr" target="#b17">[18]</ref> provide a detailed view of multiple gesture datasets, combining them into a single visualization in an efficient way is not a trivial problem. MotionExplorer <ref type="bibr" target="#b4">[5]</ref> features a motion graph view to integrate multiple motion data into a directed graph. However, it suffers from edge crossings and lacks a complete view of gesture sequences. To resolve such issues, we should consider how to efficiently eliminate and effectively simplify overlapping information. In practical scenarios, categorizing gestures cannot be formulated as a single exploration process. Instead, it commonly involves iterative refinement and modification of the current aggregation based on domain knowledge and the interaction context (R4). Similarly, the search and retrieval of subsets of motion data provided in MotionExplorer <ref type="bibr" target="#b4">[5]</ref> does not support flexible and user-driven pattern analysis to generate motion patterns. To support such aggregation, a new system should consider progressive and dynamic modification of gesture pattern groups. The domain experts also expressed a desire to have a summary of the analysis results (R5), so that the value of results can be transferred to support other researchers and their work, e.g. designing pattern classifiers for identified gestures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THE MOTIONFLOW SYSTEM</head><p>MotionFlow consists of four key components combined to support the workflow in <ref type="figure" target="#fig_0">Figure 2:</ref> (1) user-driven pose states clustering, (2) a tree diagram with flow visualization, (3) progressive organization of motion patterns, and (4) user interactions. Here, we first define the data model used in the system. Then, we provide details on the visual and interaction design, and how each analytical requirement is supported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Model</head><p>MotionFlow is based around a gesture database recorded during elicitation studies with mid-air hand gestures <ref type="bibr" target="#b17">[18]</ref>. This database consists of a variety of natural human gestures, each representing a single trial of meaningful human activity (i.e., a trial defined by a distinct starting and ending pose). We use such a dataset as a source of motion data for evaluating our system in supporting practical gesture analysis tasks. This database includes a total of 816 clips recorded by 17 participants while performing 48 gestures as inputs to gestural interaction systems. A Microsoft Kinect 1 camera was used to capture 3D coordinates of 11 upper-body joints (hands, elbows, shoulders, head, neck, chest, and pelvis) at 30 frames per second. A collection of motion data is loaded to the system as a binary file without annotating gesture style.</p><p>Motion Data. We define a motion, J = (id, [ j 1 , j 2 , ..., j n ]) as an identifier of human subject id and a sequence of n human poses where j i is a pose at ith frame.</p><p>Feature Descriptor of Human Poses. We define the feature descriptor of poses X ∈</p><formula xml:id="formula_0">R 3 * d as [(x 1 , y 1 , z 1 ), (x 2 , y 2 , z 2 ), ..., (x d , y d , z d )]</formula><p>1 https://www.microsoft.com/en-us/kinectforwindows/ where d is the number of tracked body joints and (x i , y i , z i ) is a 3D position of i-th body joint. Here, we are not concerned with improving clustering results through improved feature descriptors, but rather with involving human perception in the clustering process. For this purpose, we use a simple distance measure, Euclidean distance, to evaluate similarity between poses.</p><p>Pose Motion Sequences. Each motion pose belongs to a pose state, and we partition the motion data into k segments with respect to the frame where the pose state changes. Then, we replace each motion data segment with the corresponding pose state. We call this simplified data </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">User-Driven Pose State Clustering</head><p>To provide an overview of the gesture data, we formalize motion data with sequential events where each event represents a human pose (R1). As seen in <ref type="figure" target="#fig_0">Figure 2</ref>, users begin by analyzing motion data to generate representative pose states. Users are involved in the clustering process by subjectively surveying all pose states and their similarities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Partition-Based Clustering</head><p>Our target users do not necessarily have significant background in data mining techniques. So, we aim at providing an easy-to-understand and intuitive-to-control pose clustering process. There exist substantial work in clustering multivariate data such as hierarchical, spectral, density-based, and partition-based clustering (see <ref type="bibr" target="#b3">[4]</ref> for a review). Even though our user-driven clustering approach can be applied in most of the state-of-the-art clustering methods, we decided to use a partition-based clustering, K-Means, in the interactive clustering. The K-Means clustering approach is simple enough to understand and it is easy-to-manipulate the output clusters by simply adjusting the number of partitions. Hierarchical clustering also provides a simple way of adjusting clustering process; however, the structure of clustering is pre-determined, and the users cannot alter it progressively. We initialize centroids of K-Means to be uniformly distributed over the pose space having maximized sum of Euclidean distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Poselets</head><p>We define a poselet to be the visual entity encoding a pose cluster in MotionFlow <ref type="figure" target="#fig_1">(Figure 3</ref>  specific perspectives where the spatial information is well-reflected into 2D space. However, it is hard to automatically generate such a view for a general pose. To mitigate this issue, the user can control the view of poselets by dragging the mouse around each poselet. <ref type="figure">Figure 1(d)</ref> shows an interface for user-driven pose states definition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Pose States Graph</head><p>Here, we provide an overview of pose clustering, and let users progressively manipulate the pose clusters. For an overview of pose clusters and their sequential relationship, we use a motion graph, a directed graph where nodes represent pose clusters and edges encode motion sequences <ref type="bibr" target="#b19">[20]</ref>. Within the graph layout, we consider distances among cluster nodes to encode pairwise similarity among clusters. To visually encode such information, we employed a force-directed graph layout algorithm <ref type="bibr" target="#b18">[19]</ref> where the dynamics among cluster nodes is defined by a spring force model. When a dataset is loaded, MotionFlow computes the Euclidean distance between pose clusters, then the pairwise distances define the forces between the nodes in the spring model. As shown in <ref type="figure">Figure 1(d)</ref>, we use spatial proximity to encode the similarity of pose clusters (e.g., c 1 , c 3 , c 5 are close to each other while c 2 and c 5 are not). The edges in the motion graph visually encode the directed transition frequency between two pose states using a color gradient. This color-coding is intended to highlight frequent transitions (e.g., dark gray colored edges encodes frequent transitions between c 3 and c 4 , while infrequent transitions are observed between c 3 and c 5 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Interactive Cluster Control</head><p>Since the pose states are directly used in expressing sequential motion data, it is important to extract appropriate key pose states that do not conflict with the users' perception and domain knowledge. We designed three interaction techniques to support dynamic and progressive refinement of pose clustering guided by the users. Split, Merge, and Lock. MotionFlow provides the interactions split and merge, a pair of dynamic manipulation methods to control clustering at a local level. Split partitions an overcrowded pose cluster into two partitions, while merge combines multiple clusters into a single node <ref type="figure" target="#fig_5">(Figure 4(a),(b)</ref>). This pair of cluster manipulations enables users to dynamically correct local clustering results, and directly reflect their perception of human pose similarity. Such manipulations change the layout of motion graph since new nodes are generated. To help users to keep track of such dynamic changes, we adopt animated transitions among node positions before and after manipulation. Often, we want to preserve some cluster nodes, while other nodes are manipulated. To support such cases, MotionFlow provides lock of the node in interests indicating user intention of no further split, merge, nor re-clustering of the node <ref type="figure" target="#fig_5">(Figure 4(c)</ref>). As seen in <ref type="figure" target="#fig_5">Figure 4</ref>, users can select individual or multiple nodes, and right-clicking on the node activates a pop-up menu listing possible manipulations.</p><p>Adjusting Partition Numbers. MotionFlow also allows the user to control the number of clustering partitions by adjusting a vertical slider <ref type="figure" target="#fig_5">(Figure 4(c),(d)</ref>). When the slider is changed, a new set of pose clusters is generated, and previous nodes are replaced with a new one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Visual Motion Concordance: Pose Tree</head><p>After users define pose states, the system simplifies and aggregates the motion data into a pattern visualization. Here, we provide an overview of multiple motion data clearly showing common pathways of sequences (R1) and support exploratory analysis of motion patterns (R2, R3). In Section 2.1, we discussed limitations of existing techniques for visualizing motion patterns using a graph-based approach <ref type="bibr" target="#b4">[5]</ref> and small-multiples <ref type="bibr" target="#b17">[18]</ref>. Considering both analytical requirements and the limitation of existing approaches, we decided to use a tree layout design. The main advantage of the tree-based approach is to allow a complete view of motion including clear indication of start, end, and intermediate motion trends. However, trees have a potential scalability issue when considering the visualization of large motion patterns. To alleviate this issue, we provide interactive sub-sequence searching; and navigation of tree structure and a separate view dedicated to show the selected sub-tree structure (Section 4.3.5). Thus, inspired by the Word Tree <ref type="bibr" target="#b39">[40]</ref> visualization technique, we designed the Pose Tree, a visual motion pattern concordance ( <ref type="figure" target="#fig_7">Figure 5</ref>). In the Pose Tree, nodes represent the pose states and edges encode transitions between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Abstraction and Aggregation</head><p>The first step in generating the pose tree is to simplify each motion data as a sequence of representative pose states. The pose states being used in this step are defined through the clustering process described in Section 4.2. Based on this visual abstraction, we consider the problem of visualizing sequential motion data as state transition of the pose states. The next step is to aggregate the motion sequences into a set of motion patterns. Our aggregation procedure is equivalent to creating a prefix tree <ref type="bibr" target="#b12">[13]</ref> where each child node has only one prefix, a series of parent nodes. The aggregation starts with finding the pose states from which each motion sequence starts and then generating a list of root nodes where pose trees start to branch off. Then, it recursively searches the motion sequences, and adds non-existing child nodes to a corresponding prefix. The result is the pose tree structure where each leaf node stands for the last pose state of a unique motion pattern. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Tree Layout</head><p>In the pose tree layout, motion patterns are ordered from left to right representing temporal occurrence. To draw the pose tree, we first define the position of a leaf node in each pattern. We then vertically and horizontally partition the pose tree into m and n layers, respectively, where m is equal to the highest order of state transitions and n is the number of motion patterns. In the i-th vertical layer, leaf nodes having i-th order of state transitions are horizontally positioned. In <ref type="figure" target="#fig_7">Figure 5</ref>(a), the leaf node of the motion pattern P 4 is a 4th-order state transition, so it is horizontally positioned at the 4th vertical layer. Similarly, leaf nodes are vertically positioned based on the order of their motion patterns. The motion patterns are then sorted based on frequency. For example, P 1 is the most frequent pattern, so it is vertically positioned on the first horizontal layer. Once all leaf node positions are defined, the positions of all remaining nodes are recursively decided based on the position of their child nodes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Edge: Flow Visualization of Pose Transitions</head><p>To provide an overview of the frequency of motion patterns, we take a flow visualization approach in visual encoding the pose tree edges. The edges connect the tree nodes, and each connection involves a certain number of motion data which is equivalent to the frequency of state transitions. The width of edges is proportional to the number of associated motion data over the number of the entire motion data. For example, in <ref type="figure" target="#fig_7">Figure 5</ref>(b), motion pattern P 1 and P 5 consists of six and two motion data respectively. So, the width of edges in P 1 is three times thicker than the edges in P 5 . To distinguish the patterns identified in the pose tree, we assign different colors to each pattern.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Alternative View: Motion Pattern Treemap</head><p>To support exploration of individual or groups of motions (R2), Mo-tionFlow employs a treemap as an alternative representation of the pose tree using an ordered space-filling layout <ref type="bibr" target="#b31">[32]</ref>. This is to preserve the order and hierarchical relationship in the pose tree structure using a treemap layout. In this view, individual rectangles represent a single motion data, and the vertical and horizontal adjacency of rectangles encode hierarchical relationship among motion data. <ref type="figure" target="#fig_8">Figure 6</ref> shows a treemap layout of pose tree in <ref type="figure" target="#fig_7">Figure 5(a)</ref>. The root node is split into four child nodes, so the original rectangle in the tree map is first horizontally divided into four motion pattern groups; G 1 = [ P 1 , P 2 , P 3 , P 4 , P 5 ], G 2 = P 6 , G 3 = P 7 , and G 4 = [ P 8 , P 9 ]. In the next step, G 1 is further vertically divided into two pattern groups. Animating Motion Data. Selecting individual or multiple items animates human motion in the animation view <ref type="figure">(Figure 1(f)</ref>). Animating complex human body motion may yield clutter when representing the body limbs as 3D lines. To address this issue, we used categorical qualitative color scales informed by ColorBrewer <ref type="bibr" target="#b15">[16]</ref>, and applied different colors to the body limbs to help users discern individual limbs and joints during the animation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.5">Searching for Similar Motion Patterns</head><p>The pose tree visualization can significantly reduce the visual space and information overload by aggregating coincident motion sequences into a motion pattern set. However, if two perceptually similar motion data have different pose state transitions at the initial motion, they would not be aggregated into the same motion pattern even if they share the same intermediate motion sequences. This is one of the main concerns in using a tree layout to provide an efficient overview of motion patterns. Based on this observation, we implement a sequential pattern mining technique to support pattern selection and exploration (R1, R2). In particular, we implemented PreFixSpan <ref type="bibr" target="#b23">[24]</ref> with constraints on having non-intervals between pose states. In this approach, individual motion sequence is defined by a string of pose states, and frequent sub-sequences are generated based on their frequency and transition length <ref type="bibr" target="#b23">[24]</ref>. We first generate a bag of frequent sub-sequential patterns (FSPs) from the motion sequences. When the users query a specific motion pattern, a list of associated FSPs is extracted from the bag of FSPs. Then, we search the extracted FSPs to identify sub-sequences matching to the queried motion pattern. As a result, a list of candidate similar motion sequences are generated. In our application, we restrict the minimum length of FSP as three successive pose states regarding the length of motion data.</p><p>Clicking a tree node selects all pathways passing through the node, and right-clicking the node toggles a pop-up menu, enabling similar pattern searching. In <ref type="figure" target="#fig_7">Figure 5</ref>  <ref type="figure">(Figure 1-b)</ref>. Here, the users are able to group similar sequential motion data distributed in different location of the pose tree. To support detailed and focused display of specific region of interest in the pose tree, we also provide zooming and panning capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Exploration and Organization of Motion Patterns</head><p>After motion sequences are aggregated into a pose tree, users investigate this overview for detailed exploration. To support such analysis, MotionFlow allows users to dynamically create and modify a group of motion data using tabbed windows (R4). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Motion Pattern Tabs</head><p>In the process of organizing motion patterns, users can progressively create and modify pattern tabs by adding or removing motion sequences. Using this dynamic and iterative refinement of pattern tabs, users can categorize patterns based on criteria such as their perception, and motion pattern context. <ref type="figure" target="#fig_10">Figure 7</ref> illustrates how users interact with the pattern tab interface. Users select a motion pattern ( P s as the most frequent) from the initial pose tree (a), then create a pattern tab for storing the selected sub-pattern. (b) is a treemap view of the initial tree in (a). Users can select motion data included in P s on the treemap (individuals) or the pose tree (groups). Right clicking on the selected data on treemap activates a pop-up context menu including a list of possible operations (create, add, and remove). (c) shows a new tabbed window storing the selected pattern P s . Pattern names can be customized so that the user can remember what type of motion pattern is aggregated in a certain tab. (a) and (c) show zoomed-in views of the pattern tab names (red box). A colored box icon is placed on the left side of each tab name in order to link the sequential patterns contained in each tab to the pose tree. These boxes can be dynamically changed as the users manipulate the data organization. The size of subordinate color boxes is proportional to the frequency of the corresponding motion pattern. For example, the Initial Tree tab in (a) consists of three sub-patterns; light blue, pink, and dark blue (from left to right order of appearance in the color-box). As we can see from the color-box icon, the dark blue motion pattern (left) has the largest portion in the pattern tab, while the other sub-patterns have smaller contributions in creating the pattern. As seen in (d), the treemap view reflects current selection of tabbed windows by representing non-associated data using hatching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Specifying Motion Pattern Models</head><p>After creating the pattern tab containing interesting patterns, the users can further explore and organize them into a meaningful set of subpatterns. <ref type="figure">Figure 8</ref> illustrates a workflow of specifying pattern models of the subset of motion data ( P s in <ref type="figure" target="#fig_10">Figure 7)</ref>. When users explore a subset of motion patterns in a tabbed window, only associated pose states are highlighted, while the others (green boxes in <ref type="figure">Figure 8(a)</ref>) are grayed out in the motion graph. Users can identify a cluttered pose state (red box), and split the pose state in the motion graph (blue boxes in <ref type="figure">Figure 8(a)</ref>). In return, the pose tree and the tree map change their structure showing hidden sub-patterns as seen in <ref type="figure">Figure 8(b)</ref>. Only motion data included in the pattern tab are highlighted in the treemap view <ref type="figure">(Figure 8(b)</ref>,(c)). The selective highlighting in the motion graph and the tree map supports the users in focusing on relevant information in analyzing and refining a specific group of motion data. By investigating the sub-patterns, the user can further specify pattern models, and organize the data based on the models as shown in <ref type="figure">Figure 8(c)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Creating an Organized Motion Database</head><p>After the analysis phase, users are able to save the gesture pattern organization to form a gesture database for later presenting and sharing insights with other researchers. Also, the visual representation of each gesture pattern (i.e., pose tree, pose state graph) can be further exploited to generate a statistical model informed by natural human gestures (R5). Users can project their domain knowledge and the context of human gestures directly into this output gesture database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION: EXPERT REVIEW</head><p>We performed an expert review <ref type="bibr" target="#b32">[33]</ref> to evaluate the usability of Mo-tionFlow in supporting pattern analysis of human motion tracking data. The goals of our study were to (1) evaluate how domain experts use MotionFlow in practical analysis tasks, and (2) understand the role of visual components in supporting the sensemaking process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Design</head><p>We recruited six Ph.D. students, three of which had extensive experience in the design of gestural interactions, denoted as E1, E2, and E3. The other three participants (E4, E5, E6) indicated that they had expertise in pattern analysis of motion tracking data. All six participants had no prior experience with MotionFlow. The study consisted of (1) an introduction to MotionFlow, <ref type="bibr" target="#b1">(2)</ref> four open-ended tasks lasting one hour in total, and (3) a post-questionnaire with an informal interview. The introduction phase lasted about 25 minutes, including a 10-minute demonstration by a proctor and 15 minutes of self-exploration. In each task, the participants were instructed to think aloud and, once finished, were given a post-questionnaire. We also recorded the participants' screens and comments for each session. Once all tasks were completed, the participants completed a final post-questionnaire including an informal interview about their experience with MotionFlow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Tasks and Questionnaires</head><p>The tasks given in our study were (T1) generating a set of representative pose states as per the perception of the participants; identifying (T2) the most common and (T3) the unique motion pattern; and (T4) organizing unlabeled motion data into a meaningful set of motion patterns. The last task was based on free exploration, and intended to evaluate system usability and study expert strategies in solving a practical analytics task. Since there is no ground truth answer for each task, we did not quantitatively measure the analysis results users generated. We were more interested in studying how the experts connect each visual component of the system. We also designed tasks to expose participants to all components of MotionFlow in order to better understand the sensemaking process supported by visual components.</p><p>At the end of each task, the experts were given a set of in-study questionnaire (Q2-Q6: T1, Q7-Q10: T2 &amp; T3, Q11-Q13: T4, Q1: end of all tasks), and it consists of thirteen 7-point Likert scale (1 = strongly agree, 7 = strongly disagree) questions: Q1 I was able to learn how to use MotionFlow; Q2 I was able to categorize poses as per my perception by manipulating the structure of the clusters; Q3 I was able to recognize similar pose clusters from the data; Q4 I was able to understand variations in pose clusters from the data; Q5 I was able to identify frequent transitions among pose clusters; Q6 I was able to able to generate a pose tree following guidelines; Q7 I was able to identify the trends in motions from the pose tree; Q8 I was able to compare difference between motion sequences; Q9 I was able to identify frequent motion patterns; Q10 I was able to identify distinct/unique motion patterns; Q11 I was able to categorize motion sequences as per my perception by creating the motion pattern tabs; Q12 I feel confident about my pose organization; and Q13 I feel confident about replicating my pose organization.</p><p>We also asked participants seven open-ended questions on their procedure in completing each task in order to elicit feedback regarding the visualizations and interactions provided by MotionFlow. <ref type="figure">Fig. 8</ref>. Workflow of exploring and organizing motion patterns. After selecting an interesting sub-pattern in <ref type="figure" target="#fig_10">Figure 7</ref>, users can further analyze it for specifying pattern models. (a) By manipulating pose state clustering in the motion graph, (b) the structure of motion pattern is transformed showing hidden sub-patterns. (c) Through multi-tab interactions, the users can further organize the motion pattern into two sub-pattern groups. <ref type="table">Table 1</ref>. Collection of motion data used for the analysis tasks. The data is obtained from gesture elicitation studies on mid-air hand gestures <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gesture Dataset</head><p>Gesture Styles Changing volume of music player (T1-3)</p><p>(1) moving both hands down, (2) moving both hands up, (3) clapping hands, (4) moving right hand high-up and left hand mid-up, (5) moving right hand down, (6) swiping right hand.</p><p>Starting/stopping music player (T4)</p><p>(1) pulling one hand close, (2) pushing one hand away, (3) drawing a triangle, (4) drawing a rectangle, (5) moving right hand up, (6) swiping right hand, (7) moving both hands up, (8) moving both hand away, (9) crossing two hands, (10) moving both hands down, (11) both hands swiping, (12) touching shoulders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Dataset</head><p>In Tasks 1-3, we used 34 clips of motion data with a total of 3,080 human poses and an average of 91 frames (min:66, max:133). In Task 4, we used 68 clips of data with a total of 5,657 poses and an average of 83 frames (min:62, max:128). Using MotionFlow, we organized the dataset into a set of gesture styles <ref type="table">(Table 1)</ref>. Our categorization was based on personal experience, thus it does not imply a ground-truth for gesture data aggregation. We provide such aggregation to show the complexity of tasks in terms of gesture style variations in the dataset. <ref type="figure">Figure 9</ref> summarizes that experts indicated that MotionFlow is easy to learn, and effectively supported their pattern analysis tasks. Here, we summarize our findings from the expert review.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Analysis Process and Strategy</head><p>T1: Generating Representative Pose States. Most experts first tried to find the optimal number of partitions, so that the pose tree has a reasonable number of states and transitions. Then, they manipulated pose clusters while inspecting the structure of the pose tree. One expert (E5) tried to iteratively modify the clusters without linking the motion graph and the pose tree views. T2 &amp; 3: Identifying Common/Unique Patterns. All experts first explored the pose tree to identify and select candidate patterns. Then, they formed a hypothesis that the current selection is reasonable to their perception and interaction context. For confirmation, four experts (E1, E2, E4, E6) manipulated pose states in the motion graph and then observed whether the manipulation results affect the structure of the selected sub-tree patterns. If changes were not noticed, they ran the gesture animation to check for irregular motion. The other experts (E3, E5) inclined to directly run the gesture animation to observe detailed motion rather than using the visual overview. T4: Organizing Motions into Patterns. Task 4 was comprehensive of previous tasks, so the experts adopted their strategies for Task 2 and 3. Additional interactions included creating and modifying tabbed windows for storing and organizing identified gesture patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Efficiency</head><p>The experts were impressed that they could explore, understand, and organize an unstructured motion database in less than half an hour. E1 commented, "Saving time. No need for tagging the video. The frequency of sequences can be easily determined." E6 also mentioned, "Video annotation is usually done by the highly paid researchers, <ref type="bibr">[...]</ref> but simply going over the whole dataset. Using this tool, I think we can easily find out different set of unique gestures."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Effectiveness</head><p>Interactive Pose Clustering. The responses for Q2-Q6 indicate that participants agreed that MotionFlow is effective in generating representative human poses based on their perception. All experts preferred local manipulations (split/merge) than global control of cluster numbers (adjusting slider-bar) in the clustering process. E6 particularly appreciated the split/merge manipulation: "Split/Merge is especially useful when I want to find the uniqueness of the gesture." The expert also appreciated the adjusting the view of poselets. Also, E2 mentioned that "rotating view of poses inside of poselet is extremely helpful in understanding the context of poses in 3D space." Pose Tree with Flow Visualization. As shown in the rating results for Q7-Q10, experts were able to understand complete motion trends (i.e., start and end of motion sequence) and transition frequency from the flow visualization in the pose tree. All experts mentioned that the flow visualization in the pose tree gives a clear understanding of frequent and infrequent motion patterns. E6 mentioned that flow visualization required only low cognitive load when identifying aggregation of similar gesture patterns, and it does not require iteratively playing gesture animation. He could intuitively recognize which gestures to focus on for detailed analysis. Similarly, E1 said, "the pose tree gives me full access into specific intermediary poses, that would not be available in typical video-based observational data." The experts echoed that a complete view of motion sequences cannot be observed in the motion graph view, where only the connectivity between two successive pose states is accessible. Also they agreed that the pose tree is "good for comparing multiple sequences". Organizing Motion Patterns. Responses on questions Q11-Q12 indicate that all experts, except for E5, were able to confidently create a meaningful set of patterns. E1 mentioned that organizing patterns by grouping them into the tabbed windows helped avoid gesture clustering errors, which are common to automated methods. E4 commented, "[...] with human perception, we can overcome the errors which come from computational methods." E1, E2, and E4 all noted that the search <ref type="figure">Fig. 9</ref>. Results from 7-point Likert scale questionnaire. Individual column groups correspond to an in-study question. The task description is provided above each group. Within a group, each of the 6 responses represents each of the 6 experts, ordered E1 through E6, from left to right.</p><p>and retrieval of similar patterns by querying interesting pattern is helpful in generating a set of motion patterns distributed in the pose tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Applicability</head><p>E4 and E5 noted that MotionFlow can be useful in organizing their database. E4 noted that "in my case, hand motion data should be captured and clustered based on its similarity. However, there exists no such tool to do this kind of work. ... <ref type="bibr">[I]</ref> hope this system is applied to my project to generate optimal pose clusters from huge amount of data soon." Similarly, E5 commented that MotionFlow is "valuable for separating noisy poses." E1 mentioned that MotionFlow could be useful for improving gesture recognition: "[The] pose tree <ref type="bibr">[could]</ref> help us identify frequent poses that can cause misclassification of a gesture. This information can be used to subsequently improve the system for identifying erroneous or ambiguous gesture cues."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>Our expert review revealed that MotionFlow is effective in analyzing and aggregating motion sequences into a set of patterns. Specifically, our pose tree and motion graph visualizations were effective in specifying the underlying structure and relationship of the motion dataset while leveraging human perception and gesture context.</p><p>The experts used the interactive pose clustering not only for generating distinct pose states, but also for understanding the context of pose states by exploring the transition patterns in the pose tree. One expert noted that "the combination of motion graph and pose tree was very helpful during the initial organization of the data into reasonable chunks to start with." However, one participant (E5) lacked such insight and found it difficult to organize the gesture database. He could not easily relate the pose tree and motion graph in understanding the role of pose states in the pose tree. Rather, he tried to obtain a wellorganized pose tree by only manipulating pose clusters in the motion graph. In the informal interview, he reported that "I fixated on defining pose states, and could not relate the visual components of Motion-Flow." From observation, we noticed that (a) relating the pose tree and motion graph is critical for understanding and organizing the database, and (b) the sensemaking process is supported well by guiding experts to understand the linking between the two visual components.</p><p>In Tasks 2-4, the experts used the animation view to validate their hypothesis that the selected candidate motion pattern is coincident with their background knowledge. They explored motion patterns by looking at details, such as thick edges in the pose tree and the large screen area in the treemap. To refine the flow visualization, they manipulated the pose states in the motion graph. However, when a group of motions being explored consisted of a smaller number of instances, they preferred the animation view. This implies that the motion tracking data itself is a crucial component for our understanding of human motion as it directly represents parts of the human body. However, when analyzing large amounts of motion data, such organization with a low level data representation is difficult. Our interactive visualizations provide us visual summaries that enable us to efficiently reach an acceptable understanding of the data. A key insight is that the animation cannot be replaced by pose tree or motion graph alone, but all components should be integrated to support the sensemaking process.</p><p>Limitations and Future Work. Even if our initial results are promising in analyzing and organizing motion patterns, there are limitations in MotionFlow's current form. While gesture elicitation stud-ies normally start from a common neutral pose, some applications may start from different poses, leading to many root nodes branching out to multiple tree diagrams. This will limit the pose tree in providing a concise overview of patterns having multiple starting poses. In such cases, MotionFlow provides a global overview as a forest of trees. Then users can explore motion patterns through searching and navigating the multi-tree structure, and progressively organize the data into pattern tabs. Another potential solution is to develop a visual aggregation method that combines intermediate motion sequences into a single structure (e.g., single tree and graph) while preserving sequential completeness and conciseness. However, this is a very challenging problem, involving a NP-complete problem of subgraph isomorphism <ref type="bibr" target="#b34">[35]</ref>.</p><p>In broader contexts, general motion data (i.e., beyond the gesture elicitation studies that MotionFlow was designed for) can involve hours of activity recording rather than single gesture trials. Our current approach does not support the analysis of long motion sequences. Although analyzing long motion sequences is not the main concern in our application scenario, this could be an interesting direction to investigate in the future. Even despite scale issues, human motion is intrinsically compositional, i.e., longer motions are composed of sequences of shorter motions. This means that we can even use Motion-Flow's shorter sequences to manage longer periods of motion data. For example, for motion data consisting of multiple human actions such as walking, crawling, or drinking, we may use the individual action as states, and represent the motion data as a transition between them. Then, we can apply our visual encoding approach to simplify and aggregate motion patterns using the action state transitions.</p><p>The scalability issue can also occur in the color coding applied to the pose tree, treemap, and multi-tab windows. When presenting a large number of sequential patterns, the color boxes can be cluttered and hard to discern. Specifically, the colored box in the tabbed windows was not actively used by the experts compared with other views. As one expert participant suggested, we will explore this visual space to provide a more effective overview of the categorization in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We presented MotionFlow, a visual analytics system that supports pattern analysis of human motion targeting applications in gesture pattern studies in HCI. MotionFlow provides interactive clustering with local and global manipulations that enables users to generate representative pose states based on their perception. The pose tree with flow visualizations provides an overview of complete motion trends. MotionFlow also provides a motion pattern tab interface allowing users to explore and organize motion data into a set of meaningful patterns. Results from an expert review showed that MotionFlow is easy to learn, and effectively supports experts in performing gesture pattern analysis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>System workflow. (a) Users first define representative pose states. (b) The system simplifies the motion data into sequences of the states, and aggregates them into pattern visualizations. (c) Users interact with visualizations to explore motion data, and iteratively specify pattern models involving similarity of motions. (d) Based on the pattern models, users organize the database. The cycle is repeated as needed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Poselets of human pose clusters. A tear-drop like shape is used as the outline of poselet. At the center of a poselet, the centroid pose is drawn. The other poses are displayed using semi-transparent strokes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>States. As a result of pose clustering (Section 4.2), we obtain a set of representative human poses, C = [c 1 , c 2 , c 3 , ..., c m ] where c j is a pose state representing the j-th pose cluster centroid.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>representation a motion sequence, S = (id, [s 1 , s 2 , ..., sk ]) where s i is a pose state representing the i-th pose segment and id as its identifier. Motion Patterns. Multiple individual motion sequence can be aggregated into a motion pattern, P = S 1 , S 2 , S 3 , ... S l where l sequences are grouped together. Each motion sequence has a unique identifier id, and shares pose states [s 1 , s 2 , ..., s k ] with other sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>). Poselets are a glyph-based approach consisting of a stick-figure representation of a human pose<ref type="bibr" target="#b4">[5]</ref>. The stick figure is centered in a circle, and other similar poses are displayed as semi-transparent ghosts around the center pose. Cluster IDs are given in the top-left corner of poslets. The variance of pose clusters is normalized into [0, 1], and visually encoded using a gradient color scheme (green-to-yellow-to-red). For example, inFigure 3, pose state c 2 has a higher variance (red boundary), while c 3 indicates a lower variance (green boundary). To annotate and classify human body poses, Bourdev and Malik<ref type="bibr" target="#b8">[9]</ref> defined a poselet as an encoding of 3D joint information of human body into a 2D representation. Projecting a 3D pose into 2D space involves a loss of depth information. Each poselet has</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Interactions on pose clustering. (a) A pose cluster (red box) is selected for a split, and (b) is separated into two pose clusters (blue boxes). They can be merged back to retrieve the original cluster in (a). (c) Dragging a slider-bar (green box) adjusts the number of clusters (K) while a pose cluster (red box) is locked. (b) The locked cluster remains unchanged while the other clusters are regenerated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Figure 5(a) shows an example of the pose tree visualization. Here, pose states are given byC = [c 0 , c 1 , c 2 , c 3 , c 4 , c 5 ]where six distinct representative poses are defined. There is a single root node representing c 4 state (a natural standing pose). By expanding a pose tree from the root node, we get nine motion patterns ( P 1 ∼ P 9 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>Pose Tree visualizations. (a) Visualization of sequential motion patterns. Here, 9 unique patterns are identified, and colors are assigned to each pattern. The link thickness encodes the frequency of transitions between two poses. (b) Similar sub-sequence search result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>A treemap layout for an alternative representation of pose tree in Figure 5(a). Each rectangle has the same color with corresponding motion pattern in the pose tree. G 11 = [ P 1 , P 2 , P 3 ] and G 12 = [ P 4 , P 5 ]. Similarly, the rest of space are divided into sub-rectangles inside of the tree map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>(a), a motion pattern P 9 is queried to the original pose tree. The queried motion pattern is defined by five pose states [c 4 , c 0 , c 5 , c 2 , c 4 ]. As a result of the search, three similar motion patterns are retrieved by mutual sub-sequences; [c 5 , c 2 , c 4 ] of P 1 and P 5 , [c 4 , c 0 , c 5 , c 2 ] of P 8 . MotionFlow provides a window dedicated to show the search result, represented in a sub pose tree structure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 7 .</head><label>7</label><figDesc>(a) A motion pattern P s with the thickest edge in the initial pose tree is considered as a potential frequent pattern. (b) The data included in P s is selected on the treemap to create a new pattern tab and window. (c) RightSwiping tab only containing P s is created. (d) The data not associated with the pattern P s is unhighlighted in the treemap.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Manuscript received 31 Mar. 2015; accepted 1 Aug. 2015; date of publication 20 Aug. 2015; date of current version 25 Oct. 2015. For information on obtaining reprints of this article, please send e-mail to: tvcg@computer.org. Digital Object Identifier no. 10.1109/TVCG.2015.2468292</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was partially supported by the NSF Award No.1235232 from CMMI and 1329979 from CPS, as well as the Donald W. Feddersen Chaired Professorship from Purdue School of Mechanical Engineering. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Steerable clustering for visual analysis of ecosystems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mcgovern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Weaver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EuroVis Workshop on Visual Analytics</title>
		<meeting>the EuroVis Workshop on Visual Analytics</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evaluating a dancer&apos;s performance using kinect-based skeleton tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Alexiadis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Daras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boubekeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Moussa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Multimedia</title>
		<meeting>the ACM Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="659" to="662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Clustering with interactive feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-F</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Algorithmic Learning Theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="316" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A survey of clustering data mining techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Berkhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Grouping multidimensional data</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="25" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">MotionExplorer: Exploratory search in human motion capture data based on hierarchical aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wilhelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schreck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kohlhammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2257" to="2266" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Smooth graphs for visual exploration of higher-order state transitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blaas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Botha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grundy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Laramee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">H</forename><surname>Post</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="969" to="976" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unraveling students&apos; interaction around a tangible interface using gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blikstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Educational Data Mining</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Sequential machines and automata theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Booth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1967" />
			<publisher>Wiley</publisher>
			<biblScope unit="volume">3</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Poselets: Body part detectors trained using 3D human pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision</title>
		<meeting>the IEEE Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1365" to="1372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Timeline trees: visualizing sequences of transactions in information hierarchies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Diehl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Advanced Visual Interfaces</title>
		<meeting>the ACM Conference on Advanced Visual Interfaces</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="75" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">TextFlow: Towards better understanding of evolving topics in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2412" to="2421" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient incremental constrained clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the ACM Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="240" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Trie memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fredkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="490" to="499" />
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">DecisionFlow: Visual analytics for highdimensional temporal event sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gotz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stavropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1783" to="1792" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Opening the black box: interactive hierarchical clustering for multivariate spatial patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peuquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gahegan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Symposium on Advances in Geographic Information Systems</title>
		<meeting>the ACM Symposium on Advances in Geographic Information Systems</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="131" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Colorbrewer. org: an online tool for selecting colour schemes for maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harrower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Brewer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Cartographic Journal</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="37" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scatter/gather clustering: Flexibly incorporating user feedback to steer clustering results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K R</forename><surname>Ojili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Grimm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">T</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ramakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2829" to="2838" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">GestureAnalyzer: visual analytics for pattern analysis of mid-air hand gestures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Symposium on Spatial User Interaction</title>
		<meeting>the ACM Symposium on Spatial User Interaction</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="30" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An algorithm for drawing general undirected graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kawai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing Letters</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="15" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Motion graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kovar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pighin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="473" to="482" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Gestures in human-computer communication. The Art of Human-Computer Interface Design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kurtenbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Hulteen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="309" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">StoryFlow: Tracking the evolution of stories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2436" to="2445" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Temporal event sequence simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Plaisant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2227" to="2236" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">PrefixSpan: Mining sequential patterns efficiently by prefixprojected pattern growth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mortazavi-Asl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Dayal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Data Engineering</title>
		<meeting>the IEEE Conference on Data Engineering</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="215" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A research study of hand gesture recognition technologies and applications for human vehicle interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Pickering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Burnham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Automotive Electronics</title>
		<meeting>the IEEE Conference on Automotive Electronics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Visual analysis of multivariate state transition graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Pretorius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Van Wijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="685" to="692" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">gcluto: An interactive clustering, visualization, and analysis system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<idno>UMN-CS TR-04</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Interactive Sankey diagrams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Riehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hanfler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Froehlich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Symposium on Information Visualization</title>
		<meeting>the IEEE Symposium on Information Visualization</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">User-defined motion gestures for mobile interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Human Factors in Computing Systems</title>
		<meeting>the ACM Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Open-box spectral clustering: applications to medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Kindlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2100" to="2108" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Interactively exploring hierarchical clustering results [gene identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="80" to="86" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Tree visualization with tree-maps: 2-D space-filling approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="92" to="99" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Evaluating visualizations: Do expert reviews work?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Möller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="8" to="11" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">eSeeTrack-visualizing sequential fixation patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Swindells</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="953" to="962" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An algorithm for subgraph isomorphism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Ullmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="42" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Interactive visualization of state transition systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Van Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van De Wetering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Van Wijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="319" to="329" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Shape-It-Up: Hand gesture based creative expression of 3D shapes using intelligent generalized cylinders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vinayak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Murugappan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer-Aided Design</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="277" to="287" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Constrained k-means clustering with background knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wagstaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schrödl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="577" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Flexible constrained spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Davidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the ACM Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="563" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The Word Tree, an interactive visual concordance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Viégas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1221" to="1228" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Research challenges in gesture: Open issues and unsolved problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wexelblat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Gesture and Sign Language in Human-Computer Interaction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">1371</biblScope>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Maximizing the guessability of symbolic input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">O</forename><surname>Wobbrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Aung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rothrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Myers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Extended Abstracts of the ACM Conference on Human Factors in Computing Systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1869" to="1872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Exploring flow, factors, and outcomes of temporal event sequences with the Outflow visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wongsuphasawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gotz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2659" to="2668" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">LifeFlow: visualizing an overview of event sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wongsuphasawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Guerra Gómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Plaisant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Taieb-Maimon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Human Factors in Computing Systems</title>
		<meeting>the ACM Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1747" to="1756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">OpinionFlow: Visual analysis of opinion diffusion on social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1763" to="1772" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Human motion tracking for rehabilitation-a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomedical Signal Processing and Control</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
