<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Supporting Analysis of Dimensionality Reduction Results with Contrastive Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takanori</forename><surname>Fujiwara</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oh-Hyun</forename><surname>Kwon</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwan-Liu</forename><surname>Ma</surname></persName>
						</author>
						<title level="a" type="main">Supporting Analysis of Dimensionality Reduction Results with Contrastive Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TVCG.2019.2934251</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Dimensionality reduction</term>
					<term>contrastive learning</term>
					<term>principal component analysis</term>
					<term>high-dimensional data</term>
					<term>visual analytics</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Dimensionality reduction (DR) is frequently used for analyzing and visualizing high-dimensional data as it provides a good first glance of the data. However, to interpret the DR result for gaining useful insights from the data, it would take additional analysis effort such as identifying clusters and understanding their characteristics. While there are many automatic methods (e.g., density-based clustering methods) to identify clusters, effective methods for understanding a cluster&apos;s characteristics are still lacking. A cluster can be mostly characterized by its distribution of feature values. Reviewing the original feature values is not a straightforward task when the number of features is large. To address this challenge, we present a visual analytics method that effectively highlights the essential features of a cluster in a DR result. To extract the essential features, we introduce an enhanced usage of contrastive principal component analysis (cPCA). Our method, called ccPCA (contrasting clusters in PCA), can calculate each feature&apos;s relative contribution to the contrast between one cluster and other clusters. With ccPCA, we have created an interactive system including a scalable visualization of clusters&apos; feature contributions. We demonstrate the effectiveness of our method and system with case studies using several publicly available datasets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>High-dimensional data visualization is one of the major research topics in the visualization community <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b47">47]</ref>. Various types of visualization methods (e.g., the parallel coordinates <ref type="bibr" target="#b33">[33]</ref>, scatterplot matrices <ref type="bibr" target="#b27">[27]</ref>, and star coordinates <ref type="bibr" target="#b38">[38]</ref>) have been introduced to present high-dimensional information in a space <ref type="bibr" target="#b47">[47]</ref> (typically 2D on a computer screen) that human viewers can perceive and interpret. Among these methods, dimensionality reduction (DR) methods are suitable to provide an overview of the relationships across the high-dimensional data points <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b61">61]</ref>.</p><p>The strength of DR methods is their capability of uncovering the similarity between data points as spatial proximity <ref type="bibr" target="#b75">[75]</ref>. In DR results, by referring to the "similarity â‰ˆ proximity" <ref type="bibr" target="#b75">[75]</ref> relationship, we can intuitively find useful patterns, such as clusters and outliers. Many fields of study, including biology <ref type="bibr" target="#b31">[31]</ref>, social science <ref type="bibr" target="#b68">[68]</ref>, and machine learning <ref type="bibr" target="#b58">[58]</ref>, require analyzing high-dimensional data and thus rely on DR methods.</p><p>According to the recent surveys <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b54">54]</ref>, analyzing a DR result involves the following tasks: <ref type="bibr" target="#b0">(1)</ref> identifying clusters in the DR result, <ref type="bibr" target="#b1">(2)</ref> understanding the characteristics of the clusters, and (3) comparing the clusters with predefined classes of data points <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b54">54]</ref>. In the case that the DR result has interpretable axes, such as the dimensions generated by principal components analysis (PCA) <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b37">37]</ref>, understanding the characteristics of each axis and comparing the axis with the original dimensions (or features) are also included as part of the analysis tasks.</p><p>Among the aforementioned tasks, the main task sequence is first identifying clusters and then understanding their characteristics <ref type="bibr" target="#b11">[12]</ref>. While many automatic methods (e.g., density-based clustering methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b40">40]</ref>) have been introduced to identify clusters (the first task), methods to assist the second task have still not been well studied, especially in the case that the data has many features. Reviewing the original feature values is essential to understanding each cluster's characteristics. To support this task, many existing visual analytics systems <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b63">63]</ref> employ basic statistical plots, such as histograms and parallel coordinates, for inspecting each feature of the selected clusters. However, because these visualizations render all of the features' values, they are limited in handling a large number of features. In addition, even if we were able to show all the features, Takanori Fujiwara, Oh-Hyun Kwon, and Kwan-Liu Ma are with University of California, Davis. E-mail: {tfujiwara, kw, klma}@ucdavis.edu it could be very time-consuming to find the common patterns within each cluster or find the differences among the clusters by individually referring to the values for each of the many associated features.</p><p>To address these problems, we have developed an analysis method that highlights those essential features for understanding characteristics of each cluster in a DR result. For our method, we adopt contrastive learning <ref type="bibr" target="#b77">[77]</ref>, a new emerging analysis approach for high-dimensional data. Contrastive learning aims to discover "patterns that are specific to, or enriched in, one dataset relative to another" <ref type="bibr" target="#b3">[4]</ref>. Among the contrastive learning methods, we specifically choose contrastive principal component analysis (cPCA) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b25">25]</ref> and enhance it for visual analysis. Our usage of cPCA, which we call ccPCA (contrasting clusters in PCA), can measure each feature's relative contribution to each cluster's contrast to the others. By referring to these relative contributions, users can easily focus on the features they should review in detail. We describe the strengths of using ccPCA with both numerical formulas and concrete examples. In addition, because cPCA requires parameter tuning to obtain a useful result, we develop an automatic selection method that finds the best parameter value.</p><p>Moreover, we introduce a heatmap-based visualization showing all the features' contributions of each cluster. By employing hierarchical clustering and matrix reordering, our visualization helps the user find where clusters have similar features' contributions or how the features have similar contributions within or across clusters. Additionally, with these methods, we are able to provide a scalable visualization that can handle the case of analyzing many features (e.g., 100 features or more). We have built an interactive visual analytics system using ccPCA and a heatmap-based visualization. We demonstrate the effectiveness of our methods and system with case studies using several publicly available datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>We survey the relevant works in (1) visualization for exploring DR results and (2) discriminant analysis and contrastive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Visualization for Exploring DR Results</head><p>Various visualizations have been developed to assist analysis tasks for a DR result <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b47">47]</ref>. Here, we focus on describing the works that supports the aforementioned main task sequence (i.e., identifying clusters and understanding clusters' characteristics). Stahnke et al. <ref type="bibr" target="#b63">[63]</ref> developed visualizations to help understand multidimensional-scaling (MDS) <ref type="bibr" target="#b67">[67]</ref> results. To support a feature comparison of clusters in the MDS result, their visualization allows the user to manually select clusters and then it depicts the selected clusters' density plots for each of the features. Similarly, for a cluster comparison in the DR results, other works <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b56">56]</ref> visualized statistical charts (e.g., bar charts and boxplots) of the features for each manually or automatically selected cluster. However, because the approaches in <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b63">63]</ref> depict the statistical chart for each feature, they are not scalable when there is a large number of features (e.g., 10 features). Broeksema et al. <ref type="bibr" target="#b13">[14]</ref> took further steps to provide a summary of the DR results. They developed visualizations to help understand patterns that appeared in multiple correspondence analysis (MCA) <ref type="bibr" target="#b2">[3]</ref>, which is a similar DR method as PCA for categorical data. They visualized each data point's salient feature value extracted with MCA as a colored Voronoi cell around each projected point in the MCA result. This linking of the DR result and the salient features helps the user interpret the DR result. Similarly, Joia et al. <ref type="bibr" target="#b36">[36]</ref> linked the DR result and the information of features into one plot. In addition to an automatic selection of clusters, they obtained representative features for each cluster by using PCA. Afterward, they visualized these features' names as a word cloud within each clustered region instead of showing the projected points. Turkay et al. <ref type="bibr" target="#b69">[69]</ref> also used PCA to obtain the representative features in the MDS result.</p><p>Among the mentioned studies, the works by Joia et al. <ref type="bibr" target="#b36">[36]</ref> and Turkay et al. <ref type="bibr" target="#b69">[69]</ref> are most related to ours in terms of identifying the representative features for each cluster. To identify such features, both methods refer to each cluster's principal components (PCs) computed by PCA (and the correlation between the features and PCs). Even though they applied PCA within each cluster, the computed PCs might capture only the global tendency in the dataset. For example, all clusters may have similar or even the same PCs. Also, their methods cannot find features that highly contribute to the differentiation or contrast between one cluster and the others. It is important to provide features that make each cluster's characteristics unique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Discriminant Analysis and Contrastive Learning</head><p>Discriminant analysis, including linear discriminant analysis (LDA) <ref type="bibr" target="#b34">[34]</ref>, quadratic discriminant analysis (QDA) <ref type="bibr" target="#b51">[51]</ref>, and mixture discriminant analysis (MDA) <ref type="bibr" target="#b29">[29]</ref>, is a supervised learning method used for classification and DR. Discriminant analysis methods use labeled data points as a learning set and construct a classifier to distinguish each class as much as possible <ref type="bibr" target="#b34">[34]</ref>. For example, LDA finds new dimensions (or components) which provide good separations between each class. Note that while both PCA and LDA can be categorized as linear DR methods, PCA is an unsupervised method and finds dimensions which maximize the variance of the input data points.</p><p>As similar to PCA, we can obtain the contribution of each original dimension (or feature) to each component constructed by LDA. Therefore, for visual analytics, LDA has been utilized to inform the features which have an important role to distinguish clusters. For example, Wang et al. <ref type="bibr" target="#b72">[72]</ref> developed linear discriminative star coordinates (LDSC). LDSC shows each feature's contribution to distinguishing a cluster from each other as a length of a corresponding axis of the star coordinates <ref type="bibr" target="#b38">[38]</ref>. To obtain a better-clustered result, the user can use these axes as interfaces to discard the less contributed features or change the weight of the features used for clustering.</p><p>While discriminant analysis is used for discriminating the data points based on their classes, contrastive learning <ref type="bibr" target="#b77">[77]</ref> focuses on finding patterns which contrast one dataset with another <ref type="bibr" target="#b3">[4]</ref>. For example, contrastive PCA (cPCA) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b25">25]</ref> is the extended version of PCA for contrastive learning. cPCA takes two different datasets (i.e., target and background), and then identifies the directions (or contrastive principal components) that have a higher variance in the target dataset when compared to the background dataset. Projection of the target dataset with these contrastive principal components provides the patterns which are uniquely found only in the target dataset. In addition to cPCA, several extended methods for contrastive learning have been developed (e.g., contrastive versions of latent Dirichlet allocation <ref type="bibr" target="#b77">[77]</ref>, hidden Markov models <ref type="bibr" target="#b77">[77]</ref>, regressions <ref type="bibr" target="#b25">[25]</ref>, multivariate singular spectrum analysis <ref type="bibr" target="#b19">[19]</ref>, and variational autoencoders <ref type="bibr" target="#b5">[6]</ref>).</p><p>To the best of our knowledge, this paper is the first research using a contrastive learning method, specifically cPCA, for interactive visual analytics. We demonstrate the major advantages of using cPCA instead of PCA or LDA in Sect. 4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">WORKFLOW AND AN ANALYSIS EXAMPLE</head><p>We first define a workflow for analyzing high dimensional data using DR, and then provide an analysis example to motivate our work. <ref type="figure" target="#fig_0">Fig. 1</ref> shows an analysis workflow using our method. It starts from (a) applying a DR method (e.g., MDS, PCA, or t-SNE <ref type="bibr" target="#b71">[71]</ref>) on highdimensional data. Then, the task is (b) to identify clusters in the DR result by applying a clustering method (e.g., k-means <ref type="bibr" target="#b28">[28]</ref>, DB-SCAN <ref type="bibr" target="#b22">[22]</ref>, or spectral clustering <ref type="bibr" target="#b53">[53]</ref>) or selecting clusters manually. Afterward, the task is to understand the clusters' characteristics. This task has two steps. The first step is (c) finding features (or dimensions) which have a high contribution to contrasting each cluster with the others. For this step, we utilize cPCA <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b25">25]</ref>, as described in Sect. 4. The second step is (d) reviewing the detailed differences of values of the highly contributed features between each corresponding cluster and the other data points. We use existing methods for DR and clustering while we introduce new methods for the last two steps. With the last two steps, we can obtain an understanding of which and how features contribute to the uniqueness of each cluster. After understanding the selected clusters' characteristics, as indicated with the arrows from (d) to (a) and (b), the user can update the DR result or clusters by selecting a subset of the data points based on his/her interest, changing the parameters of the algorithms, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Analysis Workflow</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">An Analysis Example</head><p>We analyze the Wine Recognition dataset from UCI Machine Learning Repository <ref type="bibr" target="#b21">[21]</ref> while following the workflow shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. The dataset includes 178 data points (wines) with 13 features (e.g., alcohol, color intensity, and flavanoids). First, to generate a DR result, we use t-SNE <ref type="bibr" target="#b71">[71]</ref> for all of the data points. Then, to detect clusters, we apply DBSCAN <ref type="bibr" target="#b22">[22]</ref> to the DR result. As shown in <ref type="figure" target="#fig_1">Fig. 2a</ref>, we identify three clusters, colored with green, orange, and brown. The black data points are outliers or noise points labeled by DBSCAN. To understand the characteristics of the wines in each cluster, the system immediately applies our cPCA-based analysis method for each detected cluster. Now, we have obtained the features' contributions to contrasting each cluster. The measures of contributions are visualized with a blue-to-red divergent colormap, as indicated in <ref type="figure" target="#fig_1">Fig. 2b</ref>. As the absolute value of the measure approaches 1, the corresponding feature has a higher contribution. Finally, for each cluster, we visualize histograms of values of the three features that have the highest contributions. The results are shown in <ref type="figure" target="#fig_1">Fig. 2c</ref>. The histograms for each target cluster are colored with its respective cluster color, while the others are colored gray. The y-axis shows relative frequency and its maximum limit is set to the maximum relative frequency of each pair of the histograms.</p><p>Based on the result shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, we can easily perceive each cluster's characteristics. For example, the green cluster has higher alcohol percentage ('Alc') and flavanoids when compared to the others. The orange cluster has lower magnesium, proline, and alcohol percentage. Also, the brown cluster has lower OD280/OD315 (i.e., low dilution degree), lower hue, and higher color intensity. The black outliers have higher magnesium and proanthocyanidins ('Proanth').</p><p>Even though this analysis example uses relatively a small number of features and clusters, finding these results is not a trivial task without the suggestions of highly contributed features. For example, in <ref type="figure" target="#fig_2">Fig. 3</ref>, similar to <ref type="bibr" target="#b42">[42]</ref>, we visualize each cluster's feature values with parallel coordinates <ref type="bibr" target="#b33">[33]</ref>. Without our method, to find the same results, the user would need to review all the features of each cluster one by one. This is not only time-consuming but also introduces a possibility of overlooking important characteristics.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODOLOGY</head><p>As demonstrated in Sect. 3.2, when a dataset has many features, even only around ten, reviewing the values for each feature becomes tedious. Finding features which contrast each cluster with the other data points is the core analysis of our approach. To do this, we utilize cPCA <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> and its linearity to obtain the features' contributions (FCs) to the contrast.</p><p>There is a clear advantage of using cPCA over PCA <ref type="bibr" target="#b37">[37]</ref> and LDA <ref type="bibr" target="#b34">[34]</ref>, both of which are linear DR methods. PCA has been used to find the representative features within the selected data points <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b69">69]</ref>. However, as shown in the examples of <ref type="figure" target="#fig_3">Fig. 4</ref>(middle), while PCA is useful to find variations within each cluster, it cannot consider the differences between one cluster and the others. This consideration is important to find the unique characteristics in the target cluster. On the other hand, LDA focuses only on distinguishing the target cluster from the others. Thus, as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>(left), LDA would judge whether a feature has a high contribution to distinguishing the target cluster even in the case where the feature has little variance in the target cluster and zero variance in the others. This could frequently happen especially when the number of features is large. Our cPCA-based method, ccPCA, finds the features which are well-balanced in terms of variety (similar to PCA) and separation (similar to LDA). Also, this balance can be controlled with the contrast parameter, as described in Sect. 4.2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Contrastive PCA (cPCA)</head><p>We provide a brief introduction to cPCA [4, 5, 25], which we utilize to find features contrasting a target cluster with the other data points. We compare LDA, PCA, and ccPCA. All of these methods can calculate the features' relative contributions to the first component by respectively referring to either LDA's loadings, PC loadings, or cPC loadings described in Sect. 4.3. We scale each loading in the range from -1 to 1 by dividing the maximum absolute value of the loadings. We visualize the scaled loading for each pixel with a blue-to-red colormap. For LDA, we perform classification between the target digit and the others. The LDA results, placed on the left column, show that the outside pixels have high contributions. We can consider that LDA tries to distinguish each target digit from the others by referring to the pixels that are less frequently used in the other digits. We apply PCA to each target cluster in the same manner as <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b69">69]</ref>. We can see that the PCA results show variations of the strokes when drawing each digit. The cPCA results are obtained from ccPCA with the automatic selection of Î± (refer to Sect. 4.2). When compared with PCA, the cPCA results clearly show the strokes contrasting the target digit with the others. For example, for Digit 5, the pixels on the upper right have high contributions, as indicated in dark red. When only drawing Digit 5, we tend to use these pixels, and thus, we can see that cPCA captures Digit 5's characteristics. Similarly, for Digit 4, we can see that there are dark red pixels around the middle left.</p><p>cPCA is developed for "the setting where we have multiple datasets and are interested in discovering patterns that are specific to, or enriched in, one dataset relative to another" <ref type="bibr" target="#b3">[4]</ref>. For instance, from the examples in <ref type="bibr" target="#b3">[4]</ref>, when we have a medical dataset X of diseased patients, we would want to find trends and variations of the disease's influence. If we apply the classical PCA <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b37">37]</ref> to X, the first principal component  <ref type="bibr" target="#b30">[30]</ref> from <ref type="bibr" target="#b3">[4]</ref>. A different contrast parameter Î± value is used for each result. When Î± = 0, cPCA generates the same result when applying PCA to the target dataset. In this result, we cannot see clear differences between down syndrome (DS) and non-DS mice, indicated with red and blue points, respectively. While clear differences between DS and non-DS start to appear when Î± = 1.7, we can see that DS is further separated into two groups when Î± = 36.7. More examples can be found in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>would only present the diseased patients' demographic variations <ref type="bibr" target="#b24">[24]</ref>, instead of showing the variation of the disease's effects. However, if there is another medical dataset Y of healthy patients, cPCA can utilize the fact that Y could have similar demographic variations as X, and no variations related to the disease. By taking X and Y as the target and background datasets, respectively, cPCA can find the directions (or components) in which X has high variance but Y has low variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Description of the Algorithm</head><p>Now, we describe how cPCA obtains such directions by using the target and background datasets. Let X = {x i } n i=1 be the target dataset and Y = {y i } m i=1 be the background dataset where x i , y i âˆˆ R d , n and m are the numbers of data points, and d is the number of dimensions (or features). Similar to the classical PCA, for the first step, cPCA applies centering to each dimension of X and Y and then obtains their corresponding empirical covariance matrices C X and C Y . Let v be any unit vector of d dimensions.</p><p>Then, with a given direction v, the variances for the target and background datasets can be written as:</p><formula xml:id="formula_0">Î» X (v) def = v T C X v, Î» Y (v) def = v T C Y v.</formula><p>Now, the optimization that finds a direction v * where X has high variance but Y has low variance can be written as:</p><formula xml:id="formula_1">v * = argmax v Î» X (v) âˆ’ Î±Î» Y (v) = argmax v v T (C X âˆ’ Î±C Y )v<label>(1)</label></formula><p>where Î± is a contrast parameter (0 â‰¤ Î± â‰¤ âˆž). We describe the details of Î± in Sect. 4.1.2. From Eq. 1, we can see that v * corresponds to the first eigenvector of the matrix C</p><formula xml:id="formula_2">def = (C X âˆ’ Î±C Y ).</formula><p>The eigenvectors of C can be calculated with eigenvalue decomposition (EVD). These computed eigenvectors are called contrastive principal components (cPCs) and are orthogonal to each other. Similar to the classical PCA, by using these cPCs (typically two cPCs), we can plot the DR result of X. An example from <ref type="bibr" target="#b3">[4]</ref> is shown in <ref type="figure" target="#fig_4">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">The Contrast Parameter and Semi-Automatic Selection</head><p>The contrast parameter Î± controls the trade-off between having high target variance and low background variance. When Î± = 0, cPCs will only maximize the variance of the target dataset. These cPCs are the same as the principal components (PCs) of the target dataset when computed with the classical PCA. As Î± increases, cPCs will become more optimal directions that reduces the variance of the background dataset. <ref type="figure" target="#fig_4">Fig. 5</ref> shows the example from <ref type="bibr" target="#b3">[4]</ref> with different Î± values.</p><p>As shown in <ref type="figure" target="#fig_4">Fig. 5</ref>, the selection of Î± has a strong impact on the DR result. Thus, Abid and Zhang et al. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> introduced an algorithm suggesting multiple Î± values. Their algorithm calculates a set of cPCs for each of the multiple values of Î± (with 40 values as their default), and the Î± values are logarithmically spaced in a certain range (the default is between 0.1 and 1000). Then, the similarity between each pair of the different cPCs, each obtained with a different Î± value, is measured by calculating the product of the cosine of the principal angles. Afterward, based on the user's input p (the number of values of Î± to suggest), the algorithm finds p clusters from the similarities with spectral clustering <ref type="bibr" target="#b53">[53]</ref>. Finally, the algorithm returns p values of Î± which correspond to the medoids of the clusters. From the suggested p Here, we try to find the (c)PC contrasting the green cluster. In (a), we apply the classical PCA to the entire dataset. Though there is a separation of the green cluster when using the first and second PCs, there are overlaps of the green and orange clusters when only using the first PC (PC 1). In (b), the data points in the green cluster are used as the target dataset and the other data points are used as the background dataset. Î± value is selected from the suggestions using the semi-automatic selection in Sect. 4.1.2. We cannot see a clear separation of the green cluster from the others. In (c), we use the entire data points instead of only the green cluster as the target dataset. Î± value is selected with our automatic selection method in Sect. 4.2.3. We can see a better separation when compared to that of (a) and (b) even when using only the first cPC (cPC 1).</p><p>values, the algorithm returns a set of DR results. By referring to this set, the user can choose their preferred Î± value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Finding the Direction that Contrasts a Target Cluster</head><p>As described above, cPCA discovers patterns that are specific to, or enriched in, the target dataset relative to the background dataset. In <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, cPCA is designed for the situation where the patterns the user wants to identify are included within the target dataset X, while the background dataset Y contains the structure the user wants to remove from the target dataset. Therefore, in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, the provided examples for {X, Y } are {'diseased subjects', 'control group subjects'}, {'patients after treatment', 'patients before treatment'}, {'images mixed with interests and noises', 'images only including noises'}, etc.</p><p>In our case, we want to find the directions (i.e., cPCs) which contrast one cluster with the other data points. If we follow the examples of X and Y as stated above, X can be the target cluster and Y can be the other data points. However, in this case, cPCA will find cPCs that only enrich the variations specific to the target cluster. For example, when the target cluster includes diseased subjects and the other data points correspond to healthy subjects, cPCA will find enriched variations within the diseased subjects (e.g., differences among multiple diseases), but will not consider the differences between diseased and healthy subjects.</p><p>To utilize cPCA for finding the directions contrasting a target cluster with the others, we introduce a novel usage of cPCA, named ccPCA. Instead of using the target cluster as the target dataset X and the other data points as the background dataset Y , we use the entire dataset as X and the data points other than the target cluster as Y . With this approach, we can find the directions that contrast the target cluster. As we describe in the following subsections, ccPCA has the strengths in regards to two aspects: (1) an implicit extension of the contrast parameter Î± and (2) a proper setting of the centroid. The DR results shown in <ref type="figure">Fig. 6</ref> provide a comparison of the classical PCA, original usage of cPCA (i.e., using only the target dataset as X), and ccPCA.</p><p>Let E = {e i } s i=1 be the entire dataset and K = {k i } t i=1 be the target cluster (K âŠ‚ E, e i , k i âˆˆ R d , s and t are the numbers of data points). Then, we denote R = {r i } u i=1 as the difference of the two sets K and E (i.e., R = E \ K and u = s âˆ’ t). With these notations, we can say that ccPCA uses E and R as the target X and background Y datasets, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">An Implicit Extension of the Contrast Parameter</head><p>To provide a simple and clear explanation, we assume the centering effects to the datasets E, K, and R are all the same (i.e., E, K, and R have the same mean value for each feature). After centering the target dataset E and the background dataset R, cPCA obtains their corresponding  <ref type="figure">Fig. 7</ref>: A comparison of centering effects to the cPCA results. For this example, we generate two sets of data points from different 2D Gaussian distributions. In (a), the green circle and arrow show the centroid and the first cPC when using the target cluster K as the target dataset and the others as the background dataset. The blue circle and arrow are the centroid and the first cPC when using the entire dataset E as the target dataset. From the DR results, as shown in (b) and (c), ccPCA, using the entire dataset E as the target dataset, generates a better separation between the target cluster and the others.</p><p>empirical covariance matrices C E and C R . Then, cPCA calculates cPCs by performing EVD to C E âˆ’ Î±C R . Let C K be the empirical covariance matrix of the target cluster K after centering. Because</p><formula xml:id="formula_3">C K = âˆ‘ t i=1 k i k T i /t, C R = âˆ‘ u i=1 r i r T i /u, E = K R, and s = u + t, C E can be represented as C E = (tC K + uC R )/s. With this, C E âˆ’ Î±C R can be rewritten as: C E âˆ’ Î±C R = (tC K + uC R )/s âˆ’ Î±C R (2) = t s C K âˆ’ (sÎ± âˆ’ u) t C R = t s (C K âˆ’ Î² C R )<label>(3)</label></formula><p>where</p><formula xml:id="formula_4">Î² = (sÎ± âˆ’ u)/t. Because 0 â‰¤ Î± â‰¤ âˆž, âˆ’u/t â‰¤ Î² â‰¤ âˆž.</formula><p>Note that if we use K and R as the target and background datasets, respectively, cPCA performs EVD to C K âˆ’ Î±C R . Therefore, a fundamental difference between the cases of using E (i.e., the entire dataset) and using K (i.e., only the target cluster) as the target dataset for cPCA is the difference between Î± and Î² . While Î± only takes a non-negative value, Î² can be a negative value. When Î² = âˆ’u/t, cPCA selects the directions that maximize the variance of the entire dataset E, and hence reduces to PCA applied on E. As Î² increases to 0, cPCA provides more weight to the target cluster K than the others R to select the directions. When Î² = 0, cPCA selects the directions that maximize the variance of the target cluster K, and hence reduces to PCA applied on K. Then, as Î² increases from 0 to âˆž, the directions from cPCA will become more optimal to reduce the variance of the others R. While Eq. 3 with Î² &gt;= 0 has a capability to find the same directions with C K âˆ’ Î±C R , ccPCA also searches the directions that considers the differences between the target cluster K and the others R by using the range Î² &lt; 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">The Centering of the Target Dataset</head><p>ccPCA not only implicitly extends the searching range of Î± of C K âˆ’ Î±C R , but it also uses a proper centroid of the dataset. The centering (i.e., the mean subtraction for each feature) in cPCA is used for translating the dataset to its centroid. When using K as the target dataset, the centroid is calculated from only the target cluster K. In contrast, ccPCA uses E as the target dataset, and the centroid is calculated from all the data points. <ref type="figure">Fig. 7</ref> shows an example of the two methods of calculating the centroid and the first cPC in each case. As the same reason as the classical PCA, the centering should be applied to the entire dataset in our case. This is to ensure that the first cPC is the direction of the maximum variance, which contrasts the differences between the target cluster and the others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Automatic Selection of the Best Contrast Parameter</head><p>The selection of the contrast parameter Î± is the remaining procedure. Even though we can use the existing semi-automatic selection of Î± in Sect. 4.1.2, selecting the best alpha from the multiple suggested options is tedious when analyzing multiple clusters. Thus, we introduce a method for an automatic selection of the best Î± for our usage. The pseudocode of this method is available in the Supplementary Materials <ref type="bibr" target="#b0">[1]</ref>. To understand the characteristics of the cluster, we should find the first cPC which not only (1) shows a clear separation between the target cluster from the others, but also (2) maintains the variability in the target cluster well (i.e., a high variance within the target cluster). Similar to the classical PCA, the second condition tries to preserve the target clusters' original structure. Without the second condition, when using a large Î±, cPCA may preferentially select features where the target cluster only has subtle variability, but the other data points have no variability (i.e., zero variance). This example can be seen in the far right of <ref type="figure">Fig. 8</ref>.</p><p>Similar to the semi-automatic selection in Sect. 4.1.2, our automatic selection lists multiple candidates of Î± (our default is also 40 values). These candidates consist of 0 and a set of logarithmically spaced values given a certain range (our default also ranges from 0.1 to 1000). We denote these alphas as {Î± i } q i=1 (q is the number of candidate values for the best Î±) and assume {Î± i } is sorted by ascending order (i.e., Î± 1 = 0). Then our method selects a value that obtains the best separation while having enough variance in the target cluster K.</p><p>To measure the separation between the target cluster and the others along the first cPC, we use the histogram intersection (HI) <ref type="bibr" target="#b64">[64]</ref>, which can measure the overlaps of the histograms of the two sets. While there are many different (dis)similarity measures between two probability distributions, such as the Kullback-Leibler divergence <ref type="bibr" target="#b41">[41]</ref>, we chose HI for its robustness to outliers and low computational cost. Let</p><formula xml:id="formula_5">H A = {h A j } b j=1 , H B = {h B j } b j=1</formula><p>be the histograms of two given sets of real numbers A and B where b is the number of bins, h A j and h B j are the numbers of data points in the j-th bin of A and B, respectively. Both H A and H B have the same bins. We decide the bin-width using Scott's normal reference rule <ref type="bibr" target="#b62">[62]</ref> from the set of real numbers obtained by combining A and B. The HI of the two sets A and B is defined as:</p><formula xml:id="formula_6">I(A, B) = âˆ‘ b j=1 min(h A j , h B j )</formula><p>. Let K i and R i be the data points of 1D DR results of K and R with the first cPC corresponding to the i-th candidate Î± value (i.e., Î± i ), respectively. Then, we can calculate the measurement of separation with the inverse HI (i.e., I(K i , R i ) âˆ’1 ) for each Î± i . We refer I(K i , R i ) âˆ’1 as the discrepancy score D(Î± i ).</p><p>For the variance of K i , to handle the scaling differences in each DR result, first, we apply the min-max scaling to K i with the minimum and maximum values of K i R i . Then, we calculate the variance of the scaled K i . We denote this variance of K i as V (Î± i ).</p><p>With the measures of D(Î± i ) and V (Î± i ), our automatic selection method selects the best alpha with: argmax</p><formula xml:id="formula_7">Î± i âˆˆ{Î± 1 ,...,Î± q } D(Î± i ) s.t. V (Î± i ) â‰¥ Î³V (Î± 1 )<label>(4)</label></formula><p>where Î³ (Î³ â‰¥ 0) is a ratio that controls the threshold of the variance V (Î± i ). Note that V (Î± 1 ) is the variance of K 1 of the cPCA result with Î± = 0, which will be the same result when applying the classical PCA to the entire dataset E. While our method allows the user to select any non-negative value for Î³, we set Î³ = 0.5 as the default to ensure that V (Î± i ) has at least a half of V (Î± 1 ). <ref type="figure">Fig. 8</ref> shows the cPCA results with different Î± values. Our automatic Î± selection chooses Î± = 1.06 in this case. More comprehensive experimental results with various datasets and Î± values can be found in the <ref type="figure" target="#fig_0">Supplementary Materials [1]</ref>. In summary, the original cPCA is enhanced as ccPCA by using Eq. 1 with X = E and Y = E \ K and by selecting Î± as the solution to Eq. 4. Parallel calculation of the best contrast parameter: The original semi-automatic selection of the contrast parameter in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> calculates cPCA for each Î± i âˆˆ {Î± i } q i=1 in serial <ref type="bibr" target="#b1">[2]</ref> (q = 40 by default). Because the calculation of cPCA for each Î± i is independent of each other, in order to achieve faster computation, our method uses multi-threads and calculates each cPCA result, D(Î± i ), and V (Î± i ) in parallel. The comparison of the completion time of the original cPCA and our implementation with and without parallelization is available in <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Features' Relative Contributions to the First cPC</head><p>By using cPCA, with our automatically selected Î±, we can now obtain the direction (i.e., the first cPC) that contrasts the target cluster. Next, <ref type="figure">Fig. 8</ref>: The DR results with the first and second cPCs (top row) and the features' (i.e., pixels') relative contributions (bottom row) of the MNIST dataset <ref type="bibr" target="#b45">[45]</ref> with different Î± values (refer to Sect. 4.3 about the features' relative contributions). Here, we try to contrast Digit 1 with the other digits. We can see that when Î± = 0 (reduced to applying the classical PCA for all digits), cPCA does not separate Digit 1, and the features' contributions do not show any useful information to understand the characteristics of Digit 1. On the other hand, when Î± = 22.85, while some of Digit 1 (e.g., points placed on the top left) are well separated, the variance V is small. Also, from the features' contributions, we can see that only a few pixels in the lower left have high contributions. This is expected because these pixels are rarely used when drawing Digit 1. The result with Î± = 1.06 produces the best discrepancy score D and a large variance V . This Î± will be selected by Eq. 4. Also, we can see that cPCA highlights the pixels around the center, which are typically used for drawing Digit 1.</p><p>we determine how strongly each feature of the target cluster contributes to this direction. Similar to the classical PCA, by using the top eigenvalue Î» * and the corresponding eigenvector v * (i.e., the first cPC) of the matrix C E âˆ’ Î±C R , the relative contributions can be calculated with:</p><formula xml:id="formula_8">w * = âˆš Î» * v * where w * = {w * i } d i=1 (âˆ’1 â‰¤ w * i â‰¤ 1)</formula><p>. Analogous to the classical PCA, we call w * the cPC loadings of the first cPC. As |w i | approaches 1, the i-th feature has a stronger contribution (or correlation) to the first cPC. Based on this value, we can decide which features we should review to understand the target cluster. <ref type="figure" target="#fig_3">Fig. 4</ref> shows an example of the features' contributions and comparisons with the results from LDA and PCA. Comprehensive comparisons of LDA and PCA, using multiple datasets, can be found in the Supplementary Materials <ref type="bibr" target="#b0">[1]</ref>. As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, signed cPC loadings can clearly differentiate features whose positive centered values contribute to the negative or positive direction of the first cPC by using blue and red, respectively. This is as opposed to taking the absolute value of the signed cPC loadings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">VISUAL ANALYTICS SYSTEM</head><p>To demonstrate our methods of analyzing real-world datasets, we develop a prototype system that supports the analysis workflow shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. A major portion of the system's functionality and a video of an interaction demonstration are available in our online site <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dimensionality Reduction View</head><p>The dimensionality reduction (DR) view, as shown in <ref type="figure" target="#fig_1">Fig. 2a</ref>, is used for the first two processes: generating a DR result and identifying clusters. In this view, first, the user can visualize a 2D DR result of a high-dimensional dataset. We employ t-SNE <ref type="bibr" target="#b71">[71]</ref> (specifically, Barnes-Hut t-SNE implementation <ref type="bibr" target="#b70">[70]</ref>) as a DR method because t-SNE can effectively depict the local structure of the dataset, and thus, it is useful to visually identify the clusters within the dataset. From the settings in <ref type="figure" target="#fig_1">Fig. 2d</ref>, the user can adjust the perplexity parameter of t-SNE, which controls a balance of the effects from local and global structures of the dataset <ref type="bibr" target="#b71">[71]</ref>. While a larger perplexity will preserve more of the distance relationship in the global structure, a smaller perplexity will focus on more preserving the distance relationship among a small number of neighbors.</p><p>After obtaining the DR result with t-SNE, the user can identify clusters automatically or manually. As a default, the automatic clustering method will be immediately applied to the obtained DR result. As part of the automatic method, our system supports DBSCAN <ref type="bibr" target="#b22">[22]</ref> because the density-based clustering algorithm is able to identify clusters with arbitrary shapes <ref type="bibr" target="#b57">[57]</ref>, which are often generated from DR. The user can change the parameters required for DBSCAN from the settings in <ref type="figure" target="#fig_1">Fig. 2d</ref>. The categorical color of each point in the DR result is assigned to the clustering label obtained from DBSCAN. The color black, in particular, is used to represent outliers or noise points labeled by DBSCAN. For a manual selection of a cluster, the system supports a rectangle selection. The user can select data points by drawing a rectangle with mouse dragging in the DR result. Also, the user can add additional data points or unselect data points by using different selection modes provided in the system. From these interactions, the user can create a new cluster consisting of the selected points by clicking the "Add Cluster" button placed at the top of <ref type="figure" target="#fig_1">Fig. 2b</ref>. The system also supports basic view-level interactions, such as zooming and panning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Features' Contributions View</head><p>The two remaining processes (i.e., finding features contrasting each cluster and comparing the features' values in detail) are performed with the features' contributions (FCs) view shown in <ref type="figure" target="#fig_1">Fig. 2b</ref>. In the FCs view, the FCs contrasting each cluster described in Sect. 4.3 are visualized as a heatmap. While each row name shows the corresponding feature, each column name shows the cluster label ('Z' is used to represent the outliers, noise points, or both). Also, to indicate the corresponding cluster in the DR view, the background of each column name is colored with the corresponding color. We scale each cluster's FCs in the range from âˆ’1 to 1 by dividing each FC by the maximum absolute value of the FCs (e.g., the original range from âˆ’0.1 to 0.5 will be changed to the range from âˆ’0.2 to 1.0). Then, we encode the scaled FCs with a blueto-red colormap. In the next subsections, we describe our algorithm organizing the heatmap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Optimal Sign Flipping of cPCs and FCs</head><p>Similar to the classical PCA, cPCA has the "sign ambiguity" problem <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b35">35]</ref>. Because of this problem, arbitrary sign flipping in each (c)PC occurs when performing EVD. An example of sign flipping in cPCA is shown in <ref type="figure" target="#fig_7">Fig. 9</ref>. Sign ambiguity affects the comparison of the FCs among the clusters. Each cluster might have the opposite direction of the first cPC only due to this sign ambiguity problem. In this case, the FCs also have opposite signs, and thus, it is difficult to judge whether these clusters have similar patterns in the FCs or not.</p><p>To solve this problem as much as possible, we introduce a method to optimally reduce unnecessary sign flipping. Let v * i and v * j be the first cPCs of i-th and j-th clusters, respectively. We can measure how the directions v * i and v * j are similar with the cosine similarity</p><formula xml:id="formula_9">sim(i, j) = v * i â€¢ v * j /( v * i v * j ). v * i and v *</formula><p>j have the same direction when sim(i, j) = 1, while v * i and v * j have opposite directions when sim(i, j) = âˆ’1. Ideally, by flipping the signs of the first cPCs of some clusters, we want to ensure that all of the clusters' first cPCs face the same side (i.e., sim(i, j) â‰¥ 0 âˆ€i, j). However, the sign flipping to a certain cluster affects all cosine similarities related to this particular cluster. Thus, in many cases, it is theoretically impossible to obtain the result stated above. However, alternatively, we can maximize the sum of all sim(i, j) with sign flipping. This optimization can be written as:</p><formula xml:id="formula_10">argmax Ï•={Ï• i ,...,Ï• l } l âˆ‘ i=1 l âˆ‘ j=1, j =i (Ï• i v * i ) â€¢ (Ï• j v * j ) v * i v * j = l âˆ‘ i=1 l âˆ‘ j=1, j =i Ï• i Ï• j sim(i, j) s.t. Ï• i , Ï• j âˆˆ {âˆ’1, 1} (5)</formula><p>where l is the number of clusters and Ï• is a set of signs. We solve Eq. 5 with a heuristic approach. We initialize Ï• = {1, 1,...,1}. We can expect that there is a higher chance to obtain a better result if we start to flip the sign where i-th cluster has the largest negative value in the sum of the similarities (âˆ‘ l j=1 Ï• i Ï• j sim(i, j)). Therefore, our approach first checks whether sign flipping to the first cPC of such a cluster provides a better result in the objective function of Eq. 5. If so, we flip its first cPC's sign. Then, we repeatedly apply this procedure until âˆ‘ l j=1 Ï• i Ï• j sim(i, j) â‰¥ 0 for all i âˆˆ {1, 2,...,l} is satisfied or all clusters have been checked. Afterward, based on the optimized set Ï•, we allocate the new signs to respective cPC and FCs for each cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Ordering of Features and Clusters</head><p>The FCs view can be used for finding not only the heatmap cells which have high FCs, but also the clusters which have similar FC patterns; the features which have similar FCs within and/or among clusters. The case when the clusters have similar FCs implies that these clusters are contrasted due to the same features, but they have different distributions in their features' values. When the features have similar FCs, by reviewing the distributions of one of these features' values, we can expect that the other features may also have similar distributions.</p><p>To help find these patterns, our system applies reordering of the features (i.e, rows) and clusters (i.e., columns) based on the FCs. Ordering choice is important since this affects how easily we can find patterns in a heatmap <ref type="bibr" target="#b9">[10]</ref>. We use a hierarchical clustering, specifically the complete-linkage method <ref type="bibr" target="#b52">[52]</ref>, with the optimal-leaf-ordering <ref type="bibr" target="#b8">[9]</ref>. Recent survey <ref type="bibr" target="#b9">[10]</ref> reported that this combination tends to produce a coherent and quality result to help reveal patterns. <ref type="figure" target="#fig_0">Fig. 10a and b</ref> show the results before and after the reordering. From <ref type="figure" target="#fig_0">Fig. 10b</ref>, we can easily see a group of similar FCs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Scalable Visualization</head><p>When the number of features is large (e.g., 100 or more), the heatmapbased visualization would have a scalability issue. Moreover, in this case, many features could have high FCs, and as a result, it would still be difficult to decide which features we should review in detail. To solve this issue, we introduce an aggregation method, utilizing the hierarchical clustering result obtained through the reordering method.</p><p>When the number of features is larger than threshold Î´ (we set Î´ = 40 as a default), our method obtains Î´ clusters from the features by referring to the hierarchical clustering result. Then, our method aggregates the FCs into one representative value: the mean or the maximum absolute value. As a default, our method takes the maximum absolute value to show the most prominent feature. <ref type="figure" target="#fig_0">Fig. 10c</ref> shows an example of the aggregation. Additionally, to provide a representative name for each aggregated feature, our method chooses the name based on which FC has the maximum absolute value. With this name, our method also shows how many features are aggregated in each row, as shown with a purple underline on the right side of <ref type="figure" target="#fig_0">Fig. 10c</ref> ('PctKids2Par, 9 more').</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Interactions between Views</head><p>From DR View: When the user updates the clusters with the clustering method in the DR view, the FCs view updates the heatmap with the reordering (and aggregation) method(s). When the user adds a new cluster manually, the FCs view updates the heatmap with the new cluster. From FCs View: The FCs view can be used as an interface to compare the details of the features' values within/across features or clusters. When the user places the mouse over a certain heatmap cell, the system shows a popup window of the histograms of feature values of the corresponding cluster and the others (e.g., <ref type="figure" target="#fig_0">Fig. 2c and Fig. 14b)</ref>. We color the selected cluster's histogram with a categorical color representing its cluster label, while the gray color is used for the other data points' histogram. When hovering over a certain (representative) feature name, the system shows a value of the (representative) feature as the size of each data point in the DR view (e.g., <ref type="figure" target="#fig_0">Fig. 12a and Fig. 13a)</ref>.</p><p>Moreover, when hovering over a certain cluster label, the system highlights the corresponding cluster in the DR view. In addition, with the popup window, the system visualizes the histograms of 1D DR results of the cluster and the others. From these histograms, the user can grasp how well the cluster is contrasted with the other data points. Additionally, the system shows the histograms of three (representative) feature values that have the highest absolute FCs. These histograms are useful to understand each cluster's characteristics quickly.</p><p>Also, to make the comparison within/across features or clusters easier, our system allows the user to prevent the histograms from disappearing with a mouse-click. The clicked histograms can also be moved with mouse-dragging. The corresponding heatmap cell for each histogram is annotated with a gray line and a pair of numbers shown in the heatmap cell and the histogram (e.g., <ref type="figure" target="#fig_0">Fig. 2 and Fig. 14b</ref>). The gray line can be turned on or off by clicking the "Show/Hide Histogram Indicator" placed at the top of the FCs view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Implementation</head><p>We have developed our system as a web application. To achieve fast calculation, we have implemented our methods described in Sect. 4 with C++ and Eigen library <ref type="bibr" target="#b26">[26]</ref> for linear algebraic calculations. We have also provided Python bindings for our C++ implementation. The source code is available in <ref type="bibr" target="#b0">[1]</ref>. The back-end of the system uses Python with the stated bindings. The front-end visualization is implemented with a combination of Elm <ref type="bibr" target="#b17">[17]</ref>, HTML5, JavaScript, WebGL, and D3 <ref type="bibr" target="#b10">[11]</ref>. While we use D3 for the FCs view, WebGL is used to render the data points efficiently for the DR view. We use WebSocket to communicate between the front-and back-ends.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CASE STUDIES</head><p>We have shown the effectiveness of our methods with the Wine Recognition <ref type="bibr" target="#b21">[21]</ref> and MNIST <ref type="bibr" target="#b45">[45]</ref> datasets in the previous sections. We demonstrate three additional case studies with publicly available datasets. For each case study, we preprocess the corresponding dataset to clean up missing values in the data or extract useful information for the analysis. All the preprocessed datasets are available online <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Tennis Major Tournament Match Statistics</head><p>We analyze the Tennis Major Tournament Match Statistics dataset from UCI Machine Learning Repository <ref type="bibr" target="#b21">[21]</ref>. This dataset contains the match statistics for both females and males at four major tennis tournaments in 2013. The statistics include first serve won by each player, double faults committed by each player, etc. From this dataset, we obtain female players' mean values for each statistic across all tournaments. The obtained dataset consists of 174 data points (tennis players) and 13 features (statistics).</p><p>Similar to the analysis of Sect. 3.2, we obtain the DR result with t-SNE, clusters with DBSCAN, and FCs with our methods. Then, to analyze each cluster's characteristics, we show the histograms of the top 3 contributed features. The result is shown in <ref type="figure" target="#fig_0">Fig. 11</ref>.  From <ref type="figure" target="#fig_0">Fig. 11</ref>, we can see that each cluster has a different playing style. For example, the purple cluster tends to have low 'DBF' (double faults committed by player), high 'BPC' (break points created by player), and high 'FNL' (final number of games won by player). This indicates that these players had fewer mistakes in their serves and performed well when they were the receiver, and as a result, they won more games. Similarly, the orange cluster has high 'WNR' (winners earned by player) and 'NPA' (net points attempted by player). These statistics will tend to be higher when a player tries to obtain points aggressively during a rally. On the other hand, the brown cluster has low 'WNR' but high 'FSW' (first serve won by player) and 'TPW' (total points won by player). Therefore, we can say that these players tend to obtain more points with their serves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Food and Nutrient</head><p>We analyze the Nutrient dataset in the USDA Food Composition Databases <ref type="bibr" target="#b66">[66]</ref> as an analysis example with a large number of data points. We use the version available from <ref type="bibr" target="#b16">[16]</ref>. This dataset consists of the nutrient content for each food. The dataset has 7,637 data points (foods) and 14 features (nutrients).</p><p>This dataset has 12,507 missing values and this is 11.7% of all the values. Since this high percentage of missing values could affect an analysis result <ref type="bibr" target="#b6">[7]</ref>, we first preprocess the dataset to reduce this ratio to less than 5% <ref type="bibr" target="#b6">[7]</ref>. We remove features where more than 40% of the values are missing. Also, we remove data points where more than 40% of the feature values are missing. Afterward, 7,499 data points and 12 features remain and there are 4,447 missing values (4.9% of all the values). We replace the missing values with the mean of each corresponding feature.</p><p>The result after using t-SNE, DBSCAN, and our methods is shown in <ref type="figure" target="#fig_0">Fig. 12</ref>. As shown in <ref type="figure" target="#fig_0">Fig. 12b</ref>, we can see that all clusters except for the brown cluster have high FCs in 'calories', 'fat', or both. When comparing the histograms of 'calories' and 'fat' for each cluster, as shown in <ref type="figure" target="#fig_0">Fig. 12c</ref>, each cluster, in fact, has different distributions in 'calories' and 'fat'. For example, while the yellow cluster tends to have low calories and fat, the orange cluster tends to have high values for both.</p><p>We have understood the main characteristics of each cluster. However, the effects of the two specific features ('calories' and 'fat') are too dominant. As a result, we cannot find any other interesting patterns. We preprocess the dataset to filter out these two features and generate a new result with new cluster labels, as shown in <ref type="figure" target="#fig_0">Fig. 13</ref>. At this time, from <ref type="figure" target="#fig_0">Fig. 13b</ref>, we can see that most of the clusters are contrasted by mainly 'water', 'carbohydrate', or both. For example, the purple and orange clusters placed in the upper left of <ref type="figure" target="#fig_0">Fig. 13a</ref> have fewer carbohydrates and more water when compared with the pink and green clusters, as shown in <ref type="figure" target="#fig_0">Fig. 13c</ref>. These two examples show that the FCs are useful to know which features have a dominant effect on cluster forming in the DR result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Communities and Crime</head><p>As an example with a large number of features, we analyze the Communities and Crime dataset <ref type="bibr" target="#b59">[59]</ref> from <ref type="bibr" target="#b21">[21]</ref>. This dataset consists of both socio-economic and crime statistics (e.g., the median family income and the number of murders) for each community. The dataset contains 2,215 data points (communities) and 143 features (statistics) after excluding identifiers (e.g., county codes).</p><p>Because this dataset has many missing values <ref type="bibr">(42,147 values, 13</ref>.3% of all the values), as similar to Sect. 6.2, we remove the features where more than 80% of the values are missing. The dataset now has 121 features and only 963 missing values (0.4% of all the values). We replace the missing values with the mean of each corresponding feature. <ref type="figure" target="#fig_0">Fig. 14a (top)</ref> shows the result after DR and clustering. As indicated with the purple rectangle, we manually select an additional cluster as a pink cluster. Then, we obtain the FCs, as shown in <ref type="figure" target="#fig_0">Fig. 14b</ref>. Because there are many features, the system has aggregated them into 40 features using the aggregation method described in Sect. 5.2.3. <ref type="figure" target="#fig_0">From Fig. 14b</ref>, we can say that the small clusters (yellow, purple, brown, orange, and pink) are separated from the green cluster due to race, house size, etc.not due to the criminal statistics. For instance, as indicated with the green rectangles, the brown cluster has high FCs in race percentages of African Americans and Caucasians ('racepctblack' and 'racePctWhite'). Also, the pink cluster has high FC in 'PctLargHouseOccup' (percentage of all occupied households that are large).</p><p>We show the histograms of the features aggregated to the 'Pct-LargHouseOccup and 1 more', as shown in the lower left of <ref type="figure" target="#fig_0">Fig. 14b</ref>. We can see that both 'PctLargHouseOccup' and 'PctLargHouseFam' (percentage of family households that are large) have similar distribution patterns. These patterns can be found because our aggregation method is performed after applying the optimal sign flipping and ordering described in Sect. 5.2. Our aggregation method is able to provide a scalable visualization and help the user analyze many features. Another example for 'PctPopUnderPov and 3 more' of the orange cluster is shown in the upper left of <ref type="figure" target="#fig_0">Fig. 14b</ref>. All 'PctPopUnderPov' (percentage of people under the poverty level), 'agePct12t21' (percentage of population that is 12-21), 'agePct12t29' (percentage of population that is 12-29), and 'MalePctNevMarr' (percentage of males who have never married) tend to have a higher value in comparison to that of others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION AND LIMITATIONS</head><p>Generality of our method. We utilize cPCA <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> to find features contrasting the target cluster. We discuss the reason why we use this approach instead of analyzing how the DR method generates clusters. If possible, the latter approach would be effective because the cluster formation is a result of the DR method. However, many of the nonlinear DR methods used for visualization (e.g., t-SNE <ref type="bibr" target="#b71">[71]</ref>, LargeVis <ref type="bibr" target="#b65">[65]</ref>, and UMAP <ref type="bibr" target="#b50">[50]</ref>) generate irreversible low-dimensional projection of the original data structure. These methods do not have a parametric mapping between the original and projected dimensions; therefore, it is difficult to provide information about how these DR methods affect cluster forming. Our methods provide flexibility for analyzing results from any type of DR methods.</p><p>We introduce using cPCA to understand the characteristics of the clusters identified in the DR result. Our methods can also be used in other situations. For example, even though using DR before clustering is a common approach <ref type="bibr" target="#b74">[74,</ref><ref type="bibr" target="#b76">76]</ref>, our methods can support visual analytics of clusters that are obtained from the clustering methods without going through the DR step. This would be helpful to understand clusters' characteristics and to analyze the quality of the clustering methods without any effects derived from DR (e.g., distortion in the projection space). Another example is applying our methods to labeled data. Our methods can identify the essential features to contrast a labeled group from the others. Therefore, our methods would be useful to understand the characteristics of each group and could help design classifiers based on the gained knowledge. Our prototype system can support these types of analysis by changing the parts related to steps (a) and (b) in <ref type="figure" target="#fig_0">Fig. 1</ref>, such as the DR view and clustering algorithms. Advantages of using cPCA. In Sect. 4, we have already discussed the advantages of using cPCA when compared with using PCA and LDA. It is also possible to compute the discrepancy score D introduced in Sect. 4.2.3 for each original feature without using ccPCA and then use the score as the feature contribution. However, this approach has a similar problem with LDA because the obtained score only shows the separation and does not take into account the variety (i.e., variance) for each feature.</p><p>Another potential option is using the two-group differential statistics methods <ref type="bibr" target="#b49">[49]</ref>, such as two-sample t-test, Wilcoxon signed-rank test, and Mann-Whitney U test, to find features that have differences between the target cluster and the others. Unlike LDA, PCA, or cPCA, these methods cannot produce a quantitative measure for analyzing the FCs to the contrast of the cluster. More importantly, these statistical methods are designed to test whether there is a difference in a certain statistic (typically mean) between two clusters. Therefore, these methods are not suitable for performing exploratory analysis on clusters when we do not know their characteristics beforehand. Limitations. Since we use cPCA, we will need to address its limitations in terms of time and space complexity for a large scale problem. Similar to the classical PCA, cPCA computes the covariance matrices and then performs EVD. For a fixed Î±, it has the same time and space complexity with PCA, which are O(d 2 n + d 3 ) and O(d 2 ), respectively, where n is the number of data points and d is the number of features. Thus, cPCA can achieve fast computation for a dataset which has a large n, but not for a dataset with a large d (we include the experimental results in the <ref type="figure" target="#fig_0">Supplementary Materials [1]</ref>). For PCA, incremental algorithms <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b73">73]</ref> have been developed to solve this issue. For example, the algorithm in <ref type="bibr" target="#b60">[60]</ref> has the time and space complexity of O(dm 2 ) and O(d(k + m)), respectively, where m is the number of data points used in each batch, and k is the number of principal components. We thus plan to develop an incremental version of cPCA next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSIONS</head><p>Dimensionality reduction is widely used to analyze high dimensional data for pattern discovery and real-world problem-solving. Our work makes a tangible contribution to interpreting and understanding DR results by introducing a visual analytics method that capitalizes on contrastive learning. Using a scalable visualization, the method directs the user to the essential features within the data. Our work, thus, further enhances the usability of DR methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>The analysis workflow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>A screenshot of our prototype system. The dimensionality reduction (DR) view (a) visualizes a result after DR and clustering. The feature contributions view (b) shows the measures of each feature's contribution to contrasting each cluster with the others. The feature values of the selected cells in (b) are visualized as histograms, as shown in (c). In (d), we can change the settings for the analysis methods and visualizations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Parallel coordinates showing all features in the Wine Recognition dataset. The corresponding polylines for the wines are highlighted with (a) black, (b) green, (c) orange, and (d) brown clusters. It is difficult to discern the essential features from this visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Comparison of features' relative contributions of MNIST digits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>cPCA results of the Mice Protein Expression dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) PCA (b) cPCA (Î± = 2.15) (c) ccPCA (Î± = 4.38) Fig. 6: The DR results of the Wine Recognition dataset. The cluster labels generated in Sect. 3.2 are used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a) Centroids and cPCs (b) Histogram along the green cPC 1 (c) Histogram along the blue cPC 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 :</head><label>9</label><figDesc>Sign flipping of cPCs. We generate the cPCA results of the Wine Recognition dataset used in Sect. 3.2 with different Î± values. Sign flipping occurs between Î± = 3.3 and Î± = 3.6; Î± = 3.6 and Î± = 3.9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 :</head><label>10</label><figDesc>Reordering and aggregation of the FCs. (a) shows the original FCs. There are 8 clusters and 60 features. (b) shows the reordered FCs in both rows (i.e., features) and columns (i.e., clusters). With (b), we can see a group of similar FCs (e.g., the features are indicated with a yellow rectangle). In (c), the 60 feature clusters are aggregated into 20 rows. For example, the ten features indicated with the green rectangle in (b) is aggregated into one row indicated with the green rectangle in (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 :</head><label>11</label><figDesc>An analysis result of female players from the Tennis Major Tournament Match Statistics dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 :</head><label>12</label><figDesc>A result of the Nutrient dataset. (a) shows the result after applying t-SNE and DBSCAN. A point's color and size show the clustering label and the value of 'calories', respectively. (b) shows the FCs of each cluster. (c) shows the histograms of the selected cells in (b), as indicated with the colored numbers in both (b) and (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 13 :</head><label>13</label><figDesc>The result after filtering out the 'calories' and 'fat' features from the Nutrient dataset. In (a), a point size represents the value of 'carbohydrate'. The histograms of the selected cells in (b) are shown in (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 14 :</head><label>14</label><figDesc>(a) DR result (b) Aggregated FCs and histograms The results for the Communities and Crime dataset. The top of (a) shows the result with t-SNE and DBSCAN. In the bottom of (a), the pink cluster which was not identified by DBSCAN is manually added. (b) shows 40 aggregated features from 121 features. Also, some of the histograms of the original features are visualized at the left of (b).</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors wish to thank Suyun Bae (suybae@ucdavis.edu) of VIDI Labs at the University of California, Davis, for her assistance in improving the clarity of the paper content. This research is sponsored in part by the U.S. National Science Foundation through grants IIS-1528203 and IIS-1741536.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The experimental results, prototype system, source code, and preprocessed datasets</title>
		<ptr target="https://takanori-fujiwara.github.io/s/dr-cl/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The original cPCA implementation</title>
		<ptr target="https://github.com/abidlabs/contrastive" />
		<imprint>
			<biblScope unit="page" from="2019" to="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Multiple correspondence analysis. Encyclopedia of Measurement and Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Abdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Valentin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="651" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Bagaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.06716</idno>
		<title level="m">Contrastive principal component analysis</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploring patterns enriched in a dataset with contrastive principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Bagaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2134</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.04601</idno>
		<title level="m">Contrastive variational autoencoder enhances salient features</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The treatment of missing values and its effect on classifier accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Classification, Clustering, and Data Mining Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="639" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">OPTICS: Ordering points to identify the clustering structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ankerst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Breunig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGMOD International Conference on Management of Data</title>
		<meeting>ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="49" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast optimal leaf ordering for hierarchical clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bar-Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Gifford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="29" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Matrix reordering methods for table and network visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Behrisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Henry Riche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schreck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-D</forename><surname>Fekete</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="693" to="716" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">D 3 data-driven documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bostock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ogievetsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2301" to="2309" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visualizing dimensionally-reduced data: Interviews with analysts and a characterization of task sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brehmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sedlmair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ingram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munzner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop on Beyond Time and Errors: Novel Evaluation Methods for Visualization</title>
		<meeting>Workshop on Beyond Time and Errors: Novel Evaluation Methods for Visualization</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Resolving the sign ambiguity in the singular value decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemometrics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="140" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visual analysis of multidimensional categorical data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Broeksema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Telea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baudel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="158" to="169" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Density-based clustering based on hierarchical density estimates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J G B</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moulavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Pacific-Asia Conference on Knowledge Discovery and Data Mining</title>
		<meeting>Pacific-Asia Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<biblScope unit="page" from="160" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Nutrient explorer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<ptr target="http://bl.ocks.org/syntagmatic/raw/3150059/" />
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2019" to="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Czaplicki</surname></persName>
		</author>
		<ptr target="https://elm-lang.org/" />
		<imprint>
			<biblScope unit="page" from="2019" to="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Clustrophile: A tool for visual clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ã‡</forename><surname>Demiralp</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.02173</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Contrastive multivariate singular spectrum analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-H</forename><surname>Dirie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.13317</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SIRIUS: Dual, symmetric, interactive dimension reductions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dowling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wenskovitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>House</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>North</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="172" to="182" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Graff</surname></persName>
		</author>
		<ptr target="http://archive.ics.uci.edu/ml" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="226" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">An incremental dimensionality reduction method for visualizing streaming multidimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fujiwara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-K</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shilpika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04000</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The role of ethnicity in cancer susceptibility gene polymorphisms: The example of CYP1A1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Garte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Carcinogenesis</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1329" to="1332" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rich component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1502" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Eigen v3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guennebaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jacob</surname></persName>
		</author>
		<ptr target="http://eigen.tuxfamily.org" />
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2019" to="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Printer graphics for clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Hartigan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Computation and Simulation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="187" to="213" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A k-means clustering algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Hartigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series C (Applied Statistics)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="100" to="108" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Discriminant analysis by gaussian mixtures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="page" from="155" to="176" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Self-organizing feature maps identify proteins critical to learning in a mouse model of down syndrome</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Higuera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Gardiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Cios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">129126</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">CyteGuide: Visual guidance for hierarchical single-cell analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>HÃ¶llt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pezzotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Van Unen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Koning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>Lelieveldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vilanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="739" to="748" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Analysis of a complex of statistical variables into principal components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hotelling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Psychology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">417</biblScope>
			<date type="published" when="1933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Parallel coordinates: a tool for visualizing multi-dimensional geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Inselberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dimsdale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Visualization</title>
		<meeting>IEEE Conference on Visualization</meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="361" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Linear discriminant analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Izenman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Modern Multivariate Statistical Techniques</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="237" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Understanding principal component analysis using a visual analytics tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ziemkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ribarsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>Charlotte</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Uncovering representative groups in multidimensional projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Joia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Petronetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Nonato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="281" to="290" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Principal component analysis and factor analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">T</forename><surname>Jolliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Principal Component Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1986" />
			<biblScope unit="page" from="115" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Star coordinates: A multi-dimensional visualization technique with uniform treatment of dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kandogan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Information Visualization Symposium</title>
		<meeting>IEEE Information Visualization Symposium</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="9" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">PIVE: per-iteration visualization environment for real-time interactions with dimension reduction and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Intelligence</title>
		<meeting>AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1001" to="1009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Density-based clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>KrÃ¶ger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="231" to="240" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">On information and sufficiency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kullback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Leibler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="86" />
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Clustervision: Visual supervision of unsupervised clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Eysenbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">De</forename><surname>Filippi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">F</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="142" to="151" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">AxiSketcher: Interactive nonlinear axis mapping of visualizations through user drawings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Endert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="230" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Exploring high-dimensional data through locally enhanced projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Languages and Computing</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="144" to="156" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">The MNIST database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="2019" to="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">A survey on information visualization: Recent advances and challenges. The Visual Computer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1373" to="1393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Visualizing high-dimensional data: Advances in the past decade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maljovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-T</forename><surname>Bremer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pascucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1249" to="1268" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">An approach to perform local analysis on multidimensional projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>MarcÃ­lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Eler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Graphics, Patterns and Images</title>
		<meeting>Conference on Graphics, Patterns and Images</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="351" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Comparing groups for statistical differences: How to choose the right statistical test?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marusteri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bacarea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biochemia Medica</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="32" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">UMAP: Uniform manifold approximation and projection for dimension reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Melville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03426</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Fisher discriminant analysis with kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ratsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Mullers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks for Signal Processing IX: Proceedings of IEEE Signal Processing Society Workshop</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>MÃ¼llner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1109.2378</idno>
		<title level="m">Modern hierarchical, agglomerative clustering algorithms</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Multidimensional projection for visual analytics: Linking techniques with distortions, tasks, and layout enrichment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Nonato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aupetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">On stochastic approximation of the eigenvectors and eigenvalues of the expectation of a random matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Karhunen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="84" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Understanding attribute variability in multidimensional projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pagliosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pagliosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Nonato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Graphics, Patterns and Images</title>
		<meeting>Conference on Graphics, Patterns and Images</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A survey on density based clustering algorithms for mining large spatial databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Parimala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Senthilkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Advanced Science and Technology</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="66" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Visualizing the hidden activity of artificial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Fadel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Falcao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Telea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="101" to="110" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A data-driven software tool for enabling cooperative information sharing among police departments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Redmond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baveja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Operational Research</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="660" to="678" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Incremental learning for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="125" to="141" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Visual interaction with dimensionality reduction: A structured literature analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sacha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sedlmair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peltonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>North</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="241" to="250" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">On optimal and data-based histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="605" to="610" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Probing projections: Interaction techniques for interpreting arrangements and errors of dimensionality reductions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stahnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>DÃ¶rk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="629" to="638" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Color indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Swain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="32" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Visualizing large-scale and highdimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International World Wide Web Conferences Steering Committee</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="287" to="297" />
		</imprint>
	</monogr>
	<note>Proceedings of International Conference on World Wide Web</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">The USDA Nutrient Data Laboratory, the Food and Nutrition Information Center, and Information Systems Division of the National Agricultural Library. USDA food composition databases</title>
		<ptr target="https://ndb.nal.usda.gov/ndb/" />
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2019" to="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Multidimensional scaling: I. theory and method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Torgerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="401" to="419" />
			<date type="published" when="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Dimensionality reduction techniques for blog visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2766" to="2773" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Representative factor generation for the interactive visual analysis of highdimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Turkay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lundervold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Lundervold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2621" to="2630" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Accelerating t-SNE using tree-based algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3221" to="3245" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Linear discriminative star coordinates for exploring class and cluster separation of high dimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Theisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="401" to="410" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Candid covariance-free incremental principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1034" to="1040" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Towards a systematic combination of dimension reduction and clustering in visual analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wenskovitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Crandell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>House</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>North</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="131" to="141" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Towards a systematic combination of dimension reduction and clustering in visual analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wenskovitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Crandell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>House</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>North</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="131" to="141" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Survey of clustering algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wunsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">645678</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Contrastive learning using spectral methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Parkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2238" to="2246" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
