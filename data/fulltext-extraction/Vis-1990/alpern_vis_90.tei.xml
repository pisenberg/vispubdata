<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visualizing Computer Memory Architectures</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Alpern</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Thomas J. Watson Research Center</orgName>
								<address>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>New York</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Carter</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Thomas J. Watson Research Center</orgName>
								<address>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>New York</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Selker</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Thomas J. Watson Research Center</orgName>
								<address>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>New York</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Visualizing Computer Memory Architectures</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>The Memory Hierarchy Framework is a conceputal model together with a visual language for using t h e model. The model is more faithful t o t h e structure of computers than t h e Von Neumann and Turing models. It addresses t h e issues of data movement and exposes and unifies storage mechanisms such as cache, translation lookaside buffers, main memory, and disks. The visual language presents the details of a computer&apos;s memory hierarchy in a concise drawing composed of rectangles and connecting segments. Using this framework, we have improved t h e performance of a matrix multiplication algorithm by more than an order of magnitude. We believe the framework gives insight into computer architecture and performance bottlenecks by making effective use of human visual abilities.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper is a case study in the application of visual language techniques to a limited problem domain. This domain is performance tuning of programs to make efficient use of a particular computer's memory architecture. We present a detailed conceptual model of computer memory, and a visual language for comprehending the model.</p><p>Models allow people to concentrate on relevant features in a problem domain. Some models employ a visual Zanguage, the systematic use of visual techniques to represent the model <ref type="bibr">[7]</ref>. These techniques have been shown to improve problem solving performance in structural domains <ref type="bibr">[5]</ref>.</p><p>A good example is the Turing machine [l] model introduced in the 1930's to understand what functions are computable. A Turing machine is a mathematical construct that manipulates strings of symbols. It is most easily described visually: a long tape is divided into cells, each of which can hold a single symbol. A cart rolls along the tape, reading and possibly modifying the symbol under it. Often, a simplified visual language is used to convey specific Turing machine configurations. The position of the cart is represented by a line under a particular symbol and the program driving the cart is also presented diagrammatically. Invented before computers, the model is still used as an aid for teaching.</p><p>Since the Turing machine bears little relationship to the architecture of real computers, it is no surprise that the model is not useful for constructing efficient computer pro- show how the framework can be used to illustrate and avoid performance bottlenecks in matrix transposition and multiplication algorithms. Section 5 shows how parallelism is reflected in the framework. The rest of the paper discusses insights gained by using the framework and suggests areas for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Memory Hierarchy Framework</head><p>This section presents a framework for understanding computers' memories. The framework has two parts: a conceptual model and a visual language that communicates the model and facilitates performance tuning.</p><p>We first mention some features of real computers. A computer's memory is organized as a set of increasingly fast memory units; for example, disk, main memory, cache and registers. Blocks of data are transferred between units along wires. Moving a byte automatically moves all the bytes in the same block as well. Different wires and memory units have different speeds and sizes, as well as other peculiarities.</p><p>The above features affect performance significantly. Often programs can be made to run many times faster by properly coordinating data movement among the different units [4]. Unfortunately, programming languages and the RAM model hide the irregularities of memory. Few people master the ability to tune programs effectively. Our work is aimed at making this skill less esoteric. Conceptually, we model the non-uniform nature of memory by a hierarchy of memory modules. Each memory module consists of a box that can store data and a bus that can move data between the box and the next higher module in the hierarchy. Typically a memory unit of a computer is modeled by a single memory module.</p><p>Four quantities associated with each module reflect performance characteristics: (1) the blocksizethe number of bytes that move on the bus in an atomic operation, (2) the transfer time required to move a block on the bus, (3) the blockcountthe number of blocks that the box can hold, and (4) associativity, which will be discussed below. <ref type="figure" target="#fig_1">Figure 1</ref> is based on a particular configuration of an IBM 370 computer. It is a "sentence" in a visual language <ref type="bibr">[7]</ref> that assigns specific semantic meanings to the syntactic elements of the picture. Rectangles represent memory boxes, ladderlike line segments represent busses. The height of a rectangle tells the blocksize of the corresponding memory module, the width tells the blockcount, and the length of a line segment represents the transfer time. A logarithmic scale is used so that the syntactic elements can be seen in the same picture, despite the fact that the parameters for the top and bottom module differ by a factor of 200,000. Thus, a rectangle that is m units long by k units high represents a memory box with 2m locations, each of which can hold a block of 2k bytes. The length of a line segment that represents a bus is the logarithm of the transfer time (normalized to the cycle time of the computer).</p><p>Some of the memory modules shown in <ref type="figure" target="#fig_1">Figure 1</ref> the registers, main memory, and disk storage ~ should be familiar. The cache module represents a fast memory unit that holds recently referenced data.</p><p>The TLB (Translation Lookaside Buffer) module represents a less familiar machine feature. Many machines such as the IBM 370 and the VAX 11 have a TLB to support virtual addressing. Programmers don't need to know that the TLB is a hardwired data structure containing the most recently used page table entries (whatever that means). The impact of the TLB can be significant. This impact is conveyed by the dimensions of the TLB module.</p><p>The partial shading of some rectangles represents the associativity of the module. Consider the d-way associative cache in <ref type="figure" target="#fig_1">Figure 1</ref>. Data addresses higher in the hierarchy are partitioned into 128 associativity classes (128 being represented by the width of the shaded region). Four memory locations in the cache are reserved for each associativity class. A block moving down into the cache can only be transferred into one of the four locations associated with its address.</p><p>The shading of the cache creates a visual ambiguity as to how much of the associated memory module is really available. One may think of this cache as a memory module with an effective blockcount that varies somewhere between 4 and 512 blocks. Most machines guarantee that if the data stored in the cache have sequential addresses, the full blockcount will be available. However, the effective blockcount will be greatly reduced for a program that marches through addresses with an unfortunate stride. The unshaded subblock represents the minimum effective blockcount of the module; the full rectangle is the maximum.</p><p>The next two sections use the framework to illustrate the performance impact of the memory hierarchy.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Matrix Transposition</head><p>Transposing a matrix is a simple problem. However, ignoring the memory hierarchy can significantly degrade the performance. We use our framework to illustrate this.</p><p>The following FORTRAN code assigns the transpose of matrix A to matrix B.</p><formula xml:id="formula_0">DO 10 I = 1, N DO 10 J = 1, N 10 B ( J . 1 ) = A ( 1 , J )</formula><p>Consider any memory module where N is larger than the blocksize and the blockcount. Arrays are stored by column in FORTRAN. When A ( 1 , l ) is referenced, a block containing part of the first column of A is transferred into the memory module. <ref type="figure" target="#fig_1">Next A(1,2)</ref> is referenced, and a second block is brought in. After blockcount steps, the memory is full, and some block (typically the first brought in) must be discarded. This processbringing in a new block and discarding an old continues. Unfortunately, by the time the next row of A is begun, the needed data have already been discarded from the memory module! The result is excessive data movementeach assignment takes (at least) the amount of time required to transfer a whole block of data. This situation, where the cost of transferring data swamps the cost of computation, is called thrashing. The framework shows this problem in a natural way. Superimpose a square representing the array on top of the memory module. Notice that the vertical direction of both the array and the memory rectangle represent contiguous storage. If the square fits comfortably inside the rectangle, the entire array will fit into the corresponding memory unit. The module will not be a performance bottleneck. If the array is too tall but not too wide, the module can hold one block's worth of each column. Since each block will be brought in only once, performance will not be degraded by unnecessary data transfers. But one must examine the problem more closely when the array is too wide for the rectangle, or when part of the array falls within a shaded portion.</p><p>The unnecessary block transfers of the A matrix would be eliminated by simply interchanging the order of the loops.</p><p>Unfortunately, the B matrix would now have the identical thrashing problem.</p><p>There is a way out <ref type="bibr">[6]</ref>. The A and B matrices are partitioned into subarrays small enough that two can fit into the memory module. The program works on one subarray of A at a time, transposing it into the symmetrically located subarray of B. These subarrays must be further partitioned into subsubarrays so they can fit into the next lower memory module in the hierarchy. The entire process is shown in <ref type="figure">Figure 2</ref>. A program that implements the picture for a klevel memory hierarchy would have 2( IC-1) nested DO-loops.</p><p>We use a new scale inside the memory rectangles. As described earlier, the logarithmic scale allows the entire hierarchy to be compared in a single image.</p><p>The schematic diagrams inside the rectangles highlight issues in moving data between levels. Specifically the shaded submatrices of a level are the data which is transfered to the next level. It is expanded to the next level but retains its shape.  <ref type="figure">Figure 2</ref>. Transposing a Matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Matrix Multiplication</head><p>Matrix multiplication is basic to many scientific application such as LAPACK [3]. Almost any effort to speed up matrix multiplication is justifiable. As was the case with transposing a matrix, failure to take into account the memory hierarchy can have a disastrous impact on performance.</p><p>The matrix product C = A x B is computed by the following Fortran code:</p><formula xml:id="formula_1">DO 20 I = 1, N DO 20 J = 1. N C(1,J) = 0.0 DO 20 K = 1, N 20 C(1,J) = C(1.J) + A(I,K)*B(K,J)</formula><p>As with matrix transpose, this program will thrash if the matrices do not fit in a memory rectangle.</p><p>As with matrix transpose, the way out is to partition the matrices. The i j t h entry of the product is the sum of the products of the corresponding entries.of the ith row of A and the j t h column of B. This is true even if the entries are submatrices rather than numbers. The submatrices need not be square as long as the first dimension of C matches the first dimension of A , the second dimension of A matches the first of B , and the second dimension of B matches the second of C. Subject to these constraints, any partitioning of the matrices corresponds to a valid matrix multiplication program.</p><p>The approach shown in <ref type="figure" target="#fig_2">Figure 3</ref> is to partition the matrices recursively so that each level in the hierarchy contains a subproblem of convenient size. The problem at a level is a subproblem of the parent level. The corresponding matrices (and submatrices) are shaded accordingly. Again, the arrays of <ref type="figure" target="#fig_2">Figure 3</ref> are drawn schematically.</p><p>In order to obtain an optimal partitioning, the dimensions of the submatrix should be chosen to maximize the number of multiplications per block transfer. To this end, all dimensions should be large, all dimensions should be roughly equal, and the first dimension of each matrix should be an integral multiple of the blocksize. (Since the second dimension of A matches the first of B , both dimensions of A should be integral multiples of the block size. This constraint could be avoided by transposing the A matrix before the multiplication.) These three goals can be achieved unless the effective blockcount of a module is small relative to its blocksize (i.e. the module cannot hold three square matrices with blocksize rows and columns).</p><p>It turns out that if the effective blockcount of a module is at least six times larger than its blocksize, the depicted algorithm could be further improved. The computation of a subproblem can be overlapped with communication (of the results of the previous subproblem) to and (of the inputs to the next subproblem) from the parent. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Parallel Computers</head><p>Currently there exists a vast collection of different architectures for machines with more than one processor. Such parallel and distributed machines can be modeled in the Memory Hierarchy Framework. The visual language allows several memory modules to hang below a single rectangle. To avoid overlapping rectangles, busses can have horizontal segments; the transfer time is depicted by just the vertical distance the bus covers. An entire machine is represented as a tree of memory modules as in <ref type="figure" target="#fig_3">Figures 4, 5, and 6</ref>. The multiple processors are at the leaves of the tree at the bottom of each picture.</p><p>The semantic meaning of multiple bus lines hanging below a rectangle is that data can be transferred along all of the busses simultaneously. The busses can carry data from different addresses in the shared memory module.</p><p>As with any model, certain hardware restrictions have been ignored. In particular, each machine and operating system has a specific set of rules about data sharing and data protection. A programmer may need to consider these issues carefully for a particular application.</p><p>The pictures in <ref type="figure" target="#fig_3">Figures 4, 5</ref> , and 6 are representative of three very dissimilar multicomputer architectures; the supercomputer, the massively parallel computer, and heterogeneous networks of computers. The different architectures are reflected in how much branching is at different levels. Any particular machine in a given class will also vary from the pictures in specific details, such as the dimensions and number of processors. <ref type="figure" target="#fig_3">Figure 4</ref> is a typical picture of a supercomputer, such as the CDC and CRAY machines and large IBM mainframes.</p><p>These machines have a relatively small number of processors, and the branching occurs near the leaves. Supercomputers often have vector processors that allow them to "crunch" data extremely efficiently, provided that data are arranged properly in memory. The vector processor shown in <ref type="figure" target="#fig_3">Figure 4</ref> is long, thin, and connected by a relatively long bus. This describes a machine that requires vector data to be stored contiguously. Some machines have more flexible vector processing. They would be depicted with shorter and wider vector rectangles.</p><p>Some supercomputers gain speed by having several functional units (ALUs) that work on data in a single group of registers. These could be represented by another level of branching below the registers. Again, it is interesting to see that the parallelism of supercomputers is concentrated towards the leaves. Another class of machine, the massively parallel computer, has a very high branching factor near the middle, but little at the top or bottom (see <ref type="figure" target="#fig_4">Figure 5 )</ref> . The particular machine of the picture has 1024 processors, each with a quarter megabyte of memory, connected by a fast switching network. The machine has a communication latency of 32 cycles and allows data to be transferred at a rate of 1 byte per cycle. The picture approximates this by showing that 32 byte blocks can be transferred in 64 cycles. The picture is accurate within a factor of 2 for any sized message.</p><p>Another class of multicomputer is the loosely coupled network. Pictures of these machines are characteristically bushy near the root of the tree, as in <ref type="figure">Figure 6</ref>. A set of workstations of different capabilities can share the same disk memory (fileservers). Files are moved through the network. The top rectangle represents the totality of data stored in the network. Although these data are distributed physically, the system behaves as if there is a memory unit containing a copy of all the data.  <ref type="figure">Figure 6</ref>. A Loosely Coupled Network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MINI COMPUTER</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Pictures in the Memory Hierarchy Framework convey more than a set of parameters. Machines that support efficient computations are represented by pictures in which the memory rectangles and bus lines grow more or less evenly from bottom to top <ref type="bibr">[2]</ref>. The eye notices certain ways that Figure 1 fails to grow uniformly from bottom to top. The bus between the disk and main memory appears long. In fact, for many applications the most serious performance bottleneck is the slow speed of accessing data from disk. If only the unshaded portions of the cache or TLB were available, then they would appear to be skinny. This reflects the fact that cache and TLB associativity problems can cause significant performance degradation. The picture appears non-uniform in some other waysthe cache could be narrower, the TLB could be shorter, and the bus from main memory to the TLB could be longer. These don't matter since they indicate that the machine has more capacity and faster busses than necessary.</p><p>Computationally-intensive algorithms including matrix multiplication, transpose, and the Fast Fourier Transform are rigorously analyzed <ref type="bibr">[2]</ref>. The paper shows that the uniform growth of the memory modules is important for approaching 100% CPU utilization. If any memory module is too narrow or if any bus is too long (i.e. too slow), it will be a performance bottleneck. The complete story is more complicated: transpose requires that each bus be no longer than the corresponding rectangle is tall; more CPU-intensive applica-tions such as multiplication,has relax this requirement; and several applications require that each box be wider than the corresponding bus is long. The less slack the machine has over the requirements, the more programming effort it takes to coordinate the data movement properly. But in each case, once an algorithm has been analyzed, the picture suggests where to concentrate the programming effort.</p><p>Our framework does not capture all of the performance relevant features of computer architecture. Disk storage is imperfectly modeled as a memory module. In particular, accessing any block of data in a memory modules takes a fixed amount of time. However, the access time to a block on disk may be faster or slower depending on which track of the disk the read/write head happens to be focused on, and how much the disk must rotate to bring the desired data to the head. To a first approximation, we can use the average access time as the transfer time from disk to main memory. However, in applications where disk access time is critical (and assuming the programmer can control where blocks are placed on disk), it might be desirable to extend the framework to reflect variable access time.</p><p>The Memory Hierarchy Framework has been effective for understanding the efficient utilization of a computer's memory hierarchy. We believe that other problem domains would benefit from the use of similar visual modeling techniques. For instance, a systematic study of the taxonomy and evolution of parallel computers might yield important insights. To do this requires extending the framework to reflect data consistency and protection issues.</p><p>Another line of research might be to create a programming language that is closely associated with the framework. A programmer would describe the problem decomposition visually, and a compiler would supply the exact machine parameters. Such a system could make automatic program tuning a reality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>The Memory Hierarchy Framework consists of a conceptual model of computer memory and a visual language for depicting the model. The model reflects important practical considerations that are hidden by the RAM model: data are moved in fixed size blocks, data can be moved simultaneously at different levels in the hierarchy, and memory capacity and bus bandwidth are limited at each level. In our model, the memory of a computer is conceptualized as a (mathematical) tree of memory modules. A memory module is characterized by four values: transfer time, blocksize, blockcount, and associativity.</p><p>The visual language allows memory to be expressed as a diagram. The following design decisions define the language. A memory hierarchy is depicted as a (visual) tree. The nodes of the tree are rectangles that represent memory boxes. The height of a rectangle represents the corresponding module's blocksize. The width of a rectangle represents blockcount.</p><p>The width of the unshaded sub-rectangle represents associativity. A branch of the tree represents a bus. Its length represents transfer time. In order to comprehend widely disparate quantities visually in a confined space, sizes are chosen to be proportional to the logarithms of quantities rather than to the quantities themselves.</p><p>The visual language and the model were developed in tandem. Indeed, the language facilitated the development of the model. It also facilitates thinking about the model. In addition, it makes identification of potential performance bottlenecks easy. Techniques described in Sections 3 and 4 were used to obtain a 30-fold speed-up in an implementation of the matrix multiplication routine of LAPACK. We believe the framework will be useful in speeding up the implementation of other computationally intensive applications. We hope that the visual language will prove a convenient medium for communicating our model to others.</p><p>This paper is a case study of a successful application of visual language techniques in a limited domain. We hope that it can serve as a demonstration to encourage others to build languages to aid problem analysis. Only a tiny collection of visual techniques is used in our framework. Barely scratching the surface of the visual language approach we have found this approach powerful. In order to unleash the full power of visual language, a large collection of techniques must be systematically catalogued, and experience must be developed as to the appropriate uses of each. To this end, further case studies are called for.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>A Memory Hierarchy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Matrix Multiplication.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>A Supercomputer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>A Massively Parallel Architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>grams. Instead, programmers use the Von Neumann or RAM model [I]. A computer is modeled as a memoryless processor and a separate random access memory. The assumption is made that each memory reference takes one unit of time. This assumption permits the running time of a program to be analyzed. The model allows programming without consideration of hardware details. Unfortunately, some of the details ignored by the model have</figDesc><table /><note>a significant impact on performance. Thus, RAM analysis may be inaccurate. In trying to analyze and improve some programs, we found the RAM model inadequate. Section 2 describes the Memory Hierarchy Framework, a more accurate computer model together with visual language for presenting it. Sec- tions 3 and 4</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">CH2913-2/90/0000/0107/$01 .OO -1990 IEEE I</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The Design and Analysis of Computer Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Aho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Ullman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1974" />
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Uniform Memory Hierarchies,&apos;&apos; to appear Proc</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alpern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Feig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Foundations of Comp. Sci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="1990-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">LAPACK Working Note #1: Prospectus for the Development of a Linear Algebra Library for High-Performance Computers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Croz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Greenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hammarling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sorensen</surname></persName>
		</author>
		<idno>ANL- MCS-TM-97</idno>
		<imprint>
			<date type="published" when="1987-09" />
		</imprint>
		<respStmt>
			<orgName>Argonne National Laboratory</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Impact of Hierarchical Memory Systems on Linear Algebra Algorithm Design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gallivax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jalby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Sameh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J Supercomputer Appl</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="12" to="48" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Which Task Is Which Representation on What Kind of Interface</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rohr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interact &apos;87</title>
		<meeting>Interact &apos;87</meeting>
		<imprint>
			<date type="published" when="1987-09" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Matrix Algebra Programs for the UNIVAC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Rutledge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rubinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Presented at the Wayne Conference on Automatic Computing Machinery and Applications</title>
		<imprint>
			<date type="published" when="1951-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Elements of Visual Laxguage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Selker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Koved</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop On Visual Languages, October</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
