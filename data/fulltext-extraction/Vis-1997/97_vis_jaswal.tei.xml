<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CAVEvis: Distributed Real-Time Visualization of Time-Varying Scalar and Vector Fields Using the CAVE Virtual Reality Theater</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijendra</forename><surname>Jaswal</surname></persName>
						</author>
						<title level="a" type="main">CAVEvis: Distributed Real-Time Visualization of Time-Varying Scalar and Vector Fields Using the CAVE Virtual Reality Theater</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>This paper discusses CAVEvis and a related set of tools for the interactive visualization and exploration of large sets of time-varying scalar and vector fields using the CAVE virtual reality environment. Since visualization of large data sets can be very time-consuming in both computation and rendering time, the task is distributed over multiple machines, each of which is specialized for some aspect of the visualization process. All modules must run asynchronously to maintain the highest level of interactivity. A model of distributed visualization is introduced that addresses important issues related to the management of time-dependent data, module synchronization, and interactivity bottlenecks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>CAVEvis is a visualization tool for interactively exploring large time-varying three-dimensional scalar and vector field data. It utilizes the CAVE virtual reality theater <ref type="bibr" target="#b3">[4]</ref> to provide an immersive three-dimensional environment with fast and intuitive visualization and navigation tools to navigate to any to position, orientation, scale, or time in the data sets. For scalar field data, it can display isosurfaces, cutplanes, and can probe the values at any locations. Although each instance of a scalar field visualization only utilizes a single time step, there may be any number of these visualizations sampled from anywhere in the data set at any time step. CAVEvis was primarily designed to aid the study of unsteady flow fields. Thus for vector field data, it can show the time-varying flow of large numbers (e.g. 100K) of particles each colored by some scalar field value. It also provides related visualization tools such as colored particle pathlines (time-varying) and streamlines (time-invariant).</p><p>CAVEvis was designed originally to explore data sets generated from the simulation of severe thunderstorms and tornados <ref type="bibr" target="#b8">[9]</ref>. The tornado simulations generated 40 gigabytes of data consisting of wind velocity vector fields and scalar fields for temperature, pressure, water density, and vorticity, over hundreds of time steps. It is only feasible to visualize a subset of that data. We process and cache in memory up to about 300 megabytes of data during typical visualization sessions with this data. <ref type="figure" target="#fig_3">Figure 1</ref> is sample screen image showing a number of these visualizations from the CAVE at a time step just after the simulated tornado touched down.</p><p>Large data sets, such as these, require powerful visualization tools to help researchers study or discover phenomena present in the data. Generating the geometry data for these visualizations usually requires processing a great of data and is often timeconsuming. Furthermore, the generated geometries may be very complex, putting great demands on the graphics rendering system. See http://www.ncsa.uiuc.edu/People/vjaswal/cavevis/, for more information.</p><p>y email: vjaswal@ncsa.uiuc.edu z NCSA, 152 Computer Appl Bldg, 605 E. Springfield, Champaign, IL</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>61820</head><p>These are both significant obstacles to interactive exploration of that data. Even worse, virtual reality environments put very high demands on rendering rates, since the entire scene displayed to the user must be updated as the user moves in physical space. Rendering rates should be no slower than about 10 stereo frames per second, below which, responsiveness becomes unacceptably slow.</p><p>In the CAVE environment, every scene is rendered twice, to provide slightly different interleaved views for the left and right eyes. Such stereo systems as this would therefore require rendering rates of 20 per second or greater. Thus, it is infeasible for many interesting domains, such as large CFD simulations, to have a visualization tool both compute the geometries and render them. Consequently CAVEvis only renders the visualizations and interacts with the user. Other modules (i.e. separate programs) generate the geometry data under the control of CAVEvis and send the results to the reader process of CAVEvis. This paper will discuss the CAVEvis system and how it provides real-time interactive visualization. This will involve discussion of distributed visualization using asynchronous computation and rendering modules, obstacles to interactivity, and various issues related to time-management in dealing with both time-varying data and the time-dependent visualizations. Section 2 discusses the overall architecture of this system. Section 3 will briefly describe how to operate some aspects of CAVEvis to motivate discussion of its design, followed by a discussion of synchronization of the modules. Subsequently, some implementation issues regarding distributed visualization will be addressed in section 4. Problems with I/O bottlenecks will be briefly noted in section 5. Finally some related work will be briefly mentioned in section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Architecture</head><p>This system performs visualization by distributing the task over a number of closely communicating asynchronous modules. Each module is a separate program that performs some aspects of the visualization task, such as rendering, computing particle trajectories, generating isosurfaces, or sonifying data. As shown in figure 2, they may run as separate processes on the same machine, or more likely, on separate workstations and supercomputers, each of which is specialized for the module's task. For example, CAVEvis itself is usually run on an 8 to 12 processor Onyx equipped with two or three Reality Engine 2's or two Infinite Reality boards. The computation servers usually run on whatever machines hold the data. In our case, the data resides both on the Onyx itself and a number of 16-node (of R10000 processors) Power Challenge machines that actually generate the simulation data. Thus the computation server, the isosurface generator and other data processing modules may run on separate supercomputers and communicate with CAVEvis over high-speed HIPPI socket connections. In past conferences, the server has even been located at separate supercomputing centers and communication was done over both VBNS and normal Internet connections. <ref type="bibr" target="#b7">[8]</ref> Figure 1: This sample screen shows a number of different visualizations approximately when the tornado touched down. Shown are an isosurface over rain-water density, and particle flow from three emitters, at the ground plane, at the base of the funnel, and vertically along the column of the funnel.</p><p>CAVEvis is responsible only for interacting with the user and rendering the visualizations in stereo as fast as possible. It uses separately running modules to generate the geometry data for visualizations, such as modules to generate isosurfaces or particle paths.</p><p>The most important auxiliary module, called the Function Module (FM), is a computation server that computes various functions asynchronously and sends back the series of results to the requesting clients. For example, this module is responsible for computing the trajectories of particles and coloring them by a scalar field. Other simpler modules include an isosurface generator that reads in a data file, generates an isosurface, and sends the resulting geometry data to a rendering module such as CAVEvis. Currently the isosurface module is not interactively controlled, so the data files and other parameters are all determined by command line options when it is started. CAVEvis also supplies quantitative and event data to the Vanilla Sound Server, or VSS <ref type="bibr" target="#b0">[1]</ref>, which sonifies some data and provides audio feedback for user actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CAVEvis -Rendering and Control</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">CAVE Virtual Environment</head><p>CAVEvis utilizes the CAVE virtual reality theater <ref type="bibr" target="#b3">[4]</ref> and related configurations such as the Immersadesk and Infinity Wall, all of which utilize the same CAVE libraries. Briefly, the CAVE is a surround-screen, surround-sound, projection-based hardware and software system that provides a virtual environment with 3-D graphics and 3-D sound. Unlike head-mounted display systems, the images are projected onto screens that surround one or multiple simultaneous users. To see in stereoscopic 3D, users wear lightweight LCD shutter glasses. One user also wears a special tracking device that the CAVE hardware uses to adapt the displays to that user's position and orientation. CAVE applications use a 3-D pointing device called the wand which has 3 buttons and a joystick and is also tracked by the CAVE hardware. The wand is basically a 3-D mouse that most CAVE applications use as the primary control device. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">User and Domain Space</head><p>As the rendering and user interface module, CAVEvis generates the scenes that create the virtual reality environment for the user. There are two spaces of virtual objects with which the user interacts. In the user space and the associated coordinate frame, objects are always drawn relative to the user. This is used to draw control interface objects such as the 3-D menus and other widgets. The user interacts with these objects in much the same way as one uses a mouse in a window-based GUI. Since these widget controls are always relative the user, it is analogous to the cockpit of an airplane or dashboard of a car. The controls are always in a quickly accessible and familiar location, even as the VR user, pilot, or driver navigates in a different space. In the domain space, objects represent some aspect of the scientific domain and are all drawn in the same coordinate frame. These include the bounding box that represents the 3-D volume of the data sets and all the visualizations of the data. There are also domain space control objects that the user can interact with, such as particle or streamline emitter tools. With the wand, the user can intuitively select, move, manipulate, and interact in other ways with these domain space objects to do things such as emit sets of particles or probe for data values. <ref type="figure" target="#fig_1">Figures 3 and  4</ref> show examples of the user interacting with both types of objects, and how they are independently drawn and manipulated. In domain space, the units and scales of space and time are always expressed in terms relevant to the domain. Thus the locations of all emitters, etc. are in terms of the domain rather than those used to render them. CAVEvis handles all the mappings automatically, based on options set in configuration files, so that the user only thinks in terms of the domain. The CAVEvis user can navigate arbitrarily in time and space and at any speed, scale, or orientation. Thus, one can view the entire domain so that it appears to be 10 feet across at waist height (the default); or zoom in and fly around so that everything appears 100 times larger; or one can "grab" the whole domain and orient it arbitrarily without flying around.</p><p>Since CAVEvis handles time-dependent data, efficient time navigation is also needed. For example, we frequently examine the tornado simulation data from the simulation time steps of 5700 seconds to 6600 seconds. With configuration files, the clock in fig-  In this screen the user is manipulating the location of the blue particle emitter, which is a domain space control object. This is done by "grabbing" it (using a button press) and moving the wand. A new set of particles has just been emitted, by clicking on another wand button. In the figure, the particles from the blue emitter are colored so those going up are more orange, and those going down are more cyan. ure 3, that represents domain time, is set to range over these values. By grabbing and twisting the clock dial, the user can jump to any available time and instantly view the visualizations saved at that time step. There are also auto-advance modes, where the domain clock is advanced according to the Speed setting.</p><p>This versatility in time and space navigation is necessary because CAVEvis was designed to explore arbitrary scalar and vector fields (currently only on a rectangular grid), so no assumptions can be made about the scale, position, orientation, or speed or about the ranges of these values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Module Coordination</head><p>This section will illustrate a typical CAVEvis session used to visualize particle flow in a time-varying vector field, and describe how information and control are distributed between modules. To begin a session, a user sets configuration options, in a front-end Java-based GUI, that define different aspects of behavior for that session. These include domain and data dependent options defining the locations of the data files and their time steps, the recommended size of the domain space, initial locations for some domain space tools (such as particle emitters), and ranges of domain time values. They also define different color map specifications that control how particles are colored according to different scalar fields at their locations. Additional options define program specific options such as the number of lightweight and heavyweight computation threads, memory usage, the machines to run on, level of feedback, etc. Then the front-end GUI launches all the necessary modules on the requested machines.</p><p>When CAVEvis is first started, the user only sees the menu system and the domain's bounding box and tools (such as the emitters) at their initial positions. While in the CAVE, one can no longer quickly access the keyboard. Therefore, with the wand and 3-D menu system, the user selects the desired time step and domain scale, navigates to the area to investigate, selects and repositions the emitter tools as needed, and sets advection parameters like the time delta and advection method. Then from the menus or domain space controls, they can emit particles from one or more emitters.</p><p>CAVEvis then sends the sets of particles to the FM for advection. Without ever waiting for results from this or other modules, CAVEvis continues to render the visualization data it currently has, process user actions, and issue further commands and visualization requests to other modules as needed. The time on the domain clock is the rendering time. CAVEvis renders only visualization objects whose associated time-stamp corresponds to the current rendering time. Therefore the visible objects usually represent only a time slice of all the objects available. In auto-advance mode, the domain clock flows according to the Speed setting, which controls the mapping from the flow of user's time (i.e. real time) to the flow of domain time. For example, setting Speed to 100 means that the domain time is advanced by 100 domain time units (e.g. seconds) for every second of real time. With these controls, one can jump to any domain time or auto-advance forward or backward, see the visualizations recorded at those times, and animate through them at any speed.</p><p>Meanwhile, the FM is busy advecting the sets of particles, computing other visualizations, or servicing other function requests. As it advects and colors each successive step of the particles' trajectories, it also updates their time-stamp and sends back the set of updated locations to the requesting client, CAVEvis. Unlike CAVEvis which has a single clock representing the current global domain time, the FM has no sense of global time at all. Rather, the FM uses the time-stamp on each time-dependent object to select which data sets to read. For return values, it updates the time-stamp of the resultant visualization objects according to the protocol required of the request.</p><p>For example, the time-stamp of particle sets is incremented by the advection time delta used, in the case of a time-varying vector field. However, requests for streamlines, isosurfaces, and cutplanes specify a time value that is used to select the data set to read, but the resulting visualization object has the same time-stamp. One can also have time-invariant particle flow, in which case, the advection time delta is only used to move the particles but not to update their time value. In this case, there would be multiple iterations of the particle set that all share the same time-stamp. The model of time could simply be extended to also consider an optional iteration value, if attached to an object, to sort those objects with the same time-stamp.</p><p>With this model, a single global time is not shared across all participating modules. Instead a single module controls what is the current time and that is distributed and maintained in a fine-grained manner as an object attribute. No time synchronization is required between CAVEvis and supporting modules. Therefore, regardless of how long each advection step takes, or how many sets of particles are being processed, the FM will send back the results as soon as it computes them. Neither it nor CAVEvis waits for any results to arrive, but continue to perform their time-critical tasks, such as rendering or computation. In this way, interactivity and high throughput is maintained because no module blocks. The flow of information is controlled by CAVEvis, which interactively responds to user commands and supplies and directs the information to process to other modules. The FM acts as a server that merely fulfills one specific request after another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Distributed Model of Visualization</head><p>Distribution of visualization requires a clean separation of responsibilities and the information used for those tasks. In keeping with this separation, CAVEvis knows nothing about how the scalar and vector fields are obtained or used for advection or other visualization computations. It refers to these data sets only by user-specified string names. By default, the FM reads data sets from HDF files <ref type="bibr" target="#b5">[6]</ref> in the file system, but they could even have been obtained from dynamically steered applications. That is encapsulated within the FM. Similarly, the FM knows nothing about how the visualization objects that it generates will be used; whether, for example, they are displayed by CAVEvis or even written to a file and displayed with a VRML browser. In this section we consider further ways in which visualization and computation are separated in our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Display Model of CAVEvis</head><p>CAVEvis attempts to adhere to a model where it simply displays the data that it has stored corresponding to the current domain clock. In other words, this model does not specifically address scientific visualization, but only the display of time-dependent data. In particular, every time-dependent display object, such as a set of particles, set of streamlines, or isosurface, has a sequence name and a domain time-stamp. The name and time-stamp are attributes that are part of the visualization object itself and independent of the clock with which CAVEvis is rendering. All objects received with the same name are stored in the same sequence sorted by their time-stamp, regardless of the order in which they were received. A sequence element of time ts is said to cover a time t if ts = t and there is no other sequence element inbetween. This model assumes persistence of visualization data, since an element is considered valid and therefore rendered as long as it covers the rendering clock time. (This assumption can be adapted to also include an expiration time or to represent the age of a visualization object.)</p><p>For example, the set of particles emitted from the yellow emitter might be called Yellow-Set which would be the name given to and returned by the FM. Given an advection delta of dt and initial time of t, the FM would iteratively send back a series of objects (i.e. the sets of the successive locations of those particles), stamped with the times t + dt, t + dt 2, etc, up to the maximum advection time requested. CAVEvis would store each object as an element in the sequence named Yellow-Set. It would render only the element of each sequence which covers the current time of the domain clock. Therefore, assume that the Yellow-Set sequence has elements at times f6300; 6305; 6310; :::g, the Blue-Set sequence has f6300; 6301; 6302; :::g, and the Isosurface-0.5 sequence has f7000; 7100; :::g. Then when the clock is at 6301.5, CAVEvis will render element 6300 of Yellow-Set, 6301 of Blue-Set, and nothing from Isosurface-0.5. Note that the times of elements in each sequence need not correspond in any way to other sequence elements. For time-dependent visualizations where each object is computed independently, as with an isosurface, the visualizations may be computed and therefore received in an order.</p><p>This model of visualization allows one to program only rendering and user interface functionality into CAVEvis. Therefore, CAVEvis knows about domain time and space, how to navigate in this configurable space, and how to draw different classes of domain space objects. For example, to visualize particle flow one needs to specify where to inject a new set of particles. This is done with the particle emitter, or rake, which is an interactive domain space object that shows where each new particle will be injected into the data set. Internally, this is almost identical in appearance and functionality to those emitters for streamlines, cutplanes, etc. All of these visualization tools can be said to "emit" time-dependent visualization objects, many of which emit from a 2-D rectangle or 3-D box shaped rake.</p><p>In addition to drawing a tool's graphic representation, CAVEvis needs to know how to generate new visualizations and how to render the results. Minimally, this requires functionality to issue specific requests to other modules and ways to alter the parameters in those requests. Inside CAVEvis, this is done in a generic way by reusing the emitter tool (to obtain position information), creating widget controls for the parameters of each new tool type, and using high level message-passing libraries to issue tool's requests and read the results.</p><p>Finally, CAVEvis needs to be able to render the results of a visualization tool. Often this is an instance of common classes of visualization objects, such as sets of colored lines, colored particles, shaded polygons, etc. Thus the isosurface generator responds with a set of shaded polygons, the particle advector with colored points, and the streamline generator with colored lines. New tools may require creating new classes of visualization objects. Timestamps are used uniformly, as described above, to manage timedependent objects. In this way, CAVEvis requires little additional programming to add new visualization tools, since the hard task of time-synchronization is handled simply and uniformly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Computation Model of the Function Module</head><p>The FM employs something like a server model for handling computation requests. In a typical server model, the server waits for incoming requests, processes the requests (e.g. to perform a computation), and then sends the (computation) results to the client. With an optional timeout value, the client usually waits until the request is satisfied for the return values. This of course may suspend the execution of the client for an arbitrarily long time, dependent upon the message overhead (i.e. amount of data to transfer to and from the server), the complexity of the request, and any number of network traffic and system related variables. However, for interactive programs, this is unacceptable when a great deal of data must transmitted or when high interactivity must be maintained at all times; both of which are true for our task.</p><p>In order to keep both the client and server executing simultane-ously, the communication model employed separates a service request into a service request transmission, service computation, and return value transmission. The client process is only engaged (and blockable) while sending a service request. It continues executing afterwards without explicitly waiting for the return value. As is typical of multi-process servers, the FM consists of a reader process to obtain service requests, multiple computation threads, and a sender process that sends back data to the client, all of which are decoupled. After reading the service request message, the FM processes the request using all the computation threads, and then sends the return values back to the client using the sender process. The client must have a reader process that awaits the return values from requests it has issued. In particular, the FM's reader process exclusively reads and decodes service request messages, from any number of clients, from the server input port. These messages are self-describing in that they consist of a header that defines the type of request, other message and client information, the types and sizes of all message arguments, and it may include short message arguments. Following this is the data segment which contains arguments that are large data sets. The reader process then creates a computation function object to service the request, which it schedules for the computation threads. The reader then awaits the next incoming message.</p><p>Meanwhile, the computation threads perform each computation function one at a time, using a round-robin scheduling policy. That is, one computation function at a time is executed in parallel by all computation threads. Usually a computation function will generate data that must be sent back to a client, to the port defined in the message header. For this, the computation function adds a send function and the associated return data to the sender process's queue. Afterwards, the computation process invokes the next computation function, and so on.</p><p>Meanwhile, the sender process extracts send functions and return data from its queue and invokes them to send data back to the client. As stated before, the client must have a process that is ready to receive return values for all the types of messages it has issued. If not, the FM sender process will block. Return messages have priority levels so that high priority messages may jump ahead in the send queue. This allows some functions, that must have low latency, to deliver their return values quickly to the client. In particular, high-priorities are used for short status messages and for functions that are closely tied to user motion. For example, a scalar field probe samples data set values at locations that are selected by the wand. As a user continuously moves the wand, CAVEvis sends many messages requesting scalar field values from those locations. These must be quickly returned to CAVEvis and presented to the user to be of most value. At the same time, the FM may be servicing complicated requests that place large data sets in the send queue, such as particle advection. Therefore, this priority mechanism allows the low-latency functions to respond quickly by sending back data using higher priority levels.</p><p>For some requests, the FM will invoke a computation function once and the request will be satisfied, and therefore unscheduled. Examples of this include the generation of an isosurface, a set of streamlines from a single time-step, or a cutplane. However, particle advection consists of many iterations. Each invocation of the advection computation function will generate a set of new particle locations. Each of these sets will have the time-stamp updated, and the result will be scheduled to be sent back to the client.</p><p>Thus, some service requests consist of a single iteration of computation. But important classes of visualization requests require an arbitrary number of iterations and therefore return values; including time-dependent particle-advection or generating a series of isosurfaces, streamlines, etc., each sampling different time steps. For these cases, the computation function is only unscheduled when all the iterations have been completed or when the client tells it to pause or abort the request. This class of iterative requests is handled in a uniform way in the FM using the notion of a sequence of iterative objects (e.g. timedependent) and a successor function attached to a sequence. As in CAVEvis, a sequence is a sorted list of time-stamped objects and is given a symbolic name, which is passed between the client and server to uniquely refer to that sequence. For example, CAVEvis might name the set of particles from the yellow emitter, Yellow-Set. Then a sequence of objects (i.e. particle sets) would be maintained in the FM, under the same name. As each iteration of Yellow-Set particles are sent back to CAVEvis, they would be labeled Yellow-Set. In this way, both modules share the same concept of a sequence of time-dependent data, without having to retransmit intermediate elements in a sequence.</p><p>The advector function becomes the designated successor function for the Yellow-Set. A sequence may only have one successor function so that only one function will be adding to a sequence at a time. As the successor function, it is responsible for generating successive values in the sequence. In this computation model, particle advection is regarded as a multi-valued iterative function, which is handled simply and generically using the successor function mechanism. Under client control, a sequence's successor function may be replaced, deleted, or paused (to temporarily unschedule it).</p><p>The motivation behind the computation model of the FM is to provide a clean way to compute arbitrary functions as if they were asynchronous calls. Thus service requests to the Function Module should be regarded as asynchronous function calls. This model also allows a function caller to receive multiple return values at different times and perhaps in an arbitrary order. Whether one or many values are returned for a function request, the caller does not wait for the values, but must instead provide a means to receive the values whenever they become available. This is done by a reader process of the client. For example, for any message named " F U N C T I O N " that a client sends to the FM, the client will receive and must handle a corresponding message named " F U N C T I O N " which contains the return values. Such a message will be used to transmit return values for each iteration of an iterative function.</p><p>For this model, a high-level library layered on top of sockets and NCSA libraries was created and is utilized for messages between the modules. With this Remote Function Call interface, one can pass and receive the same sort of parameters one would to a locally executed function. The libraries have a C++ streams-like interface, perform argument type checking, and ensure that all arguments have been fully sent and received without error. The underlying libraries handle the fast transmission and conversion of large data sets between heterogeneous architectures. Utilizing this interface, the details of message tranmission can be ignored so that one only needs to focus on the function computation itself.</p><p>The FM was initially created only to advect particles specifically for CAVEvis. As more functionality was added, the design was increasingly generalized until it evolved into the model described above. Thus the model does not relate specifically to the generation of visualization geometries, but only to time-stamped sequences of objects. The FM may serve any client and any number of them, though only CAVEvis utilizes it presently. Its current major services include functions to integrate particle trajectories for pathlines and streamlines, etc., using Euler, Midpoint, or 4th-order Runge-Kutta methods. Other functions include trilinear interpolation within any scalar field and very general mapping of scalar values to arbitrary colors. In the future, dynamically loadable libraries will be used to extend functionality. <ref type="table">R10000  R4400  Operation  1  1  2  4  8  Euler  2328  797 1263 1490 4048  Midpoint  1368  537  877  1240 2714  RK 4th-Order  814  286  480  906  1865  I/O to CAVEvis  822  795  809  795  795   Table 1</ref>: Computation versus I/O throughput, in KBytes per sec. These results were taken from performance tests, where sets of 5000 to 40000 particles were advected and delivered back to the client. Each entry is the average of at least 50 and up to 1000 such operations. In columns where the computation rate exceeds the I/O rate, it means that data is being generated faster than it can be sent.  <ref type="table">Table 2</ref>: Ratios of computation to I/O throughput. Each entry is the ratio of the computation to the I/O rate in the same column of table 1. Ideally it should be less than 1, meaning that the computation function will not be blocked by an overfull Send Queue. The ratio is the amount by which the I/O bandwidth is too slow for the computation function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computation Threads</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computation Threads</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">I/O Performance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Encountering Bottlenecks</head><p>Although the distribution of visualization tasks greatly speeds overall computation and rendering, I/O can become the new bottleneck. Our experience has shown I/O to be the single most constraining factor in the level of interactivity, because it can introduce large latencies. Previously, for example, when the FM server was advecting on the order of tens of thousands of particles with only 4 computation threads (on a multi-processor R4400 machine), the advector function easily outpaced the transmission of the data. The queue of new particle sets, awaiting transmission to CAVEvis, would quickly reach over a hundred, even though the FM and CAVEvis were communicating as rapidly as possible. This had no effect on the rendering or computation speed, but did have a large impact when the user wanted to advect additional particles based on what they were presently observing. The user would try to advect a new set of particles, which would be read by the FM, quickly advected, and then added to the queue of data to send back. But that queue would already contain hundreds of older particle sets. Consequently, the user would only see those new particles many seconds later, even though the actual results had been computed almost immediately after the user's emit action. The latency ranged from one second to over a minute.</p><p>To solve this problem, every instance of a computation function can only have a small number of its return values waiting to be transmitted in the send queue. Beyond this, the computation function is temporarily blocked (i.e. unscheduled) until the sender process can empty enough of its queue. When this upper limit was set to about 4 or 5 and was reached, while advecting roughly 10 to 20 thousand particles, there was a lag of about a second from when the user emitted new particles to when CAVEvis received their new locations from the FM. This illustrates how much I/O is the bottleneck in our system, since the blocking level was so low. <ref type="table">Tables 1 and 2</ref> summarize the results of some performance tests to compare computation to I/O throughput. The computation throughput is the measured speed at which one of the advection functions in the FM can advect and colorize sets of particles. For each particle, there are 12 bytes of position data and 4 bytes of color data, which must be computed and sent back to CAVEvis. All rates are in the same units of kilobytes per second. For these tests, the number of threads used by the FM and the machine on which it ran were varied. For the column headed R10000, an SGI PowerChallenge with one computation thread was used. The other tests used a multi-processor R4400 SGI Onyx, utilizing between 1 and 8 threads for computation. Further details of the computer configuration and network are not important. Instead, the numbers illustrate the important point that the computation performance can vary widely but the I/O bandwidth is fairly constant and is often the limiting factor. In 11 of the 16 entries, the computation rate exceeds the I/O rate, illustrating that I/O is the bottleneck and table 2 shows the degree to which it is. Ideally, each ratio should always be less than 1, meaning that the data can be delivered back to the client faster than it is computed. Therefore the computation function would not block. In a non-distributed visualization system, file I/O and computation are usually the bottlenecks. In our case, however, where we have significant processing power in computation and rendering machines, network I/O is the limiting factor. (Incidentally, we have had similar experience while using high speed networks, such as HIPPI and vBNS. Namely, I/O can always be overrun.)</p><p>More formal performance tests must still be done to determine how much I/O is the limiting factor, relative to computation and rendering times. Since our recent work has focused on adding visualization functionality, rather than maximumizing data throughput, only limited and informal performance measurements have been done so far.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Handling I/O bottlenecks</head><p>I/O is a difficult constraint to avoid. The only real solution is to reduce the amount of data to send. An approach that will be attempted in the future is to perform various types of filtering on the server side. It is often the case that the change in successive particle locations, for many particles, is too small to notice or care about. In one simple approach, a filter could skip particles that have not moved a minimum distance since the last time they were sent. Such a filter would always send fast moving particles, since they would move more than the minumum distance threshold. But slow particles would be sent only periodically, after their accumulated motion exceeds the minimum threshold. Therefore, the slow particles, that move only a neglible distance per step, would not be repeatedly sent, yet the inaccuracy should not be noticeable for small thresholds.</p><p>Slightly more complicated filters could analyze a particle's velocity and skip those that are traveling with near constant velocity over two or three successive steps. A compact bit vector could be used to identify which particles have been skipped in a data set transmission. The client would then assume that the skipped particles' velocities have not changed significantly, and interpolate the new positions based on the last known velocity for each particle. The filter would have to ensure that the positions that would be estimated by the client, for skipped particles, is within some error bound. The FM should send a particle's location just before the client-side error bound would be exceeded.</p><p>In yet another approach, the FM could avoid sending positions altogether and instead send only changes in positions (i.e. velocities). The change in position between successive steps of the same particle is almost always very small. Normally a full 12 bytes are used to encode the 3 floating point numbers in the position. But since this position is used to render onto a dynamic raster device for humans, and isn't used for further computation, far less accuracy is required. Therefore, fewer bits of precision can be used, especially to represent the position change. For example, using merely 10 bits per axis, 30 bits can be used to represent the position change by mapping to a small grid of 1024 3 points that closely surrounds the particle. In the vast majority of cases, the particle's new location will lie inside such a grid, so the 30-bit grid location could be used to compute the new position. As above, the FM would have to keep track of the client's accumulated error in each particle's position. If a particle's new location is outside the grid, or if the accumulated error is too high, then the FM would send the full position.</p><p>In addition to positions, a color value is also sent for each particle that represents a scalar value at that location. The color is 4 bytes of integer data and usually changes with location, which is 12 bytes of floating point data. The color should always be sent since it cannot be predicated like motion. However, some simple compression can also be done since 32 bits of color information is obviously far more than perceptible to humans. Rather than sending the RGB color code, only a color map index should be sent. For the tornado data, the colors cyan and orange are very often used to visually distinguish particles that are qualitatively different-i.e. those that move up versus down. This requires only one bit of color index information to represent the two color entries in this color map. Combining filtering and compression of positions and color, it should be relatively easy and very efficient to greatly reduce the amount of data sent. This in turn would greatly reduce the latency to produce visualizaton geometries and would enhance interactivity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related work</head><p>A number of other systems have utilized virtual environments in scientific visualization. The Virtual Windtunnel <ref type="bibr" target="#b2">[3]</ref> created at the VIEW Lab at NASA Ames Research Center was designed to study unsteady 3-D vector fields. Their VR system utilized a BOOM as the display system and the VPL DataGlove for input. CAVEvis is similar in many ways, in principle, to the Virtual Windtunnel. They both attempt to explore large sets of time-varying scalar and vector fields and utilize a virtual environment for display. CAVEvis however is more distributed in nature. The approach to timemanagement and time-critical (e.g. interactive) computation in the Virtual Windtunnel <ref type="bibr" target="#b1">[2]</ref> differs significantly from CAVEvis.</p><p>Cosmic Explorer <ref type="bibr" target="#b6">[7]</ref> was another large application developed at NCSA that employed the CAVE for visualization. It was designed specifically to explore hierarchical types of data sets, such as those produced in cosmological simulations, that vary widely in scale. At the Electronic Visualization Lab in the University of Illinois in Chicago, the Cosmic Worm <ref type="bibr" target="#b4">[5]</ref> was created to utilize the CAVE as a vehicle for scientific discovery for NCSA's astrophysics group. This application also used a distributed model for visualization. Isosurfaces were generated from simulation data that was being computed in real-time. In some of the same ways as CAVEvis, they also recorded the visualizations computed remotely and then replayed them under user control.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>All along, the goal in developing CAVEvis has been to create a general purpose high-performance visualization tool that effectively utilizes the CAVE virtual environment. Initially this was designed to study scalar and vector field data such as those from our tornado simulations and CFD applications. The size and complexity of the data placed great demands on both the rendering and computation hardware. Consequently, CAVEvis has evolved toward a distributed architecture.</p><p>The task of visualization has been divided up among different modules. Each module performs specific and related tasks and run on machines optimized for those tasks. Despite the power of the individual machines, a great deal of effort was required to ensure that every part of the visualization process would execute as fast as possible and without blocking. This was done to try to achieve the highest levels of interactivity possible and still perform complex data-intensive visualization. The result of this work has been the distributed models of visualization and computation described in this paper.</p><p>The current state of these tools represents only the starting point. By far, most of the work has gone into the overhead necessary to effectively process and render large quantities of data. With this foundation, future work will focus on creating more complex and rich interactive visualizations.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Architecture of CAVEvis and related modules. Data flow is shown by solid lines and arrows. The direction of control signals are shown by dotted lines where the arrows point to the controlled process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>This screen shows the user adjusting the clock widget, which is an interactive control in user space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: In this screen the user is manipulating the location of the blue particle emitter, which is a domain space control object. This is done by "grabbing" it (using a button press) and moving the wand. A new set of particles has just been emitted, by clicking on another wand button. In the figure, the particles from the blue emitter are colored so those going up are more orange, and those going down are more cyan.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 :</head><label>1</label><figDesc>This sample screen shows a number of different visualizations approximately when the tornado touched down. Shown are an isosurface over rain-water density, and particle flow from three emitters, at the ground plane, at the base of the funnel, and vertically along the column of the funnel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>This screen shows the user adjusting the clock widget, which is an interactive control in user space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>In this screen the user is manipulating the location of the blue particle emitter, a domain space control object.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgements</head><p>The author would especially like to thank Crystal Shaw, Jeff Terstriep, and Bob Wilhemson for their continued help, support and feedback. The storm and tornado simulation data was generously provided by colleagues in NCSA and members of the Department of Atmospheric Sciences, including Bob Wilhemson, Lou Wicker, David Wojtowicz, Crystal Shaw, Brian Jewett and Bruce Lee. This work was funded in part by NCSA and NSF grants NSF 92-14098 and NSF 96-33228.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Model-based interactive sound for an immersive virtual environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barbar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc Intl Computer Music Conf</title>
		<meeting>Intl Computer Music Conf</meeting>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Time Management, Simultaneity and Time-Critical Computation in Interactive Unsteady Visualization Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Bryson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandy</forename><surname>Johan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Visualization</title>
		<meeting>Visualization</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="255" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Virtual Windtunnel: An Environment for the Exploration of Three-Dimensional Unsteady Flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Bryson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Creon</forename><surname>Levit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<date type="published" when="1992-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Surround-Screen Projection-Based Virtual Reality: The Design and Implementation of the CAVE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Defanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cruz-Neira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Sandin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Siggraph. ACM SIG-GRAPH</title>
		<meeting>Siggraph. ACM SIG-GRAPH</meeting>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The Cosmic Worm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trina</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="12" to="14" />
			<date type="published" when="1994-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">HDF reference manual</title>
		<ptr target="http://hdf.ncsa.uiuc.edu/refman/refmanual.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Looking In, Looking Out: Exploring Multiscale Data with Virtual Reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Norman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computational Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="53" to="64" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Visualizing Simulated Tornadoes Associated with Supercell and Non-Supercell Convection</title>
		<ptr target="http://www.ncsa.uiuc.edu/general/training/sc95/gii.apps2.html#wilhelmson" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Visualization of Storm and Tornado Development for an OMNIMAX Film and for the CAVE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wilhelmson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th International Conference on Interactive Information and Processing Systems (IIPS) for Meteorology, Oceanography and Hydrology</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
