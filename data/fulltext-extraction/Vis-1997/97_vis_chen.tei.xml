<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image Synthesis From A Sparse Set Of Views</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
							<email>chenqian@iris.usc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Robotics and Intelligent Systems University of Southern California</orgName>
								<orgName type="institution">University of Southern California * PHE 204</orgName>
								<address>
									<postCode>90089</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gérard</forename><surname>Medioni</surname></persName>
							<email>medioni@iris.usc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Robotics and Intelligent Systems University of Southern California</orgName>
								<orgName type="institution">University of Southern California * PHE 204</orgName>
								<address>
									<postCode>90089</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Image Synthesis From A Sparse Set Of Views</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>image-based rendering</term>
					<term>epipolar geometry</term>
					<term>projective invariant</term>
					<term>homography</term>
					<term>Constrained Delaunay Triangulation</term>
				</keywords>
			</textClass>
			<abstract>
				<p>We present an image synthesis methodology and a system built around it. Given a sparse set of photographs taken from unknown viewpoints, the system generates images from new, different viewpoints with correct perspective, and handles occlusion. It achieves this without requiring any knowledge about the 3-D structure of the scene nor the intrinsic camera parameters. The photo-realistic rendering process is polygon based and can be potentially implemented as real time texture mapping. The system is robust to noise by taking advantage of duplicate information from multiple views. We present results on several example scenes.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Conventional VR authoring using solid modeling techniques is time consuming in both the modeling and rendering stages, and the final images look artificial. Recently, people have proposed to use photographs or image sequences directly, with the advantages of modeling in the image space, rendering speed independent of the scene complexity and producing photo-realistic images. While building 3-D models from images has been the theme of computer vision for over two decades, rendering using images has just started. In this paper, we present an image synthesis methodology and a system built around it. The system is purely image based, meaning that at no point during the process is the three dimensional Euclidean information explicitly recovered and/or used. We show that in terms of displaying images, almost everything can be accomplished in the image space, including the correct perspective and occlusion effects which are critical in 3-D perception. We define our problem as follows:</p><p>Given images of a static scene taken from different angles, how to reproject and integrate them into a new image as if it were obtained from a viewpoint that is different from any of the source views.</p><p>Two fundamental questions need to be addressed: how to warp existing frames, and how to resolve occlusion in the generated frame as a result of changing view point. We shall present algorithms to address both of them with the following assumptions:</p><p>• the object is polygonizable. • reflections can be ignored. • the camera lens distortions can be ignored.</p><p>In the following, we first survey related work in this area. We then introduce the computer vision tools to be used. We show a high level flow-chart of our system afterwards. In the following section, we describe in detail the algorithms. Finally, we present our experimental results and conclude the paper with a brief discussion. In this paper, we use interchangeably the words image and view. For example, the given images are called source views, a synthesized image is called a novel view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Existing approaches can be taxonomized into three categories: image based, reconstruction based and light field based.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image based methods</head><p>One early paper in this area is <ref type="bibr" target="#b1">[2]</ref>, where the warping function is simply the scaled down optical flow vector and the visibility is resolved by using the depth value of each pixel which is cached when the frame is rendered. The image planes are parallel to each other and the virtual camera's movement is restricted to be parallel to the image planes. This restriction is relaxed by pre-warping the two source views to a common virtual plane <ref type="bibr" target="#b13">[14]</ref>. This step simplifies problems, because warping then becomes the linear interpolation of the pre-warped views, and depth can be estimated from disparity. This method does not extend to multiple views, because the virtual plane is specified in terms of two views, and only two views. In addition, the virtual camera's movement is still limited in between the model views, a result of using linear interpolation. Arbitrary virtual camera placement is allowed in <ref type="bibr" target="#b9">[10]</ref> by using projective methods. However, only two views are dealt with. Our work extends the method to multiple views and addresses occlusion. In <ref type="bibr" target="#b2">[3]</ref>, each location (a node) is associated with a cylindrical map of the environment which is obtained by rotating a camera and then stitching together the pictures. There is no view interpolation. The viewer has to jump to different nodes. Another pitfall is the limit on the vertical field of view. The same idea of cylindrical projection is used in <ref type="bibr" target="#b11">[12]</ref>. But the latter allows reprojection to an arbitrary location. It extends the traditional structure-from-motion method to cylindrical projections. It simplifies the correspondence matching problem by manually picking "tie points". Then a nonlinear process is carried out to find everything about the camera including its projection center, focus length and structural matrix. With these pieces of information at hand, reprojection is an easy task. During warping, pixels in the reference cylinders are traversed orderly so that the epipoles are visited last. This method suffers from the same drawback of limited elevation as <ref type="bibr" target="#b2">[3]</ref>. All these image-based methods render the new image in a pixel-bypixel fashion, not taking advantage of modern graphics hardware, which is mostly polygon based.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reconstruction based methods</head><p>In <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b12">[13]</ref>, the warping and visibility become non-issues because a complete 3-D reconstruction of the scene is built. Source images are then "pasted" to the reconstructed model. In fact, a different terminology -structure from motion -is used in the computer vision community and the underlying problem has been studied from the beginning of the discipline but still there is no robust system. Accurate results are obtained in <ref type="bibr" target="#b4">[5]</ref> by cleverly restricting its domain to simple geometric primitives such as box, prism and surfaces of revolution, each of which has inherent constraints. In <ref type="bibr" target="#b12">[13]</ref>, the scene is divided into voxels and the object boundaries are found by using the marching cube algorithm, which makes the method expensive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Light field based methods</head><p>The main idea in <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b10">[11]</ref> is borrowed from holography, which is to capture the wavefront of all rays that are emitted from an object. Since a large amount of sample images are needed to produce reasonable quality rendering, both papers deal with the image compression problem. This method is non-geometrical.</p><p>We distinguish our method from the above by not performing camera calibration or 3-D reconstruction. Furthermore, it has several other characteristics (which will be detailed later) which sets it apart from previous work: In essence, the system works as follows: For each feature point in the source images, a scalar value called projective depth is first extracted. It is similar to the disparity in stereo vision but is a projective invariant. It allows us to transfer any feature point to a new view. The images are divided into triangles using Constrained Delaunay Triangulation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8]</ref>. The triangles perform similar roles as the blocks in <ref type="bibr" target="#b1">[2]</ref> in the sense that they carry pixels from the source views to the destination view. But in our case, each triangle corresponds to a planar facet of the real object, hence each pixel inside it can be warped by a 2-D homography. Partial occlusion is handled by drawing the triangles in a certain order, similar to the painter's algorithm, but performed in the image space. Total occlusion is handled by checking the orientation of the polygons. In our setting, correlation based algorithms would be hard to implement as it is difficult to track features across a sparse set of views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">COMPUTER VISION TOOLS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Epipolar Geometry</head><p>As stated in <ref type="bibr" target="#b7">[8]</ref>, given two images of a static scene, if the only information we have about the scene is point correspondence and we have no knowledge about the camera such as focus length, optical center, etc., the strongest constraint we can obtain that relates these two images is the so-called epipolar geometry. This is described by the following equation:</p><p>where F is 3x3 and is called the fundamental matrix, p 1 and p 2 are images of a common 3-D point and are expressed in their homogeneous coordinates. This equation says that p 2 is on the line defined by Fp 1 which is called the epipolar line corresponding to p 1 . All epipolar lines on an image plane intersect at a point called the epipole. When the two planes are coincident, all the epipolar lines are parallel. Said in another way, the epipoles are at the infinity. Fortunately, by using projective geometry, this situation does not have to be treated differently. The epipolar geometry is graphically depicted in <ref type="figure" target="#fig_0">Figure 1</ref>. In <ref type="formula" target="#formula_0">1</ref>, F is defined up to a scale factor. So it has at most 8 independent coefficients. Actually, it is also known that F is of rank 2. Therefore, generally, 7 point correspondences are required to find it. If 4 points are co-planar, then 6 points are sufficient. The recovery of epipolar geometry is a well studied problem in computer vision. For a good overview and state of the art, the readers are referred to <ref type="bibr" target="#b15">[16]</ref>. As seen in <ref type="figure" target="#fig_1">Figure 2</ref>, a plane in space introduces a 2-D homography H between the two image planes. H is a 3x3 non-singular matrix subject to a scale factor, hence has only 8 independent coefficients. 4 point correspondences suffice to define the solution. Please be reminded that to compute H, all we need are point correspondences in the source and destination image planes. We do not</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">2-D Projective Transformation</head><formula xml:id="formula_1">p 2 T F p 1 0 = p 1 p 2 l 1 l 2 o 1 o 2 O 1 O 2 P O 1 O 2 H A B C D</formula><p>require any knowledge about the 3-D points which determine the plane in the 3-D Euclidean space. This implies that, given a set of four matching points, we know how to transfer points inside the trapezoids defined by each of the four point set. Can we transfer points between triangles? The answer is YES, only in this case the fourth pair of matching points is supplied by the epipoles which requires the epipolar geometry to be recovered a priori. The triangle determines a plane which, in projective space, always intersects the line connecting the two camera centers. The epipoles are the images of this intersection point (D in <ref type="figure" target="#fig_1">Figure 2</ref>). Let </p><formula xml:id="formula_2">p ij = [u ij , v</formula><p>where α j is the scale factor. In <ref type="bibr" target="#b1">(2)</ref>, there are 2x4+4=12 unknowns and 12 linear equations, so H is readily solvable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Projective Depth</head><p>Photogrammetrist established a long time ago that photos could be transferred without extracting the 3-D Euclidean information about the scene. For instance, in <ref type="figure" target="#fig_2">Figure 3</ref>, the projection of a 3-D point in the central image is the intersection of the epipolar lines corresponding to the point's projections in the left and right images. In the parallel case, however, the intersection cannot be found because the epipolar lines are parallel too. Shashua <ref type="bibr" target="#b14">[15]</ref> proposed to use the cross-ratio, a projective invariant, to get around this. (We have mentioned that by going to projective space, parallelism does not warrant any special treatment). He named the quantity projective depth. In <ref type="figure" target="#fig_3">Figure 4</ref>, A, B, C, D are four non-coplanar points in the scene. O 1 and O 2 are the camera projection centers. o 1 and o 2 are the epipoles. P is a scene point whose projective depth is to be computed and whose images are p 1 and p 2 . Let the ray O 1 P intersect two hypothetical triangular facets ABC and ACD at I 1 and I 2 respectively, then P's projective depth is defined as the cross ratio of O 1 , I 1 , I 2 and P and is denoted by <ref type="bibr" target="#b2">(3)</ref> In fact, we do not know the coordinates of any of these points.</p><p>However, if we look at their projections on the image plane of O 2 , we immediately see that o 2 and p 2 are already known. Since the cross ratio is preserved under perspective projection, all we need to recover are the image coordinates of I 1 and I 2 in O 2 's image plane (i 1 and i 2 respectively). From the previous section, we know that they are actually the projections of p 1 under the homographies induced by the plane ABC and ACD respectively which is recovered from the images of A, B, C and D. If we denote them by H 1 and H 2 , then P's projective depth can be calculated by (4) can be computed the same way as before. Then from P's projective depth, p 2 can be recovered. What this algorithm says is that if enough point correspondences among three views have been established so that the epipolar geometry is computable, then images from two of them can be transferred to the third one. For the convenience of reference, we will call in our paper the simplex ABCD a projection basis to denote the fact that it is used in a projective sense and to be different from the common definition of projective basis. It should be pointed out that the projection basis is implicitly specified since only the images of the basis points are known.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SYSTEM</head><p>Shown in <ref type="figure" target="#fig_4">Figure 5</ref> is a high level flow-chart of the system. Currently, the low level work is performed manually assisted by the computer. In the future, we want to reverse the relationship -have them done semi-automatically, assisted by a human.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage1 -Corner extraction and matching</head><p>Corner points are extracted and matched manually. Epipolar geometry is then computed between each pair of points. The average distance of each point to its correspondent epipolar line is displayed. This helps the operator to refine those with large errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage2 -Edge detection and labeling</head><p>Again, in our current implementation, this step is done manually, by simply connecting points. In the future, this will be replaced by human assisted edge detector which outputs edges as NURBS. We choose to not use fully automatic edge detection algorithms</p><formula xml:id="formula_4">α j u 2 j v 2 j t 2 j H u 1 j v 1 j t 1 j j , 1 2 3 4 , , , = = p 1 p 2 l 1 l 2 D P ( ) Cross O 1 I 1 , I 2 , P , ( ) = D P ( ) Cross o 2 H 1 p 1 ( ) H 2 p 1 ( ) , p 2 , , ( ) = C O 1 O 2 A B D o 1 o 2 P I 1 I 2 p1 p 2 i 1 i 2</formula><p>because, in most cases, they do not produce results that can be used without a lot of cleanup work. An example of using the Canny edge detector is shown in <ref type="figure" target="#fig_5">Figure 6</ref>. The original image is <ref type="figure" target="#fig_0">Figure  14</ref>(a).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage3 -Constrained Delaunay Triangulation (CDT)</head><p>A Constrained Delaunay Triangulation is then conducted in each source image to divide the scene into triangles. Later each triangle will be warped to the novel view using the projective depths associated with its vertices. A CDT is a triangulation that preserves the existing edges <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8]</ref>. Strictly speaking, a CDT is not a Delaunay triangulation -in order to keep the edges, not all triangles are optimal. There are two reasons to perform the triangulation. Firstly, this avoids identifying and matching faces, another tedious and not-so-easy task. Secondly, this establishes the potential to implement the warping as a texture mapping process and to make use of hardware capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage4 -Triangle sorting</head><p>As will be detailed in 5.3, triangles have to be filled in a certain order so that occlusion is handled correctly in the novel view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage5 -Triangle mapping</head><p>Triangle vertices are mapped to the novel view using the projective depth. Three problems are faced: i) uncertainties associated with the choice of the projection basis. ii) partial occlusion resulting in the formation of T-junctions. One example is given in <ref type="figure" target="#fig_6">Figure 7</ref>; iii) total occlusion -a face disappears in the new view as a result of changing view point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage6 -Triangle filling</head><p>Now that all the vertices are at the right positions (in the novel view), pixels inside them can be filled by warping from source images using a 2-D homography. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ALGORITHMS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Image Warping</head><p>We have stated in the previous section that the warping process consists of two steps: the mapping of the triangles and the filling of pixels within each triangle. For the first purpose, we use the projective depth concept introduced in 3.3. For the second, the projection matrices are computed using <ref type="bibr" target="#b1">(2)</ref>. Thanks to projective geometry, we are able to place the virtual camera arbitrarily and still maintain correct perspective effect in the synthesized images. Understandably, this warping implementation is in fact a texture mapping process, only that the mapping function is defined by a projective transformation rather than an affine transformation as provided by most commercial graphics packages. Had it been implemented in hardware, our algorithm should be able to work in real time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Dealing With Uncertainties</head><p>The accuracy of the recovered projective depth largely depends on that of the chosen basis. We measure the quality of a basis by the average distance of the four points to their corresponding epipolar lines across all views. Among the valid candidates * . we choose the one with the least amount of error. We also take advantage of duplicate information resulted from multiple source views: each point has several estimated mappings, one from each pair of views. Outliers are rejected by thresholding on the variance. The final coordinate of a point is a weighted average of the remaining estimates where the weight is proportional to the inverse of variance.  <ref type="figure" target="#fig_7">Figure 8</ref> shows an example result. The vertices of the irregular polygon around a corner are the candidate points for that corner and their distribution indicates the uncertainty. The outliers have already been removed. The upper-right point comes from two views, and therefore only has one candidate. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Partial Occlusion</head><p>As depicted in <ref type="figure" target="#fig_6">Figure 7</ref>, partial occlusion is characterized by the appearance of T-junctions. A T-junction can be easily identified by checking whether' its match is on the corresponding epipolar line. The difficulty arises, however, when trying to recover its projective depth because its peer is not the real correspondent point in the sense that they are not the images of a common 3-D point. This implies that such a point cannot be reprojected directly. We propose our solution in two steps, referring to <ref type="figure" target="#fig_8">Figure 9</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Recovery of Projective Depth For T-junctions</head><p>Let us name the two edges forming a junction top and support as labeled in the figure. Let us denote the left junction as T l and the right one T r . They were referred to earlier as peer junctions. Let us further denote the correspondent point of T l in the right image as T l ' and that of Tr in the left image as T r '. Since T l ' also locates on the epipolar line of T l in the right image plane, T l ' is the intersec-tion of the right support and the epipolar line. Now that we have the pair (T l , T l '), we can compute the projective depth for T l and reproject it onto the new view which we denote as T l n . If the new view is between the two existing views, there is a gap between T l n and the top in the new view. The same thing is done to T r . But the projected support intersects the top forming an overlap. Consequently, when warping facets from the right view, it must be done in a certain order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Ordering Of Triangular Facets</head><p>If the Euclidean information were available, the order could be easily determined by comparing their depth values. However, what we have is the projective depth, which is a cross ratio, and does not carry any metric information. So how can this be done? We know that occlusion is naturally resolved in all source images. The question then becomes whether the order information is somehow encoded in these images, and, if yes, how to extract it. The answer again lies in the epipolar geometry. Actually, we have seen it in <ref type="figure" target="#fig_3">Figure 4</ref>: I 1 , I 2 , P are all projected to the same pixel p 1 in O 1 , but their depth order is preserved in the image plane of O 2 . To see why this is generally true, notice that if I 1 occludes I 2 , then the sequence O 1 -I 1 -I 2 is preserved under any perspective projection as long as the line O 1 I 1 (or O 1 I 2 ) is not projected into a point. In <ref type="figure" target="#fig_0">Figure 10</ref>, F 1 and F 2 are 3-D facets, f 1 and f 2 are their projections on the right image plane. The darkened line emitting from O 1 is an optical ray. The plain line starting from o 2 is its projection in the right image plane. It is also an epipolar line. If F 1 occludes F 2 , there exists at least one point on F 1 which occludes a point on F 2 but not vice versa (otherwise they would intersect resulting an edge which separates each of them into two smaller facets). Consequently, if in the right image plane a point which belongs to f 1 can be found is closer to the epipole than a point belongs to f 2 , it can be concluded that f 1 should be mapped after f 2 . All triangles are sorted based on this relationship and are mapped accordingly. We say the order determined this way is visibility compatible, the same terminology used in <ref type="bibr" target="#b11">[12]</ref>. Notice that it is only valid with respect to the given views. For a different view, the epipole is changed, thus the order. However, a perturbation to the configuration is merely a perturbation to the existing order which can be quickly rearranged.</p><formula xml:id="formula_5">T r T l ' T l n T l T r n top support H l H r o 2 F 1 F 2 O 1 f 1 f 2</formula><p>This sorting algorithm is therefore very efficient for walk-through type of applications. In implementation, we only compare triangles which share a vertex or an edge. If two triangles share an edge, we compare the opposite vertices. If they share only a vertex, the remaining vertices are checked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Total Occlusion</head><p>Total occlusion happens when a front facet in a source view goes to the back in the destination view and is completely occluded by other facets. Actually the previous ordering algorithm is also applicable here. The occluded facet is guaranteed to be at the beginning of the display list. However, since totally occluded facets are not observable in the new view anyway, we want to be able to mark them so that they will be ignored during warping. It turns out that these facets can be easily identified: when projected to the new view, the orientation of their boundaries is reversed. We can assign an arbitrary vertex sequence (clockwise or non-clockwise) to each facet, and check if it is preserved after the projection. If it is not, then we know the facet is at the back when looking from the new camera position. As a final note, we would like to mention that the same observation has been made in <ref type="bibr" target="#b3">[4]</ref> where the total occlusion is further categorized into orientation-discontinuity occlusion and limb occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Summary</head><p>We now summarize our algorithm into the following steps: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RESULTS</head><p>Test results are shown in the color plates. To specify the novel view, we need some seed points, at least four of them not co-planar to form the projection basis and a large enough number so that the epipolar geometry can be estimated. We have tried three methods: 1) interpolating points from the rectified views using the method of <ref type="bibr" target="#b13">[14]</ref>; 2) interpolating (extrapolating) points directly from source views; 3) picking points from a real view. <ref type="figure" target="#fig_0">Figure 11</ref> is an example involving two source views (a) and (b). (c), (d) depict the corners, edges and T-junctions (circled in red). (e) is the synthesized image. The seed points are obtained using 1). Therefore (e) is physically valid. If (a) or (b) were warped individually, there would be gaps (magenta) and overlaps (blue) as shown in (f) and (g). <ref type="figure" target="#fig_0">Figure 12</ref> shows four frames extracted from an mpeg movie produced by interpolating three views. The seed points are obtained using 2). Although the frames are not physically valid, they do not show any abnormality visually. <ref type="figure" target="#fig_0">Figure 13</ref> is an example of extrapolation. In <ref type="figure" target="#fig_0">Figure 14</ref>, (d) is generated from (a), (b) and (c). The top face is missing because it does not exist in any of the source views. (g) and (h) are synthesized from (e) and (f). Both miss the front thin face of the terminal support. (g) shows the correct handling of total occlusion. But the shape of the screen is not well preserved. This is because at the current stage we do not handle curved edges. Using all five source views, the resultant view (j) includes all visible faces. Seed points are picked from (i). Therefore (d), (h) and (j) are synthesized copies of (i).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">DISCUSSION AND FUTURE WORK</head><p>In section 2, we taxonomized the existing approaches into three categories: image based, reconstruction based and light field based. Among them, the third category is non-geometrical and, from our perspective, deals with quite different issues. For the first two, the second one is harder. So either the domain is restricted <ref type="bibr" target="#b4">[5]</ref> or the method is expensive <ref type="bibr" target="#b12">[13]</ref>. Among the approaches in the first category where ours belongs to, there are two subcategories based on whether or not calibration is used. In the extreme case where the environment is synthetic <ref type="bibr" target="#b1">[2]</ref>, the camera can be thought of as being calibrated exactly and the depth information is perfectly known. In a real environment, camera calibration is a non-linear and unstable process and/or is subject to the range and pattern of the calibration object used. On the other hand, while keeping every thing in the image space avoids the process, it has the shortcoming of not allowing the user to specify the exact viewing parameters. For instance, both <ref type="bibr" target="#b13">[14]</ref> and ours can generate a fly-by of a sequence of views, but they cannot generate a view at a specific location and viewing direction. To be able to do this, there is no way but to recover some Euclidean information. Minimally, the camera's projection center and focus length should be recovered. Fortunately there are ways <ref type="bibr" target="#b7">[8]</ref> to achieve this without pre-calibrating the camera. We are currently exploring this direction. We are also investigating human assisted edge detection methods which can output edges as NURBS curves. We hope that accomplishing these will allow us to model more complex scenes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Epipolar geometry. P -a 3D point; p 1 , p 2 -P's projections; O 1 , O 2 -camera centers; o 1 , o 2 -epipoles; l 1 , l 2 -epipolar lines. When the two image planes are coincident, l 1 and l 2 are parallel, and o 1 and o 2 are at right and left infinity respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Homography between two images planes. Images of a planar facet are related by a 2-D homography. D is the intersection of ABC and the segment O 1 O 2 . The images of D happen to be the epipoles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Image transfer. The correspondent position in the central view is the intersection of the epipolar lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Image warping based on projective depth. Now suppose O 2 is the new camera position. If o 2 and the projections of A, B, C, D in the new image plane are all known, i 1 and i 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>System flow-chart.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Result of using Canny edge detector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Partial occlusion and T-junction. F 2 is partially occluded by F 1 . T 1 and T 2 are T-junctions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Display of uncertainties.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Mapping of T-junctions. T l n forms a gap with the top, while T r n goes across it.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Visibility compatible order.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>ij , t ij ] T , i=1,2 and j=1,2,3,4 be four point pairs where t ij is 0 if p ij is an epipole at infinity or 1 otherwise. By definition, we have</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>The degenerate case happens if a point resides on one of the four facets of the base simplex thus its projective depth is undefined.</figDesc><table><row><cell></cell><cell>Image 1</cell><cell>Image 2</cell><cell>Image n</cell><cell></cell></row><row><cell>Human Assisted</cell><cell cols="3">Edge Detection and Labeling Corner Extraction and Matching corners corners</cell><cell></cell></row><row><cell></cell><cell></cell><cell>edges</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">T-junctions</cell><cell>T 1</cell></row><row><cell></cell><cell cols="3">Constrained Delaunay Triangulation</cell><cell></cell></row><row><cell>Fully Automatic</cell><cell cols="2">Triangle Sorting Triangle Mapping Triangles</cell><cell>F 1</cell><cell>T 2</cell><cell>F 2</cell></row><row><cell></cell><cell cols="2">Triangle Warping Triangle Filling</cell><cell></cell><cell></cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>1. Tessellate each source image. 2. Determine the correct warping sequence with respect to the current new view. 3. Recover projective depth for all vertices including T-junctions. 4. Project all vertices to the new view and remove outliers. 5. Mark those totally occluded facets. 6. Warp all unmarked facets from the source images.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Funding was provided in part by a NSF grant to the Integrated Media Systems Center(IMSC) of the University of Southern California. The authors thank Dr. Zhengyou Zhang of INRIA, France, for making the FMatrix software available.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic Mesh Generation for Complex Three Dimensional Regions Using a Constrained Delaunay Triangulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Engineering with Computers</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="161" to="175" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">View Interpolation for Image Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH 93</title>
		<meeting>SIGGRAPH 93</meeting>
		<imprint>
			<publisher>ACM SIGGRAPH</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="279" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">QuickTime VR -An Image-Based Approach to Virtual Environment Navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Eric</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH 95</title>
		<meeting>SIGGRAPH 95</meeting>
		<imprint>
			<publisher>ACM SIGGRAPH</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recovering LSHGCs and SHGCs from Stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C-K</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="58" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Modeling and Rendering Architecture from Photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH 96</title>
		<meeting>SIGGRAPH 96</meeting>
		<imprint>
			<publisher>ACM SIGGRAPH</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Three-Dimensional Computer Vision -A Geometric Viewpoint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Faugeras</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">3-D Reconstruction of Urban Scenes from Sequences of Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Faugeras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laveau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Robert</surname></persName>
		</author>
		<idno>2572</idno>
		<imprint>
			<date type="published" when="1995-06" />
			<pubPlace>INRIA Sophia-Antipolis</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Research Report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An On-Line Algorithm for Constrained Delaunay Triangulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">De</forename><surname>Floriani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graphical Models and Image Processing</title>
		<imprint>
			<date type="published" when="1992-07" />
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="290" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gortler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grzeszczuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
		<title level="m">Lumigraph. Proc. SIGGRAPH 96</title>
		<imprint>
			<publisher>ACM SIGGRAPH</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="43" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">View Synthesis from Unregistered 2-D Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Havaldar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Graphics Interface&apos;96</title>
		<meeting>Graphics Interface&apos;96<address><addrLine>Toronto, Ontario, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-05" />
			<biblScope unit="page" from="61" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<title level="m">Light Field Rendering. Proc. SIG-GRAPH 96</title>
		<imprint>
			<publisher>ACM SIGGRAPH</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="31" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Plenoptic Modeling: An Image-Based Rendering System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mcmillan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH 95</title>
		<meeting>SIGGRAPH 95</meeting>
		<imprint>
			<publisher>ACM SIGGRAPH</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="39" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reality Modeling and Visualization from Multiple Video Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moezzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Katkere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Y</forename><surname>Kuramura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics &amp; Applications</title>
		<imprint>
			<biblScope unit="page" from="58" to="63" />
			<date type="published" when="1996-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Dyer</surname></persName>
		</author>
		<title level="m">View Morphing. Proc. SIG-GRAPH 96</title>
		<imprint>
			<publisher>ACM SIGGRAPH</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="21" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Projective Depth: A Geometric Invariant for 3D Reconstruction from Two Perspective/Orthographic Views and for Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1993-05" />
			<biblScope unit="page" from="583" to="590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A New Multistage Approach to Motion and Structure Estimation: From Essential Parameters to Euclidean Motion Via Fundamental Matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno>2910</idno>
		<imprint>
			<date type="published" when="1996-06" />
			<pubPlace>Sophia-Antipolis</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Research Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
