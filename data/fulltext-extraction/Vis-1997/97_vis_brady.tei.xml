<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Two-Phase Perspective Ray Casting for Interactive Volume Navigation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Brady</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Microcomputer Research Labs</orgName>
								<orgName type="institution">Intel Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Jung</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Microcomputer Research Labs</orgName>
								<orgName type="institution">Intel Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Microcomputer Research Labs</orgName>
								<orgName type="institution">Intel Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thinh</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Microcomputer Research Labs</orgName>
								<orgName type="institution">Intel Corporation</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Two-Phase Perspective Ray Casting for Interactive Volume Navigation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Volume navigation</term>
					<term>volume rendering</term>
					<term>3D medical imaging</term>
					<term>scientific visualization</term>
					<term>texture mapping</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Volume navigation is the interactive exploration of volume data sets by &quot;flying&quot; the view point through the data, producing a volume rendered view at each frame. In this paper, we present an inexpensive perspective volume navigation method designed to run on a PC platform with accelerated 3D graphics hardware. We compute perspective projections at each frame, allow trilinear interpolation of sample points, and render both gray scale and RGB volumes by volumetric compositing. Our implementation handles arbitrarily large volumes, by dynamically swapping data within the local depth-limited frustum into main memory as the viewpoint moves through the volume. We describe a new ray casting algorithm that takes advantage of the coherence inherent in adjacent frames to generate a sequence of approximate animated frames much faster than they could be computed individually. We also take advantage of the 3D graphics acceleration hardware to offload much of the alpha blending and resampling from the CPU.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Volume rendering <ref type="bibr" target="#b5">[6]</ref> techniques can be used to create informative two dimensional rendered views from large 3D images (volumes) such as those arising in scientific and medical applications. In a typical volume rendering scenario, rays are cast from an observation point outside the volume through the entire volume to obtain a view of the entire volume. In dealing with large 3D data sets, this approach has several problems. First, it can take a prohibitively long time to render a single image. Interactive update of the point of view is thus precluded. This can be a significant disadvantage, since an animated sequence of views sometimes reveals more information than a set of static images, even if the quality of the individual frames is reduced <ref type="bibr" target="#b8">[9]</ref>. In addition, although volumetric compositing techniques display some of the internal data via translucency effects, it can be difficult to discern small complicated internal structures within a large data set when generating an image from the entire volume.</p><p>In volume navigation <ref type="bibr" target="#b1">[2]</ref>, the viewing frustum is placed inside the volume data set. The volume acts as a virtual environment in which the user can navigate (translate and rotate the point of view). Within medical data, for example, one could simulate an endoscopic examination of structures such as bronchial passages, arteries, or the intestinal tract using 3D radiological images <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14]</ref>. Furthermore, this digital technique can be more flexible, allowing one to traverse complex branching structures, pass through solid objects, or render obscuring objects transparent.</p><p>2200 Mission College Blvd., Santa Clara, CA 95052 {Martin_Brady, Kenneth_K_Jung, H_T_Nguyen, Thinh_Q_Nguyen} @ccm.sc.intel.com A key ingredient for volume navigation is the interactive volume rendering of a sequence of frames in real time. We assume that it is acceptable to generate these frames at reduced resolution, in order to enable interactive navigation. This allows one to quickly browse the data looking for interesting structures, and to take advantage of the depth information conveyed by an animated sequence of frames. Although useful information can be obtained from parallel projections in this scenario, perspective projection is required if the views are to look natural, since most of the information is near the viewer. Note that most previous fast volume rendering algorithms have assumed parallel projection.</p><p>In this paper, we present an inexpensive perspective volume navigation method designed to run on a high-end PC platform with accelerated 3D graphics hardware. We compute perspective projections at each frame, allow trilinear interpolation of sample points, and render both gray scale and RGB volumes. Our implementation handles arbitrarily large volumes by dynamically swapping data within the local depth-limited frustum into main memory as the viewpoint moves through the volume. We describe a new ray casting algorithm that takes advantage of the coherence inherent in adjacent frames to generate a sequence of approximate animated frames much faster than they could be computed individually. We also take advantage of the 3D graphics acceleration hardware to offload much of the alpha blending and resampling from the CPU.</p><p>In Section 2, we describe previous work in fast volume rendering and navigation. In Section 3 we present our new rendering algorithm. Section 4 describes our implementation and the results obtained.</p><p>Finally, Section 5 contains some conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PREVIOUS WORK</head><p>A number of different methods have been proposed for interactively exploring volume data. One method is to convert the volume data into a surface-based representation using an isosurface extraction algorithm such as Marching Cubes <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. This preprocessing step is time consuming, taking up to a few minutes, but is done only once. The resulting isosurface representation is compatible with the standard 3D rendering pipeline, and thus takes advantage of standard 3D graphics acceleration hardware.</p><p>This method has proven useful for navigating though data containing fairly well defined surfaces, such as the colon wall within 3D radiological images <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">16]</ref>. The disadvantage of this method is that much of the information contained within the 3D data set is lost in the conversion to isosurfaces. Structures with densities that are not near the isosurface value are not visible. This limits the ability to fully explore a 3D data set by navigating through it.</p><p>Brady, et al., <ref type="bibr" target="#b3">[4]</ref> proposed a method for volume navigation that takes advantage of the inter-frame coherence by rendering a dense set of short parallel ray segments, which are then used to construct entire rays from many neighboring points of view. The method is restricted to parallel projected volume rendering, however. Note that parallel projection is commonly used for rendering 'external' views of volume data and is sufficient, since the distance from the viewer to the data is relatively large. This assumption is violated in the extreme in the case of volume navigation. Useful information can be obtained by browsing the data in this way, but the view is highly distorted, and does not look at all like a 'realistic' walk through the volume data from within.</p><p>We use the general idea of computing multiple views from stored volume rendered ray segments, but extend this idea to render perspective views. This requires a significantly different approach, since the previous method relies on the fact that stored ray segments are parallel. In addition, we show how sets of ray segments can be grouped and defined as 2D textures in order to allow a standard 3D graphics accelerator to perform the final phase of the rendering. A number of previous researchers have proposed various techniques to take advantage of existing 2D or 3D texture mapping hardware to accelerate volume rendering <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b18">18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">FAST PERSPECTIVE PROJECTION VOLUME NAVIGATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>In our navigation scenario, we assume a uniform volumesampled 3D data set of size N 1 × N 2 × N 3 . The user can travel through the volume following an arbitrary navigation path. At any given time, only a small subregion of the volume of size n 1 × n 2 × n 3 is used to produce the current view. (In a highly transparent region, limiting the depth of the view may cause rays to clip before they have saturated. This assumption could be enforced by incorporating an attenuation term to limit the depth of the view.) We assume that the entire volume is too large to fit into main memory, but that the working subregion is small enough to fit. Thus, a dynamic I/O technique is implemented to incrementally update the subregion with data from disk as we navigate.</p><p>Although the effective viewing volume for a single frame is limited, it is typically still too large to be rendered at interactive rates by conventional methods. However, in successive steps, view position or direction can be changed only by a small increment. In general, a navigation trajectory will tend to string together a sequence of incremental steps in a given direction or incremental rotations about a given axis. We use this coherence between nearby frames to accelerate their computation. In particular, intermediate results from the computation of a given view are used to quickly compute f successive frames, after which a new, fresh set of intermediate results must be produced. By amortizing this work over the f frames generated, an approximately factor f speedup is obtained in the core rendering portion of the computation. Below we describe our new twophase perspective ray casting algorithm which is the heart of the method. We then explain how to incorporate it into volume navigation.</p><p>Finally, we describe some extensions to the technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Two-Phase Perspective Ray Casting Algorithm</head><p>Perspective ray casting algorithms often cast a ray at a time, iteratively computing the next sample location, sampling, and compositing the sample onto the current ray. One ray is cast for each pixel in the rendered image. Thus, to render an m × m view at a depth of d samples per ray requires m 2 d sampling steps. Our algorithm is divided instead into two phases. In the first phase, we cast a set of short ray segments, computing a composite color and transparency (i.e., one minus opacity) for each segment. These segments are then used to construct approximations of the full rays in the second phase.</p><p>In Phase 1, the sample points are divided into L levels, 0 ≤ l &lt; L, based on their distance from the viewpoint. The distance of the first sample of level l from the viewpoint is defined as D l . Level 0 consists of a set of ray segments cast from the viewpoint (D 0 = 0) to a distance D 1 -1. Level 1 consists of a set of ray segments cast from distance D 1 to distance D 2 -1, and so forth. We assume for simplicity in the following discussion that the sample rate along the rays is 1 (but in fact this rate can be arbitrarily specified). Under this assumption, each segment in level l consists of D l+1 -D l samples. We refer to the set of segments in level l, computed from position p in viewing direction v, as S l (p,v), or simply as S l when position and direction are understood. The set of all segments at all levels is denoted S(p,v), or simply S, respectively.</p><p>If we choose to sample each level at the screen size, then Phase 1 results in a set of L planes of segments of size m × m. We then simply blend these planes to form the final view in Phase 2. This would amount to a simple reordering of the ray sample and composite operations from the ray-at-a-time algorithm, and would produce the same output, using the same number of sampling steps. In general, however, we allow each level, l, to be sampled at different rates, casting W l × H l ray segments in level l. Phase 2 then resamples each level to screen resolution and then composits the results to form the final view. (Alternately, the overall resampling work could be reduced by iteratively resampling level l to W l+1 × H l+1 and then compositing with level l+1. This turns out to be unimportant, since the resampling step will be performed off the CPU, on the graphics accelerator hardware in our implementation.) Algorithm 1 below gives a high level overview of this technique. Resample segments S l at screen resolution. Composite onto the back of current view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>end for</head><p>An advantage of the two-phase algorithm is that it allows a form of adaptive sampling. Each of the layers can be sampled in the horizontal and vertical directions at a rate near that of the underlying data. Observe that in a standard ray cast, the lateral sampling rates vary in proportion to the distance from the viewpoint. Novins, et al. <ref type="bibr" target="#b12">[13]</ref>, proposed a somewhat similar adaptive sampling approach, with the primary objective of avoiding undersampling of distant sample points by splitting rays and averaging their results. Conversely, our main concern is to avoid oversampling in the regions near the viewpoint, since we are immersed in the data and most of the information is nearby.</p><p>In our use of the two-phase algorithm, we generally set the resolution of each level so that the lateral sampling rates are similar at all levels. A level's dimensions, W l × H l , are then proportional to its distance from the viewpoint, D l . This means that the first levels, being near the viewpoint, may cast very few rays. The result is some reduction in total sampling time, although more important for volume navigation is the fact that there is a large reduction in the sampling time for ray segments near the viewpoint (and little change for the most distant segments).</p><p>The two-phase algorithm can be naturally partitioned between the CPU and 3D graphics accelerator hardware, as described below. In essence, Phase 1 is performed on the CPU, and Phase 2 can be done entirely on the accelerator. Each level, l, of rendered segments from Phase 1 is defined as a 2D texture, T l , in Phase 2. For each level, we draw a rectangle R l at distance D l , perpendicular to the view direction, and map texture T l into the rectangle (see <ref type="figure" target="#fig_0">Figure 1</ref>). (Level 0 is an exception, since D 0 = 0. We choose to draw its rectangle at distance 1, although anywhere between 0 and D 1 will do.) The rectangles are drawn in sequence from front to back, and the texture attributes are set so that the rectangles are alpha blended onto the view to produce a volume rendered frame. One can balance the time spent by the CPU and the accelerator by adjusting the number of levels.</p><p>Note that this scheme requires only 2D texture mapping, and is thus amenable to PC-based workstations with commercially available graphics accelerators. In contrast, many of the previous texture mapped volume rendering methods have required 3D texture mapping hardware, and most have been dependent upon or skewed toward a parallel rendering geometry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Interactive Navigation</head><p>The major advantage of the two-phase algorithm is that many of the segments computed in Phase 1 can be used to construct approximate frames from viewpoints near the original position. We use them for viewpoints within a radius δ of the original position, and within angle θ of the original viewing direction for which the segments were computed. In particular, at each step we recompute the first λ levels, but reuse the last L-λ levels. Note that since the levels near the viewpoint contain fewer segments, these are inexpensive to compute. Thus, given a set of segments S l (p,v), λ ≤ l &lt; L, we quickly compute an approximate view within (δ,θ) of (p,v).</p><p>A straightforward implementation of this idea would be to initialize the S l (p,v) data structure from the current position. Then a sequence of frames can be quickly produced under interactive control, until the position or orientation exceeds one of the thresholds. At this point, a fresh set of segments S l (p´,v´) is computed, and the process continues.</p><p>Unfortunately, this produces jerky motion due to the periodic pauses to recompute the segment data structure. Instead, we amortize the time to compute new segments, assuming that we can reliably estimate the location at which we will require the update. This is true for long sweeps of translation in a fixed direction or rotation about a fixed axis. (This is precisely the case in which smooth motion is most important. If one is changing between translation and rotation, it generally requires a change in user input, and during this time a short delay (~1 sec.) in updating the data structure is more tolerable.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Viewpoint Translation</head><p>Consider the case of forward movement. If we move a distance ∆p in each frame, then a total of f = 2δ/∆p frames are within the scope of a single data structure, S(p,v). The next predicted data structure is S(p+2δ,v). Therefore, we amortize the cost to compute S(p+2δ,v) by completing 1/f of the new data structure in each step. Then, after f steps, we swap in the new data structure and repeat.</p><p>A pseudo code overview of the forward translation algorithm is shown below in Algorithm 2 and illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>. Notice that while the positions of the first λ levels move about with the viewpoint, the last L-λ are fixed, relative to the base position p. Thus, the number of samples within the first λ levels depends upon its offset from p. This is handled by varying the depth of level λ-1 by the amount of the offset, δ. Second, for each step forward, one plane of samples should be rendered onto the back of the last level, so that the total viewing depth remains constant. This is achieved by adding a level L to the back of the data structure that is updated at every step. We have omitted this detail in Algorithm 2 to avoid overcomplicating the discussion. Finally, notice that for positions behind the base (p,v), rays on the border of the frustum may pass outside the border of S(p,v). To avoid this, we compute S at a slightly larger field of view than the viewing frustum to accommodate all positions within radius δ of the base. Specifically, for a square field of view of angle φ, each rectangle R l is expanded by border of δ tan(φ/2) units on each side.</p><p>Algorithm 2 provides nearly a factor of f speedup Phase 1 of the algorithm over rendering each frame independently. Higher values of f increase the error in the approximation, however, and this error is difficult to characterize. Consider a segment s in level l that is to be approximated at position p+offset by resampling S l (p,v) (see <ref type="figure" target="#fig_3">Figure 3)</ref>. Ideally, we would resample by determining a weight for segments near s based on their distance from s. However, segment s is not parallel to any of the neighboring segments in S l (p,v), so it is difficult to define a distance. One could take an average distance of the individual sample points in the segments, but the resulting color and opacity of a segment is a non-linear function of the individual sample values, highly dependent on individual opacities, and tends to emphasize the nearby samples.</p><p>In our implementation, each level S l (p,v) is mapped into a rectangle R l at distance D l , perpendicular to v. Thus, the resampling depends upon relative distances within the plane of this rectangle. Near the center of the view, where the difference in orientation of s from its samples is small, the distances are based on the position of the front of the segments. Towards the edges of the view, the weighting is closer to the middle of the segments. This tends to favor resampling accuracy in the center of the view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Viewpoint Rotation</head><p>For rotation, we must begin with a data structure S(p,v), computed for a field of view f∆v/2 degrees wider than the size of the viewing frustum in the direction of rotation, where ∆v is the size of the incremental rotations. This allows a sequence of f rotations before rays reach the edge of the data structure. In each </p><formula xml:id="formula_0">(p+2δ,v), λ &lt; l &lt; L. if offset &gt; δ for l from λ-2 to L-1 Swap S l (p+2δ,v) in for S l. end for offset = -δ end if /* Phase 2 */ for l from 0 to L-1</formula><p>Resample segments S l at screen resolution at position p+offset.</p><p>Composite onto the back of current view. end for step, we compute 1/f of the next data structure, S(p,v+ f∆v). This is illustrated in <ref type="figure">Figure 4</ref>, and outlined in Algorithm 3. Notice that we do not need to recompute levels 0 through λ-1 at each step, assuming that their field of view is increased by f∆v/2. The accuracy of resampling is less an issue for rotations. Given that the point of view remains at p during rotation, the orientation of the segments at the new rotation are consistent with those in the data structure, and so relative distance between the segments is well defined, and is handled properly by the resampling on the projected rectangle.</p><formula xml:id="formula_1">D 1 D 2 D 3 R 2 R 2 p p + offset σ 1 σ 2 r</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Extensions and Enhancements</head><p>There are several possible extensions to enhance this interactive volume navigation technique. We describe these briefly below.</p><p>The fast approximation technique that we are using assumes that all frames are viewed for no more than 1/10 th of a second, and therefore need not be of extremely high quality. However, during a pause in motion, lower quality views become more noticeable. However, this idle time can be used to compute and display incremental improvements to the original view (see, e.g., <ref type="bibr" target="#b2">[3]</ref> for progressive refinement techniques). For each successive frame during a pause, we successively recompute high resolution renderings for level 0, then level 1, etc., and update the view at each step. Thus the longer the idle time, the farther back into the view the high quality rendering proceeds. (This could be done in several sweeps, in which an intermediate improvement is computed in the first sweep, then a higher quality, etc.)</p><p>Instead of, or in addition to increasing the quality of the rendered segments, one can incrementally increase the depth of the viewing frustum. This can be implemented by adding a level L+1 to the back of the view. We then iteratively sample at depth D l , D l +1, etc., and update level L+1 in each iteration. This will require modification of the dynamic I/O procedure which maintains the current volume region in RAM to obtain more distant samples. However, after a band of data near the current depth is composited, it can be deleted to make room for the next band.</p><p>It is inexpensive to create stereo pairs using the current data structure S, as long as the two eye positions and orientations are within (δ,θ) of the central view. This requires computing the lessexpensive front levels, 0 through λ-1, at two different positions. The higher levels, λ through L-1, are computed at a single central position shared for both eye positions. The technique of reprojecting short ray segments has been used by <ref type="bibr" target="#b6">[7]</ref> to accelerate stereo volume rendering. Earlier work by <ref type="bibr" target="#b0">[1]</ref> accelerated the computation of right eye views by reprojecting individual samples from the left eye's rays.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION OF THE ALGORITHM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation</head><p>Our implementation consists of four major blocks which are performed to compute each frame of output:</p><p>Recast. Recast the first λ levels of segments. Recast and Update are performed on the CPU, and consist primarily of trilinear resampling, lighting, and blending calculations to construct ray cast segments. Opacity is obtained by lookup on the luminance of the sample data. Lighting calculations include ambient and diffuse terms. The computation was simplified in our implementation by assuming a point source located at the viewpoint (i.e., a "miner's helmet" that moves and rotates with the viewer). The gradient computation was reduced to taking a single difference along the ray and scaling the diffuse factor by this term.</p><p>Display is computed almost entirely on the graphics acceleration hardware. The segment data is assembled into textures and dumped onto the accelerator for resampling and alpha blending. <ref type="figure">Figure 4</ref>. The initial segment data structure is depicted in gray, and should be 2θ wider than the field of view. The next segment data structure for right rotation is depicted in black. Algorithm 3. Incremental rotation about a fixed axis.</p><formula xml:id="formula_2">D 1 D 2 D 3 R 2 R 1 R 0</formula><p>Rotate viewing frustum to v+offset at position p. /* Phase 1. Update next data structure */ Compute 1/f of the segments</p><formula xml:id="formula_3">S l (p,v+2θ), λ &lt; l &lt; L if offset &gt; θ</formula><p>Recompute levels 0 through λ-1 from orientation v+2θ at position p. for l from λ to L-1</p><p>Swap S l (p,v+2θ) in for S l . end for offset = -θ end if /* Phase 2 */ for l from 0 to L-1</p><p>Resample segments S l at screen resolution at orientation v+offset. Composite onto the back of current view. end for I/O is more difficult to characterize. Initially, the time is dominated by reads and seeks from the hard disk. In this case, I/O dominates the other three blocks. However, the operating system performs file caching, and once this data has been traversed, I/O accesses hit the RAM and the access times become relatively small, except for occasional misses, until one moves into a new region of the data file. For our measurements, the cache was "warmed" by traversing the data before taking measurements.</p><p>The implementation is dependent on numerous parameters, including L (number of levels), depth of each level, sample rate for each level, field of view, λ (number of levels that are recomputed at every step), f (number of steps between recomputation of the remaining levels), and m (screen size). We selected a set of parameters that seemed a reasonable balance between frame rate and image quality, but due to the large search space, better settings undoubtedly exist. The sampling rates for each level were set sufficiently high so that lateral distance between samples was always less than one unit. The parameters used are given in <ref type="table">Table 1</ref>. <ref type="table">Table 1</ref> The total number of voxels enclosed by the view frustum is approximately 200,000.</p><p>The total number of trilinearlyinterpolated samples used to create the initial display and data structure is 342,160. We measured times on both 8-bit gray scale data (using both color and opacity look up tables) and 24-bit RGB data (requiring an opacity table, indexed by luminance computed from the sampled RGB value on the fly).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance Measurement</head><p>The volume navigation technique was implemented and evaluated on single and quad-CPU platforms. Single processor measurements were made on an Intel 300 MHz Pentium® II processor system. The platform was equipped with 128 MB RAM and an Intergraph Intense 3D Pro graphics accelerator card. The multiprocessor measurements were taken on an Intergraph TDZ-610 platform, containing four 200 MHz Intel Pentium® Pro processors. The platform was equipped with 256 MB RAM, and Intergraph RealiZm Z-25 3D graphics acceleration hardware with 64 MB of texture memory. Both machines ran the Windows® NT 4.0 operating system. <ref type="table">Table 2</ref> contains the main timing results for the algorithm. The code was written as five threads which synchronize in each frame. Two threads handled the Update block in parallel, one handled Recast, one Display, and one I/O. We compare the single-CPU implementation and the multiprocessor versions, on both 8-bit gray and 24-bit RGB data. Translation and Rotation represent the average time to compute a single frame using the segment data structure. The time to recompute the view without the benefit of the data structure using our two-phase algorithm is given for comparison. Finally, we show the time for a standard "brute force" front to back ray cast, in which the number of rays is held constant at 80 by 80, the same as the resolution at the last level of our algorithm. This was done by setting our algorithm parameters for a single level, W 0 = H 0 = 80, which is recast at every frame. Thus, the single Recast thread dominates the computation, and effectively runs on just one CPU. <ref type="table">Table 2</ref>. Times for incremental forward step, incremental rotational step, and time to recompute the entire view from scratch using Algorithm 1. This is compared to the time for a "brute force" perspective ray cast, which cast all rays at the highest resolution used in Algorithm 1. (* The brute force ray cast is dominated by a single thread, and thus effectively runs on a single CPU of the Intergraph platform in this implementation.) For a more detailed breakdown of the times required for each of the major subroutines, we measured the average time per frame for each of the four major blocks, as shown in <ref type="table" target="#tab_1">Table 3</ref>, for both RGB and 8-bit gray scale data. These times were taken on both the Pentium® II and on a single CPU of the Intergraph platform, using a single threaded version of the code. Note that much of the display computation is performed on the graphics accelerator, and can be overlapped with CPU computation, even on a single processor platform. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pentium®</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Images</head><p>We present for comparison images produced by our algorithm, shown in the color plate in <ref type="figure" target="#fig_6">Figures 5 and 6</ref>. The pictures were generated using the Male Visible Human Dataset™, courtesy of the National Library of Medicine. The original data is 24-bit RGB, sampled at .33mm by .33 mm by 1mm. It has been downsampled to 1mm by 1mm by 1mm in our examples. We use the rendering parameters given in Section 4.1. <ref type="figure" target="#fig_6">Figure 5</ref> shows a position within the superior vena cava. <ref type="figure" target="#fig_6">Figure 5a</ref> is a standard perspective ray cast of 80 by 80 rays, the control image that we compare to our approximate views. <ref type="figure" target="#fig_6">Figure  5b</ref> shows a rendering using our two-level algorithm at the same position. In this case, the view point is at the center of the radius δ range, i.e., the offset is zero. <ref type="figure" target="#fig_6">Figure 5c</ref> shows a rendering at the same position, but with the maximum possible offset <ref type="bibr" target="#b7">(8)</ref> for the set of segments. This is a worst case position for our approximation.</p><p>Figures 5d, 5e, and 5f are difference images. In each case, the magnitude of red, green, and blue channel differences is taken, and multiplied by a factor 10 in order to make the values visible. <ref type="figure" target="#fig_6">Figure 5d</ref> depicts the difference between the standard (5a) and two-phase (5b) images, i.e., the error due to the adaptive sampling of the image. Note that much of the error occurs at nearby structures within the image, where the sampling resolution was lowest. <ref type="figure" target="#fig_6">Figure 5e</ref> gives the difference between the two-phase algorithm at zero offset (5b) and the worst-case offset (5c), i.e., the error due to re-using the segment data structure for views at a distance δ from the position from which the segments were cast. The errors are more widely dispersed in this case. <ref type="figure" target="#fig_6">Figure 5f</ref> displays combination of these errors, the difference between <ref type="figure" target="#fig_6">Figures 5a and 5c</ref>.</p><p>A second example, with higher-contrast edges was used in <ref type="figure">Figure 6</ref>. The data set was segmented to remove all tissue except bone. The rendering is of the thoracic vertebrae. <ref type="figure">Figure 6a</ref> shows the standard perspective ray cast, <ref type="figure">Figure 6b</ref> is a rendering using the two-phase algorithm, and the difference is shown in <ref type="figure">Figure 6c</ref> multiplied by 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>Our algorithm provides a unique balance between speed and quality for interactive volume rendering by taking advantage of inter frame coherence. Our implementation is by no means optimized, but serves to verify the promise of the technique. There are a large number of parameters available to trade off these two goals, and we expect that the settings we have chosen can be improved.  <ref type="figure">Figure 6</ref>. Rendering images of the thoracic vertebrae of the bone segmented Visible Human Dataset™. The view is at the cervical vertebrae (vertebral body has been masked out at this section). The vertebral foramen is clearly visible in black and thoracic vertebral body is in pink. (a) standard 80x80 perspective ray cast, (b) two-phase volume rendering, and (c) is the difference of the two, multiplied by 5.</p><p>The Visible Human Dataset™ is courtesy of the National Library of Medicine and its segmentation is courtesy of the Center for Information-enhanced Medicine (CIeMed), National University of Singapore.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1 .</head><label>1</label><figDesc>Two-Phase Perspective Ray Casting. /* Phase 1 */ for l from 0 to L-1 /* Generate a W l × H l array of segments at level l. */ for depth from D l to D l+1 -1 Sample an array of W l × H l points, at distance depth. Composite the array onto the back of the set of level l segments, S l . end for end for /* Phase 2 */ for l from 0 to L-1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>2D illustration of the segments computed by the twophase algorithm. Solid horizontal lines represent the rectangles that the segments are to be mapped into.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>2D depiction of forward translation. Gray lines represent the view from the original position, while black indicate the position of the translated view. In this example, λ = 2; i.e., the first two levels are recomputed (the recomputed segments are depicted in black), while the last two are simply resampled from the segment data structure from the new position. Note that the depth of level λ is reduced by the offset distance so that level λ-1 samples just up to the front of the original S 2 . Algorithm 2. Incremental forward translation. Move viewing frustum forward to p+offset. /* Phase 1. */ /* Recompute S 0 to S λ-2 from position p+offset */ for l from 0 to λ-2 for depth from D l to D l+1 -1 Sample an array of W l × H l points at distance depth, from position p+offset. Composite onto the back of level l segments, S l . end for end for /* Recompute S λ-1 from position p+offset */ for depth from Dλ -1 to Dλ-1 + offset Sample an array of Wλ -1 × Hλ -1 points at distance depth, from position p+offset. Composite onto the back of level l segments, S l . end for /* Amortized update of next data structure */ Compute 1/f of the segments S l</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>A 2D illustration of the resampling of S during translation. A ray r cast from position p+offset (black) is resampled in level 2 using segments σ 1 and σ 2 cast from position p (gray). The (bi)linear interpolation is based on the relative positions of r, σ 1 , and σ 2 projected into R 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Update.</head><label></label><figDesc>Incrementally update 1/f of the predicted future levels λ through L-1. Display. Resample and blend each level at the screen resolution. I/O. If necessary, read slice(s) of raw data from disk and insert into the active cube of data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>. List of parameters used for volume navigation implementation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Rendering images of the right atrium of the Visible Human Dataset™ in optics modality (each voxel is 24-bit RGB) . The view point is at the proximity of superior vena cava entrance. (a) standard 80x80 perspective ray cast, (b) two-phase volume rendering, (c) twophase volume rendering at maximum offset, (d), (e), and (f) are the differences of (a) and (b), (b) and (c), and (a) and (c) respectively. Difference images were multiplied by 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Average times, in milliseconds, for the major blocks of the algorithm. This times were measured on a single CPU on the Pentium® II and Intergraph platforms, by running a single threaded version of the code.</figDesc><table><row><cell></cell><cell cols="2">Pentium® II</cell><cell cols="2">Pentium® Pro</cell></row><row><cell></cell><cell cols="2">300 MHz</cell><cell cols="2">200 MHz</cell></row><row><cell></cell><cell>RGB</cell><cell>Gray</cell><cell>RGB</cell><cell>Gray</cell></row><row><cell></cell><cell>(msec)</cell><cell>(msec)</cell><cell>(msec)</cell><cell>(msec)</cell></row><row><cell>Recast</cell><cell>40</cell><cell>18</cell><cell>64</cell><cell>31</cell></row><row><cell>Update</cell><cell>85</cell><cell>53</cell><cell>131</cell><cell>80</cell></row><row><cell>Display</cell><cell>150</cell><cell>150</cell><cell>71</cell><cell>71</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>During the course of this work, we have benefited from helpful discussions with Lichan Hong, Bill Higgins, and Krishnan Ramaswamy.</p><p>Thanks to the National Library of Medicine for providing the Visible Human Dataset™ used in all of our measurements and images, and to CIeMed for the segmentation of the Visible Human Dataset™.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast Stereoscopic Images with Ray-Traced Volume Rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Symp. On</title>
		<imprint>
			<biblScope unit="volume">Visualization</biblScope>
			<biblScope unit="page">125</biblScope>
			<date type="published" when="1994-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">VolVis: A Diversified Volume Visualization System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Avila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sobierajski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visualization &apos;94</title>
		<meeting><address><addrLine>Washington D. C.</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1994-10" />
			<biblScope unit="page" from="31" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Image Rendering by Adaptive Refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Spach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="29" to="37" />
			<date type="published" when="1986-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Interactive Navigation Inside 3D Radiological Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Biomedical Visualization Symp</title>
		<imprint>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="1995-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Accelerated Volume Rendering and Tomographic Reconstruction Using Texture Mapping Hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Foran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Symp. On</title>
		<imprint>
			<biblScope unit="volume">Visualization</biblScope>
			<biblScope unit="page">131</biblScope>
			<date type="published" when="1994-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Volume Rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Drebin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="65" to="74" />
			<date type="published" when="1988-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast Stereo Volume Rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kaufman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Visualization &apos;96</title>
		<meeting>Visualization &apos;96</meeting>
		<imprint>
			<date type="published" when="1996-10" />
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3D Virtual Colonoscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Viswambharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Biomedical Visualization Symp</title>
		<imprint>
			<biblScope unit="page" from="26" to="32" />
			<date type="published" when="1995-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hierarchical Splatting: A Progressive Refinement Algorithm for Volume Rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Laur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="285" to="288" />
			<date type="published" when="1991-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Marching Through the Visible Man</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lorensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Visualization &apos;95</title>
		<meeting>of Visualization &apos;95</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="1995-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Marching Cubes: A High Resolution 3D Surface Construction Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lorensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics</title>
		<imprint>
			<biblScope unit="page" from="163" to="189" />
			<date type="published" when="1987-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Interactive Technology and the New Medical Paradigm for Health Care</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lorensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jolesz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kikinis</surname></persName>
		</author>
		<editor>R. Satava and K. Morgan</editor>
		<imprint>
			<date type="published" when="1995" />
			<publisher>IOS Press</publisher>
			<biblScope unit="page" from="221" to="230" />
			<pubPlace>Washington D. C.</pubPlace>
		</imprint>
	</monogr>
	<note>The Exploration of Cross-Sectional Data with a Virtual Endoscope</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An Efficient Method for Volume Rendering Using Perspective Projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Novins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">X</forename><surname>Sillion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Greenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="285" to="288" />
			<date type="published" when="1990-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Endoscopic Exploration and Measurement in 3D Radiological Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Higgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPIE Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">2710</biblScope>
			<biblScope unit="page" from="511" to="523" />
			<date type="published" when="1996-02" />
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Direct Volume Rendering with Shading via Three-Dimensional Textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Gelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Symp. on</title>
		<imprint>
			<biblScope unit="volume">Visualization</biblScope>
			<biblScope unit="page">98</biblScope>
			<date type="published" when="1996-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Technical Feasibility of Colon Imaging with Helical CT and Virtual Reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vining</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gelfand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bechtold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Scharling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grishaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shifrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Presented at the Annual Meeting of the</title>
		<meeting><address><addrLine>New Orleans</addrLine></address></meeting>
		<imprint>
			<publisher>American Roentgen Ray Society</publisher>
			<date type="published" when="1994-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Coherent Projection Approach for Direct Volume Rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wilhelms</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Gelder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="275" to="283" />
			<date type="published" when="1991-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hardware Assisted Volume Rendering of Unstructured Grids by Incremental Slicing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yagel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-W</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shareef</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1996 Symp. on Volume Visualization</title>
		<meeting>1996 Symp. on Volume Visualization</meeting>
		<imprint>
			<date type="published" when="1996-10" />
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="55" to="62" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
