<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Case Study: Visualization for Decision Tree Analysis in Data Mining</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Barlow</surname></persName>
							<email>todd.barlow@sas.com</email>
							<affiliation key="aff0">
								<orgName type="institution">SAS Institute Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padraic</forename><surname>Neville</surname></persName>
							<email>padraic.neville@sas.com</email>
							<affiliation key="aff0">
								<orgName type="institution">SAS Institute Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Case Study: Visualization for Decision Tree Analysis in Data Mining</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T18:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Decision trees are one of the most popular methods of data mining. Decision trees partition large amounts of data into smaller segments by applying a series of rules. Creating and evaluating decision trees benefits greatly from visualization of the trees and diagnostic measures of their effectiveness. This paper describes an application, EMTree Results Viewer, that supports decision tree analysis through the visualization of model results and diagnosis. The functionality of the application and the visualization techniques are revealed through an example of churn analysis in the telecommunications industry.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Data mining uses a set of analytical methods for discovering previously unknown relationships in data. The relationships are often complex. Visualization facilitates understanding of complex models that arise in data mining. This paper presents visualization ideas to help users understand a type of predictive model called decision trees. The ideas have been implemented in the EMTree Results Viewer.</p><p>Decision trees are one of the most popular types of predictive models. A decision tree is created by partitioning a large data set into subsets, and then partitioning each of the subsets, until the subsets cannot be partitioned further. In keeping with the tree metaphor, the original data set is the root node, the subsets are nodes, and the unpartitioned subsets are leaves. Branches from a node are the subsets created by partitioning a node. The purpose of building a decision tree is to partition a large heterogeneous group of things (usually people) into smaller, homogeneous groups. By creating homogeneous groups, the analyst can predict with greater certainty how individuals in each group will behave. The final groups, shown as leaves in the tree, are defined by a sequence of partitioning rules.</p><p>Typically a partitioning rule uses a single variable when assigning a case to a branch. Anybody can quickly comprehend a single rule and judge whether it is sensible. Unfortunately, judging the sensibility of a sequence of simple rules is complicated, and a large tree with lots of partitions is difficult to comprehend. EMTree is designed to help analysts build and understand complex decision trees by visualizing the partitioning of cases, which helps the analyst comprehend the predictions of a model, and by visualizing model diagnostics, which helps the analyst assess the reliability of a model. <ref type="figure" target="#fig_0">Figure 1</ref> shows screenshots of the application with 4 of the 14 linked views available to the user. In <ref type="figure" target="#fig_0">Figure 1</ref>, window A contains the assessment plot that shows the subtrees available for analysis. Selecting a point in the plot updates the other views to show details about the selected subtree. Window B contains a compact view of the tree. It shows the tree topology and some information about the quality of the model. Window C contains the traditional tree view and some information about the model. Window D contains a list of variables in the model. Selecting a node in windows B or C highlights the node or row in the other windows. Selecting a variable in D highlights the nodes using that variable in B or C. Each of these views and their relationships will be discussed in more detail later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Visualization of decision trees</head><p>In this paper, we demonstrate the visualization tools in the application through an example of churn data analysis, i.e., the analysis by a telecommunications company of their service subscribers to determine which subscribers should receive special offers to discourage them from switching their service to another company. Using historical usage and cancellation data, an analyst creates a decision tree that predicts the probability that a subscriber would cancel their service. The analyst then inspects the model to judge whether it makes sense, then applies the model to new validation data to judge whether the model is reliable.   <ref type="figure" target="#fig_1">Figure 2</ref> shows a view of the root node in the tree. The text in the node shows that there are 7,020 people in the training data set. Two percent of those people cancelled their service. The tree creates the first two branches by partitioning people based on the number of months remaining in the service obligation. Cancellations occur at a higher rate among people with less than 5.5 months remaining. The higher rate is indicated both by the text and by the intensity of the purple color. Choosing to color the nodes by the main statistic of interest allows the user to turn off the text and view more of the tree.</p><p>Windows B and C in <ref type="figure" target="#fig_0">Figure 1</ref> show the 24-leaf tree selected by the application. The node colors indicate that there are several leaves with concentrations of cancellations high enough to be of potential interest.</p><p>While identifying the leaves with high concentrations of the target group, the analyst also has to determine if the leaves are large enough to be useful in categorizing people. Small leaves are less interesting than large leaves because it is more cost effective and statistically reliable to target a few large groups than many small groups. In the tree, branch line thickness is proportional to the  square root of the number of cases in each branch. The square root transformation helps distinguish differences between nodes with fewer people. This transformation is one of several available to the user when setting branch line width. The user can interactively choose which transformation works best.</p><p>Window B in <ref type="figure" target="#fig_0">Figure 1</ref> shows a compact representation of the tree based on methods of displaying clusters <ref type="bibr" target="#b0">[1]</ref> and hierarchies <ref type="bibr" target="#b1">[2]</ref>. This view maintains the top-to-bottom, left-to-right orientation of the traditional tree view while representing node size through node width. The root node is at the top of the tree. It contains all the people in the data set. All other nodes contain a percentage of those people. Their width is proportional to the number of people in the node. The red lines represent nodes that contain so few people that they cannot be drawn accurately in the space allocated to the view. This view was designed to be used as a navigation tool for larger trees. The user could display the traditional tree with text in the node and navigate around the tree using the compact view. Selecting a node in the compact view moves the same node in the traditional view to the center of the window. When the traditional tree contains text, selecting nodes in the compact view is a way of navigating through the tree without having to manually scroll the traditional view's window.</p><p>The compact view also clearly displays node color and size, the two primary clues for finding subsets of interest. The selection of leaves that are large enough and have a high enough concentration of the target group is a subjective process. Business rules and statistical rules guide the judgment of the analyst. The interaction between the tree and compact tree facilitates this process. Those leaves of a useful size with interesting concentrations of canceling subscribers are selected in both views.</p><p>The analyst now might want to characterize the people represented in the selected leaves, and determine whether the isolation of this group makes sense. In this example, we can simply list the values of the variables required for a person to be in a selected leaf. These rules can be displayed as part of the tree or in a separate window. <ref type="figure" target="#fig_2">Figure 3</ref> shows both methods of displaying rules. The rules window updates to show the rules for the selected node.</p><p>If the rules make sense, the analyst needs to determine if the model is reliable. To test the model, the rules are applied to a validation data set of 7020 cases withheld from the training process. The bar chart in <ref type="figure" target="#fig_3">Figure 4</ref> is one  <ref type="figure" target="#fig_3">Figure 4</ref> shows several leaves in which the validation bar height does not match the training bar height, thereby indicating some unreliability.</p><p>A likely explanation of the poor prediction is that the model overfit the training data, possibly because nodes deep in the tree have too few observations on which to base reliable predictions. Breiman <ref type="bibr" target="#b2">[3]</ref> and Quinlan <ref type="bibr" target="#b3">[4]</ref> recommend pruning large trees to create smaller trees that predict more reliably. To facilitate this, the application automatically generates a series of subtrees with an increasing number of leaves. <ref type="figure" target="#fig_4">Figure 5</ref> shows an assessment plot of subtree effectiveness verses the number of leaves. In this example, effectiveness is measured as the proportion of cancellations among the 10% of customers the tree predicts to be most likely to cancel. A user may interactively change this measure. The two curves show effectiveness on training and validation data. The separation of the curves, with the validation curve below the training curve, suggests that model reliability drops off for subtrees with more than 6 leaves. The flatness of the validation curve between 8 and 23 leaves shows that there is little improvement in predictive accuracy as the subtrees become more complex. There is a slight improvement for the subtree with 24 leaves but the curve is flat for the remaining subtree. Based on these curves, the analyst selects the subtree with 8 leaves. Selecting the point in the plot updates the other views. <ref type="figure" target="#fig_4">Figure 5</ref> also shows the bars for a tree with only eight leaves. The validation bars increase monotonically which is consistent with reliable predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Conclusions and Future Work</head><p>The creation and evaluation of decision trees is an iterative process. It requires visualizations of trees at varying levels of detail, diagnostic plots, and a variety of tables. Our experience and our interaction with other data analysts shows that the analysis process involves frequent switching between views, and modification of views. We have described an application, EMTree Results Viewer, designed to support these activities. EMTree Results Viewer comprises a set of linked views. These views offer a variety of ways of visualizing relevant information. They also interact so that the presentation of the information supports the understanding of relationships among the views. Interviews with users and feedback from early adopters suggest that this approach to decision tree analysis will be successful. NOTE: The EMTree Results Viewer discussed in this paper incorporates major contributions in design and development from Jennifer Clegg, Lina Pratt, John Schroedl, and Pei-Yi Tan.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Four of the linked views in the decision tree application.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Root node with splitting rule and branch statistics</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Tree nodes with text and rules defining selected leaf</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Bar chart showing percentage of cancellation in each leaf for training and validation data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Assessment plot and leaf bar chart for 8-leaf subtree of four views that compare the validation data with the training data. The height of a bar represents the proportion of cancellations in a leaf. The dark, wide bars represent the training data. The light, narrow bars represent the validation data. The leaves are arranged in ascending order of the concentration of people in the training data who cancelled. The model uses the training data to make predictions. If the model were reliable, the bars representing the validation data would increase in a manner similar to the training bars.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Presented at IEEE Symposium on Information Visualization October 22 -October 23, 2001 San Diego Paradise Point Hotel, San Diego</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Representing Points in Many Dimensions by Trees and Castles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Kleiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Hartigan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="page" from="260" to="272" />
			<date type="published" when="1981-06" />
			<publisher>American Statistical Association</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dykstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xdu</forename></persName>
		</author>
		<ptr target="http://sd.wareonearth.com/~phil/xdu/" />
		<imprint>
			<date type="published" when="1991" />
		</imprint>
		<respStmt>
			<orgName>US Army Research Laboratory</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Classification and Regression Trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
			<pubPlace>Wadworth, Belmont, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Programs for Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Quinlan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Morgan Kaufmann</publisher>
			<pubPlace>San Mateo, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
