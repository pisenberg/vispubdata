<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic Depth of Field</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 22 -October 23, 2001</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Kosara</surname></persName>
							<email>rkosara@asgaard.tuwien.ac.at</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Miksch</surname></persName>
							<email>silvia@asgaard.tuwien.ac.at</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helwig</forename><surname>Hauser</surname></persName>
							<email>hauser@vrvis.at</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Vienna University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">VRVis Research Center</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">IEEE Symposium on Information Visualization</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semantic Depth of Field</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published">October 22 -October 23, 2001</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T18:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Depth of Field</term>
					<term>Focus and Context</term>
					<term>Information Visualization</term>
				</keywords>
			</textClass>
			<abstract>
				<p>We present a new technique called Semantic Depth of Field (SDOF) as an alternative approach to focus-andcontext displays of information. We utilize a well-known method from photography and cinematography (depth-offield effect) for information visualization, which is to blur different parts of the depicted scene in dependence of their relevance. Independent of their spatial locations, objects of interest are depicted sharply in SDOF, whereas the context of the visualization is blurred. In this paper, we present a flexible model of SDOF which can be easily adopted to various types of applications. We discuss pros and cons of the new technique, give examples of application, and describe a fast prototype implementation of SDOF.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Whenever large amounts of data are to be investigated, visualization potentially becomes a useful solution to provide insight into user data. Especially for exploration and analysis of very large data-sets, visualization not only needs to provide an easy-to-read visual metaphor, but also should enable the user to efficiently navigate the display, allowing for flexible investigation of arbitrary details.</p><p>Focus and Context (F+C) techniques enable the user to investigate specific details of the data while at the same time also providing an overview over the embedding of the data under investigation within the entire dataset. But F+C encompasses a number of very different techniques that achieve similar goals in very different ways.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Different Kinds of Focus and Context</head><p>The most prominent group of F+C methods are distortionoriented <ref type="bibr" target="#b11">[12]</ref> or spatial methods. The geometry of the display is distorted to allow a magnification of interesting in-formation without losing the (less magnified) context. It is thus possible to navigate information spaces that are far too large to be displayed on a screen. Examples are fisheye views <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20]</ref>, hyperbolic trees <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18]</ref>, stretchable rubber sheets <ref type="bibr" target="#b20">[21]</ref>, etc. Distortion-oriented techniques are usually used in an explicit way, by actively bringing the interesting objects into focus, e.g. by clicking on objects or dragging them around.</p><p>For smaller numbers of objects that have a lot of data associated with them, a visualization method is useful that shows just a limited number of data dimensions, and allows the user to select which of the objects are to be shown in more detail -we call these dimensional methods. The context in this case are not only the other objects, but also the remaining data dimensions. This type of method also shows more detail, but in terms of data dimensions, not screen size. Examples are magic lenses <ref type="bibr" target="#b21">[22]</ref> and tool glasses <ref type="bibr" target="#b1">[2]</ref>, where the user moves a window over the display, the objects inside which are displayed in more detail.</p><p>The third type of focus and context allows the user to select objects in terms of their features, not their spatial relations; usually by assigning a certain visual cue to themwe therefore call these methods cue methods. They make it possible to query the data for information which is not immediately visible in the initial visualization, while keeping the original layout, and thus not destroying the user's mental map <ref type="bibr" target="#b16">[17]</ref>. An example for such a system is a Geographic Information System (GIS) that makes it possible to display crime data, certain cities, or hospitals <ref type="bibr" target="#b13">[14]</ref>. This data is displayed in the same context as before, but the relevant parts of the display have a higher color saturation and opacity than the rest. This leads the viewer's attention to the relevant objects easily without removing context information. In contrast to distortion-oriented techniques and magic lenses, with this type of method, the user first selects the criteria, and then is shown all the objects fulfilling them.</p><p>The technique presented in this paper is of the third type, but we use a different visual cue for discriminating focus and context. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">The Uses of Blur and Depth of Field</head><p>Blur is usually considered to be an imperfection: it makes features harder to recognize and can hide information completely. But the difference between sharp and blurred parts of an image is a very effective means of guiding the viewer's attention. In photography, the depth-of-field (DOF) effect leads to some parts of the image being depicted sharply, while others are blurred <ref type="bibr" target="#b0">[1]</ref>. The viewer automatically looks at the sharp parts, while the blurred parts provide nondisturbing context for the objects of interest (see <ref type="figure" target="#fig_0">Fig. 1</ref> for an example). The same effect is also used in cinematography <ref type="bibr" target="#b7">[8]</ref>, where focus changes can guide the audience's attention from one character to another, from a character to an object he or she just noticed, etc.</p><p>Because the human eye (like every lens system) also has limited DOF <ref type="bibr" target="#b5">[6]</ref>, an important characteristic of human vision is that whenever we get interested in a specific part of our environment, we 1) bring the the object of interest into the center of our eye (where the area of most acute vision, the fovea centralis, is located), and 2) focus on that object. From the above applications of DOF (photography and cinematography), we know that this process is easily inverted: If we display sharp objects in a blurred context, the viewer's attention is automatically guided to the sharp objects. This also gives us reason to believe that DOF is perceived preattentively, i.e. within 50ms of exposure to the stimulus, and without serial search <ref type="bibr" target="#b22">[23]</ref>. This means, it very efficiently makes use of the bandwidth of the human visual system to convey a lot of information in very little time.</p><p>We have developed an F+C technique we call Semantic Depth of Field (SDOF) for information visualization, which renders objects sharp of blurred, depending on their current relevance. It thus makes use of the phenomena described above to guide the viewer's attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There have been surprisingly few attempts to use DOF or blur in visualization at all; the ones relevant to this work are shortly summarized here.</p><p>In a system for the display of time-dependent cardiovascular data <ref type="bibr" target="#b24">[25]</ref>, a stereoscopic 3D display is included that is controlled by the viewer's eyes. Like a microscope, only one thin slice through the data appears sharp, all others are blurred and therefore almost invisible. Eye tracking equipment determines what the user is looking at, and that point is brought into focus. This makes it possible to concentrate on one detail without the surrounding structures confusing the viewer. Later work <ref type="bibr" target="#b25">[26]</ref> describes "non-linear depth cues", which means displaying structures that currently are of interest (like single organs) in focus, and other objects out of focus, not based on their distance from the camera, but on their importance.</p><p>The Macroscope <ref type="bibr" target="#b12">[13]</ref> is a system for displaying several zoom levels of information in the same display space. For this purpose, the images on all levels are drawn over each other, with the more detailed ones drawn "in front", i.e., drawn over the less magnified layers. The layers' transparency can be changed so that the background (context) can be more or less visible. The less detailed layers are blurred so as to not distract the viewer, but serve as context.</p><p>The most interesting existing approach for this work is a display of geographical information <ref type="bibr" target="#b2">[3]</ref>. In this system, up to 26 layers of information can be displayed at the same time. Each layer has an interest level associated with it that the user can change. The interest level is a combination of blur and transparency, making less interesting layers more blurred and more transparent at the same time. This work does not seem to have been followed up on recently. In this paper, we describe a general model of SDOF, i.e., of selectively using sharpness vs. blur to emphasize/deemphasize certain parts of the data. We clearly embed SDOF within the scope of information visualization and computer graphics. In addition to the above examples, we provide a flexible solution which easily is adopted to various kinds of applications, as demonstrated later on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Semantic Depth of Field (SDOF)</head><p>SDOF allows the user to select relevant parts of a visualization that are then pointed out by deemphasizing all the rest through blur. The discrimination between relevant and irrelevant objects can be binary (an object is either relevant or irrelevant) or continuous (an object can have a relevance value between the two extremes).</p><p>Different relevance metrics for objects have to be offered by the application, that have to deal with the specific information and tasks the application is made for. Examples for binary relevance measures are the set of chessmen that threaten a specific piece in a chess tutoring system (see <ref type="figure">Fig. 5c</ref> and the accompanying video), the layer containing roads in a GIS application ( <ref type="figure">Fig. 5d</ref>), or all incidents related to high blood glucose in a graphical patient record. Continuous functions could express the age of files in a file system viewer ( <ref type="figure">Fig. 5a</ref>), the recent performance of stocks in a stock market browser, or the distance of cities from a specified city in terms of flight hours.</p><p>The building blocks of SDOF are discussed in the following subsections, and are summarized in <ref type="figure" target="#fig_1">Fig. 2</ref> and Tab. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Spatial Arrangement</head><p>In information visualization, usually some kind of layout algorithm is used to arrange objects in the visualization space (typically 2D or 3D). The special challenge of information visualization is the fact that data often does not have any inherent structure that naturally translates to a layout. Mapping functions are a very important part of visualization because they determine how well the user can build a mental map that he or she can use to understand and navigate the visualization. Changing the layout often means having to learn a new layout, and thus losing one's ability to navigate easily.</p><p>In our model, the spatial mapping function is called place; it translates from the original data space to an intermediate visualization space (2D or 3D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Relevance and Blurring</head><p>Independently of the spatial arrangement, the blur level of each object is determined. This is done in two steps: First, The relevance function is application-specific and thus can be very different between applications (see Sect. 5.2 for examples). The blur function can theoretically also take on any shape, but we have found the function depicted in <ref type="figure" target="#fig_2">Fig. 3</ref> to be sufficient for our current uses. The user can specify the threshold value t, the step height h, and the maximum blur diameter b max . The gradient g is then calculated by the application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Viewing and Camera Models</head><p>In order to provide a consistent model, and to embed the idea of SDOF in existing work in computer graphics, we discuss camera models for generating images with SDOF. Depending on whether the visualization space is two-or three-dimensional, different camera models can be used to finally achieve the SDOF effect. The camera provides two functions: camera projects data values from an intermediate space (where the information was laid out by the place function) to screen space; and dof, which calculates the blur level of each data item depending on its z coordinate and the z focus value the camera is currently focused at.</p><p>In the following, we describe two camera models: a regular photo-realistic camera (camera P ) can be used in the 2D case; for 3D, we present the adaptive camera (camera A ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">2D SDOF and the Photo-realistic Camera</head><p>In the 2D case, objects get a third coordinate in addition to their x and y values. This additional z value depends on the intended blur diameter b of the object: If the camera is focused at z focus , an object with intended blur b has to be moved to a distance of z from the lens of the camera (see  </p><formula xml:id="formula_0">b = dof P (z, z focus ) = D f (z focus − z) z focus (z − f ) (1) z = dof −1 P (b, z focus ) = D + b D z focus + b f (2)</formula><p>where D is the effective lens diameter as defined in the thin lens model <ref type="bibr" target="#b10">[11]</ref>, and f is the focal length of the lens in use. The above equations apply to camera models such as distribution ray tracing <ref type="bibr" target="#b3">[4]</ref>, linear post-filtering <ref type="bibr" target="#b18">[19]</ref>, etc.</p><p>If the camera uses perspective projection, objects also have to be scaled (and possibly moved) to compensate for depth effects that are not desired in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">3D SDOF and the Adaptive Camera</head><p>In the 3D case, of course, it is not possible to directly map blur factors to depth values, because the spatial arrangement of data items already contains a third dimension. However, using a simple extension of the photo-realistic camera, it is possible to also handle the 3D case.</p><p>The adaptive camera is a modification of a photorealistic camera that can change its focus for every object point to be rendered. This is easily possible with objectorder rendering, but can also be achieved when rendering in image order. In contrast to the photo-realistic camera, the adaptive camera can render SDOF in 2D and 3D scenes. The photo-realistic camera is, in fact, a special case of the adaptive camera (which simply stays focused at the same distance for the whole image).</p><p>Function dof A is defined like dof P in Eq. 1. Different to the 2D case, now the inversion of dof A must be resolved for z focus -values:</p><formula xml:id="formula_1">b = dof A (z, z focus ) = dof P (z, z focus )<label>(3)</label></formula><formula xml:id="formula_2">z focus = dof −1 A (b, z) = D D+b z − b f<label>(4)</label></formula><p>An example for an adaptive camera is splatting <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b23">24]</ref>, which is a volume rendering technique, but which also can be used for information visualization. By changing the size of the splat kernel depending on the b value of a data point, SDOF can be implemented easily. Another possibility is to use pre-blurred billboards (Sect. 6 and <ref type="bibr" target="#b15">[16]</ref>). Objects are rendered into memory, the images are then blurred and mapped onto polygons in the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Properties and Applicability</head><p>This section discusses some high-level properties of SDOF, how it can be principally applied, and what challenges it brings with it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Properties</head><p>SDOF, being yet another F+C highlighting technique, has the following properties that make it an addition to the current toolbox:</p><p>• SDOF does not distort geometry. It is therefore usable when sizes (of objects or parts of objects (glyphs)) and positions are used as visual parameters. We also believe that it is easier to recognize blurred icons than distorted ones.</p><p>• SDOF does not alter colors. If color is used to convey meaning, or the visualization is to be used by colorblind people, SDOF can be used instead of color highlighting. This also means that SDOF is independent of color, and can therefore be used when only gray-scale is available (e.g., printed images).</p><p>• SDOF changes the irrelevant objects, rather than the relevant ones. It is therefore useful whenever the relevant objects contain a lot of information whose perception might be impeded by changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Applicability</head><p>SDOF requires concrete queries to the data (which can be simple, but have to be formulated nonetheless), and is therefore useful for analyzing and presenting data. SDOF can serve as an additional aid to guide the viewer's attention, together with brighter colors, etc., or as a completely separate dimension of data display. Because blur is very naturally associated with importance (even more than color), we do not believe that it is suitable for true multi-variate data visualization. It can, however, add another dimension for a short time, when the displayed data is to be queried.</p><p>Blurring needs space, so when a lot of very small objects are depicted, it is only of little use. The application can deal with this problem by drawing the objects in such an order that sharp objects are drawn on top of blurred ones. But this </p><formula xml:id="formula_3">data[i] place 2D −−−−− −−→ rel −−→ r blur −−→   x y b = blur(r)    view2D −−−−→ dof −1 P −−→    x y z = dof −1 P (b)    cameraP −−−−→ x y data[i] place 3D −−−−− −−→ rel −−→ r blur −−→    x ŷ z b = blur(r)     view3D −−−−→ dof −1 A −−→     x y z z focus = dof −1 A (b)     cameraA −−−−→ x y</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Challenges</head><p>SDOF images depend on the output device (similar to tone mapping <ref type="bibr" target="#b14">[15]</ref>, for example). The reason for this is that blur is not an absolute measure, but depends on the viewing angle that the image covers -this is also the reason why small images look sharper than larger ones: the circles of confusion are not visible in the smaller version, or at least to a smaller extent. We use a calibration step at program startup to account for this problem (see Sect. 5.1).</p><p>Images that contain SDOF effects are also problematic when lossy compression is used (like MPEG, JPEG, etc.). In this case, artifacts can be introduced that create a high contrast in a blurred area, and thus distracting the user. But SDOF is most useful in interactive applications, so this problem should play no big role in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Parameterization</head><p>Parameterization of SDOF consists of two parts: Adaptation to current viewing parameters and user interaction to change the relevance mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Output Adaptation</head><p>We ask the user to select two blur levels on program startup: a) the minimal blur level that can be easily distinguished from a sharp depiction -this value translates to the step height h in <ref type="figure" target="#fig_2">Fig. 3</ref>; b) the minimal difference in blur that can be distinguished -this value can be used to calculate g, if the smallest difference between any two r values is given. Because this is generally not the case, the blur function is adapted for every image after examining the r values of all objects. These values can vary with the use of the generated image (printing out, projecting onto a wall, etc.), the use of different screens, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">User Interaction</head><p>Interaction is a key part of SDOF. Blurred objects are unnatural, and it is therefore important for the user to be able to change the relevance mapping and blur function quickly, and to return to a depiction that shows all objects in focus.</p><p>Depending on the application, there are different usage patterns. In many applications, it is useful to be able to point at an object and say "Show me all objects that are older than this", "Show me all chessmen that cover this piece" <ref type="figure">(Fig. 5e)</ref>, or "Show me the cities weighed by their railway distance from this city".</p><p>Another way is to select values independently of objects: "Show me all threatened chess pieces of my color", "Show me all files that were changed today" <ref type="figure">(Fig. 5b)</ref>, or "Show me all current patients weighed by their need for drug xyz".</p><p>An additional feature we believe is useful is the auto focus. After a pre-specified time, it makes all objects appear sharp again, thus making examination of all objects easier (this function can be turned off).</p><p>Transitions between different displays are always animated to enable the user to follow the change and immediately see which objects are relevant in the new display. This is another reason for separating r and b (see section 3.2): The animation is done between the old and the new b values, rather than the r values. This is because the blur function can contain discontinuities that can lead to jumps between blur levels of objects, and are therefore undesirable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Implementation</head><p>A method in information visualization should not only be visually effective, but also fast, so that it can be used in-e) A chess tutoring system showing the chessmen that cover the knight on e3. c) A chess tutoring system showing the chessmen that threaten the knight on e3. teractively. Blurring used to be a very slow operation because it involves a sum of three color components of many neighboring pixels for every single pixel in the image, and is still not supported by hardware except in high-end graphics workstations. We have implemented SDOF using texture mapping hardware, which makes it fast on currently available consumer PCs. The described method is an implementation of the adaptive camera model (see Sect. 3.3.2). Blur can be understood as a convolution operation of the image with a blur kernel. In photography, this blur kernel ideally is round, but usually is a regular polygon with six to eight vertices, due to the shape of the aperture.</p><p>The more common type of blur kernel in computer science is the box filter <ref type="figure">(Fig. 6, left)</ref>. It has the big advantage of being separable <ref type="bibr" target="#b15">[16]</ref>, which reduces its computational cost from O(n 2 ) to O(2n), where n is the filter size. It can also be generalized quite easily to arbitrary sizes <ref type="figure">(Fig. 6, right)</ref> other than just odd integers. This implementation directly uses b as its filter size n.</p><p>Using graphics hardware is different from a software implementation of a filter in that it does not sum up the color values of surrounding pixels for every single pixel. Rather, it adds the whole image to the frame buffer in one step by drawing it onto a textured polygon (this is done by blending with a special blend function). When the image is drawn in different positions (with one pixel distance between the images), several image pixels are added to the same frame buffer pixel. Because of the limited accuracy of the frame buffer (typically eight bits per color component), this can only be done for small values of n (we have found n ≤ 4 to yield acceptable images).</p><p>For larger blur diameters, we use a two-step approach. First, we sum up four images into the frame buffer, with their color values scaled so that the sum uses the entire eight bits. We then transfer this image to texture memory (this is a fast operation) and use this auxiliary sum as the operand for further calculations. The auxiliary sum already contains the information from four addition steps, so when summing them up further, only one quarter of the addition steps is needed. Because all the values in the box filter (except for the border, which is treated separately) are equal, all auxiliary sums are equal -they are only displaced. This means, that the auxiliary sum only needs to be computed once (as well as another auxiliary for the borders). Summing up auxiliary sums is therefore not only more accurate, it is also faster.</p><p>For blur diameters larger than 20 pixels, we first scale the image to one quarter of its size, then blur with half the diameter, and then scale it back ("quarter method"). Using the described method, it is possible to run applications -like the ones shown in the images and the accompanying video -at interactive frame rates (at leat 5 frames per second) on cheap consumer graphics hardware. This number is likely to increase with some further optimizations as well as the use of multi-texturing (which is supported by more and more consumer graphics cards).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Evaluation</head><p>To show that SDOF is actually perceived preattentively, and to demonstrate its usefulness in applications, we are currently performing a user study with 16 participants. We want to find out a) if SDOF is, indeed, perceived preattentively, which includes the detection and localisation of targets, as well as the estimation of the number of targets on screen (as a number relative to all objects in the image) in the presence of distractors; b) how many blur levels people can distinguish, and how blur is perceived (e.g., linear, exponential, etc.); c) how blur compares to other visual cues which are known to be perceived preattentively (such as color and orientation); and d) how well SDOF can be used to solve simple problems with simple applications (where the emphasis is on the use of SDOF). This study is still in progress at the time of this writing, but we will publish the results as soon as they are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusions and Future Work</head><p>We have presented an extension to the well-known depthof-field effect that allows objects to be blurred depending on their relevance rather than on their distance from the camera. This technique makes it possible to point the user to relevant objects, without distorting the geometry and other features of the visualization.</p><p>Because of the similarity to the familiar depth-of-field effect, and the fact that DOF is an intrinsic part of the human eye, we believe that it is a quite natural metaphor for visualization and can be used quite effortlessly by most users.</p><p>SDOF can be used when analyzing and presenting data, and also seems to be effective as a tool for pointing information out in tutoring systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>A lantern with a bridge as context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>SDOF Building Blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The Blur function.each object is assigned a relevance value r by the relevance function rel. The value of r is in the interval [0; 1], where 1 means the object is maximally relevant, and 0 means the object is completely irrelevant. This relevance value is translated into a blur value b through the blur function blur.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4</head><label>4</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>The photo-realistic camera and 2D SDOF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 . 8 Figure 6 .</head><label>586</label><figDesc>file browser showing the age of files through blur. Continuous relevance function. b) A file browser showing today's files sharply, older ones blurred. Binary relevance function. d) A Geographic Information System (GIS) showing the roads layer in focus. SDOF in action. See Sect. 5.2 and 3 for details. The Box Filter (left), and the generalized box filter for arbitrary sizes (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>All steps necessary for visualizing data values data[i] with 2D (top) and 3D SDOF (bottom).</figDesc><table><row><cell>can introduce artifacts, where parts of the display appear</cell></row><row><cell>sharp only because of the contrast between sharp objects</cell></row><row><cell>and the background.</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We expect to learn a lot about SDOF's properties during our user study, and will use this information to define criteria when and how SDOF can be best used.</p><p>As one of the next steps, we want to investigate the applicability of SDOF to other areas of scientific visualization, like volume and flow visualization.</p><p>We also want to find out how SDOF can be applied to human computer interaction, to enable the user to grasp important information faster, and to be alerted to important changes without being distracted too much.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Acknowledgments</head><p>We would like to thank Markus Hadwiger for his help in coming up with a fast method for rendering SDOF images. This work is part of the Asgaard Project, which is supported by Fonds zur Förderung der wissenschaftlichen Forschung (Austrian Science Fund), grant P12797-INF. Parts of this work have been carried out in the scope of the basic research on visualization at the VRVis Research Center (http://www.VRVis.at/vis/) in Vienna, Austria, which is funded by an Austrian governmental research program called Kplus.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The Camera. Little Brown &amp; Company</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Toolglass and magic lenses: The see-through interface</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Bier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Derose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (Proceedings SIGGRAPH&apos;93)</title>
		<meeting><address><addrLine>Annual Conference Series</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Transparency and blur as selective cues for complex visual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Colby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Scholl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Handling and Reproduction Systems Integration</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">1460</biblScope>
			<biblScope unit="page" from="114" to="125" />
		</imprint>
	</monogr>
	<note>SPIE</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Distributed ray tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carpenter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (Proceedings SIGGRAPH&apos;84)</title>
		<imprint>
			<date type="published" when="1984-07" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="137" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Association for Computer Machinery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Human Factors in Computer Systems, SIGCHI Bulletin</title>
		<editor>M. M. Mantei and P. Orbeton</editor>
		<meeting>the ACM Conference on Human Factors in Computer Systems, SIGCHI Bulletin<address><addrLine>New York, U.S.A.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="page" from="16" to="23" />
		</imprint>
	</monogr>
	<note>Generalized fisheye views</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Sensation and Perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Goldstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998-06" />
			<publisher>Brooks/Cole Publishing Company</publisher>
		</imprint>
	</monogr>
	<note>5 th edition</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fastsplats: Optimized splatting on rectilinear grids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shareef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Crawfis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Visualization</title>
		<meeting>Visualization<address><addrLine>Salt Lake City, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Film directing shot by shot: Visualizing from concept to screen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Katz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<publisher>Focal Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A scalable framework for information visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kreuseler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Information Vizualization</title>
		<meeting>Information Vizualization<address><addrLine>Salt Lake City, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A focus+context technique based on hyperbolic geometry for visualizing large hierarchies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lamping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pirolli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings CHI&apos;95</title>
		<meeting>CHI&apos;95</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Review of image-blur models in a photographic system using principles of optics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="405" to="421" />
			<date type="published" when="1990-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A review and taxonomy of distortion-oriented presentation techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Apperley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Computer-Human Interaction</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="126" to="160" />
			<date type="published" when="1994-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A multi-scale, multi-layer, translucent virtual space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lieberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Information Visualization</title>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Geospace: An interactive visualization system for exploring complex information spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lokuge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ishizaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI&apos;95 Proceedings</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A survey of tone mapping techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Matkovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Purgathofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Spring Conference on Computer Graphics</title>
		<meeting>the Thirteenth Spring Conference on Computer Graphics<address><addrLine>Budimerce, Slovakia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="163" to="170" />
		</imprint>
		<respStmt>
			<orgName>Comenius University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Advanced graphics programming techniques using OpenGL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mcreynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blythe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Course Notes</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Layout adjustment and the mental map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Misue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Languages and Computing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="210" />
			<date type="published" when="1995-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Drawing large graphs with H3Viewer and Site Manager</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munzner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Graph Drawing&apos;98, number 1547 in Lecture Notes in Computer Science</title>
		<meeting>Graph Drawing&apos;98, number 1547 in Lecture Notes in Computer Science</meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="1998-08" />
			<biblScope unit="page" from="384" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A lens and aperture camera model for synthetic image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Potmesil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chakravarty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (Proceedings SIGGRAPH&apos;81)</title>
		<imprint>
			<date type="published" when="1981-08" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="297" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Brown</surname></persName>
		</author>
		<title level="m">Graphical fisheye views. Communications of the ACM</title>
		<imprint>
			<date type="published" when="1994-12" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="73" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stretching the rubber sheet: A metaphor for visualizing large layouts on small screens</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Snibbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">J</forename><surname>Tversky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Reiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Symposium on User Interface Software and Technology, Visualizing Information</title>
		<meeting>the ACM Symposium on User Interface Software and Technology, Visualizing Information</meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="81" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The movable filter as a user interface tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Bier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM CHI&apos;94 Conference on Human Factors in Computing Systems</title>
		<meeting>ACM CHI&apos;94 Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="306" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Preattentive processing in vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Treisman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, Graphics, and Image Processing</title>
		<imprint>
			<date type="published" when="1985" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="156" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Footprint evaluation for volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Westover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (Proceedings SIGGRAPH&apos;90)</title>
		<imprint>
			<date type="published" when="1990-08" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="367" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Four-dimensional processing tools for cardiovascular data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wixson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="53" to="59" />
			<date type="published" when="1983-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The display of 3d MRI data with non-linear focal depth cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wixson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computers in Cardiology</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1990-09" />
			<biblScope unit="page" from="379" to="380" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
