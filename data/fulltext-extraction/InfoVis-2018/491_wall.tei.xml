<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Heuristic Approach to Value-Driven Evaluation of Visualizations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Wall</surname></persName>
							<email>emilywall@gatech.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meeshu</forename><surname>Agnihotri</surname></persName>
							<email>magnihotri6@gatech.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Matzen</surname></persName>
							<email>lematze@sandia.gov</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristin</forename><surname>Divis</surname></persName>
							<email>kmdivis@sandia.gov</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Haass</surname></persName>
							<email>mjhaass@sandia.gov</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Endert</surname></persName>
							<email>endert@gatech.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Stasko</surname></persName>
							<email>stasko@gatech.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">are with Sandia National Laboratories</orgName>
								<address>
									<settlement>Albuquerque</settlement>
									<region>NM</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Heuristic Approach to Value-Driven Evaluation of Visualizations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Visualization evaluation</term>
					<term>heuristics</term>
					<term>value of visualization</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Recently, an approach for determining the value of a visualization was proposed, one moving beyond simple measurements of task accuracy and speed. The value equation contains components for the time savings a visualization provides, the insights and insightful questions it spurs, the overall essence of the data it conveys, and the confidence about the data and its domain it inspires. This articulation of value is purely descriptive, however, providing no actionable method of assessing a visualization&apos;s value. In this work, we create a heuristic-based evaluation methodology to accompany the value equation for assessing interactive visualizations. We refer to the methodology colloquially as ICE-T, based on an anagram of the four value components. Our approach breaks the four components down into guidelines, each of which is made up of a small set of low-level heuristics. Evaluators who have knowledge of visualization design principles then assess the visualization with respect to the heuristics. We conducted an initial trial of the methodology on three interactive visualizations of the same data set, each evaluated by 15 visualization experts. We found that the methodology showed promise, obtaining consistent ratings across the three visualizations and mirroring judgments of the utility of the visualizations by instructors of the course in which they were developed.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Evaluating the utility of visualizations is notoriously difficult <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20]</ref>. While the field of human-computer interaction has provided many techniques to assess the usability of an interactive system <ref type="bibr" target="#b27">[28]</ref>, determining the ability of a visualization to assist in understanding and analyzing data presents unique challenges <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>One approach to evaluating a visualization's utility is to measure accuracy and time in a study where participants perform benchmark tasks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14]</ref>. These studies can be helpful to determine if people can manipulate the user interface and interpret the visualization to read data properly. However, they usually only assess a visualization's ability to communicate data "facts", that is, attributes of individual data elements and core statistical values such as correlations, distributions, and outliers. Many researchers seek to go beyond this evaluation approach in order to determine the potential utility or value of a visualization.</p><p>One approach to achieving a more in-depth assessment of a visualization's utility is the insight-based visualization evaluation methodology <ref type="bibr" target="#b23">[24]</ref>. Using this approach, experts in the domain of a data set put a system to trial use to determine if the tool provides insights that are valuable to its end-users. Evaluators must determine how many insights about a data set the visualization inspired. An insight is defined to be complex, deep, qualitative, unexpected, and relevant <ref type="bibr" target="#b17">[18]</ref>. While determining a visualization's ability to generate insights is clearly a big step toward determining its utility, this evaluation methodology can still be quite challenging. First, the study must be conducted with domain experts who have an appropriate level of knowledge about the data. Further, determining whether a unit of knowledge acquisition is an "insight" or not is still relatively subjective.</p><p>An alternative approach to determining utility is to deploy a visualization in the field and conduct a more in-depth, longitudinal evaluation. This type of study seeks to move beyond the limitations of short-term, lab-based evaluations. Perhaps the best known example of this evaluation methodology is the MILC (Multi-dimensional In-depth Long-term Case study) technique <ref type="bibr" target="#b26">[27]</ref> that has been used to evaluate political analysis, biomedical research, and intelligence analysis <ref type="bibr" target="#b18">[19]</ref>. System use is observed "in the field" as people apply it to real data and problems. The power and potential benefit of this approach for helping to determine the utility of a visualization is obvious. However, such an evaluation may be logistically challenging, very time-consuming, and pragmatically difficult to implement. Developers of new visualization techniques may seek evaluation methodologies that are lower cost but still achieve many of the same benefits.</p><p>In 2014, Stasko proposed a new framework for identifying the value of visualization <ref type="bibr" target="#b29">[30]</ref>. In particular, this approach sought to move beyond the types of questions and tasks usually found in usability studies. As stated in the article, "[A measure of value] goes beyond the ability to support answering questions about data-it centers upon a visualization's ability to convey a true understanding of the data, a more holistic broad and deep innate sense of the context and importance of the data in 'the big picture'." The value framework contained four components corresponding to the time savings a visualization provides, the insights and insightful questions it spurs, the overall essence of the data it conveys, and the confidence about the data and its domain it inspires. The evaluation approach advocated in the work was largely descriptive. Each of the four components was explained, but no concrete techniques for assessing a visualization along those components was provided. To be more pragmatically beneficial, an accompanying evaluation methodology or corresponding prescriptive approach is also needed.</p><p>The goal of our research is to develop just such a methodology. We seek to provide an evaluation approach to estimate and even quantify the potential value of visualization for understanding a data set, centered on the four value components introduced in <ref type="bibr" target="#b29">[30]</ref>. We also want this approach to be relatively "low cost" in terms of time and resources required to employ it. We fully acknowledge that longitudinal studies of deployed system usage are the hallmark for truly understanding a system's value. We similarly seek an approach that provides feedback about a system's utility, especially that beyond simple low-level task completion. But we seek an approach that is practical and relatively easy to utilize, one providing rapid feedback that also allows comparisons to be drawn between different visualization applications.</p><p>We intend the methodology to be useful for evaluations of the potential utility and value of both research and commercial visualization applications. Researchers and developers frequently desire feedback about new systems they develop and want help identifying the strengths and limitations of their systems. Other potential uses include evaluation and grading of academic class projects or visualization contests and providing information to decide between commercial tools. Our goal is not to replace traditional time and error usability evaluations, but to complement existing evaluation techniques with a higher-level, value-driven evaluation focus.</p><p>In this paper, we describe the development of a methodology that enables a quantitative assessment of a visualization's value according to the value equation. Our approach to this challenge involved the identification of more specific guidelines under each component, and then a set of low-level heuristics to be judged under each guideline. All three levels of the value framework combine to create a form for use in evaluating a visualization.</p><p>This article describes the process we undertook to create the evaluation methodology and an initial assessment in which 15 visualization researchers evaluated three different visualizations of the same data set developed by student groups in an information visualization course project. Although our expert visualization participants expressed doubts about the evaluation instrument's ability to assess the value of the visualizations, their evaluation responses were consistent, achieving high inter-rater reliability. Their average ratings mirrored instructor feedback on the visualizations from course project evaluations. Thus, we believe that this evaluation methodology shows promise as a low-cost estimate of a visualization's value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Evaluating visualizations is an open and difficult research challenge <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20]</ref>. This complexity stems from the broad set of design goals that visualizations can be built to support. For many of these goals, specific evaluation methodologies have been presented <ref type="bibr" target="#b14">[15]</ref>.</p><p>For example, visualizations are commonly designed to help the user gain insights about a data set. In response, North et al. presented insight-based evaluation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24]</ref> as a methodology to assess how well a visualization supports people gaining insight into the data being shown. However, as discussed above, operationalizing the methodology is challenging due to the difficulty of defining, observing, and counting insights <ref type="bibr" target="#b3">[4]</ref>. Further, insights may be dependent on the domain expertise or familiarity with the data set, making it difficult to use as a benchmark by which multiple visualizations can be compared.</p><p>Alternatively, task performance methodologies can be used. These approaches set up a series of tasks that users should be able to complete with the given visualization <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b30">31]</ref>. Then, metrics such as task completion time and accuracy are used to evaluate how well a visualization performed. While these methodologies provide quantitative data which can ease comparisons, designing the set of tasks can be subjective, and the data sets require ground truth in order to evaluate accuracy. In addition to task performance and usability approaches, methodologies exist that evaluate visualizations based on user experience goals such as engagement, enjoyability, memorability, and others <ref type="bibr" target="#b22">[23]</ref>.</p><p>Particularly within the visual analytics community, contests have been used to help evaluate data visualization systems <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26]</ref>. Developing data sets, problems, and scenarios for such contests is extremely time-consuming and difficult <ref type="bibr" target="#b5">[6]</ref>, however, and each focuses on a very specific type of data.</p><p>Deployment studies, where a system is used for everyday tasks in context outside the lab, provide a deeper look into a visualization system's utility. The MILC technique <ref type="bibr" target="#b26">[27]</ref> is one example of this approach. Such evaluations generally are viewed as powerful instruments of assessment, but they can be logistically challenging and time-consuming. Grounded in research from the human-computer interaction community, heuristic-based evaluation methodologies <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> for visualization have been proposed <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35]</ref>. For example, Amar and Stasko <ref type="bibr" target="#b0">[1]</ref> identify heuristics designed to cover the known "gap" in visual analytics processes. However, these heuristics are fairly high level, suggestive, and provide limited guidance on improving specific visual or interactive aspects of a visualization tool. Conversely, Zuk and Carpendale <ref type="bibr" target="#b34">[35]</ref> suggest a set of ten "Cognitive and Perceptual Heuristics" for designing visualizations. But their high specificity in wording leads to less flexibility in interpretation from one visualization to another. Forsell and Johansson <ref type="bibr" target="#b9">[10]</ref> instead compiled 63 published heuristics and tested them on a collection of 74 usability problems from previous information visualization evaluations to identify the top 10 heuristics that covered 87% of the 74 problems. However, as Tarrell et al. <ref type="bibr" target="#b31">[32]</ref> point out, by broadly wording such heuristics, they may be misinterpreted by different evaluators. Furthermore, as these heuristics have solely been tested on usability issues, they might not be effective for visual data analysis and reasoning evaluations. Some researchers have compared heuristic evaluation of visualizations to alternatives like usability evaluation with benchmark tasks <ref type="bibr" target="#b28">[29]</ref> or having evaluators answer questions about the data <ref type="bibr" target="#b11">[12]</ref>. These studies revealed that heuristic evaluation can complement other evaluation alternatives.</p><p>Tory and Möller <ref type="bibr" target="#b32">[33]</ref> adapted an expert review process with heuristics to get feedback on design alternatives for specific visualizations. They found that experts can provide quick and useful feedback on specific design goals and heuristics. Ardito et al. <ref type="bibr" target="#b1">[2]</ref> provided additional context and guidance around heuristics to assist less skilled inspectors in the evaluation of domain-specific visualization tools. Perhaps most closely related to our work is the heuristic-based methodology by de Oliveira and da Silva <ref type="bibr" target="#b21">[22]</ref>. They presented a set of 15 heuristics based on common visualization design goals distilled from a literature review. While these heuristics are meaningful, a method for translating them into an operational methodology is missing. Our proposed value-based methodology includes not only heuristics, but realizes them in a full methodology.</p><p>Finally, using value or utility as a metric to characterize visualizations has been previously explored, though in a markedly different approach than we follow. van Wijk proposed an economic value model <ref type="bibr" target="#b33">[34]</ref> that mathematically represents and calculates the value of a visualization in purely numerical terms. We posit that the value of a visualization is often difficult to define through strictly mathematical terms, and thus adopt a heuristic-based approach for determining value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">VALUE OF A VISUALIZATION</head><p>In 2014, Stasko explored some of the different objectives of evaluating a visualization <ref type="bibr" target="#b29">[30]</ref>. Potential goals include improving a system, comparing two systems, or simply determining the quality or "goodness" of a system. While all are helpful applications of evaluation, he argued for a broader, more encompassing notion of the value of a visualization.</p><p>Some of the motivation for this focus on value was to move beyond evaluations involving participants performing low-level benchmark tasks and answering specific questions about a data set. While this type of evaluation can help determine whether a visualization is learnable and comprehensible, it fails to examine some of the larger benefits of visualization. Stasko felt that these larger benefits are what makes visualization unique among data analysis approaches. He states, "Visualization should ideally provide broader, more holistic benefits to a person about a data set, giving a "bigger picture" understanding of the data and spurring insights beyond specific data case values." <ref type="bibr" target="#b29">[30]</ref> He described a simple value equation</p><formula xml:id="formula_0">V = T + I + E + C</formula><p>with the following four components:</p><p>• T -A visualization's ability to minimize the total time needed to answer a wide variety of questions about the data • I -A visualization's ability to spur and discover insights and/or insightful questions about the data • E -A visualization's ability to convey an overall essence or takeaway sense of the data • C -A visualization's ability to generate confidence, knowledge, and trust about the data, its domain and context.</p><p>The article introducing this value equation was limited to a qualitative discussion of its details and the four components. While examples of applying the equation to specific visualizations were provided, they were simply narrative descriptions. No accompanying methodology or quantitative breakdown was provided, thus it lacked prescriptive power to evaluate visualizations and compare their potential values. Hence, our goal in this work is to provide an actionable methodology to accompany the value equation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DEVELOPING THE HEURISTICS</head><p>We developed the value-driven visualization evaluation heuristics through an iterative process, shown in <ref type="figure" target="#fig_1">Figure 2</ref>. As a starting point, we surveyed many visualization evaluation and design heuristics papers to help generate an initial list of 17 heuristics of value, each falling under one of the high-level components from the original value equation. We held four additional brainstorming sessions, one for each high-level component, resulting in the expansion of the list of heuristics to 70. At this point, our objective was to be inclusive of all reasonable heuristics.</p><p>With the list of heuristics expanded, we conducted a half-day workshop with three high-level goals: (1) refine the list of heuristics, (2) assess the rateability of each heuristic, and (3) test the heuristics on visualizations. We first removed any heuristics that were very similar to others. Each member of the research team also selected the five heuristics that they viewed as most important for each high-level component in order to establish a type of prioritization.</p><p>Next, we discussed the rateability of each heuristic. We each assessed all of the heuristics in two ways: as rateable with a yes/no judgment and through a more nuanced low/medium/high judgment. Heuristics that could not be rated with either approach were eliminated or reworded.</p><p>Then the research team was joined by two additional visualization experts who had not been involved in the process of developing the heuristics. The group studied a sample visualization and each individual rated each heuristic. We examined the resulting ratings to assess their consistency. We discussed these ratings at length to understand the causes of any particularly noteworthy disparities.</p><p>We found that different individuals interpreted some heuristics in different ways, so we rephrased them. For example, we changed "The visualization facilitates learning more broadly about the domain of the data" to "The visualization promotes understanding data domain characteristics beyond the individual data cases and attributes." The initial phrasing was more abstract and led raters to focus on specific data points or attributes they may not have previously known about. However, our goal with this heuristic was to promote a higher-level understanding of the domain (the "forest") rather than small details of knowledge about specific data points or attributes (the "trees").</p><p>Some heuristics were not difficult to understand but turned out to be very difficult to rate. For example, we altered "The visualization highlights potential data issues like unexpected, duplicate, missing, or invalid data" to become "If there were data issues like unexpected, duplicate, missing, or invalid data, the visualization would highlight those issues." The first phrasing proved difficult because it presumed problems in the data that might not be present. If a rater did not spot such data issues, was it because the visualization failed to highlight them or because none were present?</p><p>Some heuristics that were difficult to rate were discarded or rephrased. Despite rephrasing, a few remained difficult to rate, but we were reluctant to remove them because we felt that they ultimately captured an important aspect of a visualization's value. For example, the guideline "The visualization provides opportunities for serendipitous discoveries" proved difficult for assigning a rating, but we felt that it captured a core element of insight. This ultimately led us to restructure the value framework into a three-level hierarchy, adding a set of mid-level guidelines to each of the four components.</p><p>To form the hierarchy, we conducted an affinity diagramming exercise to organize the heuristics into their new structure ( <ref type="figure" target="#fig_0">Figure 1</ref>). We then repeated the process of rating a visualization, analyzing inconsistencies, and rephrasing, removing, or adding to the hierarchy of components, guidelines, and heuristics. The resulting hierarchy is presented in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Components</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Guidelines</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Heuristics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Insight</head><p>Confidence Time Essence <ref type="figure">Fig. 3</ref>: The structure and terminology used to describe the hierarchical value framework. Each component is made up of guidelines which describe important aspects of the high-level component. Each guideline is then comprised of a small set of low-level heuristics that are designed to be actionable, rateable statements reflecting how a visualization achieves that guideline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">VALUE-DRIVEN EVALUATION METHODOLOGY</head><p>The value framework consists of three hierarchical levels ( <ref type="figure">Figure 3</ref>). The top level contains the original four components: insight, time, essence, and confidence. Within each component, a small set of midlevel guidelines capture the core concepts of the high-level components.</p><p>Finally, each guideline contains one to three low-level heuristics. We developed these heuristics to be actionable, rateable statements that embody the core concepts of the guidelines and components in the hierarchy above them. Hence, the upper-level guidelines and components themselves are not intended to be directly rated in this methodology. Instead, the ratings of the individual heuristics are aggregated up the hierarchy to form the overall score for a visualization (described in more detail later). We informally refer to the methodology as ICE-T, an anagram of the four value components (Insight, Confidence, Essence, and Time).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Framework Realization</head><p>We present the entire value hierarchy in <ref type="figure">Figure 7</ref>. Below, we briefly describe the contents of each component.</p><p>Insight -This component is comprised of three mid-level guidelines, which are roughly intended to capture how a visualization supports intentional and incidental insights. Intentional insight refers to tasks or questions a person sets out to address, while incidental insight refers to serendipitous discoveries where the user may have stumbled upon an unexpected piece of knowledge.</p><p>Time -This component is comprised of two mid-level guidelines, intended to capture how a visualization facilitates faster, more efficient understanding of data with respect to both searching and browsing of data. Searching refers to a user's deliberate task to locate particular information within a data set, while browsing refers to a user's more casual scanning of a data set to find potentially interesting information.</p><p>Essence -This component is comprised of two mid-level guidelines, intended to capture how a visualization communicates the essence of the data set with respect to overview and context. Overview refers to a high-level view or summarization of the data set, while context refers to relevant information surrounding the data set.</p><p>Confidence -This component is comprised of three mid-level guidelines, intended to capture how a visualization helps a user feel confident in his/her understanding of the data set with respect to the quality of the data and quality of the visualization. Confidence in the quality of the data refers to an understanding of potentially missing or erroneous data, while confidence in the quality of the visualization refers to an understanding of the accuracy of the representation of the data (e.g., does the visualization mislead?).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation</head><p>We intend the methodology to be administered using a survey (available in the supplemental materials). Each heuristic should be individually rated for a visualization along a 7-point scale ranging from 1-strongly disagree to 7-strongly agree, or N/A-not applicable. All the heuristics are stated in a positive manner, that is, a higher score (strongly agree) aligns to a visualization being more valuable. We performed a trial of the methodology in the early stages of developing and refining the heuristics with static and minimally interactive visualizations. We found that many heuristics were not applicable to these types of visualizations because they assumed that the rater could interact with the data. For example, the heuristic "The interface supports using different attributes of the data to reorganize the visualization's appearance" is not applicable to a static visualization. Thus, the methodology is intended to be applied to interactive visualizations.</p><p>To evaluate a visualization, a small number of visualizationknowledgeable raters should interact with the visualization and complete the survey. We recommend five raters based on our analysis in Section 7.1.2. These raters should have knowledge about and experience working with visualizations. Domain knowledge is also relevant, so the raters should have at least some familiarity with concepts from the domain of the data set being visualized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Score Aggregation</head><p>In order to achieve an overall value rating for a visualization, we propose an initial approach of aggregating scores at each level of the hierarchy using a simple average. Let s h be the score for a heuristic h ranging from 1-7 identified by a rater. Each mid-level guideline is scored by averaging its corresponding j low-level heuristics from the hierarchy:</p><formula xml:id="formula_1">s g = 1 j ∑ j i=1 s h,i .</formula><p>Each high-level component is then scored by averaging its corresponding k guideline scores:</p><formula xml:id="formula_2">s c = 1 k ∑ k i=1 s g,i ,</formula><p>where c ∈ {insight,time, essence, con f idence}. Finally, a visualization's overall score is defined as s = <ref type="bibr" target="#b0">1</ref> 4 (s insight +s time +s essence +s con f idence ). This method serves as an initial aggregation approach, not favoring any one component, guideline, or heuristic over another. In a subsequent section, we discuss alternative ways that scores might be aggregated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ASSESSING THE METHODOLOGY</head><p>To assess this value-driven evaluation approach, we conducted a user study with visualization experts who we asked to use the methodology to rate three visualizations. In this assessment, our primary goals include (1) assessing the inter-rater reliability of the evaluators' ratings and the corresponding statistical power, (2) understanding how heuristic ratings map to properties of individual visualizations, (3) gauging evaluators' confidence in assigning scores to heuristics, and (4) gathering overall impressions of the methodology from the visualization experts. This section describes the design of the assessment and the results are presented in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Participants</head><p>We sent a recruitment email to 23 people, all of whom hold a Ph.D. and perform visualization-related research, so thus can be considered visualization experts. A total of 15 participants (12 male, 3 female) ultimately completed the study. The participants included six research staff, eight professors, and one software engineer. The participants had a range of 7-30 years of professional experience in the field of visualization (mean = 14 years).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Materials</head><p>For the experiment, we selected three visualizations developed by student groups in an undergraduate information visualization course at Georgia Tech (Visualization A 1 , B 2 , and C 3 ). Rather than choosing existing published or publicly available visualizations, this ensured that participants in the study would not have prior exposure to the visualizations. Further, the three visualizations all utilize the same data set (information about U.S. colleges) to ensure that there were no confounding differences between visualizations in terms of the participants' familiarity with the data sets. <ref type="figure" target="#fig_2">Figure 4</ref> shows the three visualizations. Visualization A used a map to show college locations and parallel coordinates for comparing attribute values. Visualization B used a scatterplot, two focus views, and extensive filtering and interaction. Visualization C employed a bubble clustering view along with a scatterplot.</p><p>We explicitly chose visualizations with varying quality and design decisions to try to capture a larger range of ratings in the value heuristics. Project grades from the course, assessed by the professor and teaching assistants, suggested that Visualizations A and B would receive higher scores than C, with Visualization B slightly ahead of A. This ordering corresponded to the research team's assessment of the relative value of the three visualizations, based on a qualitative assessment of their features and design choices. Therefore, we predicted that if the valuedriven evaluation methodology is effective, Vis B would receive the highest overall score and Vis C would receive the lowest.</p><p>Each participant rated all three visualizations via a web-based survey form. They scored the 21 low-level heuristics using the 1-7 rating scale described earlier. We further augmented the survey for the purposes of this assessment so that each low-level heuristic rating was accompanied by a rating of the participant's confidence in assigning the score, judged from 1 (very low) -4 (very high). We gave evaluators no specific directions beyond assessing the potential value of each visualization. We were confident that all knew the data domain well because of their background with universities and higher education.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Procedure</head><p>We emailed participants an electronic consent form to sign and return. Upon receiving signed consent, we emailed participants instructions for completing the study, including a background questionnaire, a link to the online survey containing the value hierarchy and heuristics, and links to the three interactive visualizations, each with an annotated screenshot to inform participants about each visualization's affordances.</p><p>We asked the participants to examine the annotated screenshot for a visualization, then use the visualization to familiarize themselves with its representation, interactions, and data. Finally, we asked them to complete the heuristic survey for the visualization. In addition to rating each heuristic and denoting their confidence in that score, participants also had the option of typing comments about each heuristic.</p><p>We used a pseudorandom order of visualizations to minimize potential ordering effects. There were six possible visualization orderings and all six were used for at least two participants. We gave participants two weeks to complete the study with no explicit time limit for how long to spend familiarizing themselves with a visualization or completing the heuristic survey. Therefore we do not know how long each  person spent on each evaluation. Once they completed the study, we sent each participant a thank-you email that solicited their summative thoughts about the methodology and study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Participant Ratings</head><p>We aggregated the scores from each participant as described earlier in Section 5.3 to identify an overall value score for each visualization. Total scores computed from all participants are shown in <ref type="figure">Figure 5</ref>. The rows are sorted top-to-bottom by the average value each participant gave across all three visualizations, so the most favorable raters appear at the top and the most difficult raters appear at the bottom. Vis B received the highest average score of 5.30, Vis C received the lowest score at 3.96, and Vis A received an intermediate score of 4.67. These ratings aligned with the relative ranking of the visualizations that we received from the instructors of the course in which they were created, as well as our own assessment of their relative value. Scores from the individual raters were generally consistent with the group average, with 11 of the 15 participants scoring Vis B highest and no participant scoring it lowest. Similarly, Vis C received the lowest score from 11 participants and never received the highest score. <ref type="figure" target="#fig_3">Figure 6</ref> drills down a level on the data to show participants' component ratings for each visualization. Here, we use a green-red color map to highlight regions of positive (green) and negative (red) views of the visualizations. The patterns of scores were somewhat more variable for the mid-level guidelines and the low-level heuristics. We discuss some observations at these lower levels and their relationship to specific characteristics of the visualizations in more detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1">Inter-Rater Reliability</head><p>We assessed inter-rater reliability using the rater vs. group approach. We calculated the mean rating for each visualization on each heuristic, and then calculated the correlation between each participant's scores and the mean scores. The mean rater-to-group correlation was significant for all three visualizations (for Vis A: r = 0.68, t(13) = 3.33, p &lt; 0.01; for Vis B: r = 0.75, t(13) = 4.06, p &lt; 0.01; for Vis C: r = 0.54, t(13) = 2.29, p &lt; 0.05), indicating that there was substantial agreement among the raters. This suggests that although raters each had their own backgrounds and individual differences, the overall ratings were consistent for the three visualizations.</p><p>We also calculated inter-rater reliability at the component level to assess whether the participants' scores were more consistent for some components than for others. For this analysis, the participants' scores were collapsed across all three visualizations to ensure that the number of ratings for each participant was sufficient to produce a meaningful correlation. The analysis revealed that the mean rater-to-group correlation was significant for three of the four components and marginally significant for the fourth. There was significant inter-rater reliability for the insight component (r = 0.56, t(13) = 2.46, p &lt; 0.05), the time component (r = 0.58, t(13) = 2.55, p &lt; 0.05), and the confidence component (r = 0.55, t(13) = 2.40, p &lt; 0.05). However, the rater-to-group correlation did not quite reach significance for the essence component (r = 0.49, t(13) = 2.03, p = 0.06).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2">Power Analysis</head><p>Given the size of the correlations observed in this evaluation, we conducted a power analysis to calculate the number of raters that would be required to achieve consistent results using this methodology. Using the average rater-to-group correlation for the overall scores across all three of the visualizations (r = 0.66), and the conventional values for Type I and Type II errors (α = 0.05 and β = 0.20, respectively), we estimate that five raters would be sufficient. <ref type="figure">Figure 7</ref> shows the scores for each visualization on every heuristic. Vis B received the highest average score on all but two of the lowlevel heuristics. For the insight heuristic "The visualization shows multiple perspectives about the data," Vis A had the highest average score at 5.4 while Vis B and Vis C were tied at a slightly lower score of 5.2. In this particular evaluation, all three visualizations showed multiple perspectives, so this heuristic does not do much to distinguish between them. On the confidence heuristic "If there were data issues like unexpected, duplicate, missing, or invalid data, the visualization would highlight those issues," Vis A received the highest average score at 4.07, with Vis B and Vis C receiving lower scores of 3.33 and 3.29, respectively. Some of the participants (P7 and P13) who provided comments for this heuristic noted that missing data was evident due to the zero values in the parallel coordinates plot in Vis A. In Vis B, zero values do not appear in the scatterplot, making it less obvious that there is missing data. In Vis C, zero values are shown in the scatterplot, but as one participant noted, a user would have to go through all of the dimensions, one by one, to understand which data is missing. Another illustrative case is the time heuristic "The interface supports reorganizing the visualization by the data's attribute values." This heuristic has the biggest differences in average scores across the three visualizations, with Vis B receiving the best average score at 6.07, Vis C receiving a score of 4.93, and Vis A receiving a very poor score of 2.73. In this case, Vis A suffers due to the lack of flexibility in the parallel coordinates plot. The features of Vis B, including filtering, search, and the highly-flexible scatterplot, lead to a very high score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Relationships Between Scores and the Characteristics of the Visualizations</head><p>At the guideline level, Vis B had the highest average score for all of the guidelines except for the confidence guideline: "The visualization helps understand data quality." This guideline has only one heuristic underneath it, and Vis B scores relatively poorly on this heuristic because it does not make missing data readily apparent, as discussed above. This reveals a potential weakness in our method of using a simple average to aggregate the scores at each level of the hierarchy. Since some of the mid-level guidelines have more low-level heuristics than others, some of the heuristics get weighted more heavily in the aggregation process.</p><p>Vis C had the lowest average scores on all of the guidelines except for two. It outperformed Vis A on the time guideline "The visualization provides mechanisms for quickly seeking specific information" and on the essence guideline "The visualization provides an understanding of the data beyond individual data cases." For the heuristics under both of these guidelines, participants remarked that the parallel coordinates plot in Vis A was too limited. The scatterplot in Vis C provided more support for these goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Confidence in Ratings</head><p>In addition to collecting a rating for each heuristic, we also gathered a confidence level for each, ranging from 1-very low confidence to 4-very high confidence. In general, the participants reported that they were confident in their responses, with a mean confidence rating of 3.22 and a standard deviation of 0.70. None of the heuristics had an average confidence rating lower than 3.</p><p>One or more participants gave a confidence rating of 1 to a total of five heuristics, three related to insight and two related to time. For the heuristics related to insight, one participant (P13) had low confidence in the heuristic "The visualization promotes exploration of relationships among different aggregation levels of the data" and commented that it was unclear what "aggregation" meant for this data set. Another participant (P3) had low confidence in their ratings for both of the heuristics that fell under the guideline "The visualization provides a new or better understanding of the data." P3 commented "If I were a school administrator I suspect that this would generate more questions."</p><p>For the heuristics related to time, two different participants (P2 and P13) had low confidence in their ratings for the heuristic "The visualization supports smooth transitions between different levels of detail in viewing the data." P2 commented that there was not enough information to rate this heuristic, and P13 commented that they were unsure of what levels of detail the question referred to. Another participant (P11) had low confidence in their ability to rate the heuristic "The visualization avoids complex syntactic querying by providing direct interaction" and commented that they did not understand what this heuristic meant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Qualitative Feedback</head><p>We subsequently invited study participants to share their feedback and comments about the evaluation methodology. Although the results of the evaluation indicated that the participants were fairly consistent in their responses, many of those who offered feedback were skeptical about this approach. The feedback from the participants fell into two general categories: comments about specific heuristics, and comments about the evaluation process itself.</p><p>The participants' comments about specific heuristics indicate that the wording of the heuristics was confusing in some cases. For example, two participants (P10 and P14) were unsure of what was meant by the phrase "data cases." We simply used this term to refer to a single item or instance in the data set; in our study, this would be a university. Others felt that specific heuristics were too broad, too subjective, or too multi-faceted, making them difficult to evaluate.</p><p>The comments about the evaluation process itself revealed three general themes. First, two participants (P8 and P13) felt that the evaluation process would have been more effective if they were given a persona or a task to complete using each visualization. A frequent comment was that the ratings for each visualization might differ for different kinds of tasks. Second, three participants (P1, P10, and P11) noted that it was difficult to rate some of the heuristics when the visualizations provided multiple views of the data. One view might score well on the heuristic while another might score poorly, and the participants were unsure of how to coalesce those differences into a single score. Finally, the most   <ref type="figure">Fig. 7</ref>: This figure depicts the entire value evaluation hierarchy and framework, including the four components, the guidelines under each component, and the constituent heuristics for each guideline, both as used in our study (crossed-out text) and as updated afterwards. The figure also shows summary (average) ratings for the three visualizations on each of the heuristics, as well as the standard deviation of each rating. We employ a red-green color map to help communicate at a glance lower/poorer ratings (red) to higher/better ratings (green). Both versions of the hierarchy, in addition to other study materials, can be found at visvalue.org.</p><p>common comment, offered by four different participants (P1, P2, P8, and P9), was that this type of evaluation might be more effective if a small set of visualizations were rated relative to one another, rather than applying the heuristics to one visualization in isolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">REFINING THE METHODOLOGY AND HEURISTICS</head><p>We used the results of the study and feedback from the evaluators to revise the methodology and heuristics. As two of the participants noted, the efficacy of a visualization is highly dependent on its context of use. Thus, we recommend that the intended users and task be communicated to evaluators prior to an evaluation. Further, participants sometimes found it difficult to decide how to rate a heuristic for a visualization with multiple views. We suggest that they be rated according to the best view for the task, as discussed further in Section 10. Some participants indicated that they were unsure of the meaning of specific terms, such as "data cases," that were used in the heuristics. To address this concern, we added a terminology table to the beginning of the heuristic questionnaire to clarify common language.</p><p>In addition to clarifying common terminology, we rephrased five of the individual heuristics based on participant feedback. Specifically, participants were confused by the usage of "aggregation levels" in the insight heuristic "The visualization promotes exploration of relationships among different aggregation levels of the data." We rephrased the heuristic to read "The visualization promotes exploring relationships between individual data cases as well as different groupings of data cases." Participants were also confused by the use of the term "syntactic querying" in the time heuristic "The visualization avoids complex syntactic querying by providing direct interaction." We rephrased this heuristic to read "The visualization avoids complex commands and textual queries by providing direct interaction with the data representation." Evaluators also commented that there were too many concepts to evaluate in the essence heuristic "The visualization provides an effective, comprehensive and accessible overview of the data." To simplify this heuristic, we removed the word "effective."</p><p>We rephrased two of the heuristics because the evaluators' ratings had a high standard deviation, indicating disagreement among the evaluators that seemed to be caused by differing interpretations of the heuristics. The first was the time heuristic "The visualization provides key characteristics of the data at a glance." We believe this is because the use of the word "provides" was unclear, so it was replaced with "shows." The second was the time heuristic "The interface supports reorganizing the visualization by the data's attribute values" We rephrased this heuristic to read "The interface supports using different attributes of the data to reorganize the visualization's appearance."</p><p>Other heuristics had ratings with relatively high standard deviations because they did not apply to particular visualizations. Evaluators had the option of choosing "not applicable" (N/A) for any given heuristic, but they were inconsistent in their use of that option. When a heuristic did not apply, some evaluators instead gave it a low or neutral score. Rather than changing the wording of the heuristics in such cases, we suggest explicitly instructing evaluators to use the N/A option whenever they question the applicability of a heuristic to a particular visualization.</p><p>Both the initial heuristics used in our assessment and their revisions are shown in <ref type="figure">Figure 7</ref> and included in the supplemental materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">APPLYING THE METHODOLOGY</head><p>Moving forward, we believe that the ICE-T evaluation methodology shows promise for future visualization evaluations. In this section, we provide guidelines for applying and implementing the methodology.</p><p>Recruiting Evaluators. Rating the heuristics requires thought about the holistic design and implementation of a visualization, how it applies principles of perception, appropriate use of visualization techniques, and so on. As a result, the methodology is best applied by individuals who have experience in and knowledge about developing visualizations. However, it is difficult to specify a precise ideal level or duration of experience. It may be desirable in some cases to have evaluators who have designed and developed multiple systems over many years. Alternatively, for some scenarios, students who have completed a course on data visualization may suffice. Furthermore, depending on the evaluation goals, other criteria may be important for identifying evaluators. For example, visualizations of data in a specific domain may require that the evaluators also have knowledge about that domain.</p><p>Administering the Survey. We have deployed guidance and materials, including both electronic and printable versions of the heuristics survey, at visvalue.org. The evaluators should first familiarize themselves with the visualization tool being assessed and the data it depicts. We recommend accompanying the visualization with a short overview or tutorial as we did in our study. Furthermore, a description of the potential users and the context of use is also recommended. The evaluators should complete the heuristic form, being permitted to refer back to the visualization throughout.</p><p>Determining and Reporting Scores. Once evaluators have completed their ratings of the visualizations, the scores can be compiled into a succinct report summarizing the value of the visualization from the point of view of this methodology. It may be useful to numerically and visually report the averaged scores for each heuristic (and potentially the variance of those scores as well). A color-coded table, similar to those shown in <ref type="figure">Figures 5-7</ref>, could be used to visually indicate strengths of a visualization and areas for improvement. This can be used by the developers of the visualization to refine the design and functionality of the visualization and increase its overall value.</p><p>Interpreting the Scores. Within the 7-point Likert scale ratings of heuristics, a score of 4 indicates a neutral rating. The statements are phrased positively, so higher scores are considered "better." While obviously there is no set quality level or scale, from our initial assessment of the methodology, we find that a visualization with an average score of 5 or greater for a particular heuristic across all evaluators represents a strength of the visualization, while a score of 4 or lower represents a heuristic for which the visualization has a weakness. Based on our initial assessment of the methodology, we find that valuable, good visualizations should be earning an overall cumulative average score of 5 or higher. Visualizations earning an overall cumulative score of 4 or less are candidates for redesign and further thought. The establishment of more specific score guidelines is possible with additional usage and testing of the methodology.</p><p>Unlike some other evaluation methodologies, the ICE-T approach does not produce an actionable list of design problems and suggested modifications. However, the scores from a visualization's evaluation could be used to create actionable suggestions for areas of improvement in a visualization. For example, a visualization that has a low score on the insight heuristic "The visualization facilitates perceiving relationships in the data like patterns &amp; distributions of the variables" could be improved by adding a representation of the data that can show potential correlations or clusters.</p><p>Potential Applications. In our assessment of the methodology, the heuristics were used by experts to rate three visualizations. However, we believe the methodology usage is not limited to comparative scenarios. Since the methodology results in quantitative measures of a visualization, it can be used to evaluate a single visualization in isolation. As discussed above, developers may seek to achieve a particular score level, and evaluators could establish score zones corresponding to outstanding, satisfactory, or poor performance. Potential uses of the ICE-T methodology include early evaluations of the efficacy of a research or commercial system in order to find relative strengths and weaknesses, much like that proposed for MILC evaluations <ref type="bibr" target="#b26">[27]</ref>, academic project evaluation and grading, decisions among alternatives for commercial or application-driven contexts, or similar scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">DISCUSSION</head><p>Reflections on the Assessment. Our analysis of the study results indicated that the evaluation methodology shows promise for identifying the value of a visualization. While participants voiced skepticism about some aspects of the methodology, their ratings were highly consistent nonetheless. Furthermore, the average scores for the visualizations corresponded to the relative rankings provided by the course instructors and our own prior assessment of their relative quality. At the lower levels of the hierarchy there were occasional discrepancies between the average scores of the visualizations and the order of the overall scores, but these patterns could always be traced back to specific design features in the visualizations, such as the affordances of a scatterplot as opposed to a parallel coordinates plot. Another important outcome was the power analysis indication that a consistent result could be achieved with as few as 5 raters, suggesting the potential for our approach to be an effective, relatively "low-cost" evaluation methodology.</p><p>Aggregating Scores. In our assessment of the heuristic-based evaluation methodology, we aggregated scores using a top-down approach. That is, the visualization's score is comprised of a simple average of its score for each high-level component. The component scores are a simple average of the associated mid-level guideline scores, and the mid-level guideline scores are a simple average of the ratings for the low-level heuristics. The implication of this choice is that some lowlevel heuristics will ultimately carry more weight in the final rating of a visualization. For example, two of the guidelines under confidence each only have a single low-level heuristic, while two of the guidelines under insight each have three low-level heuristics. The guidelines with fewer low-level heuristics (and the components with fewer guidelines) will ultimately have a greater impact on a visualization's rating.</p><p>This approach could be modified according to individual evaluation goals, however. One alternative could include a bottom-up scoring approach, where each low-level heuristic is given equal weight. The tradeoff then would be that guidelines and components with more heuristics beneath them in the hierarchy would have a greater impact on a visualization's score. Fully custom heuristic weightings could also be employed, defined by the visualization developer or the evaluators. By applying higher or lower weights to specific heuristics, different capabilities could be emphasized toward the particular evaluation goals for a visualization. Furthermore, the evaluators themselves could be given control to increase or decrease importance of different components.</p><p>Multiple Views. One source of confusion that became clear in the assessment of the methodology was how to rate a single heuristic when a visualization contains multiple views, where one view might do something well while the other one does it poorly. For example, P1 commented "it was challenging to choose an answer because of the use of multiple views in the visualization (...) I found myself taking a mean of the answers for the multiple views to answer the questions." P10 said "I wished I could specify different answers for different parts of the visualization. Because in the same visualization there were several views that would perform quite differently on these scales." Evaluators may rate the heuristic according to the best-case (the best view for that heuristic determines the rating), the worst-case (the worst view for that heuristic determines the rating), or the average-case (some overall impression given multiple views determines the rating).</p><p>This issue could lead to inconsistent ratings among evaluators. For example, when rating Vis A under essence "the visualization helps understand how variables relate in order to accomplish difference analytic tasks," P13 noted that it was true for one view (parallel coordinates) and gave a rating of 6. On the other hand, P8 commented that the ability to understand relationships in the data using the vis as a whole was too limited and hence gave a rating of 3. This disparity can be mitigated by prescribing either best-case, worst-case, or average-case ratings to be used by evaluators. The purpose of having additional views is often to capture an aspect of the data or provide an analytic capability not well-supported by other views. Hence, we suggest that the intuitive choice is to prescribe that evaluators utilize best-case ratings. That is, if any one view of a visualization satisfies a heuristic well, then the entire visualization itself should be considered to do it as well.</p><p>Validating the Methodology. The visualization ratings that our study evaluators produced aligned with those that we received from the instructors of the class in which the visualizations were created. While this gives us confidence that the ratings from the study were appropriate, it is not a formal validation of the study results. Ideally, one should more rigorously confirm that the evaluation methodology produces accurate ratings of visualizations, a so-called "ground truth" <ref type="bibr" target="#b12">[13]</ref>.</p><p>It may be tempting to use other established visualization evaluation techniques (i.e., time &amp; error-focused benchmark tasks, long-term deployment studies, etc.) to perform such a validation. However, we suggest that those techniques capture somewhat different aspects of a visualization's quality and utility than what our approach is intended to capture. We would expect results from the different methods to broadly align, but they might produce slightly different findings due to the different goals of each method.</p><p>In future work, we would like to better understand the effectiveness of the methodology compared to alternative evaluation approaches. For example, would the results of an insight-based evaluation <ref type="bibr" target="#b23">[24]</ref> correlate to a rating produced by our I(nsight) component? By directly comparing evaluation results using our methodology to other approaches, one could gain a better understanding of the tradeoffs and appropriateness of the value-driven evaluation methodology.</p><p>Limitations. While our assessment shows promise for the ICE-T methodology, it is not without its limitations. Our assessment only addressed three specific visualizations. To better understand the generalizability of our methodology, we must examine its use on a wider range of visualization types with varying data domains, representations, and intended task support. Furthermore, we employed only visualization experts in our study. We do not know whether other evaluators with less visualization expertise would achieve similar results. Finally, the heuristics themselves require subjective interpretation by the evaluators, which may be unsettling to those people seeking more objective, precise assessments. However, we believe that the subjectivity is inherent to evaluating the overall value of a visualization and is hence a part of this methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">CONCLUSION</head><p>Numerous past papers have noted that evaluating visualizations is difficult. The process of developing a survey to quantify the value of visualizations confirmed this trope, but it also helped to pinpoint some of the reasons why evaluation is so difficult. It is hard to define the value of a visualization in terms that multiple raters can understand and apply with consistency. To be effective, the heuristics that raters will use must be easy to evaluate, but they must also be meaningful and able to differentiate between different design choices in visualizations.</p><p>Throughout the process of developing the ICE-T methodology, we created, eliminated, and refined numerous heuristics. The evaluation study showed that the resulting set of heuristics does a good job of distinguishing between three visualizations, ranking their potential value, and identifying particular points of strength or weakness. Although the expert raters were somewhat skeptical about the methodology, the results revealed that they were highly consistent with one another. The pattern of scores conformed to our own qualitative assessment of the value of the three visualizations. Furthermore, the effect size achieved by this evaluation indicates that a consistent score could be achieved with only 5 raters, which would make this kind of evaluation feasible for real-world use.</p><p>In summary, we have described the development of a new methodology for evaluating interactive visualizations. Our initial assessment shows promise for the methodology as a low-cost, but effective evaluation approach. The methodology is intended to identify a visualization's holistic value, and thus presents a complementary approach to existing evaluation techniques such time &amp; error, insight-based, or deployment studies. The full value-driven methodology, including the heuristic survey and guidelines for use, is available online at visvalue.org.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>View of materials from the affinity diagramming exercise to create the initial version of the three-level value framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>The five stage process used for developing the value heuristics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>The visualizations of U.S. university data used in the assessment of the value-driven methodology for evaluating visualizations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>Participants' ratings of the different visualizations, broken down by the four value components. The color mapping is red (1) to green (7) with white (4) being neutral. This table shows the overall strength of each visualization with respect to each of the four components. Scanning the values in a column shows how all the different raters scored a visualization with respect to a specific component.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>terminology data case-refers to an instance of the data set; synonymous with data item or data point attribute-refers to properties of the data cases in the data set; synonymous with feature, dimension, or variable data-refers to attributes among the data, such as correlations, clusters, or distributions answering questions about the dataThe visualization exposes individual data cases and their attributes</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://vis.gatech.edu/demo/value/vis133/ 2 http://vis.gatech.edu/demo/value/vis460/ 3 http://vis.gatech.edu/demo/value/vis745/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Knowledge precepts for design and evaluation of information visualizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="432" to="442" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Systematic inspection of information visualization systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ardito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Buono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Costabile</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lanzilotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 AVI workshop on BEyond time and errors: novel evaluation methods for information visualization, BELIV &apos;06</title>
		<meeting>the 2006 AVI workshop on BEyond time and errors: novel evaluation methods for information visualization, BELIV &apos;06</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Evaluating information visualizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carpendale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Visualization</title>
		<editor>A. Kerren, J. T. Stasko, J.-D. Fekete, and C. North</editor>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="19" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Defining insight for visual analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ziemkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ribarsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="14" to="17" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Empirical studies of information visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Studies</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="851" to="866" />
			<date type="published" when="2000-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The vast challenge: History, scope, and outcomes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Grinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Whiting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Visualization</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="301" to="312" />
			<date type="published" when="2014-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Advancing usercentered evaluation of visual analytic environments through contests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Costello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Grinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Plaisant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Scholtz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">formation Visualization</title>
		<imprint>
			<date type="published" when="2009-06" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="230" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Beyond guidelines: What can we learn from the visual information seeking mantra?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Craft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cairns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Information Visualisation, IV &apos;05</title>
		<meeting>the Ninth International Conference on Information Visualisation, IV &apos;05</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="110" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Evaluation in information visualization: Heuristic evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Information Visualisation (IV)</title>
		<meeting>the 16th International Conference on Information Visualisation (IV)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="136" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An heuristic set for evaluation in information visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Forsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johansson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Advanced Visual Interfaces, AVI &apos;10</title>
		<meeting>the International Conference on Advanced Visual Interfaces, AVI &apos;10</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="199" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">User-centered evaluation of information visualization techniques: Making the hci-infovis connection explicit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Pimenta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Scapin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of human centric visualization</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="315" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Evaluating information visualization via the interplay of heuristic evaluation and question-based scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Laskowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, CHI &apos;16</title>
		<meeting>the 2016 CHI Conference on Human Factors in Computing Systems, CHI &apos;16</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5028" to="5033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Establishing usability heuristics for heuristics evaluation in a specific domain: Is there a consensus?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hermawati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lawson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied ergonomics</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="34" to="51" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An empirical comparison of three commercial information visualization systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kobsa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Symposium on Information Visualization</title>
		<meeting>the IEEE Symposium on Information Visualization</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page">123</biblScope>
		</imprint>
	</monogr>
	<note>InfoVis &apos;01</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Empirical studies in information visualization: Seven scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bertini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Plaisant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carpendale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1520" to="1536" />
			<date type="published" when="2012-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Finding usability problems through heuristic evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI &apos;92</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems, CHI &apos;92</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="373" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Heuristic evaluation of user interfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Molich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI &apos;90</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems, CHI &apos;90</meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Toward measuring visualization insight</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>North</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="6" to="9" />
			<date type="published" when="2006-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Integrating statistics and visualization: Case studies of gaining clarity during exploratory data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI &apos;08</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems, CHI &apos;08</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="265" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The challenge of information visualization evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Plaisant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Working Conference on Advanced Visual Interfaces, AVI &apos;04</title>
		<meeting>the Working Conference on Advanced Visual Interfaces, AVI &apos;04</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="109" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Promoting insight-based evaluation of visualizations: From contest to benchmark repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Plaisant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-D</forename><surname>Fekete</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Grinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="120" to="134" />
			<date type="published" when="2008-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adapting heuristic evaluation to information visualization -a method for defining a heuristic set by heuristic grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rossi De Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guimaraes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications, VISGRAPP &apos;17</title>
		<meeting>the 12th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications, VISGRAPP &apos;17</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="225" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Beyond usability and performance: A review of user experience-focused evaluations in visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Saket</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Endert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Workshop on Beyond Time and Errors on Novel Evaluation Methods for Visualization, BELIV &apos;16</title>
		<meeting>the Sixth Workshop on Beyond Time and Errors on Novel Evaluation Methods for Visualization, BELIV &apos;16</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An insight-based methodology for evaluating bioinformatics visualizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Saraiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>North</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="443" to="456" />
			<date type="published" when="2005-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Developing guidelines for assessing visual analytics environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Scholtz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Visualization</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="212" to="231" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Evaluation of visual analytics environments: The road to the visual analytics science and technology challenge evaluation methodology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Scholtz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Plaisant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Whiting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Grinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Visualization</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="326" to="335" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Strategies for evaluating information visualization tools: Multi-dimensional in-depth long-term case studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Plaisant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 AVI Workshop on BEyond Time and Errors: Novel Evaluation Methods for Information Visualization, BELIV &apos;06</title>
		<meeting>the 2006 AVI Workshop on BEyond Time and Errors: Novel Evaluation Methods for Information Visualization, BELIV &apos;06</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Designing the User Interface: Strategies for Effective Human-Computer Interaction. Pearson</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Plaisant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Diakopoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>6th ed.</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An exploratory study on the predictive capacity of heuristic evaluation in visualization applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sousa Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Quintino</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Human-Computer Interaction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="369" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Value-driven evaluation of visualizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Workshop on Beyond Time and Errors: Novel Evaluation Methods for Visualization, BELIV &apos;14</title>
		<meeting>the Fifth Workshop on Beyond Time and Errors: Novel Evaluation Methods for Visualization, BELIV &apos;14</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="46" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An evaluation of space-filling information visualizations for depicting hierarchical structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Catrambone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guzdial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Studies</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="663" to="694" />
			<date type="published" when="2000-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Toward visualization-specific heuristic evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fruhling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Borgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Forsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Grinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Scholtz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Workshop on Beyond Time and Errors: Novel Evaluation Methods for Visualization</title>
		<meeting>the Fifth Workshop on Beyond Time and Errors: Novel Evaluation Methods for Visualization</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="110" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Evaluating visualizations: Do expert reviews work?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Möller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="8" to="11" />
			<date type="published" when="2005-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Views on visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Wijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="421" to="432" />
			<date type="published" when="2006-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Heuristics for information visualization evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schlesier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carpendale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 AVI Workshop on BEyond Time and Errors: Novel Evaluation Methods for Information Visualization, BELIV &apos;06</title>
		<meeting>the 2006 AVI Workshop on BEyond Time and Errors: Novel Evaluation Methods for Information Visualization, BELIV &apos;06</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
