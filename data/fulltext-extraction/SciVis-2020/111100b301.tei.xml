<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VC-Net: Deep Volume-Composition Networks for Segmentation and Visualization of Highly Sparse and Noisy Image Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoli</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haikuan</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagar</forename><surname>Buch</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ewart</forename><forename type="middle">Mark</forename><surname>Haacke</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Hua</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichun</forename><surname>Zhong</surname></persName>
						</author>
						<title level="a" type="main">VC-Net: Deep Volume-Composition Networks for Segmentation and Visualization of Highly Sparse and Noisy Image Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep neural network</term>
					<term>3D cerebrovascular segmentation and visualization</term>
					<term>maximum intensity projection (MIP)</term>
					<term>joint embedding Direct 3D Segmentation Volume Rendered as a 2D MIP 2D MIP Segmentation Raw Image Slices Traditional Volume Rendering 3D Volume Patch 3D Region to Be Processed Joint 3D Exploration 3D Entire Volume</term>
				</keywords>
			</textClass>
			<abstract>
				<p>The fundamental motivation of the proposed work is to present a new visualization-guided computing paradigm to combine direct 3D volume processing and volume rendered clues for effective 3D exploration. For example, extracting and visualizing microstructures in-vivo have been a long-standing challenging problem. However, due to the high sparseness and noisiness in cerebrovasculature data as well as highly complex geometry and topology variations of micro vessels, it is still extremely challenging to extract the complete 3D vessel structure and visualize it in 3D with high fidelity. In this paper, we present an end-to-end deep learning method, VC-Net, for robust extraction of 3D microvascular structure through embedding the image composition, generated by maximum intensity projection (MIP), into the 3D volumetric image learning process to enhance the overall performance. The core novelty is to automatically leverage the volume visualization technique (e.g., MIP-a volume rendering scheme for 3D volume images) to enhance the 3D data exploration at the deep learning level. The MIP embedding features can enhance the local vessel signal (through canceling out the noise) and adapt to the geometric variability and scalability of vessels, which is of great importance in microvascular tracking. A multi-stream convolutional neural network (CNN) framework is proposed to effectively learn the 3D volume and 2D MIP feature vectors, respectively, and then explore their inter-dependencies in a joint volume-composition embedding space by unprojecting the 2D feature vectors into the 3D volume embedding space. It is noted that the proposed framework can better capture the small / micro vessels and improve the vessel connectivity. To our knowledge, this is the first time that a deep learning framework is proposed to construct a joint convolutional embedding space, where the computed vessel probabilities from volume rendering based 2D projection and 3D volume can be explored and integrated synergistically. Experimental results are evaluated and compared with the traditional 3D vessel segmentation methods and the state-of-the-art in deep learning, by using extensive public and real patient (micro-)cerebrovascular image datasets. The application of this accurate segmentation and visualization of sparse and complicated 3D microvascular structure facilitated by our method demonstrates the potential in a powerful MR arteriogram and venogram diagnosis of vascular disease.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Nowadays, there is a pressing need for better visualizing and understanding microstructures in the raw and wild datasets. For instance, the acquisition of the in-vivo micro-level 3D vasculature from image data is a grand challenge. The notorious difficulties of microvascular data analytics lie in high sparseness of vessel data in a large-sized 3D volume, e.g., the scattered vessel fragments in the angiographic data against the otherwise encompassing white and grey matter (as well as background and noise); high noisiness, such as low signal-to-noise ratio (SNR), e.g., about 10:1 in cerebrovascular images; tininess of micro-level vessels, e.g., the diameter of the micro-level vessels in images is merely 1∼2 voxels (e.g., 50∼100 microns); and sophisticated vessel geometry and topology variations, e.g., local "crossing", "kissing", or "tortuous" vessel structures, etc. Currently, for such complex 3D micro-level data, it would be impossible from a timing perspective for clinicians to review all this data manually and label abnormalities slice by slice. The 3D structural / contextual information and quantitative metrics are still missing, although maximum intensity projection (MIP) <ref type="bibr" target="#b30">[31]</ref>, a widely-used approach for qualitatively visualizing and analyzing the 3D vasculature, has been employed to enhance the local vessel signal, allowing for geometric variability and scalability. The labor-intensive, time-consuming, and 3D global / contextual information-missing nature of the procedure makes it very challenging to fully take advantage of the large number of 3D datasets (images and shapes) available for reference and comparison, and reach more informed and accurate decisions. In recent decades, the automatic model-driven vessel extraction and segmentation approaches have been proposed, such as multiscale filtering <ref type="bibr" target="#b11">[12]</ref>, region growing techniques <ref type="bibr" target="#b26">[27]</ref>, active contours <ref type="bibr" target="#b29">[30]</ref>, geometric flow <ref type="bibr" target="#b7">[8]</ref>, level-set approach <ref type="bibr" target="#b10">[11]</ref>, nonlinear subtraction (NLS) method <ref type="bibr" target="#b46">[47]</ref>, template-based predictor-corrector algorithm <ref type="bibr" target="#b13">[14]</ref>, etc. However, these approaches are easily overwhelmed by tons of low-level handcrafted features and complicated manual parameter adjustment to overcome aforementioned difficulties and subject variations.</p><p>Recently, data-driven approaches have been proposed to robustly investigate the correlations between different objects / instances without relying on hard-coded metrics. In medical image visualization and processing, several deep learning based methods have been proposed to extract vessels from 2D retinal images, such as DeepVessel <ref type="bibr" target="#b12">[13]</ref>, multilevel deep supervised networks <ref type="bibr" target="#b28">[29]</ref>, deep neural network (DNN)-based method <ref type="bibr" target="#b25">[26]</ref>, unified convolutional neural network (CNN) and graph neural network (GNN) <ref type="bibr" target="#b36">[37]</ref>, etc. These methods can perform 2D vessel segmentation tasks well, but are far from satisfactory on 3D vessel scenario. There are still very few dedicated deep learning architectures for 3D vessel segmentation, such as Uception <ref type="bibr" target="#b33">[34]</ref>, DeepVesselNet <ref type="bibr" target="#b39">[40]</ref>, and VesselNet <ref type="bibr" target="#b21">[22]</ref>, etc. Existing methods do not consider to use the visualization techniques in the 3D vessel extraction and are not specifically designed for solving the aforementioned challenges in 3D micro-cerebrovascular segmentation.</p><p>The fundamental motivation of the proposed work is to present a new visualization-guided computing paradigm to combine direct 3D volume processing and volume rendered clues for effective 3D exploration. In order to fill the gap in the high-fidelity 3D micro-cerebrovascular segmentation and visualization for the medical data in-vivo, we present a DNN method, VC-Net, for robustly extracting sparse microvascular structures through embedding the 2D image slice composition by MIP into the 3D volumetric image learning process to enhance the overall performance on 3D vasculature segmentation. The core novelty is to automatically leverage the volume visualization technique (e.g., MIP -a volume rendering technique for 3D volume images) to enhance the qualitative 3D data exploration, especially for 3D in-vivo segmentation and visualization, at the deep learning level. It is noted that the proposed framework can better capture the micro vessels and improve the vessel connectivity. The key motivation of our network is to integrate the trustworthy auxiliary from learned 2D MIP features into the 3D volume segmentation and visualization network, instead of using more complicated networks empirically. Experimental results are evaluated and compared with the traditional 3D vessel segmentation methods and the state-of-the-art in deep learning, using extensive public and real patient (micro-)cerebrovascular image datasets. The key contributions of our work are as follows:</p><p>• It proposes an effective end-to-end deep learning method to segment and visualize high-fidelity 3D sparse microvascular structure with complicated geometry and topology variations from volumetric images with significant noise. • A multi-stream CNN framework is designed to effectively learn the feature vectors of 3D raw volume and multislice composited 2D MIP (volume rendering), respectively, and explore interdependencies between 3D and 2D embedded features in a joint volume-composition embedding space by unprojecting (inverse volume rendering) the 2D features, learned from MIP, into the 3D volume embedding space. • To our knowledge, this is the first time that a deep learning framework is proposed to construct such a joint convolutional embedding space, where the computed joint vessel probabilities from 2D projection and 3D volume can be integrated synergistically. • The application and experiments on the accurate in-vivo segmentation and visualization of sparse and complicated 3D microvascular structure facilitated by our method demonstrate the potential in a novel and powerful MR arteriogram and venogram (MRAV) diagnosis of vascular disease.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we review most related work on 2D / 3D vessel extraction and segmentation in visualization and medical imaging domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model-Driven Vessel Extraction and Segmentation</head><p>Traditionally, doctors have to manually segment each image slice to obtain accurate vessel structures, which is extremely tedious and timeconsuming. Therefore, it is important to develop automatic vessel segmentation methods. For instance, Wilson and Noble <ref type="bibr" target="#b44">[45]</ref> introduced a mixture distribution for the data, motivated by a physical model of blood flow, that is used in a two-stage segmentation algorithm with a statistical classifier and structural criteria. Chung and Noble <ref type="bibr" target="#b5">[6]</ref> presented an extended version of the previous 3D cerebral vessel segmentation algorithm <ref type="bibr" target="#b44">[45]</ref>, and introduced a Rician distribution for background noise modeling and used a modified expectation-maximization (EM) algorithm for the parameter estimation procedure. Frangi et al. <ref type="bibr" target="#b11">[12]</ref> developed a vessel enhancement filter by computing the multiscale second order local structure of an image (i.e., Hessian). A vesselness measure is obtained on the basis of all eigenvalues of the Hessian. Based on multiscale filtering method <ref type="bibr" target="#b11">[12]</ref>, Descoteaux et al. <ref type="bibr" target="#b7">[8]</ref> developed a novel geometric flow for segmenting vasculature in proton-density images, which can also be applied to the cases of magnetic resonance angiography (MRA) or MRI data. Martínez-Pérez et al. <ref type="bibr" target="#b26">[27]</ref> presented a retinal blood vessel segmentation method based on scale-space analysis of obtaining the vessel geometrical features by the first and the second derivative of the intensity in the image. Then they used a multiple pass region growing procedure which progressively segments the blood vessels. Nain et al. <ref type="bibr" target="#b29">[30]</ref> combined image statistics and shape information to derive a region-based active contour that segments tubular structures and penalizes leakages. Liao et al. <ref type="bibr" target="#b24">[25]</ref> introduced a fast marching approach with curvature regularization for vessel segmentation, since most vessels have a smooth path and curvature can be used to distinguish desired vessels. Florin et al. <ref type="bibr" target="#b9">[10]</ref> proposed a particle filter based propagation approach for the segmentation of vascular structures in 3D volumes. To obtain posterior probability estimation of the vessel location, Wang et al. <ref type="bibr" target="#b42">[43]</ref> employed sequential Monte Carlo tracking and proposed a vessel segmentation method by fusing multiple cues extracted from CT images for enhanced segments from global path minimization. Forkert et al. <ref type="bibr" target="#b10">[11]</ref> presented and evaluated a level-set segmentation approach with vesselness-dependent anisotropic energy weights, which focuses on the exact segmentation of malformed as well as small vessels from time-of-flight (TOF) MRA datasets. Ye et al. <ref type="bibr" target="#b46">[47]</ref> proposed non-linear subtraction (NLS) method <ref type="bibr" target="#b46">[47]</ref>, which is employed for selective MRA enhancement utilizing the flow rephrased and dephased images. Then the vessel label can be obtained based on an enhanced angiography map. Govyadinov et al. <ref type="bibr" target="#b13">[14]</ref> described a template-based predictor-corrector method for tracing filaments that is robust in microvascular datasets, and applied a number of glyph-based visualization techniques to represent the aggregated and biologically relevant information of the extracted microvascular network. Then, they developed a bi-modal visualization framework <ref type="bibr" target="#b14">[15]</ref>, leveraging graph-based and geometry-based techniques to achieve interactive visualization of microvascular networks. However, these approaches are exhausted by handcrafted features (e.g., gradients of the intensity, second order local structures, maximum principal curvatures) and complicated manual parameter adjustment to adapt to the subject variations. Therefore, their robustness and accuracy across subjects are limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data-Driven Vessel Extraction and Segmentation</head><p>Recently, there is an emerging trend to automatically extract, segment, and reconstruct shape objects of interest from input 2D / 3D images <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44]</ref> or 3D meshes / point clouds <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b45">46]</ref> by deep neural network (DNN) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18]</ref>. Particularly for vessel structures, several deep learning based methods have been proposed to extract vessels from 2D retinal images. DeepVessel <ref type="bibr" target="#b12">[13]</ref> addresses retinal vessel segmentation as a boundary detection task that is solved using a CNN with a side-output layer to learn discriminative representations, and a conditional random field (CRF) layer that accounts for non-local pixel correlations. Li et al. <ref type="bibr" target="#b23">[24]</ref> presented a supervised method for vessel segmentation by using the cross-modality data transformation from retinal image to vessel map. Mo and Zhang <ref type="bibr" target="#b28">[29]</ref> developed a deep supervised fully convolutional network by leveraging multi-level hierarchical features of the deep networks for retinal vessel segmentation. Liskowski and Krawiec <ref type="bibr" target="#b25">[26]</ref> proposed a supervised segmentation technique that uses a DNN trained on a large number of samples preprocessed with global contrast normalization, zero-phase whitening, and augmented using geometric transformations and gamma corrections. Shin et al. <ref type="bibr" target="#b36">[37]</ref> incorporated a graph neural network (GNN) into a unified CNN architecture to jointly exploit both local appearances and global vessel structures. Their framework has been evaluated on retinal image datasets and a coronary artery X-ray angiography dataset. These methods can perform well on the 2D vessel segmentation task, but are far from satisfaction / feasibility on 3D micro vessel scenario, since their designs either do not consider the correlation / inter-information between slices in 3D volumetric images or cannot afford the computational and memory burdens in the large 3D volume at the micro-level. As for deep learning-based 3D vessel segmentation, for instance, Uception <ref type="bibr" target="#b33">[34]</ref> presents a network inspired by the 3D U-Net <ref type="bibr" target="#b6">[7]</ref> and the Inception modules <ref type="bibr" target="#b37">[38]</ref> for segmentation of the cerebrovascular network in MRA images. DeepVesselNet <ref type="bibr" target="#b39">[40]</ref> and VesselNet <ref type="bibr" target="#b21">[22]</ref> propose 2D orthogonal cross-hair filters in all sagittal, coronal, and axial planes on each voxel to make use of 3D context information at a reduced computational burden and memory cost. However, the challenging problems in 3D micro-cerebrovascular segmentation are complicated vessel geometry and topology, high sparseness and noise of vessel data in a large-sized 3D volume, and the limited resource of 3D microvascular datasets. The above methods do not overcome these challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">VC-NET</head><p>The fundamental inspiration of the proposed work is to mimic the observation of human exploration in 3D aided by volume rendering. Our work presents a new paradigm to combine direct 3D volume processing and volume rendered clues for effective 3D exploration. For instance in <ref type="figure">Fig. 1</ref>, for a micro-cerebrovascular dataset, the 3D volume image can <ref type="figure">Fig. 1</ref>: A novel 3D data analytic paradigm in a joint volumecomposition space, where volume rendered results are used to support the visualization-guided 3D volume processing by deep learning. more accurately represent the 3D spatial information, but the desired task is easy to be confused by challenging SNR and sparse vesselness, as shown in the raw image slices; while the volume rendered 2D MIP image can better enhance the local vessel signal by enforcing vessel continuity and adapt to the geometric variability and scalability of vessels. However, it always lacks 3D spatial sense, e.g., two "crossing" and "kissing" vessels as circled in red (in fact, the bigger one is above the smaller one in 3D space). So, it is deficient to investigate the (sparse and noisy) 3D data from either 3D volume or volume rendered 2D MIP, respectively. In this work, we design a novel paradigm to support the 3D data analytics, such as segmentation, etc., by using the visualizationguided computing. Instead of conducting the rendering / composition at the final stage as in the traditional visualization pipelines, this paradigm qualitatively investigates the 3D volume data from 2D composited (rendered) images. Essentially, this procedure makes the visualization more important via an early and simultaneous involvement of volume rendering (composition). Finally, we explore the 3D data analytics in a joint volume-composition space. In the following, we introduce the components of the VC-Net model: network architecture and loss function, and dataset generation and preparation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network Architecture</head><p>The proposed VC-Net mainly consists of a dual-stream component (i.e., a 3D volume segmentation stream and a 2D composited MIP segmentation stream) and the bi-directional operations between these two streams (i.e., 3D-to-2D projection and 2D-to-3D unprojection). The overall architecture is demonstrated in <ref type="figure">Fig. 2</ref>. The two-stream segmentation component can learn vessel feature vectors in 3D volume and corresponding multiple 2D MIPs (enhanced and dense depiction of 3D relationships via a 3D-to-2D projection computation) contexts, respectively. After that, the embedded features from the 2D composited MIP are transformed from the 2D MIP domain into the 3D volume domain through a 2D-to-3D unprojection process. Then, the extracted 2D and 3D embedded features from two streams are integrated together, constructing a unified high-dimensional joint convolutional embedding space, which can strengthen the original sparse vessel features from the 3D volume. Finally, the vessel segmentation prediction can be learned at the fusion stage in this joint convolutional embedding space.</p><p>In this work, we use a 3D U-Net <ref type="bibr" target="#b6">[7]</ref> as the 3D volume segmentation stream and a half 2D U-Net <ref type="bibr" target="#b32">[33]</ref> (in terms of feature channel numbers) as the 2D composited MIP segmentation stream, respectively. U-Netlike networks are the most commonly-used and robust medical imaging segmentation neural networks across different data modalities for varying organ / tissue geometries, and thus it is suitable for us to justify the benefits from our 2D-to-3D unprojection and joint embedding of 3D volume and 2D composited MIP. A U-Net-like network is essentially a convolutional encoder-decoder network, which first embeds the input into a high-dimensional feature vector through hierarchical convolution and pooling at the encoder stages, and then decodes the feature vector in the hidden space through hierarchical upsampling and convolution at the decoder stages with the integration of the features directed from different encoder stages through the long-skipped feature concatenations.</p><p>In <ref type="figure">Fig. 2</ref>, the layer output feature channel numbers are denoted in the corresponding blocks and layer input spatial dimensions are shown in the horizontal levels of every block.</p><p>Due to the limited data availability and large volume size in microcerebrovascular image datasets, we choose to train the network patchwisely. Specifically, from the observation that most brain MRAs have much higher resolutions in axial plane than other planes, we adaptively train our network using non-cubic patches, which have larger dimension size across axial plane, instead of resizing the data into a uniform voxel spacing through an interpolation before the network training, to avoid potential data corruption. As shown in <ref type="figure">Fig. 2</ref>, the key step in our network is the effective integration of the features from two different streams / domains. In order to fuse the 2D composited MIP stream into the 3D segmentation main stream within the network during the learning process, one may first find out that segmentation task is essentially a dense voxel (pixel)-wise classification problem, and the integration of embedded feature vectors from different learning domains and objectives (i.e., 3D volume segmentation and 2D composited MIP segmentation) must be fused voxel-wisely with the correct spatial correspondence in the volumetric domain. Accordingly, there are two main challenges that need to be overcome in this work. The first one is the effective format of the corresponding 2D composited MIPs from a randomly-extracted 3D volume patch that is adequate and suitable for delivering dense voxel correspondence in the simultaneous dual-stream learning design. The second one is the effective approach for mapping / unprojecting the feature vectors extracted from the composited MIP image plane pixels (in a dimension-reduced 2D) back to the corresponding 3D volume spacing voxels. More details are introduced in following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">3D-to-2D Projection in Dual-Stream Design</head><p>The major motivation for projecting the 3D volume space into the 2D MIP space is to enhance the local vessel probability. Given a randomlyextracted 3D volume patch V of the size K1 × K2 × K3 (e.g., we use 128 × 128 × 16 in our experiments) and K3 is along the vertical axis. We compute s-sliced (e.g., s = 5 in our experiments as suggested by domain experts) MIPs of V along vertical axis with overlapping coverage every t slice interval. Consequently we can get a set of m consecutive / sliding MIPs, i.e., P = {P1, P2, . . . , Pk, . . . , Pm−1, Pm}, in which Pk is the MIP across the [(k − 1)t + 1] th slice to the [(k − 1)t + s] th slice in V . It is noted that in a 2D MIP, only one voxel with the maximum intensity among the s voxels along the vertical axis in V will be recorded, which is prone to an information loss, considering the segmentation task actually needs the information of every voxel. Consequently, we set t = 2 as a trade-off between computation cost and information completeness / denseness. We can get m MIPs of size K1 × K2 for V , where the MIP number m is computed as:</p><formula xml:id="formula_0">m = 1 t (K3 − s) + 1.<label>(1)</label></formula><p>A MIP conveys denser vessel information and is also naturally suitable for 2D convolution. However, we now have m different MIPs and need to feed them to our network in the MIP stream in company with the 3D volume stream V as an input pair to our entire network. The information from the m MIPs is equally important, which means every pixel information should be kept during learning for later back projection. In order to avoid intuitively stacking them to a K1 × K2 × m volume such that the 2D CNN (in 2D MIP stream) would essentially treat it as a 2D input of a spatial dimension K1 × K2 with m different properties (feature channels), which is essentially deficient in terms of the spatial domain size as well as the operation motivation, we convert the m MIPs to a tiled MIP with a larger 2D spatial size, such as 0.5mK1 × 2K2. In this case, the 2D convolution is operated equally across the 2D composited MIP plane domain. The slice indices from where the MIP pixels are selected in the original V are also recorded so as to effectively restore the pixel-wise information extracted from MIP to the 3D volume space, which will be used in the 2D-to-3D unprojection in the following process. The format of the 2D composited MIP (e.g., m = 6 consecutive MIPs) computed from a 3D volume patch is shown in <ref type="figure" target="#fig_3">Fig. 3 (a)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv3D 3x3x3</head><p>Maxpooling3D 2x2x2</p><p>Upsampling3D 2x2x2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concatenation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv2D 3x3</head><p>Maxpooling2D 2x2</p><p>Upsampling2D 2x2</p><p>Conv3D  <ref type="figure">Fig. 2</ref>: The architecture of VC-Net. The major procedure includes obtaining the composited MIPs via 3D-to-2D projection, dual-stream segmentation learning for 3D volume and 2D composited MIP feature vectors, back projecting 2D composited MIP feature vectors into the 3D volume feature space via 2D-to-3D unprojection, building a joint convolutional embedding for learning the final vasculature mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">2D-to-3D Unprojection for Joint Embedding</head><p>Once the 3D volume and 2D MIP streams learn their segmentation features respectively, we intend to integrate them in a unified joint hidden feature embedding space to yield the final 3D segmentation prediction. In order to achieve this, we conduct several operations within our network to unproject (i.e., back project) the pixel features extracted from the composited MIP back to their corresponding 3D voxel feature space. The final-stage hidden feature from 2D composited MIP segmentation stream has the size 0.5mK1 × 2K2 with C1 channels (C1 = 32 as shown in <ref type="figure">Fig. 2)</ref>, which is the input of the back projection layers. We first disassemble it to restore m C1-channel features for the corresponding MIPs (e.g., P1, P2, . . . , Pm−1, Pm, where m = 6 as illustrated in <ref type="figure" target="#fig_3">Fig. 3 b)</ref>. Then we use the recorded index information to map the MIP pixel features back to where they are selected from V during the 2D composited MIP generation. <ref type="figure" target="#fig_3">Fig. 3 (b)</ref> shows how the feature vectors of two consecutive MIPs (e.g., P1 and P2) are disassembled from the composited MIP. They unproject their pixel feature space (Pm−j, i.e., the j-th slice among 5-sliced MIP Pm, 1 ≤ j ≤ 5) back to the voxel feature space (Sn, i.e., the n-th slice in the input 3D patch, 1 ≤ n ≤ 16). It is noted that the feature dimension is folded from 3D to 2D for a convenient illustration in <ref type="figure" target="#fig_3">Fig. 3</ref> (b) (i.e., hiding the feature dimension).</p><p>For the features of overlapping slices (from the consecutive MIPs), which are covered by multiple MIPs, we take the element-wise maximum value across the overlapping restoration through the feature channels:</p><formula xml:id="formula_1">FS n [i] = max(FP 1−1 [i], . . . , FP 6−5 [i]), 1 ≤ i ≤ 32,<label>(2)</label></formula><p>where FS n [i] represents the i-th channel in feature F at the n-th slice in the 3D patch. For example, the feature FS 9 is computed across the overlapping slices of P3−5, P4−3, P5−1 as highlighted in pink in <ref type="figure" target="#fig_3">Fig. 3 (c)</ref>. The whole process of the cross-MIP fusion in the feature channels of the 3D volume feature space is shown in <ref type="figure" target="#fig_3">Fig. 3</ref> (c) in detail. After that, the unprojected 2D MIP features and 3D volume features from two streams are integrated together, constructing a unified high-dimensional joint convolutional embedding for predicting the final vessel segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Loss Function</head><p>The major learning objective of our VC-Net is to extract the sparse 3D vasculature structure from the 3D MRI volume image using a 3D segmentation network supplemented by information from multiple denser and more connected 2D MIPs. Consequently the network loss function consists of two terms:</p><formula xml:id="formula_2">L = Lvox 3D−2D + λLmip,<label>(3)</label></formula><p>where Lvox 3D−2D is a joint 3D-2D segmentation Dice loss adopted in 3D volume stream and defined as:</p><formula xml:id="formula_3">Lvox 3D−2D = − 2Σx∈V p(x)g(x) + δ Σx∈V p(x) + Σx∈V g(x) + δ ,<label>(4)</label></formula><p>where p(x) and g(x) are the predicted voxel-wise vessel probability maps and ground truth binary labels within the query volume patch V , respectively. δ is a small smooth constant. Lmip is applied in 2D composited MIP stream and acts as a regularization term during the learning process, which is also a Dice loss function defined (similarly to Lvox 3D−2D ) within the 2D composited MIP plane and supervised by the ground truth MIP vessel binary labels. λ is the constant coefficient of Lmip, which is set to be 0.2 for our best experiment performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dataset Generation and Preparation</head><p>In this work, we use two different real patient datasets to evaluate our proposed VC-Net method.</p><p>Novel MICRO-MRI Imaging and Dataset. Some researchers have recently developed a next generation of microvascular imaging, i.e., Microvascular In-vivo Contrast Revealed Origins Magnetic Resonance Imaging (MICRO-MRI) <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b40">41]</ref>. Thanks to MICRO-MRI, we became the first ever to be able to acquire such brain imaging datasets and observe the complicated micro cerebral vessels. This dataset is produced by neurologists and radiologists within our collaborative group. Data was acquired with an adapted 3D gradient echo susceptibility weighted imaging (SWI) sequence <ref type="bibr" target="#b4">[5]</ref> collected from a 3T MR scanner. The post-contrast data were acquired during a gradual increase in dose (final concentration = 4 mg / kg). Eleven healthy volunteers were scanned in brain regions with a dual echo SWI sequence at four time points: the first was acquired pre-contrast and the remaining three were acquired post-contrast during a gradual increase in dose delivered over the time frame of 20 min; with the imaging parameters: echo time (TE)1 / TE2 / repetition time (TR) = 7.5 / 22. Major-level vessel data. This protocol enables multiple image sources for producing both MR arteriogram (MRAG) and venogram (MRVG). For the MRVG, the pre-contrast quantitative susceptibility mapping (QSM) and R2 * constitute two different representations of veins. In order to obtain the pre-contrast QSM data, the original phase data was unwrapped using the 3D best path method <ref type="bibr" target="#b1">[2]</ref>. The sophisticated harmonic artifact reduction for phase data (SHARP) method  was used to estimate the background field and remove it from the unwrapped phase <ref type="bibr" target="#b34">[35]</ref>. The truncated k-space inverse filter approach with an iterative geometric constraint (also known as iSWIM) was applied to the resultant phase to generate the QSM data <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b38">39]</ref>. The QSM data was further refined by removing the strong phase gradients from the long TE phase data based on a quality phase mask. The resultant phase was used to obtain a QSM data QSM TE2 . The QSM of the short TE data QSM TE1 was also generated, but without using a quality map, since at a low TE the phase gradients were not that strong. Finally, the missing information on QSM TE2 was filled in by applying an inverted quality mask to QSM TE1 . To obtain the pre-contrast R2 * , the short and long TE magnitude data S(t) were fitted to the monoexponential equation: S(t) = ρe −(tR 2 * ) , where ρ is the tissue intrinsic proton density.</p><p>Another MRVG was generated by subtracting the short TE magnitude data of pre-contrast from the short TE magnitude data of the first post-contrast. The above-mentioned subtraction provides a venousonly map VT 1 . The QSM, R2 * , and VT 1 maps were then normalized to values between 0 and 1, and an average of these different sources produced a high-quality MRVG referred to as MRVGavg.</p><p>An MRAG was then calculated using a nonlinear subtraction (NL-S) <ref type="bibr" target="#b46">[47]</ref>, i.e., MRAGnls, of the long TE S from the short TE S of the pre-contrast magnitude data as: MRAGnls = S 2 − αS 2 , where α is a constant with an empirically selected value of 1.5. Due to the T2 * effect, this subtraction also enhances the veins, but to a much smaller extent than the arteries. Nevertheless, any venous enhancement is discarded by using a mask generated from MRVGavg. Finally, the ultimate ground truth vessel labels are obtained by integrating the enhanced angiography (i.e., arteriogram and venogram) maps <ref type="bibr" target="#b3">[4]</ref> from the computed MRAGnls and MRVGavg, with a threshold-based method for the initial masks, followed by domain experts' post-manual labeling refinement using our developed cerebrovascular labeling and visualization tool. Supplemental Material and Video are included for demonstrating the interactive interface and basic functions in detail.</p><p>Micro-level vessel data. SWI images were generated by homodyne high-pass filtering (filter size = 96 × 96) the phase images to generate a phase mask, which was multiplied with the original magnitude images four times, for all time points <ref type="bibr" target="#b16">[17]</ref>. All the original magnitude and corrected phase data were then registered to the pre-contrast data. The short TE (7.5 ms) magnitude data of pre-contrast and the first postcontrast time points were averaged. This averaged magnitude data was subtracted by the long TE (22.5 ms) SWI data from the last postcontrast time point (4 mg / kg) to enhance the vessels. The vessels were further enhanced on the resultant subtracted image by applying the vesselness algorithm <ref type="bibr" target="#b11">[12]</ref> to obtain the micro-level vessel map. The micro-level vessels from this resultant vessel map were extracted using an adaptive threshold-based region growing method (ATRG) <ref type="bibr" target="#b19">[20]</ref>, i.e., SWIATRG, as the initial masks, followed by domain experts' manual inspection of the extracted vessels for quality control.</p><p>Public MRA Dataset. In order to compare with the existing methods, we use a public TubeTK Toolkit MRA dataset from University of North Carolina at Chapel Hill <ref type="bibr" target="#b0">[1]</ref>, acquired by a 3T MR system. There are 42 patient cases in the whole dataset, which have the manual-labeled vessel segmentation masks. The voxel spacing of the MRA images is 0.5 × 0.5 × 0.8 mm 3 with a volume size of 448 × 448 × 128 voxels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULTS</head><p>For both MRA TubeTK and MICRO-MRI datasets (the different modalities of input image examples are provided in Supplemental Material), we first apply the MR-based skull-stripping method <ref type="bibr" target="#b18">[19]</ref> to extract the pure brain from each image. As we mentioned in Sec. 3.1, our VC-Net network is designed for patch-wise training and the 3D training patches with the imbalanced dimensions are randomly-extracted with overlapping focusing on the brain area in the whole 3D MRA / MICRO-MRI, e.g., 80 patches for each TubeTK case and 440 patches for each MICRO-MRI major-level vessel case. The random training / validation / testing case split is 33 / 3 / 6 and 6 / 2 / 3 for the TubeTK dataset and the MICRO-MRI major-level vessel case, respectively. All the numerical evaluations are reported in terms of whole brain volume image patched with no overlapping.</p><p>Our VC-Net adopts the Adam optimizer with 0.0001 as an initial learning rate, 0.5 as the learning decay factor, and 10 epochs as the learning patience across all datasets. In our implementation, we restrict our batch size to 4 due to hardware limits. No batch normalization is adopted in either stream in VC-Net and we use ReLU (Rectified Linear Unit) activation for both 2D and 3D convolutional layers in corresponding streams and sigmoid activation for the final vessel probability output from both 2D and 3D streams. The network is implemented in Tensor-Flow framework and the total training time is around 10 hours on two NVIDIA GeForce GTX 1080 GPUs with 8 GB GDDR5X memory. The inference time is given in the following subsection. Data and source code of this work will be made available.</p><p>The performance of our VC-Net and all methods in comparison are numerically evaluated by the following three quantitative metrics, which are defined from the classifier confusion matrix from different aspects:</p><p>Dice Similarity (Dice), 2T P/(2T P + F P + F N), (the same as F-score under most of the circumstances) generally measures the intersection over union between prediction and ground truth. It involves true positive (T P ), false positive (F P ), and false negative (F N), so as to be the most comprehensive indicator to evaluate the sparse vessel segmentation in a large portion of background, i.e., true negative (T N).</p><p>Precision, T P/(T P + F P ), measures the model ability of ruling out the noise contributions and obtaining the correct vessel voxels.</p><p>False Positive Rate (FPR), F P/(F P + T N), examines the model ability of distinguishing the real background and noise against vessels, which is crucial for the clinical purpose.</p><p>The best results in tables are shown in bold font. Here we do not include the metric of Accuracy, due to its extremely high value (e.g., ≥ 99%) for all methods. The reason is that it involves dominant portion of background (true negative) together with highly sparse target (e.g., the segmented vessels in our task) in computation and consequently loses its effectiveness for segmentation evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison with the State-of-the-Art</head><p>We first compare our VC-Net performance on TubeTK dataset with four state-of-the-art deep learning based methods (i.e., 3D U-Net <ref type="bibr" target="#b6">[7]</ref>, 2D U-Net <ref type="bibr" target="#b32">[33]</ref>, DeepVesselNet <ref type="bibr" target="#b39">[40]</ref>, and Uception <ref type="bibr" target="#b33">[34]</ref>) and one classical parametric intensity-based method (i.e., vesselness algorithm <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12]</ref>) in 3D vessel segmentation. All deep learning methods in comparison are trained until convergence by using the same dataset split or using the results reported from their original publication (such as Uception). For 2D U-Net, we train it with 128 × 128 2D patches, whose amount is over 10 times of the 3D patch amount extracted for the 3D CNN based methods in comparison with on-the-fly data augmentation for a fair data acquisition. For DeepVesselNet, we have tried different combinations of their data pre-processing process and chosen the image intensity clipping for obtaining an optimal performance on TubeTK dataset.</p><p>The quantitative performance comparison of these methods on Tu-beTK dataset is shown in Tab. 1. '−' means 'not applicable' due to lack of their implementations or results. Here we also provide the pervolume inference time and the parameter number to evaluate the model efficiency besides the segmentation performance. From Tab. 1, we can see that our VC-Net has overall the best segmentation performance among all the methods on TubeTK dataset. With the 2D composited MIP feature integration, our network performs better than a pure 3D U-Net <ref type="bibr" target="#b6">[7]</ref> over the three different metrics on segmentation results. The qualitative comparison of MIP-wise (e.g., 5-sliced) segmentation results and 3D global vessel segmentation results between our VC-Net and 3D U-Net (one of the most robust state-of-the-art deep learning based methods for biomedical image segmentation) is shown in <ref type="figure" target="#fig_4">Fig. 4  (a)</ref>. With the 2D composited MIP complementary information, the final vessel segmentation shows better connectivity and better small vessel capturing as marked in red circles (3D global vessel segmentation visualization) and green circles (2D MIP vessel segmentation visualization). Besides the segmentation performance gain, the increase of time and space complexities in VC-Net is not high compared with a standalone 3D U-Net as shown in Tab. 1, since only a half 2D U-Net (i.e., 7.8 million parameters) is involved in the 2D MIP stream. On the other hand, since the computational complexity of 3D convolution operations apparently overweighs that of 2D convolution operations, the 3D stream still dominates the computational complexity of the entire VC-Net. Another observation is that the 3D U-Net greatly outperforms 2D U-Net <ref type="bibr" target="#b32">[33]</ref> even if the latter contains many more feature embedding channels, since the former method is able to capture the cross-slice continuity and that is why 3D CNN should be involved in such sparse 3D object segmentation with complex topology. Moreover, the full 2D U-Net implies much larger amount of 2D convolution operations and model parameters, which lead to the unsatisfactory model efficiency. DeepVesselNet <ref type="bibr" target="#b39">[40]</ref> fails to yield a good performance as they reported in their own dataset, which could result from the lack of the pre-training procedure, i.e., a relatively complicated data preprocessing, and the instability of their loss function, which is severely sensitive to the training perturbation. In addition, their light-weighted network only consists of five convolutional layers for high efficiency, whereas its simplicity may undermine its cross-dataset robustness. The qualitative comparison with DeepVesselNet is given in <ref type="figure" target="#fig_4">Fig. 4 (b)</ref>, from which you can see that the DeepVesselNet result is much noisier and has severer connectivity issues. Our method also outperforms the best Uception result reported in <ref type="bibr" target="#b33">[34]</ref> on TubeTK dataset with even less data pre-processing procedure, which implies that empirical neural network modification to increase the model complexity does not guarantee a better performance all the time. To be comprehensive, we apply the vesselness algorithm <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12]</ref> as a traditional benchmark method for comparison based on the available data modality in TubeTK dataset, which is a widely-used approach to segment the cylindrical vessel structures in medical field. As shown in Tab. 1, all deep learning methods greatly outperform classical vesselness method on TubeTK dataset with much higher efficiency, which is beneficial from the essence of deep learning techniques: mostly end-to-end, more adaptive non-linearity, controllable ambiguity for feature extraction and integration process than traditional complex manual parameter-driven algorithms to boost the overall robustness and accuracy for real-world image learning tasks. More qualitative comparisons with 3D U-Net and DeepVesselNet are provided in Supplemental Material. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiments and Evaluation on MICRO-MRI Dataset</head><p>As mentioned in Sec. 2, most of the recently related work on brain vasculature segmentation tasks is limited to major arteries (in Sec. 4.1) since currently most of the available brain MRI image datasets with adequate amount and consistent quality are MRAGs. However, with assistance from the neurologists and radiologists under our collaboration, we can now extend VC-Net from general artery segmentation to the vasculature segmentation of major artery and major vein, separately. More inspiringly, we also demonstrate that our VC-Net is capable of extracting the micro vessels in the complicated real patient MICRO-MRIs. It is noted that the segmentation of brain vessels becomes more challenging in micro-level than major-level, and more challenging in veins than arteries. The following experiments show that our method has bigger improvements on more challenging cases (i.e., micro-level vessel and major-level vein segmentations) compared with other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Major-Level Artery Segmentation and Visualization</head><p>The MRAGs in clinic MICRO-MRI datasets under our collaboration as mentioned in Sec. 3.2 focus on midbrain area from where the majorlevel vessels are relatively denser and more observable. Currently our collaborative domain experts apply the state-of-the-art model-driven NLS method <ref type="bibr" target="#b46">[47]</ref> (i.e., MRAGnls) followed by case-wise threshold selection to extract the clean midbrain vessels, which requires different data modalities and tedious manual parameter-tuning as stated in Sec. 3.2. However, from <ref type="figure" target="#fig_5">Fig. 5 (a)</ref> we can see that its segmentation result still fails to be free from location-dependent interference, such as superior sagittal sinus (red dotted circles in 3D visualization and green dotted circles in MIP visualization) and some random scattered voxel noise (red solid circles). Aiming to improve the segmentation performance with less manual-parameter tuning and less modality requirement yet provide a much more efficient method for major artery extraction that is well applicable for future patient case collection, we train our VC-Net with only TE1 pre-contrast SWI (a single-modal MRAG) data as input. From Tab. 2 we can see that our quantitative evaluation results outperform the MRAGnls method on all metrics even if the latter one integrates and enhances artery signal from several different data modalities. Here we also include our numerical comparison with 3D U-Net (under the same experiment setting), the most competitive method on TubeTK dataset to show our network's cross-dataset robustness and superiority. One can also observe that the numerical difference of the performance in MICRO-MRI major-level artery dataset is not as obvious as the major-level vein and micro-level vessel datasets as shown in the following two subsections, which may result from the fact that the MRAGs are relatively clearer in terms of the image dose effect and the noise type. More qualitative comparisons with the MRAGnls method are provided in Supplemental Material.   <ref type="figure" target="#fig_5">Fig. 5 (b)</ref> that the vein labels still fail to be free from artery artifacts as marked in red / green dotted circles. In addition, the MRVGs overall have more challenging noise type (e.g., very strong artery artifacts) than single-modal MRAGs due to modality formulation; therefore, the corresponding intensity-based vein labels tend to have more fuzzy edges. However, our VC-Net is able to effectively overcome the aforementioned difficulty as shown in <ref type="figure" target="#fig_5">Fig. 5</ref> (b). Tab. 3 shows the numerical comparison among ours, MRVGavg, and 3D U-Net under the same experiment setting. We can see that our numerical results overall outperform the other two methods. The general lower numerical performance compared to artery segmentation (in the previous subsection) may result from more challenging input data type and the sinus region (i.e., the large and thick vein-like area at the bottom in 3D global segmentation). More qualitative comparisons with the MRVGavg method are provided in Supplemental Material. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Micro-Level Vessel Segmentation and Visualization</head><p>As mentioned in the introduction, the micro-cerebrovasculature turns out to be a good physical indicator of many neurological disorders and vascular diseases; thus it is extremely important and a breakthrough for MICRO MRAV diagnosis of vascular disease to trace small vessels and analyze their topology, morphology, density, and distribution with direct visual inspection of microvascular abnormalities in-vivo. Besides the major-level vessel segmentation, our VC-Net shows great capability to track the micro vessels as well. In this experiment, it is noted that we have the input data modality format which is different from those in the previous experiments as shown in the first column in <ref type="figure">Fig. 6 (c)</ref>, i.e., the minimum intensity projection (MinIP) images. The different modalities of input image slice examples in the experiments are provided in Supplemental Material. All vessels, including major and micro ones, appear to be dark (very low voxel intensity) and show no contrast to the dark background, which may cause confusion to our network in the 2D composited MIP segmentation stream even if we accordingly switch to compute the MinIP instead. In order to keep the framework consistency and take advantage of the pre-trained network in the previous subsections, we inverse the voxel intensity within the brain foreground area in the whole 3D SWI image and then extract 1320 random patches from each training image (considering the vessels are much denser in a large-sized 3D volume, more patches per case can make up for limited datasets, e.g., two training cases). By fine-tuning our VC-Net pre-trained on major-level MRI images with these patches, our network is capable of capturing the continuous micro vessels clearly  as shown in <ref type="figure">Fig. 6</ref>. Currently, SWI is the only data modality available to capture the micro-level vessels and it includes a large number of major-level vessels as well. Consequently, it is quite challenging to provide a rigorous numerical evaluation on pure micro-level vessels. Alternatively, Tab. 4 shows the numerical evaluation based on the whole SWI image for reference, in which our method quantitatively outperforms 3D U-Net (also fine-tuned from the weights pre-trained on the same major-level MRI images) and the SWIATRG method on Dice, precision, and FPR metrics. The SWIATRG method is a state-of-the-art algorithm that our collaborative domain experts are currently using as described in Sec. 3.2. <ref type="figure">Fig. 6 (a)</ref> and (b) show our whole brain segmentation result (in gold) accompanied by non-overlapping midbrain subarea segmentation results (in red) and their corresponding ground truth (in blue). <ref type="figure">Fig. 6</ref> (c) and (d) visualize the qualitative performance of pair-wise comparisons. From <ref type="figure">Fig. 6 (d)</ref>, we can see that the result from the SWIATRG method suffers severe voxel intensity noise (circled in yellow) and unexpectedly thicker vessels (circled in blue) due to the bold intensity threshold in sacrifice to capture as many micro vessels as possible; however, it still lacks the satisfiable ability of detecting micro vessels as shown in the corresponding zoom-in green patch error maps (white: true positive, red: false positive, blue: false negative, black: true negative). In addition, the SWIATRG method requires several data modalities which are acquired from different time points as mentioned in Sec. 3.2; consequently, the corresponding computed vessel mask also has nonnegligible registration errors (circled in white). However, even if 3D U-Net can alleviate most of the issues that the SWIATRG method is faced with, it is still insufficient to track the super micro vessel without the 2D MIP complementary information as shown in first two zooming-in patches (circled in yellow) and their corresponding error maps in <ref type="figure">Fig. 6  (c)</ref>. Similar as the SWIATRG method, 3D U-Net also performs more boldly on covering major-level vessels (less blue on error maps) as shown in the third zooming-in patch in <ref type="figure">Fig. 6 (c)</ref>. However, 3D U-Net incurs more noises (more red on error maps) and thus it has a worse precision. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Discussion on Labeling and Visualization Tool</head><p>It is interesting to note that in the TubeTK dataset even the ground truth vessel mask does not cover certain vessel continuity, which can be clearly traced on MIPs (such as some yellow circles in the ground truth MIPs in <ref type="figure" target="#fig_4">Fig. 4</ref>), since it is very difficult to label all the vessels in corresponding MRA slices in a single slice-by-slice manipulation without referring to MIP and 3D global visualization. However, based on the ground truth MIP labeling slices in the MICRO-MRI datset from our experiments and collaborative evaluations, we can see that such issue is greatly alleviated, since our cerebrovascular labeling and visualization tool is applied to generate (refine) these ground truth vessel labels. In the future, we will further refine the ground truth vessel labels of TubeTK dataset by using our developed labeling tool (such as examples shown in Supplemental Video) under the domain experts' guidance for better public sharing and use. Last but not least, to our knowledge, we are the first to investigate and apply our 3D brain vasculature segmentation to different vessel types and levels, especially the micro-level vessel segmentation; also, it is the first time that the whole brain vessels with different types / levels can be visualized in-vivo. Therefore, we have also designed a visualization tool for jointly showing different vasculature systems. Our  tool enables the visualization for any combination of different vessel systems in user-defined color and lighting, and support all essential auxiliary interactions such as rotation, translation, scaling, zooming in / out, clipping, etc., for better examination. <ref type="figure">Fig. 7 (a)</ref> shows the joint visualization for our prediction results of major-level midbrain arteries and veins; and <ref type="figure">Fig. 7 (b)</ref> shows all three vasculature systems aligned together, i.e., major-level midbrain arteries and veins, and micro-level vessels, from MICRO-MRI dataset. Supplemental Video is included for demonstrating the dynamic visualization and interaction in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this work, we have proposed the VC-Net, a deep neural network to extract and visualize high-fidelity 3D cerebrovascular structure from highly sparse and noisy images. VC-Net has three major components, i.e., 3D and 2D dual-domain segmentation streams, 3D-to-2D projection for two-stream design, and 2D-to-3D unprojection for joint embedding operations. By unprojecting the learned multislice composited 2D MIP feature vectors into the 3D volume embedding space, the proposed framework can strengthen the sparse 3D vascular representation by better capturing the small / micro vessels as well as improving the vessel connectivity, which outperforms the state-of-the-art classical and deep learning based methods. In medical practice, this work can be used as the key functions for real-time in-vivo segmentation and visualization of sparse and complicated 3D microvascular structure to improve MICRO MRAV diagnosis of vascular disease.</p><p>In the future, we will continue to explore research problems related to volume rendering supported 3D exploration and analysis to leverage both the 2D findings and the 3D knowledge and analytics by deep neural networks. We will extend current MIP-based volume rendering (i.e., a special case of volume rendering) into more general volume rendering scenarios, such as X-ray projections, full RGB composition, multi-view MIPs, and flow modeling concepts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>5 / 27 ms, bandwidth = 180 Hz / pxl, flip angle = 15 • (pre-contrast and final post-contrast data) and 20 • (first and second post-contrast data). The voxel spacing is 0.22 × 0.22 × 1 mm 3 with a volume size of 1024 × 832 × 96 voxels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>input 3D patch, 1 ≤ ≤ 6 : the slice in input 3D patch, 1 ≤ ≤ 16 : the restored slice among 5 slices in MIP , 1 ≤ ≤ 5 [ ] = the dimension of the feature of , 1 ≤ ≤ 32</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>(a) Illustration of the 3D-to-2D projection in the spatial domain for computing a 2D composited MIP from a 3D volume patch. (b) and (c) Illustration of the detailed computations in back projection layers for 2D-to-3D unprojection process in the embedded feature domain. As illustrated in bottom (b), the consecutive MIPs P1 and P2 with overlapping slice coverage of S3, S4, S5 contribute to information completeness in 3D patch volume. A pixel location on the 5-sliced MIP 2D plane which keeps the feature information of only one voxel out of five (e.g., the middle orange pixel and the left bottom blue pixel in P1 are back projected to S2 and S5) now can be supplemented by P2's back projection (e.g., the middel green voxel on S3 and the left bottom purple voxel on S6).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Some qualitative comparison results from TubeTK dataset: The 3D global vessel segmentations are shown from superior direction. The MIP segmentations are visualized by 5-sliced MRA images, and the corresponding vessel masks in MIPs are marked in semi-transparent red. The highlighted comparison areas are marked in circles. Yellow-circled areas are some minor mistakes in the ground truth (discussed in Sec. 4.2.4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Some qualitative comparison results from MICRO-MRI major-level vessel dataset: The 3D global vessel segmentations are shown from superior direction. The MIP segmentations are visualized by 5-sliced MICRO-MRI images, and the corresponding vessel masks in MIPs are marked in semi-transparent red. The highlighted comparison areas are marked in circles. The 3D MRAG / MRVG images from MICRO-MRI dataset only focus on midbrain area and thus have less vessels compared with TubeTK dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :Fig. 7 :</head><label>67</label><figDesc>(a) Whole brain micro vessel segmentation result of our method from superior direction. (b) Midbrain non-overlapping subarea detail visualization (in red) with ground truth in comparison (in blue). (c) Qualitative comparison results between our method and 3D U-Net on MICRO-MRI dataset, shown as three patch details extracted from three different 5-sliced MinIPs and their corresponding error maps on micro-level and major-level vessels. (d) Qualitative comparison results between our method and the SWIATRG method on MICRO-MRI dataset, shown as two 5-sliced MinIP segmentations and their corresponding error maps on micro-level vessels. The highlighted comparison areas are marked in circles. Joint 3D visualization of our segmentation results on MICRO-MRI dataset in two different testing cases: (a) Whole midbrain majorlevel arteries (in red) and veins (in blue). (b) Major-level arteries (in red), major-level veins (in blue), and micro-level vessels (in pink) from slice No. 20 to 40 within midbrain area. Some large pink vessels are also major-level ones which are absent from major-level MRAGs and MRVGs due to the different image acquisitions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>• Y.Wang, G. Yan, H. Zhu, J. Hua, and Z. Zhong are with the Department of Computer Science, Wayne State University, Detroit, MI 48202. E-mail: {yifan.wang2,guoliyan,hkzhu,jinghua,zichunzhong}@wayne.edu. • S. Buch, Y. Wang, and E. M. Haacke are with the Department of Radiology,</figDesc><table /><note>Wayne State University, Detroit, MI 48201. E-mail: {sagarbuchmri,neuying}@gmail.com, nmrimaging@aol.com. Manuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication xx xxx. 201x; date of current version xx xxx. 201x. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org. Digital Object Identifier: xx.xxxx/TVCG.201x.xxxxxxx</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>3D Volume Segmentation Stream 2D Composited MIP Segmentation Stream</head><label></label><figDesc></figDesc><table><row><cell></cell><cell cols="3">32 64</cell><cell></cell><cell>196 64 64</cell></row><row><cell></cell><cell cols="2">64x64x8</cell><cell cols="3">64 64 128</cell><cell>384 128 128</cell></row><row><cell>Random 3D Patch 128x128x16</cell><cell></cell><cell></cell><cell></cell><cell>32x32x4</cell><cell>128 128 256</cell><cell>768 256 256</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>16x16x2</cell><cell>256 256 512</cell></row><row><cell>3D-to-2D Projection</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MIP Pixel Index Information</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Joint Embedding</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Final Vasculature Mask</cell></row><row><cell>256x384</cell><cell cols="2">32 32</cell><cell></cell><cell></cell><cell>96</cell><cell>32 32</cell><cell>Prediction 128x128x16</cell></row><row><cell></cell><cell>128x192</cell><cell cols="3">32 64 64</cell><cell>192 64 64</cell></row><row><cell></cell><cell></cell><cell></cell><cell>64x96</cell><cell cols="2">64 128 128</cell><cell>384 128 128</cell><cell>256x384</cell></row><row><cell>2D Composited MIP 256x384</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>32x48</cell><cell>128 256 256</cell><cell>768 256 256</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>16x24</cell><cell>256 512 512</cell><cell>1x1x1</cell><cell>Conv2D 1x1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Layer Output Flow</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Quantitative performance evaluation of different methods on TubeTK dataset.</figDesc><table><row><cell cols="6">Methods / Metrics Dice (%) ↑ Precision (%) ↑ FPR (%) ↓ Time (s) ↓ # Para. ↓</cell></row><row><cell>Ours</cell><cell>71.81</cell><cell>76.66</cell><cell>0.0821</cell><cell>10.8</cell><cell>24 M</cell></row><row><cell>3D U-Net</cell><cell>71.01</cell><cell>74.00</cell><cell>0.0958</cell><cell>7.5</cell><cell>19 M</cell></row><row><cell>Uception</cell><cell>67.01</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell></row><row><cell>DeepVesselNet</cell><cell>64.12</cell><cell>63.75</cell><cell>0.1465</cell><cell>1.8</cell><cell>0.06 M</cell></row><row><cell>2D U-Net</cell><cell>65.10</cell><cell>70.05</cell><cell>0.1041</cell><cell>16.7</cell><cell>31 M</cell></row><row><cell>Vesselness</cell><cell>37.71</cell><cell>47.69</cell><cell>0.1393</cell><cell>186.6</cell><cell>−</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Quantitative performance evaluation of different methods on major-level artery segmentation.Currently, the MRVGs are not readily and directly available from the scanner, so there is no raw MRI image that can produce pure veins. There are different ways that people have used to derive it such as the SWI, QSM, or R2 * data, where the veins are highlighted. However, they all have background tissues as well as noise associated with them. In this work, the MRVGs from the MICRO-MRI dataset are ultimately acquired through the MRVGavg method by enhancing vein signals from different data resources as mentioned in Sec. 3.2. Our collaborative domain experts compute the vein labels by post-manual case-wise threshold adjustment on the MRVGs. However, we can see from the major-level vein case in</figDesc><table><row><cell cols="4">Methods / Metrics Dice (%) ↑ Precision (%) ↑ FPR (%) ↓</cell></row><row><cell>Ours</cell><cell>82.98</cell><cell>83.69</cell><cell>0.0337</cell></row><row><cell>3D U-Net</cell><cell>82.64</cell><cell>83.52</cell><cell>0.0342</cell></row><row><cell>MRAGnls</cell><cell>80.60</cell><cell>82.07</cell><cell>0.0354</cell></row><row><cell cols="4">4.2.2 Major-Level Vein Segmentation and Visualization</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Quantitative performance evaluation of different methods on major-level vein segmentation.</figDesc><table><row><cell cols="4">Methods / Metrics Dice (%) ↑ Precision (%) ↑ FPR (%) ↓</cell></row><row><cell>Ours</cell><cell>76.46</cell><cell>82.20</cell><cell>0.0849</cell></row><row><cell>3D U-Net</cell><cell>76.02</cell><cell>80.40</cell><cell>0.0946</cell></row><row><cell>MRVGavg</cell><cell>73.73</cell><cell>64.99</cell><cell>0.2294</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Quantitative performance evaluation of different methods on micro-level vessel segmentation.</figDesc><table><row><cell cols="4">Methods / Metrics Dice (%) ↑ Precision (%) ↑ FPR (%) ↓</cell></row><row><cell>Ours</cell><cell>74.40</cell><cell>74.74</cell><cell>0.7052</cell></row><row><cell>3D U-Net</cell><cell>74.08</cell><cell>72.70</cell><cell>0.7989</cell></row><row><cell>SWIATRG</cell><cell>70.23</cell><cell>61.84</cell><cell>1.413</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank the reviewers for their valuable comments. We are grateful to Yongsheng Chen from Neurology for the early discussion of this work, Pavan K. Jella from Radiology for preparing and collecting the clinical datasets, and Michelle Hua from Cranbrook Schools for pre-processing the datasets and proofreading the paper. This work was partially supported by the NSF under Grant Numbers IIS-1816511, CNS-1647200, OAC-1657364, OAC-1845962, OAC-1910469, the Wayne State University Subaward 4207299A of CNS-1821962, NIH 1R56AG060822-01A1, NIH 1R44HL145826-01A1, ZJNSF LZ16F020002, and NSFC 61972353.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mra</forename><surname>Tubetk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dataset</surname></persName>
		</author>
		<ptr target="https://public.kitware.com/Wiki/TubeTK/Data" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast and robust three-dimensional best path phase unwrapping algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Abdul-Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gdeisat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Burton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lalor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lilley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Optics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">26</biblScope>
			<biblScope unit="page" from="6623" to="6635" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond Euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Subvoxel vascular imaging of the midbrain using USPIO-Enhanced MRI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-G</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Haacke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="page">117106</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An interleaved sequence for simultaneous magnetic resonance angiography (MRA), susceptibility weighted imaging (SWI) and quantitative susceptibility mapping (QSM)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Haacke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magnetic Resonance Imaging</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Statistical 3D vessel segmentation using a Rician distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Noble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Medical Image Computing and Computer Assisted Intervention</title>
		<meeting>International Conference on Medical Image Computing and Computer Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="82" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3D U-Net: learning dense volumetric segmentation from sparse annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ö</forename><surname>Içek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Medical Image Computing and Computer Assisted Intervention</title>
		<meeting>International Conference on Medical Image Computing and Computer Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A geometric flow for segmenting vasculature in proton-density weighted MRI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Descoteaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Siddiqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="497" to="513" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A point set generation network for 3D object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="605" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Globally optimal active contours, sequential Monte Carlo and on-line learning for vessel segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Florin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="476" to="489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3D cerebrovascular segmentation combining fuzzy vessel enhancement and level-sets with anisotropic energy weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Forkert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schmidt-Richberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fiehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Illies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Möller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Säring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Handels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ehrhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magnetic Resonance Imaging</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="262" to="271" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multiscale vessel enhancement filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Niessen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vincken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Viergever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Medical Image Computing and Computer Assisted Intervention</title>
		<meeting>International Conference on Medical Image Computing and Computer Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="130" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deepvessel: Retinal vessel segmentation via deep learning and conditional random field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Medical Image Computing and Computer Assisted Intervention</title>
		<meeting>International Conference on Medical Image Computing and Computer Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="132" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Robust tracing and visualization of heterogeneous microvascular networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Govyadinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Womack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eriksen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mayerich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1760" to="1773" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Graphassisted visualization of microvascular networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Govyadinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Womack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eriksen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mayerich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization Conference</title>
		<meeting>IEEE Visualization Conference</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Susceptibility mapping as a means to visualize veins and quantify oxygen saturation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Haacke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neelavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Magnetic Resonance Imaging</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="663" to="676" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Susceptibility weighted imaging (SWI)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Haacke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Reichenbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magnetic Resonance in Medicine</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="612" to="618" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<title level="m">Spectral Geometry of Shapes: Principles and Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">BET2: MR-based estimation of brain, skull and scalp surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jenkinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pechaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ARGDYP: an adaptive region growing and dynamic programming algorithm for stenosis detection in MRI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Haacke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE International Conference on Acoustics, Speech, and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">465</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning facial expressions with 3D mesh convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">VesselNet: A deep convolutional neural network with multi pathways for robust hepatic vessel segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kitrungrotsakul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Iwamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foruzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="74" to="83" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A-CNN: Annularly convolutional neural networks on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Komarichev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7421" to="7430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A crossmodality learning approach for vessel segmentation in retinal images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="109" to="118" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Globally optimal curvature-regularized fast marching for vessel segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rohr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wörz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Medical Image Computing and Computer Assisted Intervention</title>
		<meeting>International Conference on Medical Image Computing and Computer Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="550" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Segmenting retinal blood vessels with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liskowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Krawiec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2369" to="2380" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Retinal blood vessel segmentation by means of scale-space analysis and region growing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Martínez-Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bharath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Parker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Medical Image Computing and Computer Assisted Intervention</title>
		<meeting>International Conference on Medical Image Computing and Computer Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="90" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Geodesic convolutional neural networks on Riemannian manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-level deep supervised networks for retinal vessel segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Assisted Radiology and Surgery</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2181" to="2193" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Vessel segmentation using a shape driven flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yezzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Turk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Medical Image Computing and Computer Assisted Intervention</title>
		<meeting>International Conference on Medical Image Computing and Computer Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="51" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">CT angiography with spiral CT and maximum intensity projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Napel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mcdonnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Enzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jeffrey</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">185</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="607" to="610" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">PointNet: Deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Medical Image Computing and Computer Assisted Intervention</title>
		<meeting>International Conference on Medical Image Computing and Computer Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cerebrovascular network segmentation of MRA images with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sanchesa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vigon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Naegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Symposium on Biomedical Imaging</title>
		<meeting>IEEE International Symposium on Biomedical Imaging</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="768" to="771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Quantitative imaging of intrinsic magnetic tissue properties using MRI signal phase: an approach to in vivo brain iron metabolism?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schweser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deistung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lehr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Reichenbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2789" to="2807" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Detecting sub-voxel microvasculature with USPIO-enhanced susceptibility-weighted MRI at 7 T</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Eteer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alhourani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Haacke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magnetic Resonance Imaging</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="90" to="100" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep vessel segmentation by learning graphical connectivity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page">101556</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Inception-v4, inceptionresnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The AAAI Conference on Artificial Intelligence</title>
		<meeting>The AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Improving susceptibility mapping using a threshold-based k-space/image domain iterative reconstruction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neelavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Haacke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magnetic Resonance in Medicine</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1396" to="1407" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">DeepVesselNet: Vessel segmentation, centerline prediction, and bifurcation detection in 3-D angiographic volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tetteh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Efremov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Forkert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirschke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Piraud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Menze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09340</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The capability of detecting small vessels beyond the conventional MRI sensitivity using iron-based contrast agent enhanced susceptibility weighted imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Haacke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NMR in Biomedicine</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pixel2Mesh: Generating 3D mesh models from single RGB images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="52" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sequential Monte Carlo tracking for marginal artery segmentation on CT angiography by multiple cue fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Peplinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Medical Image Computing and Computer Assisted Intervention</title>
		<meeting>International Conference on Medical Image Computing and Computer Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="518" to="525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">DeepOrganNet: On-the-fly reconstruction and visualization of 3D/4D lung models from single-view projections by deep deformation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="960" to="970" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Segmentation of cerebral vessels and aneurysms from MR angiography data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Noble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Biennial International Conference on Information Processing in Medical Imaging</title>
		<meeting>Biennial International Conference on Information Processing in Medical Imaging</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="423" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Directionally convolutional networks for 3D shape segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2698" to="2707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Noncontrast-enhanced magnetic resonance angiography and venography imaging with enhanced angiography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Haacke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Magnetic Resonance Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1539" to="1548" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
