<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Interactive Visual Study of Multiple Attributes Learning Model of X-Ray Scattering Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suphanut</forename><surname>Jamonnak</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyu</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Hoai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Yager</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
						</author>
						<title level="a" type="main">Interactive Visual Study of Multiple Attributes Learning Model of X-Ray Scattering Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Fig. 1. System interface: (A) Control panel. (B) Attribute panel. (C1) Embedded view in ACT (Actual attribute vector) space with a highlighted image group in red. (C2) Embedded view in FEA (Feature vector) space. (C3) Embedded view in PRD (Prediction vector) space. (D1) Group panel 1: a cluster view showing selected images as attribute flowers in two spatial clusters of PRD (C3). (D2) Group panel 2: a cluster view showing the images in two spatial clusters of FEA (C2). (E) Detail image view with ACT/PRD values.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>• X. Huang, S. Jamonnak, Y. <ref type="bibr">Zhao</ref>  X-ray scattering helps scientists discover molecular and nano-level physical structures of materials such as nano-particles, protein, lithographic gratings, polymer films, and so on. X-ray beam hits on a sample material, and the scattered x-ray diffraction patterns are collected by the detector which are eventually presented in the x-ray scattering images. This technique is widely used in biomedical, material, and physical applications by analyzing structural patterns in the x-ray scattering images <ref type="bibr" target="#b36">[37]</ref>. In general cases, human experimenters must apply their domain expertise to interpret the features in the image (such as rings, diffraction spots, or diffuse scattering) in order to understand the image and select the most appropriate follow-up analysis. However, the scale of an image dataset (an x-ray equipment generates up to 1 million images per day) often imposes a heavy burden in image screening and understanding.</p><p>Recently, deep learning models are employed in classifying and annotating multiple image attributes from experimental or synthetic images, which were shown to outperform previously published methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b32">33]</ref>. As most deep learning paradigms, these methods are not easily understood by material, physical, and biomedical scientists. The lack of proper explanations and absence of control of the decisions would make the models less trustworthy. While considerable effort has been made to make deep learning interpretable and controllable by humans <ref type="bibr" target="#b4">[5]</ref>, the existing techniques are not specifically designed for the scientific image classification models of x-ray scattering images, which requires extra consideration in finding:</p><p>• How the learning models perform for a diverse set of overlapped image attributes with high variation? • How the co-existence of attributes in x-ray images may affect the classification results? Unfortunately, few existing visualization tools <ref type="bibr" target="#b4">[5]</ref> are devoted to visually analyzing the learned results of multiple attributes, objects, or segments with these models.</p><p>In response, we develop a visual analysis system for users to interactively study the model predictions with respect to the multiple structural attributes of x-ray scattering images. The system has several features:</p><p>• Image instances are projected with t-SNE and visualized in three vector spaces: actual labeling space from domain scientists, feature space extracted by a residual network, and prediction space of the model output. Users can explore the instances in these spaces interchangeably for visual comparison of different groups, outlier detection, and drill-down study of images. • Users can select any image group and then observe their visual features and study the attribute detection performance. In particular, the distributions of these images in different spaces are easily discovered by user controlled spatial clustering in the embedded spaces. • An attribute-flower visualization is designed to represent an image to manifest the attribute recognition results. Compared with the ground truth labels, it depicts false positive (FP), false negative (FN), true negative (TN), and true positive (TP) predictions of multiple attributes. • The learning outcome of the images with different groups of coexisted attributes is visualized, which provides visual cues of model performance with respect to attributes relationships. • The visual interface integrates multiple coordinated views and includes easy user interactions that facilitate iterative exploration and comparison. The system alleviates domain scientists' efforts in understanding the performance of deep learning models for x-ray scattering images. They can identify outlier images, find spurious data clusters, understand the impacts of multiple attributes, and from this improve the training data or the learned model. Several case studies show the utility and effectiveness of the system. Domain scientists provide positive feedback about the usefulness of this visual interaction tool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>It is of great importance to analyze x-ray scattering images by recognizing structural attributes such as ring, halo, diffuse scattering, and so on <ref type="bibr" target="#b11">[12]</ref>. Recently, deep learning models are employed for the x-ray scattering data <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b29">30]</ref>. Wang et al. <ref type="bibr" target="#b32">[33]</ref> apply Convolutional Neural Networks (CNN) over both experimental and synthetic images to detect important attributes. Guan et al. <ref type="bibr" target="#b6">[7]</ref> further develop a DVFB-CNN model (Double-View Fourier-Bessel CNN) which combines Fourier-Bessel transform (FBT) with a CNN model. With the improvement of the performance by different designs of models, the application of deep learning in this domain would make the attribute annotation convenient for the domain experts.</p><p>It is also important to open the black-box by model interpretation to build confidence for domain experts to apply their models. Choo et al. <ref type="bibr" target="#b4">[5]</ref> categorize explainable deep learning into three major directions: model understanding, model debugging, and model refinement. Computational approaches discover important scores of the input features contributing to the prediction results. Perturbation methods <ref type="bibr" target="#b0">[1]</ref>, saliency-based methods <ref type="bibr" target="#b25">[26]</ref>, LIME <ref type="bibr" target="#b23">[24]</ref>, and influence functions <ref type="bibr" target="#b13">[14]</ref> are proposed for the purposes. In particular, class activation map (cam) <ref type="bibr" target="#b40">[41]</ref> and grad-cam method <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> can detect pixel areas of one individual image which contributes to a corresponding class prediction. They find the important neurons in a hidden layer and explain them by highlighting direct visible and understandable features in every single image. However, these methods are not designed to understand the model performance of a large set of images with multiple attributes. Our system does not focus on mathematical algorithms that find features in an image linking to classification results and then visualizing them such as by image heatmaps. Instead, our system is built up on studying the distributions of many images in different data spaces.</p><p>Interactive visualization tools are developed in providing in-depth understanding of how deep learning models work. Tools such as Tensorflow Playground <ref type="bibr" target="#b27">[28]</ref>, Tensor Board <ref type="bibr" target="#b34">[35]</ref>, and ConvNetJS <ref type="bibr" target="#b10">[11]</ref> allow users to visualize and interact with the activation maps and network structures, together with line graphs and histograms of characteristic statistics. DeepVis <ref type="bibr" target="#b37">[38]</ref> shows that optimizing synthetic images with better natural image priors produces more recognizable visualizations. CNNVis <ref type="bibr" target="#b16">[17]</ref> system helps designers in their understanding and diagnosis of CNNs by exploring the learned representations in the graph layout of the neural networks. ActiVis <ref type="bibr" target="#b8">[9]</ref> integrates an embedding view with multiple coordinated views for visual model exploration. Embedding Projector <ref type="bibr" target="#b28">[29]</ref> visualizes input images in a 2D or 3D embedding space (by PCA or t-SNE), to reveal the relationship among these instances. For studying the training process, Deep View <ref type="bibr" target="#b39">[40]</ref> presents a level-of-detail framework that measures the evolution of the deep neural network both on a local and on a global scale. Recently, visualization tools <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b33">34]</ref> have been developed for studying deep generative models (e.g. GAN) working on benchmark natural images. These methods have not been specifically designed for x-ray scattering images.</p><p>Our approach allows users to analyze the results of physical attributes recognition in special sets of scientific image data, where multiattributes may have inherent correlations and co-exist in one x-ray image. And these relationships among attributes may play an important role and affect the performance of the trained model predictions for each image. Therefore, it is different from those existing tools designed for explaining disjoint multi-attributes classification <ref type="bibr">[20-22, 29, 32, 40]</ref>. The scientific attributes have large structural variations, for example, rings can be very large circular structures and can also be very small and indiscernible by eyes <ref type="bibr" target="#b38">[39]</ref>. The attributes are also correlated in many cases (see Sec. 3.1). Based on these features, existing tools cannot be directly applied. Effective visualization tools are needed to discover the relationship among the scientific structural objects on scientific images and the performance of deep learning models. Our system is different from most existing methods, in which image, audio, and natural language datasets are projected and visualized by linking the final decision with the origin images/text data elements. Our visualization techniques focusing on multiple structures on the images may also be extended to multiple object detection and segmentation models, such as (fast-, faster-) R-CNN <ref type="bibr" target="#b38">[39]</ref>, YOLO <ref type="bibr" target="#b22">[23]</ref>, and SegNet <ref type="bibr" target="#b1">[2]</ref>.  <ref type="bibr" target="#b7">[8]</ref> and three data spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">X-Ray Scattering Images and Attributes</head><p>Automatic attribute recognition in x-ray scattering image data is a challenging problem due to the high variation in the same classes. The same structural attributes (patterns) can be of great variety in their appearances. On the other hand, a diverse set of characteristic attributes may co-occur and overlap in the scattering images. The attributes represent the following information about the material being probed in scientific experiments:</p><p>• Experimental conditions, such as the type of beamstop used to block the x-ray beam (e.g., linear or circular beamstop) or the detector position (e.g., beam off image); • Scattering patterns either holistically (e.g, structure factor, high background, strong/weak scattering) or based on visible features (e.g., ring, many rings, halo, high order, diffuse high/low-q); • Material structures implied by the scattering, such as BCC (bodycentered cubic), FCC (face-centered cubic), and polycrystalline; <ref type="figure" target="#fig_0">Fig. 2</ref> shows nine images including 17 structural attributes (see Appendix for details). The attributes have relations to each other. Some may be highly correlated while others may be mutually exclusive. In some cases, these relationships are inherent to the definitions, while in other cases they may only be ascertained by exploring correlations in real experimental datasets. It is thus important to evaluate how the trained models have learned these attributes.</p><p>In this work, we utilize an open x-ray scattering dataset <ref type="bibr" target="#b35">[36]</ref>, and an updated ResNet model <ref type="bibr" target="#b32">[33]</ref> that was designed for multiple attributes classification of the dataset. About 1,000 x-ray scattering images were employed in our visualization system. They include different types of images including semiconductors, nano-particles, polymer, lithographic gratings, and so on. The attributes in these images are either labeled by domain experts or synthetically generated by a simulation algorithm <ref type="bibr" target="#b32">[33]</ref>. Each image thus has an actual attribute vector (ACT vector) consisting of 17 Boolean (0 or 1) values to show if the image has a number of the 17 attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">ResNet Deep Learning Model and Data Spaces</head><p>The original 50-layer Residual Network (ResNet) proposed in <ref type="bibr" target="#b7">[8]</ref> is designed for the classification of mutually exclusive image attributes, where the softmax cross-entropy is used as the loss function. For multiple attributes classification of x-ray images, its loss function is modified <ref type="bibr" target="#b32">[33]</ref> by computing the sigmoid cross-entropy for each attribute and then defining the final loss function as the sum of the losses incurred by each attribute. This modified ResNet model <ref type="bibr" target="#b32">[33]</ref> was trained for 17 attributes by more than 100,000 x-ray images. Our system is built up on this model. <ref type="figure" target="#fig_0">Fig. 2</ref> shows the abstract architecture of this model. It has the first 49 hidden convolutional layers for the feature extraction and 1 fully connected layer for classification. A feature vector (FEA vector) with 2048 dimensions is learned by the model for each image. This FEA vector is obtained at the end of the feature extraction phase after average pooling. Then, a fully connected layer is used to generate the final output, i.e. a 17-dimensional prediction vector (PRD vector) representing the predicted structural attributes. The deep learning model reported a mean average precision (mAP) is about 77% <ref type="bibr" target="#b32">[33]</ref>.</p><p>From the model, the x-ray images are represented by vectors in three different spaces (i.e., ACT, FEA, and PRD). The ACT vector has 17 dimensions with each being the Boolean label corresponding to one attribute. The PRD vector with the same size 17 is obtained from the output of the trained ResNet model with fully connected layers for classification <ref type="bibr" target="#b32">[33]</ref>. Each PRD element is the prediction value (0.0 to 1.0) for each attribute with a cut-off value of 0.5 for the final decision. The FEA vector with the size of 2048 dimensions contains the activation values of the last feature extraction layer in ResNet <ref type="bibr" target="#b7">[8]</ref>. The characteristics of an image are supposed to be extracted and representative by its FEA vector, though it is not directly interpretive.</p><p>Studying and comparing the x-ray scattering images in these spaces can reflect the performance of the ResNet model. For example, a group of images has similar FEA vectors means the model extracts similar feature elements. If they also have similar PRD vectors, it shows that the model well utilizes the extracted features in the classification phase. On the other hand, these images may not be close in ACT space, which indicates that the actual labels do not agree on the classification results. In such cases, users can study these images to identify labeling errors or find model design issues. Therefore, our system is designed to help users interactively study the x-ray images in these three spaces simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Performance Measures</head><p>Model performance metrics help users understand and study the model output and performance. They will play the role of visual cues for effective interactive exploration. For a set of x-ray scattering images, the prediction output of the learning model generates the standard classification evaluation including FP, TP, FN, and TN for each specific attribute, as well as the metrics including the accuracy, precision (recall) and the F1 score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Co-existence Relationship of Attributes</head><p>Having multiple attributes in the same images is a unique feature of x-ray scattering images. It is of interest and importance for scientists to discover the impact of attribute relations for model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Pairwise co-existence</head><p>For two attributes, we employ the measures of Pearson correlation, mutual information, and conditional entropy. Pearson correlation coefficient can be used to measure the linear dependence of two attributes. For two vectors x and y representing distributions of two attributes in a set of images, the correlation r xy = 0 indicates that the two attributes have no linear relationship, while r xy = 1(−1) shows perfect positive (negative) linear dependence. In addition, mutual information can be used to quantify the mutual dependence of two attributes. It measures the reduction of uncertainty of one attribute due to the knowledge of another attribute. Moreover, conditional entropy can also measure the uncertainty of one attribute, under the conditional state of another attribute. It can reflect the weighted average uncertainty of one attribute, given one known attribute. Please see Sec. 4.4 for their usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Multivariate co-existence</head><p>A variety of metrics is presented to measure the multivariate relationship with more than two attributes <ref type="bibr" target="#b30">[31]</ref>. For example, total correlation measures relationships with more than two variables and interaction information extends the concept of mutual information to many variables. However, based on our experiments with domain scientists, they seldomly use such multivariate measures in their work, and these metrics are hard to understand when analyzing x-ray image results. Therefore, we do not employ them but instead enumerate all real combinations of multiple attributes within the image dataset, for example, any pair of two attributes, and 3-tuples of three attributes, etc. For each combination, the prediction accuracy is represented by finding whether all the involved attributes are correctly detected. By visualizing this information, users can find which set of co-existed attributes is interesting for further study. A large number of attributes (more than 8) do not appear together in one image so that this approach is valid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DESIGN OF THE VISUALIZATION SYSTEM</head><p>The visualization system is designed for domain scientists to analyze the model that classifies x-ray images of multiple attributes. Its interface and functions are designed based on a set of analysis tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Analysis Task Characterization</head><p>Domain scientists currently often use the statistical metrics such as precision, recall, and accuracy for learning model evaluation of xray scattering images. However, this only presents a general classwise evaluation rather than an in-depth investigation connecting the groups of images and their various structural patterns. Visual analytics tools can play a vital role in model understanding through interactive visualizations directly over images and attributes. We scheduled several interview meetings with domain scientists who wanted to understand the modified ResNet model behavior over the x-ray scattering image datasets. In these meetings, we conducted a requirement analysis by discussing the topics and their preferred functions in visually analyzing the trained model and datasets. A set of visual analysis tasks were identified as: T1. Analysis in Model Spaces: Investigate scientific images within the spaces of ACT, PRD, and FEA, for users to understand how the images are modeled by the ResNet in the feature space and then classified by fully connected layers in the prediction space, with respect to the real labels. Users can study ResNet model performance by comparing the distributions of images after feature extraction (FEA), after classification (PRD), and with actual labels (ACT). This study needs to be performed in an exploratory process. Therefore, it is important to visualize the images in the three spaces at the same time. T2. Analysis with Group Behaviors: Select and examine specific groups of images in the ACT, PRD, and FEA spaces, in order to find important clusters and outliers with respect to the learning model. T3. Analysis with Image Attributes: Identify important image instances with the performance metrics of individual attributes and coexistent attributes to perform the first two tasks. T4. Analysis with Comparisons: Compare individual images and image clusters for the model prediction performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Visualization Design and Interface Overview</head><p>With respect to these tasks, we design the visualization system by integrating a set of visual interaction functions including: • For T1: Coordinated Visualization in Embedded Spaces: Images are visualized in the 2D canvases of ACT, PRD and FEA spaces, respectively. The goal is to allow users to (1) easily observe many x-ray images and their relationships in these spaces simultaneously, and (2) interactively select, compare, and study images of interest. Therefore, the 2048-dimensional FEA space and 17-dimensional ACT and PRD spaces are projected to the embedded 2D spaces to fulfill the goal. In our system, we have included two commonly used dimension reduction (DR) algorithms, t-SNE and PCA, for deep learning visualization. Other DR methods may further be added. • For T2: Image Group Selection and Visualization: Within the embedded spaces, users are enabled to flexibly select images into groups at each embedded space by lasso tools. Then the selected images in each group are visually highlighted in other spaces. This function is very important for users to freely explore images of interest and conduct comparative analysis among the three spaces. The grouped images are also visualized by a statistic view of image metrics and an image gallery view. They can be further clustered for drill-down study and comparison. . Users can interactively select (with zooming, panning, and lasso-selection) image groups in either embedded view, which is highlighted to show their distributions in the other views. Images in the embedded space are shown as dots whose transparency indicates the model predicted errors. One limitation is that the transparent dots may overlap and lead to a misleading "artificial" transparency value (See Sec. 6.3). Users can filter the visualizations with single or multiple attributes (B). Moreover, the co-existence measures provide visual cues for attribute combinations of interest. Users can study multiple selected groups in the group panels (D1 and D2), where they can also compare them. In each panel, three tab views can be switched to visualize: (1) attribute measures as parallel coordinates plots (see <ref type="figure" target="#fig_4">Fig. 6</ref>) including TP, TN, FP, FN and accuracy, precision, etc.;</p><p>(2) image clusters (D1) and (D2) based on their distance in different data spaces; (3) image thumbnails (See <ref type="figure">Fig. 8</ref>). All these views are coordinated for synchronized changes. Clicking any image instance also adds it to a detail image view (E). The attribute values of actual labels and predicted labels are visualized. Next, we discuss details about visual exploration with these views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Image Exploration in Embedded Spaces</head><p>When a set of x-ray scattering images is loaded into the system, the corresponding structural attributes are loaded and shown in the attribute panel <ref type="figure">(Fig. 1B)</ref>. In each data space (ACT, FEA, and PRD), the image vectors are plotted into a 2D space as points through t-SNE projection. Users select these points in one space and meanwhile, the images are highlighted in other spaces. By comparing their distributions, users are hinted for the model behaviors. For example, a close group of images in ACT means that the images have similar actual labels of attributes. But their distributions in PRD space may be far away indicating that the model made wrong predictions. Similarly, departing images in FEA space shows that the neural network finds different high-level features in these images. Then other views provide tools for users to further study these findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Image Exploration from Attributes</head><p>An alternative exploration path is to study the images from their attributes. First, users can interactively select any combinations among all attributes in the attribute panel. Then an attribute set view shows the model performance of the images with these attributes. In <ref type="figure" target="#fig_1">Fig.  3</ref>, "Many rings", "Ring" and "Strong scattering" are selected as an example. The bottom row shows there are 34 images that are correctly classified (by the dark dots). The top row indicated that all the three attributes are not correctly classified in 5 images. Users can click any row so that all the images in the corresponding set are selected for further study.</p><p>Users can further discover image instances based on co-existence metrics. We visualize these metrics in interactive color-enhanced matrices. For both Pearson correlation and mutual information, the generated matrix is symmetrical since changing the order of two attributes will not affect the dependence result. To save space, we use half of a matrix to visualize one relationship of two attributes, as shown in the top of <ref type="figure" target="#fig_2">Fig.4</ref> where the green and blue triangles form a full matrix. The green triangle represents the correlation of two attributes in the true space, while the blue triangle shows the correlation in the prediction space. In addition, the values in the diagonal cells are computed with one attribute from the true space and the other from the prediction space and colored in orange. A high-value cell in the matrix indicates a strong correlation. Users can identify interesting pairs of attributes in one triangle, while the same pairs in the other triangle are also highlighted. This design helps users evaluate the model performance by comparing the relationship between the two attributes in two spaces.</p><p>For conditional entropy, the order of the two attributes matters. A full matrix is used for the attributes in either true space or prediction space. The horizontal attributes are conditional attributes, and the vertical ones are the uncertain attributes. We show the matrix for the true space at the bottom of <ref type="figure" target="#fig_2">Fig.4</ref>. For example, zero value cells of the matrix indicate the two attributes are either simultaneously existing (e.g., high orders and many rings) or are mutually excluded (e.g., circular beamstop and linear beamstop).</p><p>In addition, <ref type="figure" target="#fig_3">Fig. 5</ref> is the table view of multi-attributes. Here users can choose a different number of attributes and three attributes are shown in this figure. The top three attributes can be ranked (with user interaction) by the total number of images with them or the number of correctly predicted images. Users can select any group of images for further study.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Image Group Exploration and Comparison</head><p>While the groups of images are selected, they can be drilled down for the characteristic measures as shown in <ref type="figure" target="#fig_4">Fig. 6</ref>, where two parallel coordinates plots (PCP) are used to show the measures of each attribute inside one group. On the left, the attributes that exist in most images are shown on the top. Users can hover over each attribute to highlight the polyline, while the corresponding images are highlighted in the embedded space views as well.</p><p>The selection of images can also be visualized in a gallery with thumbnail views, as shown in <ref type="figure">Fig. 8</ref>. Here, the images are grouped by the number of attributes they include (in ACT or PRD). It is easy for users to identify outliers that may be wrongly labeled of attributes.</p><p>Moreover, these images can be displayed in a cluster view as shown in <ref type="figure" target="#fig_0">Fig. 1(D1-D2)</ref>. The motivation of using the clustering methods here is to help users discover sub-groups of images so that the data distribution patterns can be studied and compared easily. There are difficulties in finding the absolute optimal solution of clustering <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13]</ref> due to different distance metrics and user preference. Therefore, two methods, K-Means as a centroid-based clustering and DBSCAN as a density-based clustering, are provided so that they have the flexibility to interactively find good clustering results for drill-down study. DBSCAN can find clusters with respect to spatial distribution while K-Means favors globular clusters. To help them in verifying the clustering, two unsupervised clustering validation scores, Silhouette Coefficient <ref type="bibr" target="#b24">[25]</ref> and Davies-Bouldin score <ref type="bibr" target="#b5">[6]</ref>, are computed and visualized in real time. For example, <ref type="figure">Fig. 1(D1)</ref> shows two spatial clusters generated by applying DBSCAN to the blue image points in PRD space in <ref type="figure" target="#fig_1">Fig.  1(C3)</ref>. This clustered view provides a more clear depiction and easier interaction of the images. Users can control the tuning parameters, "EPS" and "MinPts" for DBSCAN to generate a good clustering result based on the validation sores. Similarly, in K-Means, the number of clusters can be tuned directly. Moreover, users can cluster the same selection of images according to the different latent spaces in <ref type="figure" target="#fig_0">Fig. 1(D1  and D2)</ref>. For example, in contrast with the clusters in PRD space in <ref type="figure">Fig. 1(D1)</ref>, the same selection of images highlighted by the blue image points were dragged into <ref type="figure" target="#fig_0">Fig. 1(D2)</ref> where it shows the two clusters by K-Means method in FEA space and matches with the points distribution in <ref type="figure" target="#fig_0">Fig.1(C2)</ref>.</p><p>In addition, an attribute flower visualization <ref type="figure" target="#fig_5">(Fig. 7)</ref> is designed (based on Astor charts) to show each attribute as a "petal". Each petal is filled in the corresponding color of one selected attribute if the image has this attribute in its ACT vector. The petal has a black border if this attribute does not exist in the ACT vector but appears in its PRD vector. The missing petals simply indicate true negative prediction. Therefore, FN, FP, TP and TN attributes can be easily discerned for each image and for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CASE STUDIES</head><p>We have conducted several case studies with two domain experts in physical and material sciences who have applied DL for x-ray data analysis and one graduate student who has worked on ResNet model development for x-ray data. They operated our Web-based system remotely through web browsers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Case 1: Studying model performance from an ACT im-</head><p>age group <ref type="figure">Fig. 1</ref> shows an image group (in blue dots) selected from ACT space, based on its high prediction error rate (the dots with higher opacity). These images are the most problematic candidates. By putting this selection into the group panel, <ref type="figure" target="#fig_4">Fig. 6</ref> indicates that a structural attribute "Linear beamstop" is the most important in this group. Switching to the clustering view in <ref type="figure">Fig. 1(D1)</ref>, the images form two spatial clusters in PRD space. The red petal of the attribute flowers indicates "Linear beamstop". In the bottom cluster, images with this attribute are predicted correctly such as Image 54, 62, and 139. From the detail view ( <ref type="figure">Fig.1(E)</ref>), the three images are very different (Image 62 has a blue background and needle shape under the beam) while the learning model gives a correct prediction on this attribute. However, most images in the top cluster of <ref type="figure">Fig. 1(D1)</ref> are wrongly classified. For example, Images 44, 38, and 4 fail the detection of linear beamstop. Instead, they are detected as other types of beam stops: "Wedge Beamstop" or "Circular Beamstop". Referring to <ref type="figure" target="#fig_2">Fig. 4</ref>, the coefficients of the correlation matrix between any pair of these three different beamstops are negative, telling they are negatively dependent and exclusive to each other in an image. The values of conditional entropy between them in the green true space are zero, which further verifies they are exclusionary. If users switch to the conditional entropy in the predication space from the drop-down box, they will find that the conditional entropy values for these pairs are non-zero. Thus, it shows the model cannot tell the differences of the three different beamstop attributes.</p><p>By further putting this group into <ref type="figure" target="#fig_0">Fig. 1(D2)</ref> and using FEA space instead with 2 spatial clusters, Images 44, 38, 4, and 139 are close together. The convolutional part of the network finds they have similar high-level image features. But the fully connected classifier may be confused and give different predictions of beam stop types. This shows that the x-ray scattering attributes of different types of beam stops are quite hard to detect due to the small size and especially with a dark black background. Therefore, the visualization system helps scientists recognize the weak point of a learning model, which may be addressed by further extending the convolutional layers and training the model with more images to distinguish them in the feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Case 2: Studying model behavior with three coexistence attributes</head><p>From the 3-attribute co-existence table of <ref type="figure" target="#fig_3">Fig. 5</ref>, three attributes of interest are identified: "Many rings", "Ring", and "Strong scattering". These attributes are highly correlated and scientists want to confirm how the network distinguishes them. In the dataset, there are totally 54 images sharing the attributes which are correctly detected in 34 images.</p><p>By selecting these attributes, from the attribute set view of <ref type="figure" target="#fig_1">Fig. 3</ref>, users can identify different combinations and find the model output of them. For example, two combinations are selected to groups of interest: (G1) six images where "Many rings" are not detected; (G2) three images where "Strong scattering" are not detected. The two selections are put into the group panel for comparison, as shown in <ref type="figure">Fig. 8</ref>. Here the thumbnail galleries of them are shown. Note that all the images have the attribute "Ring" detected correctly while their visual appearances are quite diverse, where the high variation of x-ray scattering structures are manifested.</p><p>In the embedded view of FEA space, G1 images are shown in blue dots and G2 images are in green dots (the color can be selected by users flexibly). It can be seen that G2 images are far away from each other. But Image 77 in G2 are very close to Images 80 and 296 in G1 (see the pink circle). When opened in the detail view, the three images have similar features of rings whose center roughly lies on edges and corners. The model detects many rings in Image 77 but not in Image 296. It helps scientists to identify that the model does not perform as expected for "Many rings" due to the fact that the rings in Image 296 are not as complete as in Image 77. Here, since high-level FEA features are extracted correctly, to improve the model, the fully connected layers may be replaced with other classifiers (e.g., Support-Vector Machine).</p><p>In G1 images, 183, 112, and 132 are close in FEA space (see the blue circle). In the detail view (first three images), they have specular backgrounds with strong scattering (which is related with Bragg angle of light beams). The "many rings" structure is quite hard to discern from low-level pixel-based metrics. This type of "Many rings" may need to be labeled in a separate way, which suggests scientists to use a refined attribute list that sufficiently describes the image structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Case 3: Studying the pre-trained model error</head><p>By exploring the co-existence matrices in <ref type="figure" target="#fig_2">Fig. 4</ref>, we notice that two attributes "BCC" (body-centered cubic) and "FCC" (face-centered cubic) are presented as anomalies. For example, in the correlation matrix (the top one in the figure), the values paired with "BCC" in the prediction space are zero, which are inconsistent with the values in the true space. The other attribute "FCC" is similar. Moreover, the conditional entropy in the true space (the bottom matrix in the figure) for the two attributes present normal relationships with other attributes. Both observations indicate the predictions by the pre-trained model are erroneous for these two attributes.</p><p>As shown in <ref type="figure" target="#fig_6">Fig. 9</ref>, we study the two attributes by filtering out all other labels in the attributes list. In the ACT space, three clusters are formed that are highlighted in blue, purple, and green (highlighted in the orange box), correspondingly. They represent the image clusters with only "BCC", only "FCC", and both attributes, respectively. These two attributes describe the feature of crystal structures. Typically, since the atoms in a unit cell can only form one structure, the two structures are mutually exclusive to each other. But some material samples can have two structure units superimposed. The six abnormal image points, shown as a small group in the orange box of ACT space, are identified. Next, we examine the two dense clusters in blue and purple colors in ACT space to learn more details. These two attributes cannot be separated well in both FEA and PRD spaces. As shown in <ref type="figure" target="#fig_6">Fig. 9</ref>, the image points with purple and blue colors are mixed in the two embedded visualizations.</p><p>To find out the reason, we check the performance of the two groups by looking into the parallel coordinates in the group panel (in the middle of <ref type="figure" target="#fig_6">Fig. 9</ref>). First, Selection 1 (the blue cluster in ACT space) shows that all 247 images in this selection with attribute "BCC" have zero true positives and zero true negatives. The accuracy, precision, and recall are all zeros and the F1 score shows NaN for "BCC". It means the model is unable to predict the "BCC" attribute. At the same time, we find the model does not confuse the "BCC" attribute to "FCC", since this group has all images predicted true negative for "FCC". So it has a good accuracy value 1 for "FCC" prediction. In Selection 2, the situation for attribute "BCC" and "FCC" in parallel coordinate curves is the same as in Selection 1. Both of these indicate that the ResNet model does not successfully classify these two attributes. The possible reason <ref type="figure">Fig. 8</ref>. Studying two groups of images where attributes "Many rings", "Ring", and "Strong scattering" co-exist. (G1) "Many rings" not detected; (G2) "Strong scattering" not detected. The image galleries and the FEA space are shown, together with the detail view of several images. could be: 1) insufficient data labeled with these attributes in the training set; 2) the feature of these attributes are too similar to distinguish; or 3) they are affected and overwritten by other strong signal attributes.</p><p>To further check the reason, we click on some of the images from the two groups to observe their original x-ray images, as shown in the image view at the bottom of <ref type="figure" target="#fig_6">Fig. 9</ref>. The "BCC" and "FCC" attributes can be found by domain scientists on the colorful spots with periodical intensity variation on the rings. They are not suppressed by other attributes like "many rings". We also find that a quite small number of images with these two attributes were used in model training. Finally, we check the model implementation and identify an error in the data processing function that learned incorrect names for the two attributes. This example shows that the visualization system helps users discover mistakes or errors in modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATION AND DISCUSSION</head><p>The visualization system was evaluated in two stages. One is to get feedback from domain scientists about the usability, limitation, and suggestions of the system. The other is to evaluate the visual interface functions. We also discuss the system from multiple aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Interview with Domain Scientists</head><p>We conducted an interview with the two domain scientists who have contributed to the case studies. One of the scientists is a co-author. They were given a thorough introduction to the system and its functions. Then, they practiced freely with the system for the investigation of the x-ray scattering data and performed the following study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">A Usage Scenario:</head><p>Domain experts found two groups with images that show big "ring" and small "ring" on FEA space. In the semantic PRD space and ACT space, these two groups are supposed to indicate the same feature. However, they are separated in the FEA space, which means the neural network takes the scale of the attribute "ring" into account significantly. This is an interesting point found by experts. It means that in the FEA space, the clusters are formed by the data points visually close to each other rather than semantically. Thus, the function of the fully connected layers is to collect the visual features from a widely distributed area in FEA for classification. Since ResNet50 contains only a single fully connected layer to do this work, the domain experts expected that more fully connected layers may be added in the end of the classification phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Feedback on Usability</head><p>Based on the experience with the usage scenario, the domain scientists provided feedback on the system usability in research: Good points:</p><p>• About system interface: This interface allows users to explore the training data and the trained model side-by-side. The tool is very easy to explore and identify the issue of the model. • About interlinked embedded views: The linking of data between embedded views is extremely important and well-executed. This allows users to follow a chain of logic from one representation to another to study model performance. • About outliers and hypotheses: The system allows users to explore and search for outliers, as well as to test hypotheses by grouping data and performing analytic tests. This kind of interaction is extremely useful for being able to then refine the training model. • About model behavior: Perhaps, more importantly, this tool allowed users to identify limitations and biases in the training data itself. Thus, this tool is a useful guide towards improving training datasets. Limitation and suggestion:</p><p>• About network layers: Only the feature space after all convolutional layers is used, it would be good to further show the study of the feature maps from other layers in the deep network. • About embedded space control: For embedded spaces, it is very useful to see how data is organized. However, because these spaces are transformed from the high dimensional spaces, it would be nice to have more control over how one is viewing these embedded spaces, such as using alternate layout modes, ability to rotate the view, etc. • About group study: For group study panel, it requires some training to know how to use them and interpret the information. Perhaps some additional feedback to users could make these tools more immediately understandable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">Feedback on System Impact</head><p>The domain scientists agree that this visualization tool provides an easy way to explore tagged datasets from the output of a trained machinelearning model. The primary advancement herein is providing accessibility of this meta-data to the domain expert. In their current workflows, the domain experts search meta-data in an ad hoc manner using database-like interfaces. They have no ability to search or visualize the tagging outputs of machine-learning models, other than to evaluate statistical metrics such as average precision. The presented tool thus provides a way for domain experts to easily study the machine-learning annotations of their data. This allows them to build confidence in their machine-learning models, to direct improvements in those models, and (eventually) as a way to browse their data looking for interesting metadata correlations that would be hard to otherwise see. In a conventional experiment, the researchers collect large amounts of data in the model, then manually search these data for both expected patterns (hypothesis testing) and unexpected patterns (exploration). Once a pattern is identified, a detailed data analysis is performed in order to highlight and explain it. The proposed visualization tool dramatically alters the second step (searching data for trends) by affording the opportunity for the domain expert to identify interesting and unexpected relationships between meta-data attributes. The tool also provides a convenient interface for summarizing the results of a given experiment, and thus allows data browsing in a more convenient manner as compared to conventional workflows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Evaluation for Visual Interactions</head><p>Participants: Eleven graduate students who are computer science majors (4 females, 7 males) participated in the evaluation of the visualization system. All the participants had knowledge of visualization and interface design and experience in web-based visualization tools. Six of them have experiences of deep learning models and tools.</p><p>Procedures: An instructor first introduced the x-ray scattering images and structural attributes, and then the visualization and interaction functions to each participant. They were guided to freely explore the system with image data for about 10 minutes. Then, they completed the tasks: (a) find outliers in an image group; (b) discover the model behavior of attributes in the image groups. Then, they provided the evaluation by filling a QUIS (Questionnaire for User Interaction Satisfaction) form <ref type="bibr" target="#b3">[4]</ref>. Finally, they gave comments and suggestions for the system. QUIS evaluations: <ref type="table" target="#tab_1">Table 1</ref> shows the questions and ratings in two parts about 1) The visualization design and functions, and 2) The system performance. The mean and standard errors of the user ratings are displayed as well. The average score of the visualization and interaction functions are very good above 8.0, which indicates the users were satisfied overall. For example, the interactive operations on the embedded spaces like "Zooming and Panning" and "Free selection" received excellent rating with an average of about 8.91. "Multiple spaces comparison and drill down study" also got a high average score, 8.82. It indicated that the interaction in exploring data in the embedded spaces was well implemented. The ratings of the visualization system are also very good. The Web-based system performance with fast response impressed the users. They also felt the system layout and organization was easy to follow. The average rating scores for "Learning to operate the system" and "Designed for all levels of users" were good but relatively lower at 7.18 and 7.82. The standard deviations of these two were also high at 1.17 and 1.32. This is reasonable as the users need to understand the deep learning model performance over the scientific images. In future work, we will add more labels and guidance in the system to shorten the learning curve.</p><p>The users' comments also identified system limitations to be further addressed. For example, visual clutters appear when the image labels of a large group of selected images are shown in the embedded spaces. Here, a smart labeling algorithm is needed. In addition, the selected groups are fixed and more flexibility may be added to insert and delete specific images to/from a group. Moreover, the color selection for different attributes should be adjustable by user-definition. We will further improve the system according to these suggestions. Reading labels and icons. 0(very hard)-9(very easy) 8.10 1.04 Highlighting selected focuses. 0(not at all)-9(very much) 8.27 0.65 Information organization. 0(confusing)-9(very clear) 8.45 0.69 Sequential operations. 0(confusing)-9(very clear) 8.18 0.98 Interactions. 0(very hard)-9(very easy) 8.23 0.79 learning Learning to operate the system. 0(difficult)-9(easy) 7.18 1.17 Performing tasks is straightforward. 0(never)-9(always) 8.09 0.83 System System response speed. 0(very slow)-9(fast enough) 8.36 0.81 Designed for all levels of users. 0(never)-9(always) 7.82 1.32 System reliability. 0(unreliable)-9(very reliable) 8.18 0.75</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Discussion</head><p>Based on our design, implementation, and evaluation of the visualization system, we discuss our approach in several aspects. Complexity and Scalability The visualization system works on about 1,000 images in the study. The computational complexity is mainly determined by the t-SNE algorithm that projects them from high dimensions to 2D, which however only needs to be performed once when the dataset is loaded into the system. The clustering methods are only applied to selected groups of images for an interactive study so that the computational performance is not a problem for real time responses. Visualizing thousands of data points and the selected clusters can be executed very fast, so those smooth interactions are easily supported. Therefore, from the computational aspect, we expect the system can be scaled to a larger set of thousands of images easily. The visual cluttering issue may arise when more points are injected into the canvas. The system supports zooming and scaling for users to investigate data points. However, the capability of effective exploration may be hindered due to the increasing scale of loaded images.</p><p>Transferability Our system is built up on the input of FEA, PRD, and ACT vectors for multiple attributes image datasets. Therefore, upon the availability of these vector representations of data items from a trained model, our approach can be transferred to other contexts or settings. Moreover, the system has also been extended to natural image datasets (e.g., CIFAR10 <ref type="bibr" target="#b14">[15]</ref>) with single attribute classification. It can help diagnose the reasons for wrongly predicted images. Limitation Overlapped transparent dots (Sec. 4.2) may form misleading error rate. When users select them and study the details they can find the facts. Interactive lens and/or jittering tools may help solve the visual cluttering issue. In addition, the direct use of existing scientific categories of attributes in deep learning models may not be very effective. For example, "ring" may be further divided into different types as found in the case study. Second, the training data based on the labeling of domain scientists often suffer from more mislabels and errors than natural images. An effective labeling tool may be developed to improve the accuracy and effectiveness of the labeling process itself. Moreover, the system is built up on a trained model and datasets. It may be integrated into the training process. It will also be extended to study new incoming images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION AND FUTURE WORK</head><p>We present a visualization system for understanding the learning model of x-ray scattering images with multiple attributes. The system allows users to visually discover the embedded distributions of feature vectors, predictions, and actual labels of these images. User interactions are supported to compare selected image instances and study their prediction results related to the attributes. In future work, we will address the limitations and extend the work for model debugging and refinement, by taking neurons and different network layers into account.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>(A) X-ray scattering images (1-10) including 17 attributes (see details in Appendix). (B) ResNet50 architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>An attribute set view in the attribute panel showing the numbers of images with different attribute combinations. • For T3: Attribute Co-existence Visualization: The model performance with the relations of co-existing attributes is visualized in an interactive view. Users can then define image groups based on the visual cues of model performance. • For T4. Group Comparative View and Image Comparison: The selected groups can be easily investigated and compared with group panels for detailed views. Through interactive selection over all the above visualizations, users can also open multiple images to compare their details of raw data and model predictions. Fig. 1 shows the visualization system interface. It displays the scientific images in the coordinated views (C1-3) of three different embedded spaces (ACT, FEA, PRD)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Top: Matrix view of pairwise attribute correlation. The green triangle represents the correlation of two attributes in True space, while the blue triangle shows the correlation in Prediction space. Bottom: Matrix view of conditional entropy in True space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>A co-existence table view of multiple attributes. It shows the number of images having the attribute combination in ACT space (Number), and the number of correctly predicted images (CorNum).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Two parallel coordinates plots showing attribute measures of model performance with various metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>The attribute flower visualization where each petal represents one attribute. A blank petal indicates FN, a solid petal indicates TP, a missing petal indicates TN, and a solid petal with a black border means FP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>Debugging the issue of the pre-trained ResNet model for two attributes BCC and FCC through the entire visualization elements.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>are with Kent State University (Email: xhuang, sjamonna, zhao@cs.kent.edu). • B. Wang, M. Hoai are with Stony Brook University (Email: boywang, minhhoai@cs.stonybrook.edu). • K. Yager, W. Xu are with Brookhaven National Laboratory (Email: kyager, xuw@bnl.gov).</figDesc><table /><note>Manuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>QUIS questions and ratings</figDesc><table><row><cell>Questions</cell><cell>Means</cell><cell>Standard</cell></row><row><cell></cell><cell></cell><cell>Deviation</cell></row><row><cell>Part I: Visual Tools Questions: 0(poor) -9(excellent)</cell><cell></cell><cell></cell></row><row><cell>Coordinated Visualization with Embedded Spaces</cell><cell></cell><cell></cell></row><row><cell>Embedded views for showing image groups and outliers.</cell><cell>8.45</cell><cell>0.52</cell></row><row><cell>Zooming and panning operations.</cell><cell>8.91</cell><cell>0.30</cell></row><row><cell>Multiple spaces comparison and drill down study.</cell><cell>8.82</cell><cell>0.40</cell></row><row><cell>Image item transparency showing errors.</cell><cell>8.27</cell><cell>0.79</cell></row><row><cell>Image item labeling.</cell><cell>8.36</cell><cell>0.67</cell></row><row><cell>Image Group Selection</cell><cell></cell><cell></cell></row><row><cell>Free selection on embedded spaces.</cell><cell>8.91</cell><cell>0.30</cell></row><row><cell>Image group management.</cell><cell>8.64</cell><cell>0.50</cell></row><row><cell>Image group coloring function.</cell><cell>8.82</cell><cell>0.40</cell></row><row><cell>Image Group Visualization</cell><cell></cell><cell></cell></row><row><cell>Group panel drag and drop.</cell><cell>8.73</cell><cell>0.64</cell></row><row><cell>Group measures with PCP.</cell><cell>8.27</cell><cell>1.27</cell></row><row><cell>Group thumbnail gallery overview.</cell><cell>8.63</cell><cell>0.50</cell></row><row><cell>Group spatial clustering.</cell><cell>8.73</cell><cell>0.47</cell></row><row><cell>Image attribute flower view.</cell><cell>8.73</cell><cell>0.64</cell></row><row><cell>Attribute Co-existence Visualization</cell><cell></cell><cell></cell></row><row><cell>Attribute list with interaction.</cell><cell>8.64</cell><cell>0.50</cell></row><row><cell>Attribute set view and group selection.</cell><cell>8.36</cell><cell>0.57</cell></row><row><cell>Attribute co-existence table view.</cell><cell>8.63</cell><cell>0.57</cell></row><row><cell>Detail View Image Visualization</cell><cell></cell><cell></cell></row><row><cell>Image visual comparison.</cell><cell>8.64</cell><cell>0.67</cell></row><row><cell>Image ACT and PRD value comparison.</cell><cell>8.45</cell><cell>0.52</cell></row><row><cell>PartII. Visualization system rating</cell><cell></cell><cell></cell></row><row><cell>Interface</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors wish to thank the anonymous reviewers. This work was partly supported by KSU graduate assistantship, BNL LDRD grant 18-009 and ECP CODAR project 17-SC-20-SC.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A causal framework for explaining the predictions of black-box sequence-to-sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Alvarez-Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1042</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="412" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2016.2644615</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A dendrite method for cluster analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Caliński</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harabasz</surname></persName>
		</author>
		<idno>doi: 10.1080/ 03610927408827101</idno>
	</analytic>
	<monogr>
		<title level="j">Communications in Statistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Development of an instrument measuring user satisfaction of the human-computer interface</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Diehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Norman</surname></persName>
		</author>
		<idno type="DOI">10.1145/57167.57203</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGCHI</title>
		<meeting>of SIGCHI</meeting>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="page" from="213" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Visual Analytics for Explainable Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<idno>doi: 10. 1109/MCG.2018.042731661</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="84" to="92" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A cluster separation measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Bouldin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="224" to="227" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic X-ray Scattering Image Annotation via Double-View Fourier-Bessel Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dantong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">245</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kalro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H P</forename><surname>Chau</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2017.2744718</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Understanding Complex Deep Generative Models using Interactive Visual Experimentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H P</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gan Lab</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2018.2864500</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="310" to="320" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<title level="m">ConvNetJS: Deep Learning in your browser</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Materials discovery: Fine-grained classification of X-ray scattering images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Kiapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<idno type="DOI">10.1109/WACV.2014.6836004</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="933" to="940" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An impossibility theorem for clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on Neural Information Processing Systems, NIPS&apos;02</title>
		<meeting>the 15th International Conference on Neural Information Processing Systems, NIPS&apos;02<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="463" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Understanding Black-box Predictions via Influence Functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">nternational Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Cifar-10 (canadian institute for advanced research)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Analyzing the Training Processes of Deep Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2017.2744938</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="87" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Towards Better Analysis of Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2016.2598831</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="100" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for grazing incidence x-ray scattering patterns: Thin film structure identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Melton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Pandolfi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Freychet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hexemer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Ushizima</surname></persName>
		</author>
		<idno type="DOI">10.1557/mrc.2019.26</idno>
	</analytic>
	<monogr>
		<title level="j">MRS Communications</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Classification of crystal structure using a convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Sohn</surname></persName>
		</author>
		<idno type="DOI">10.1107/S205225251700714X</idno>
	</analytic>
	<monogr>
		<title level="j">IUCrJ</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="486" to="494" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deepeyes: Progressive visual analytics for designing deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pezzotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Höllt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P F</forename><surname>Lelieveldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eisemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vilanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="108" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visualizing the Hidden Activity of Artificial Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Fadel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Falcão</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Telea</surname></persName>
		</author>
		<idno>doi: 10. 1109/TVCG.2016.2598838</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="101" to="110" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visualizing time-dependent data using dynamic t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Falcão</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Telea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eurographics / IEEE VGTC Conference on Visualization: Short Papers, EuroVis &apos;16</title>
		<meeting>the Eurographics / IEEE VGTC Conference on Visualization: Short Papers, EuroVis &apos;16<address><addrLine>Goslar, DEU</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="73" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.690</idno>
		<title level="m">YOLO9000: Better, faster, stronger. Proceedings -30th IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Why Should I Trust You?&quot;: Explaining the Predictions of Any Classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-3020</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="97" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Silhouettes: A graphical aid to the interpretation and validation of cluster analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Rousseeuw</surname></persName>
		</author>
		<idno type="DOI">10.1016/0377-0427(87)90125-7</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="53" to="65" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Grad-cam: Why did you say that? visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Revista do Hospital das Cl??nicas</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="331" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradientbased localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Directmanipulation visualization of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Smilkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<idno>abs/1708.03788</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Embedding projector: Interactive visualization and interpretation of embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Smilkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nicholson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">BraggNet: integrating Bragg peaks using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Archibald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Azadmanesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Vandavasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Langan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Langan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Cryst</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="854" to="863" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Synergy, redundancy, and multivariate information measures: An experimentalist&apos;s perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Timme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Alford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Flecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Beggs</surname></persName>
		</author>
		<idno>doi: 10. 1007/s10827-013-0458-4</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Neuroscience</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="140" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">X-Ray scattering image classification using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<idno type="DOI">10.1109/WACV.2017.83</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="697" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">GANViz: A Visual Analytics Approach to Understand the Adversarial Game</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Shen</surname></persName>
		</author>
		<idno>doi: 10. 1109/TVCG.2018.2816223</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1905" to="1917" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visualizing Dataflow Graphs of Deep Learning Models in TensorFlow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wongsuphasawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Smilkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<idno>doi: 10.1109/ TVCG.2017.2744878</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dataset of Synthetic X-ray Scattering Images for Classification Using Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Yager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lhermitte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://pe" />
	</analytic>
	<monogr>
		<title level="j">Material Data Facility</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Periodic lattices of arbitrary nano-objects: Modeling and applications for self-assembled systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Yager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gang</surname></persName>
		</author>
		<idno>doi: 10.1107/ S160057671302832X</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Crystallography</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="118" to="129" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<title level="m">Understanding Neural Networks Through Deep Visualization. ICML Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Object Detection With Deep Learning: A Review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2018.2876865</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mueller</surname></persName>
		</author>
		<title level="m">Evolutionary Visual Analysis of Deep Neural Networks. International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
