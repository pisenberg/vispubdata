<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">V2V: A Deep Learning Approach to Variable-to-Variable Selection and Translation for Multivariate Time-Varying Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Han</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhao</forename><surname>Xing</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Danny</forename><forename type="middle">Z</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoli</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">V2V: A Deep Learning Approach to Variable-to-Variable Selection and Translation for Multivariate Time-Varying Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multivariate time-varying data</term>
					<term>variable selection and translation</term>
					<term>generative adversarial network</term>
					<term>data extrapolation</term>
				</keywords>
			</textClass>
			<abstract>
				<p>We present V2V, a novel deep learning framework, as a general-purpose solution to the variable-to-variable (V2V) selection and translation problem for multivariate time-varying data (MTVD) analysis and visualization. V2V leverages a representation learning algorithm to identify transferable variables and utilizes Kullback-Leibler divergence to determine the source and target variables. It then uses a generative adversarial network (GAN) to learn the mapping from the source variable to the target variable via the adversarial, volumetric, and feature losses. V2V takes the pairs of time steps of the source and target variable as input for training, Once trained, it can infer unseen time steps of the target variable given the corresponding time steps of the source variable. Several multivariate time-varying data sets of different characteristics are used to demonstrate the effectiveness of V2V, both quantitatively and qualitatively. We compare V2V against histogram matching and two other deep learning solutions (Pix2Pix and CycleGAN).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>To understand various physical and natural phenomena, scientists produce multivariate time-varying data (MTVD) from large-scale scientific simulations. For large-scale simulations, the limited I/O bandwidths cannot match the rate of data production. In most cases, scientists could only afford to sparsely store the outputs for post hoc analysis and visualization. In this paper, we focus on the variable-tovariable (V2V) translation as an extrapolation task for MTVD. That is, given a variable sequence, for example, variable MF of the combustion data set, we aim to generate another variable sequence, for example, variable YOH of the same data set.</p><p>For scientific applications, it is meaningful to generate one variable sequence conditioned on another variable sequence because of the following reasons. First, scientists often simulate a large number of ensemble runs for generating multiple MTVD sequences but are only allowed to store a fraction of these sequences. Our solution allows saving one ensemble run entirely (i.e., all the time steps) while sparsely sampling the rest of runs (e.g., only the early time steps) as they can be faithfully recovered later on. In this way, scientists can save more runs, supporting a more accurate examination of the dynamic features of MTVD. Second, through V2V translation, scientists can discover the relationships among different variables and focus on variables of interest during post hoc investigation. As variable sequences depend on each other, studying the difficulty of transferring one sequence to another will shed new light on MTVD analysis and visualization.</p><p>Translating one variable sequence to another variable sequence poses four main challenges. First, understanding the relationships among different variables in MTVD is critical for V2V translation. Choosing two arbitrary variables for translation could lead to unexpected results since the randomly chosen variables may exhibit dramatically different patterns. Therefore, variable selection should be considered so that high-quality V2V translation can be achieved. Second, once the transferable variables are determined, choosing the appropriate source and target variables is still crucial since the translation difficulty could vary given a different variable as input. Third, unlike volume temporal and spatial super-resolution tasks that aim to interpolate data through their neighborhood information, V2V translation</p><p>â€¢ J. <ref type="bibr">Han</ref> performs extrapolation instead of interpolation, which is a much more difficult task. Fourth, both global and local information must be considered simultaneously as multivariate temporal patterns in different regions are non-linear and non-uniform. Assuming the translation is local and linear may not ensure acceptable results: we may produce blurred features, resulting in fewer details in the visualization (i.e., direct volume rendering and isosurface rendering).</p><p>To tackle these challenges, we propose a novel solution for addressing the V2V translation problem for MTVD analysis and visualization, inspired by image-to-image translation tasks and representation learning techniques. V2V is a comprehensive framework for selecting transferable variables and synthesizing variable sequences. We leverage generative adversarial networks (GANs) to learn the variable mapping non-linearly and non-locally. Our solution consists of three stages: feature learning (aiming to find the relationships among different variables for MTVD), translation graph construction (aiming to detect the source and target variables), and variable translation (aiming to learn a mapping function from one variable to another variable). The training data could be obtained at earlier time steps from the two variable sequences. During inference, V2V can synthesize a variable sequence conditioned on another variable at later time steps. Quantitative and qualitative results with several data sets with different characteristics demonstrate the effectiveness of V2V. Also, we compare V2V against three other solutions: histogram matching <ref type="bibr" target="#b34">[35]</ref>, Pix2Pix <ref type="bibr" target="#b18">[19]</ref>, and CycleGAN <ref type="bibr" target="#b46">[47]</ref>. Our results indicate that V2V achieves better quality using the data-, image-, and feature-level quality measures.</p><p>We summarize our contributions as follows. First, V2V is the first work in scientific visualization that applies deep learning techniques for variable selection and translation. Second, we propose a new architecture for the V2V translation task, which is different from the ones often used in the image-to-image translation task. Third, we conduct a thorough experiment to investigate how variable selection results could impact variable translation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Multivariate relationships. Researchers have studied point-wise correlation coefficients <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b2">3]</ref> and gradient similarity measure <ref type="bibr" target="#b35">[36]</ref>. Wang et al. <ref type="bibr" target="#b40">[41]</ref> studied the information flow between variable pairs using transfer entropy for investigating variable causal relationships. Biswas et al. <ref type="bibr" target="#b1">[2]</ref> classified variables using surprise and predictability derived from information theory and leveraged a graph-based representation for variable exploration. Liu et al. <ref type="bibr" target="#b25">[26]</ref> designed the probabilistic association graph based on the informativeness and uniqueness concepts to uncover the hidden associations between different variables. Tao et al. <ref type="bibr" target="#b38">[39]</ref> considered isosurface similarities across the time and variable dimensions for time-varying multivariate data and designed the matrix of isosurface similarity map for visual exploration.</p><p>Unlike these works, we investigate variable relationships using their latent features learned from a neural network and select variable pairs suitable for subsequent translation study.</p><p>Deep learning for scientific visualization. As deep learning solutions have solved a series of problems in image classification, image super-resolution, and image generation, researchers have recently attempted to explore the use of deep learning in solving scientific visualization problems. Examples include super-resolution generation in the spatial <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b8">9]</ref>, temporal <ref type="bibr" target="#b12">[13]</ref>, and image <ref type="bibr" target="#b41">[42]</ref> domains, volume rendering pipeline replacement <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b36">37]</ref>, data reconstruction <ref type="bibr" target="#b10">[11]</ref>, workload balancing <ref type="bibr" target="#b17">[18]</ref>, and ensemble parameter space exploration <ref type="bibr" target="#b15">[16]</ref>. Even though interpolation tasks have been investigated <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b8">9]</ref>, in the context of MTVD extrapolation, no work has been done that synthesizes a variable sequence conditioned on another sequence, which is accomplished by this work.</p><p>Representation learning. Representation learning is a focused goal of many deep learning solutions. For example, Girdhar et al. <ref type="bibr" target="#b6">[7]</ref> utilized an autoencoder to learn representative features of 3D objects for producing novel 3D objects and the corresponding 2D images. Chen et al. <ref type="bibr" target="#b3">[4]</ref> designed LassoNet that attempts to learn a latent mapping from viewpoint and lasso to point cloud regions for lasso selection of 3D point clouds. Han et al. <ref type="bibr" target="#b9">[10]</ref> proposed FlowNet, an autoencoder that learns the latent representations of streamlines and stream surfaces for dimensionality reduction and representative selection. Porter et al. <ref type="bibr" target="#b30">[31]</ref> established a CNN to select representative time steps for time-varying multivariate data. Instead of selecting representative data or patches, V2V aims to find relationships among different variables in MTVD. Moreover, the learned relationships can guide us in choosing transferable variables for V2V translation.</p><p>Paired image-to-image translation. Deep learning solutions have achieved great success in image-to-image translation tasks, such as super-resolution and colorization. For image super-resolution, Dong et al. <ref type="bibr" target="#b5">[6]</ref> proposed a CNN that learns the mapping from low-resolution images to high-resolution images. Johnson et al. <ref type="bibr" target="#b19">[20]</ref> designed a CNN that simultaneously processes image style transfer and superresolution tasks by minimizing content and style losses. Ledig et al. <ref type="bibr" target="#b21">[22]</ref> presented a GAN for inferring photo-realistic high-resolution images from low-resolution images via optimizing adversarial and perceptual losses. For image colorization, Zhang et al. <ref type="bibr" target="#b43">[44]</ref> designed a deep learning solution that produces vibrant and realistic colorful images conditioned on gray-scale images. Zhang et al. <ref type="bibr" target="#b44">[45]</ref> established a CNN which directly maps a gray-scale image, along with sparse, local user "hints" to an output colorization and propagates user edits. Isola et al. <ref type="bibr" target="#b18">[19]</ref> utilized conditional GANs for studying various image-to-image translation problems, such as aerial-to-map, day-tonight, and edge-to-photo. Park et al. <ref type="bibr" target="#b28">[29]</ref> proposed a spatially-adaptive normalization layer for synthesizing photo-realistic images based on a semantic layout. Unlike image-to-image translation, in the context of scientific visualization, V2V establishes a variable selection process for choosing appropriate variable pairs for translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">V2V</head><p>Let us denote</p><formula xml:id="formula_0">V var = {V var1 , V var2 , â€¢â€¢â€¢ , V</formula><p>varm } as a set of variables in the given MTVD, and</p><formula xml:id="formula_1">V vari = {V vari 1 , V vari 2 , â€¢â€¢â€¢ , V vari n }</formula><p>as the temporal sequence of variable i, where m is the number of variables and n is the number of time steps. V vari [1 : k] is a subset of V vari , which has the first k time steps (n k). V vari [1 : k] is also the samples we take to train our deep learning model. V T and V S denote, respectively, the ground truth (GT) and synthesized variables from V2V. F vari j is the feature of variable i at the jth time step. Finally, let L, H, and W be the spatial dimensions of V var .</p><p>Our goal is to learn a mapping function T from one variable sequence V vara to another variable sequence V varb , namely, V varb = T (V vara ). As sketched in <ref type="figure" target="#fig_0">Figure 1</ref> (a), our approach consists of three stages: feature learning, translation graph construction, and variable translation. At the feature learning stage, we collect the available time steps from all variables of the given MTVD and utilize a U-Net <ref type="bibr" target="#b32">[33]</ref> to learn their latent features. Then we leverage t-SNE <ref type="bibr" target="#b39">[40]</ref> for dimensionality reduction where the latent feature of per variable and per time step is projected onto a 2D space. The t-SNE projection helps us analyze and understand the similarities and differences among these variables, which provide us hints on whether or not a given pair of variables is transferable. Among all the variables, we select a transferable variable group (e.g.,</p><formula xml:id="formula_2">{V var1 , â€¢â€¢â€¢ , V varp }), where p â‰¤ m.</formula><p>As an example, the transferable variable group for the example shown in <ref type="figure" target="#fig_0">Figure 1</ref> is {H, H+, He, He+}.</p><p>At the translation graph construction stage, given the transferable variable group, we estimate the transferable difficulty of variable b conditioned on variable a, and construct a translation graph G based on the computed transferable difficulty among different variables. Then, the source variable and target variable are selected from G .</p><p>At the variable translation stage, we train a V2V network to learn the mapping between the two variable sequences (i.e., V vara â†’V varb ) based on the translation graph result. Our V2V includes one generator (G) and one discriminator (D). G consists of three modules: feature extraction, feature translation, and variable translation. The feature extraction module extracts rich semantic information from the input variable. The feature translation module translates the features from the source variable to the target variable at different scales. The variable translation module translates the refined features to the target variable domain. As shown in <ref type="figure" target="#fig_0">Figure 1</ref> (b), in our experiments, the training data consist of early time steps of V vara and V varb , and the testing data consist of later time steps of V vara and V varb .</p><p>For U-Net training, we utilize the mean squared error (MSE) as the loss function to compute the difference between the reconstructed and GT variables. For V2V training, we leverage adversarial, volumetric, and feature losses to optimize the network. Next, we describe our approach in detail, including the network architectures of feature learning (Section 3.1) and variable translation (Section 3.3), as well as the algorithm for constructing the translation graph (Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature Learning</head><p>At the feature learning stage, we leverage a U-Net that takes the available time steps of all variables from the same MTVD as input and outputs feature descriptors per variable and per time step. U-Net also allows the reconstruction of a variable at a time step from the corresponding feature descriptor. Skip connections in U-Net can bridge different semantic features of the same scale, avoiding information loss during reconstruction. Require: A set of variables:</p><formula xml:id="formula_3">{V var1 , V var2 , â€¢â€¢â€¢ , V varp }.</formula><p>initialize a translation graph G with var 1 , â€¢â€¢â€¢ , var p as nodes and no edge</p><formula xml:id="formula_4">for i = 1 â€¢â€¢â€¢ p do for j = 1 â€¢â€¢â€¢i do if E (F vari , F var j ) &lt; Îµ then Compute TD(V var j ||V vari ) and TD(V vari ||V var j ) if TD(V var j ||V vari ) &lt; TD(V vari ||V var j )</formula><p>then add an edge from var i to var j to G else add an edge from var j to var i to G end if end if end for end for return G</p><p>In general, U-Net is composed of an encoding path and a decoding path. There are four convolutional (Conv) layers in the encoding path and four composites of deconvolutional (DeConv) layers and Conv layers in the decoding path. In the encoding path, each of the first three Conv layers reduces the input's dimension by half. The feature maps start with 64 and double in the following Conv layers. Then, we apply one Conv layer to transform the learned features into a 1D vector with 512 components. In the decoding path, DeConv layers are utilized to upscale the feature back to the original dimension, and the following Conv layers are utilized to fuse and refine feature maps. To keep the information flowing smoothly and avoid information loss, we concatenate the feature maps from the Conv layer in the encoding path and the feature maps from the DeConv layer as input for the consecutive Conv layer for refinement. The feature maps start with 256 and reduce by half in the following DeConv layers. We keep the same feature maps in each Conv layer followed by each DeConv layer. Note that the concatenation happens at the corresponding scale (i.e., these two tensors have the same resolution). Rectified linear unit (ReLU) <ref type="bibr" target="#b27">[28]</ref> is utilized after each Conv or DeConv layer to help the network learn faster and perform better. After the final Conv layer, tanh(â€¢) is applied for normalization (in the range of [âˆ’1, 1]).</p><p>Loss function. In order to ensure that the synthesized variables are close to the GT variables, we use MSE as the loss function to train U-Net. The MSE loss is defined as</p><formula xml:id="formula_5">L = u âˆ‘ j=1 ||V j âˆ’ V j || 2 ,<label>(1)</label></formula><p>where V j andV j are, respectively, the GT and synthesized variables of the jth training sample, u = k Ã—m is the number of training samples, and || â€¢ || 2 is L 2 norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Translation Graph Construction</head><p>After the transferable variable group (e.g.,</p><formula xml:id="formula_6">{V var1 , â€¢â€¢â€¢ , V varp })</formula><p>is determined, we define the transferable difficulty</p><formula xml:id="formula_7">(TD) of V var j conditioned on V vari as follows TD(V var j ||V vari ) = 1 k k âˆ‘ t=1 KL(F var j t ||F vari t ),<label>(2)</label></formula><p>where F</p><formula xml:id="formula_8">var j t</formula><p>is the feature of variable j at time step t, KL(â€¢||â€¢) is Kullback-Leibler divergence, and k is the total available time steps. The transferable order for the variable pair i and j is given by</p><formula xml:id="formula_9">min{TD(V var j ||V vari ), TD(V vari ||V var j )}.<label>(3)</label></formula><p>A translation graph G can be constructed based on the calculation of TDs between different variable pairs. The process is described in Algorithm 1. Given a pair of variables, we first compute the Euclidean distance of these two variables in the feature space. The distance determines whether or not these two variables are transferable. If the distance is less than a threshold Îµ, then we compute TD for the two variables, and determine the transferable order based on Equation 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Variable Translation</head><p>Generator. G consists of three modules. The feature extraction module utilizes four Conv layers to extract the features from the input variable. Each Conv downscales the input by half, and a ReLU is followed to accelerate the training and improve model performance. The feature translation module leverages three transformation blocks to translate the features from the source variable to the target variable at different scales and feed into the variable translation module. Each transformation block includes two paths. One path contains three Conv layers, and the other path contains one Conv layer. Finally, these two paths are connected by addition <ref type="bibr" target="#b14">[15]</ref>. The variable translation module utilizes four DeConv layers and four Conv layers followed by ReLU to map the feature to the output variable domain. Each DeConv upscales the input twice. In addition, after each DeConv layer, we stack the outputs from DeConv and the feature translation stage together and feed into a Conv layer. The two stacked outputs share the same scale, and the Conv layer does not change the scale of the input. Note that we only use tanh(â€¢) in the final Conv layer. The architecture of G is sketched in <ref type="figure">Figure 2</ref> (a). Discriminator. D includes four Conv layers, four spectral normalization (SN) <ref type="bibr" target="#b26">[27]</ref> layers, and one global average pooling (GAP) <ref type="bibr" target="#b24">[25]</ref> layer. Each Conv downscales the input by half, and SN is followed by Conv to normalize the weights in Conv for training stabilization. Leaky ReLU activation (Î± = 0.2) is applied after each Conv layer. Finally, one GAP is leveraged to squeeze the output into a tensor with 1 Ã— 1 Ã— 1 Ã— 1. No activation function is added after GAP. The architecture of D is sketched in <ref type="figure">Figure 2</ref> (b).</p><p>Loss function. As suggested by Han and Wang <ref type="bibr" target="#b12">[13]</ref>, we apply adversarial, volumetric, and feature losses to optimize V2V so that the synthesized variables are close to the GT variables. The adversarial loss is defined as</p><formula xml:id="formula_10">min Î¸G L G = E V âˆˆV S (D(G(V )) âˆ’ 1) 2 ,<label>(4)</label></formula><formula xml:id="formula_11">min Î¸D L D = 1 2 E V âˆˆV S D(V ) + 1 2 E V âˆˆV T (D(G(V )) âˆ’ 1) 2 ],<label>(5)</label></formula><p>where Î¸ G and Î¸ D are the learnable parameters in G and D, and</p><formula xml:id="formula_12">E[â€¢]</formula><p>denotes the expectation operation. The volumetric loss is defined as</p><formula xml:id="formula_13">L V = E V âˆˆV S ,V âˆˆV T ||G(V ) âˆ’V || 2 ,<label>(6)</label></formula><p>where || â€¢ || 2 denotes the L 2 norm. The feature loss is defined as</p><formula xml:id="formula_14">L F = E V âˆˆV S ,V âˆˆV T N âˆ‘ k=1 1 N k ||F k (G(V )) âˆ’ F k (V )|| 1 ,<label>(7)</label></formula><p>where N is the total number of Conv layers in D, N k is the number of elements in the kth Conv layer, and F k (â€¢) denotes the feature representation at the kth Conv layer. The overall loss for G is the combination of the three losses</p><formula xml:id="formula_15">min Î¸G L G = Î» 1 E V âˆˆV T D(G(V )) âˆ’ 1 2 + Î» 2 L V + Î» 3 L F ,<label>(8)</label></formula><p>where Î» 1 , Î» 2 , and Î» 3 are weights, each in the range of [0, 1]. Note that adversarial, volumetric, and feature losses serve different purposes in V2V training. Adversarial loss aims to judge the realness of the synthesized volumes from the generator. Volumetric loss seeks to ensure that the synthesized volumes are close to the GT volumes. Feature loss aims to stabilize the training process and enhance the visual quality.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULTS AND DISCUSSION 4.1 Data Sets and Network Training</head><p>We experimented with our approach using the data sets listed in Table 1. We implemented V2V based on PyTorch <ref type="bibr" target="#b29">[30]</ref> and used a single NVIDIA TESLA P100 GPU for training. For feature learning, we used the bicubic kernel with a downscaling factor of four to downscale combustion and ionization data sets for fast training. For variable translation, we used the original resolution for training; however, for each epoch, we randomly crop the volumes. This cropping mechanism can reduce training cost and GPU memory consumption. We point out that V2V can be applied to volumes of arbitrary size because it is fully convolutional. We scaled the range of V var to [âˆ’1, 1]. All learnable parameters in U-Net and V2V are initialized using He et al. <ref type="bibr" target="#b13">[14]</ref> and the Adam algorithm <ref type="bibr" target="#b20">[21]</ref> is applied for parameter update. We set one training sample per mini-batch. For training U-Net, the learning rate is set to 10 âˆ’4 . For training V2V, different learning rates for G and D are set as suggested by Roth et al. <ref type="bibr" target="#b33">[34]</ref>. The learning rates for G and D are 10 âˆ’4 and 4 Ã— 10 âˆ’4 , respectively. Î² 1 = 0.0, Î² 2 = 0.999. Î» 1 = 10 âˆ’3 , Î» 2 = 1, and Î» 3 = 5 Ã— 10 âˆ’1 . We trained U-Net and V2V for 50 and 150 epochs for all data sets, respectively. We sampled the first 40% time steps for training and the rest for inference. All these hyperparameters are determined based on experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Baselines. We compare one baseline solution for variable selection:</p><p>â€¢ Biswas et al. <ref type="bibr" target="#b1">[2]</ref>: It is an information-theoretic approach for variable grouping. Once grouped, users can select representative variables for further exploration. We leverage this solution to select variable pairs as the input to the V2V translation task.</p><p>Note that Biswas et al. is a solution for variable selection only, and not for variable translation. For translation comparison, we implement three baseline solutions for the V2V translation task:</p><p>â€¢ Histogram matching (HM) <ref type="bibr" target="#b34">[35]</ref>: HM is a traditional approach for translating one data set to another one conditioned on the content and style of the data. We apply HM to translate variable j at time step k conditioned on variable i at time step k and variable j at time step k âˆ’ 1. â€¢ Pix2Pix <ref type="bibr" target="#b18">[19]</ref>: Pix2Pix is the first paired image-to-image translation framework. The original Pix2Pix architecture is leveraged for the V2V translation task. â€¢ CycleGAN <ref type="bibr" target="#b46">[47]</ref>: CycleGAN is a deep learning solution for unpaired image-to-image translation. Since the variables are paired in our V2V translation task, we replace the identity loss in Cy-cleGAN with the volumetric loss in V2V.     For a fair comparison, we use the same loss functions (i.e., adversarial, volumetric, and feature losses) designed for V2V to train Pix2Pix and CycleGAN. Due to the page limit, we show the frame-to-frame comparison results in the accompanying video. Unless otherwise stated, we display all visualization results using the inferred volumes (refer to <ref type="figure" target="#fig_0">Figure 1</ref> (b) for an example). The same settings for lighting, viewing, transfer function (for direct volume rendering), and isovalue (for isosurface rendering) are applied to all visualization results for the same data set. With respect to the GT, we compare our V2V results with those of Evaluation metrics. For quantitative evaluation, we compute, between the synthesized variables and GT variables, the peak signal-tonoise (PSNR) at the data level, structural similarity index (SSIM) at the image level, and isosurface similarity (IS) <ref type="bibr" target="#b12">[13]</ref> at the feature level.</p><formula xml:id="formula_16">v 1 â†’v 2 ) v = âˆ’0.4 v = 0.3 v = âˆ’0.4 v = 0.3 v = âˆ’0.4 v = 0.3 v = âˆ’0.4 v = 0.</formula><p>Quantitative and qualitative analysis. In <ref type="figure">Figure 3</ref>, we show the data (PSNR) and image (SSIM) level results using HM, Pix2Pix, Cy-cleGAN, and V2V. At the data level, for the climate (SALTâ†’TEMP) data set, all four curves exhibit a periodic pattern since each time step denotes the temperature for each month and 12 time steps are for one year. The PSNR values of V2V outperform those of HM, Pix2Pix, and CycleGAN. For the combustion (MFâ†’YOH) data set, PSNR values decrease as time step goes. This is because, at the later time steps, the temporal behavior becomes more turbulent and complex, making the prediction more difficult. Again, V2V still outperforms HM, Pix2Pix, and CycleGAN. For the ionization (Hâ†’H+) data set, it is clear that V2V achieves the highest PSNR values for each time step. At the image level, V2V can still produce higher SSIM values compared with HM, Pix2Pix, and CycleGAN. It is the clear winner for the climate (SALTâ†’TEMP), combustion (MFâ†’YOH), and ionization (Hâ†’H+) data sets. For the combustion (MFâ†’YOH) data set, due to the increase of visual content, the SSIM values decrease as time step goes. In <ref type="table" target="#tab_2">Table 2</ref>, the average PSNR and SSIM values for HM, Pix2Pix, Cy-cleGAN, and V2V are reported. Again, V2V achieves the best PSNR and SSIM values. Note that the PSNR and SSIM curves of HM suddenly decrease after time step 40 for the combustion and ionization data sets since we only use 40% data for training. The error accumulates when predicting the later time steps. Since the climate data set is periodic, the PSNR and SSIM curves of HM do not exhibit a similar pattern as that of the other two data sets.</p><p>In <ref type="figure" target="#fig_3">Figure 4</ref>, the volume rendering results of the volumes synthesized by HM, Pix2Pix, CycleGAN, and V2V are shown. For the climate (SALTâ†’TEMP) data set, the rendering results synthesized by Pix2Pix and CycleGAN contain artifacts. The result generated by HM cannot well capture the main structure, while the result produced by V2V is much smoother and similar to the GT. For the combustion (MFâ†’HR) data set, V2V produces finer details with respect to GT, while HM and CycleGAN fail to recover the volume well. Pix2Pix generates some artifacts and is unable to recover the content around the volume boundary. For the ionization (Hâ†’H+) data set, V2V achieves the best result compared with HM, Pix2Pix, and CycleGAN. For example, for the Pix2Pix result, there are fewer details at the top part, and there are some artifacts at the bottom layer. For the CycleGAN result, it produces more orange content at the bottom part and fails to accurately recover the top part. For the HM result, it generates more purple and yellow content at the top part.</p><p>In <ref type="figure">Figures 5, 6</ref>, and 7, the isosurface rendering results of the volumes synthesized by HM, Pix2Pix, CycleGAN, and V2V using the climate (SALTâ†’TEMP), combustion (MFâ†’YOH), and ionization (Hâ†’H+) data sets are displayed. For each data set, we choose one time step and two isovalues to render the isosurfaces. For the climate (SALTâ†’TEMP) data set, it is evident that V2V can generate the highest quality isosurfaces compared with HM, Pix2Pix, and Cycle-GAN. HM fails to construct the isosurfaces close to GT, and the isosurfaces extracted from Pix2Pix and CycleGAN are filled with noises and artifacts. Similar observations can be made for the combustion (MFâ†’YOH) data set where V2V generates the highest quality isosurfaces compared with HM, Pix2Pix, and CycleGAN. For the ionization (Hâ†’H+) data set, V2V produces the highest quality isosurfaces.</p><p>Pix2Pix and CycleGAN fail to construct the isosurface at the top part, and HM synthesizes fake features compared with the GT results. Furthermore, the average IS values for these three data sets are reported in <ref type="table" target="#tab_3">Table 3</ref>. The average IS values also demonstrate that V2V achieves the best quality. Moreover, among Pix2Pix, CycleGAN, and V2V, Cy-cleGAN has almost the worst performance in terms of PSNR, SSIM, and IS. This is because, unlike image-to-image translation, where the translation is symmetric (e.g., day to night), in V2V translation, the translation is asymmetric (e.g., it is more challenging to translate from CHI to MF compared with translating from MF to CHI). Therefore, adding cycle consistency will hurt the translation performance. As for Pix2Pix, this architecture is too simple to capture the complex structure changes between the variables.</p><p>In <ref type="table" target="#tab_4">Table 4</ref>, we report the total training time (in hour), the average inference time (in second), and model size for Pix2Pix, CycleGAN, and V2V, respectively. As we can see, CycleGAN takes the longest training time since it needs to go through the network six times in one iteration (i.e., two discriminators, one cycle of Xâ†’Yâ†’X, and another cycle of Yâ†’Xâ†’Y). Pix2Pix and V2V only need to go through the network twice in one iteration (i.e., one discriminator and one generator). As for the inference time, there is no significant difference. In terms of model size, V2V needs 14MB to store parameters.</p><p>Comparison against compression. In <ref type="figure">Figure 8</ref>, we compare V2V and an advanced lossy compression (LC) method <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b22">23]</ref> using isosurface rendering results. This LC method can achieve a high compression rate while producing less data distortion. To achieve a fair comparison, we set a similar PSNR value (i.e., 29.5 dB) for both approaches. As we can see, the isosurfaces generated by LC include significant noises and artifacts compared with those produced by V2V.</p><p>Evaluation of variable selection. To show the effectiveness of the proposed variable selection solution, we compare V2V against Biswas et al. <ref type="bibr" target="#b1">[2]</ref>. We only use Biswas et al. to choose transferable variable pairs, as it does not perform variable translation. Once the pairs are selected from either V2V or Biswas et al. we apply the same solution (i.e., V2V) for translation. The training time of the variable selec- tion stage for the combustion and ionization data sets is 1.86 and 2.14 hours, respectively. The training time depends on the number of variables and the dimension of the data set. In <ref type="figure">Figure 9</ref>, we show clustered graphs and translation graphs of the combustion and ionization data sets using Biswas et al. and V2V, respectively. Note that the clustered graphs of Biswas et al. are fully-connected and undirected, while the translation graphs of V2V are partially-connected and directed. For the combustion data set, Biswas et al. demonstrates that YOH and CHI are more similar compared with MF and CHI, while V2V leads to the opposite conclusion. For the ionization data set, Biswas et al. demonstrates that T and H+ are similar, while H and H+ are distinguishable; however, V2V gets the opposite results.</p><p>To evaluate the effectiveness of these two variable selection approaches, we choose two pairs from Biswas et al. (i.e., YOHâ†’CHI and Tâ†’H+) and two from V2V (i.e., MFâ†’CHI and Hâ†’H+) for the translation task. The results are shown in <ref type="figure" target="#fig_0">Figures 10 and 11</ref>. As we can see, for Biswas et al. YOHâ†’CHI and Tâ†’H+ are not successfully judged from both volume and isosurface rendering results. For example, the volume rendering of CHI and H+ cannot exhibit a good visual quality compared with GT. As for the variable pairs selected by V2V, the translation results are satisfactory. <ref type="table" target="#tab_5">Table 5</ref> reports the average PSNR and SSIM values under these two variable translation schemes. Overall, based on the chosen source and target variables, V2V achieves higher PSNR and SSIM values in the translation task. These results indicate that, unlike V2V, variable pairs selected according to Biswas et al. may not be suitable for variable translation.</p><p>To further evaluate the effectiveness of the variable selection process, we use the ionization data set, choose H as the source variable, (a) Biswas et al. <ref type="bibr" target="#b1">[2]</ref> (b) V2V <ref type="figure">Fig. 9</ref>: Comparison of clustered graphs (left column) and translation graphs (right column). Top row: combustion. Bottom row: ionization. For both graphs, the distance between two variables in the 2D graph indicates their similarity.</p><p>and translate it to H+, He, He+, and PD. In <ref type="figure" target="#fig_0">Figure 12</ref>, we show the volume rendering results. For Hâ†’H+, Hâ†’He, Hâ†’He+, the synthesized results are similar to GT. However, for Hâ†’PD, V2V fails to recover the details of PD, particularly, the structure of the top part is not captured. This failure may be explained by a large distance between H and PD shown in the translation graph <ref type="figure">(Figure 9 (b)</ref>). The isosurface rendering results are shown in <ref type="figure" target="#fig_0">Figure 13</ref>. For Hâ†’H+ and Hâ†’He, the isosurfaces generated by V2V are close to GT and almost exhibit the same volumetric features. For Hâ†’He+, V2V can still recover the isosurfaces but miss some details. For example, the detailed surface features at the top part are missing in the isosurface synthesized by V2V. For Hâ†’PD, V2V fails to generate high-quality isosurface compared with the GT isosurface. For example, the isosurface generated by V2V consists of noises and artifacts, and details are missing at the bottom layer. We also report the average PSNR and SSIM values in <ref type="table" target="#tab_6">Table 6</ref>. The quantitative results also confirm the difficulty of translating H to PD. Based on the proposed solution, for the combustion set, scientists can save 60 time steps for the variables YOH and CHI if MF is the source variable, and 18.53GB storage is saved in total. As for the ionization, 21.05GB can be saved if H is the source variable since Evaluation of variable order. To verify that the translation order does impact the translation performance, we use the combustion data set and choose two translations, MFâ†’CHI and CHIâ†’MF. The results are demonstrated in <ref type="figure" target="#fig_0">Figure 14</ref>. As we can see, CHIâ†’MF is unsatisfactory since the synthesized isosurfaces fail to capture the interesting features compared with GT. However, MFâ†’CHI is successful since the generated isosurfaces are very close to GT. This asymmetric translation is likely because the essential information in MF is richer than that in CHI, which makes MFâ†’CHI easier than CHIâ†’MF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Hyperparameter Study</head><p>To evaluate V2V, we study these hyperparameter settings: training epochs, training samples, crop size, and feature translation module. The detailed discussion is given below.</p><p>Training epochs. We investigate how the increasing number of training epochs influences the rendering quality of the synthesized volumes. Isosurface rendering results obtained at different numbers of training epochs are shown in <ref type="figure" target="#fig_0">Figure 15</ref> for the combustion (MFâ†’YOH) data set. We can see that there are some artifacts at the bottom-right and top-left corners with 100 epochs, while these artifacts are eliminated with 150 epochs. Moreover, we observe that the PSNR values improve with 150 epochs. However, there is no significant dif- ference between synthesized results between 100 and 150 epochs. So, we recommend using 150 epochs to train V2V.</p><p>Training samples. We study how the number of training samples impacts visual quality, PSNR, and SSIM. 20%, 40%, and 60% training samples are applied to train V2V using the climate (SALTâ†’TEMP) data set. As shown in <ref type="figure" target="#fig_0">Figure 16</ref>, only using 20% samples to train V2V could lead to some artifacts in volume rendering results while using 40% can mostly remove these artifacts. As for isosurface rendering, the isosurface result generated by using 20% samples could miss some details with v = 0.45. In addition, the average PSNR and SSIM curves under different training samples are displayed in <ref type="figure" target="#fig_0">Figure 17 (a)</ref>. As we can observe, PSNR and SSIM values improve with the use of more training samples. But this comes with longer training time, as indicated in <ref type="figure" target="#fig_0">Figure 17 (b)</ref>. We observe that beyond 40% samples, there is almost no improvement in visual quality. Hence, we suggest using 40% samples to train V2V.</p><p>Crop size. Due to the GPU memory constraint, V2V cannot process the whole scalar data at the same time. Therefore, we crop subvolumes to train V2V. We train V2V with subvolume sizes of 128 Ã— 64 2 ,  192 Ã— 96 2 , and 256 Ã— 128 2 using the ionization (Heâ†’He+) data set. The average PSNR and SSIM curves are shown in <ref type="figure" target="#fig_0">Figure 17 (c)</ref>. We can see that using a larger subvolume size helps as V2V can learn richer semantic information. As for visual quality, we calculate the difference images <ref type="bibr" target="#b12">[13]</ref>, which are provided at the bottom-right corner, for a clear comparison. As shown in <ref type="figure" target="#fig_0">Figure 18</ref>, we can now see visual differences more clearly, particularly at the head of the ionization. Even though it takes more time to train with a larger subvolume size, as shown in <ref type="figure" target="#fig_0">Figure 17 (d)</ref>, we still recommend using the subvolume size of 256 Ã— 128 2 to train the ionization (Heâ†’He+) data set.</p><p>Feature translation module. To study what influences the visual quality of the volumes generated by V2V, we conducted such an experiment that trains V2V without using the feature translation module (FTM), i.e., the three purple transformation blocks shown in <ref type="figure">Figure 2</ref> (a). The results are shown in <ref type="figure" target="#fig_0">Figure 19</ref>. We can see that more green content and less yellow content are rendered from the volume generated by V2V without FTM. We speculate that FTM serves the role of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS AND FUTURE WORK</head><p>We have presented V2V, a new deep learning solution for selecting variables and translating variable sequences for MTVD analysis and visualization. Leveraging GAN, V2V can map one variable sequence to another variable sequence while achieving better visual quality of direct volume rendering and isosurface rendering than HM and two other deep learning solutions (Pix2Pix and CycleGAN). Besides qualitative comparison, quantitative evaluation results using PSNR (datalevel), SSIM (image-level), and IS (feature-level) also confirm the effectiveness of our approach.</p><p>V2V can work in the in situ visualization setting. In this scenario, at simulation time, we store the complete sequence for one variable (i.e., all the time steps) while saving the rest of variable sequences sparsely (i.e., only the early time steps) for storage saving. During postprocessing, these reduced variable sequences are synthesized back to their original sequences with high fidelity.</p><p>V2V is part of our research effort on data augmentation for scientific visualization, which refers to the addition of spatial, temporal, and variable details to reduced data by incorporating information derived from internal and external sources. V2V addresses the variable-domain data augmentation, while our previous work on TSR-TVD <ref type="bibr" target="#b12">[13]</ref> addresses the temporal-domain data augmentation. We are working on SSR-TVD <ref type="bibr" target="#b11">[12]</ref>, the spatial-domain data augmentation, to complete this research. In the future, we would like to extend our framework to handle multiple variable translations. That is, given a variable sequence, our framework can simultaneously extrapolate multiple variable sequences using multi-domain translation <ref type="bibr" target="#b4">[5]</ref>. Besides, we will also explore other applications of the extracted features, for example, utilizing these features in other scientific data generation and analysis tasks, such as super-resolution and feature tracking.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>(a) Overview of V2V. For feature learning, a U-Net is applied to extract features from variables and t-SNE is used to project the features for estimating variable similarity. A translation graph is constructed based on the learned variable features. For variable translation, variable pairs are selected and V2V is trained for learning the translation mapping. (b) Training and testing data from the volume sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :Algorithm 1</head><label>21</label><figDesc>Network architecture of V2V. (a) G contains eight Conv layers, four DeConv layers, and three transformation blocks. (b) D includes four Conv layers, four SN layers, and one GAP layer. Translation graph construction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) climate (SALTâ†’TEMP) (b) combustion (MFâ†’YOH) (c) ionization (Hâ†’H+) Fig. 3: PSNR (top row) and SSIM (bottom row) of synthesized variables (TEMP, YOH, and H+) under HM, Pix2Pix, CycleGAN, and V2V.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4 :</head><label>4</label><figDesc>Comparison of volume rendering results. Top to bottom: the climate (SALTâ†’TEMP), combustion (MFâ†’YOH), and ionization (Hâ†’H+) data sets. Displayed here are the renderings of TEMP at time step 159, YOH at time step 65, and H+ at time step 70, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>v = âˆ’0.96 v = âˆ’0.9 v = âˆ’0.96 v = âˆ’0.9 v = âˆ’0.96 v = âˆ’0.9 v = âˆ’0.96 v =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>5 : 6 :</head><label>56</label><figDesc>Comparison of isosurface rendering results of the climate (SALTâ†’TEMP) data set at time step 167. The chosen isovalues are v = âˆ’0.4 (top row) and v = 0.3 (bottom row). Comparison of isosurface rendering results of the combustion (MFâ†’YOH) data set at time step 53. The chosen isovalues are v = âˆ’0.9 (top row) and v = âˆ’0.55 (bottom row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>7 : 8 :</head><label>78</label><figDesc>Comparison of isosurface rendering results of the ionization (Hâ†’H+) data set at time step 92. The chosen isovalues are v = âˆ’0.96 (top row) and v = âˆ’0.9 (bottom row). (a) LC [24, 23] (b) V2V (c) GT Fig. Isosurface rendering results using the combustion (CHI) data set at time step 60. The chosen isovalues are v = âˆ’0.7 (top row) and v = 0.3 (bottom row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>10 :</head><label>10</label><figDesc>Comparison of variable selection approaches via volume rendering. Variable pairs selected by Biswas et al. are YOHâ†’CHI (top row) and Tâ†’H+ (bottom row). Variable pairs selected by V2V are MFâ†’CHI (top row) and Hâ†’H+ (bottom row). The displayed time steps are 80 and 50 for CHI and H+, respectively. (a) Biswas et al. [2] (b) V2V (c) GT Fig. 11: Comparison of variable selection approaches via isosurface rendering. Variable pairs selected by Biswas et al. are YOHâ†’CHI (top row) and Tâ†’H+ (bottom row). Variable pairs selected by V2V are MFâ†’CHI (top row) and Hâ†’H+ (bottom row). The chosen isovalues are v = âˆ’0.6 (top row) for CHI and v = âˆ’0.1 (bottom row) for H+. The displayed time steps are 80 and 50 and for CHI and H+, respectively. these variables (H+, He, and He+) are only stored 40 time steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>12 :</head><label>12</label><figDesc>Comparison of volume rendering results of the ionization data set at time step 60. H is chosen as the source variable. Top row: V2V. Bottom row: GT. (a) H+ (b) He (c) He+ (d) PD Fig. 13: Comparison of isosurface rendering results of the ionization data set at time step 60. H is chosen as the source variable. Top row: V2V. Bottom row: GT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>(a) V2V (b) GT (c) V2V (d) GT Fig. 14: Evaluation of translation order using the combustion data set via isosurface rendering at time step 72. Top row: MFâ†’CHI (TD = 7.10). Bottom row: CHIâ†’MF (TD = 8.07). The chosen isovalues are v = âˆ’0.6 (1st and 2nd columns) and v = 0.5 (3rd and 4th columns).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>15 :</head><label>15</label><figDesc>Comparison of isosurface rendering results under different training epochs using the combustion (MFâ†’YOH) data set at time step 70. The chosen isovalues are v = âˆ’0.7 (top row) and v = âˆ’0.3 (bottom row). (a) 20% (b) 40% (c) 60% (d) GT Fig. 16: Comparison of volume rendering (1st row) and isosurface (2nd row) rendering results under different training samples using the climate (SALTâ†’TEMP) data set at time step 176. The chosen isovalue is v = 0.45 (2nd row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>(a) climate (SALTâ†’TEMP) (b) climate (SALTâ†’TEMP) (c) ionization (Heâ†’He+) (d) ionization (Heâ†’He+) Fig. 17: Comparison of hyperparameter settings. (a) Average PSNR and SSIM under different training samples. (b) Average training time (per epoch) under different training samples. (c) Average PSNR and SSIM under different crop sizes. (d) Average training time (per epoch) under different crop sizes. (a) 128 Ã— 64 2 (b) 192 Ã— 96 2 (c) 256 Ã— 128 2 (d) GT Fig. 18: Comparison of volume rendering (1st row) and isosurface (2nd row) rendering results under different crop sizes using the ionization (Heâ†’He+) data set at time step 80. The chosen isovalue is v = âˆ’0.9. (a) V2V w/o FTM (b) V2V (c) GT Fig. 19: Comparison of volume rendering results under different architectures using the combustion (MFâ†’CHI) data set at time step 70. refining and filtering the features extracted at different scales during variable translation, improving translation quality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, H. Zheng, D. Z. Chen, and C. Wang are with the University of Notre Dame. E-mail: {jhan5, hzheng3, dchen, chaoli.wang}@nd.edu. â€¢ Y. Xing is with Sichuan University. E-mail: yhxing98@gmail.com. Manuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication xx xxx. 201x; date of current version xx xxx. 201x. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org. Digital Object Identifier: xx.xxxx/TVCG.201x.xxxxxxx/</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The variables and dimensions of each data set.</figDesc><table><row><cell>data set</cell><cell>variables</cell><cell>dimension (x Ã— y Ã— z Ã— t)</cell></row><row><cell>climate</cell><cell>SALT, TEMP</cell><cell>360 Ã— 66 Ã— 27 Ã— 200</cell></row><row><cell cols="2">combustion CHI, HR, MF, YOH</cell><cell>480 Ã— 720 Ã— 120 Ã— 100</cell></row><row><cell>ionization</cell><cell></cell><cell></cell></row></table><note>H, H+, He, He+, H2, PD, T 600 Ã— 248 Ã— 248 Ã— 100</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Average PSNR and SSIM values. The best ones are highlighted in bold.</figDesc><table><row><cell>data set (v 1 â†’v 2 )</cell><cell>method</cell><cell cols="2">PSNR (dB) SSIM</cell></row><row><cell></cell><cell>HM</cell><cell>13.12</cell><cell>0.642</cell></row><row><cell>climate (SALTâ†’TEMP)</cell><cell cols="2">Pix2Pix CycleGAN 20.78 21.39</cell><cell>0.695 0.616</cell></row><row><cell></cell><cell>V2V</cell><cell>31.69</cell><cell>0.797</cell></row><row><cell></cell><cell>HM</cell><cell>12.96</cell><cell>0.291</cell></row><row><cell>combustion (MFâ†’YOH)</cell><cell cols="2">Pix2Pix CycleGAN 20.56 20.46</cell><cell>0.585 0.351</cell></row><row><cell></cell><cell>V2V</cell><cell>28.73</cell><cell>0.776</cell></row><row><cell></cell><cell>HM</cell><cell>19.62</cell><cell>0.668</cell></row><row><cell>ionization (Hâ†’H+)</cell><cell cols="2">Pix2Pix CycleGAN 37.59 40.58</cell><cell>0.887 0.812</cell></row><row><cell></cell><cell>V2V</cell><cell>45.75</cell><cell>0.951</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Average IS values at chosen isovalues. The best ones are highlighted in bold.</figDesc><table><row><cell>HM</cell><cell>Pix2Pix</cell><cell>CycleGAN</cell><cell>V2V</cell></row><row><cell>data set (</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="4">training training inference model</cell></row><row><cell>data set</cell><cell>method</cell><cell>epochs</cell><cell>time</cell><cell>time</cell><cell>size</cell></row><row><cell></cell><cell>Pix2Pix</cell><cell>150</cell><cell>6.79</cell><cell>3.52</cell><cell>6</cell></row><row><cell>climate</cell><cell cols="2">CycleGAN 200</cell><cell>56.44</cell><cell>4.63</cell><cell>26</cell></row><row><cell></cell><cell>V2V</cell><cell>150</cell><cell>15.36</cell><cell>4.01</cell><cell>14</cell></row><row><cell></cell><cell>Pix2Pix</cell><cell>150</cell><cell>31.21</cell><cell>187.45</cell><cell>6</cell></row><row><cell>combustion</cell><cell cols="2">CycleGAN 200</cell><cell>169.08</cell><cell>220.36</cell><cell>26</cell></row><row><cell></cell><cell>V2V</cell><cell>150</cell><cell>53.14</cell><cell>194.72</cell><cell>14</cell></row><row><cell></cell><cell>Pix2Pix</cell><cell>150</cell><cell>21.43</cell><cell>122.39</cell><cell>6</cell></row><row><cell>ionization</cell><cell cols="2">CycleGAN 200</cell><cell>125.46</cell><cell>130.93</cell><cell>26</cell></row><row><cell></cell><cell>V2V</cell><cell>150</cell><cell>40.76</cell><cell>129.07</cell><cell>14</cell></row></table><note>Total training time (in hour), average inference time (in sec- ond), and model size (MB) under Pix2Pix, CycleGAN, and V2V.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Average PSNR and SSIM values for variable translations using Biswas et al.<ref type="bibr" target="#b1">[2]</ref> and V2V. The better ones are highlighted in bold.</figDesc><table><row><cell>data set</cell><cell cols="2">variable pair approach</cell><cell cols="2">PSNR (dB) SSIM</cell></row><row><cell>combustion</cell><cell>YOHâ†’CHI MFâ†’CHI</cell><cell>Biswas et al. V2V</cell><cell>24.76 35.76</cell><cell>0.607 0.829</cell></row><row><cell>ionization</cell><cell>Tâ†’H+ Hâ†’H+</cell><cell cols="2">Biswas et al. 33.41 V2V 45.75</cell><cell>0.827 0.951</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Average PSNR and SSIM values for different V2V translations of the ionization data set. H is the source variable.</figDesc><table><row><cell>target variable</cell><cell cols="2">PSNR (dB) SSIM</cell></row><row><cell>H+</cell><cell>45.75</cell><cell>0.951</cell></row><row><cell>He</cell><cell>37.44</cell><cell>0.837</cell></row><row><cell>He+</cell><cell>39.99</cell><cell>0.874</cell></row><row><cell>PD</cell><cell>31.68</cell><cell>0.616</cell></row><row><cell>HM, Pix2Pix, and CycleGAN.</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research was supported in part by the U.S. National Science Foundation through grants IIS-1455886, CCF-1617735, CNS-1629914, DUE-1833129, and IIS-1955395, and the NVIDIA GPU Grant Program. The authors would like to thank the anonymous reviewers for their insightful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A generative model for volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1636" to="1650" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An informationaware framework for exploring multivariate data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Woodring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2683" to="2692" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Static correlation visualization for large time-varying volume data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Wittenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Pacific Visualization Symposium</title>
		<meeting>IEEE Pacific Visualization Symposium</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Las-soNet: Deep lasso-selection of 3D point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="195" to="204" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">StarGAN: Unified generative adversarial networks for multi-domain image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8789" to="8797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning a predictable and generative vector representation for objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="484" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scalable data servers for large multivariate volume visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Glatter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mollenhour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1291" to="1299" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SSR-VFD: Spatial super-resolution for vector field data analysis and visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Pacific Visualization Symposium</title>
		<meeting>IEEE Pacific Visualization Symposium</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="71" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">FlowNet: A deep learning framework for clustering and selection of streamlines and stream surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1732" to="1744" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Flow field reduction via reconstructing vector data from 3D streamlines using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="54" to="67" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SSR-TVD: Spatial super-resolution for time-varying data analysis and visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Under Minor Revision</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">TSR-TVD: Temporal super-resolution for timevarying data analysis and visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="205" to="215" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">InSituNet: Deep image synthesis for parameter space exploration of ensemble simulations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S G</forename><surname>Nashed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peterka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="33" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">DNN-VolVis: Interactive volume visualization supported by deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Pacific Visualization Symposium</title>
		<meeting>IEEE Pacific Visualization Symposium</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="282" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Access pattern learning with long shortterm memory for parallel particle tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Pacific Visualization Symposium</title>
		<meeting>IEEE Pacific Visualization Symposium</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="76" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference for Learning Representations</title>
		<meeting>International Conference for Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>HuszÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An efficient transformation scheme for lossy data compression with point-wise relative error bound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cappello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Cluster Computing</title>
		<meeting>IEEE International Conference on Cluster Computing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="179" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Error-controlled lossy compression optimized for high compression ratios of scientific datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cappello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Big Data</title>
		<meeting>IEEE International Conference on Big Data</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="438" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference for Learning Representations</title>
		<meeting>International Conference for Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Association analysis for visual exploration of multivariate scientific data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="955" to="964" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference for Learning Representations</title>
		<meeting>International Conference for Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted Boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2337" to="2346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A deep learning approach to selecting representative time steps for time-varying multivariate data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">P</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Von Ohlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Visualization (Short Papers)</title>
		<meeting>IEEE Conference on Visualization (Short Papers)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="131" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Visual analysis of the air pollution problem in Hong Kong</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1408" to="1415" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<meeting>International Conference on Medical Image Computing and Computer-Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stabilizing training of generative adversarial networks through regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cosegmentation of image pairs by histogram matching-incorporating a global constraint into MRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="993" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multifield-Graphs: An approach to visualizing correlations in multifield scalar data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Theisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="917" to="924" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">CNNs based viewpoint estimation for volume visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<idno>27:1-27:22</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Correlation study of time-varying multivariate climate data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sukharev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Wittenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Pacific Visualization Symposium</title>
		<meeting>IEEE Pacific Visualization Symposium</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="161" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Exploring time-varying multivariate volume data using matrix of isosurface similarity maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Imre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1236" to="1245" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Analyzing information transfer in time-varying multivariate data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Grout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Pacific Visualization Symposium</title>
		<meeting>IEEE Pacific Visualization Symposium</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="99" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Volumetric isosurface rendering with deep learning-based super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thuerey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Westermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Accepted</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">tempoGAN: A temporally coherent, volumetric GAN for super-resolution fluid flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thuerey</surname></persName>
		</author>
		<idno>95:1-95:15</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Real-time user-guided image colorization with learned deep priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno>119:1-119:11</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Volume upscaling with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Computer Graphics International</title>
		<meeting>Computer Graphics International</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
