<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Volumetric Ambient Occlusion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Engel</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Ropinski</surname></persName>
						</author>
						<title level="a" type="main">Deep Volumetric Ambient Occlusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Volume illumination</term>
					<term>deep learning</term>
					<term>direct volume rendering</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Fig. 1: Volume rendering with volumetric ambient occlusion achieved through Deep Volumetric Ambient Occlusion (DVAO). DVAO uses a 3D convolutional encoder-decoder architecture, to predict ambient occlusion volumes for a given combination of volume data and transfer function. While we introduce and compare several representation and injection strategies for capturing the transfer function information, the shown images result from preclassified injection based on an implicit representation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Direct volume rendering (DVR) is the most common tool for volume visualization in practice. In DVR, first a transfer function is defined to map volume intensity to optical properties, which are then used in a raycaster to compute the color and opacity along a ray using the emission-absorption model. This technique usually leverages local shading for individual samples along the rays, which results in a lack of global illumination (GI) effects. However, non-local effects like ambient occlusion can greatly enhance the visualization quality <ref type="bibr" target="#b25">[26]</ref>. While several volumetric lighting techniques have been proposed in the past in order to improve classical DVR, until today no deep learning (DL) based volumetric lighting approaches have been proposed.</p><p>DL has recently proven to be a very effective tool in a variety of fields. In fact DL techniques dominate the state of the art (SOTA) in many computer vision problems <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b40">41]</ref>, the majority of them using convolutional neural networks (CNNs). CNNs learn to extract complex high-level representations of the input data in order to solve a task at hand, which makes them extremely flexible. Recently, they have also been exploited in the context of rendering, such as in image based shading <ref type="bibr" target="#b33">[34]</ref> or denoising of single sample ray traced images <ref type="bibr" target="#b3">[4]</ref>. In the field of volume visualization, CNNs are for instance used to enable super-resolution in the context of iso-surface renderings <ref type="bibr" target="#b48">[49]</ref>. While these works use classical 2D convolutional networks, there has also been research on learning directly on volumetric data using 3D convolutions. Examples include classification <ref type="bibr" target="#b21">[22]</ref>, segmentation <ref type="bibr" target="#b8">[9]</ref>, or using 3D CNNs to learn a complex feature space for transfer functions <ref type="bibr" target="#b6">[7]</ref>.</p><p>Most of the work conducted on 3D CNNs concentrates on extracting information from structured volume data alone, however DVR requires additional global information, in the form of the transfer function, that is not directly aligned with the structural nature of the volume data set. Therefore, such global data cannot be trivially injected into existing CNN architectures.</p><p>In this work we propose and investigate strategies to inject such global unstructured information into 3D convolutional neural networks to be able to compute volumetric ambient occlusion, an effect which has received much attention prior to the deep learning era <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43]</ref>. In this scenario, we focus on providing transfer function information, which is also an essential part in many other volume visualization scenarios, to the network. To provide this gloabal information to the learner, we propose and discuss a variety of representations and injection strategies, which are influenced by the SOTA in image-based learning tasks tackled in computer vision. To investigate these strate-gies, we compare them based on quality and performance, and derive general recommendations for the representation and injection of global unstructured information into CNNs in the context of more general volume rendering. Further, our training data set, source code and trained models are publicly available <ref type="bibr" target="#b0">1</ref> .</p><p>Our main contributions can be summarized as follows:</p><p>• We introduce DVAO, a novel approach to predict volumetric ambient occlusion during interactive DVR, by facilitating a 3D CNN.</p><p>• We present different representations and injection strategies for providing global unstructured information to the CNN, and compare them when applied to transfer function information.</p><p>• We demonstrate the effectiveness of DVAO in an extensive evaluation, where we show that it generalizes beyond both, structures and modalities seen during training.</p><p>• We formulate guidelines applicable in other volume visualization learning scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>A natural consequence of the break-throughs of CNNs applied to 2D images, was the application to volumetric data sets <ref type="bibr" target="#b17">[18]</ref>. While these techniques are mostly used for data processing tasks, such as semantic segmentation <ref type="bibr" target="#b36">[37]</ref>, more recently, researchers have also investigated, how CNNs can aid the volume visualization process. In the following, we will first discuss volumetric ambient occlusion techniques, before we provide an overview of learning based volume illumination methods. Volumetric illumination. While classical direct volume rendering makes use of the local emission absorption model <ref type="bibr" target="#b29">[30]</ref>, in the past years, several volumetric illumination techniques have been proposed, that aim at incorporating more global effects. Ambient occlusion, as also addressed in this paper, was one of the first more advanced volume illumination effects, researchers have targeted.</p><p>Ropinski et al. used clustering techniques, applied to voxel neighborhoods in order to enable a transfer function independent AO precomputation <ref type="bibr" target="#b37">[38]</ref>. During rendering, interactive transfer function updates were possible by classifying cluster representatives. Due to the fact, that cluster representatives are represented as a distribution, rather than a spatial data structure, artifacts can be expected. To allow for interactive AO rendering, Hernell et al. instead have proposed a spatial data structure together with efficient ray evaluation schemes <ref type="bibr" target="#b12">[13]</ref>. With their approach, they are able to capture local illumination effects, as they limit the length of the ambient occlusion rays in order to limit the number of needed operations. Naturally, this limit depends on the feature scale of the dataset, which requires manual tuning according to the data set at hand. Another interactive approach has been proposed by Diaz et al., who have exploited summed area tables in order to approximate occlusion-based effects <ref type="bibr" target="#b10">[11]</ref>.</p><p>While the previous techniques were in principle independent of the underlying rendering technique, Schott et al. have proposed directional occlusion shading, a method that exploits the sequential processing of slice-based volume rendering <ref type="bibr" target="#b39">[40]</ref>. This technique has later been extended byŠoltészová et al., in order to support directional illumination effects <ref type="bibr" target="#b42">[43]</ref>, while Schott et al. extended it to also incorporate tubular structures <ref type="bibr" target="#b38">[39]</ref>. The work by Kroes and Eisemann describes how volumetric ambient occlusion can be realized efficiently on modern GPUs <ref type="bibr" target="#b23">[24]</ref>.</p><p>After interactive volumetric ambient occlusion has been tackled by these approaches, researchers started focusing on more complex volume illumination challenges. As these methods are largely beyond the scope of this paper, we rather focus on a few milestones, than addressing the entire body of work. Kroes et al. have presented the exposure renderer, which realizes Monte Carlo volume ray-tracing, leading to an unprecedented quality while unfortunately only allowing progressive updates <ref type="bibr" target="#b22">[23]</ref>. Ament et al. have preintegrated multiple <ref type="bibr" target="#b0">1</ref> Project Page: dominikengel.com/dvao scattering effects, by considering a finite spherical region centered at the current volume sample <ref type="bibr" target="#b1">[2]</ref>. Later, Ament and Dachsbacher proposed to realize anisotropic shading effects in DVR by also analyzing the ambient region around a volume sample <ref type="bibr" target="#b0">[1]</ref>. Jönnson et al. instead developed an efficient data structure, which enables them to apply photon mapping in the context of DVR <ref type="bibr" target="#b19">[20]</ref>. An entirely different approach has been followed upon by Wald et al., as they present efficient CPU data structures to realize interactive ray tracing, which they also demonstrate by realizing ambient occlusion effects <ref type="bibr" target="#b46">[47]</ref>. More recently, Magnus et al. have realized the integration of refraction and caustics into an interactive DVR pipeline, leading to realistic results <ref type="bibr" target="#b28">[29]</ref>.</p><p>While all these techniques have a similar goal as DVAO, i.e., achieving advanced volumetric illumination effects, none of the previous work facilitated deep learning architectures to reach this goal. CNN-based volume visualization. While many approaches have been published regarding volumetric illumination models, rather few CNNbased volume visualizations exist.</p><p>Jain et al. present a deep encoder decoder architecture, which compresses a volume, which is then uncompressed before rendering via ray casting <ref type="bibr" target="#b16">[17]</ref>. While also exploiting 3D learning, the process does, in contrast to our approach, not involve any rendering related properties, such as for instance ambient occlusion or the transfer function. Quan et al. instead introduce a probabilistic approach that exploits sparse 3D convolutional encoding in order to generate probabilistic transfer functions <ref type="bibr" target="#b35">[36]</ref>. With a similar goal, Cheng et al. extract features from a trained CNN, which are then quantized to obtain high-dimensional features for each voxel, such that classification is aided <ref type="bibr" target="#b5">[6]</ref>.</p><p>One of the first published approaches for direct image generation in the context of volume rendering is a generative adversarial network (GAN) trained for the generation of volume rendered images <ref type="bibr" target="#b2">[3]</ref>. Instead of operating on actual volumes, the authors train an image-based GAN on a set of volume rendered images, generated using different viewpoints and transfer functions. Based on this training data, their model learns to predict volume rendered images. Unfortunately, changing the data set means training a new model, which is a costly process also incorporating the generation of new renderings for the training set. Hong et al. follow another approach, whereby they exploit the volumetric data set and an example image <ref type="bibr" target="#b13">[14]</ref>. Based on this combination a GAN synthesizes a new rendering of the data set, while obeying to rendering parameters extracted from the sample image. In contrast, Weiss et al. propose a super-resolution approach for the generation of isosurface renderings, whereby they also take into account ambient occlusion <ref type="bibr" target="#b48">[49]</ref>. However, instead of learning in the 3D domain, their approach learns on low-resolution normal and depth maps in order to predict high-resolution maps which are then illuminated using screenspace shading. Similar to our approach, Tkachev et al. rather operate on the volumetric data at hand <ref type="bibr" target="#b43">[44]</ref>. Their approach enables the prediction of future data values in time-varying data sets, by exploiting neighborhood information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>In this section we introduce DVAO. In contrast to prior approaches that compute ambient occlusion numerically, we frame the problem of volumetric ambient occlusion as a supervised learning problem. Thus, we train a 3D convolutional neural network to predict volumetric ambient occlusion. Our predicted AO volume is at a resolution of 128 <ref type="bibr" target="#b2">3</ref> and our figures are rendered by ray casting the full resolution volume, while sampling the AO at the given 128 <ref type="bibr" target="#b2">3</ref> .</p><p>Since volumetric ambient occlusion depends highly on the opacity of the volume, our neural net needs to consider opacity, which is usually modulated by means of the transfer function. Unfortunately, conventional CNNs can only operate on structured data, such as images or volumes, and are thus incompatible with the typically unstructured representations of transfer functions. In order to make such unstructured information compatible with our CNN, we investigate a variety of possible representations and injection strategies for providing the information represented by the transfer function to the CNN.</p><p>In the following we first discuss the data that is necessary to train and validate our neural network, as well as challenges arising from their raw form. Next we describe our proposed neural network architecture and present the different representations and injection strategies for transfer functions. Furthermore, we discuss details regarding the choice of loss function and the overall training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data</head><p>In this section we describe what kind of data is mandatory to learn volumetric ambient occlusion and discuss the problems arising from the raw data representations.</p><p>In order to compute ambient occlusion a spatial description of optical properties is necessary. Traditionally this is done by defining a transfer function consisting of a color mapping c(s) and an opacity mapping τ(s) that describes the extinction density for a scalar field s. The amount of incident light for any position within the scalar field is then inherently defined through this mapping. In order to train a neural network to predict the amount of ambient occlusion, a large amount of training examples consisting of pairs of volume data and opacity mappings is necessary. Further, a ground truth ambient occlusion must be provided for each training example to enable supervised training.</p><p>In the following we first describe the CQ500 dataset <ref type="bibr" target="#b7">[8]</ref> containing the volume data we use for training, as well as the pre-processing necessary to use the volumes in our proposed neural network. For the opacity mappings we randomly generate reasonable transfer functions that we can use for training. Lastly we describe how we compute the ground truth ambient occlusion volumes using Monte Carlo raycasting. See the supplemental material for visualized training examples. CQ500 We trained and validated our method on Qure.ai's CQ500 dataset <ref type="bibr" target="#b7">[8]</ref>, which consists of 491 CT scans of human heads. We used 397 of those scans for training and 80 for validation of the neural network. The remaining scans were omitted due to being outliers in terms of aspect ratio and number of slices. The chosen subset of the data has a resolution of 512 × 512 per slice and has between 101 and 645 slices per volume. The volumes are given in Hounsfield units in the range of [0, 4095] and normalized to [0, 1]. Furthermore we crop each volume to its largest non-transparent subvolume, according to the transfer function it is paired with during training. The cropped volume is resized to 128 × 128 × 128 before feeding into the neural network. Note that this resolution is determined by the amount of GPU memory available in our hardware and we recommend using the highest resolution possible. Lastly we apply data augmentation to increase the effective amount of training volumes available using random permutation and flipping of the spatial dimensions. Transfer Function Generation For each training example we generate a random opacity transfer function in order to maximize the variation in our training data. Note that there is a large corpus of work regarding the choice of transfer functions for volume rendering <ref type="bibr" target="#b27">[28]</ref>, however these works usually try to find the single best transfer function for a visualization, while we are mostly looking for a wide variety of transfer functions that are still reasonable in the sense that they map opacity to coherent structures in the volume. In order to learn a large and representative space of transfer functions, we generate random piece-wise linear opacity transfer functions using between 1 and 5 nonoverlapping trapezoids. To ensure that every transfer function actually assigns non-zero opacity to prominent structures in the volumes, the trapezoids are centered around peaks in the volume histogram, while ignoring peaks representing air or visible parts of the scanner. The trapezoids are described with a top and bottom height h top , h bottom , as well as an inner and outer width w inner , w outer , which are randomly generated according to the following rule:</p><formula xml:id="formula_0">h top ∼U(h min , h max ), h bottom ∼U(h min , h top ) w outer ∼U(w min , w max ), w inner ∼U(w min , w outer )</formula><p>where U denotes a uniform distribution. We empirically chose h min = 0.1, h max = 0.9 and w min = 0.01 , w max = 0.1 to generate the transfer functions. After generating the trapezoids, we take their corner points to form an unordered set of points S ⊂ R 2 consisting of pairs of normalized intensity value and opacity mapping. This set resembles an unstructured transfer function representation. Furthermore a discretized transfer function representation T ∈ [0, 1] R in the form of a 1D texture with resolution R can be easily derived from S using linear interpolation. Note that neither representation is compatible with 3D convolutional neural networks by default and specialized TF representations have to be derived in order to feed the transfer function into the network. We will use both the unstructured representation S and the discretized representation T to derive CNN compatible TF representations in Sect. 3.3.</p><p>The discretized representation T is further used to compute the ground truth ambient occlusion. Ambient Occlusion Ground Truth In order to train our neural network in a supervised fashion, we need to provide a ground truth ambient occlusion volume for each pair of input volume and transfer function. The goal is for this ground truth to be as accurate as possible while suiting the needs of volume visualization. To achieve this we computed the ambient occlusion ground truth using Monte Carlo simulation of light rays, while restricting the length of the rays to a maximum of D, which is 10% of the volume diameter. We chose to restrict the ray length in order to avoid fully shadowed regions, which is usually desirable in volume visualization. We cast rays from each voxel center x in random directions ω ∼ Ω and integrate an opacity value along the ray using the emission and absorption model. The occlusion of ambient light at voxel center x is described by Equation 1.</p><formula xml:id="formula_1">AO(x) = Ω p(ω) 1 − x+ωD x+ωε q(s)e − s x+ωε τ(t)dt ds dω (1)</formula><p>with extinction coefficient τ, probability of sampled angle p(ω), small offset ε from the voxel center to prevent self-occlusion and light contribution q(s) at point s along the ray. We chose to cast 512 rays per voxel to generate the ground truth ambient occlusion volume. <ref type="figure">Fig. 2</ref> illustrates the main convolutional architecture which we propose to learn volumetric ambient occlusion. It resembles an encoder decoder network consisting of 3D convolutions and with skip connections similar to U-Net <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b36">37]</ref>. We design our architecture to encode volumes of size 128 3 × 1 to a latent representation of size 1 3 × 512, which is then decoded to a 128 3 × 1 ambient occlusion volume. The latent representation resembles a low-dimensional compressed feature vector describing the full volume and contains the high-level features that the convolutional encoder learns to extract. Our architecture is composed of ConvBlocks, which consist of two 3D convolutions, instance normalizations (IN) <ref type="bibr" target="#b44">[45]</ref> and Mish activation functions <ref type="bibr" target="#b32">[33]</ref> each, as depicted in <ref type="figure">Fig. 2b</ref>. Note hereby that the IN layer is equivalent to a standard batch normalization (BN) layer <ref type="bibr" target="#b15">[16]</ref> in our case, since we use a batch size of 1. However we would still use IN over BN with larger batch sizes, since we modify this layer for one of our injection strategies (see Sect. 3.4), where we explicitly modulate feature tensors according to the transfer function, which would be different for every item in a batch. Also we chose Mish over a standard ReLU activation, since it performs slightly better on a wide range of computer vision tasks <ref type="bibr" target="#b32">[33]</ref> and we found this to also be true for our task in our early experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Neural Network Architecture</head><p>During encoding, we trilinearly downsample the feature volumes to half resolution between each ConvBlock, while doubling the number of convolutional filters in the next ConvBlock, as it is conventional in CNNS. We limit the number of filters to a maximum of 512 to conserve memory. The decoder part is symmetric to the encoder, trilinearly upsampling and halving the number of filters every step. Like U-Net we employ skip connections at every spatial resolution and concatenate the encoder's feature tensors with the decoded feature tensors before each ConvBlock in the decoder to make early low-level features available during decoding.</p><p>Note that in contrast to the 3D U-Net by Ç içek et al. <ref type="bibr" target="#b8">[9]</ref>, we chose to encode the volume all the way down to a spatial resolution of 1 <ref type="bibr" target="#b2">3</ref> . This makes the network deeper and enables it to learn more complex representations. Furthermore, one of our injection strategies requires the volume input to be reduced to a vector and we wanted to use the same common base network for all strategies to enable a fair comparison.  <ref type="figure">Fig. 2</ref>: An overview of the main architecture that operates on the volume data. The architecture consists of ConvBlocks with two 3D-convolutional layers each. The ConvBlocks are parameterized by the number of filters inside the convolutional layers. Using those ConvBlocks, the architecture forms a typical encoder decoder architecture in a U-shape, similar to a 3D U-Net <ref type="bibr" target="#b45">[46]</ref>. Note that between each ConvBlock in the encoder / decoder, we downsample / upsample the volume by a factor of two and double / halve the number of filters for the next ConvBlock respectively. The arrows from left to right denote skip connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Transfer Function Representation</head><p>In Sect. 3.1 we defined two types of raw representations for transfer functions, namely a set S of 2D TF points consisting of intensity and opacity, and a discretized representation T in the form of a 1D texture. Unfortunately neither of those representations is compatible with the input layer of our CNN, so we need to derive special TF representations that we can then inject into the network. We define two types of transfer function representations in order to lastly describe our injection strategies: implicit and explicit representations. An implicit representation modifies the actual volume data to implicitly include the transfer function information. Implicit representations are therefore volumetric representations that are directly compatible with the input layer of our 3D CNN. In contrast to that, explicit representations use TF Extractors to extract an explicit TF descriptor from raw transfer function data. The TF descriptor is a feature vector containing a high-level description of the transfer function that is learned by the TF extractor network during training. The explicit representation approach is thereby analogous to existing late-fusion concepts <ref type="bibr" target="#b41">[42]</ref> of different modalities in neural networks, where a feature vector is extracted separately for each modality and then fed into a final network making the prediction. In our case the feature extractors are the convolutional encoder and the TF extractor, while the convolutional decoder makes the final prediction. Note that our extracted TF descriptor is also not directly compatible with the CNN, however it has a fixed length and allows us to inject the TF information in several ways that we explain in Sect. 3.4. In the following we present the implicit and explicit representations that we compare in this work. Explicit point-based representation The explicit point-based representation is based on the raw TF representation S consisting of a list of points. While this representation is very efficient in representing a piece-wise linear transfer function, it is inherently incompatible with CNNs due to its unstructured nature. The unstructured nature arises from the fact that the set S may contain a varying number of points depending on the transfer function. Unfortunately this varying size prevents us to use the representation directly in a CNN, since every tensor in a CNN needs to have a fixed shape.</p><p>In order to solve the problems with order independence and varying length of this representation, we looked at promising approaches from point cloud learning that deal with very similar problems. We design our TF extractor similar to PointNet <ref type="bibr" target="#b4">[5]</ref>, which has proven very successful in extracting meaningful features from unstructured point data. The exact architecture is illustrated in <ref type="figure">Fig. 3a</ref> and uses three shared multilayer perceptrons (MLP) to describe each point as a feature of size 64 and reduces the set of points to a fixed-length TF descriptor using max pooling. While this fixed-length feature vector is still not directly compatible with the input layer of the CNN, it is compatible with different strategies explained in Sect. 3.4. Explicit texture-based representation This representation is based on the raw discretized TF representation T , as defined in Sect. 3.1, that resembles a 1D texture as it is common in DVR. This 1D texture representation is also not directly compatible with 3D CNNs, which is why we again extract a TF descriptor from this raw representation. Note that the 1D texture is already a fixed-length 1-dimensional representation of the transfer function that would be compatible with our injection strategies. The problem with using the TF texture directly arises from the fact that due to the discretization, the resulting vector is either unnecessarily large and sparse when using high resolutions or loses a lot of information when using low resolution. The 1D texture is therefore a very inefficient representation for neural networks.</p><p>To solve this problem we propose to again extract an efficient TF descriptor from high resolution TF textures using a 1D convolutional network. We chose 1D CNNs due to their parameter efficiency and their success on other 1D modalities like time-series data <ref type="bibr" target="#b11">[12]</ref>. Our 1Dconvolutional TF extractor is illustrated in <ref type="figure">Fig. 3b</ref>. The extractor CNN consists of three strided convolution layers (including normalization and activation), an average pooling layer and lastly an MLP that outputs the TF descriptor. Implicit representation The explicit representations use special extractors to describe transfer functions as low dimensional vectors that are fed into the network through specialized injection strategies (compare Sect. 3.4). In contrast to that, the implicit representation incorporates the transfer function directly into the volume, preserving the volumetric shape and thus being directly compatible with CNNs. We obtain the implicit representation by directly applying the transfer function to the input volume, resulting in an opacity volume capturing the transfer function information. The implicit representation allows us to combine the transfer function with basically any volumetric feature tensor inside our network, while explicit representations can only be combined with scalars or vectors. Note that this representation has the benefit of directly representing the optical properties spatially, while the network has to learn this spatial relation itself when using explicit representations.  The images on the right compare results of different injection strategies (all difference images are scaled by a factor of 3). We can see that the strategies using implicit representations b) and f) produce the best results, which falls in line with our recorded metrics in <ref type="table">Table 1</ref>. The explicit representations perform consistently worse and mostly coincide with our recorded metrics, with the exception of the Texture-based AdaIN approach. This approach turns out to be instable in practice and produces mixed results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Injection Strategies</head><p>An injection strategy describes a method of feeding a transfer function representation into the CNN. In general implicit and explicit TF representations require different strategies, since explicit representations have the shape of a vector and the implicit representation has the shape of a volume. Here we outline and describe possible injection strategies for all these scenarios. For the implicit representations we test two different injection strategies. The Preclassification strategy makes direct use of the volumetric shape of the implicit representation and injects the combined volume and TF information, in the form of an opacity volume, directly into the CNN's input layer. Since this fine-grained spatial representation of the TF might be useful information to many layers in our CNN, we further test Global Concatenation which is a multi-scale approach that injects the opacity volume on every scale.</p><p>For the explicit TF representations we test two approaches that have been very successful in combining different modalities in other fields and thus deserve in-depth investigations in our work. The first of the two approaches is Latent Concatenation, which combines both the extracted volume and TF information in the latent space of our network. The other approach is adaptive instance normalization (AdaIN) <ref type="bibr" target="#b14">[15]</ref> that modulates the feature tensors in the decoder using the transfer function. In the following we explain those strategies in detail. Preclassification As shown in <ref type="figure" target="#fig_2">Fig. 6a (green arrow)</ref>, the preclassification strategy directly injects the opacity volume representation τ(x) into the CNN's input layer instead of the intensity volume x. With this strategy the volume and transfer function information is jointly processed by the convolutional encoder and a combined representation is learned. In a sense this is similar to the approach used in deep shading techniques <ref type="bibr" target="#b33">[34]</ref>, where relevant geometric and optical information is provided to a 2D CNN in an image-based form. This strategy has the benefit of directly representing the TF information spatially, providing fine structural details in opacity directly to the CNN, compared to the strategies based on explicit representations that do not have this spatial relation. Note that as a downside, the preclassification strategy requires the re-execution of the full network upon transfer function change, while other representations can omit the execution of the convolutional encoder. Global Concatenation With global concatenation we use a multi-scale version of the implicit opacity volume representation τ(x) to inject the transfer function information at multiple scales into the network. This is motivated by the general success of multi-scale architectures for computer vision problems <ref type="bibr" target="#b24">[25]</ref>. Injecting the opacity volume at multiple scales allows the convolutional layers to focus on feature extraction, rather than wasting capacity to remember opacity structure that we can readily provide. For the global concatenation strategy the opacity volume is trilinearly downsampled to all resolutions present in the network and concatenated with the output of each encoder ConvBlock, effectively combining extracted features and opacity in one tensor. The combined opacity and feature volume is then made available to the decoder ConvBlocks at all scales through the skip connections, as illustrated in <ref type="figure" target="#fig_2">Fig. 6a (blue</ref>  (a) Injections for Implicit Representations. In the Preclassified strategy (green) we first apply the transfer function to the volume to get an implicit representation that combines volume and TF information in one tensor, which is fed into the CNN. The Global Concatenation (blue) uses the same implicit representation, but injects it in the skip connections through concatenation.  (b) Injections for Explicit Representations. The Latent Concatenation strategy (green) computes an explicit representation of the TF which is concatenated in the latent space of the CNN. The Adaptive Instance Normalization (AdaIN) strategy (blue) uses an explicit representation to modulate the feature tensors in the decoder through the normalization layers. Note that with AdaIN, each normalization layer has an additional MLP to predict the appropriate means and variances, increasing the parameters in the decoder.  Note that the prediction from the model trained with MSE lacks very bright and very dark extremes and mostly predicts an average occlusion. In contrast to that, the DSSIM-2D and DSSIM-3D models accentuate such bright and dark spots, often overshooting. Using a combination of DSSIM and MSE as loss alleviates the problems of each individual loss and results in predictions with good average occlusion and highlights. The volume is from the CQ500 dataset <ref type="bibr" target="#b7">[8]</ref>.</p><p>Latent Concatenation As discussed in Sect. 3.2, our proposed CNN resembles an encoder decoder architecture. For the latent concatenation strategy we consider the encoder and decoder separately as a convolutional feature extractor for volumes, and a generative network that produces volumetric ambient occlusion respectively. The encoder hereby processes the input volume and extracts a low-dimensional latent vector of length 512 that contains highly compressed global information about the input volume. This latent vector can thus be seen as input to the generative decoder. For the latent concatenation strategy we propose to combine the this latent vector with the explicit TF descriptor by concatenation as depicted in <ref type="figure" target="#fig_2">Fig. 6b (green arrow)</ref>. The concatenated vector contains global information of both the volume and the transfer function and thus conditions the decoder to predict illumination that is coherent with both the volume modality and the transfer function. This kind of strategy has proven very useful in conditional generative modeling <ref type="bibr" target="#b31">[32]</ref> and deserves investigation for our volumetric illumination problem. Also note that this strategy will be beneficial during inference, compared to Preclassification, due to the fact that it only requires the TF extractor and the decoder to be re-executed upon transfer function updates, while the compressed volume representation from the encoder stays constant. Adaptive Instance Normalization Our last injection strategy is to feed the transfer function information to the network through TF-conditioned feature tensor modulation in the decoder network. This is achieved by replacing the instance normalization layers in the decoder with adaptive instance normalization (AdaIN) <ref type="bibr" target="#b14">[15]</ref> layers. Instance normalization (IN) <ref type="bibr" target="#b44">[45]</ref> normalizes an incoming tensor to zero mean and unit variance across spatial dimensions for each feature and for each item in a mini-batch. The normalized feature tensors are then scaled and shifted using learned parameters, as described by Equation 2.</p><formula xml:id="formula_2">IN(x) = γ x − μ x σ x + β<label>(2)</label></formula><p>In traditional IN the new scale γ and shift β of feature tensors is learned directly, while AdaIN uses a multi-layer perceptron (MLP) to predict appropriate scales γ and shifts β from external inputs. This approach has shown great success in both style transfer <ref type="bibr" target="#b14">[15]</ref> and conditional image generation <ref type="bibr" target="#b20">[21]</ref> tasks recently and enabled networks from these domains to control their outputs based on external information, which seems directly applicable to our problem. Thus, for this injection strategy we propose to replace the IN layers in the decoder with AdaIN layers and use our learned explicit TF descriptor as their modulation input. For each AdaIN layer, as depicted by the blue arrows in <ref type="figure" target="#fig_2">Fig. 6b</ref>, an additional MLP is needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training</head><p>We train our network in a supervised fashion using stochastic gradient descent. As optimizer we use rectified Adam <ref type="bibr" target="#b26">[27]</ref> with Lookahead <ref type="bibr" target="#b49">[50]</ref> with the default parameters and a learning rate of 0.001. As batch size we use 1, since we cannot fit larger batches in memory with the proposed architecture. Simulating larger batch sizes using gradient accumulation did not improve training performance in our tests, however we did not analyse this in full detail. Our network was implemented in PyTorch <ref type="bibr" target="#b34">[35]</ref> and is trained using mixed precision. The training takes around 20 hours on a single RTX 2080 Ti GPU, requiring 10.5 GB of GPU memory. Loss The loss function defines the objective of the neural network and is critical to produce accurate volumetric ambient occlusion. We tested several loss functions and propose to use a combination of mean squared error (MSE) and structural dissimilarity index (DSSIM). The final loss L (p,t) of a prediction p and target t is defined in Equation 3 for volumes of shape</p><formula xml:id="formula_3">W × H × D with N = W • H • D: L (p,t) = 1 − (2μ p μ t + c 1 )(2σ pt + c 2 ) (μ 2 p + μ 2 t + c 1 )(σ 2 p + σ 2 t + c 2 ) + α N N ∑ i (p i − t i ) 2 (3) = DSSIM(p,t) + α • MSE(p,t)</formula><p>Hereby c 1 = 0.01, c 2 = 0.03 are small constants for numerical stability and μ x , σ 2</p><p>x and σ xy are the means, variances and covariances within a local neighborhood for all voxels of x (and y) respectively. α is a hyperparameter to balance the two losses and we empirically chose α = 5 based on our experiments, however we found that the training is not very sensitive to this parameter.</p><p>Note that traditional image-based SSIM <ref type="bibr" target="#b47">[48]</ref> uses a 2D neighborhood to compute local means, variances and covariances. We use this imagebased method on all slices along the z dimension and use the average over all slices to form the DSSIM-2D loss. Analogously, we define the DSSIM-3D loss, which uses 3D neighborhoods instead, in order to better assess structural similarity along the z dimension. <ref type="table">Table 1</ref>: Performance comparison of the injection strategies (columns) for different loss functions (rows). We report SSIM, MSE and inference time for each method. The Preclassified strategy in general performs the best while being the slowest. While DSSIM-2D results in the best test SSIM and DSSIM-3D + MSE in the best MSE for this strategy, we determine the DSSIM-2D + MSE model to perform the best overall since it nearly matches our best results in both SSIM and MSE. The inference times were measured on an RTX 2070 and mostly show that strategies that can omit execution of the encoder during inference get a slight performance advantage over the Preclassified strategy. Mixed precision training Training 3D convolutional neural networks on large volume data is very computationally expensive and requires a lot of memory. In order to counter current hardware limitations we use mixed precision <ref type="bibr" target="#b30">[31]</ref> for both training and inference. Traditionally neural networks are trained with 32-bit floating point weights and all computations are performed with 32-bit precision. We use 16bit floating point computations and weights in all layers except for instance normalization, due to numerical stability. While we still hold a full 32-bit precision set of all parameters, as suggested by NVIDIA's implementation 2 , we can drastically reduce the memory consumption and running time of our network. In fact, using this techniques enables us to use a comparably large network and volume resolution <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REPRESENTATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION</head><p>In this section we present our evaluation of DVAO. First, we compare the different transfer function representations and injection strategies. After identifying the best strategy, we further investigate the effect of the loss function and the effect of changing the network size. Lastly, we investigate generalization across structures and modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Representations and Injection Strategies</head><p>In order to evaluate all combinations of transfer function representation, injection strategy and loss function that we proposed in Sect. 3.4, we run 30 different training runs, by training 6 different architectures with 5 different losses each. The obtained results, which we report via error metrics and generated images, are generated from a heldout set not seen during training. Our results are summarized in <ref type="table">Table 1</ref>, while <ref type="figure">Fig. 4</ref> shows example predictions for the best performing model of each strategy. As laid out in <ref type="table">Table 1</ref>, both the model with the highest SSIM (0.879) and the model with the lowest MSE (0.007) use the Preclassified injection strategy. Comparing the numbers with actually rendered results in <ref type="figure">Fig. 4b</ref> confirms that indeed the Preclassified strategy achieves the best results. Since the best scores on our error metrics for this technique are achieved by different models, we found the Preclassified method using the DSSIM-2D + MSE loss to be the overall best model, since it is very close to the best MSE and SSIM at the same time. This is also further investigated in the loss function comparison below in Sect. 4.2. The Global Concatenation strategy scores second place according to our metrics and <ref type="figure">Fig. 4f</ref> clearly confirms this placement. The latent concatenation strategies <ref type="figure">(Fig. 4c and 4d</ref>) produce a lot blurrier results, which makes sense considering that this injection strategy combines the transfer function with a volume representation (the latent vector) that has its spatial dimensions completely reduced. By injecting the TF information only in the bottleneck, it is hard for the network to reconstruct fine structures accurately. Lastly we can see that while the AdaIN-based strategies subjectively perform the worst visually in <ref type="figure">Fig. 4g and 4h</ref>, they can still match the latent concatenation strategies in terms of error metrics. We found that the AdaIN-based methods are rather instable and produce very mixed results on all datasets. However, since they performed generally worse than the implicit techniques, we did not further investigate this behavior.</p><p>Overall, we found that the implicit representations usually work better than the explicit representations, with the Preclassified strategy performing best in all our tests. The rendered results shown in <ref type="figure">Fig. 4</ref> confirm this finding. We believe, that the Preclassified strategy works best, because the network can learn the volume representation jointly with the transfer function and has in general more parameters to process the transfer function information through the convolutional encoder. In contrast to that, the other methods extract the volume and transfer function information separately and only process them jointly in the decoding part of the network. Nevertheless, to investigate this hypothesis, more experiments are required in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Loss functions</head><p>We investigate five different loss functions, namely the structural dissimilarity index in 2D (DSSIM-2D) and 3D (DSSIM-3D) (see Sect. 3.5), as well as MSE, and combinations of DSSIM and MSE. The resulting prediction performance comparison is summarized in <ref type="table">Table 1</ref>.</p><p>Starting with the simple MSE loss, we found that our network is able to already learn rough and blurry volumetric ambient occlusion. However, while on average producing reasonable ambient occlusion, the predictions often lack fine details and contrast (compare for example the gap between skin and skull or the throat region in <ref type="figure" target="#fig_3">Fig. 5a</ref>). To counter this issue we trained our networks using DSSIM. The models trained with DSSIM produce more fine structured details in the ambient occlusion, while often deviating from the ground truth in overall brightness (compare <ref type="figure" target="#fig_3">Fig. 5b and 5d</ref>). This falls in line with our expectations, since DSSIM is known to tolerate slight variations in brightness more than typical MSE <ref type="bibr" target="#b47">[48]</ref>. Finally we found that the combination of DSSIM and MSE combines the advantages of both losses, and is thus able to produce ambient occlusion volumes with a sufficient degree of details, while staying close to the overall brightness of the ground truth. 2D vs. 3D SSIM As detailed in Sect. 3.5, we investigated both 2D image-based SSIM and 3D volume-based SSIM. In the evaluation we did not find large differences between the two, as the resulting models perform very similar (Test SSIM 0.879 vs 0.874). We hypothesized that the 3D SSIM should perform better, since the 2D SSIM only takes into account similarity within each slice of the volume, as compared to small bricks around each voxel. We explain this result with the large receptive field that is used to predict each voxel. The prediction of a single output voxel takes a large 3D neighborhood into account, due to the receptive field of the prior convolutional layers. Even though DSSIM-2D does not penalize missing similarity along the z dimension directly, the network is still forced to learn a coherent 3D structure internally, since the 3D convolutions incorporate a 3D neighborhood by design, regardless of the loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Network Size</head><p>After finding the best injection strategy and loss function, we evaluated the effect of changing the number of parameters in the neural net. The goal of this experiment was to determine if DVAO has a sufficient number of parameters or if it could be reduced in order to speed up our technique. To control this parameter we adjusted the number of filters in the convolutional layers of our main architecture, since this allows us to use a different amount of parameters without changing the overall architecture.</p><p>In addition to the default model, which starts with 16 filters in the first ConvBlock, doubling upon downsampling, we trained models with 8 and 24 filters in the first convolution. With 8 filters the network has roughly half the amount of parameters of the default size and 24 filters maxes out our memory capacity, representing the largest model we can train in practice. The results are provided in <ref type="table" target="#tab_6">Table 2</ref> and indicate that the model improves only very slightly when using more parameters, while the inference time increases significantly. For the large network the actually rendered results are very close to the default network size, while halving the number of filters significantly reduces the quality of the predictions, both in terms of error metrics and also visually. Renders for qualitative comparison can be found in the supplemental material. Note that while the smaller model significantly reduces the inference time from 411ms to 239ms on average, the practical impact of the reduced inference time is rather low for our application, due to the fact that the network is only executed when the volume or transfer function is changed. However, other applications might benefit from trading speed for quality. 0.880 0.007 572</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Generalization</head><p>In the previous experiments we evaluated DVAO's performance on held-out CT data that was not used for training, however the modality was CT only and consisted only of human heads. In this section, we apply our method to other structures and other modalities, in order to investigate DVAO's generalization capabilities. As displayed in the supplemental material, we found that DVAO is able to predict ambient occlusion in computed tomography scans of animals. Comparing our predictions against the ground truth we achieve a SSIM of 0.786 on the mouse micro-CT dataset and a SSIM of 0.866 on the chameleon dataset. This indicates that DVAO generalizes beyond the structure of human head data. Furthermore, <ref type="figure" target="#fig_4">Fig. 7</ref> shows our results on different input modalities, namely magnetic resonance imaging (MRI) and electron microscopy (EM) volume data. For the MRI data we achieve a SSIM of 0.751 and for the EM a SSIM of 0.869. As it can be seen, DVAO is still able to predict accurate ambient occlusion, despite having never encountered these modalities during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>In this section we discuss the limitations of our approach, and the implications of our findings for volume illumination learning in general.</p><p>As we have demonstrated in our evaluation, DVAO is able to produce detailed volumetric ambient occlusion on a variety of structures and modalities, making it a valid choice to enhance classical DVR as it is still often used in medicine. Our approach can easily be integrated in existing DVR pipelines through the addition of a simple texture lookup, once predictions have been performed. However, our approach is limited to rather low resolution volumes of size 128 3 , since 3D CNNs become too computationally expensive with higher resolutions and are quickly limited by GPU memory. Nevertheless, since ambient occlusion is a rather low frequency illumination effect, and the original volume is provided in original resolution, this does not compromise image quality. While we cannot do predictions for each frame, as these would take up too much time, we only predict a new ambient occlusion volume, once the transfer function has been changed. As reported, this takes on average 411 ms with our best method, which we found sufficiently fast in practice to not restrict interactivity. This is demonstrated in our video in the supplemental material. Furthermore our predicted occlusion volumes can still differ significantly from the ground truth occasionally and especially deviations in overall brightness and contrast are noticeable in those cases. We discuss failures in visual quality further in the supplemental material. Our experiments with deep learning based volumetric illumination lead us to the following guidelines applicable to similar volume learning tasks with additional global information:</p><p>• When learning on volumes in the context of volume visualization, additional global unstructured information, such as the transfer function in our case, is best represented implicitly in the input volume itself. Combining the global information at the very beginning of the CNN allows to use the full feature extraction capabilities of the neural network, which produces better results than using an explicitly learned representations of global information.</p><p>• When predicting volumetric illumination, the structural similarity index is a very effective objective function and should be used in addition to standard regression losses like MSE or mean absolute error. According to our findings the 2D image-based SSIM also performs well on 3D data, while being significantly cheaper to compute.</p><p>• Reducing the standard floating point precision in neural networks to 16 bit makes volume learning more feasible, allowing for higher resolutions, more parameters and faster computation. We did not find significant downsides to mixed precision training and recommend it in volume learning scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implicit vs. Explicit TF Representation</head><p>In this section we further discuss pros and cons for implicit and explicit TF representations and argue why our explicit representations deserve future investigations, despite performing inferior in the reported tests. As we stated before, the implicit representations clearly make the learning task easier for the network, because it is not required to learn the transfer function application on top of the illumination task, as the explicit approaches do. Furthermore, using a preclassified volume to represent the TF information makes the network generally invariant to the actual input modality and helps greatly in generalizing the approach to different modalities, since only the structures of the new data are unseen to the network, but not the general input scale.</p><p>On the other hand, the implicit approach steps away from the paradigm of training neural nets on raw input data to learn their own representations, instead of hand-engineering features for the network. On top of that, the TF application can be expensive for large volumes and the preclassified approach must always run the full network during inference, which leads to increased running times and memory requirements. Our explicit TF approaches can significantly save running time and, while they were less successful in our experiments, similar techniques have achieved great success in generative tasks for images <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32]</ref>. Given that the explicit approaches have to solve a significantly harder task, they might require larger networks with more parameters to solve the volumetric AO task as well as the implicit approaches. We hope to explore this in future work, when new hardware enables the training of even larger networks. Training one of the explicit approaches with sufficient quality might also enable follow-up work, using the potentially very efficient TF representations that are learned jointly to design new, or edit existing transfer functions using the learned feature space, instead of using the intensity of a given modality as input to the transfer function. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison with Traditional Methods</head><p>Here we compare our approach with existing methods for volumetric AO. Specifically, we chose to compare our method to the Local Ambient Occlusion (LAO) approach published by Hernell et al. <ref type="bibr" target="#b12">[13]</ref>, because this approach directly produces an occlusion volume, like ours, and thus we can compare occlusion volumes directly. Furthermore, LAO, like our approach, only needs to be executed when the volume or transfer function is changed. Since there was no implementation publicly available, we re-implemented LAO in CUDA. For the experiment, the volume resolution is again 128 3 and the TF resolution is 4096.</p><p>Our time measurements include memory transfers of the inputs and outputs for both techniques, accounting for a maximum of 8 ms, which may be saved with further optimization. <ref type="table">Table 3</ref> shows how our approach performs compared to LAO with different amounts of rays per voxel. In terms of SSIM, our approach performs comparable with the 32-ray version of LAO, which takes around 111 ms to run, compared to DVAO's 411 ms. We also scaled up the number of rays such that LAO takes roughly the same amount of time as our approach. With 190 rays, LAO takes 417 ms, while being very close to our ground truth with a SSIM of 0.96 and an MSE of 8e − 5. LAO reaches a relatively low MSE with few samples, while the SSIM is still comparatively low. Similarly, the qualitative comparison in <ref type="figure">Fig. 8</ref> shows that LAO-32 still has a lot of artifacts compared to DVAO, which performs quite similar quantitatively. A more extensive qualitative discussion can be found in the supplemental material. Lastly, we compared the approaches in terms of memory usage. LAO requires 42 MiB of GPU RAM, compared to our expensive neural net that requires 1322 MiB during inference. Clearly, our approach is not yet competitive with existing techniques, however we believe it is a good starting point for deep learning based volumetric illumination, and has great potential to improve through follow-up research. <ref type="table">Table 3</ref>: Comparison of our method with LAO <ref type="bibr" target="#b12">[13]</ref>. We vary the amount of rays used in LAO to compare running times for similar quality (LAO-32) and quality with similar running time  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS &amp; FUTURE WORK</head><p>In this work we show that neural networks are capable of learning volumetric illumination. We demonstrate this on the example of volumetric ambient occlusion on a variety of modalities. Therefore, we compared six different injection strategies for the incorporation of global information, like a transfer function, which is in general not compatible with CNN architectures, but is still required to solve most illumination tasks. Based on our experiments with volumetric ambient occlusion, we have derived guidelines for volumetric illumination learning, which we believe to be also applicable in similar scenarios. While our approach is not able to compete with algorithmic approaches for AO in terms of quality or performance yet, we see this work as a starting point for future research in volumetric illumination using neural networks. From this starting point we hope to see increased interest in DL-based volumetric illumination research, and we believe with further advances in the field, DL-based approaches might soon match or even surpass algorithmic approaches in terms of quality. Future DL-based approaches might also extend to other GI tasks and predict multiple effects simultaneously without fundamentally changing the network, providing a flexible tool for volume rendering. While we do not expect DL-based approaches to be real-time capable without significant graphics hardware advances, semi-real-time implementations like DVAO, where re-execution is not required upon view-change, may be beneficial in many scenarios. Furthermore, DL approaches might be increasingly competitive in terms of running time with increased complexity of the illumination task, where algorithmic approaches have no real-time solution either, like multiple scattering.</p><p>Having explored design decisions for volumetric lighting with neural nets in this paper, we will focus our future research on extending the applications to a wider variety of GI effects, including higher frequency effects that require the incorporation of additional unstructured global information, like light sources and camera parameters. Additionally, splitting volumes into smaller bricks might be an interesting direction for future work to increase the resolution of the predictions. Lastly, there is still further research needed on better transfer function representations, since the currently best performing representation is very expensive during inference. One possible direction for such a representation might be changing the transfer function input space to a jointly learned feature space.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 :Fig. 4 :</head><label>34</label><figDesc>The transfer function extractor architectures. Comparison of injection strategies. Images a) and e) on the left show the fully rendered ground truth and ambient occlusion respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>128</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 :</head><label>6</label><figDesc>Overview of the injection strategies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>(a) MSE (b) DSSIM-2D (c) DSSIM-2D + MSE (d) DSSIM-3D (e) DSSIM-3D + MSE (f) Ground Truth Comparison of predictions from models trained with different loss functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 :</head><label>7</label><figDesc>Renders of MRI and EM modalities. The top row shows renders without AO, the middle row shows renders with AO and the bottom row shows the AO volume. Those renders demonstrate that our approach generalizes to other modalities like MRI and EM, despite only being trained on CT data. The MRI scan is from the brainweb dataset<ref type="bibr" target="#b9">[10]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>• Dominik Engel is with Ulm University. E-Mail: dominik.engel@uni-ulm.de • Timo Ropinski is with Ulm University and Linköping University</figDesc><table /><note>(Norrköping). E-Mail: timo.ropinski@uni-ulm.de Manuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication xx xxx. 201x; date of current version xx xxx. 201x. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org. Digital Object Identifier: xx.xxxx/TVCG.201x.xxxxxxx</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>The PointNet-like extractor used for the explicit point-based representation. Each point is individually processed by the shared MLPs to produce a feature descriptor of size 64. The feature points are reduced using max pooling to produce the TF descriptor.</figDesc><table><row><cell>TF Points</cell><cell>MLP 64</cell><cell>MLP 64</cell><cell>MLP 64</cell><cell>MaxPool</cell><cell>TF Desc</cell></row><row><cell cols="2">(a) Conv 8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TF Texture</cell><cell>Conv 16</cell><cell>AvgPool</cell><cell>Flatten</cell><cell>MLP 64</cell><cell>TF Desc</cell></row><row><cell></cell><cell>Conv 32</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>(b) The 1D convolutional extractor used for the explicit texture-based representation. The Conv blocks each depict a combination of 1D convolution (stride=2), Mish activation and instance normalization. After three of those blocks the 32 × 512 feature is pooled to a spatial resolution of 16, flattened and fed into the final MLP which outputs the TF descriptor of size 64</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>arrows).</figDesc><table><row><cell>Opacity</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Volume</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Intensity 128 3 × 1</cell><cell>128 3 × 16</cell><cell>64 3 × 32</cell><cell>32 3 × 64</cell><cell>16 3 × 128</cell><cell>8 3 × 256</cell><cell>4 3 × 512</cell><cell>2 3 × 512</cell><cell>1 ×512</cell><cell>1 ×512</cell><cell>1 ×512</cell><cell>2 3 × 512</cell><cell>4 3 × 512</cell><cell>8 3 × 256</cell><cell>× 128 16 3</cell><cell>32 3 × 64</cell><cell>64 3 × 32</cell><cell>128 3 × 16</cell><cell>Ambient Occlusion Volume 128 3 × 1</cell></row><row><cell>Volume</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>128 3 × 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>The effect of increasing the number of network parameters. No. Filters depicts the number of filters in the first ConvBlock in our architecture, which is doubled after each downsampling step.</figDesc><table><row><cell cols="3">NO. FILTERS SSIM MSE INFERENCE TIME (ms)</cell></row><row><cell>8</cell><cell>0.843 0.013</cell><cell>261</cell></row><row><cell>16</cell><cell>0.875 0.008</cell><cell>411</cell></row><row><cell>24</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Comparison of DVAO and LAO with different amounts of rays.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>).</cell></row><row><cell></cell><cell cols="4">DVAO LAO-16 LAO-32 LAO-190</cell></row><row><cell>SSIM</cell><cell>0.81</cell><cell>0.74</cell><cell>0.83</cell><cell>0.96</cell></row><row><cell>MSE</cell><cell>7e-3</cell><cell>2e-3</cell><cell>7e-4</cell><cell>8e-5</cell></row><row><cell>TIME (ms)</cell><cell>411</cell><cell>87</cell><cell>111</cell><cell>417</cell></row><row><cell>VRAM (MiB)</cell><cell>1322</cell><cell>42</cell><cell>42</cell><cell>42</cell></row><row><cell>(a) DVAO</cell><cell>(b) Ground Truth</cell><cell></cell><cell>(c) LAO-190</cell><cell>(d) LAO-32</cell></row><row><cell>Fig. 8:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/nvidia/apex</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was partially funded by the Deutsche Forschungsgemeinschaft (DFG) under grant 391107954 (Inviwo). The renderings have been produced using Inviwo <ref type="bibr" target="#b18">[19]</ref> (www.inviwo.org).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Anisotropic ambient volume shading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ament</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dachsbacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1015" to="1024" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ambient volume scattering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ament</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sadlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2936" to="2945" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A generative model for volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1636" to="1650" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Interactive reconstruction of Monte Carlo image sequences using a recurrent denoising autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R A</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Kaplanyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schied</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lefohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nowrouzezahrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Q</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaichun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.16</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="1063" to="6919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep-learning-assisted volume visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cardone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Krokos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Subramaniam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varshney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1378" to="1391" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep-Learning-Assisted Volume Visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cardone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Krokos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Subramaniam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varshney</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2018.2796085</idno>
	</analytic>
	<monogr>
		<title level="m">Conference Name: IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2019-02" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1378" to="1391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Development and Validation of Deep Learning Algorithms for Detection of Critical Findings in Head CT Scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tanamala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Biviji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">G</forename><surname>Campeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Venugopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05854</idno>
		<idno>arXiv: 1803.05854</idno>
		<imprint>
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ö</forename><surname>Içek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46723-849</idno>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention -MICCAI 2016</title>
		<editor>S. Ourselin, L. Joskowicz, M. R. Sabuncu, G. Unal, and W. Wells</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Brainweb: Online interface to a 3D MRI simulated brain database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Cocosco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kollokian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K S</forename><surname>Kwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Pike</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Evans</surname></persName>
		</author>
		<editor>NeuroImage. Citeseer</editor>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Real-time ambient occlusion and halos with summed area tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Díaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-P</forename><surname>Vázquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Navazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Duguet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="337" to="350" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep learning for time series classification: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">I</forename><surname>Fawaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Forestier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Idoumghar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="917" to="963" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Local ambient occlusion in direct volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hernell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ljung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ynnerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="548" to="559" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dnn-volvis: Interactive volume visualization supported by deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Pacific Visualization Symposium (PacificVis)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="282" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1501" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<idno>arXiv: 1502.03167</idno>
		<title level="m">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<imprint>
			<date type="published" when="2015-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Godil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Bullard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Terrill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Varshney</surname></persName>
		</author>
		<title level="m">Amitabh. Compressed volume rendering using deep learning</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep 3d convolution neural network for ct brain hemorrhage classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jnawali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Arbabshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">p. 105751C. International Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2018</biblScope>
		</imprint>
	</monogr>
	<note>In Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Inviwo-a visualization system with usage abstraction levels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jönsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Steneteg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sundén</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Englund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottravel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Falk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ynnerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hotz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ropinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Correlated photon mapping for interactive global illumination of time-varying volumetric data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jönsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ynnerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="901" to="910" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A Style-Based Generator Architecture for Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04948</idno>
		<idno>arXiv: 1812.04948</idno>
		<imprint>
			<date type="published" when="2019-03" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">tional neural networks for classification of functional connectomes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jamison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuceyeski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="137" to="145" />
		</imprint>
	</monogr>
	<note>3d convolu</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Exposure render: An interactive photo-realistic volume rendering framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kroes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">H</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Botha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Smooth probabilistic ambient occlusion for volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kroes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eisemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Rendering Techniques</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">475</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">About the influence of illumination models on image comprehension in direct volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lindemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ropinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1922" to="1931" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03265</idno>
		<idno>arXiv: 1908.03265</idno>
		<title level="m">On the Variance of the Adaptive Learning Rate and Beyond</title>
		<imprint>
			<date type="published" when="2020-04" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">State of the art in transfer functions for direct volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ljung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krüger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Groller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hadwiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ynnerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="669" to="691" />
			<date type="published" when="2016" />
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Interactive dynamic volume illumination with refraction and caustics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Magnus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bruckner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="984" to="993" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Optical models for direct volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Max</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="108" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Mixed Precision Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mish</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08681</idno>
		<idno>arXiv: 1908.08681</idno>
		<title level="m">A Self Regularized Non-Monotonic Neural Activation Function</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep Shading: Convolutional Neural Networks for Screen Space Shading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nalbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Arabadzhiyska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ritschel</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.13225</idno>
		<idno>doi: 10.1111/ cgf.13225</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.13225" />
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="65" to="78" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An intelligent system approach for probabilistic volume rendering using hierarchical 3d convolutional sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Jeong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="964" to="973" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>U-Net</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<idno type="arXiv">arXiv:1505.04597</idno>
		<idno>arXiv: 1505.04597</idno>
		<title level="m">Convolutional Networks for Biomedical Image Segmentation</title>
		<imprint>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Interactive volume rendering with dynamic ambient occlusion and color bleeding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ropinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Meyer-Spradow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Diepenbrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mensmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hinrichs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ambient occlusion effects for combined volumes and tubular geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Grosset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="913" to="926" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A directional occlusion shading model for interactive direct volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pegoraro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Boulanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bouatouch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="855" to="862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Early versus late fusion in semantic video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th annual ACM international conference on Multimedia</title>
		<meeting>the 13th annual ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="399" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A multidirectional occlusion shading model for direct volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Šoltészová</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bruckner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Viola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="883" to="891" />
			<date type="published" when="2010" />
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Local prediction models for spatiotemporal volume visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tkachev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Instance Normalization: The Missing Ingredient for Fast Stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<idno>arXiv: 1607.08022</idno>
		<imprint>
			<date type="published" when="2017-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">3D convolutional neural network for feature extraction and classification of fMRI volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1109/PRNI.2018.8423964</idno>
	</analytic>
	<monogr>
		<title level="m">2018 International Workshop on Pattern Recognition in Neuroimaging (PRNI)</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Ospray-a cpu ray tracing framework for scientific visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Amstutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brownlee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Knoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jeffers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Günther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Navrátil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="931" to="940" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simoncelli</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2003.819861</idno>
	</analytic>
	<monogr>
		<title level="m">Conference Name: IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2004-04" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="600" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Volumetric Isosurface Rendering with Deep Learning-Based Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thuerey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Westermann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06520</idno>
		<idno>arXiv: 1906.06520</idno>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.08610</idno>
		<idno>arXiv: 1907.08610</idno>
		<title level="m">Lookahead Optimizer: k steps forward, 1 step back</title>
		<imprint>
			<date type="published" when="2019-12" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
