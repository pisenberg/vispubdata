<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Video Visualization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gareth</forename><surname>Daniel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Wales Swansea</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Wales Swansea</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Video Visualization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CR Categories: I.2.10 [Artificial Intelligence]: Vision and Scene Understanding-Video Analysis</term>
					<term>I.3.8 [Computer Graphics]: Applications</term>
					<term>I.4.10 [Image Processing and Computer Vision]: Image Representation-Volumetric Video visualization, volume rendering, video surveillance, change detection, image-swept volume</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Video data, generated by the entertainment industry, security and traffic cameras, video conferencing systems, video emails, and so on, is perhaps most time-consuming to process by human beings. In this paper, we present a novel methodology for &quot;summarizing&quot; video sequences using volume visualization techniques. We outline a system pipeline for capturing videos, extracting features, volume rendering video and feature data, and creating video visualization. We discuss a collection of image comparison metrics, including the linear dependence detector, for constructing &quot;relative&quot; and &quot;absolute&quot; difference volumes that represent the magnitude of variation between video frames. We describe the use of a few volume visualization techniques, including volume scene graphs and spatial transfer functions, for creating video visualization. In particular, we present a stream-based technique for processing and directly rendering video data in real time. With the aid of several examples, we demonstrate the effectiveness of using video visualization to convey meaningful information contained in video sequences.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In February 2002, it was reported that there were 25 million CCTV cameras in operation worldwide <ref type="bibr" target="#b19">[Wakefield 2002]</ref>. In some parts of the world, such as the United Kingdom, it is estimated that on average a citizen is caught on security and traffic cameras 300 times a day. Along with digital camcorders, digitized movies, video conferencing and video emails that are also making their ways into everyday life, it is almost certain that there will be a multi-fold increase in video data in the coming years.</p><p>A video is a piece of ordered sequential data, and viewing videos is a time-consuming and resource-consuming process. For example, an increasing problem in the security industry is the ratio of surveillance cameras to security personnel. Imagine that security officers have to review an overnight collection of video tapes when they arrive at their desks in the morning. It is simply not possible for any security officer to study a large number of video tapes every- * e-mail: {csgareth, m.chen}@swansea.ac.uk day. It is hence highly desirable to develop methods for extracting and highlighting interesting features in video sequences.</p><p>Automated video processing is a research topic that is of significant importance to the security and entertainment industries. There is a rich collection of techniques for analyzing imagery data, and for computing various statistical indicators. However, there is a general lack of effective techniques to convey complex statistical information intuitively to a layperson such as a security officer, except using line graphs to depict 1D signal levels. Most of the techniques have not reached such an intelligent level that they can be relied upon to make decisions in place of a human. <ref type="figure">Figure 1</ref>: Visualization of four experimental videos based on different scenarios, namely walking, running, mischief and burglary. <ref type="bibr">IEEE Visualization 2003</ref><ref type="bibr">, October 19-24, 2003</ref>, Seattle, Washington, USA 0-7803-8120-3/03/$17.00 ©2003 IEEE</p><p>In this paper, we present a novel approach to the handling of a very large amount of video data. We propose to employ volume visualization techniques for "summarizing" video sequences, and to render video volumes into appropriate visual representations that can be used to assist in the decision making processes of a human operator. For example, every morning, security officers can be presented with one or a few visualizations for each surveillance camera that has been monitoring a premise during the previous night. <ref type="figure">Figure 1</ref> shows the visualization of four experimental videos of different staged activities that simulate some typical scenarios which may take place in a university building at night. From the visualizations, one can observe the levels and patterns of the activities recorded. We believe that such visualizations can convey much more information, especially spatial information, than a few statistical indicators or line graphs. With carefully prepared visualizations, the human vision system, perhaps the most intelligent vision system, is able to become accustomed to certain kinds of "normal" visual patterns, and react to unusual levels or patterns of activities that need further investigation. Video visualization can also be used to assist in processing videos, such as video segmentation, and video annotation.</p><p>Video data is a type of volume data. Hence the key to our approach is the volume visualization technology, which has been successfully and extensively deployed in medical imaging and scientific visualization. Localized statistical indicators of video data can also be represented in a volumetric form. This conceptual similarity allows us to utilize some powerful volume visualization and volume graphics techniques, such as, volumetric scene graphs, opacity and color transfer functions and spatial transfer functions.</p><p>In Section 2, we will briefly review the previous work on video processing, focusing on methods for change detection, and recent work on video cube rendering. In Section 3, we will propose three hypotheses and outline our contributions in this context. In Section 4, we will consider a conceptual pipeline for capturing, managing, processing, rendering and visualizing videos. We will briefly describe the design and development of a prototype system, called V 3 (short for Volume Visualization for Videos). In Section 5, we will discuss various ways for extracting and creating feature volumes, and we will focus on several image comparison metrics for detecting changes in video sequences. This is followed by Section 6, where we will describe the use of a set of volume modeling and rendering techniques for video visualization. In Section 7, we will present some visualization results and discuss the effectiveness of different change detection methods for presenting visual features in video visualizations. We will offer our concluding remarks and an indication of future work in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In order to overcome the sequential and time-consuming process of viewing video <ref type="bibr" target="#b21">[Yeo and Yeung 1997]</ref>, a noticeable amount of effort has been made, largely by the image processing and vision community, to devise methods for processing video data automatically. One of the research focuses is change detection. A common goal in change detection is to ascertain image differences that relate to object changes in a scene. <ref type="bibr" target="#b13">[Narasimhan et al. 2002</ref>] outlined many factors, such as reflectance, illumination and atmospheric conditions, which might complicate such a process. A variety of methods were proposed, including thresholding <ref type="bibr" target="#b14">[Rosin 1997</ref>], different image comparison metrics <ref type="bibr" target="#b23">[Young et al. 1999;</ref><ref type="bibr" target="#b24">Zhou et al. 2002]</ref>, morphological filters <ref type="bibr" target="#b15">[Stringa 2000</ref>], statistical models <ref type="bibr" target="#b0">[Brocke 2002</ref>], combined audio-visual analysis <ref type="bibr" target="#b17">[Tsekeridou et al. 2001]</ref>, linear dependence models <ref type="bibr">[Durucan and Ebrahimi 2001a;</ref><ref type="bibr">Durucan and Ebrahimi 2001b]</ref>, color edge detection <ref type="bibr" target="#b1">[Cavallaro and Ebrahimi 2001]</ref>, and homomorphic filtering <ref type="bibr" target="#b16">[Toth et al. 2000]</ref>. Many researchers studied video processing in the context of video surveillance <ref type="bibr" target="#b4">[Collins et al. 2000]</ref>, for instance, monitoring crowds <ref type="bibr" target="#b22">[Yin et al. 1996]</ref>, monitoring vehicles <ref type="bibr" target="#b5">[Cutler et al. 1999]</ref>, and recognizing pedestrians <ref type="bibr" target="#b18">[Vannoorenberghe et al. 1997]</ref>.</p><p>However, two problems remain in automatic video processing: (i) How will the results of video processing, such as detected changes, be communicated to human operators? Statistical results are not easily comprehensible, while sequences of difference images again require sequential viewing. (ii) How reliable will these automatic techniques be in different circumstances? It is generally difficult to develop an automatic video processing technique that can adapt to different situations with little or no calibration.</p><p>An alternative approach is to help users obtain an overview of a video by extracting interesting information from video data, and present the information to users in a meaningful way. One suggestion was to use browsing techniques for viewing a video like flipping through a book <ref type="bibr" target="#b21">[Yeo and Yeung 1997]</ref>. A number of researchers noticed the structural similarity between video data and volume data commonly seen in medical imaging and scientific computation. For the latter, there is a large collection of methods <ref type="bibr" target="#b12">[Lorensen and Cline 1987;</ref><ref type="bibr" target="#b11">Levoy 1988</ref>] that enable 3D information in a volumetric dataset to be selectively rendered into a single 2D image. Attempts were made to render videos as inter-related image frames by <ref type="bibr" target="#b9">[Hertzmann and Perlin 2000]</ref>, and as volume data sets by <ref type="bibr" target="#b10">[Klein et al. 2002]</ref>. Both focused on non-photorealistic rendering for creating an artistic visual experience, and both treated videos as solid video cubes or cuboids. There was also a recent report on an initial investigation, in the context of sign language analysis, into the volumetric representation of object movement in a simple video sequence <ref type="bibr" target="#b8">[Hajek 2002</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Hypotheses and Contributions</head><p>Considering video visualization as a new scientific subject, we propose the following three hypotheses:</p><p>1. Video visualization is an (i) intuitive and (ii) cost-effective means of processing large volumes of video data.</p><p>2. Well constructed visualizations of a video are able to show information that numerical and statistical indicators (and their conventional diagrammatic illustrations) cannot.</p><p>3. Users can be accustomed to visual features depicted in video visualizations, or can be trained to recognize specific features.</p><p>To ascertain these hypotheses will no doubt require a substantial amount of scientific investigation and technical development. As no previous work on this subject was found in the literature, we hope this paper will serve as a brave path-finder. Our main contributions include:</p><p>• We have initiated an original investigation into the subject, and offered a general solution by utilizing volume visualization techniques, focusing on difference volumes, spatial and opacity transfer functions, and stream-based rendering.</p><p>• We have designed and implemented a video visualization pipeline by integrating a collection of techniques, and we have demonstrated that it is technically feasible to offer video visualization as a practical tool to applications such as video surveillance and video segmentation.</p><p>• We have conducted several case studies, including television programmes, and indoor and outdoor video sequences. The results of these studies have provided the first set of evidence to support hypotheses (1) and (2).</p><p>For the third hypothesis, we believe that some comprehensive user studies are necessary, but we are confident about the likelihood of a positive conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Video Visualization Pipeline</head><p>A video visualization pipeline is a data flow pipeline, consisting of a series of functional components, namely video capture ⇒ data communication ⇒ data management ⇒ video processing ⇒ video visualization. Each component may accommodate a range of mechanisms and techniques. For example, the video capture component may capture videos from a set of security cameras, or be a tool that receives video emails. The video processing component may house a collection of image processing techniques and statistical methods. V 3 -Volume Visualization for Videos -is a prototype system designed to demonstrate the technical feasibility of such a pipeline. Its primary design objective is to facilitate quick analysis of recently archived video data, such as in the security industry, through the use of volume visualization. This objective is reflected strongly in the design of the V 3 system architecture and user interface ( <ref type="figure">Figure 2</ref>). Figure 2: The system architecture and UI of V 3 .</p><p>V 3 allows multiple sites to be monitored concurrently in realtime from a single control center. At each remote site, we have a set of cameras that can be interactively controlled (e.g., EVI-D31/B Sony video cameras). The imagery data captured by the cameras are combined by, and transported through, an MV87 Quad box, with which an individual or combined view can be selected at the control center. The main software system of V 3 is expected to be installed in a control center, where users can interactively control the remote cameras, select views, setting up recording processes, and most importantly "visualizing" the captured data in many forms.</p><p>There are three major algorithmic modules in the software framework of V 3 , namely feature processor, statistical analyzer and volume renderer. The feature processor module consists of a number of image processing filters and image comparison metrics. The module takes the raw imagery data as inputs, and generates appropriate outputs for the statistical analyzer and volume renderer modules. The statistical analyzer takes inputs from the feature processor module and produces numerical statistical indicators, which are then forwarded to the visualization module where the statistical indicators are presented as 2D charts (such as line graphs). The volume renderer module handles only volumetric data, which includes the raw video data as well as that generated by the feature processor module. This modular design gives us the flexibility to replace existing metrics, filters and algorithms, and add new ones, whenever necessary.</p><p>One of the design objectives of V 3 is to provide novice users with some intuitive but powerful visual representations that facilitate a quick decision-making process. After having experimented with many visual designs, we selected five representations as standard options in V 3 for the default display as shown in <ref type="figure" target="#fig_0">Figure 3</ref>. The Microsoft Visual C# .NET development environment has been used to implement the main software components of V 3 , though many filters, metrics and algorithms were first implemented in C and tested in a Linux environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Creating Feature Volumes</head><p>A video data set V is composed of a series of images I 1 , I 2 ,...,I tn , where all images are normally of the same resolution xn × yn. Hence V can be considered as a collection of voxels that are organized into a 3D regular grid as:</p><formula xml:id="formula_0">V = {v x,y,t |1 ≤ x ≤ xn, 1 ≤ y ≤ yn, 1 ≤ t ≤ tn}.</formula><p>Each voxel v is addressed by its grid coordinates (x, y,t), and is associated with one or more scalar values representing imagery properties such as intensity and color components. In volume visualization, such a structure is commonly referred to as a volume data set, 3D raster or a volume buffer. Because the temporal dimension t is of a different nature from that of the spatial dimensions x and y, V should normally be manipulated as an anisotropic grid, whenever the spacing between neighboring voxels is a matter of interest.</p><p>The primary objective of video visualization is to extract meaningful information from original video data sets, i.e., solid video cuboids. In principle, the extracted information can be represented using any data type. In the context of this work, we concentrate our discussions on information that is suited to volumetric representations. We conveniently call a volumetric representation of such extracted information a feature volume.</p><p>Given an RGB video volume V , a feature volume F can be constructed using a filter as F = Θ(V ). In general, such a filter may operate on a sub-domain of V , and generate a sub-domain of F. Some example filters considered in this work include:</p><p>• color space converters -Each filter converts a video volume in the RGB space to that in an alternative color space such as YIQ and HSV.</p><p>• opacity filters -Each filter creates an opacity feature volume that highlights or de-highlights particular components of an RGB volume. <ref type="figure" target="#fig_1">Figure 4</ref> shows a visualization supported by such a filter that determines the opacity of each voxel based on the hue values and edge properties in a small window associated with that voxel.</p><p>• change detection filters -Each filter typically creates a feature volume that represents the magnitude of temporal changes in a video volume V . This is the focus of the following discussions in the rest of this section. We have considered two types of temporal changes, namely relative difference between consecutive images in V :</p><formula xml:id="formula_1">F r = {∆(I 1 , I 2 ), ∆(I 2 , I 3 ),...,∆(I tn−1 , I tn )}</formula><p>and absolute difference between a reference frame R and each frame in V :</p><formula xml:id="formula_2">F a = {∆(I 1 , R), ∆(I 2 , R),...,∆(I tn , R)}</formula><p>where F r and F a are the corresponding feature volumes, and ∆ is an image comparison metric that returns a grey-scale image representing the magnitude of changes between two input images. F r typically gives an indication of the pace of movement or changes, and for instance, it can be used to depict the boundaries between different segments in a TV news programme and the speed of cars on a road. F a usually gives a good indication of the scale of occupancy, such as the size of a pedestrian crowd in a shopping center. We will further examine the use of these two types of feature volumes, with some examples, in Section 7.</p><p>There is a large collection of image comparison metrics in the literature <ref type="bibr" target="#b23">[Young et al. 1999;</ref><ref type="bibr" target="#b24">Zhou et al. 2002]</ref>. We have studied the effectiveness of several image comparison metrics in the context of a number of case studies, three of which are discussed in this paper. They are (i) a short TV news programme of 1262 frames recorded from a terrestrial broadcast <ref type="figure" target="#fig_0">(Figure 3)</ref>, (ii) a set of indoor experimental videos, each of 90 seconds and 1770 frames <ref type="figure">(Figure 1)</ref>, and (iii) a 12-hour outdoor surveillance video of a university car park of 662 frames ( <ref type="figure" target="#fig_2">Figure 5</ref>). All frames are of the same resolution, i.e., 352 × 288. All our image comparison metrics have a built-in RGB-to-YIQ converter. They take RGB images as the inputs, but computes difference in the YIQ space, often using only the luminance channel Y. In order to differentiate the effectiveness of video visualization in depicting features to users, and that of computer vision in identifying features for users, it is important to consider some less sophisticated, or algorithmically trivial metrics. Among many metrics studied, four have been found interesting. They are:</p><p>• Y-DIF(I 1 , I 2 ) -simple difference metric -It takes two input images, I 1 and I 2 , and computes a grey-scale output image O where each pixel represents the linear distance between the Y-values of two corresponding pixels in I 1 and I 2 respectively.</p><p>• Y-NMSE(I 1 , I 2 ) -normalized mean squared error metric -Instead of the linear distance, it computes the squared distance (i.e., error) between the Y values of each pair of corresponding pixels. The name of the metric is inherited from the corresponding statistical indicator that calculates the mean of the squared errors of all pairs of pixels in two images. In addition, the Y-component of each input image is normalized based on its mean value and standard deviation prior to the computation of mean squared errors. This may reduce the luminous difference caused by different lighting and atmospheric conditions.</p><p>• IQ-DIF(I 1 , I 2 ) -color difference metric -It computes the angle between the IQ vectors of the two corresponding pixels in I 1 and I 2 , and sets the corresponding pixel value in O to the angle. It gives a result similar to that obtained by computing the hue difference in the HSV space.</p><p>• Y-LDD(I 1 , I 2 ) -linear dependence detector (LDD) -We use the improved version proposed by <ref type="bibr">[Durucan and Ebrahimi 2001b]</ref>. This is an illumination invariant change detector, and it examines the changes from a reference image R = I 2 to another image I 1 using a small window. Given a k × k window centered at (x, y), for each of the two input images, we place the Y values of all pixels in the window into a vector, that is, vector M = (m 1 , m 2 ,...) for image I 1 , and N = (n 1 , n 2 ,...) for R. We normally increase the values in M and N by one to ensure no zero component is in either vector. Based on the algebraic properties of the two vectors, the level of dependence between the two vectors can be measured by:</p><formula xml:id="formula_3">c = 1 k 2 k 2 ∑ i=1 m i n i 2 − 1 k 2 k 2 ∑ i=1 m i n i</formula><p>When c = 0, Y-LDD detects no changes; when c &lt; 0, it detects illumination changes; when c &gt; 0, it detects other changes, including object changes. <ref type="figure">Figure 6</ref> shows the application of the above four metrics when comparing images in <ref type="figure" target="#fig_2">Figure 5</ref>. As image B represents an empty car park in a reasonably good lighting condition, it is chosen as a reference image. Images A, C and D were chosen as they exhibit different levels of activities and different lighting conditions. The value ranges of all resultant images have been re-mapped to the [0, 255] domain for maintaining a fair comparison in the evaluation. All resultant images have been inverted for clearer printing. Hence, in <ref type="figure">Figure 6</ref>, the darker the color is, the higher the level of changes detected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Y-DIF(A, B)</head><p>Y-DIF(C, B)</p><formula xml:id="formula_4">Y-DIF(D, B) Y-NMSE(A, B) Y-NMSE(C, B) Y-NMSE(D, B) Y-LDD(A, B) Y-LDD(C, B) Y-LDD(D, B) Y-QDIF(A, B) Y-QDIF(C, B) Y-QDIF(D, B)</formula><p>Figure 6: Images A, C and D in <ref type="figure" target="#fig_2">Figure 5</ref> are compared with the reference image, B, using four different metrics.</p><p>From all three figures, we can see that IQ-DIF does not perform</p><p>as well as what one would expect. This is partially due to the fact that all images were JPEG-compressed by the image capturing device. The compression seems to be optimized for luminance at the cost of redistributing colors within small regions across the image. IQ-DIF is also not so effective when handling noise in a video sequence. It performed poorly when it was applied to the TV news programme recorded from terrestrial broadcast. Y-DIF seems to be affected badly by the lighting conditions, and has a lot of difficulties in distinguishing object changes from luminance changes. Y-NMSE has substantially reduced the level of false detection of changes, but it suffers from the fact that global normalization is influenced by dynamic changes such as the movement of cars and office lights, in the scene. In other words, it is not able to maintain a consistent correction of lighting conditions. For Y-LDD, we used a 3×3 window for the above tests. It has shown its effectiveness in illumination-invariant change detection. The crisscross yellow lines painted on the ground is hardly detected in all three resultant images of Y-LDD. The pixel values of the images returned by Y-NMSE and Y-LDD are normally very low. Appropriate re-scaling is thus necessary prior to the generation of output images. In practice, it is essential to apply a constant scaling factor for an entire video sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Rendering Video/Feature Volumes</head><p>A video volume and its associated feature volume(s) can be treated exactly as conventional volume data sets during rendering. From such data sets, we can construct a spatial object o, which is composed of a set of geometrically-bounded attribute fields (A 0 , A 1 ,...,A k ) <ref type="bibr" target="#b2">[Chen and Tucker 2000]</ref>. Let R denote the set of all real numbers, and E 3 denote 3D Euclidean space. Each attribute field is a scalar field function A : E 3 → R. A typical raw RGB video data set is thus a discrete specification of a spatial object with three attribute fields, namely red, green and blue channels. Its bounding box in effect defines a solid video cuboid, and its volume contents define a solid texture. In V 3 , each spatial object o is defined with four attribute fields, (O, R, G, B), namely opacity, red, green and blue. For example, the spatial object o shown in <ref type="figure" target="#fig_0">Figure 3</ref> is in fact associated with a uniform, fully opaque, opacity field within the bounding volume.</p><p>Opacity and color transfer functions (which are often referred to simply as transfer functions) are an intrinsic part of volume visualization. We may use color transfer functions to indicate different magnitudes of changes as in <ref type="figure">Figure 1</ref>, where red depicts a higher level of change detected and green depicts a lower level. In this case the color fields of each spatial object is defined upon a feature volume. We may use opacity transfer functions to remove or de-highlight parts of a spatial object. As shown in <ref type="figure" target="#fig_1">Figure 4</ref>, we can assign a feature volume to the opacity field of a spatial object, which turns the parts with blue as the dominant wavelength into translucent amorphous matter. In the visualization, it is noticeable that the blue background behind the newscaster has almost disappeared from the video volume.</p><p>A technique called image-swept volume <ref type="bibr" target="#b20">[Winter and Chen 2002]</ref> has been employed to facilitate the horseshoe view, which in general conveys more information than the other four views if they are constrained by the same display space. Instead of deforming a video volume during modeling, we associate the object with a spatial transfer function, Ψ : E 3 → E 3 . Ψ defines a sweeping transformation for every point p in E 3 . Ψ is used to modify the sampling position of a scalar field A during rendering in the form of A (p) = A(Ψ(p)). Rendering of a spatial object using a ray casting algorithm is essentially a discrete sampling process for evaluating scalar fields. With Ψ, an evaluation of A at p implies the evaluation of A at q = Ψ(p).</p><p>The spatial transfer function for the horseshoe view is a semicircular sweep. Consider our video volume is defined in a normalized coordinate system of a domain [0, 1] 3 . Let r = (p x − 0.5) 2 + (p y − 0.5) 2 + (p z − 0.5) 2 1/2 and φ = arctan(p z − 0.5, p x − 0.5) ∈ [−π, π]. We have: <ref type="bibr" target="#b3">[Chen et al. 2003</ref>] recently demonstrated that spatial transfer functions can be defined as spatial objects, and they can be integrated into a volume scene graph <ref type="bibr" target="#b2">[Chen and Tucker 2000]</ref> in the same way as conventional spatial objects. As V 3 is a special purpose visualization system, we purposely moderate its complexity and memory consumption by not equipping it with the facilities for building an arbitrary scene graph. Instead, we have a reconfigurable object tree with a fixed set of built-in tree nodes. For the five standard visual representations shown in <ref type="figure" target="#fig_0">Figure 3</ref>, we have (a) an internal node with a union operator, (b) an internal node with a spatial transfer operator, (c) a leaf node for a video object, which a video volume and/or a feature volume may be loaded onto, (d) a leaf node for a box frame, and (e) a leaf node for an object that defines a sweeping function using a set of scalar fields. Depending on the view selection and the requirement of the box frame, the system reorganizes the object tree into one of the four optional configurations <ref type="figure" target="#fig_3">(Figure 7)</ref>. A stream-based rendering algorithm can benefit video visualization enormously. Video data sets and their feature volumes can be excessively large, containing thousands of frames. Such data sets not only consume a huge amount of memory, but also put a lot of strain on the rendering speed largely due to memory swapping. However, in many applications, such as surveillance, videos are coming in streams as image frames. It is hence desirable to start the rendering process as soon as the first frame arrives, and to continue the construction of a visualization progressively following the receipt of each new frame. This strategy is well suited for constructing summary visualizations from overnight video recordings.</p><formula xml:id="formula_5">q x = 2 − 4r r ∈ [0.25, 0.5] 0 r / ∈ [0.25, 0.5] q z = 1 − φ ÷ π φ ≥ 0 0 φ &lt; 0 q y = 1 − p y</formula><p>With the ray casting method, we need to build both front-to-back and back-to-front rendering into the stream-based algorithm, because of the order of arriving video frames. Consider the five visual representations in <ref type="figure" target="#fig_0">Figure 3</ref>. The vertical view and the downward diagonal view require back-to-front ray casting, whilst the upward diagonal view requires front-to-back ray casting. The horseshoe view requires both mechanisms, with front-to-back on the left and back-to-front on the right. Similarly the horizontal view also requires both mechanisms.</p><p>In general, for an arbitrary view, one may determine the ray direction for each pixel in an initialization stage where the ray-box intersection is computed. One must note that it is possible for a ray to enter a horseshoe object twice, if we allow arbitrary viewing positions. Fortunately, the ray casting direction will always remain the same for both intersections. However, this observation cannot be generalized to arbitrary spatial transfer functions.</p><p>For the standard viewing options in V 3 , the ray casting direction for each pixel, and the ray-box intersection points, are predetermined and stored in a set of system files. The data is loaded into a "t-buffer" before rendering starts. The t-buffer also maintains the current rendering status, including the accumulated opacity and color, and the point on the ray where the last casting paused. The algorithm is outlined below: With this stream-based algorithm, the system needs only to keep a small collection of "active" frames of an incoming video stream and its feature stream(s) in an in-core volume buffer as shown in <ref type="figure" target="#fig_6">Figure 8</ref>. The rendering of the each intermediate visualization is on average less than 0.2 second on a 2 GHz Pentium station.</p><p>in in in out out out out out out in in out due due due due due due due  </p><formula xml:id="formula_6">¢ ¢ ¢ ¢ ¢ ¢ ¢ ¢ ¢ ¢ ¢ ¢ ¢ ¢ ¢ ¢ ¢ ¢ ¢ ¢ ¢ ¢ ¢ ¢ ¢ ¢ ¢ ¢ ¢ ¢ ¢ ¢ ¢ ¢ ¢ £ £ £ £ £ £ £ £ £ £ £ £ £ £ £ £ £ £ £ £ £ £ £ £ £ £ £ £ £ £ £ £ £ £ £ £ £ £ £ £ £ £ ¤ ¤ ¤ ¤ ¤ ¤ ¤ ¤ ¤ ¤ ¤ ¤ ¤ ¤ ¤ ¤ ¤ ¤ ¤ ¤ ¤ ¤ ¤ ¤ ¤ ¤ ¤ ¤ ¤ ¤ ¤ ¤ ¤ ¤ ¤ ¤ ¤ ¤ ¤ ¤ ¤ ¤ ¥ ¥ ¥ ¥ ¥ ¥ ¥ ¥ ¥ ¥ ¥ ¥ ¥ ¥ ¥ ¥ ¥ ¥ ¥ ¥ ¥ ¥ ¥ ¥ ¥ ¥ ¥ ¥ ¥ ¥ ¥ ¥ ¥ ¥ ¥ ¥ ¥ ¥ ¥ ¥ ¥ ¥ ¦ ¦ ¦ ¦ ¦ ¦ ¦ ¦ ¦ ¦ ¦ ¦ ¦ ¦ ¦ ¦ ¦ ¦ ¦ ¦ ¦ ¦ ¦ ¦ ¦ ¦ ¦ ¦ ¦ ¦ ¦ ¦ ¦ ¦ ¦ ¦ ¦ ¦ ¦ ¦ ¦ ¦ § § § § § § § § § § § § § § § § § § § § § § § § § § § § § § § § § §</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results and Remarks</head><p>Referring to the three case studies mentioned in Section 5, we have examined the effectiveness of the image comparison metrics in conjunction with video visualization. Here we report a selection of our tests and findings. One common task in video segmentation is to identify transition frames in a video and select a representative image for each segment. Any automatic algorithm for the task will face various challenges such as blending between segments, fast camera movement and flash photography. It is most likely that its parameters would also require frequent adjustments for different scenes. Hence errors are inevitable. For example, we applied Y-DIF, Y-NMSE and Y-LDD metrics to the TV news video in <ref type="figure" target="#fig_0">Figure 3</ref> to compute F r , the relative difference between consecutive images in the video volume. <ref type="figure" target="#fig_7">Figure 9</ref> shows a line graph for each metric depicting the mean intensity of every frame in F r . All three metrics have resulted in some errors, in comparison with the transition frames determined manually. Verifying the correctness of such a process usually requires the viewing of the video sequentially, which nevertheless undermines the advantages of using an automatic algorithm. Video visualization can provide an effective means to the verification, for instance, using the 3D visualization in <ref type="figure" target="#fig_7">Figure 9</ref>, which shows the transition frames (in the form of ) identified using Y-NMSE and representative images in individual segments. The bottom quarter of the video is also displayed to offer a visual cue for different segments. By examining the visualization, we can observe some anomalies that indicate possible errors. These include (a) the sharing of a representative image by two segments, (b) a possibly over-segmented region featuring similar color patterns, and (c) a missing representative frame after the "presenter" segment (cf. the presenter on the left). When future development enables interactive navigation through such a visualization, identifying these errors in video segmentation will become even easier.</p><p>In Section 5, we have found that Y-LDD has shown to be capable of detecting object changes in different illumination conditions, and it has generally performed better than other metrics with outdoor images. Y-NMSE was also shown to be reasonably reliable. <ref type="figure">Figures 10</ref> shows the visualizations of relative and absolute difference volumes of the car park video, which were computed using the Y-LDD and Y-NMSE metrics respectively. All these feature volumes were visualized on their own with a color transfer function, indicating the scale of changes (i.e., red for large intensity changes, green for medium and blue for small).</p><p>The visual representations of the relative difference indicates the level of activities during the recording period, that is, movement of cars. The similar pattern of activities are shown in the visualizations created with both Y-LDD and Y-NMSE. In general a feature volume of relative difference created by Y-LDD may contain more noise than that by Y-NMSE as Y-NMSE seems to be more effective in relative, Y-LDD relative, Y-MNSE absolute, Y-LDD absolute, Y-MNSE <ref type="figure">Figure 10</ref>: The visualization of relative and absolute difference volumes computed from the car park video sequence. dealing with images taken in similar lighting conditions. The feature volumes showing absolute difference have resulted in interesting visualizations, where the swept lines indicate many stationary cars in most parts of the recording period. Such a visualization shows the level of usage of the car park, with little occupancy in the early morning, a full car park during the day, and some dynamic activities in the evening when staff were leaving for home and evening students were coming to the university. The Y-NMSE resulted in a slightly more noisy visualization, as it cannot remove unwanted features such as the criss-cross yellow lines as effectively as Y-LDD. On the video, there was a major change of the weather condition during the afternoon, and this change is clearly visible from both visualizations. A line graph depicting mean intensity of each frame could easily misinform us of some extra activities or occupancy. However, in the visualizations, it is much easier to discard such changes as the amorphous green patterns are perpendicular to the time line.</p><p>We have found that Y-LDD and Y-NMSE did not perform as effectively as Y-DIF for the indoor experimental videos. All visualizations in <ref type="figure">Figure 1</ref> were generated with absolute difference detected by Y-DIF. Some interesting features can be identified from such visualizations, once an observer is accustomed to some regular patterns. For example, as shown in <ref type="figure" target="#fig_8">Figure 11</ref>(a), after a man ran up the corridor, some changes to papers on the noticeboards remained, and such changes were shown as sweeping lines in the corresponding visualization (similar to parked cars). A wavy pattern in <ref type="figure" target="#fig_8">Figure 11</ref>(b) was largely the result of the arm movement of the lady walking up the corridor. The opening of an office door also has a noticeable pattern as shown in <ref type="figure" target="#fig_8">Figure 11(c)</ref>. In fact, one can find a few similar patterns in <ref type="figure">Figure 1</ref>, all resulting from the opening of the door on the left of the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>We have described an approach that can effectively "summarize" a video sequence and can be deployed to deal with the problem of rapid explosion of video data. We have shown that video data can be processed and visualized in the same manner as other volumetric data. We have examined several image comparison metrics, and have found that in different circumstances, some perform better than others. In well-controlled environments, such as the ITN news video and the indoor experimental videos, simple Y-DIF can be used effectively to create feature volumes. In more dynamic conditions, Y-LDD and Y-NMSE performed better. With the aid of (a) changes that remain for a period. three sets of examples, we have provided the first set of evidence to support two of the three hypotheses outlined in Section 3, and have demonstrated the usefulness of video visualization in applications such as video surveillance, and video processing and labeling.</p><p>Our future work will be focused on further investigation into image comparison metrics in order to improve the effectiveness of change detections in outdoor conditions, and the capabilities of depicting more features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>A video is a volumetric object. V 3 provides five standard visual representations of a video object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>The application of an opacity filter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Four images extracted from an outdoor surveillance video of a car park. Image B will be used as a reference image in computing absolute differences inFigure 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Four configurations of object trees in V 3 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Dynamic management of the in-core volume buffer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Line graphs depicting the change-detection results produced by Y-LDD, Y-NMSE, Y-DIF, and human decision. A visualization (based on Y-NMSE) of transition frames and representative images can help identify errors in the numerical results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>Visual patterns in the visualizations for the experimental videos, where corresponding features are manually highlighted.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors are particularly grateful to Justin Biddle, David Clark, Emma Jeffery and Mark Kiddell for their assistance in making some experimental videos. They also wish to acknowledge various financial supports, in the form of studentships and equipment grants, received from UK-EPSRC, University of Wales Swansea, DHI Powell Memorial Fund, and Knowledge Exploitation Fund.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Statistical image sequence processing for temporal change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brocke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 24th DAGM Symposium: Pattern Recognition</title>
		<meeting>24th DAGM Symposium: Pattern Recognition<address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Change detection based on color edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ebrahimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Symposium on Circuits and Systems (ISCAS-2001)</title>
		<meeting>IEEE International Symposium on Circuits and Systems (ISCAS-2001)<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="141" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Constructive volume geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="281" to="293" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spatial transfer functions -a unified approach to specifying deformation in volume modeling and animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cornea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Volume Graphics</title>
		<meeting>Volume Graphics<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>Eurographics/ACM Publications</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A system for video surveillance and monitoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fujiyoshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duggins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tolliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Enomoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hasegawa</surname></persName>
		</author>
		<idno>CMU-RI- TR-00-12</idno>
		<imprint>
			<date type="published" when="2000-05" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University, Robotics Institute</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Monitoring human and vehicle activities using airborne video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 28th Applied Imagery Pattern Recognition Workshop (AIPR)</title>
		<meeting>28th Applied Imagery Pattern Recognition Workshop (AIPR)<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Change detection and background extraction by linear algebra</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Durucan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ebrahimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE 89</title>
		<meeting>the IEEE 89</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1368" to="1381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improved linear dependence and vector model for illumination invariant change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Durucan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ebrahimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE<address><addrLine>San Jose, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">4303</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Time reconstruction of video sequence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hajek</surname></persName>
		</author>
		<ptr target="http://www.cg.tuwien.ac.at/studentwork/CESCG/CESCG-2002/" />
	</analytic>
	<monogr>
		<title level="m">Electronic Proc. Central European Seminar on Computer Graphics</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Painterly rendering for video and interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Perlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st International Symposium on Non-Photorealistic Animation and Rendering</title>
		<meeting>1st International Symposium on Non-Photorealistic Animation and Rendering<address><addrLine>Annecy, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stylized video cubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Sloan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGGRAPH Symposium on Computer Animation</title>
		<meeting>ACM SIGGRAPH Symposium on Computer Animation<address><addrLine>San Antonio, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="15" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Display of surfaces from volume data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="29" to="37" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Marching cubes: a high resolution 3D surface construction algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lorensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH&apos;87</title>
		<meeting>SIGGRAPH&apos;87<address><addrLine>Anaheim, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="163" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">All the images of an outdoor scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV</title>
		<meeting><address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">2352</biblScope>
			<biblScope unit="page" from="148" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Thresholding for change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rosin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th British Machine Vision Conference</title>
		<meeting>8th British Machine Vision Conference<address><addrLine>Essex, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="212" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Morphological change detection algorithms for surveillance applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stringa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th British Machine Vision Conference</title>
		<meeting>11th British Machine Vision Conference<address><addrLine>Bristol, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="402" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Illumination-invariant change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Toth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th IEEE Southwest Symposium on Image Analysis and Interpretation</title>
		<meeting>4th IEEE Southwest Symposium on Image Analysis and Interpretation<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="3" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scene change detection based on audio-visual analysis and interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tsekeridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Krinidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multi-Image Analysis</title>
		<meeting><address><addrLine>Dagstuhl, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">2032</biblScope>
			<biblScope unit="page" from="214" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic pedestrian recognition using realtime motion analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vannoorenberghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Motamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Blosseville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-G</forename><surname>Postaire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICIAP 97 Image Analysis and Processing</title>
		<meeting>ICIAP 97 Image Analysis and essing<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">1311</biblScope>
			<biblScope unit="page" from="493" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Watching your every move</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wakefield</surname></persName>
		</author>
		<ptr target="http://news.bbc.co.uk" />
	</analytic>
	<monogr>
		<title level="j">BBC News Online</title>
		<imprint>
			<date type="published" when="2002-02-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Image-swept volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="441" to="450" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Retrieving and visualizing video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-L</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="43" to="52" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Incident detection in pedestrian traffic using image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Velastin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Davies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th IEE International Conference on Road Traffic Monitoring and Control</title>
		<meeting>8th IEE International Conference on Road Traffic Monitoring and Control<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="115" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image comparison methods for perimeter surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Forshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodgetts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th IEE International Conference on Image Processing and its Applications</title>
		<meeting>7th IEE International Conference on Image essing and its Applications</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">465</biblScope>
			<biblScope unit="page" from="799" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Comparative evaluation of visualization and experimental results using image comparison metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Webster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Visualization</title>
		<meeting>IEEE Visualization<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="315" to="322" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
