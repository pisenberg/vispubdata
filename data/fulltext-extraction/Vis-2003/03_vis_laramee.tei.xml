<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image Space Based Visualization of Unsteady Flow on Surfaces</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">S</forename><surname>Laramee</surname></persName>
							<email>laramee@vrvis.at</email>
							<affiliation key="aff0">
								<orgName type="laboratory">VRVis Research Center</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Jobard</surname></persName>
							<email>bjobard@univ-pau.fr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helwig</forename><surname>Hauser</surname></persName>
							<email>hauser@vrvis.at</email>
							<affiliation key="aff0">
								<orgName type="laboratory">VRVis Research Center</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Pau</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<settlement>Seattle, Washington</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Image Space Based Visualization of Unsteady Flow on Surfaces</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.3.3 [Computer Graphics]: Picture/Image Generation</term>
					<term>I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Color, shading, shadowing, and texture</term>
					<term>[Simulation and Modeling]: Simulation Output Analysis Unsteady flow visualization, computational fluid dynamics (CFD), surface representation, texture mapping</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Figure 1: Visualization of flow on the surface of an intake manifold. The ideal intake manifold distributes flow evenly to the piston valves.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Dense, texture-based, unsteady flow visualization on surfaces has remained an elusive problem since the introduction of texturebased flow visualization algorithms themselves. The class of fluid flow visualization techniques that generate dense representations based on textures started with the Spot Noise <ref type="bibr" target="#b16">[van Wijk 1991]</ref> and LIC <ref type="bibr" target="#b1">[Cabral and Leedom 1993]</ref>. The main advantage of this class of algorithms is their complete depiction of the flow field while their primary drawback is, in general, the computational time required to generate the results.</p><p>Recently, two new algorithms, namely Lagrangian-Eulerian Advection (LEA) <ref type="bibr" target="#b8">[Jobard et al. 2001]</ref> and Image Based Flow Visualization (IBFV) <ref type="bibr" target="#b17">[van Wijk 2002]</ref>, have been introduced that overcome the computation time hurdle by generating two-dimensional flow visualization at interactive frame rates, even for unsteady flow. This paves the way for the introduction of new algorithms that overcome the same problems on boundary surfaces and in three dimensions. In this paper we present a new algorithm that generates dense representations of arbitrary fluid flow on complex, nonparameterized surfaces, more specifically, surfaces from computational fluid dynamics (CFD). However, the algorithm is general enough to apply to other vector field data associated with a surface such as blood vessel flow.</p><p>Traditional visualization of boundary flow using texture mapping first maps one or more 2D textures to a surface geometry defined in 3D space. The textured geometry is then rendered to image space.</p><p>Here, we alter the classic order of operations. First we project the surface geometry to image space and then apply texturing. In other words, conceptually texture properties are advected on boundary surfaces in 3D but in fact our algorithm realizes texture advection solely in image space. The result is a versatile visualization technique with the following characteristics:</p><p>• generates a dense representation of unsteady flow on surfaces</p><p>• visualizes flow on complex surfaces composed of polygons whose number is on the order of 200,000 or more • visualizes flow on dynamic meshes with time-dependent geometry and topology • visualizes flow independent of the surface mesh's complexity and resolution • supports user-interaction such as rotation, translation, and zooming always maintaining a constant, high spatial resolution • the technique is fast, realizing up to 20 frames per second</p><p>The performance is due, among other reasons, to the exploitation of graphics hardware features and utilization of frame-to-frame coherency. The rest of the paper is organized as follows: in Section 2 we discuss related work, Section 3 details unsteady flow visualization on surfaces from CFD. Implementation details are described in Section 4 while results and conclusions are discussed in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our work focuses on texture-based representations of unsteady flow on complex, non-parameterized surfaces. The challenge of visualizing time-dependent vector fields on surfaces at fast frame rates remains unsolved in surveys of the research literature <ref type="bibr" target="#b14">[Post et al. 2002;</ref><ref type="bibr" target="#b15">Stalling 1997]</ref>. However, several techniques have been proposed to successfully resolve parts of the problem. In the next two sections we describe the two main categories of approaches for dense representations on surfaces and dense representations of unsteady 2D vector fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Texture-Based Flow Visualization on Surfaces</head><p>Previous research with a focus on representations of the vector field on boundary surfaces is generally restricted to steady-state flow. This is mainly due to the prohibitive computational time required. An enhanced version of Spot Noise is applied to surfaces by de <ref type="bibr" target="#b3">Leeuw and van Wijk [de Leeuw and van Wijk 1995]</ref>. <ref type="bibr" target="#b0">Battke et al. [Battke et al. 1997]</ref> describe an extension of LIC for arbitrary surfaces in 3D. Some approaches are limited to curvilinear surfaces, i.e., surfaces that can be parameterized using 2D coordinates. Forssell and Cohen <ref type="bibr" target="#b4">[Forssell and Cohen 1995]</ref> extend LIC to curvilinear surfaces with animation techniques and add magnitude and directional information. <ref type="bibr" target="#b12">Mao et al. [Mao et al. 1997]</ref> present an algorithm for convolving solid white noise on triangle meshes in 3D space and extend LIC for visualizing a vector field on arbitrary surfaces in 3D. Stalling <ref type="bibr" target="#b15">[Stalling 1997</ref>] provides a helpful overview of LIC techniques applied to surfaces. In particular, a useful comparison of parameterized vs. non-parameterized surfaces is given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">LEA and IBFV</head><p>The algorithm in this paper is a new approach that incorporates features of both LEA and IBFV. These very effective algorithms have recently been introduced to produce dense representations of unsteady, 2D vector fields.</p><p>Jobard et al. introduced a Lagrangian-Eulerian texture advection technique for 2D vector fields at interactive frame rates <ref type="bibr">[Jobard Figure 3</ref>: A wire frame view of the surface of two intake ports showing its 221,000 polygonal composition: (left) an overview from the top, note that many polygons are cover less than one pixel (right) a close-up view of the mesh between the two intake ports. <ref type="bibr" target="#b10">Jobard et al. 2002]</ref>. The algorithm produces animations with high spatio-temporal correlation. Each still frame depicts the instantaneous structure of the flow, whereas an animated sequence of frames reveals the motion of a dense collection of particles when released into the flow. Particle paths are integrated as a function of time, referred to as the Lagrangian step, while the color distribution of the image pixels is updated in place (Eulerian step). The result represents a large step forward in bringing the visualization of unsteady flow to interactive frame rates.</p><p>Image Based Flow Visualization by van Wijk <ref type="bibr" target="#b17">[van Wijk 2002]</ref> is the most recent algorithm for dense, 2D, unsteady vector field representations. It is based on the advection and decay of textures in image space. Each frame of the visualization is defined as a blend between the previous image, warped according to the flow direction, and a number of background images composed of filtered white noise textures. Performance times up to 50 frames per second are achieved through effective use of the graphics hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Unsteady Flow Visualization on Surfaces</head><p>In this section we describe our technique in detail, starting with a discussion of those factors motivating the approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Physical Space vs. Parameter Space vs. Image Space</head><p>One approach to advecting texture properties on surfaces is via the use of a parameterization, a topic that has been studied ad nauseam (e.g., Levy et al. <ref type="bibr" target="#b11">[Lévy et al. 2002]</ref>). According to Stalling <ref type="bibr" target="#b15">[Stalling 1997]</ref>, applying LIC to surfaces becomes particularly easy when the whole surface can be parameterized globally in two dimensions, e.g., in the manner of Forssell and Cohen <ref type="bibr" target="#b5">[Forssell 1994;</ref><ref type="bibr" target="#b4">Forssell and Cohen 1995]</ref>. However, there are drawbacks to this approach. Texture distortions are introduced by the mapping between parameter space and physical space and, more importantly, for a large number of surfaces, no global parameterization is available such as isosurfaces from marching cubes and most unstructured surface meshes resulting from CFD. Surface meshes from CFD may consist of smoothly joined parametric patches, but can have a complex topology and therefore, in general, cannot be parameterized globally. <ref type="figure">Figures 2 and 3</ref> are examples of surfaces for which a global parameterization is not easily derived. Another approach to advecting texture properties on surfaces would be to immerse the mesh into a 3D texture, then the texture properties could be advected directly according to the 3D vector field. This would have the advantages of simplifying the mapping between texture and physical space and would result in no distortion of the texture. However, this visualization would be limited to the maximum resolution of the 3D texture, thus causing problems with zooming. Also, this approach would not be very efficient in that most of the texels are not used. The amount of texture memory required would also exceed that available on our graphics card, e.g., we would need approximately 500MB of texture memory if we use 4 bytes per texel and a 512 3 resolution texture.</p><p>Can the problem be reduced to two dimensions? The surface patches can be packed into texture space via a triangle packing algorithm in the manner described by Stalling <ref type="bibr" target="#b15">[Stalling 1997</ref>]. However, the packing problem becomes complex since our CFD meshes are composed of many scalene triangles as opposed to the equilateral and isosceles triangles often found in computational geometry. The problem of packing scalene triangles has been studied by Carr et al. <ref type="bibr" target="#b2">[Carr and Hart 2002]</ref>. For CFD meshes, triangles generally have very disparate sizes. For a given texture resolution, many triangles would have to be packed that cover less than one texel. To by-pass this, the surfaces could be divided into several patches which could be stored into a texture atlas <ref type="bibr" target="#b11">[Lévy et al. 2002]</ref>. In any case, computation time would be spent generating texels which cover polygons hidden from the current point of view. The preceding discussion leads us to an alternative solution that, ideally, has the following characteristics: works in image space, efficiently handles large numbers of surface polygons, spends no extra computation time on occluded polygons, does not spend computation time on polygons covering less than a pixel, and supports user interaction such as zooming, translation, and rotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Method Overview</head><p>The algorithm presented here simplifies the problem by confining the advection of texture properties to image space. We project the surface geometry to image space and then apply a series of textures. This order of operations eliminates portions of the surface hidden from the viewer. In short, our proposed method for visualization of flow on surfaces is comprised of the following procedure:</p><p>1. associate the 3D flow data with the polygons at the boundary surface i.e., a velocity vector is stored at each polygon vertex of the surface 2. project the surface and its vector field onto the image plane 3. identify geometric discontinuities 4. advect texture properties according to the vector field in image space 5. inject and blend noise 6. apply additional blending along the geometric discontinuities previously identified 7. overlay all optional visualization cues such as showing a semitransparent representation of the surface with shading These stages are depicted schematically in <ref type="figure" target="#fig_0">Figure 4</ref>. Each step of the pipeline is necessary for the dynamic cases of unsteady flow, time-dependent geometry, rotation, translation, and scaling, and only a subset is needed for the static cases involving steady-state flow and no changes to the view-point. We consider each of these stages in more detail in the sections that follow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Vector Field Projection</head><p>In order to advect texture properties in image space, we must project the vector field associated with the surface to the image plane, taking into account that the velocity vectors are stored at the polygon vertices. We chose to take advantage of the graphics hardware to  project the vector field to the image plane. We assign a color whose R, G, and B values encode the x, y, and z components of the local vectors to each vertex of the boundary surface respectively. The velocity-colored geometry is rendered to the framebuffer. We use the term velocity image to describe the result of encoding the velocity vectors at the mesh vertices into color values. The velocity image is interpreted as the vector field and is used for the texture advection in image space. More precisely, the color assignment can be done with a simple scaling operation. For each color component, h rgb , we assign a velocity, v xyz component according to:</p><formula xml:id="formula_0">h r = v x − v min x v max x − v min x h g = v y − v min y v max y − v min y (1) h b = v z − v min z v max z − v min z</formula><p>The minimum velocity component is subtracted for each color component respectively, in an effort to minimize loss of accuracy.</p><p>The use of a velocity image yields the following benefits: (1) the advection computation and noise blending is simpler in image space, thus we inherit advantages from the LEA and IBFV, (2) the vector field and polygon mesh are decoupled, thereby freeing up expensive computation time dedicated to polygons smaller than a single pixel, (3) conceptually, this is performing hardware-accelerated occlusion culling, since all polygons hidden from the viewer, are immediately eliminated from any further processing, and (4) this operation is supported by the graphics hardware. Saving the velocity image to main memory is simple, fast, and easy. A sample velocity image is shown in <ref type="figure" target="#fig_1">Figure 5</ref> (top, left).</p><p>The construction of the velocity image allows us to take advantage of hardware-accelerated flow field reconstruction. During the construction of the velocity image, we enable Gouraud Shading, also supported by the graphics hardware. Since each velocity component is stored as hue at each polygon vertex of the surface, the smooth interpolation of hue amounts to hardware-accelerated vector field reconstruction. This is important for a minimum of two reasons. First, the polygonal primitive we choose at image advection time is independent of the original mesh polygons (more in Section 3.4). In other words, the vertices of the mesh we use to dis- tort the image are not the same vertices where the original velocity vectors are stored. Second, interpolation is essential for flow field reconstruction. When the surface is rendered with velocity encoded as hue, the vertices of some polygons are clipped during the projection process. However, we still need to access the vector field values inside those polygons, and not just at the vertices, hence the need for reconstruction. We also note that we are not necessarily limited to linear interpolation for reconstruction. Higher order interpolation schemes can be supported by graphics hardware <ref type="bibr" target="#b6">[Hadwiger et al. 2001]</ref>.</p><p>The velocity vectors are de-coded from the velocity image according to:</p><formula xml:id="formula_1">v x = h r • (v max x − v min x ) + v min x v y = h g • (v max y − v min y ) + v min y (2) v z = h b • (v max z − v min z ) + v min z</formula><p>The de-coded velocity vectors used to compute the advection mesh (Sec 3.4) are then projected onto the image plane. The magnitude of the velocity vectors at those parts of the surface orthogonal to the image plane may be shortened as a result of perspective projection, i.e., if the dot product between the image plane normal and the 3D surface normal is zero or close to zero. This can reduce the visual clarity of the vector field's direction during animation. In our implementation, we added an option that allows the user to apply a bias to the velocity vectors that are shortened close to zero due to the projection. We can use this bias to reduce the scaling effect for curved surfaces. Conceptually it is like applying a reverse velocity clamp.</p><p>The projection of the vectors to the image plane is done after velocity image construction for 2 reasons: (1) not all of the vectors have to be projected (Sec. 3.4), thus saving computation time and (2) we use the original 3D vectors for the velocity mask (Sec. 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Advection Mesh Computation and Boundary Treatment</head><p>After the projection of the vector field we compute the mesh used to advect the textures similar to IBFV. We distort a regular, rectilinear mesh according to the velocity vectors stored at mesh grid intersections. The distorted mesh vertices can then be computed by advecting each mesh grid intersection according to the discretized Euler approximation of a pathline, p, (the same as a streamline for steady flow) expressed as:</p><formula xml:id="formula_2">p k+1 = p k + v p (p k ;t) ∆t (3)</formula><p>where v p represents a magnitude and direction after projection to the image plane. The texture coordinates located at the regular, rectilinear mesh vertices are then mapped to the (forward) distorted mesh positions. The distorted mesh positions are stored for fast advection of texture properties for static scenes. Special attention must be paid in order to handle flow at geometric boundaries of the surface. <ref type="figure" target="#fig_2">Figure 6</ref> shows an overview of the original IBFV process. During the visualization, each frame is advected, rendered, and blended in with a background image. If we look carefully at the distort phase of the algorithm, we notice that there is nothing to stop the image from being advected outside of the physical boundary of the geometry. While this is not a problem when the geometry covers the entire screen, this can lead to artifacts for geometries from CFD, especially in the case of boundaries with a non-zero outbound flow, e.g., flow outlets.</p><p>To address this problem we borrow a notion from LEA that treats non-rectangular flow domains, namely, the use of backward coordinate integration (also proposed by Max and Becker <ref type="bibr" target="#b13">[Max and Becker 1999]</ref>). Using backward integration, equation 3 becomes:</p><formula xml:id="formula_3">p k−1 = p k − v p (p k−1 ;t) ∆t<label>(4)</label></formula><p>In this case the texture coordinates located at the (backward) distorted mesh positions are mapped to the regular, rectilinear mesh vertices. Backward integration does not allow advection of image properties past the geometric boundaries. </p><formula xml:id="formula_4">¢ £ ¤ ¥ ¦ § © ! " # $ % &amp; '<label>( ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Edge Detection and Blending</head><p>While we gain many advantages by decoupling the image advection process with the 3D surface geometry, artifacts can result, especially in the case of geometries with sharp edges. If we look carefully at the result of advecting texture properties in image space, we notice that in some cases a visual flow continuity is introduced where it may be undesirable. A sample case is shown in <ref type="figure">Figure 7</ref>.</p><p>World Space I m a g e S p a c e <ref type="figure">Figure 7</ref>: When a 3D surface geometry (left) is projected, continuity is created in image space (right). If the flow aligned texture properties are advected across this edge, an artificial flow continuity may result.</p><p>A portion of the 3D geometry, shown colored, is much less visible after the projection onto the image plane. If the flow texture properties are advected across this edge in image space, also shown colored, an artificial continuity results. To handle this, we incorporate a geometric edge detection process into the algorithm. During the image integration computation, we compare spatially adjacent depth values during one integration and advection step. We compare the associated depth values, z k−1 and z k in world space of p k−1 and p k from equation 4, respectively. If</p><formula xml:id="formula_5">|z k−1 − z k | &gt; ε • |p k−1 − p k | (5)</formula><p>where ε is a threshold value, then we identify an edge. All positions, p, for which equation 5 is true, are classified as edge crossing start points. Special treatment must be given when advecting texture properties from these points. This process does not detect all geometric edges, only those edges across which flow texture properties should not be advected. <ref type="figure" target="#fig_1">Figure 5</ref> top, right shows one set of edges from the detection process. The geometric edges are identified and stored during the dynamic visualization case and additional blending is applied (depicted schematically in <ref type="figure" target="#fig_0">Figure 4</ref>). During the edge blending phase of the algorithm we introduce discontinuities in the texture aligned with the geometric discontinuities from the surface, i.e., gray values are blended in at the edges. This has the effect of adding a gray scale phase shift to the pixel values already blended. This could obviously be handled in different ways, e.g., choosing a random noise value to advect or inverting the noise value already present. Some results of the edge detection and blending phase are illustrated in <ref type="figure">Figure 8</ref>. In our data sets an ε of 1-2% of depth buffer is practical. However, the users may set their own value if fine tuning of the visualization is needed.</p><p>The same edge detection and blending benefits incoming boundary flow. Also an artifact of the IBFV algorithm, geometric boundaries with incoming flow may appear dimmer than the rest of the geometry. This is a result of the noise injection and blending process described in Section 3.6. In short, the background color shows through more in areas of incoming flow because not as much noise has been blended in these areas. <ref type="figure" target="#fig_3">Figure 9</ref>, top, shows a 2D slice through a 3D mesh from a CFD simulation with incoming boundary flow coming in through the narrow inlet from the right. Note that the edge of the inlet appears dim. <ref type="figure" target="#fig_3">Figure 9</ref>, bottom, shows the same slice with edge blending turned on. The boundary artifacts <ref type="figure">Figure 8</ref>: A close-up example of geometric edge detection: on the left side, geometric edge detection is disabled, on the right side enabled.</p><p>of the noise injection and blending process are no longer a distraction. Edge detection and blending also plays in important role while an object is rotating. Without special treatment, contours in image space become blurred when different portions of a surface geometry overlap, such as when blood vessels in <ref type="figure" target="#fig_4">Figure 12</ref> overlap during rotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Noise Blending</head><p>By reducing the image generation process back to two dimensions, the noise injection and blending phase falls in line with the original IBFV technique, namely, an image, F, is related to a previous image, G, by <ref type="bibr" target="#b17">[van Wijk 2002]</ref>:</p><formula xml:id="formula_6">F(p; k) = α k−1 ∑ i=0 (1 − α) i G(p k−i ; k − i)<label>(6)</label></formula><p>where p represents a pathline, α defines a blending coefficient, and k represents time as a frame number. Thus a point, p k , of an image F k , is the result of a convolution of a series of previous images, G(x; i), along the pathline through p k , with an decay filter defined by α(1 − α) i . The blended noise images have both spatial and temporal characteristics. In the spatial domain, a single noise image, g(x), is described as a linearly interpolated sequence of n random values, G i , in the range [0, n − 1], i.e.,</p><formula xml:id="formula_7">g(x) = ∑ h s (x − is) G i mod n<label>(7)</label></formula><p>where the spacing, s, between noise samples is generally greater than or equal to the distance traversed by an image property in one advection step and h s represents a triangular black and white pulse function. Here x represents a location in the flow domain. In practice, we give the user control of s, resulting in multi-frequency texture resolutions in the spacial domain. The background textures used for blending also vary in time. In the temporal domain, each point, G i in the background texture, periodically increases and decays according to a profile, w(t), e.g.,</p><formula xml:id="formula_8">G i;k = w((k/M + φ i ) mod 1)<label>(8)</label></formula><p>where φ i represents a random phase, drawn from the interval [0,1), M is the total number of background noise images used, and where w(t) is defined for all time steps. We use a square wave profile, i.e., w(t) = 1 if t &lt; 1/2 and 0 otherwise. In our application, the user has the option of varying M. Smaller values of M result in higher frequency noise in the temporal domain whereas higher values M result in a lower temporal frequency. <ref type="figure" target="#fig_1">Figure 5</ref> (middle, left) shows a sample blended image and <ref type="figure" target="#fig_1">Figure 5</ref> (middle, right) shows a sample noise image. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Image Overlay Application</head><p>The rendering of the advected image and the noise blending may be followed by an optional image overlay. An overlay enhances the resulting texture-based representation of surface flow by applying color, shading, or any attribute mapped to color <ref type="figure" target="#fig_1">(Fig. 5, bottom,  left)</ref>. In implementation, we generate the image overlay following the construction of the velocity image. This overlay may render any CFD simulation attribute mapped to hue. The overlay is constructed once for each static scene and applied after the image advection, edge blending, and noise blending phases. Since the image advection exploits frame-to-frame coherency, the overlay must be applied after the advection in order to prevent the surface itself from being smeared. Also worthy of mention, is that the opacity value of the image overlay is a free parameter we provide to the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation</head><p>In this section we consider some aspects of the algorithm not previously discussed which are important for implementation. Our implementation is based on the highly portable OpenGL 1.1 (www.opengl.org) library.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Texture Clipping</head><p>In our application, the resolution of the quadrilateral mesh used to warp the image can be specified by the user. The user may specify a coarse resolution mesh, e.g., 128 × 128, for faster performance or a fine resolution mesh, e.g., 512 × 512, for higher accuracy. However, if the resolution of the advection mesh is too coarse in image space, artifacts begin to appear. <ref type="figure">Figure 10</ref>, left, illustrates these artifacts zoomed in on the edge of a surface. In order to minimize the jagged edges created by coarse resolution texture quadrilaterals, we apply a texture clipping function. Subsets of textured quadrilateral that do not cover the surface are clipped from the visualization as shown in <ref type="figure">Figure 10</ref>, right. This can be implemented simply with the image overlay by maximizing the opacity wherever the depth buffer value is maximized, i.e., wherever there is a great depth. <ref type="figure">Figure 10</ref>: The result of, left, a coarse resolution advection mesh with artifacts and, right, the application of texture clipping. The resolution of the advection mesh shown on the left is 32 × 32 for illustration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Velocity Mask</head><p>In order to dim high frequency noise in low velocity regions, the user also has the option of applying a velocity mask. We adopt the velocity mask of Jobard et al. <ref type="bibr" target="#b8">[Jobard et al. 2001]</ref> for our purposes here, namely:</p><formula xml:id="formula_9">α = 1 − (1 − v) m<label>(9)</label></formula><p>where α decreases as a function of velocity magnitude. In our case, the image overlay becomes more opaque in regions of low velocity and more transparent in areas of high velocity. With the velocity mask enabled, the viewer's attention is drawn away from areas of stagnant flow, and towards areas of high flow velocity. We note that in the context of CFD simulation data, engineers are often very concerned about areas of stagnant flow. In the case of a cooling jacket, stagnant flow may represent a region of the geometry where the temperature is too high, possibly leading to boiling conditions thus reducing the effectiveness of the cooling jacket itself. Therefore, in our case the engineers may disable the velocity mask or use the velocity mask to highlight areas of flow, e.g., make the hue brighter in areas of low velocity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Performance and Results</head><p>Our visualization technique is applied primarily to large, highly irregular, adaptive resolution meshes commonly resulting from computational fluid dynamics simulations. <ref type="bibr">1</ref> The ideal intake manifold <ref type="figure">(Fig. 1)</ref> supplies an equal amount of fluid flow to each piston valve. Visualizing the flow at the surface gives the engineer insight into any imbalances between the inlet pipes, in this case, the 3 long narrow pipes of the geometry. <ref type="figure">Figure 13</ref> shows our method applied to a surface of an intake port mesh (from <ref type="figure">Fig. 3</ref>) composed of 221K polygons. The intake port mesh is composed of highly adaptive resolution surface polygons and for which no global parameterization is readily available. The method described here allows the user to zoom in at arbitrary view points always maintaining a high spatial resolution visualization. The algorithm applies equally well to meshes with time-dependent geometry and topology. <ref type="figure">Figure 11</ref> shows the surface of a piston cylinder with the piston head (not shown) defining the bottom of the surface. The method here enables the visualization of fuel intake as the piston head slides down the cylinder. The resulting flow visualization has a smooth spatiotemporal coherency. Our algorithm also has applications in the field <ref type="figure">Figure 11</ref>: Snapshots from the visualization of a time-dependent surface mesh composed of a 79K polygons with dynamic geometry and topology. This intake valve and piston cylinder can also be used to analyse the formation of wall film, the term used to describe the liquid buildup on surfaces.</p><p>of medicine. <ref type="figure" target="#fig_4">Figure 12</ref> shows the circulation of blood at the junction of 3 blood vessels. An abnormal cavity has developed that may hinder the optimal distribution of blood. Performance was evaluated on an HP Visualize workstation with an HP fx graphics card, running Red Hat Linux 7.2 with a 1 GHz Pentium III dual processor and 1 GB of RAM. The performance times reported in <ref type="table" target="#tab_2">Table 1</ref> support interactive exploration of unsteady flow on surfaces. The first time reported in the FPS column is for the static cases of steady-state visualization and the absence of changes to the view point. The times shown in parenthesis indicate the dynamic cases of unsteady flow and interactive zooming and rotation. More specifically, the dynamic cases require the construction of a velocity image, image overlay, as well as geometric edge detection. We include geometric edge detection in the frame rates reported in <ref type="table" target="#tab_2">Table 1</ref>. It does not introduce significant overhead since it is easily built into the advection process itself.</p><p>The performance time of our algorithm depends on the resolution of the mesh used to perform the advection and the number of polygons in the original surface mesh. In the case of steady-state flow, the algorithm no longer depends on the number of polygons in the surface mesh, but on the area covered in image space. The data set shown in <ref type="figure">Figure 1</ref>, left, does not cover as much image space, so its performance times are somewhat higher in the static case.  <ref type="figure">(Fig 1)</ref> 512 × 512 6 (1) combustion 79K 128 × 128 17 (2) chamber 256 × 256 10 (2) <ref type="figure">(Fig 11)</ref> 512 × 512 4 (1) intake 221K 128 × 128 17 (0.5) port 256 × 256 7 (0.5) <ref type="figure">(Fig 13)</ref> 512 × 512 2 (0.3) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>We have presented a novel technique for dense representations of unsteady flow on boundary surfaces from CFD. The algorithm supports visualization of flow on arbitrary surfaces at up to 20 FPS via the careful use of graphics hardware. It supports exploration and visualization of flow on large, unstructured polygonal meshes, and on time-dependent meshes with dynamic geometry and topology. The method generates dense representations of time-dependent vector fields building on both the LEA and IBFV algorithms. It also does not waste computation time on occluded polygons or polygons covering less than one pixel. While the vector fields are defined in 3D and associated with arbitrary triangular surface meshes, the generation and advection of texture properties is confined to image space. Future work can go in many directions including visualization of unsteady 3D flow, something we expect to see soon. Challenges will include both interactive performance time and perceptual issues. Future work also includes the application of more specialized graphics hardware features like programmable per-pixel operations in the manner of Weiskopf et al. <ref type="bibr" target="#b19">[Weiskopf et al. 2002;</ref><ref type="bibr" target="#b18">Weiskopf et al. 2001]</ref> and the use of pixel textures like <ref type="bibr" target="#b7">Heidrich et al. [Heidrich et al. 1999]</ref>.</p><p>Portions of this work have been done via a cooperation between two research projects of the VRVis Research Center (www.VRVis.at) which is funded by AVL (www.avl.com) and an Austrian governmental research program called Kplus (www.kplus.at). We would also like to extend a special thanks to <ref type="figure">Figure 13</ref>: A view of the surface of an 221K polygonal intake port mesh show in <ref type="figure">Figure 3</ref>. Texture-based flow visualization is applied to the surface.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Flow diagram of texture-based flow visualization on complex surfaces -k represents time as a frame number.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>The 5 component images, plus a 6 th composite image, used for the visualization of surface flow on a ring: (top, left) the velocity image, (top, right) the geometric edge boundaries, (middle, left) the advected and blended textures, (middle, right) a sample noise image, (bottom, left) an image overlay, (bottom, right) the result of the composited images with an optional velocity color map. The geometric edge boundaries are drawn in black for illustration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>An overview of the original image based flow visualization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 9 :</head><label>9</label><figDesc>Here we see a 2D slice through a 3D geometry from a CFD simulation. (top) With no edge blending, the background color shows through boundary areas with incoming flow. (bottom). With edge blending, these artifacts are no longer a distraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 12 :</head><label>12</label><figDesc>Visualization of blood flow at the surface of the junction of 3 blood vessels. Stagnant blood flow may occur within the abnormal pocket at the junction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Sample frame rates for the visualization algorithm.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Supplementary video available at http://www.VRVis.at/ar3/pr2/vis03/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>J. J. van Wijk for helping us to understand the IBFV algorithm and to Jürgen Schneider of AVL for his valuable insight into the CFD simulation data sets. Thanks to Jeroen van der Zijp and the FOX Windowing Toolkit (www.fox-toolkit.org) for help with the implementation. Thanks to Michael Mayer for medical the simulation data. We also thank Helmut Doleisch for his contributions. All CFD simulation data presented in this research is courtesy of AVL.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast Line Integral Convolution for Arbitrary Surfaces in 3D</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Battke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stalling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hege</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visualization and Mathematics</title>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="181" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imaging Vector Fields Using Line Integral Convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Leedom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (SIGGRAPH &apos;93 Proceedings)</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Meshed Atlases for Real-Time Procedural Solid Texturing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Hart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="106" to="131" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Enhanced Spot Noise for Vector Field Visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>De Leeuw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Wijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization &apos;95 Proceedings</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="233" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Using Line Integral Convolution for Flow Visualization: Curvilinear Grids, Variable-Speed Animation, and Unsteady Flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Forssell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="133" to="141" />
			<date type="published" when="1995-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Visualizing Flow over Curvilinear Grid Surfaces Using Line Integral Convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Forssell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Visualization</title>
		<meeting>the Conference on Visualization<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="240" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hardware-Accelerated High-Quality Reconstruction on PC Hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hadwiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Theussl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gröller</surname></persName>
		</author>
		<idno>VMV-01</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Vision Modeling and Visualization Conference</title>
		<meeting>the Vision Modeling and Visualization Conference</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A close-up view of the surface of the intake port mesh show in Figure 13. Here we illustrate user-supported zooming with automatic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Westermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Interactive 3D Graphics</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
	<note>Applications of Pixel Textures in Visualization and Realistic Image Synthesis. on the fly recalculation of the flow texture</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jobard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Erlebacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Hussaini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Advection for Unsteady Flow Visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lagrangian-Eulerian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization</title>
		<imprint>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Lagrangian-Eulerian Advection of Noise and Dye Textures for Unsteady Flow Visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jobard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Erlebacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hussaini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="211" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Least Squares Conformal Maps for Automatic Texture Atlas Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lévy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petitjean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Maillot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 2002 Conference Proceedings, Annual Conference Series</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="362" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Line Integral Convolution for 3D Surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kikukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Imamiya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visualization in Scientific Computing &apos;97. Proceedings of the Eurographics Workshop in</title>
		<meeting><address><addrLine>Boulogne-sur-Mer, France, Eurographics</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="57" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Flow Visualization Using Moving Textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Max</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Becker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Visualization Techniques</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="99" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Feature Extraction and Visualization of Flow Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">H</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vrolijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Laramee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Doleisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics 2002 State-of-the-Art Reports, The Eurographics Association</title>
		<meeting><address><addrLine>Saarbrücken Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="69" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">LIC on Surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stalling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Texture Synthesis with Line Integral Convolution, SIGGRAPH &apos;97, Int. Conf. Computer Graphics and Interactive Techniques</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="51" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spot noise-texture synthesis for data visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Wijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (SIGGRAPH &apos;91 Proceedings)</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="309" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image Based Flow Visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Van Wijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 2002 Conference Proceedings, Annual Conference Series</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="745" to="754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hardware-Accelerated Visualization of Time-Varying 2D and 3D Vector Fields by Texture Advection via Programmable Per-Pixel Operations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Vision Modeling and Visualization Conference</title>
		<meeting>the Vision Modeling and Visualization Conference</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="439" to="446" />
		</imprint>
	</monogr>
	<note>VMV-01</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hardware-Accelerated Lagrangian-Eulerian Texture Advection for 2D Flow Visualizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Erlebacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Vision Modeling and Visualization Conference</title>
		<meeting>the Vision Modeling and Visualization Conference</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="439" to="446" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
