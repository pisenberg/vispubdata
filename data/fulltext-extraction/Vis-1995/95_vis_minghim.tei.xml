<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Illustrated Analysis of Sonification for Scientific Visualisation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Minghim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of East Anglia School of Information Systems</orgName>
								<address>
									<postCode>NR4 7TJ</postCode>
									<settlement>Norwich</settlement>
									<country>England -UK</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Forrest</surname></persName>
							<email>forrest@sys.uea.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of East Anglia School of Information Systems</orgName>
								<address>
									<postCode>NR4 7TJ</postCode>
									<settlement>Norwich</settlement>
									<country>England -UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Illustrated Analysis of Sonification for Scientific Visualisation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>This paper presents an analysis of progress in the use of sound as a tool in support of visualisation and gives an insight into its development and future needs. Special emphasis is given to the use of sound in Scientific and Engineering Applications. A system developed to support surface data presentation and interaction by using sound is presented and discussed.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Most graphics applications use only the visual channel for information display. Use of sound is restricted to signalling errors with no additional attributes. Apart from specific applications like games, sonar, music, or multimedia presentations, very little sound is used as support to imagery. This situation is set to change due to pioneering work in recent years, followed by a rapid growth in use of sound to convey information <ref type="bibr" target="#b24">[24]</ref>: in general the results have proved that sound is a valuable tool for yielding meaning and even structured information, particularly in demanding operations where the visual channel is overloaded with information or in monitoring situations where the user needs knowledge of hidden events and objects.</p><p>Sound can be a useful tool to tackle some of the challenges in visualisation. Possible areas that can benefit from use of sound are:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Data representation:</head><p>sound may add further dimension to visual presentations (complementary) or provide redundancy of representation (supplementary). Much scientific information is not necessarily graphic in nature, and sound might be a better tool for representing some data. Moreover, the number of different dimensions that can be visually interpreted at one time is limited, 2. Perceptual Issues: sound has properties different from vision which might shed light on patterns and structures not observed visually. In complementary presentation, the image drives the sound interpretation and more information can be gathered by combining both senses. In supplementary presentation, data which might be missed by visual overload or by lack of visual detail may be detected aurally. 3. Sound can contribute at several levels of the interaction process: As a tool for navigation in a fured image, sound can help provide feedback on what is being hidden by the visible part of the image. Sound can help relieve visual workload, so as to speed up interpretation and therefore speed up interaction. If the user can get at least an initial idea of the contents of portions of data by sound, this reduces the number of costly interactions like viewpoint change, which take long to execute. One important aspect of interaction by sound is that with time and practice, capabilities of sound understanding and segregation increase and the user is able to interpret information or even create mental image associations using sound alone, in a process similar to that observed with the blind. If well designed, sound is non-intrusive, and should not affect, but instead improve, visual interpretation. 4. The problem of adding time dimension to data presentation may well be supported by sound mappings. The temporal nature of sound makes that association plausible and desirable. 5. Another aspect that can clearly be helped by use of sound is the validation of the graphical processes. The tasks of proving a graphical mapping correct and testing graphical processes are complex. Sound mappings of data and properties during presentation can facilitate the task. 6. Memory of data and properties can be improved by meaningful auditory representation.</p><p>Tne word Sonification has generally been used to indicate the mapping to sound of some sort of numerical data with the intention of conveying information <ref type="bibr" target="#b39">[39]</ref>. The term Sonic Design indicates the definition of the components of the sound stimuli that will correspond to particular pieces of information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">On the Use of Sound to Convey Information Prior Work</head><p>Pioneering work using sound at the interface <ref type="bibr" target="#b46">[46]</ref> [4] <ref type="bibr" target="#b33">[33]</ref> and in multi-parameter sound presentation <ref type="bibr" target="#b36">[36]</ref> as well as the potential of presenting of data on two channels simultaneously <ref type="bibr" target="#b27">[27]</ref> has indicated a strong case for the use of sound at the interface <ref type="bibr" target="#b8">[9]</ref>  <ref type="bibr" target="#b24">[24]</ref>.</p><p>Particularly interesting results supporting the display of events were obtained by Gaver <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b19">19]</ref> and by <ref type="bibr">Blattner et al. [2]</ref>, <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8]</ref>, <ref type="bibr" target="#b22">[22]</ref>. Two types of auditory presentation were developed: earcons and auditory icons. Both represent an object by use of a sound burst or sequence of sounds that meaningfully identify an event or object. Earcons use musical listening as basis while auditory icons use 'everyday' sounds, exploiting the familiar associations we use: when identifying the source of a particular sound in everyday life.</p><p>Progress is now being made in mappings and programming environments for use of sound in multivariate data presentation <ref type="bibr">[38,393 [23,25]</ref> [5] especially where graphical display is used in parallel with audio signals <ref type="bibr" target="#b38">[38]</ref>   <ref type="bibr" target="#b32">[32]</ref> 128-l. 3D sound is expected to play an important role in the presentation of multi-dimensional objects <ref type="bibr">[42, 431.</ref> The advantages and disadvantages of using sound can be summarised thus: Advantages: Sound is an alternative representation for certain types of information which are difficult to represent graphically; it provides relief for the overloaded visual channel; it promotes a sense of engagement; it can be used in monitoring situations outwith the visual field; it is a very advanced human sense for detection; it allows a limited possibility of passive perception; it improves memory support; we naturally interact with sound in everyday situations, and it improves access for visually impaired users. Disadvantages:</p><p>Insufficient research in auditory interpretation of complex sounds makes it difficult to design proper sonifications; ambiguity may occur when a mapping is generalised; masking is a serious problem; there are difficulties in testing techniques by properly controlled experiments; and training is usually necessary.</p><p>Some of the difficulties in working with sound are also shared by graphical representations, but sound perception has properties very different from visual perception. In particular, sounds usually represent temporal structures while spatial structures are best represented visually. The mapping of data to sound, however, can encompass both aspects (temporal and spatial). Masking in hearing is also more likely to occur for simple structures than in vision. The resolution in the perception of sound properties is relatively poor compared to the continuous nature of objects in everyday life.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Perception and Sonification</head><p>To work with sound for information display it is very important to understand how human auditory perception and processing occurs <ref type="bibr">[10]</ref>[20] <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b31">31]</ref>  <ref type="bibr" target="#b6">[7]</ref>. It is essential to identify the basics of sound perception in advance of designing any mappings, to avoid ambiguity in the result. All sonifications used to convey meaning should be perceptually based, Auditory perception is much less studied than visual perception, we are very far from a full understanding, and many of the psychological results are contradictory. Only in recent years are Gestalt principles that have proved effective in visual cognition being confirmed or adapted to hearing <ref type="bibr" target="#b44">[44]</ref>.</p><p>Both musical and everyday listening can contribute to sonic designs. While everyday listening is based on our ability to recognise sound sources, there are few techniques that can synthesise natural sounds. They can, however, be simulated. Aspects of everyday listening are important both in the representation of events and in recognition of different sources in multiple-stream sound production. There is also a strong case for use of Music to represent information. Music is actually a means of communication <ref type="bibr" target="#b21">[21]</ref>, although most of us are not aware of it. Numbers and mathematical relations are used to this day as tools for composition. All this can be used to communicate data in a manner that can be learned by most.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Perspectives on the Use of Sound as a Supportive Tool in Computer Applications</head><p>Whilst sonification research has advanced in recent years, there is need for more research before the field evolves into a well established area in human/computer interaction. Some frameworks, languages, and equipment are available, but these resources can only be developed to full potentiality once the requirements of more applications have been established. Techniques have to be tested under a wider range of conditions. The best way of proving the utility of sound is by using it. More research is necessary to measure the extent to which sound improves performance. Hardware and software for sound manipulation is still limited. <ref type="bibr" target="#b38">[38,</ref><ref type="bibr">391</ref> describe hardware (Capybara) and software (Kyma) systems to support sound generation and sonification in real time. The system has evolved over recent years and represents a powerful tool to generate sound functions from a programming environment, using an object oriented graphical language.</p><p>Many workstations now come with good quality sound built-in, offering a low-cost alternative for sound generation well suited to many applications. MIDI synthesis equipment is inexpensive and provides very good quality sound and synthesis flexibility. The current drawback is the great differences between different equipment, different synthesis techniques, and different sets of MIDI messages. The researcher ends up having to design his own language for sound manipulation as a first step to sonitication. Further research is necessary in integrating and standardising hardware and software sound synthesis capabilities for use in data display. In summary, all aspects of sonic display of information need further research before it can be established as an easy option for the visualisation programmer to work with. In <ref type="bibr" target="#b32">[32]</ref> McCabe, and Rangwalla describe mappings for Computational Fluid Dynamics. They identify four categories of mappings: (1) Direct Simulation, whereby data is translated into sound; (2) Virtual Reality feedback, to facilitate the sensation of presence; (3) Annotations of Scenes and Animation, for the accompaniment of image presentations; (4) VaEidation, for confirmation that calculations are correct. Two case studies are presented, one for mapping of the functioning of a heart pump and another for data mapping of rotor-stator tonal acoustics.</p><p>Astheimer <ref type="bibr">[l]</ref> developed a model to include sonification in a Data Flow type Visualisation System as well as a set of sound producing tools to allow users of such systems to select sound parameters onto which data can be mapped. A visualisation tool like Exvis 1401 was built as an example and corresponding sonifications were created. An extension of the Iconographic Presentation of Exvis was developed with correlated Sonification. A model and tools for sonification of environment interaction in Virtual Reality were also implemented.</p><p>Minghim and Forrest [341 present details of sound mappings for surface data provided by SSound, the surface sonitication system developed by the authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">An Example -SSound</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualisation and Surfaces</head><p>One of the basic ways to present graphically is by use of surfaces. For volumetric data, isosurface rendering is commonly used in visualisation systems. Surfaces are reconstructed from voxel data, by checking which voxels contain specific values and determining the geometry of a surface inside the voxels <ref type="bibr" target="#b47">[47]</ref>. The result is a polygonal approximation of surfaces, usually by triangular elements. Surfaces are also useful for extension of 2D mappings (like height fields) and CAD applications, and for other multivariate data presentations. Ssound implements a number of sound functions to support surface-based data presentation and analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Surface Definition</head><p>In order to allow for further graphical manipulation and property calculation, it is common practice to fit a surface to the triangulation resulting from the surface reconstruction process I121 or to estimate parameters of the polygonal description itself <ref type="bibr" target="#b47">[47]</ref>  <ref type="bibr" target="#b45">[45]</ref>. In Ssound we fitted a bicubic surface to the bilinear isosurface using the Clough-Tocher finite element as part of the 'Data and Image Manipulation Functions' component of SSound (see <ref type="figure">Figure 6</ref>). The resulting equations are used to calculate geometric properties of the surface that are in turn responsible for sound generation and control in the sonic mappings</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Approach to Sonification of Surface Data</head><p>The sonifications designed and implemented for SSound were based in first instance on perceptual principles of Musical and Everyday Listening and aural recognition of timbre. The decision of what sound to use in each situation depends on some basic issues: 1. The particular sound structures and combinations which can be detected and interpreted by the, human hearing apparatus. 2. The meaning that is 'naturally' attributed to a particular sound. 3. What new meaning can be attributed to a particular sound or sound structure by learning which does not conflict with preconditioned ideas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">What sound and meaning association may be used</head><p>to represent the particular event, action or object under analysis.</p><p>There are a number of means by which any form of communication can convey information. One classification for the type of codification of data is <ref type="bibr" target="#b12">[13]</ref>: (1) Symbolic: There is a symbol to which a certain datum or event is associated. Learning is necessary, for it relies on conventions for meaning. (2) Nomic: There is physical association between data and representation. (3) Metaphorical: Although not physically related to the information they represent, their mappings are not totally symbolic, but consider representative weight of the representation by use of similarities among different experiences. For example, the representation of an 'uphill' journey would be a 'slow' sound with increasing pitch; hierarchical structure would be represented by rhythm ordering, etc Most elements used in SSound are Metaphorical, although some symbolic relations are also used.</p><p>The building blocks of the sonifications are multiple sound streams (1 to 4) created and controlled independently or in combination depending on the specific graphical or data property, object or event that occurs. The control is transparent to the user so that from his point of view, what is heard is one sound burst, a sequence of tones, or a continuous presentation of multiple sounds that vary individually. From changes in sound, the user detects change in data or property and from the type of change in sound he detects type of change in data.</p><p>To use sound in Three-Dimensional Visualisation, there are problems inherent in interaction and synchronisation with complex objects. Three types of synchronisation are used: one is controlled by a change in one of three possible dimensions (usually x, y and zr,) or in all three directions simultaneously; one user action causing property calculations that generate a chain of calculations and tone displays; and finally one associated with property calculation that generates a series of simultaneous sounds. <ref type="figure" target="#fig_1">Figure 2</ref> shows the facilities for synchronisation in SSound KEY: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SSound Components</head><p>The graphic basics in SSound use NCSA Image software. Image builds isosurfaces using an extension to the Marching Cubes algorithm <ref type="bibr" target="#b26">[26]</ref> applied to data defined on a regular volumetric grid.</p><p>Ssound was developed on a Macintosh Quadra 950. Sound is produced by a synthesiser module Korg Wavestation SR, connected via MIDI interface to the computer. Software was developed using MPW C. All graphic and sound functions were implemented as extensions to NCSA Image. <ref type="figure" target="#fig_2">Figure 3</ref> shows the organisation of the modules that comprise SSound. 'MIDI Management Tools Set' (MIDI Manager for short) is a library provided by Apple to control communications via MIDI between drivers and programs or between different program modules. 'MIDI Messages' stores a table of the possible messages implemented by the synthesiser. 'MIDI Kernel' controls the composition of MIDI messages required by the Sound Library and the formatting of messages in the MIDI Manager format. 'Sound Tools Library' is the set of sound generation and playback functions implemented to provide the tools for the visualisation program. 'Sound Data' stores the current status of the different sound streams. 'Data and Image Manipulation Functions' is the module responsible for image creation and interaction, partly comprising NCSA Image functions and tools, partly developed by the authors to extend graphical presentation, property calculation and interaction in Image. 'Sonification Functions' is the module responsible for the interfaces between users, graphics, data, and sound, implementing the sound mappings. 'HDF files' are data files, formatted in Hierarchical Data Format.</p><p>The whole of the sound tools library is dependent upon the sound organisation data structure that controls the channels currently playing, the status of each of these channels and sound timbre settings. A convention for timbre assignment in the sound module is adopted. Organisation of the 15 possible channels is in progressive order of timbres. A fuller description of Ssound is given in <ref type="bibr" target="#b35">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Sound Mappings</head><p>Some of the sound mappings implemented are based directly on the original data set, some on the volumetric distribution of the surface and some on the geometric properties of the isosurfaces.</p><p>Grid Sonification is obtained straight from the data values. The original three-dimensional grid is projected onto the window where de isosurface(s) is being presented (see <ref type="figure">Figure 4)</ref>. As the user moves the mouse around the window, values inside the current voxel are mapped into frequency, so that by 'listening' to the data the user is able to detect values that 'stand out' and data trends in a way similar to that presented in <ref type="bibr" target="#b40">[40]</ref>. The grid display frames the perception of the sound tones. Position inside the voxel is reinforced by stereo channel distribution or by quadraphonic sound, whichever is available. If the volume selected for rendering is partial the selected portion is highlighted on screen. The user has the option of 'sonifying' either the selected sub-volume, or the whole volume, and in each case all values or only isosurface values of the data set (see <ref type="figure">Figure 4 (c)</ref>). This gives a series of options to the user to compare isovalues to nonisovalues, selected sub-volume to the whole volume, or simply to try to localise special cases or trends in the data set by sound. Higher values are mapped onto higher frequencies and lower values onto lower frequencies. <ref type="figure">Figure 4</ref> shows an example of this 'grid mapping', and its options. Most Sonification Functions and associated graphic processes were developed as extensions to the Toolbox section of NCSA Image (last five icons in <ref type="figure">Figure 5(a)</ref>). A Volume Scan Process which creates a bounding box around the portion of the volume being analysed, and a 'volume probe' that can be moved around the volume 'collecting' information about the surface or surfaces rendered into that volume were implemented. The volume probe can be moved in the x, y and z directions and can change size. As the probe moves, a number of sound functions are available to help estimate contents of the volume probe, the bounding box, and relationships between them. A 'Score Scheme' estimates the number of triangles of the surface in any particular volume. In one sonification, a tone tells how much of the box probe is occupied, followed by another tone that tells how much of the total surface the volume probe contains. In another sonification, the volume is split in two in a chosen direction and two tones represent how much of the surface is present in each half, thus directing the user towards the more populated part of the volume. A further sound function presents this twotone display for three directions in sequence. In addition, the user can remove the external bounding box, select the current volume probe as the bounding box of the next rendering process, as well as memorise a particular position of the volume probe and restore that position, in this way being able to compare contents between two different volume probes by sound as well as visually. High densities are mapped into low frequencies and low densities into high frequencies due to our perception of 'full' or 'heavy' corresponding to low frequencies, and 'empty' or 'light' corresponding to high frequencies. Stereo is used to indicate orientation inside the volume, by presenting 'low' coordinates to the left ear, 'high' coordinates to the right ear and 'middle' to both ears. Volume is controlled inversely to frequency as audio resolution is lower in the low range of frequencies, if volume is kept constant.  <ref type="figure">Figure 6</ref> shows surface slices with detail obtained by the volume scan process. Average coordinate values in a triangle are mapped into frequency. The three streams are represented by different timbres, chosen well apart to ensure discrimination. Different coordinates are mapped to different channels which may be distributed in a number of ways . Volume, timbre and tempo are used to identify 'progression' on the surface so that 'rougher' areas give the impression of 'difficulty' of generation communicated by slow and 'heavy' timbres, while smoother areas have 'faster' sounds. This analogy is repeated in the sonifications based on geometric properties.  Several different geometric properties of the surfaces were mapped into sound, to support shape identification and surface investigation. As the surface is displayed the value of the chosen property is presented aurally. This helps one to analyse not only the evolution of the property itself over the surface, but also to identify unseen features or to clarify ambiguous visual information. <ref type="table" target="#tab_1">Table 1</ref> shows the particular surface properties mapped and corresponding sound controls used in each sonification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(cl</head><p>A further sonification identifies a surface by means of a surface-based earcon to help catalogue and memorise surface information. Shape and roughness are mapped into a sequence of sounds using an earcon type mapping, helping the user to remember aspects of previously analysed surfaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Perception Justification and Reinforcements</head><p>The sound mappings were chosen according to our capabilities of recognition and associations of sounds. For example, high frequencies were chosen to represent high absolute values, low densities, big, and empty; low frequencies were chosen to identify full, low absolute values, small. Stereo balance was used to convey information of direction of orientation. Left-&gt;right analogy was used because of its association with western reading convention, volume was used mainly to reinforce perception of other properties, but on some occasions to reinforce low speed. Timing was used in many circumstances. In gradient sonification, changes in rhythm indicate how much change occurs. In curvature mappings, timing is responsible for conveying 'slow' and 'fast' notions for high and low values of curvature respectively. Timbre changes are used in most sound functions. Timbres are organised by channels, so that 'thicker' timbres are analogous to 'full' or 'rich' and 'thinner' timbres to 'little', 'not full', 'poor', 'weak'. Significant change in data provoked timbre changes. The sonic mappings rely also on the ability of human hearing to segregate sound streams playing simultaneously, commonly referred to as Auditory Scene Analysis and Sound Grouping <ref type="bibr" target="#b6">[7]</ref>  <ref type="bibr" target="#b44">[44]</ref>.</p><p>There are a number of tricks that can be used to avoid ambiguity and guarantee good recognition: multiple streams always have timbres sufficiently far apart perceptually: when one sound property is used in a 'risky' resolution, another property is always controlled simultaneously to reinforce perception. An extremely powerful way of reinforcing mappings of three-dimensional structures is the use of quadraphonic sound.: when depth information is to be conveyed, the sounds are distributed between up to four channels and presented on properly positioned loudspeakers so that the user has the sensation of a third dimension as sound moves back to front, front to back, left to right, right to left. All mappings that involve quadraphonic sound are also implemented as stereo, since few systems can cope with four different sound channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions</head><p>As the effectiveness of sound representation for 3D form, shape and interaction is established, sound may become an integral part of most visualisation systems. We have confirmed the effectiveness of auditory display in conveying information and complex structures. The meaning of most sound mappings is immediately understood, given simple explanations, although some srequire training. Further testing is needed to determine the improvement in performance over purely graphical presentation for redundant mappings, but most sonifications described here are capable of helping the user identify aspects of surfaces not present or not easily conveyed visually. With time and practice understanding is dramatically improved, so that it is possible to understand a great deal of data solely by sound. The expected improvements in memory and interpretation were observed. Sound programming is not difficult using MIDI if the sound equipment is flexible enough to accept a high level of control, but lack of sound libraries and standard sound equipment handling synthesis and MIDI poses a difficulty, since all sound creation, playback and manipulation as well as synchronisation has to be programmed from scratch, a situation which will change with introduction of a number of languages and environments for sonification. Sound can be useful in several ways in visualisation systems as a supportive tool, provided sonic designs take account of fundamentals of human hearing and correspondence with familiar analogies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FigureFigure 1 :</head><label>1</label><figDesc>Research Areas in Sonification and Auditory DisplaySound in VisualisationMost of the work in sound for data representation and interpretation is concerned with multi-variate data.Blattner et al.<ref type="bibr" target="#b2">[3]</ref> describe the use of sound to represent aspects of Fluid Flow. Suggested sonifications are given for several aspects of fluids: fluid characteristics, motion, vortex characterisation and energy dissipation. Steady sounds and sound messages (extensions of earcons) are used to represent different situations. Thus laminar and injection flow are steady sounds because of their continuous nature. Register -is used to represent different viscosities. Number of Frequencies is suggested to identify density of materials (the more frequencies, the denser the material), timbre represents temperature, tempo, speed of injection, stereo balance (or threedimensional sound), direction of flow, rapid change of pitch may represent vorticity, and the pitch span of these changes can represent vortex size. Event earcons are inserted to reflect particular situations like change in state or exceptional events, with the object of drawing attention to a specific event. Graphical representation of fluids is visually demanding and particularly important events may not be noticed. Justifications with basis in 'natural' associations between sound parameters and user expectations of sound changes are given,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Synchronisation of Sound Streams</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>SSound Organisation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Grid mapping from 3D volume to 2D grid on Screen (b) Screen during the process of grid sonification for a partially displayed surface with highlighted sub-grid. (c) Options for Grid Sonification in SSound.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Volume Scan Extension (a) Tools dialogue with Sound Extensions (b) Dialogue for Volume Scan. Corresponding keys on the keyboard will have the same functions. (c) Surface with bounding box and box probe. In Coordinate Mapping, coordinates are mapped into frequency. Three streams are presented continually, representing variations in the x, y and z coordinates of surface triangles. When a variation in any direction occurs, the value of the corresponding sound stream changes. Because the order of generation of the surface in Marching Cubes is 'volume driven', z varies more slowly, y second, and x more rapidly. The result of the sonification is to represent progression in coordinate values, so that variations in shape generate variations in sound that can be detected aurally. Surface Fitting Example (a) A slice of a surface, as generated by Marching Cubes (b) Fitted surface corresponding to (a) (c) Detail of surface in (a) (d) Fitting of (c)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>validation Mean C mm fnzqusncy, volume Shape variation .,...,,_,,,, Gaussian ,..........,,,, ~ _........,,..,.,.,........~...,............................ fiewency. volume Shape vacation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Surface property sound mappings</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1070">-2385/95$4.00O1995IEEEProceedings of the 6th IEEE Visualization Conference(VISUALIZATION '95)1070-2385/95 $10.00 © 1995 IEEE</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Proceedings of the 6th IEEE Visualization Conference(VISUALIZATION '95)1070-2385/95 $10.00 © 1995 IEEE</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Proceedings of the 6th IEEE Visualization Conference(VISUALIZATION '95)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Acknowledgements</head><p>We wish to acknowledge Apple Computer Advanced Technology Group, Cupertino, for providing computing equipment and software, the Computing Studies Sector, UEA, for providing sound equipment, and CNPq -Conselho National de Desenvolvimento Cient'fico e Tecnologico, Brazil, for financing the studies of one of the authors.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sonification Tools to Supplement Dataflow Visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Astheimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scientific Visualization: Advanced Software Techniques</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="15" to="36" />
		</imprint>
	</monogr>
	<note>in Palamidese</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blattner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sumikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Greenberg</surname></persName>
		</author>
		<title level="m">Earcons and Icons: their structure and common design principles</title>
		<imprint>
			<publisher>Human-Computer Interaction</publisher>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Listening to Turbulence: An Example of Scientific Audiolization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Blattner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kamegai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Interface Design</title>
		<editor>Blattner &amp; Dannenberg</editor>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="87" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Sound and Computer Information Presentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982" />
		</imprint>
		<respStmt>
			<orgName>Lawrence Livermore Laboratory, University {of California</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S. -Multivariate Data</forename><surname>Bly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mappings</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">405416</biblScope>
		</imprint>
	</monogr>
	<note>in [24</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An Evaluation of Earcons for Use in Auditory Human-Computer Interfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Brewster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D N</forename><surname>Edwards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM INTERCHI&apos;93</title>
		<meeting>ACM INTERCHI&apos;93</meeting>
		<imprint>
			<biblScope unit="page" from="222" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Auditory Scene Analysis: Hearing in Complex Environments, in [30</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Bregman</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="10" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A Detailed Investigation into the Effectiveness of Earcons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Brewster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Edwards</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="471" to="498" />
		</imprint>
	</monogr>
	<note>in [24</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Empowering People -The Use of Non-Speech Audio at the Interface</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Gaver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Human Factors in Computing Systems, Tutorial 4 Notes</title>
		<meeting><address><addrLine>Seattle</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Crowder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Auditory</forename><surname>Memory</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="113" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Correlating Sonic and Graphic Materials in</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scientific Visualization, Proceedings SPIE -Extracting Meaning from Complex Data</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">1259</biblScope>
			<biblScope unit="page" from="154" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Visualizing and Modeling Unstructured Data, The Visual Computer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Foley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nielson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993-08" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="439" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Gaver</surname></persName>
		</author>
		<title level="m">Auditory Icons: Using Sound in Computer Interfaces, Human-Computer Interaction</title>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="167" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Gaver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-The</forename><surname>Sonicfinder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">An Interface that Uses Auditory Icons</title>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Empowering People -The Use of Non-Speech Audio at the Interface</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Gaver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Gaver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Human Factors in Computing Systems</title>
		<imprint/>
	</monogr>
	<note>Auditory Icons in Large-scale Collaborative Environments. Tutorial 4 Notes</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Effective Sounds in Complex Systems: The ARKOLA Simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Gaver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc.ACM CHI&apos;M</title>
		<meeting>.ACM CHI&apos;M</meeting>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="85" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Gaver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sound</surname></persName>
		</author>
		<imprint/>
		<respStmt>
			<orgName>Support for Collaboration -Proc</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m">Second Ruropean Conference on Computer-Supported Cooperative Work</title>
		<imprint>
			<date type="published" when="1991-09" />
			<biblScope unit="page" from="293" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Gaver</surname></persName>
		</author>
		<title level="m">Synthesizing Auditory Icons -Proc. ACM INTERCHI&apos;M</title>
		<imprint>
			<biblScope unit="page" from="228" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Using and Creating Auditory Icons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Gaver</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="417" to="446" />
		</imprint>
	</monogr>
	<note>in [24</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Timbre Discrimination in Musical Patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Grey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="467" to="472" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A Grammatical Parallel Between Music and Language, in Music, Mind and Brain -The Neuropsychology of Music</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jackendoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lerdahl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982" />
			<publisher>Plenum Press</publisher>
			<biblScope unit="page" from="83" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Furrier</surname></persName>
		</author>
		<title level="m">The construction of Audio Icons and Information Cues for Human Computer Dialogues -Contemporary Ergonomics: Proceedings of Ergonomics Society&apos;s 1989 Annual Conference</title>
		<imprint>
			<biblScope unit="page" from="436" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ellison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>-Audification</surname></persName>
		</author>
		<title level="m">The Use of Sound to Display Multivariate Data Proc. International Computer Music Conference</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="214" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kramer</surname></persName>
		</author>
		<title level="m">Auditory Display: Sonification, Audification and Auditory Interfaces -Proc. ICAD&apos;92, the First International Conference on Auditory Display</title>
		<imprint>
			<publisher>Addison -Wesley</publisher>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Some Organizing Principles for Representing Data With Sound -in [24</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kramer</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="185" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Marching Cubes: A high resolution 3D surface construction algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Lorensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Cline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="163" to="169" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bisensory Presentation of Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>Loveless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brebner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="161" to="199" />
			<date type="published" when="1970-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A Framework for Sonification Design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Madhyastha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Reed</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="267" to="290" />
		</imprint>
	</monogr>
	<note>in [24</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hearing Musical Streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mcadams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bregman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Music Journal</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="26" to="44" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S;</forename><surname>Mcadams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Biggand</surname></persName>
		</author>
		<title level="m">Thinking in Sound -The Cognitive Psychology of Human Audition</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Recognition of Sound Sources and Events -in [30</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Meadams</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="146" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Auditory Display of Computational Fluid Dynamics Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mccabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rangwalla</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="327" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dynamic Representation of Multivariate Time Series Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Mezrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Frysinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Slivjanovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">385</biblScope>
			<biblScope unit="page" from="34" to="40" />
			<date type="published" when="1994-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mappings for Surface Visualisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Minghim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Forrest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sound</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proce. WSCG&apos;95, The Third International Conference in Central Europe on Computer Graphics and Visualisation 95</title>
		<meeting>e. WSCG&apos;95, The Third International Conference in Central Europe on Computer Graphics and Visualisation 95</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Minghim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Forrest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ssound</surname></persName>
		</author>
		<title level="m">A System for Sound Support of Surface Visualisation</title>
		<imprint/>
	</monogr>
	<note>in preparation</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Information of Elementary Multidimensional Auditory Displays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pollack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ficks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="158" />
			<date type="published" when="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">-Complementary Visualizati&apos;on and Sonification of Multi-Dimensional Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Rabenhors</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Jameson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Linton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Mandehnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE -Extracting Meaning from Complex Data</title>
		<meeting>SPIE -Extracting Meaning from Complex Data</meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">1259</biblScope>
			<biblScope unit="page" from="147" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scaletti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Craig</surname></persName>
		</author>
		<title level="m">Using Sound to Extract Meaning from Complex Data -Proc. SPIE -Extracting Meaning from Complex Data II</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">1459</biblScope>
			<biblScope unit="page" from="206" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Sound Synthesis Algorithms for Auditory Data Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scaletti</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="223" to="252" />
		</imprint>
	</monogr>
	<note>in [24</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">-Stereophonic and Surface Sound Generation for Exploratory Data Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S;</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Bergeron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">G</forename><surname>Grinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM CHI&apos;BO</title>
		<meeting>ACM CHI&apos;BO</meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="125" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Global Geometric, Sound, and Color Controls for Iconographics Displays of Scientific Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Grinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rickett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIR -Extracting Meaning from Complex Data II</title>
		<meeting>SPIR -Extracting Meaning from Complex Data II</meeting>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">1459</biblScope>
			<biblScope unit="page" from="192" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wenzel</surname></persName>
		</author>
		<title level="m">Three-Dimensional Virtual Acoustic Displays -in Blattner, M.M</title>
		<editor>Dannenberg, R.B.</editor>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="257" to="288" />
		</imprint>
	</monogr>
	<note>Multimedia Interface Design</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wenzel</surname></persName>
		</author>
		<title level="m">Spatial Sound and Sonification</title>
		<imprint>
			<biblScope unit="page" from="127" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="95" to="126" />
		</imprint>
	</monogr>
	<note>Perceptual Principles in Sound Grouping -in [24]</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Normal Estimation in 3D Discrete Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yagel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Kaufman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Visual Computer</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="278" to="291" />
			<date type="published" when="1992-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pattern Recognition by Audio Representation of Multivariate Analytical Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Analytical Chemistry</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1120" to="1123" />
			<date type="published" when="1990-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Surface Modeling Method by Polygonal Primitives for Visualizing Three-dimensional Volume Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Visual Computer</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="246" to="259" />
			<date type="published" when="1992-04" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
