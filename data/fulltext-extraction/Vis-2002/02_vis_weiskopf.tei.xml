<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Volume Clipping via Per-Fragment Operations in Texture-Based Volume Visualization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Weiskopf</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visualization and Interactive Systems Group</orgName>
								<orgName type="institution">University of Stuttgart</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Engel</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visualization and Interactive Systems Group</orgName>
								<orgName type="institution">University of Stuttgart</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Ertl</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visualization and Interactive Systems Group</orgName>
								<orgName type="institution">University of Stuttgart</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Volume Clipping via Per-Fragment Operations in Texture-Based Volume Visualization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CR Categories: I.3.3 [Computer Graphics]: Picture/Image Generation; I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Color</term>
					<term>shading</term>
					<term>shadowing</term>
					<term>and texture volume rendering</term>
					<term>clipping</term>
					<term>hardware acceleration</term>
				</keywords>
			</textClass>
			<abstract>
				<p>We propose new clipping methods that are capable of using complex geometries for volume clipping. The clipping tests exploit per-fragment operations on the graphics hardware to achieve high frame rates. In combination with texture-based volume rendering, these techniques enable the user to interactively select and explore regions of the data set. We present depth-based clipping techniques that analyze the depth structure of the boundary representation of the clip geometry to decide which parts of the volume have to be clipped. In another approach, a voxelized clip object is used to identify the clipped regions.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Volume clipping plays a decisive role in understanding 3D volumetric data sets because it allows to cut away selected parts of the volume based on information on the position of voxels in the data set. Very often clipping is the only way to uncover important, otherwise hidden details of a data set. We think that this geometric approach can be regarded as complementary to the specification of transfer functions, which are based on the data values and their derivatives only. Therefore, we suggest to use a combination of data-driven transfer functions and geometry-guided clipping to achieve very effective volume visualization.</p><p>The design of useful transfer functions has recently attracted some attention <ref type="bibr" target="#b12">[13]</ref>. One important issue is how to generate a transfer function automatically <ref type="bibr" target="#b9">[10]</ref>. Current work is focused on other aspects and mainly deals with the question how multidimensional transfer functions <ref type="bibr" target="#b10">[11]</ref> are better suited to extract information than a classification that is only based on the scalar values of the data set.</p><p>In most applications of interactive volume clipping, however, only a straightforward approach is adopted-with one or multiple clip planes serving as clip geometry. Typical applications are medical imaging of radiological data or volume slicing of huge seismic 3D data sets in the oil and gas industry <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16]</ref>. Although some clip geometries can be approximated by multiple clip planes, many useful and important geometries are not supported. A prominent example is shown in <ref type="figure" target="#fig_0">Figure 1</ref>: cutting a cube-shaped opening into a volume is not facilitated by clip planes. Therefore, it is quite surprising that volume clipping has rather been neglected as a research topic in scientific visualization and that clipping approaches beyond standard clip planes have rarely been considered.</p><p>In this paper, we present various ways to efficiently implement complex clip geometries. All methods are tailored to texture-based volume rendering on graphics hardware. All hardware-based volume visualization approaches reduce the full 3D problem to a collection of 2D slices. Since these slices are displayed by the graphics hardware, we can make use of its features, such as advanced fragment operations. Our clipping approaches support both 3D textures with slices perpendicular to the viewing direction <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and stacks of 2D textures. For each fragment in a slice, fragment operations and tests on the dedicated graphics hardware decide whether the fragment should be rendered or not. Therefore, operations on the slower CPU are avoided. The proposed clipping techniques allow to change the position, orientation, and size of clip objects in real time. This is an important advantage because interactivity provides the user with an immediate visual feedback.</p><p>In our first approach, a clip object is represented by a tessellated boundary surface. The basic idea is to store the depth structure of the clip geometry in 2D textures whose texels have a one-to-one correspondence to pixels on the viewing plane. The fragment operations provided by NVidia's GeForce 3/4 allow to manipulate depth values; in this way, the depth structure of the clip object can be used to clip away unwanted parts of the volume. This depth-based approach is particularly well-suited for producing high-quality images because clipping is performed with per-pixel accuracy.</p><p>In the second approach, a clip object is voxelized and represented by an additional volume data set. Clip regions are specified by marking corresponding voxels in this volume. Graphics hardware can map multiple textures onto a single slice polygon by means of multiple parallel texture stages. Therefore, the volume for the actual data set and the clipping volume are combined by fragment operations to perform clipping.</p><p>We think that complex clip geometries could improve visualization in many fields of application. For example, current implementations of 3D LIC (line integral convolution) <ref type="bibr" target="#b13">[14]</ref> already benefit from the use of volume clipping. Without clipping, the dense representation of streamlines in 3D LIC would cover important features of the flow further inside the volume. Complex clip geometries can be adapted to curved boundaries of the problem domain in flow simulations. For example, the clip object could be aligned to the surface of a car body in automotive applications of CFD (computational fluid dynamics). Many interesting features of a flow are close to such boundaries and could easily be explored after clipping.</p><p>Medical imaging is another field that would profit from sophisticated clipping approaches. For example, segmentation information, which is often available, could serve as a basis for defining curved clip geometries. In this way, features of the data set could specifically be uncovered. In a sense, volume clipping is able to map outside semantic information into the volume data set. Finally, volume clipping could be used for interactively modeling simple CSG (constructive solid geometry) objects.</p><p>The paper is organized as follows. In the next section, previous work is briefly described. In Sections 3-5, different clipping techniques based on the evaluation of depth information are presented. Section 6 contains a description of clipping by means of a voxelized volumetric object. It is followed by a presentation of results and a discussion. The paper ends with a short conclusion and an outlook on future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PREVIOUS WORK</head><p>Van Gelder and Kim <ref type="bibr" target="#b14">[15]</ref> use clip planes to specify the boundaries of the data set in 3D texture-based volume rendering. Westermann and Ertl <ref type="bibr" target="#b16">[17]</ref> propose volume clipping based on stencil tests. In this approach, the geometry of the clip object has to be rendered for each slice to set the stencil buffer correctly. A clip plane that is co-planar with the current slice discards the clip geometry in front of this slice. In this way, stencil buffer entries are set at those positions where the clip plane is covered by an inside part of the clip geometry. Finally, the slice that is textured by the actual data set is rendered into the frame buffer. By using the stencil test, fragments are correctly clipped away.</p><p>In <ref type="bibr" target="#b3">[4]</ref>, an approach similar to one of the depth-based methods of this paper is used for depth sorting semi-transparent surfaces. This technique is related to depth-peeling by Everitt <ref type="bibr" target="#b5">[6]</ref>, facilitating the ideas of virtual pixel maps <ref type="bibr" target="#b11">[12]</ref> and of dual depth buffers <ref type="bibr" target="#b2">[3]</ref>. A related field of research deals with issues of spatial sorting in more generality without considering specific issues of volume rendering. For a survey on this well-established topic we refer, for example, to Foley et al. <ref type="bibr" target="#b6">[7]</ref> or Durand <ref type="bibr" target="#b4">[5]</ref>. These references also cover objectspace algorithms, which are not discussed in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BASICS OF DEPTH-BASED CLIPPING</head><p>In this section, we focus on algorithms for volume clipping that exploit the depth structure of the clip geometry. In all of our depthbased approaches (Sections 3-5), we assume that volumetric clip objects are defined by boundary surface representations. Graphics hardware expects the surface to be tessellated-usually in the form of triangular meshes. <ref type="figure" target="#fig_2">Figure 2</ref> illustrates the depth structure of a typical clip object. In what follows, we will no longer consider a complete 3D scenario, but reduce the problem to a 1D geometry along a single light ray  that originates from the eye point and reaches into the scene. From this point of view, just the corresponding single pixel on the image plane is taken into account. Therefore, the following descriptions can be mapped to operations on those fragments that correspond to the respective position of the pixel in the frame buffer. In the complete volume rendering process, slices through the volume are partitioned into a collection of these fragments during scan conversion. In this sense, the fragment-based point of view directly leads to volume clipping in full 3D space.</p><p>Let us start with a basic algorithm for clip objects of arbitrary topology and geometry. First, the depth structure of the clip geometry is built for the current pixel. This structure stores the depth values for each boundary between object and background space. In this way, the 1D depth values are partitioned into intervals whose bounds are given by boundaries of the objects. In addition, the intervals are classified either as inside or as outside of the clip object. The classification is determined by the clip geometry and, for example, can be based on the orientation of the tessellated surface or on the direction of normal vectors associated with the clip geometry. Note that, for a closed, non-selfpenetrating clip surface, the classification alters repeatedly from outside to inside and vice versa. The user can choose to clip away either the inside or outside of the clip object. In this way, the intervals are uniquely specified as visible or invisible. Secondly, the rendering of each fragment of a volume slice has to check to which class of interval the fragment belongs. Based on this visibility property, the fragment is blended into the frame buffer or clipped away.</p><p>The following issues have to be considered for an actual implementation of the above algorithm. What kind of data structure is appropriate to store the depth structure? How is the depth structure generated from the clip geometry? How is the visibility property of a fragment evaluated?</p><p>To be more specific, the question is how the above algorithm can be efficiently implemented on current graphics hardware. Since the depth structure is stored on a per-pixel basis, a pixel-oriented mechanism has to be considered, such as a buffer or texture on the graphics board. However, the need for rather high accuracy for depth values rules out color-based "storage media" on the GPU such as the frame buffer or a texture with low accuracy (for example a standard RGB or RGBα texture).</p><p>A straightforward approach exploits the depth buffer as storage medium for interval boundaries. A major disadvantage of this approach is the fact that only one depth value can be stored per pixel. Therefore, just a single boundary can be implemented, which greatly reduces the fields of application. The implementation of this approach begins with rendering a simple clip geometry to the depth buffer, i.e., the depth structure is generated from the clip geometry by standard scan conversion of the graphics hardware. Then, writing to the depth buffer is disabled and depth testing is enabled. Finally, the slices of the volume data set are rendered and blended into the frame buffer. Here, the depth test implements the evalua- tion of the visibility property of a fragment. The user can choose between two variants by setting the logical operator for the depth test either to "less" or "greater". In the first case, the volume is clipped away behind the geometry from the camera's point of view. The latter allows to clip away the volume in front of the clip geometry. This simple approach can easily be implemented by just using standard OpenGL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DEPTH-BASED VOLUME PROBING</head><p>In this section, we demonstrate how a maximum number of two boundaries along the depth structure can be efficiently implemented on graphics hardware by exploiting depth tests, depth clipping, and depth computations in the fragment operations unit. Let us assume that the volume is clipped away outside this interval. This volume probing approach leaves visible only the volume inside the clip object. A convex and closed clip geometry always obeys the restriction to a maximum number of two boundaries because, for any viewing parameter, an eye ray cannot intersect the object more than twice. Concave clip geometries can benefit from this approach as well: Very often a concave clip geometry can be approximated by two boundaries without introducing inappropriate artifacts, cf. the results and discussion in Section 7.</p><p>The basic algorithm for depth-based volume probing is as follows. First, the depth values, z front , for the first boundary are determined by rendering the frontfaces of the clip geometry into the depth buffer. Secondly, the contents of the depth buffer are stored in a texture and the fragment shader is configured to shift the depth values of all fragments in the following rendering passes by z front . Thirdly, the depth buffer is cleared and the backside of the clip geometry is rendered into the depth buffer (with depth shift enabled) to build the second boundary. Finally, slices through the volume data set are rendered and blended into the frame buffer. Here, depth shift and depth testing are enabled, but the depth buffer is not modified.</p><p>Let us examine in more detail how the depth of a fragment determines its visibility. The visibility depends on both depth clipping and depth testing. The decision whether a fragment passes depth clipping and depth testing can be expressed by a boolean function d f´z f µ, where z f is the depth of the fragment. Depth values are assumed to be described with respect to normalized window coordinates, i.e., valid depth values lie in 0 1 . In general, the value of d f´z f µ can formally be written as a logical expression,</p><formula xml:id="formula_0">d f´z f µ d clip´z f µ ´z f op z b µ (1) with d clip´z f µ ź f 0µ ´z f 1µ</formula><p>In this notation, symbolizes a logical "and". Depth clipping against the bounds 0 and 1 of the view frustum is described by d clip´z f µ. The binary logical operator op is used for depth testing against z b , the current entry in the depth buffer.  By rendering the backfaces of the clip object, entries in the depth buffer are initialized to z back z front , where z back is the unmodified depth of the backface's fragment. During the final rendering of the volume slices, the logical operator for the depth test is set to " ". Therefore, Eq. (1) can be written as</p><formula xml:id="formula_1">d f´z f µ ´z f z front 0µ ´z f z front 1µ ¡ ´z f z front z back z front µ d probe´z f µ ´z f 1 • z front µ with d probe´z f µ ź f z front µ ´z f z back µ (2)</formula><p>The term d probe´z f µ exactly represents the required logical operation for displaying the volume only in the interval z front z back . The last term,´z f 1 • z front µ, implies just a clipping against a modified far clipping plane. By shifting depth values by z front before depth clipping, only the fragments with z z front stay in the valid range of depth values and thus the volume in front of the clip geometry is cut away. Conversely, the test against the far boundary of the clip object makes use of depth testing analogously to the simple clipping algorithm in Section 3.</p><p>In the remaining of this section, we focus on how a shift of depth values by z front can be realized on graphics hardware. An analogous implementation is used in <ref type="bibr" target="#b3">[4]</ref> for depth-sorting semitransparent surfaces. The following description is based on vertex and texture shader programs available on NVidia's GeForce 3/4 in the form of OpenGL extensions <ref type="bibr" target="#b8">[9]</ref>.</p><p>After the frontfaces of the clip geometry are rendered to the depth buffer, the contents of the depth buffer are transferred to main memory as 32 bit unsigned integers per depth value z front . Subsequently, a so-called HILO texture object containing these depth values is defined. A HILO texture is a high-resolution 2D texture, consisting of two 16 bit unsigned integers per texel. An original 32 bit depth component can be regarded as a HILO pair of two 16 bit short integers. In this way, a remapping of depth values can be avoided-the unaltered contents of the depth buffer are transferred from main memory to texture memory.</p><p>Then, a texture shader program is enabled which replaces the z value of a fragment by z z front , where z front represents the z value stored in the HILO texture. The texture shader program is based on the DotProductDepthReplace fragment operation. <ref type="figure" target="#fig_4">Figure 4</ref> illustrates the structure of the depth replace program, which always consists of three texture stages. Stage zero performs a standard 2D texture lookup in a HILO texture. Stage one and two compute two dot products, Z and W , between the respective texture coordinates and the previously fetched values from the HILO texture. Still in stage two, the current fragment's depth is replaced by Z W .</p><p>The texture coordinates for each texture stage have to be set properly in order to achieve the required shift of depth values by  The standard transform and lighting part of the vertex program computes homogeneous clip coordinates from the original object coordinates by taking into account the modeling, viewing, and perspective transformations. Clip coordinates are linearly interpolated between vertices during the scan conversion of triangles and cause a hyperbolic interpolation with respect to normalized device coordinates and window coordinates. The division by w c is performed on a per-fragment basis. For perspectively correct texture mapping, texture coordinates analogous to the above clip coordinates have to be used. Consequently, the vertex program may only assign clip, but not device or window coordinates.</p><p>The homogeneous texture coordinates for stage zero are set tó</p><formula xml:id="formula_2">s 0 t 0 q 0 µ x c • w c 2 y c • w c 2 w c</formula><p>Therefore, the lookup in the HILO is addressed by</p><formula xml:id="formula_3">s 0 q 0 t 0 q 0 x n • 1 2 y n • 1 2 x w y w µ</formula><p>allowing for a mapping to the range of texture coordinates, 0 1 2 , after the division by q 0 . Therefore, a one-to-one correspondence between x-y coordinates of pixels in the frame buffer and texels in the HILO texture is established. The texture coordinates for stage one and two are set tó </p><formula xml:id="formula_4">s 1 t 1 r 1 µ w c 2 16 w c z c • w c 2 ´s 2 t 2 r 2 µ ´0 0 w c</formula><formula xml:id="formula_5">W 0 0 w c µ ¡´HI LO 1µ w c</formula><p>Still in stage two, the depth value of the fragment in window coordinates is set by the texture shader to the final, shifted value</p><formula xml:id="formula_6">z w final Z W z w z w shift</formula><p>Although the basic ideas of Everitt's <ref type="bibr" target="#b5">[6]</ref> depth-peeling and our approach are closely related, the two implementations differ significantly. In his approach, the depth structure is stored in a SGIX depth texture, and the visibility property is checked by a shadow mapping comparison (SGIX shadow) and coded into the α attribute of the fragment. Depth-peeling also makes use of DotProductDepthReplace operation, but only during the generation of the depth textures. In our approach, the depth structure is stored in a HILO texture; the visibility property is checked by a clipping test against the z values of the viewing frustum. Since the fragment's color attribute is not needed to code the result of the clipping test, clipped fragments can be distinguished from fragments that are transparent because of their α value assigned by the transfer function. On the other hand, our approach needs three texture stages to implement the depth replace operations, whereas a shadow-mapping approach needs only one lookup for the depth texture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DEPTH-BASED VOLUME CUTTING</head><p>In this section, we adapt the previous clipping mechanism to invert the role of the visibility property. Consequently, a volume cutting approach replaces the above volume probing. Here, only the volume outside the clip object remains visible. Once again, we restrict ourselves to a maximum number of two boundaries along the depth structure.</p><p>By inverting Eq. (2) for volume probing, one obtains The adaption of this idea to the graphics hardware needs to take into account that window coordinates lie in 0 1 and not in 1 1 . Moreover, the vertical axis (y axis) in <ref type="figure" target="#fig_7">Figure 5</ref> must not meet the origin, z 0, but has to be located at the midpoint of the clip geometry. The midpoint is obtained by</p><formula xml:id="formula_7">z mid z front • z back 2</formula><p>In addition, the clip object will not have a thickness of 2 as implied in <ref type="figure" target="#fig_7">Figure 5</ref>. The actual distance between front and back boundaries of the clip geometry is taken into account by the scaling factor z scale z back z front 2 <ref type="figure">Figure 6</ref> illustrates how the quantities of the clip object can be used for volume cutting via 1 z mapping. The correct function has to be g´zµ 1 2</p><formula xml:id="formula_8">z scale z z mid • 1 2</formula><p>in order to produce the graph in <ref type="figure">Figure 6</ref>.</p><p>The algorithm for depth-based volume cutting can now be formulated as follows. The implementation is again based on vertex and texture shader programs of the GeForce 3/4. First, the depth values of the frontfaces of the clip geometry are rendered into the depth buffer. Then, these values z front are transferred to main memory. Analogously, the depth values of the backfaces, z back , are determined and copied to main memory. Based on z front and z back , the CPU computes z mid and z scale and stores them as first and second components of a unsigned HILO texture. This step reduces the accuracy of depth values from originally 24 bit (as on the graphics board) to 16 bit because both components of the HILO texture have 16 bit accuracy. However, this reduced accuracy is still appropriate for almost all volume clipping applications, cf. the results in Section 7.</p><p>Then, the same texture shader program is used as for volume probing in Section 4. The depth value of each fragment is modified by DotProductDepthReplace, based on the entries in the above HILO texture. The different requirements for volume cutting are met by issuing other texture coordinates for texture stages one and two. These two stages compute the two dot products Z and W which lead to the fragment's final depth value Z W . The generation of these texture coordinates are described shortly.</p><p>Finally, the depth buffer is cleared and slices through the volume data set are rendered and blended into the frame buffer. In contrast to volume probing in Section 4, the depth buffer does not contain any information on the clip geometry and depth testing is not needed. Volume cutting is only based on the data from the HILO texture and on depth clipping in the graphics hardware.</p><p>Again we use vertex programs to set homogenous texture coordinates. The computation of coordinates for stage zero is identical to that in Section 4.</p><p>The texture coordinates for stage one and two are chosen aś</p><formula xml:id="formula_9">s 1 t 1 r 1 µ w c w c z c • w c 2 ´s 2 t 2 r 2 µ ´ 2w c 0 z c • w c µ</formula><p>In this way, the intermediate homogeneous coordinate Z is computed in texture stage one by a dot product with the entries in the HILO texture:</p><formula xml:id="formula_10">Z w c w c z c • w c 2 ¡ z w mid z w scale 1 ¡ w c¨ z w mid • z w scale • z w ©</formula><p>The first component of the HILO texture contains z w mid , the midpoint value in window coordinates; the second component is z w scale , the scaling factor with respect to window coordinates. Accordingly, W is computed by texture stage two as</p><formula xml:id="formula_11">W ´ 2w c 0 z c • w c µ ¡ z w mid z w scale 1 ¡ 2w c¨ z w mid • z w ©</formula><p>Still in stage two, the fragment's depth in window coordinates is set by the texture shader to the final value</p><formula xml:id="formula_12">z w final Z W z w z w mid • z w scale 2´z w z w mid µ 1 2 z w scale z w z w mid • 1 2</formula><p>Similarly to volume probing in the previous section, the depthpeeling approach could be used to implement a comparable version of volume cutting. Essentially, the logical operations for the depth test and the shadow test have to the inverted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CLIPPING BASED ON VOLUMETRIC TEXTURES</head><p>In contrast to the depth-based clipping algorithms from the previous sections, texture-based volume clipping approaches model the visibility information by means of a second volumetric texture whose voxels provide the information for clipping voxels of the "real" volume. Here, the clip geometry is voxelized and stored as a binary volume. This clipping texture is either a 3D texture or a stack of 2D textures. It is initialized with one for all voxels inside the clip geometry and with zero for all voxels outside. Thus, arbitrary voxelized clip objects are possible, i.e., convex and concave objects, like cubes, cylinders, spheres, tori, or even irregular objects. During rendering, a texture slice of the data set and a slice of the clip texture are simultaneously mapped onto the same slice polygon. These two textures are combined using a per-component multiplication. By means of the above texture setup and the percomponent multiplication, all the voxels to be clipped are multiplied by zero. Therefore, their color and α values do not contribute to the final image, which corresponds to a clipping of these voxels. The fragment can be completely discarded by a subsequent α test. The required functionality is completely covered by standard OpenGL 1.3 and supported by a wide range of today's GPUs.</p><p>Adapting the texture coordinates of the clipping texture allows scale and translation operations of the clip object. Additionally, when using 3D textures, a rotation of the clip object is possible. To be more precise, any affine transformation of the 3D texture-based clip geometry can be represented by a corresponding transformation of texture coordinates. A change between volume probing and volume cutting is easily achieved by applying a per-fragment invert mapping to the clipping texture. In this way, the original texture value is replaced by´1 valueµ. Since all these operations only require the change of texture coordinates or a re-configuration of per-fragment operations, all above transformations of the clip geometry are performed very efficiently. Only a complete change of the shape of the clip geometry requires a revoxelization and a reload of the clipping texture.</p><p>The above technique is based on a binary representation and therefore requires a nearest neighbor sampling of the clipping texture. If a bilinear (for a stack of 2D textures) or a trilinear (for a 3D texture) interpolation was applied, intermediate values between zero and one would result from a texture fetch in the clipping texture. Therefore, a clearly defined surface of the clip geometry would be replaced by a gradual and rather diffuse transition between visible and clipped parts of the volume, which is inappropriate for most applications. However, a missing interpolation within the clipping texture introduces quite visible, jaggy artifacts similar to those in aliased line drawings.</p><p>To overcome this problem we propose the following modification of the original approach. The voxelized clipping texture is no longer used as a binary data set, but interpreted as a distance volume: each voxel stores the (signed) Euclidean distance to the closest point on the clip object. Since all common 3D texture formats are limited to values from the interval 0 1 , the distance is mapped to this range. In this way, the isosurface for the isovalue 0 5 represents the surface of the clip object. During rendering, a trilinear interpolation in the 3D clipping texture is applied (here a stack of 2D textures is not considered because of the missing interpolation between slices). Unlike the original method, a simple multiplication of the values from the data set and the clipping texture is not computed. Instead, the value from the clipping texture is compared to 0 5. Based on this comparison, either the RGBα value from the data set (i.e., fragment is visible) or zero (i.e., fragment is invisible) is used as final color of the fragment. In this way, a clearly defined surface is introduced at the isovalue 0 5; nevertheless, jaggy artifacts are still avoided by trilinear interpolation.</p><p>The required per-fragment operations are available on current PC graphics hardware via Pixel Shader 1.0 programs (DirectX) or OpenGL extensions for NVidia's register combiners (for GeForce) and ATI's fragment shaders (for Radeon 8500). Due to the availability of multiple parallel rasterization units, usually no performance penalty is introduced for the actual rasterization; performance might be decreased by additional memory access for the clipping texture. An important advantage is the fact that any kind of clip geometry can be specified-without any limitation with respect to topology or geometry. The main disadvantages of the approach are the additional texture memory requirements for the clipping volume, the need for a voxelization of the clip geometry, and possible artifacts due to the limited resolution of the clipping texture.  <ref type="figure" target="#fig_9">Figure 7</ref> illustrates the different volume clipping methods of this paper. The underlying test data set is of size 128 3 and shows a piecewise linear change of scalar values along the vertical axis, which are mapped to yellow and blue colors. The boundary of the cubic volume has a different scalar value that is mapped to a brownishorange color. For all images of this paper, we use texture-based rendering with pre-classification based on paletted textures. Lighting is disabled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RESULTS AND DISCUSSION</head><p>The first three images in the top row of <ref type="figure" target="#fig_9">Figure 7</ref> are produced by volume probing, the images in the second row are produced by volume cutting. Images (a), (b), (e), and (f) show clipping based on a volumetric texture-as described in Section 6. The clipping texture has the same resolution as the data set. In (a) and (e), nearest neighbor sampling is applied; in (b) and (f), a trilinear interpolation within the distance field is used. Artifacts are clearly visible for nearest neighbor sampling, but not for trilinear interpolation. Images (c) and (g) demonstrate depth-based clipping as presented in Sections 4 and 5. Depth-based clipping produces visualizations of high quality because clipping is performed with per-pixel accuracy. For volume probing in (c), slight artifacts occur at the silhouette of the clip geometry. For image (h), only a single clip surface is implemented according to the simple depth-based approach of Section 3. This technique erroneously removes parts of the volume data set around the closest vertical edge. In an interactive application the introduced errors are quite disturbing; the spatial structure is very hard to grasp because clipping properties become view-dependent in this simple approach. Finally, image (d) shows the clip geometry in the form of an opaque sphere.</p><p>In <ref type="figure" target="#fig_10">Figure 8</ref>, a cube-shaped voxelized clip object is used for volume cutting within the data set of an engine block. Trilinear interpolation is applied in image (a), nearest neighbor sampling in (b). Due to trilinear interpolation, the corners and edges of the cubic clip object are smoothed out in (a), i.e., the original clip geometry cannot be reproduced. An example of a significant difference between both methods is marked by red arrows in <ref type="figure" target="#fig_10">Figure 8</ref>. For this specific example of an axis-aligned cube, nearest neighbor sampling gives good results. However, a generic clip object that contains both smooth regions and sharp features can only be appropriately handled by depth-based clipping. <ref type="figure" target="#fig_11">Figure 9</ref> shows clipping in a medical MR data set. Image (a) uses a voxelized spherical clip object and image (b) a voxelized cylindrical clip object. The clipped part of the volume is moved away from the remaining parts of the volume. <ref type="figure" target="#fig_11">Figure 9</ref> (c) demonstrates that volumetric clipping is capable of processing rather complex clip geometries and topologies. <ref type="figure" target="#fig_11">Figure 9 (d)</ref> shows that depth- The implementation of our volume renderer is based on C++, OpenGL, and GLUT. It is platform-independent and runs on Linux and Windows PCs. The performance measurements in <ref type="table" target="#tab_1">Table 1</ref> were conducted on a Windows XP PC with Athlon XP 1800+ CPU and NVidia GeForce 4 Ti 4600 graphics board. The size of the data set was 256 3 and the window sizes were 512 2 or 1024 2 . A slicing approach for a 3D texture was chosen. The performance numbers show that clipping based on voxelized geometry causes rather modest additional rendering costs. Depth-based clipping, however, is slower by a factor 3-4 (compared to rendering without clipping), which is caused by two factors. First, a transfer between graphics board and main memory is needed to build the HILO textures once per frame. Secondly and more importantly, all four texture stages are needed.</p><p>From our experiments with the different clipping technique we have learnt that each method has specific advantages and disadvantages. The best-suited technique has to be chosen by the type of application and the requirements stated by the user.</p><p>The main advantage of texture-based volume clipping is that it provides an explicit control over each voxel of a volume. Therefore, arbitrarily complex objects can be used as clip geometry. Furthermore, only modest additional rendering costs are introduced due to the availability of multiple parallel rasterization units-as long as the memory bandwidth is not the limiting factor. Another advantage is the potential extension to volume tagging applications: The voxelized "clip" object would not only contain information on the visibility property, but more detailed information such as an object identification number. This ID could help to apply different transfer functions to different regions of the data set. As another variation, the "clip" volume could contain continously varying α values to allow for a smooth fading of volumetric regions.</p><p>The main disadvantage of the volumetric clipping approach is a possibly inaccurate reproduction of the clip geometry. Nearest neighbor sampling of the clipping texture results in visible, jaggy artifacts and a poor image quality. Trilinear interpolation, on the other hand, does not allow for sharp geometric features of the clip object. Moreover, the voxelization approach prevents the user from changing the shape of a clip object in real time. Affine transformations of the clip object, however, can be performed at interactive frame rates. Another problem are the additional texture memory requirements for the clipping volume. However, the size of the clipping texture can be chosen independently of the size of the volume. Certainly, if one wants to have control over each single voxel, the clipping volume has to have the same size as the volume. A way to save texture memory for the clipping texture is to re-use the same texture slice of the clip object multiple times. For example, a cylindrical clipping volume can be generated by a single texture containing a zero-filled circle; translated versions of this texture define a cylinder.</p><p>An advantage of depth-based volume clipping is the high quality of the resulting images, since clipping is performed with per-pixel accuracy. Aliasing problems related to texture mapping cannot occur because there is a one-to-one mapping between pixels on the image plane and the texels representing the clip geometry. Arbitrary changes of the clip geometry cause (almost) no additional costs be- cause no voxelization is required. Although the performance does not reach the levels of the voxel approach, interactive applications are still feasible. Our depth-based volume clipping methods benefit from the fact that the clip geometry has to be processed only once per frame. This is a very important difference to the method by Westermann and Ertl <ref type="bibr" target="#b16">[17]</ref>. Their technique needs to update the values in the stencil buffer for each slice by rendering the whole clip geometry. This approach does not scale well for complex clip geometries or large volume data sets with many slices.</p><p>The main problem of depth-based volume clipping in the present form is the restriction to convex clip geometries. From our experience, however, artifacts that might occur for concave objects are negligible in many applications. Once a high enough opacity is collected along a light ray, voxels further away from the viewer do not contribute to the final image. Often the artifacts caused by concave objects are located in these "hidden" regions. Furthermore, the issue of concave geometries could be solved by introducing a multipass rendering approach. The clip object could be split in "convex" parts; for each part the presented algorithms provide correct results which are then combined to the final image. However, multi-pass rendering would further reduce the overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION AND FUTURE WORK</head><p>We have introduced new clipping methods that are capable of using complex clip geometries. All methods are suitable for texturebased volume rendering and exploit per-fragment operations on the graphics hardware to implement clipping. The techniques achieve high overall frame rates and therefore support the user in interactive explorations of volume data. We have presented depth-based volume probing and volume cutting. Both analyze the depth structure of the clip geometry to decide which regions of the volume have to be clipped. Depth-based volume clipping is particularly well suited to produce high-quality images. In another approach, a clip object is voxelized and represented by a volumetric texture. Information in this volume is used to identify clipping regions. This approach allows to specify arbitrarily structured clip objects and is a very fast technique for volume clipping with complex clip geometries.</p><p>In future work we will investigate how visualization applications can optimally benefit from sophisticated volume clipping. In particular, we plan to enhance medical imaging by including clipping geometries extracted from segmentation information. In this context, volume tagging will be of specific interest. Furthermore, we will try to find appropriate methods to visualize dense representations of 3D vector fields by means of volume clipping. Finally, we will investigate specific interaction techniques and user interfaces which allow to model clip geometries effectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Volume clipping in an MR data set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Depth structure of a clip geometry for a single pixel in the frame buffer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of depth-based volume probing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Texture shader for DotProductDepthReplace.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3</head><label>3</label><figDesc>illustrates how our clipping algorithm operates on the depth structure. A shift by z front is applied to all depth values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Image (a) shows valid values x for f´xµ x, image (b) shows valid values x for g´xµ 1 x. z front . Texture coordinates are issued on a per-vertex basis within the transform and lighting part of the rendering pipeline and can be computed by a vertex program. The following three coordinate systems have to be considered: homogeneous clip coordi-nates´x c y c z c w c µ, normalized device coordinates´x n y n z n µ ´x c w c y c w c z c w c µ, and window coordinates´x w y w z w µ. Valid normalized device coordinates lie in 1 1 3 , valid window coordinates are assumed to lie in 0 1 3 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>d cutting´z f µ ź f z front µ ´z f z back µ ( 3 )Figure 6 :</head><label>36</label><figDesc>for volume cutting. Logical "or" is symbolized by " ". The cutting function d cutting´z f µ is almost identical to the negation of the probing function, d probe´z f µ-except for the fact that z front and z back are valid values in both volume probing and volume cutting. Unfortunately, logical operations for depth clipping and depth testing have to be of the form Eq.(1), which is not compatible with Eq. (3).Our solution to this problem is based on the following considerations. Let us first have a look at a simplified scenario with the Schematic illustration of the depth structure for volume cutting via 1 z mapping.expression´x1µ ´x 1µ for values x. This expression can be rewritten as´f´xµ 1µ ´f´xµ 1µ by introducing an identity function f´xµ x.Figure 5(a) illustrates this scenario graphically. The valid values x are determined by the intersection of f´xµ with the horizontal lines at ¦1. Now consider a similar situation in which the function f´xµ is replaced by another function g´xµ 1 x, as shown in Figure 5 (b). Here, the logical expres-sion´g´xµ 1µ ´g´xµ 1µ leads to valid values x according to´x 1µ ´x 1µ. Therefore, a mapping between the form of Eq. (3) and the required form of Eq. (1) is achieved. Please note that the DotProductDepthReplace fragment operation is able to compute an reciprocal value with high accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Comparison of volume clipping techniques.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Comparison of trilinear interpolation (a) and nearest neighbor sampling (b) for a voxelized cubic clip object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Clipping for an MR data set. In images (a)-(c), voxelized clip objects are used; image (d) illustrates depth-based clipping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance measurements in frames per second.</figDesc><table><row><cell>Window size</cell><cell>512 2</cell><cell>1024 2</cell></row><row><cell>No clipping</cell><cell>24.1</cell><cell>8.4</cell></row><row><cell>Voxelized clip object</cell><cell>15.8</cell><cell>5.1</cell></row><row><cell>Depth-based probing</cell><cell>8.2</cell><cell>2.5</cell></row><row><cell>Depth-based cutting</cell><cell>7.1</cell><cell>2.1</cell></row><row><cell cols="3">based volume clipping allows for geometries with a large number</cell></row><row><cell cols="2">of triangles-such as the Stanford bunny.</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank the anonymous reviewers for helpful comments to improve the paper. Special thanks to Bettina A. Salzer for proofreading.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reality Engine graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Akeley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 1993 Conference Proceedings</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="109" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Accelerated volume rendering and tomographic reconstruction using texture mapping hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Foran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1994 Symposium on Volume Visualization</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Pipeline Rendering: Interaction and Realism through Hardware-Based Multi-Pass Rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Diefenbach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Transparency in interactive technical illustrations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Diepstraten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<title level="m">3D Visibility: Analytical Study and Applications. PhD thesis, Université Joseph Fourier</title>
		<meeting><address><addrLine>Grenoble I</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Interactive order-independent transparency. White paper, NVidia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Everitt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Computer Graphics: Principles and Practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Foley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Dam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Feiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Hughes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Addison-Wesley</publisher>
			<pubPlace>Reading, Massachusetts</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploring geoscientific data in virtual environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fröhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Barrass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zehner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Plate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Göbel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization Proceedings</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="169" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">NVIDIA OpenGL Extension Specifications. NVIDIA Corporation</title>
		<editor>M. J. Kilgard</editor>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semi-automatic generation of transfer functions for direct volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kindlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Durkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Volume Visualization</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Interactive volume rendering using multi-dimensional transfer functions and direct manipulation widgets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kindlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization Proceedings</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="255" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Transparency and antialiasing algorithms implemented with the virtual pixel maps technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mammen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="43" to="55" />
			<date type="published" when="1989-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visualization viewpoints: The transfer function bake-off</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lorensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Baja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kindlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Avila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Machiraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="16" to="23" />
			<date type="published" when="2001-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Interactive exploration of volume line integral convolution based on 3D-texture mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rezk-Salama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hastreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Teitzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization Proceedings</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Direct volume rendering with shading via threedimensional textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Gelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1996 Symposium on Volume Visualization</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gigabyte volume viewing using split software/hardware interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Volz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2000 Symposium on Volume Visualization</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="15" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficiently using graphics hardware in volume rendering applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Westermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 1998 Conference Proceedings</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="169" to="179" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
