<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sea of Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">G</forename><surname>Aliaga</surname></persName>
							<email>aliaga@bell-labs.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Lucent Bell Labs Princeton University Harvard University Lucent Bell Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Lucent Bell Labs Princeton University Harvard University Lucent Bell Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimah</forename><surname>Yanovsky</surname></persName>
							<email>yanovsky@fas.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Lucent Bell Labs Princeton University Harvard University Lucent Bell Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingrid</forename><surname>Carlbom</surname></persName>
							<email>carlbom@bell-labs.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Lucent Bell Labs Princeton University Harvard University Lucent Bell Labs</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Sea of Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.3.3 [Picture and Image Generation]: Display and viewing algorithms. I.3.7 [Three-dimensional Graphics and Realism]: Virtual Reality image-based rendering</term>
					<term>capture</term>
					<term>reconstruction</term>
					<term>interactive</term>
					<term>walkthrough</term>
				</keywords>
			</textClass>
			<abstract>
				<p>A long-standing research problem in computer graphics is to reproduce the visual experience of walking through a large photorealistic environment interactively. On one hand, traditional geometry-based rendering systems fall short of simulating the visual realism of a complex environment. On the other hand, image-based rendering systems have to date been unable to capture and store a sampled representation of a large environment with complex lighting and visibility effects. In this paper, we present a &quot;Sea of Images,&quot; a practical approach to dense sampling, storage, and reconstruction of the plenoptic function in large, complex indoor environments. We use a motorized cart to capture omnidirectional images every few inches on a eye-height plane throughout an environment. The captured images are compressed and stored in a multiresolution hierarchy suitable for real-time prefetching during an interactive walkthrough. Later, novel images are reconstructed for a simulated observer by resampling nearby captured images. Our system acquires 15,254 images over 1,050 square feet at an average image spacing of 1.5 inches. The average capture and processing time is 7 hours. We demonstrate realistic walkthroughs of real-world environments reproducing specular reflections and occlusion effects while rendering 15-25 frames per second.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Creating an interactive walkthrough of a complex real-world environment remains one of the most challenging problems in computer graphics. While many researchers have tackled parts of the problem, there exist to date no system that can reproduce the photorealistic richness of a large, real-world environment at interactive rates. For example, a virtual visit to the Louvre or Versailles must reproduce the exquisite detail of the paintings and sculptures while at the same time convey the grandeur of the former royal residences. And, as in a real museum visit, the user must be able to walk anywhere, even up close to an interesting work of art.</p><p>Image-based rendering (IBR) achieves photorealism by capturing and resampling a set of images. An IBR system usually takes as input photographs of a static scene, and constructs a samplebased representation of the plenoptic function <ref type="bibr" target="#b0">[1]</ref>. This function, È´Ü Ý Þ Ø µ, describes the radiance leaving or arriving at any point´Ü Ý Þµ from any direction´ µ with any wavelength at any time Ø. The plenoptic representation can be quickly resampled to render photorealistic images for novel viewpoints without constructing a detailed 3D model or simulating global illumination,  <ref type="figure">Figure 1</ref>: Sea of Images. We capture a dense Sea of Images through a large environment, store them in a multiresolution hierarchy, and generate real-time reconstructions of novel images in an interactive walkthrough system. and the rendering time for novel images can be independent of a scene's geometric complexity. However, current IBR methods are only able to represent either small scenes (e.g., a statuette) or diffuse environments with low geometric complexity (e.g., a room or a hallway). Our goal is to create an IBR walkthrough system that supports an interactive experience for large and complex real-world scenes.</p><p>We create interactive walkthroughs using a "Sea of Images" (SOI) -a collection of images every couple inches throughout a large environment. In our case, we acquire omnidirectional images on an eye-height plane throughout the environment <ref type="figure">(Figure 1</ref>). This representation provides a densely sampled 4D approximation to the plenoptic function parameterized by camera position (Ü Ý) and incoming ray direction ( ). We capture a SOI by moving a catadioptric video camera mounted on a motorized cart back and forth in a zigzag pattern through a static environment. We compress the acquired data in a multiresolution hierarchy so that it can be accessed efficiently for continuous sequences of viewpoints. We use time-critical algorithms to prefetch relevant image data and featurebased morphing methods to reconstruct novel images during interactive walkthroughs.</p><p>As compared to previous IBR methods for interior environments, our SOI approach replaces the difficult computer vision problems of 3D reconstruction and surface reflectance modeling with the easier problems of motorized cart navigation, data compression, and working set management. Rather than using sophisticated planning and reconstruction algorithms to acquire directly a minimal representation of the plenoptic function, we capture a highly redundant data set and then compress it into a representation that enables realtime working set management for walkthroughs ( <ref type="figure">Figure 2</ref>).</p><p>The advantages of this approach are four-fold. (1) It enables accurate image reconstructions for novel views in environments with</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Minimal IBR Model</head><p>Dense Sampling</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sparse</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sampling</head><p>Planning Compression <ref type="figure">Figure 2</ref>: Acquiring Image Samples. Instead of sophisticated planning and vision algorithms to acquire the minimal sample set, we oversample and use compression and working set management to access samples during a real-time walkthrough. specular surfaces, a great amount of geometrical detail, and complex visibility changes. (2) It does not require an accurate geometric model to produce novel images over a wide range of viewpoints. <ref type="formula">3</ref>It provides a method for image-based modeling of a large, concave environment without sophisticated gaze planning. And, (4) it supports rendering of inside-looking-out images in an IBR interactive walkthrough system. We believe no other IBR approach includes all these features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Our "Sea of Images" is related to several previously described IBR methods. In this section, we review the most closely related representations and discuss the advantages of our representation for walkthrough applications.</p><p>Movie maps <ref type="bibr" target="#b20">[21]</ref> and panoramas <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b6">7]</ref> sample the plenoptic function in 2D for a small, sparse set of reference viewpoints. While this representation is simple to capture, it grossly undersamples the spatial dimensions of the plenoptic function, and thus it is not able to capture specular highlights or complex occlusion effects common in interior scenes. Also, it requires either constraining the user to view the scene only from reference viewpoints <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b15">16]</ref> or acquiring accurate geometry to enable image reconstructions for viewpoints far from any captured view <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b6">7]</ref>. This is difficult for complex environments, even when the environment is modeled using laser range scanners <ref type="bibr" target="#b25">[26]</ref>. In contrast, a Sea of Images samples both spatial and angular dimensions densely and interpolates nearby reference images when reconstructing novel views. Thus, it is better able to capture and reproduce subtle lighting and occlusion changes. Furthermore, only approximate geometry or depth information is required for reconstructing novel views since the viewpoints of captured images are usually very close to novel viewpoints typical in a walkthrough.</p><p>Light Fields <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20]</ref> provide a 4D approximation to the plenoptic function parameterized by a point on a "camera plane"´× Øµ and a point on an "object plane"´Ù Úµ. In contrast, our 4D representation is parameterized by a point on a camera plane´Ü Ýµ and a view direction´ µ. This difference is significant for interior walkthrough applications for three reasons. First, it is much easier to capture a Sea of Images than it is to acquire a Light Field. Special gantries can acquire the´× Ø Ù Úµ Light Fields for small objects, but seems difficult to scale to large, complex environments. In contrast, capturing a´Ü Ý µ representation simply requires moving an omnidirectional video camera in a single plane. Second, the´Ü Ý µ parameterization simplifies coverage of a large, concave space. Since each Light Field provides samples for a limited range of novel viewpoints, it is difficult to cover a large complex environment. In contrast, we capture reference images where we expect the user to explore, and then the system can synthesize new images anywhere within the triangulated mesh of reference viewpoints. Finally, our´Ü Ý µ approach provides a pre-filtered multiresolution representation of light radiance arriving at a point, which is important for walkthroughs where surfaces in the same image can appear at significantly different distances. In contrast, Light Fields with fixed uniform sampling of the´Ù Úµ plane will either be undersampled for close-up viewpoints or oversampled for distant viewpoints. Surface Light Fields <ref type="bibr" target="#b35">[36]</ref> are similar to our representation as they also parameterize the plenoptic function by a point and a direction. However, they describe the radiance leaving a surface point rather than arriving at a camera location. This difference has two significant implications for walkthrough applications. First, the Surface Light Field requires an accurate geometric description of the scene in order to aggregate and store samples on surfaces, where as our approach works well with little or no geometry. Second, and more importantly, the data access patterns of a walkthrough application have much more coherence for nearby viewpoints and incoming ray directions than for nearby surface points and outgoing directions. <ref type="bibr">Hence</ref>  <ref type="bibr" target="#b36">[37]</ref>, and Kang et al.'s omnidirectional multibaseline stereo algorithms <ref type="bibr" target="#b16">[17]</ref>. This research is largely orthogonal to ours, as we could use their methods to acquire more accurate geometric models to improve the quality of our reconstructed images. However, an important feature of our approach is that it does not require a very accurate 3D model to produce novel images because we capture images at very high density and nearby reference images are available for almost any novel viewpoint. Our approach avoids the difficult problems of 3D scene reconstruction (e.g., dense correspondence, depth estimation) and replaces them with a simpler data management problem, which is addressed in this paper.</p><p>Delta Trees <ref type="bibr" target="#b7">[8]</ref> and LDI Trees <ref type="bibr" target="#b4">[5]</ref> are related to our approach in that they store multiresolution representations of the radiance field. Pre-filtered image data is available for every surface at multiple resolutions (e.g., like mip-maps), and thus the amount of real-time filtering required to reconstruct images from novel viewpoints is significantly reduced. Our SOI approach also stores image data for each surface at multiple resolutions (pre-filtered by the camera according to the location of each captured image). However, our multiresolution representation is viewpoint-centric, rather than object-centric, which greatly improves cache coherence during an interactive walkthrough.</p><p>Plenoptic Stitching <ref type="bibr" target="#b2">[3]</ref> is most closely related to our work. But, it samples the plenoptic function densely in only one spatial dimension, capturing images in a cross-hatch pattern of lines separated by meters. This results in a 3.5D approximation to the plenoptic function, which does not capture most visibility or lighting changes in the vertical dimension. In addition, the reconstruction algorithm warps samples only along one degree of freedom (radial lines), which generally produce lower quality reconstructions than our system. For instance, pixels are reconstructed by combining samples from a captured image significantly nearer to and another one farther from the sampled environment, thereby combining samples at different resolutions and causing blurring and ghosting in the rendered images. Our system avoids this problem by warping three nearby reference images. Additionally, we use multiresolution data compression and real-time working set management algorithms.</p><p>In summary, previous IBR methods can be classified by how many image samples are collected and how much a priori 3D scene geometry and reflectance information is required to produce realistic novel views. Some methods capture images densely, thus requiring little geometric or reflectance information (e.g., <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b12">13]</ref>). However, so far, they are limited to small scenes and small ranges of novel viewpoints. Other methods sparsely sample the space of possible viewpoints (e.g., <ref type="bibr" target="#b22">[23]</ref>). However, they must acquire de-tailed geometric information to warp images to novel views. Hybrid methods capture images from a semi-dense set of viewpoints and utilize approximate geometric information (e.g., <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">30]</ref>). These methods usually produce good results only for scenes without complex surface reflections or visibility effects.</p><p>Our system captures a dense sampling of images over a large area. This allows it to reproduce specular highlights and complex visibility effects during walkthroughs of large environments over a wide range of novel viewpoints, without requiring detailed geometric information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESEARCH CHALLENGES</head><p>In this paper, we investigate a dense sampling of the plenoptic function for interactive walkthroughs of interior environments. We capture closely spaced omnidirectional images, compress them, and resample them to form novel views. This approach allows reconstruction of novel views with subtle lighting and visibility effects. However, it requires that we address the following three questions:</p><p>"How can we obtain a dense set of calibrated images over a large area in a practical manner?" -We have built a capture system from off-the-shelf components that uses a motorized cart to move an omnidirectional video camera in a zigzag pattern at eye height through a large environment. We have also developed camera calibration and pose estimation algorithms for large, multi-room environments.</p><p>"How do we compress the massive amounts of captured data?" -We have developed a multiresolution IBR representation that enables us to exploit coherence in nearby images resulting in significant compression.</p><p>"How can we access a large out-of-core data set for real-time walkthroughs?" -We manage the working set through predictive prefetching and caching algorithms that load images from disk based on estimated "benefits" and "costs" as a user moves through an environment interactively.</p><p>In the following three sections we investigate answers to these questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CAPTURE &amp; CALIBRATION</head><p>The first problem is the capture and calibration of a dense Sea of Images in a manner both practical and automatic. The method should work reliably in large, multi-room environments without invasive hardware and without per-image manual processing. For example, capturing an image every few inches inside a non-trivial environment (e.g., a small museum) should take no more than an afternoon.</p><p>Most previous IBR capture methods rely either upon specialpurpose hardware, specific environment content, or upon careful path and gaze planning. Levoy and Hanrahan <ref type="bibr" target="#b19">[20]</ref> use a gantry to capture a dense set of images uniformly over a plane. Similarly, Shum and He <ref type="bibr" target="#b29">[30]</ref> use special-purpose hardware to capture images on a circle. Large-scale (optical) trackers could be used for image capture but require a significant hardware installation. Teller et al. <ref type="bibr" target="#b32">[33]</ref> capture and calibrate thousands of outdoor images spaced by several to tens of meters and depend on initial measurements from a global positioning system. These methods are practical only for small objects, single rooms, or outdoor environments. They seem difficult to extend to a wide range of dense viewpoints in complex interior environments.</p><p>Vision-based approaches often depend on structure-from-motion and on scene content <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b10">11]</ref>. Jung and Taylor <ref type="bibr" target="#b15">[16]</ref> describe a vision-based approach augmented with inertial sensors. To compensate for drift inherent in these devices, global features or landmarks must be identified. While this strategy could in principle work for large interior environments, it is very dependent on the content of the environment. Moreover, the results so far are not sufficiently robust to reconstruct detailed geometry and reflectance effects (e.g., visibility changes, specular highlights).</p><p>Gaze planning algorithms <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b30">31]</ref> can be used in larger environments, but require a priori knowledge of the location of the interesting detail and also what is of interest to the user. Unless an accurate model already exists, a system based on these approaches may miss significant image samples that are difficult to capture at a later date.</p><p>In our approach, we capture a dense Sea of Images by moving an omnidirectional camera on a motorized cart throughout the environment, continuously capturing image data to disk. The capture system is built entirely from off-the-shelf components. The system is small enough to move anywhere in the environment where a person may walk. We replace path planning with image redundancy. We may capture an environment more in areas of potential occlusion or where a user may want to study objects in detail -this does not require planning prior to capture, but can be decided on the spot. Redundant data is automatically discarded at a later stage.</p><p>Prior to capture, we calibrate the intrinsic camera parameters <ref type="bibr" target="#b11">[12]</ref>. The camera manufacturer provides a calibration method, but it does not yield sufficiently accurate results. The manufacturer assumes that the lens is perfectly telecentric and produces an orthographic projection onto the film plane. However, relaxing the assumption of telecentricity, we obtain more accurate intrinsic parameters <ref type="bibr" target="#b1">[2]</ref>, which we fix for the entire capture session. Our camera pose estimation algorithm tracks fiducials in the captured images and triangulates position and orientation. The process begins by an operator creating an approximate floor plan of the environment (e.g., using a tape measure). Then, the algorithm uses the floor plan to suggest fiducial locations so as to ensure a sufficient number of fiducials are visible at all times. To obtain high reliability and accuracy, the fiducials are small battery-powered light bulbs placed in the environment by the operator. After providing the initial camera position, the algorithm uses the current position estimate and the floor plan to determine which fiducials may be visible in the current image ( <ref type="figure" target="#fig_1">Figure 3</ref>), tracks these from image to image, and triangulates camera position and orientation.</p><formula xml:id="formula_0">d 1 Camera Locations f 6 f 1 f 2 f 3 f 4 f 5 f 7 f 8 f 9 f 10 d 2 a d 1 Camera Locations f 6 f 1 f 2 f 3 f 4 f 5 f 7 f 8 f 9 f 10 d 2 a</formula><p>The fiducials, used for capture, appear in the captured images. Although they do not seem to interfere with the visualization, we could later use image-processing techniques to remove the fiducials and replace them with an estimate of the local background color.</p><p>To determine a globally consistent set of camera poses and fiducial locations, we use bundle adjustment <ref type="bibr" target="#b33">[34]</ref>, a non-linear least squares optimization method. We alternate between using the estimated fiducial locations to compute the camera pose for each captured image and using a sparse subset of the camera pose estimates to refine the global position of the fiducials (i.e., about 10% of the pose estimates uniformly distributed through the data set).</p><p>The goal of the bundle adjustment procedure is to find the 3D fiducial locations´ µ and camera poses´Ü Ý µ that minimize the difference between the observed fiducial locations Ì and the projection of the estimated fiducial locations. The function È ´ Ü Ý µ encapsulates the projection from 3-space onto our omnidirectional images <ref type="bibr" target="#b24">[25]</ref>. If the observed error is zero-mean Gaussian, then bundle adjustment corresponds to the maximum likelihood estimator. The error term used for bundle adjustment is given below (the Cronecker delta term AE is 1 when fiducial f was tracked on image i):</p><formula xml:id="formula_1">AE È ´ Ü Ý µ Ì</formula><p>When this process has converged, we obtain a dense set of omnidirectional images with calibrated camera parameters on a plane at eye height.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">COMPRESSION</head><p>The second problem is the compression of the massive amounts of acquired data. Our captured data sets usually contain thousands of images, requiring gigabytes of storage. However, only a small portion of the data is needed to reconstruct an image for a novel viewpoint, and there is a great deal of coherence among the images from adjacent viewpoints. Thus, as long as our disk storage system is large enough to hold the captured data, the main problem is working set management.</p><p>Our focus is quite different from previous work on data management for IBR representations. For instance, the original Light Field paper <ref type="bibr" target="#b19">[20]</ref> describes a method that uses vector quantization and Lempel-Ziv coding. Follow-up work has investigated other compression schemes <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22]</ref>. However, these methods focus on reducing the overall storage requirements of the IBR representation. They assume that the entire data set (or large subsets) will be decompressed and stored in memory before any novel image is rendered. Of course, this assumption is unrealistic in situations such as ours where the size of the IBR representation exceeds the capacity of host memory.</p><p>We make the assumption that disk storage is sufficient to hold the entire data set and that the goal is to compress the data into a form suitable for efficient access during an interactive walkthrough. The challenge is to take advantage of the redundancy in the data while optimizing the data layout for cache coherence.</p><p>The first and most obvious option is to compress and store every captured image independently, e.g., in a separate JPEG file. (In fact, we JPEG compress the data during capture.) This approach is very straightforward, but does not take advantage of inter-viewpoint redundancy, which can improve both storage and bandwidth utilization.</p><p>A second option is to utilize prediction and replace some images with the residual, or difference, between the original image and the predicted image. With a good prediction strategy, the residual has less energy than the original image, resulting in significant compression gain. This technique is used in video codecs based on motion compensation, such as MPEG <ref type="bibr" target="#b0">1</ref> . A similar strategy has been used for encoding far-field representations in a walkthrough system for synthetic environments <ref type="bibr" target="#b34">[35]</ref>.</p><p>One difficulty in predictive coding is finding the proper spacing of the I-frames. Frequent I-frames yield little compression gain. Infrequent I-frames make non-linear image access inefficient, and also yield poor cache utilization for our walkthrough application.</p><p>We optimize both compression and cache utilization by storing images in a multiresolution hierarchy. Instead of the MPEG strategy of I-frames followed by a number of P-frames followed by an Iframe, etc., we store images in nested trees, where the root of each tree is an I-frame and all other nodes are P-frames. Since the system caches image data associated with interior nodes of the hierarchy, only P-frames must be read for nearby viewpoints lower in the hierarchy, improving the disk-to-memory bandwidth utilization.</p><p>Our multiresolution data structure is a binary tree built bottom up using half edge-collapse operations <ref type="bibr" target="#b13">[14]</ref>. The tree is initialized by creating a set of nodes representing each captured image at its viewpoint. Using a (Delaunay) triangulation of the nodes, we compute a priority for every edge and place the edges in a heap. We collapse the highest priority edge and replace the nodes of the edge endpoints with a new common parent node <ref type="figure" target="#fig_2">(Figure 4a</ref>). To avoid introducing resampled images in the hierarchy, we place the parent node at the same viewpoint location as one of its children and replace the original image with an image reference. We then locally retriangulate the current node-set, update the heap, and repeat the collapse process for the next highest priority edge <ref type="figure" target="#fig_2">(Figure 4b</ref>) until no edges remain in the heap, storing a full image with the remaining (root) node <ref type="figure" target="#fig_2">(Figure 4c</ref>). We have considered several measures to compute edge collapse priority. Ideally, we would like to collapse edges between the most similar images first. One measure is to compute the image energy of the residual. A faster approximation is to collapse the shortest edge, which tends to work well since nearby images are often the most similar.</p><p>In order to optimize the compression gain, working set size, and decompression time, we keep I-frames at every L levels in the tree (Ä for our environments), creating subtrees of P-frames, with each P-frame predicted from the root of its subtree. While prediction relative to the root instead of another P-frame does not give optimal compression, it improves the working set size and decompression time.</p><p>Our motion prediction utilizes a simple 3D geometric proxy of the environment to warp the image at the root of the subtree to the location of its child. The residual is the difference of the predicted child and the root image. To improve compression performance, we optionally use an optimization process for each difference image computation. The optimization searches for the best set of translation and rotation offsets to register the proxy to the images so as to minimize image energy and improve compression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">REAL-TIME DATA MANAGEMENT</head><p>The third problem is the management of the out-of-core data in a real-time walkthrough system. Our goal is to pre-load and cache images from disk for rendering novel views as a user navigates interactively through the IBR environment. Given the amount of available storage, bandwidth, and processing capacity of each hardware component, our task is to develop a time-critical algorithm that loads and caches data in a manner that produces the highest quality images while maintaining an interactive frame rate <ref type="bibr" target="#b9">[10]</ref>.</p><p>Ideally, the algorithm guarantees that a set of images is always surrounding the viewpoint allowing for good reconstruction and smooth transitions from one set of images to another. Moreover, upon failure, the algorithm should exhibit graceful degradation and eventually return to maximum quality. To this end, we maintain a cut through the multiresolution tree. This cut guarantees that at any location within the walk-able space we always have a set of surrounding images. These images will be used for reconstruction, as is described in the following section.</p><p>We use an asynchronous prefetching process to maintain a cache of images in main memory. The user's velocity predicts which images are needed next. The prefetcher maintains two linked lists of tree nodes ( <ref type="figure">Figure 5</ref>). The evict list defines a cut through the tree such that all nodes on and above this cut are in cache. The fetch list consists of all the immediate children of the evict list nodes. The fetch list and the evict lists are sorted by priority; the first image in the fetch list has the highest probability of being needed next and the last image in the evict list has the highest priority of being needed next. Initially, the evict list consists of just the root node and the fetch list are the immediate children of the root node. To display each frame at run-time, the renderer uses the current cut to determine which images to use for reconstructing the current view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Root Evict List</head><p>Fetch List } Evict Fetch <ref type="figure">Figure 5</ref>: Evict and Fetch Lists. We maintain two linked lists of tree-nodes that define a cut through the tree: the evict list and the fetch list. The prefetching process updates the likelihood of needing the nodes in both lists. As long as there is a fetch node that is more likely to be needed than the least likely evict node, and cache space can be made available, the nodes are swapped.</p><p>To update the cut, we swap nodes between the two lists. We use the observer's latest viewpoint and predicted velocity to compute for all nodes in the evict list and fetch list the probability that its image is needed next. The probability of a node increases as the observer gets closer to the node and as the direction from the observer to the node approaches the viewing direction. Since in our hierarchy we force nodes higher in the tree to be loaded first, the distance to a node is determined by the distance to the bounding box surrounding the parent and all its children. If the cache is not yet full, we swap the node in the evict list that has the lowest probability with the node with the highest probability in the fetch list and then load the image data. If the cache is full, we must swap and flush the image data of a sufficient number of evict nodes to fit the image data for the next fetch node. The prefetching process recomputes the probabilities and sorts the fetch and evict lists at regular intervals (e.g., 10 to 30 times a second).</p><p>If desired, we could optimize the prefetcher for narrow FOV reconstructions (e.g., 60 degrees). For instance, by subdividing each omnidirectional image into several tiles, the prefetcher could use the current predicted viewing direction to only load the subsets of the omnidirectional images necessary for the FOV to reconstruct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RECONSTRUCTION</head><p>The final stage of our process is to reconstruct synthetic images for novel viewpoints. The goal is to produce high quality images with little blurring or popping during an interactive sequence.</p><p>Since we are interested in first-person walkthroughs, we optimize our system for reconstruction from viewpoints near the eyeheight plane in which the images were captured. We project the user's viewpoint onto the triangular mesh of images and extract the three closest images, which we warp to the viewpoint, and blend using the barycentric coordinates of the viewpoint <ref type="bibr" target="#b8">[9]</ref>.</p><p>Since the images to blend have different centers of projection, we warp them using a set of 3D "feature points" distributed on the surface of a coarse polygonal proxy. We project the visible feature points onto the image planes of the novel and captured viewpoints. Then, we use the features to morph each captured image to the novel viewpoint <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19]</ref>, obtaining a reconstructed omnidirectional image. In addition, we can create arbitrary projections, including planar and cylindrical ( <ref type="figure" target="#fig_3">Figure 6</ref>).</p><p>An alternate approach is to map the captured omnidirectional images onto the proxy directly and blend them using texture mapping. But, this approach may produce artifacts, such as the incorrect mapping of portions of the images to the wrong surfaces, especially along silhouette edges of the proxy.</p><p>The accuracy of the proxy becomes increasingly important as the observer viewpoint approaches an object. Our reconstruction method may start exhibiting ghosting artifacts when the viewer gets too close to a small object, occupying the entire field-of-view (FOV). To remedy this, we could either sample more densely, create a more accurate 3D proxy <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b36">37]</ref>, or use more complex reconstruction algorithms, including ones that combine image data from multiple reference images <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20]</ref>. Even without these extensions, a novel viewpoint is almost always near three captured views, which are pre-filtered versions of images very similar to the desired image, so we can usually produce resampled images with quality almost equal to the captured images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">IMPLEMENTATION DETAILS</head><p>Our software system is implemented in C/C++ and OpenGL on a SGI Onyx2 with 4 195MHz R10000 processors using an Infinite-Reality2 graphics subsystem.</p><p>Our omnidirectional video camera is based on a commercial Cyclovision/Remote Reality S1 unit <ref type="bibr" target="#b24">[25]</ref>, which acquires 1024x1024 images over a hemispherical FOV. The camera is placed on top of a motorized cart carrying a battery and a small PC, and it is moved using radio remote control. The cart moves at an average speed of 7 inches/sec and our RAID disk holds approximately 50GB of Env.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setup</head><p>Capture   data. We compress the images on the PC and write them to disk in JPEG format at a rate of 2 frames per second, which yields an image every couple of inches for nearly 60 hours. We move the cart back and forth along closely spaced paths (separated by a couple of inches) to acquire a dense Sea of Images over a plane at eye height throughout the entire environment. We use graphics hardware to compute the difference images for the multiresolution tree. Current graphics hardware does not support an efficient way to compute image energy, so we use the CPU. To accelerate the precomputation, we typically compute image energy values at 128x128 pixel resolution, noting that at full resolution we should expect similar relative values.</p><note type="other">Fiducial Pose Area Avg</note><p>Our reconstruction algorithm also takes advantage of graphics hardware. While features are interpolated on the CPU, all image data is blended using either multiple-pass texture blending or the accumulation buffer. We rely upon a NetLib package for Delaunay triangulation of features and the graphics subsystem for paging of texture memory on demand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">RESULTS &amp; OBSERVATIONS</head><p>In this section, we evaluate how well the Sea of Images approach achieves the goal of walking through a complex environment with photorealistic images. Specifically, we ask the following questions: (1) is our capture process practical?, (2) are our data management strategies effective?, (3) can the system maintain interactive frame rates?, and (4) does it in fact reconstruct images with complex illumination and visibility effects during interactive walkthroughs?</p><p>So far, we have experimented with three environments. The first one ("Office") is a small room, around the size commonly used for IBR demonstrations. The second one ("Library") is the lobby area of a large public library. Finally, the third environment ("Museum") is a small museum. It provides our most challenging test case because it has a concave floor-plan, it contains many specular surfaces (glass cases and brass plaques on the wall), and it contains complex geometry (plants on the floor and museum pieces in the cases). No previous walkthrough system could produce photorealistic images of such a difficult environment at interactive rates. <ref type="table" target="#tab_2">Table 1</ref> presents statistics about our capture process. Overall, the capture process for each of the three environments took only a couple of hours. The setup time includes making lighting changes, camera setup (e.g. aperture adjustments) and placing and measuring the fiducial setup in the environment. The actual capture time during which the motorized cart moved through the environment was relatively short (e.g., 10 minutes per 1,000 images). The proxy for each environment was created using a measuring tape and a simple text-based polygon editor. The entire capture process yielded a Sea of Images cumulatively covering more than 1,000 square feet of walkable area at a density such that the average distance from a random point on the eye height plane to its nearest image is 1.5 inches. We conjecture that few other capture processes would be able to both cover such a large space and sample fine details within the viewpoint space typical of an interactive walkthrough. <ref type="table" target="#tab_3">Table 2</ref> lists the size of the data before and after compression, as well as compression times. Overall, the three environments contain 15,254 images and require 48GB of disk space in their raw form. JPEG compression of the images reduced the size of the data sets to 2.25GB (at quality factor 75). Our multiresolution compression strategy achieved another 2-3X compression factor. This extra compression is determined mainly by the efficiency of our coder for difference images and by the order in which we collapse nodes in our multiresolution tree structure. By computing per-difference-image optimized translation and rotation offsets for registering the proxy to the images, we obtain a compression performance of 54-79X of the original data, at the expense of 5 seconds of optimization time per difference image (totalling about 3 hours, 5 hours, and 14 hours for the Library, Office, and Museum environments, respectively). Without this optimization, we obtain 40-55X compression of the original data (totalling about 1 hour, 0.5 hours, and 2.5 hours). The images and sequences we show use the optimized offsets.  <ref type="table">Table 3</ref>: Prefetching Performance. We report the effectiveness of the prefetcher as a ratio of two distances: the average distance to a fetched image and the average distance to any captured image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Resol</head><p>The prefetcher uses the multiresolution hierarchy to ensure that the reconstruction algorithm always has some set of images in memory surrounding the novel viewpoint. To measure the performance of the prefetcher, we collected statistics as the user walked through the Museum environment at different speeds along a typical path. We also used different image resolutions to test the sensitivity of our prefetcher to increasing data sizes. During these tests, we estimated how well the prefetcher was working by taking the average distance to the surrounding closest trio of cached images and dividing it by the average distance to the closest among all imagesa value of 1.0 is a perfect score <ref type="table">(Table 3)</ref>. Note how the distance to the closest loaded image increases as the user walks faster or asks for higher resolution images. This result corresponds to graceful degradation in image quality in our walkthrough system.  Our current run-time system reconstructs novel views at an average of 15-25 frames per second. Reconstructions at 512x512 or 256x256 pixels can be performed at an average of 25 frames per second, while higher resolution views at 1024x1024 pixels are produced at about 15 frames per second. <ref type="figure">Figure 7</ref> plots the frames per second for an example path through the museum environment using 512x512 images. The frame rate is near constant because our system always warps three images into the frame buffer. We believe the variations in frame times are due to texture swapping.</p><p>Finally, <ref type="figure">Figures 8-10</ref> show the quality of novel views reconstructed by our Sea of Images approach. We can reproduce specular highlights moving over a surface without modeling material properties. For instance, <ref type="figure">Figure 8</ref> shows an example specular highlight moving over the surface of a shiny bronze plaque. Because the density of sampled viewpoints is so large, we can almost always achieve reconstructions of similar quality to the captured omnidirectional images. <ref type="figure" target="#fig_5">Figure 9</ref> shows a captured image (top) and a synthetic image (bottom) reconstructed from a novel viewpoint at the middle of a Delaunay triangle of captured viewpoints. Differences in the images are almost imperceptible. Also, we can reproduce images of objects at multiple resolutions. <ref type="figure">Figure 10</ref> demonstrates renderings of a single object at multiple distances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">CONCLUSION &amp; FUTURE WORK</head><p>We have introduced a novel approach for creating interactive walkthroughs using image-based rendering techniques. What distinguishes our approach from other IBR approaches is that we are not limited to small environments with restricted visibility changes such as with the Lightfield/Lumigraph <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20]</ref>. We are also not constrained to sparse sampling, which in turn limits rendering to diffuse environments and often requires viewpoint planning and a priori information about the geometry of the scene. We remove these constraints by mapping the IBR problem onto a compression and working-set management problem. However, the system is not without limitations. First, our current capture device (a catadioptric omnidirectional camera) has limited resolution. While this affects the resolution of images currently generated, the methods described in this paper are independent of a particular capture device and would work for any omnidirectional video camera arrangement. We have purchased a FullView camera <ref type="bibr" target="#b23">[24]</ref> and begun to investigate other multiple-camera configurations that acquire higher resolution images.</p><p>Second, we currently acquire image data sampling the lower hemisphere of directions from viewpoints at some density on an eye-height plane. The resulting representation provides enough information for reconstruction of any downward-looking view for which the eye-height plane is unoccluded. Expanding our capture procedure to acquire images containing a wider range of view directions and/or multiple viewpoint heights would expand the space of allowable novel views. We are investigating use of spherical cameras to partially address this issue. In addition, we would like to be able to determine a conservative estimate of what average image capture spacing is "dense enough" for a particular environment.</p><p>Third, we are looking into other (multiresolution) reconstruction methods for a Sea of Images. In particular, tracking visible features from image-to-image in order to generate more accurate correspondences between the images surrounding the novel viewpoint.</p><p>Finally, our capture process currently requires some manual effort. An operator estimates the locations of fiducial, builds an approximate proxy model, and drives a motorized cart back and forth via remote control. While these activities are not too burdensome (they take 15 to 112 minutes for our three environments), the system could be made easier to use by further automating the capture process. This is possible by estimating camera pose in real-time from the fiducials or from automatically detected features, and letting the <ref type="figure">Figure 10</ref>: Far-to-Near Image Sequence. We show the museum environment and focus on a particular item from three distances.</p><p>PC control the wheel motors to drive the cart autonomously. This is a topic for future work.</p><p>In conclusion, we believe the Sea of Images approach to be a fertile bed for future work. Never before have researchers had access to such a large and dense sampling of an environment. We believe it could lead to new 3D reconstruction algorithms, novel compression strategies, and new walkthrough applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Capture and Calibration. We place small portable fiducials that track robustly in the environment. Using triangulation (distances and angle ), we obtain an estimated camera position and orientation from every pair of visible fiducials. A coarse floor plan predicts the location of fiducials in the captured image. Bundle adjustment optimization refines the global location of the fiducials and the camera pose for each captured image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Multiresolution Tree. We show a three-step sequence (ac) of building a small tree for a set of five captured images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Reconstruction Examples. We show several reconstructed images for the library environment. The left image is the reconstructed omnidirectional image. The upper right is a cylindrical projection and the bottom right is a planar projection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FramesFigure 7 :Figure 8 :</head><label>78</label><figDesc>Frame Rate. We show the frame rate for a pre-recorded path through the museum environment at 512x512 pixels. The average frame rate is 25 frames per second. Specular Highlights. This figure demonstrates specular highlights moving over the surface of a shiny bronze plaque in the museum environment. The top snapshot shows a cylindrical projection of a subset of the reconstructed image. The four-image sequence is a zoomed-in planar projection of the reconstructed image. Observe the highlight moving across the plaques as the viewpoint moves laterally (a-d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>Captured vs. Reconstructed Comparison. The top omnidirectional image is one of the nearby captured images. The bottom omnidirectional image is reconstructed for a novel viewpoint that is as far as possible from the surrounding images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Capture and Image Statistics. We captured three Sea of Images, covering a total of 1050 square feet with 15254 images at an average image spacing of 1.5 inches, and average capture and calibration time of 2.8 hours per environment.</figDesc><table><row><cell>Env.</cell><cell>Raw</cell><cell>Flat</cell><cell>Diff-Image</cell><cell>Optimized</cell><cell>Tree</cell><cell>Diff</cell><cell>Optimized</cell></row><row><cell></cell><cell cols="5">(MB) (MB) Hierarchy (MB) Hierarchy (MB) Building</cell><cell>Images</cell><cell>Diff Images</cell></row><row><cell>Library</cell><cell>6000</cell><cell>318</cell><cell>150 (40:1)</cell><cell>113 (54:1)</cell><cell>4 min</cell><cell>60 min</cell><cell>3 hrs</cell></row><row><cell>Office</cell><cell>11000</cell><cell>376</cell><cell>198 (55:1)</cell><cell>139 (79:1)</cell><cell>6 min</cell><cell>35 min</cell><cell>5 hrs</cell></row><row><cell cols="3">Museum 31000 1,560</cell><cell>740 (42:1)</cell><cell>564 (55:1)</cell><cell>50 min</cell><cell>160 min</cell><cell>14 hrs</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Compression Statistics: We show the raw, JPEG-compressed, and multiresolution hierarchy sizes of our Sea of Images. The multiresolution hierarchy sizes and times are shown for both unoptimized and per-difference-image optimized compression. The average compression time is 4.7 hours per environment.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">MPEG uses the terms "I-frames" for the original frames and "P-frames" for the predicted frames; we use the same notation.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We are grateful to Sid Ahuja, Multimedia Communications Research VP at Bell Labs, for supporting this research. We also would like to extend our gratitude to Bob Holt for his mathematical help and to the Bell Labs and Princeton University staff that allowed us to capture their environments. Thomas Funkhouser is partially funded by a NSF CAREER grant (CCR-0093343).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The plenoptic function and the elements of early vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Models of Visual Processing</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1991" />
			<biblScope unit="page" from="3" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Accurate catadioptric calibration for real-time pose estimation in room-size environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aliaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2001-07" />
			<biblScope unit="page" from="127" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Plenoptic Stitching: A scalable method for reconstructing 3D interactive walkthroughs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingrid</forename><surname>Aliaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carlbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2001-08" />
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unstructured Lumigraph Rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><surname>Mcmillan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Gortler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2001-08" />
			<biblScope unit="page" from="425" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">LDI Tree: A hierarchical representation for image-based rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fa</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselmo</forename><surname>Lastra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGGRAPH 1999</title>
		<meeting>ACM SIGGRAPH 1999</meeting>
		<imprint>
			<date type="published" when="1999-08" />
			<biblScope unit="page" from="291" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quicktime VR -an image-based approach to virtual environment navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Shenchang Eric</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGGRAPH 1995</title>
		<meeting>ACM SIGGRAPH 1995</meeting>
		<imprint>
			<date type="published" when="1995-08" />
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">View interpolation for image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Shenchang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGGRAPH 1993</title>
		<meeting>ACM SIGGRAPH 1993</meeting>
		<imprint>
			<date type="published" when="1993-08" />
			<biblScope unit="page" from="279" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The Delta Tree: An object-centered approach to image-based rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><surname>Mcmillan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Fuchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MIT AI Lab Technical Memo 1604</title>
		<imprint>
			<date type="published" when="1996-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient view-dependent image-based rendering with projective texture-mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Borshukov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Rendering Workshop</title>
		<imprint>
			<date type="published" when="1998-06" />
			<biblScope unit="page" from="105" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Database management for interactive display of large architectural models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graphics Interface &apos;96</title>
		<imprint>
			<publisher>Canadian Information Processing Society / Canadian Human-Computer Communications Society</publisher>
			<date type="published" when="1996-05" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Markerless tracking using planar structures in the scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE and ACM International Symposium on Augmented Reality</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Catadioptric camera calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Geyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="398" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radek</forename><surname>Gortler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Grzeszczuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">F</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lumigraph</surname></persName>
		</author>
		<title level="m">Proceedings of ACM SIGGRAPH 1996</title>
		<meeting>ACM SIGGRAPH 1996</meeting>
		<imprint>
			<date type="published" when="1996-08" />
			<biblScope unit="page" from="43" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Progressive meshes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIG-GRAPH 1996</title>
		<meeting>ACM SIG-GRAPH 1996</meeting>
		<imprint>
			<date type="published" when="1996-08" />
			<biblScope unit="page" from="99" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On enhancing the speed of splatting with indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ihm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization</title>
		<meeting><address><addrLine>Atlanta, GA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="69" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Camera trajectory estimation using inertial sensor measurements and structure from motion results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="732" to="737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">3D scene data recovery using omnidirectional baseline stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="364" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Realistic surface reconstruction of 3D scenes from uncalibrated image sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Visualization and Computer Animation</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="115" to="127" />
			<date type="published" when="2000-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Polymorph: Morphing among multiple images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungyong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Wolberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Yong</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="58" to="71" />
			<date type="published" when="1998-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Light Field Rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Levoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">M</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH 96</title>
		<meeting>SIGGRAPH 96</meeting>
		<imprint>
			<date type="published" when="1996-08" />
			<biblScope unit="page" from="31" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Movie-maps: An application of the optical videodisc to computer graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lippman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGGRAPH 1980</title>
		<meeting>ACM SIGGRAPH 1980</meeting>
		<imprint>
			<date type="published" when="1980-07" />
			<biblScope unit="page" from="32" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Progressive compression and rendering of light fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Magnor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Endmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Girod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Vision, Modeling, and Visualization (VMV-2000)</title>
		<meeting>Vision, Modeling, and Visualization (VMV-2000)<address><addrLine>Saarbrücken, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="199" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Plenoptic modeling: An imagebased rendering system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><surname>Mcmillan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH 95</title>
		<meeting>SIGGRAPH 95</meeting>
		<imprint>
			<date type="published" when="1995-08" />
			<biblScope unit="page" from="39" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A true omnidirectional viewer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nalwa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<pubPlace>Holmdel, NJ</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Bell Laboratories</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Catadioptric omnidirectional camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="482" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Capturing, processing, and rendering real-world scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nyland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lastra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mccue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Videometrics and Optical Methods for 3D Shape Measurement, Electronic Imaging</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">4309</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The Wavelet Stream: Interactive Multi Resolution Light Field Rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Straßer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Rendering Techniques 2001: 12th Eurographics Workshop on Rendering</title>
		<imprint>
			<date type="published" when="2001-06" />
			<biblScope unit="page" from="127" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Structure and motion from image sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vergauwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cornelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tops</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Verbiest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. on Optical 3-D Measurement Techniques</title>
		<meeting>Conf. on Optical 3-D Measurement Techniques</meeting>
		<imprint>
			<date type="published" when="2001-10" />
			<biblScope unit="page" from="251" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A review of viewpoint planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Marshall</surname></persName>
		</author>
		<idno>97008</idno>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
		<respStmt>
			<orgName>University of Wales, College of Cardiff, Department of Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rendering with concentric mosaics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeung</forename><surname>Heung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Wei</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH 99</title>
		<meeting>SIGGRAPH 99</meeting>
		<imprint>
			<date type="published" when="1999-08" />
			<biblScope unit="page" from="299" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Imaging all visible surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Stuerzlinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graphics Interface &apos;99</title>
		<imprint>
			<date type="published" when="1999-06" />
			<biblScope unit="page" from="115" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Creating full view panoramic mosaics and environment maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGGRAPH 1997</title>
		<meeting>ACM SIGGRAPH 1997</meeting>
		<imprint>
			<date type="published" when="1997-08" />
			<biblScope unit="page" from="251" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Calibrated, Registered Images of an Extended Urban Area</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seth</forename><surname>Teller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Antone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Bodnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satyan</forename><surname>Coorg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manish</forename><surname>Jethwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neel</forename><surname>Master</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2001-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bundle adjustment -a modern synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mclauchlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vision Algoritms: Theory and Practice</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Spatially encoded farfield representations for interactive walkthroughs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mayer-Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="348" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Surface Light Fields for 3D Photography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">N</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">I</forename><surname>Azuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Aldinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duchamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Werner</forename><surname>Stuetzle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGGRAPH 2000</title>
		<meeting>ACM SIGGRAPH 2000</meeting>
		<imprint>
			<date type="published" when="2000-07" />
			<biblScope unit="page" from="287" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">VHS to VRML: 3D graphical models from video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Cross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMCS</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="51" to="57" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
