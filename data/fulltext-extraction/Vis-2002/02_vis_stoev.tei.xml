<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Case Study on Automatic Camera Placement and Motion for Visualizing Historical Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislav</forename><forename type="middle">L</forename><surname>Stoev</surname></persName>
							<email>sstoev@gris.uni-tuebingen.de</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Tübingen</orgName>
								<orgName type="institution" key="instit2">WSI/GRIS</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Straßer</surname></persName>
							<email>strasser@gris.uni-tuebingen.de</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Tübingen</orgName>
								<orgName type="institution" key="instit2">WSI/GRIS</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Case Study on Automatic Camera Placement and Motion for Visualizing Historical Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CR Categories: D.2.2 [Software Engineering]: Design Tools and Techniques H.5.2 [Information Interfaces and Presentation]: User Interfaces I.3.3 [Computer Graphics]: Picture/Image Generation-Viewing algorithms Automatic Camera Control</term>
					<term>Visualization</term>
					<term>Historical Data</term>
					<term>Time-dependent Data</term>
					<term>Visualization Techniques</term>
				</keywords>
			</textClass>
			<abstract>
				<p>In this paper, we address the problem of automatic camera positioning and automatic camera path generation in the context of historical data visualization. After short description of the given data, we elaborate on the constraints for the positioning of a virtual camera in such a way that not only the projected area is maximized, but also the depth of the displayed scene. This is especially important when displaying terrain models, which do not provide good 3D impression when only the projected area is maximized. Based on this concept, we present a method for computing an optimal camera position for each instant of time. Since the explored data are not static, but change depending on the explored scene time, we also discuss a method for animation generation. In order to avoid sudden changes of the camera position, when the previous method is applied for each frame (point in time), we introduce pseudo-events in time, which expand the bounding box defined by the currently active events of interest. In particular, this technique allows events happening in a future point in time to be taken into account such that when this time becomes current, all events of interest are already within the current viewing frustum of the camera.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>When visualizing historical data, until now most of the historians use some kind of maps on which motion of people in the time dimension is displayed as a line (or sometimes as an arrow). In some cases such visualizing aids may include the date at the beginning of the motion and the date at the end of the motion, as well as visualize the number of the participating people. These props may make the visualization more comprehensive and vivid. However, when complex behavior containing many details of various events in time is visualized (e.g. complex battles), the traditional illustrations fail to appropriately display these dependencies. In order to provide an insight into such types of data, we had to implement tools for navigation through the time and the space <ref type="bibr" target="#b11">[12]</ref>.</p><p>Another challenge we face when trying to explore historical data sets is defined by the too many degrees of freedom of the navigation tools. The user is often disoriented and unable to navigate adequately in the time dimension and the three space dimensions. For circumventing this problem, the visualization system has to support orientation aids for all four dimensions explored. Additionally, an automatic camera path and fly control generator turned out to be very useful exploration props.</p><p>In this paper, we present an extension of the guided exploration tools proposed in <ref type="bibr" target="#b11">[12]</ref>. In particular, we describe a method for determining a good camera position for time-invariant scenes. This is a generally applicable algorithm, which we use in the context of digital elevation models. We present a new criterion for the positioning the virtual camera in the space, which not only maximizes the projected area, but also aims to improve the depth impression of the scene. Afterwards, we extend this method towards allowing automatic tracking of multiple events in time and space. We have also paid special attention to the smooth motion of the camera, since this may be crucial for the orientation of the observer and may also cause motion sickness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we review related work in two different categories, related to the techniques presented in this paper. In the first part, we elaborate on the automatic positioning of a camera in a given scene in order to achieve a "good" view of the scene. Afterwards, we review related work in the area of object tracking and camera motion for scene exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Automatic Camera Placement</head><p>There are many approaches described in the literature, attempting to define criteria for automatic placement of a virtual camera in such a way, that the scene behind a pinhole camera is viewed in the "best" possible way. Unfortunately, until now there are no objective measurements and criteria for the quality of the resulting camera position and the corresponding rendered image. In some cases, we can easily determine which of two given views is the better one, however, the process of generating these images as well as the translation of the definition "good" into numbers are not trivial tasks.</p><p>Most of the algorithms proposed in the literature focus more often than not on stand alone objects or scenes. In other words, a sphere of virtual camera positions represents the set of all possible views. Determining the best camera position is now reduced to the task of determining the point on the sphere for which a given quality function is maximized (or a function defining the deviation from a good viewpoint minimized).</p><p>The first attempts to automatically define a good virtual camera position we know of were made by Kamada and Kawai <ref type="bibr" target="#b9">[10]</ref>. They define a (parallel) projection of a scene (displayed as a wire-frame model) to be good, if the number of surface normals orthogonal to the view direction (degenerated faces) is minimal. Unfortunately, this method has one main drawback: it does not guarantee that we will see as much details or objects as possible. In addition, the authors consider only parallel projections and do not take into account the size of the projected objects, which may vary under perspective projection depending on the distance to the viewpoint.</p><p>A more recent work <ref type="bibr" target="#b1">[2]</ref> uses a modification of the coefficients introduced in <ref type="bibr" target="#b9">[10]</ref>. In the original work, these coefficients were used to minimize the angle between the view direction and the surface normals of all surfaces present in the scene. In contrast, <ref type="bibr">Barral</ref>  projection. Additionally, they introduce balance coefficients and three different exploration coefficients, which are combined in order to define a function for determining the quality of a perspective projection.</p><p>Arbel and Ferrie <ref type="bibr" target="#b0">[1]</ref> apply entropy maps for viewpoint selection in order to perform object recognition. They use conditional probability density functions to construct entropy maps relating ambiguity as a function of viewing position. Weinshall and Merman <ref type="bibr" target="#b14">[14]</ref> presented a similar work, where they discuss a practical and empirical study of an object recognition system.</p><p>Roberts and Marshall introduce in their work <ref type="bibr" target="#b10">[11]</ref> an approach for selecting a minimized number of views allowing adequate representation of each object face. This task is performed according to given constraints and other features. The authors of this work define the best viewpoint to be the one that "has a minimum angle to all corresponding face normals".</p><p>Vázquez et al. <ref type="bibr" target="#b12">[13]</ref> propose a metric based on the entropy of the image. They define the best viewpoint as the one with the highest entropy, i.e. the one that sees the maximum of information. For this they apply the ratio of the projected area of each surface to the area covered by the projection of all surfaces in the scene.</p><p>Gómez et al. <ref type="bibr" target="#b7">[8]</ref> presented another milestone work in this area. In their work, the authors discuss a set of rules for the "niceness" of a perspective projection. Each of these rules is based on one of the criteria: regularity, simplicity and minimum crossing, and monotonicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Camera Control and Objects Tracking</head><p>A second field of related work is introduced by the aim to enable automatic motion of the virtual camera for scene exploration or to provide a simple set of declarative instructions, which allow the "directing" of an animation generation.</p><p>A pioneer work in this area is presented by Blinn <ref type="bibr" target="#b2">[3]</ref>, who derives from the desired image the position and orientation of the camera in the space. He defines a set of transformations that allow the definition of an image-space position of the tracked objects and solves the transformations for the camera position.</p><p>Gleicher and Witkin <ref type="bibr" target="#b6">[7]</ref> presented another interesting paper based on the idea to define image-space controls. They present a set of screen-space and world-space constraints, i.e. distance between two 3D points in the image plane, from which camera positions are derived.</p><p>The first who proposed a camera motion based on the shotconcept known from cinematography are Drucker and Zeltzer <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. They describe a system that is based on camera modules. Each module consists of a set of constraints, an initializer, and a controller. The constraints define the camera motion in the space and are active during the time the module is active. The controller interprets the user input and generates constraints out of it.</p><p>Christianson et al. <ref type="bibr" target="#b3">[4]</ref> describe in their work a similar system for real-time generation of camera shots and assembly of film sequences using a declarative camera control language (DCCL). The authors elaborate on both the shooting, as well as the cutting part of filmmaking.</p><p>A similar work, the Virtual Cinamatographer, is presented by He et al. <ref type="bibr" target="#b8">[9]</ref>. Principles and idioms of cinematography are encoded and in a finite state machine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Goals</head><p>The methods described in Section 2.1 work well for scenes with uniformly distributed geometry. In the scenario of terrain models, however, where most of the faces have similar normal direction, the result is a view "from above". The so generated images fail to give a good overview of the scene's depth. The rendered pictures look "flat" and the elevation details are either not perceivable for the observer, or give only a poor depth impression.</p><p>In this paper, we describe a new criterion, which attempts to cope with this drawback. In particular, we maximize simultaneously the depth in the projection of the scene and the projected surface. This allows us to display a more vivid view of the scene, which is rich on depth information. The presented approach is not only applicable on terrain models, but can be utilized for finding a good viewpoint in any other scene.</p><p>Considering the automatic tracking of dynamic objects, we describe a method for automatic positioning of the virtual camera. The problem arising when we automatically compute the camera position for each frame during the exploration of a dynamic scene is that when an event appears outside of the current viewing frustum, the camera position changes suddenly in order to display all currently active elements. This may be very awkward in an animation. A solution to this problem is proposed in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DATA VISUALIZATION</head><p>In a previous work, we have shown how motions in a historical data set are visualized and what tools are used for the exploration of the data set <ref type="bibr" target="#b11">[12]</ref>. In particular, we deal with time intervals and want to display the motion within the given time interval, as well as various other parameters of the moving armies (e.g. belligerence, religion, affiliation, weapon equipment etc.). The path of the moving troops within a time interval can be visualized as a line on top of the terrain model. In order to encode as much information as possible, instead of displaying the line, we display line-aligned cones. These primitives can be used to encode several of the attributes addressed above at a time (e.g. using the cone radius and color).</p><p>Beside the navigation techniques described in our previous work, we are still unable to automatically generate animations, given a particular data set. The only technique providing limited guided exploration is the "follower"-technique, which allows a virtual camera to track a given motion in time. Unfortunately, with this technique we are able to track only one particular motion in the time. The emphasis of this work is the visualization and adaptive camera motion in order to automatically display complex battles and other types of time dependent events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">AUTOMATIC CAMERA PLACEMENT</head><p>Let us first consider the definition of a good camera position for the given scenario: terrain model. Unlike other scenes, where the normals of the displayed geometry are distributed almost uniformly in all directions, in our scenario the majority of the normals point in the same direction: the terrain-up direction. Applying the approaches described in Section 2.1, produces an image, which provides a very poor depth impression of the scene. This observation holds for any scene, in which most of the normal vectors point in the same direction. This is due to the fact that most of the methods described above attempt to maximize the projected area, the entropy, or to determine the direction of the majority of the normal vectors.</p><p>In order to circumvent this problem, we not only maximize the projected area 1 of the scene, but also the depth of the scene and the inverse of the normalized absolute difference between the depth and the projected area. Let pa be the normalized projected area, ds be the normalized depth of the scene 2 , then the function we are maximizing is given by <ref type="bibr" target="#b2">3</ref> :</p><formula xml:id="formula_0">F (p) = αa • pa + αs • ds + α d (1 − |ds − pa|)</formula><p>. In this formula, p is the camera position. The coefficients αa (projected area), αs (depth of the scene) and α d (difference 1−αa −αs) define the impact of the given term. They sum up to 1 and serve as weights for the single terms.</p><p>Assuming that the coefficients αa, αs, and α d are equally distributed αa = αs = α d = 1/3, the function F has its maximum when the projected area and the depth of the image have their maximum. In addition, the third term has its maximum when the difference between the normalized projected area and the image depth is minimal. The values of the function F and of its single terms are shown in <ref type="figure">Figure 1</ref> and <ref type="figure">Figure 2</ref> for two sample scenes. For the terrain scenario, it turned out that a coefficient set of αa = α d = 1/4 and αs = 1/2 worked best. For other scenes this parameters may vary. These weights may also be computed automatically using the ratios between the dimensions of the scene's bounding box, respectively using the distribution of the normal vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CAMERA MOTION AND OBJECT TRACKING</head><p>In the previous section, we have introduced the computation of a good camera orientation, which corresponds to a point on a sphere surrounding a given scene. Since the terrain model used to visualize the historical data we are dealing with has most of the face-normal vectors pointing in the z-direction, we can use the computed view direction to display the entire terrain model, as well as only parts of it. In other words, we can zoom into particular regions of interest of the terrain model using the same camera orientation and varying the camera position in order to view a given target location. On the other hand, in order to unequivocally describe the virtual camera, we have to compute not only its orientation, but also the position of the camera. Fortunately, for each instant in time and for each interval of time we know the exact position of the displayed events. Thus, we can easily compute the bounding box of the currently active events. The ray defining the orientation of the camera and the midpoint of this bounding box introduce a line in the space on which the virtual camera has to be positioned.</p><p>The only unknown parameter is the distance d between the midpoint of the bounding box and the virtual camera. We assume that the opening angle of the virtual camera is given. This assumption allows us to compute d in such a way, that the entire bounding box around the active events is within the viewing frustum of the virtual camera.</p><p>This approach for determining a good position and orientation of the virtual camera is suitable for exploring and viewing timeinvariant scenes. However, when exploring historical data we are dealing with dynamic scenes, in which subjects have different position in the space depending on the currently active scene time. Moreover, we are dealing with objects that may suddenly appear and disappear at a given position and a given point in time. If we would compute the position of the virtual camera for each frame separately, this may cause sudden changes of the camera position, which may confuse the observer (see <ref type="figure" target="#fig_1">Figure 3</ref>). This jumpy camera The one computed taking into account the pseudo-events on the line l is drawn with a dashed line. At time t4, the event c is active as well. In this case the bounding box would suddenly change (from the solid line to the dashed line) and cause the camera to suddenly change its position. Applying the proposed technique results in a smooth motion from the dashed box at time t3 to the dashed box at time t4. motion is caused by the fact that troops that are inactive at a given time ti, and thus are not taken into account when the currently active bounding box is computed, may become active at time ti+1.</p><p>In order to cope with this problem, we have to compute the virtual camera position in such a way, that it simulates a zooming in and out of the scene in order to capture the appearing and disappearing events. Therefore, for each (currently inactive) event outside the bounding box of active events, we compute a line with minimum distance to the bounding box. Depending on the length of the line (distance) and the time at which the subjects will become active, we can compute a velocity value describing the required motion of the camera, in order to make the subjects visible within its viewing frustum. If this speed of motion is greater than a value that will allow a smooth camera motion, we use the points on the line to expand the bounding box of currently active events. In other words, we define a function that generates pseudo-events (on the computed line) in time ( <ref type="figure" target="#fig_2">Figure 4</ref> shows an example of the method). These events are taken into account when the bounding box of active events is (re-) computed (see <ref type="figure" target="#fig_1">Figure 3)</ref>. In this way, we can guarantee a smooth motion of the virtual camera, which is especially important for the generation of animations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">LESSON LEARNED</head><p>The presented work was motivated by the fact, that navigating the space and time simultaneously may be quite confusing to inexperienced users. Especially when dealing with historical data, where the purpose of the visualization is the explanation of particular complex events in time, the users' attention should be on the data itself not on the interaction. Thus, we have implemented a system for automatic generation of animations given a historical data set. In order to make this visualization more intuitive, a good prop is the alignment of the viewing direction with the direction "north" of the map. This preference can be taken into account as an additional term dn of the function F when it is maximized as introduced above. The term dn encodes the deviation from the "perfect" view direction and introduces a kind of penalty for not looking towards north.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>Even though the two areas addressed in this paper, namely the automatic placement and the automatic motion of a virtual camera, have been subject of intensive research during the past years, we found that none of the published approaches is suitable for the visualization of historical data. Most of the methods for automatic camera placement attempt to maximize the projected information and do not perform well when applied on data with uniform face (normal) distribution, e.g. terrain data. In this paper, we have proposed a new criterion for the quality of a perspective projection taking into account the depth of the scene. Thus, we not only attempt to maximize the projected area, but also the depth of the projected scene.</p><p>Moreover, we have presented a method for automatic generation of animations, given a particular historical data set. We described how the camera position is computed in order to guarantee smooth tracking of moving objects. The proposed techniques enable the automatic generation of vivid animations, guaranteeing that no features of the data set (or a particular part of interest) are missed during the automatic exploration.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FrameFigure 1 :FrameFigure 2 :</head><label>12</label><figDesc>The diagram shows the single terms of the function F and the function itself. The weights are set to αa = αs = α d = 1/3. The "best" camera position on the given path is at frame 43. The diagram shows the single terms of the function F and the function itself. Similar to theFigure 1, the weights are set to αa = αs = α d = 1/3. The "best" camera positions on the given path are at frames 42 and 112.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>This figure shows the path of three points in time. All of them move from left to right. At time t1, t2, and t3, the active elements are a and b. The computed bounding box is shown for each point in time. At time t3, the velocity value (time and distance l) of point c with respect to the currently active bounding box approaches a given maximum value. The bounding box computed taking into account the currently active events is shown as a solid line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>The upper image shows the terrain model and the automatically computed exploration path (αa = α d = 1/4 and αs = 1/2). The lower row shows six of the images seen from this path. The arrows in the upper image show the camera positions and orientations.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Instead of using the projected area, one may also use the entropy of the viewpoint as proposed by Vazquez et al.<ref type="bibr" target="#b12">[13]</ref>.<ref type="bibr" target="#b1">2</ref> Maximum of the scene depth over all frames is scaled to 1, all other frames are scaled properly into the interval<ref type="bibr" target="#b0">[1,</ref> 0].<ref type="bibr" target="#b2">3</ref> Occlusion is not taken into account.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Viewpoint selection by navigation through entropy maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><forename type="middle">P</forename><surname>Ferrie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th IEEE International Conference on Computer Vision (ICCV-99)</title>
		<meeting>the 7th IEEE International Conference on Computer Vision (ICCV-99)<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">I</biblScope>
			<biblScope unit="page" from="248" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scene Understanding Techniques Using a Virtual Camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Barral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Dorme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Plemenos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EUROGRAPHICS&apos;2000)</title>
		<meeting>EUROGRAPHICS&apos;2000)</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Where am I? what am I looking at?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Blinn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="76" to="81" />
			<date type="published" when="1988-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Declarative camera control for automatic cinematography (video)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">B</forename><surname>Christianson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><forename type="middle">E</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth National Conference on Artificial Intelligence and the Eighth Innovative Applications of Artificial Intelligence Conference</title>
		<meeting>the Thirteenth National Conference on Artificial Intelligence and the Eighth Innovative Applications of Artificial Intelligence Conference<address><addrLine>Menlo Park</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press / MIT Press</publisher>
			<date type="published" when="1996-08" />
			<biblScope unit="page" from="148" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Intelligent camera control in a virtual environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeltzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Graphics Interface &apos;94</title>
		<meeting>Graphics Interface &apos;94<address><addrLine>Banff, Alberta, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Canadian Information Processing Society</publisher>
			<date type="published" when="1994-05" />
			<biblScope unit="page" from="190" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">CamDroid: A system for implementing intelligent camera control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeltzer</surname></persName>
		</author>
		<idno>0-89791-736-7</idno>
	</analytic>
	<monogr>
		<title level="m">1995 Symposium on Interactive 3D Graphics</title>
		<editor>Pat Hanrahan and Jim Winget</editor>
		<imprint>
			<publisher>ACM SIGGRAPH</publisher>
			<date type="published" when="1995-04" />
			<biblScope unit="page" from="139" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Through-the-lens camera control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gleicher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Witkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 92 Conference Proceedings</title>
		<editor>Edwin E. Catmull</editor>
		<imprint>
			<date type="published" when="1992-07" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="331" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Nice perspective projections. J. Visual Communication and Image Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hurtado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Sellarès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toussaint</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The virtual cinematographer: A paradigm for automatic real-time camera control and directing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 96 Conference Proceedings, Annual Conference Series</title>
		<editor>Holly Rushmeier</editor>
		<meeting><address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="1996-08" />
			<biblScope unit="page" from="4" to="09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A simple method for computing general position in displaying three-dimensional objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomihisa</forename><surname>Kamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoru</forename><surname>Kawai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, Graphics, and Image Processing</title>
		<imprint>
			<date type="published" when="1988-01" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="43" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Viewpoint selection for complete surface coverage of three dimensional objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Marshall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference -BMVC 98</title>
		<meeting>the British Machine Vision Conference -BMVC 98</meeting>
		<imprint>
			<date type="published" when="1998-09" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Case Study on Interactive Exploration and Guidance Aids for Visualizing Historical Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Stanislav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Stoev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Straßer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Annual IEEE Conference on Visualization (VIS-2001)</title>
		<meeting>the 12th Annual IEEE Conference on Visualization (VIS-2001)</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2001-10" />
			<biblScope unit="page" from="485" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Viewpoint selection using viewpoint entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pere-Pau</forename><surname>Vázquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miquel</forename><surname>Feixas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateu</forename><surname>Sbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Heidrich</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="1" to="17071" />
		</imprint>
		<respStmt>
			<orgName>Universitat de Girona, Campus Montilivi</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report, Institut d&apos;Informàtica i Aplicacions</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spain</forename><surname>Girona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On view likelihood and stability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphna</forename><surname>Weinshall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Werman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="97" to="108" />
			<date type="published" when="1997-02" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
