<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Comparative Evaluation of Visualization and Experimental Results Using Image Comparison Metrics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hualin</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Chen</surname></persName>
							<email>m.chen@swansea.ac.uk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><forename type="middle">F</forename><surname>Webster</surname></persName>
							<email>m.f.webster@swansea.ac.uk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Wales Swansea</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Wales Swansea</orgName>
								<address>
									<postCode>SA2 8PP</postCode>
									<settlement>Sin-gleton Park, Swansea</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Comparative Evaluation of Visualization and Experimental Results Using Image Comparison Metrics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.3.8 [Computer Graphics]: Applications</term>
					<term>I.4.9 [Image Processing and Computer Vision]: Applications</term>
					<term>J.2 [Computer Applications]: Physical Sciences and Engineering Scientific visualization, comparative visualization, image comparison, error metrics, human vision systems, rheology</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Figure 1: These five images were frames extracted from a video recorded during a rheological experiment. Based on the experiment, a computer simulation was conducted. Such images were then compared with visualization results, using image comparison metrics to provide a quantitative indicator that may be used to steer the computer simulation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In science and engineering, computer simulation and laboratory experimentation often work in parallel as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Most of such processes nowadays generate visual results in the forms of synthetic images obtained by means of scientific visualization and photographic images captured during experiments. These visual results, and their comparative evaluation, provide a critical feedback to both computer modeling and experiment design, aiding scientific understanding and design optimization. However, in many applications, comparison of visual results is largely conducted through human observations, which usually lack in objectiveness and consistency. Relying on human observations also places an obstacle on automated computational steering using visual results.  There is a wealth of literature on image comparison metrics in the fields of image processing and computer vision <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b17">18]</ref>. Some of these metrics have been deployed in computer graphics for measuring the quality of synthetic images against real scenes, mostly in the context of global illumination <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13]</ref>. It is thus highly desirable to utilize and adapt image comparison metrics for scientific visualization, which exhibits a setting similar to the study of global illumination but often has different application requirements.</p><p>In many applications of scientific studies, for instance in rheol-ogy ( <ref type="figure">Figure 1</ref>), there will normally be departure between computational and experimental results. Such departure can be caused by errors of all kinds, e.g., design, timing, rendering, and digitization. The measurement of variance is hence critical in determining the correctness, or in most cases quantifying the accuracy, of a simulation or an experiment. Our work focuses on image-level comparison, aiming at serving a class of scientific studies that has the following characteristics. (i) Data captured from experiments are in the form of 2D images from a certain view point. The imagery data may contain background noise and digitization errors. There may not be sufficient captured information, or adequate system resources, to reconstruct a 3D model from experimental data. (ii) It may not be feasible to eliminate departure between computation and experimentation. The departure may result in visual variance in many forms such as geometry, topology, luminance, color, opacity, surface details, timing and motion. (iii) Computation and experimentation may both generate time-varying results, though temporal synchronization is not necessarily guaranteed. These assumptions suggest that we must broaden the search space considered by existing work on image-level comparison in computer graphics and visualization, as the majority considered only images featuring the same geometry and focused on issues such as luminance, color and rendering resolution. It is apparent that we face a hugely complex space, from which we need to identify comparison metrics that can effectively distinguish, quantitatively, the results showing similar geometry from those showing substantial difference. We also wish to identify the most suitable preprocessing methods that provide the best support to comparison metrics. These therefore became the motivation of this work. We have examined eleven metrics including spatial domain metrics, spatial-frequency domain metrics and perceptually-based metrics. Ten of them were implemented based on previous work. The 2ndorder Fourier comparison metric was designed during this study.</p><p>It is also desirable for our conclusions not to depend on circumstantial conditions which may happen to be present in the particular testing datasets, but may not generally exist in other applications. We have thus conducted our work in two stages, namely base cases as a general study on the comparison space using relatively artificial data, and field trials where the comparison space is analyzed with actual rheological data. We have also introduced a methodological framework, supported by several statistical indicators, in order to assess different metrics objectively.</p><p>In Section 2, we will give a brief review of the development of image comparison metrics and their use in computer graphics. We will also briefly examine previous work on comparative visualization. In Section 3, we will outline a methodological framework for this study. This will be followed by a description of each of the eleven comparison metrics in Section 4. In Sections 5 and 6, we will present and discuss the results obtained in our base cases and field trials stages, respectively. Finally, we will give our concluding remarks and discuss future directions in Section 7. The imagery data used in this study, and most of the testing results can be found at http://www.swan.ac.uk/compsci/research/graphics/vg/compvis/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>There are many image comparison metrics for measuring the difference between a pair of images. Ahumada <ref type="bibr" target="#b7">[8]</ref>, Watson <ref type="bibr" target="#b23">[24]</ref> and Peli <ref type="bibr" target="#b17">[18]</ref> reviewed a substantial collection of metrics commonly used in image processing and computer vision. These include metrics for (i) half-toning optimization, (ii) measurement of compression artifacts, (iii) measurement of image quality and fidelity in the context of digitization, transmission and display, and (iv) object detection and image retrieval.</p><p>Recently, Chalmers et al. <ref type="bibr" target="#b1">[2]</ref> and McNamara et al. <ref type="bibr" target="#b12">[13]</ref> surveyed applications of some of these metrics in computer graphics, espe-cially for determining the visual quality of global illumination solutions. The work by Meyer et al. <ref type="bibr" target="#b13">[14]</ref> represents the early deployment of image comparison metrics in computer graphics. Lischinski et al. <ref type="bibr" target="#b10">[11]</ref> utilized energy-based metrics to determine the quality of synthesized images. Rushmeier et al. <ref type="bibr" target="#b18">[19]</ref> explored several perceptually-based metrics for comparing real and synthetic images. Ferwerda et al. <ref type="bibr" target="#b3">[4]</ref> presented a comprehensive study on visual masking for computer graphics, and developed a computational model for predicting how the masking effects are altered by the changes in surface details and tessellation. Gibson and Hubbold <ref type="bibr" target="#b6">[7]</ref> presented a perceptually-driven radiosity algorithm with an a-priori estimate of perceived color differences. Myszkowski <ref type="bibr" target="#b14">[15]</ref> employed Daly's VDP <ref type="bibr" target="#b2">[3]</ref> to monitor the perceived quality of the progressive radiosity and Monte Carlo solutions.</p><p>In scientific visualization, a number of researchers studied methods for comparative visualization in recent years. Shen and Pang <ref type="bibr" target="#b20">[21]</ref> introduced three category levels, namely image, data and feature levels, for classifying methods in comparative visualization. Pagendarm et al. <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16]</ref>, Trapp and Pagendarm <ref type="bibr" target="#b22">[23]</ref>, and Shen and Pang <ref type="bibr" target="#b20">[21]</ref> presented methods for data-level comparison of experimental and computational flow dynamics data. Gerstner et al. <ref type="bibr" target="#b5">[6]</ref> also conducted a data level comparison for analyzing multilevel grid structures, and Kim and Pang <ref type="bibr" target="#b9">[10]</ref> conducted a data level comparison for analyzing volume rendering algorithms. For image-level comparison, Williams and Uselton <ref type="bibr" target="#b25">[26]</ref> considered a collection of comparison metrics in the context of volume visualization. Gaddipatti et al. <ref type="bibr" target="#b4">[5]</ref> employed a wavelet-based metric to guide volume rendering. Sahasrabudhe et al. <ref type="bibr" target="#b19">[20]</ref> proposed a spatial domain metric, composed of four partial metrics, for comparing images and datasets. One example of feature-level comparison is the work by Silver and Zabusky <ref type="bibr" target="#b21">[22]</ref>, who utilized visionmetrics to compare features extracted from datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CONCEPTS</head><p>In this section, we outline a methodological framework for this study in the form of a comparison space. In particular, the rules governing the validity of image comparison metrics are specified. The definition of magnitude of difference (MOD) is introduced and its normalization is discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Comparison Space</head><p>Let Å Å½ Å ¾ Å ¿ be a collection of comparison metrics, each defined by an image comparison pipeline and a specific set of parameters. Given two images and , each Å ´ µ returns a value suggesting the magnitude of difference between the two images. Without loss of generality, we assume all images concerned are of the same resolution and depth, and the values returned by all metrics fall into the non-negative real domain </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Rules for Comparison Metrics</head><p>The judgment by human observers, as to the difference between two images, is a highly complex, and usually subjective, process. It is often conducted in a certain context, focusing on one or several image properties, such as, the geometry of objects featured, color and surface details. A comparison metric Å is a simulation of such a process, mostly in a highly simplified and abstract manner. In the context of our application, we shall consider mainly the geometry difference of the featured objects.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Magnitude of Difference</head><formula xml:id="formula_0">Å ¼ ´Ú µ Å ´Ú µ Å ´Ú µ ½ ¾</formula><p>The value returned by Å ¼ is called normalized magnitude of difference (NMOD). Image is called a normalizer, the selection of which is inclined to maximize the normalization factor. Normalization can also be conducted within a smaller or larger subspace, for instance Å ¢ Î ¢ , and can involve a normalizer in each dimension of the subspace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">IMAGE COMPARISON METRICS</head><p>In this section, we describe briefly the eleven image comparison metrics considered in this work. We classify them into three categories: spatial domain, spatial-frequency and perceptually-based approaches. In the following discussions, we consider only greyscale images, and most metrics can be extended to color images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Spatial Domain Metrics</head><p>This group of metrics operate in the spatial domain of images and derive an evaluation by examining some statistical properties of images. We considered three metrics, namely Mean Squared Error, Normalized Mean Squared Error, and Template Matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Å ½ : Mean Squared Error (MSE)</head><p>This is one of the simplest comparison metrics. It examines the MOD between two images pixel by pixel in the form of the squared error of a pair of pixel intensities, and derives its measurement as:</p><formula xml:id="formula_1">Å½´ µ AE Ü Ü ½ AEÝ Ý ½´ Ü Ý Ü Ýµ ¾</formula><p>where and are two images of resolution AEÜ ¢AEÝ; and Ü Ý and Ü Ý are the intensities at pixel´Ü Ýµ in and respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Å ¾ : Normalized MSE (NMSE)</head><p>The MSE metric is sensitive to the global-shift of image intensities. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Å ¿ : Template Matching (TM)</head><p>Template matching is a commonly used technique in pattern recognition <ref type="bibr" target="#b0">[1]</ref>. It examines the cross-correlation (or autocorrelation) sequences in order to determine if a testing image contains a template image. Consider two images without any object shift. The conventional cross-correlation function of two images and is:</p><formula xml:id="formula_2">½´ µ AEÜ Ü ½ AEÝ Ý ½ Ü Ý Ü Ý</formula><p>However ½ is sensitive to the change of luminance of and . A more sophisticated function is:</p><formula xml:id="formula_3">¾´ µ È Ü Ý´ Ü Ý µ´ Ü Ý µ Õ È Ü Ý´ Ü Ý µ ¾ È Ü Ý´ Ü Ý µ ¾</formula><p>where and are the mean intensities of and , respectively.</p><p>Our metric Å¿ is defined as:</p><p>Å¿´ µ ¾´ µ ¾´ µ ½ ¾´ µ</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Spatial-Frequency Domain Metrics</head><p>The Fourier transform is a powerful tool in image analysis, and it allows us to quantify various frequency features in an "imagery signal". Many traditional image comparison metrics operate entirely in the spatial-frequency domain. A general pipeline is commonly adopted, where images to be compared are first normalized (as in NMSE), and transformed to the Fourier domain using a FFT. A contrast sensitive function (CSF), ¢, which models the sensitivity to spatial frequencies, is then applied to the resultant magnitudes. The MOD between the two resultant images is then measured using MSE. In this section, we first examine three different CSF filters in the literature. We then describe a different approach that make use of a 2nd-order Fourier transform to measure the MOD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Å : Mannos-Sakrison's Filter (FFT-M)</head><p>Let be an image matrix obtained after normalization and FFT, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Å : Daly's Filter (FFT-D)</head><p>The visible differences predictor (VDP) proposed by Daly <ref type="bibr" target="#b2">[3]</ref> is one of the most well-established algorithms for evaluating image fidelity. Rushmeier et al. <ref type="bibr" target="#b18">[19]</ref> adapted the CSF of the VDP in a spatial-frequency domain pipeline for evaluating rendering quality against a captured image. The CSF is applied to Ù Ú in a way similar to Mannos-Sakrison's filter, and it has the following form:</p><formula xml:id="formula_4">¢ ´Öµ ¼ ¼¼ Ö ¿ • ½ ¼ ¾ ½ ¾Ö ´ ¼ ¿Öµ Ô ½•¼ ¼ ¼ ¿Ö</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Å : Ahumada's Filter (FFT-A)</head><p>Ahumada <ref type="bibr" target="#b8">[9]</ref> proposed a CSF that is a balanced difference of two </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Å : 2nd-Order Fourier Comparison (FFT-2)</head><p>Given two images and , we create two new images, ¼ and .  <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b2">3]</ref> adopted a pipeline which in principle can be viewed as an extension of the general pipeline described in 4.2. However they typically follow a more sophisticated vision model, and usually account for the threshold sensitivity, contrast sensitivity, color spatial acuity, spatial frequency sensitivity, and masking properties of HVS <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>In several HVS metrics, the final calculation of MOD using MSE takes place in the spatial domain, after an inverse FFT is applied to the magnitudes in the Fourier domain. To consider the difference between calculating MODs in spatialfrequency domain and that in spatial domain, we constructed three simple HVS metrics, Å , Å and Å½¼, with the CSFs by Mannos and Sakrison, Daly, and Ahumada, respectively. Their implementation is very similar to that of Å , Å and Å except for the use of inverse FFT.</p><p>To consider the effectiveness of a complex HVS-based approach, we also constructed Daly's visual differences predictor <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Å ½½ : Visual Differences Predictor (VDP)</head><p>Daly's VDP is a HVS-based image quality metric, which takes two images as input and produces a probability map for difference detection as output. It consists of three main functional components, namely amplitude non-linearity, contrast sensitivity function and detection mechanisms.</p><p>Amplitude non-linearity simulates the adaptation of HVS to local luminance. It applies a non-linear response function to each pixel in the input images, assuming that the adaptation results from an observer fixating a small image area.</p><p>A contrast sensitivity function simulates the variations in visual sensitivity of HVS, and models the variations as a function of spatial frequency. The process is similar to that described in 4.2, applying a FFT, followed by Daly's CSF, to each image.</p><p>Detection mechanisms simulate the spatial-frequency selectivity of HVS by decomposing each image into 31 independent streams. Multiple detection mechanisms are then applied to the corresponding streams of the two images. These mechanisms include computation of contrasts, application of a masking function to increase the threshold of detectability, and use of a psychometric function to predict the probability of detecting a difference at every location in each stream. Finally, the detection probabilities for all streams are combined into a single image that describes the overall probability for every location. The final measurement of the MOD is the sum of the probability values over all locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">BASE CASES</head><p>In the base cases stage, we aim to construct a relatively artificial comparison space as a window to the complex real-world. By specifying both the reference object and pseudo-experimental results in a controlled manner, we are able to observe individual aspects of different comparison metrics. A number of tests have been conducted, one of which is reported in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparison Space</head><p>A sphere is chosen as the reference object Ó, for which a mesh of 576 triangles is employed to define a corresponding geometrical model <ref type="figure" target="#fig_2">(Figure 3)</ref>. In this stage, we consider here only one model in , and assume that the computational process for generating such a model is correct. The model is visualized with a set of 20 different rendering conditions covering attributes such as illumination and shading methods. The set Î is thus defined by these rendering results. The set is constituted by eight carefully selected images representing some pseudo-experimental results. It is further divided into two subsets, × Ñ and . × Ñ consists of 4 photographic images, each of a different spherical object <ref type="figure" target="#fig_2">(Figure 3: ½, ¾, ¿ , )</ref>. In comparison with Ó, the objects in these images are of an almost identical shape, but different surface textures.</p><p>consists of images of four non-spherical objects which are chosen to reflect different image attributes, such as image source and object geometry <ref type="figure" target="#fig_2">(Figure 3: , , , )</ref>. For example, the cube, as a rendered object, gives an emphasis to its difference in geometry and similarity in rendering, the portrait contains some complex texture and background information, the teapot offers a round but irregular shape, and the noise image represents the "maximum" magnitude of difference in this comparison space from a conventional human perception. It is desirable for a metric to have the following attributes: (a) It is capable of separating × Ñ and , preferably under a variety of rendering conditions. (b) It is capable of differentiating between images in by offering a reasonably good standard deviation. It is expected that the standard deviation for × Ñ is relatively smaller than that for . (c) It is capable of ranking images in in an order of closeness that would be consistent with human assessment.</p><p>Here, we concentrate on the first two attributes. We will discuss the third attribute in Section 6, in conjunction with our field trials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Effectiveness of Comparison Metrics</head><p>In the context of the above comparison space, we consider the effectiveness of each comparison metric in its ability to measure geometry difference. We assume that image is the worst match for our reference object (i.e., a sphere), and as such it is selected as the normalizer. As we do not assume any metric or rendering condition to be a worst case scenario, normalizers are not specified in Å or Î. <ref type="table">Table 1</ref> lists the sample mean, , and standard deviation, × , for each subspace Å ¢ Î¢ . In addition, we compute the similar statistical indicators × Ñ and × × Ñ for × Ñ , and and × for . In particular, we calculate the difference between the mean values for × Ñ and , i.e., AE × Ñ, which gives us with the most useful indicator in this test. <ref type="table">Table 1</ref> provides a summary of these indicators for each subspace Å ¢ Î ¢ , and the columns × and AE are plotted in a histogram in <ref type="figure" target="#fig_4">Figure 4</ref>. From <ref type="figure" target="#fig_4">Figure 4</ref>, it is not difficult to observe that Å¿ template matching, Å 2nd-order Fourier comparison and Å½½ visual differences predictor are shown to be more capable of separating × Ñ from . These three metrics also offer slightly better standard deviations than the rest. Hence, they are the clear winners in this test. In addition, we can also observe that three different CSF filters, namely ¢Å, ¢ and ¢ , have not shown significant difference in their performance when they are associated with a spatial-frequency pipeline, or a simple HVS-based pipeline.  </p><formula xml:id="formula_5">Î ¢ Î ¢ × Ñ Î ¢ Metrics Å × × Ñ ××</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">FIELD TRIALS</head><p>In the field trial stage, we focus on the effectiveness of different metrics in handling real-world data. In particular, we study the capabilities of these metrics in measuring MODs resulting from basic geometrical transformations. In our field trials, altering some simulation parameters would lead to scaling changes, whilst temporal mismatching between simulation and experimentation would result in rotational variations. We also considered several ways for preprocessing experimental or visualization results in order to minimize the difference present in the background of images. We lay emphasis on general and fast methods that are suitable for automated computation steering in many circumstances.</p><p>In the following discussions, we first define the comparison space of our field trials, in conjunction with the discussion of different approaches to the preprocessing of image background. We present two tests in our field trials, one for measuring scaling differences of the geometrical models, and the other for rotational differences of the experimental images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Comparison Space</head><p>Our field trials involve an industrial fluid mixing process, studied by a group of computational and experimental rheologists <ref type="bibr" target="#b24">[25]</ref>. In the experiment, a translucent fluid was mixed in a cylindrical glass container, driven by a stirring rod. Experiments were recorded using a camcorder. The five frames in <ref type="figure">Figure 1</ref>, namely ½ ¾ ¿ , were extracted from a video sequence, showing the rod at the ¼ AE , AE , ¼ AE , ½¿ AE and ½ ¼ AE positions, respectively.</p><p>The mixing process was also simulated using a finite element method. As the fluid studied is a viseoplastic fluid of a highly elastic nature, we used surface rendering to visualize the simulation results. <ref type="figure">Figure 5</ref> shows three different geometrical models, simulating the fluid dynamics at one of the instances when the rod is at the ½ ¼ AE position. The models were obtained in an early modeling stage, conveying the variation of some simulation parameters. They were rendered using flat shading with the same illumination setting.</p><p>We denote this rendering condition as Ö.</p><p>½ large ¾ medium ¿ small <ref type="figure">Figure 5</ref>: Three geometrical models used in field trials.</p><p>As exhibited in <ref type="figure">Figure 1</ref>, images captured during an experiment often contain some background information, which is not considered in the computer simulation and thus will not be present in the resultant visualization. It is difficult for any image comparison metric not to take into account the background mismatching between visualization and experimental results. Hence, it is desirable to remove or reduce such inconsistency.</p><p>Given a template image Ø of our experiment environment without the experiment fluid, we can reduce the inconsistency by removing the background information in each image ¾ , replacing it with the same background color as in the visualization, typically either black or white, as shown in <ref type="figure" target="#fig_5">Figure 6</ref>(a) and (b). This can be achieved by comparing Ø and , and thresholding very similar pixels out as background information. This approach may introduce errors, usually as a result of minor movement of the camera or experimental device, and changes of the recording conditions during the experiment. The similarity in color between part of the background and the object concerned may also lead to errors.</p><p>Alternatively, we can integrate the background template Ø into the visualization generated for each ¾ . As shown in <ref type="figure">Figure</ref> 6(c), this can be achieved by simply placing each rendered object over the background template.</p><p>The introduction of preprocessing leads to the expansion of our comparison space. We replace the rendering condition set Ö with ÖÛ Ö Ö Ø , where ÖÛ for rendering with a white background, Ö with a black background, and ÖØ with the integrated background template. We also add two sets of preprocessed experiment images into . They are ½Û ¾Û ¿Û Û Û for extracted objects with white background, and ½ ¾ ¿ for those with black background. We thus define the comparison space for our field trials as:</p><formula xml:id="formula_6">Å ¢ ¢ Ê ¢ Å½ Å ½½ ¢ ½ ¾ ¿ ¢ ÖÛ Ö Ö Ø ¢ ½ ½Û Û ½</formula><p>In the two tests to be discussed below, we will consider a subspace of Å ¢ ¢ Ê ¢ for each test. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Measuring Scaling Differences</head><p>The objective of this test is to evaluate the effectiveness of each comparison metric for measuring scaling differences. We focus on the disparity between the experiment image and the three geometrical models in terms of their sizes. We thereby choose the following comparison subspace for this purpose:  For this test, we also conducted a small survey involving twelve researchers, who were asked to rank the closeness of (the rightmost frame in <ref type="figure">Figure 1</ref>) against ½, ¾ and ¿ ( <ref type="figure">Figure 5</ref>). The mean results of the survey, after appropriate normalization, are also included in <ref type="table" target="#tab_8">Table 2</ref>.</p><formula xml:id="formula_7">Å ¢ ¢ Ê ¢ Å½ Å ½½ ¢ ½ ¾ ¿ ¢ ÖÛ Ö Ö Ø ¢ Û</formula><p>This survey allows us to examine the correlation between human evaluation and the value returned by each image comparison metric. We use the following statistical function to calculate this indicator. Ò</p><formula xml:id="formula_8">È Ò ½ Ü È Ò ½ Ü ¡ È Ò ½ ¡ Ö Ò È Ò ½ Ü ¾ È Ò ½ Ü ¡ ¾ Ò È Ò ½ ¾ È Ò ½ ¡ ¾</formula><p>This function is commonly used to measure the relationship between two data sequences, and it returns values ranging between ½ and ½. The greater the is, the better correlation the two data sequences have. In our case, we consider the first three values in each row (Å ), against those in the last row (Human). The values obtained are given in the final column of <ref type="table" target="#tab_8">Table 2</ref>.</p><p>From <ref type="table" target="#tab_8">Table 2</ref>, we can observe that only Å has offered a close correlation and identified the best match correctly. Many metrics, such as Å½ and Å have considered ¿ as the best match. This did not surprise us as five out of twelve subjects in our human survey made the same observation. However, some metrics have failed to identify the worst match, which is unacceptable in this case. In general, however, the performance of each metric is not consistent in the comparison subspace concerned, and it seems to be affected by the choice of background colors. <ref type="table">Table 3</ref> provides all values obtained for each of the five comparison subspaces corresponding to´ÖÛ µ,´ÖÛ Ûµ,´Ö µ,´Ö µ, and´ÖØ µ, respectively. The sample mean, , and standard deviation, ×, for each row are also calculated. If we rank Å in a descending order of , Å½, Å and Å½¼ will be the top three performers in terms of good correlations. If we rank Å in an ascending order of × , Å , Å and Å½½ will be the top three performers in terms of consistency. However, this test generally did not produce a meaningful indicator, as the standard deviation is far too high for each metric.</p><p>Metrics Å </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Measuring Rotational Differences</head><p>One of the major issues in computer simulation is temporal synchronization. Our second test in the field trial stage is designed to examine the effectiveness of each comparison metric for measuring rotational differences resulting from temporally mismatching between simulation and experimentation. We focus on the variance between the geometry model ¾ and the experiment images ½ with the following comparison subspace:</p><formula xml:id="formula_9">Å ¢ ¢ Ê ¢ Å½ Å ½½ ¢ ¾ ¢ ÖÛ Ö Ö Ø ¢ ½ ½Û Û ½</formula><p>where Ê ¢ can be also down-sized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>For this subspace, we consider an ideal metric ¼ ´½ • Ó×´ µµ</head><p>where is the rotational angle associated to an experimental image. This would give the best match and ½ the worst. <ref type="table" target="#tab_10">Table 4</ref> shows the testing results for subspace Å ¢ ¾ ¢ ÖØ ¢ ½ .</p><p>From <ref type="table" target="#tab_10">Table 4</ref>, one can observe that four spatial-frequency methods have all performed reasonably well. Though they all seem to consider the frame of ½¿ AE is slightly closer than that of ½ ¼ AE , which is not an unreasonable conjecture.</p><p>The overall performance of each metric in this test is summarized in   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>In this paper, we presented a carefully-managed study on the application of image comparison metrics to the comparative evaluation of visualization and experimental results. This study has achieved the followings. (a) We have introduced a methodological framework for studying image-level methods for comparative visualization, enabling an objective and scientific assessment of various factors that influence the effectiveness, accuracy and consistency of comparative visualization. This framework can provide a degree of rigor in other similar studies. (b) We have implemented and assessed a total of eleven image comparison metrics. In particular, we have proposed the 2nd-order Fourier comparison metric, which has been compared favorably against other metrics considered. (c) Our base cases study has identified three metrics, template matching (Å¿), 2nd-order Fourier comparison (Å ) and visual differences predictor (Å½½), to be most effective in separating similar and different image groups. (d) Our field trials study has involved an in-dustrial mixing process, and has shown that the 2nd-order Fourier comparison metric (Å ) is the most effective and consistent in measuring rotational differences. However, our test has failed to identify any metric that is suitable for measuring scaling differences.</p><p>The strength of the 2nd-order Fourier comparison metric (Å ) is that its quantitative measurement remains to be reasonably reliable even when the images concerned exhibit varying degrees of geometrical closeness. This criterion is particularly important to computational steering using visual results, as an optimization process is likely to start with poorly-matched imagery results and progress towards better approximation. The template matching (Å¿) and the visual differences predictor (Å½½) have demonstrated their effectiveness in distinguishing fairly different image groups, but failed to produce a sensible ordering of closeness in our field trials. For the others (Å½, Å¾, Å , Å , Å , Å , Å and Å½¼), we have observed that none of the MSE pipeline, the general spatial-frequency pipeline, and the simple HVS-based pipeline has offered an adequate solution to our problem, nor the three CSFs have shown significant difference in their effects. We have also found that all metrics are affected by the selection of image background.</p><p>We recognize that our this study does not offer a conclusive judgment about image comparison metrics. Our future work will be to conduct a larger-scale study involving more input data, to improve the design of comparison metrics and methods for preprocessing, and to carry out experimental development for integrating imagelevel comparative visualization into an optimization process.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>A typical process of scientific studies featuring comparative evaluation of visualization and experimental results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>A visualization of the sphere model, and the set .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>0.34 0.20 0.09 0.80 0.18 0.60 Å : HVS-M 0.64 0.25 0.45 0.12 0.84 0.17 0.39 Å : HVS-D 0.78 0.28 0.57 0.16 0.99 0.18 0.43 Å½¼: HVS-A 0.80 0.28 0.58 0.16 1.01 0.19 0.43 Å½½: VDP 0.77 0.51 0.50 0.37 1.04 0.48 0.54Table 1: A summary of the testing results for assessing the effectiveness of different comparison metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>The × and AE columns ofTable 1are plotted in a histogram, where the AE values are shown in pink and the × values in blue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Three preprocessing methods: (a) extracting the object from the experimental results and placing it over a black background; (b) extracting the object and placing it over a white background; (c) placing the visualization over an imagery template.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>ÅTable 3 :</head><label>3</label><figDesc>0.92 -0.83 0.86 0.65 0.49 0.75 Å¾: NMSE 0.68 0.90 -0.82 0.24 0.61 0.32 0.68 Å¿: TM 0.69 0.90 -0.82 0.22 0.62 0.32 0.68 Å : FFT-M -0.74 -0.44 0.49 0.27 -0.08 -0.10 0.50 The values for each of the five comparison subspaces corresponding to (a):´ÖÛ µ, (b):´ÖÛ Ûµ, (c):´Ö µ, (d): Ö µ, and (e):´ÖØ µ, respectively. The sample mean, , and standard deviation, × , for each row are also shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>.</head><label></label><figDesc>HVS-A 0.45 0.90 -0.91 0.90 -0.10 0.25 0.77 Å½½: VDP 0.88 0.99 -0.69 0.98 -0.32 0.36 0.81 Table 5: The values for each of the five comparison subspaces corresponding to (a):´ÖÛ µ, (b):´ÖÛ Ûµ, (c):´Ö µ, (d): Ö µ, and (e):´ÖØ µ, respectively, where ½ , The sample mean and standard deviation × for each row are also shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The measurement of difference is conceptually similar to that of distance. Hence a comparison metric Å must satisfy the following rules, for any arbitrary images , and : 1. Dichotomy rule: Å´ µ ¼ or Å´ µ ¼ . 2. Identity rule: Å´ µ ¼ , if and only if .</figDesc><table /><note>3. Commutative rule: Å´ µ Å´ µ.4. Proximity rule: If Å´ µ Ǻ µ, is said to be closer to than under metric Å .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>To reduce this problem, images and are normalized prior to the error calculation. Let be the mean intensity of image . The mean of the image is first normalized to ¼ by scaling the intensity Let × ¼ be the standard deviation of the new image ¼ . The intensity of each pixel of ¼ is further scaled as ¼¼ Ü Ý ¼ Ü Ý × ¼ . The resultant image ¼¼ is thus of a standard deviation ½. the MSE metric is then applied to ¼¼ and ¼¼ , after image is normalized in the same manner.</figDesc><table><row><cell>of each pixel of as ¼ Ü Ý</cell><cell>Ü Ý</cell><cell>½.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Ù Úµ be a direction in the Fourier domain (in c/deg), and Ö Ô Ù ¾ • Ú ¾ . Mannos and Sakrison<ref type="bibr" target="#b11">[12]</ref> proposed to filter each Ù Ú using a contrast sensitive function ¢Å, i.e.,</figDesc><table><row><cell>Ù Ú</cell><cell>Ù Ú¢Å´Öµ</cell></row></table><note>¢Å´Öµ ¾ ´¼ ¼½ ¾ • ¼ ½ Öµ ´¼ ½ Öµ ½ ½</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>where and × are the center and surround lowpass cut-off spatialfrequency, respectively.and × are the center and surround amplitudes. In our tests, we set½ , × ¼ , ¿¾¾</figDesc><table><row><cell>Gaussians as:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>¢ ´Öµ</cell><cell>´Ö</cell><cell>µ ¾</cell><cell>×</cell><cell>´Ö ×µ ¾</cell></row><row><cell>and</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>× ½ ¾ ½ ¿. Like the Mannos-Sakrison's filter and Daly's filter, it is sensitive to the middle range of spatial-frequencies.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>¼ is obtained by translating horizontally for a predefined offset , such that and ¼ do not overlap with each other. is composed of and ¼ , representing a juxtaposition of both. Let , , ¼ and be the discrete Fourier transforms of , , ¼ and , respectively. Suppose ¡´Ù Úµ represents a difference function between and in the Fourier domain, i.e., ¡´Ù Úµ ´Ù Úµ ´Ù Úµ. According to the shift theorem and addition theorem [1], we have: ´Ù Úµ ´Ù Úµ • ¼´Ù Úµ ´Ù Úµ • ¾ Ù ´Ù Úµ 1 • ¾ Ù µ ´Ù Úµ • ¾ Ù ¡´Ù Úµ We measure the significance of ¡´Ù Úµ ´Ù Úµ, in terms its capacity to distort´½ • µ, by computing the Fourier transform of ´Ù Úµ and measuring the maximal amplitude, , within a window of the 2nd-order Fourier domain. The window corresponds mainly to´½ • µ, and its location is set according to . As values do not satisfies several rules in 3.2, e.g., and¼ , the metric is actually calculated as:</figDesc><table><row><cell>Let</cell><cell>¾</cell><cell cols="3">Ù . We define a transfer function</cell><cell>as:</cell></row><row><cell></cell><cell></cell><cell cols="3">´Ù Úµ ´Ù Úµ ´Ù Úµ 1 • µ • ¡´Ù Úµ ´Ù Úµ</cell></row><row><cell cols="3">which is equal to´½ • µ when</cell><cell>.</cell></row><row><cell></cell><cell cols="2">Å ´ µ</cell><cell>¼ ´</cell><cell>•</cell><cell>µ</cell></row><row><cell cols="5">4.3 Perceptually-based Metrics</cell></row></table><note>Our human vision system (HVS) is capable of performing invariant object recognition. Although such a process is yet to be under- stood fully, a collection of image comparison metrics, commonly called perceptually-based or HVS metrics, have been developed to simulate some features of the HVS. Many HVS metrics</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Table 2shows the testing results for subspace Å¢ ¢ ´ÖÛ µ .</figDesc><table><row><cell cols="5">There is little need to evaluate image pairs that have mis-</cell></row><row><cell cols="5">matching background introduced by preprocessing, such as</cell></row><row><cell cols="5">rendering condition ÖÛ and experiment image therefore further down-size ÖÛ Ö Ö Ø ¢</cell><cell>.</cell><cell>We</cell></row><row><cell>Metrics Å</cell><cell>½</cell><cell>¾</cell><cell>¿</cell></row><row><cell>Å½: MSE</cell><cell cols="3">0.826 0.822 0.821</cell><cell>0.85</cell></row><row><cell>Å¾: NMSE</cell><cell cols="3">1.047 0.995 0.938</cell><cell>0.68</cell></row><row><cell>Å¿: TM</cell><cell cols="3">1.097 0.991 0.879</cell><cell>0.69</cell></row><row><cell>Å : FFT-M</cell><cell cols="4">0.470 0.515 0.553 -0.74</cell></row><row><cell>Å : FFT-D</cell><cell cols="4">0.509 0.567 0.609 -0.76</cell></row><row><cell>Å : FFT-S</cell><cell cols="4">0.632 0.698 0.713 -0.90</cell></row><row><cell>Å : FFF-2</cell><cell cols="3">0.804 0.693 0.759</cell><cell>0.93</cell></row><row><cell cols="5">Å : HVS-M 0.986 0.950 0.931 0.82</cell></row><row><cell cols="5">Å : HVS-D 1.279 1.218 1.148 0.67</cell></row><row><cell cols="5">Å½¼: HVS-S 1.196 1.175 1.146 0.65</cell></row><row><cell>Å½½: VDP</cell><cell cols="4">1.260 1.335 1.674 -0.41</cell></row><row><cell>Human</cell><cell cols="3">0.909 0.199 0.394</cell></row></table><note>Û to ´ÖÛ µ ´Ö µ ´ÖØ µ ´ÖÛ Ûµ ´Ö µ . As an example,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 2 :</head><label>2</label><figDesc>Part of the testing results for measuring scaling differences, with the values returned by Å ´´ Ö Ûµ µ.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5</head><label>5</label><figDesc>which shows all values obtained for each of the five</figDesc><table><row><cell>Metrics Å Å½: MSE</cell><cell cols="2">¼ AE 0.507 0.499 0.475 0.493 0.532 -0.29 AE ¼ AE ½¿ AE ½ ¼ AE</cell></row><row><cell>Å¾: NMSE</cell><cell>0.735 0.719 0.672 0.684 0.740</cell><cell>0.19</cell></row><row><cell>Å¿: TM</cell><cell>0.540 0.517 0.451 0.468 0.547</cell><cell>0.18</cell></row><row><cell>Å : FFT-M</cell><cell>0.437 0.426 0.394 0.383 0.388</cell><cell>0.94</cell></row><row><cell>Å : FFT-D</cell><cell>0.547 0.534 0.474 0.449 0.467</cell><cell>0.93</cell></row><row><cell>Å : FFT-A</cell><cell>0.572 0.557 0.536 0.527 0.528</cell><cell>0.95</cell></row><row><cell>Å : FFT-2</cell><cell>0.365 0.351 0.357 0.320 0.323</cell><cell>0.90</cell></row><row><cell cols="3">Å : HVS-M 0.673 0.657 0.634 0.647 0.680 0.00</cell></row><row><cell>Å : HVS-D</cell><cell>0.884 0.865 0.815 0.833 0.897</cell><cell>0.07</cell></row><row><cell cols="3">Å½¼: HVS-A 0.827 0.801 0.796 0.804 0.830 -0.10</cell></row><row><cell>Å½½: VDP</cell><cell cols="2">0.868 0.844 0.751 0.828 0.966 -0.32</cell></row><row><cell>Ideal Metric</cell><cell>1.000 0.854 0.50 0.146 0.000</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4 :</head><label>4</label><figDesc>Part of the testing results for measuring rotational differences. The middle five columns show the values returned by Å ´´ ¾ Ö Øµ µ, and the last column shows the correlation between the five values of each row and these of the ideal metric.comparison subspaces, together with the corresponding and × values. We rank Å in a manner similar to the previous test, i.e., in a descending order of , and an ascending order of ×. We have found that Å is clearly the leader by a long way in terms of both good correlation and consistency. Due to the relatively strong contrast in background luminance, it was the comparison betweeń</figDesc><table /><note>¾ Ö µ and ½ that let most metrics down.Metrics Å</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Castleman</surname></persName>
		</author>
		<title level="m">Digital Image Processing</title>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image quality metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chalmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mcnamara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Myszkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Troscianko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Tutorials 2000</title>
		<meeting><address><addrLine>Now Orleans, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The visible difference predictor: an algorithm for the assessment of image fidelity. Digital Images and Human Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="179" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A model of visual masking for computer graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Ferwerda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Pattanaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shirley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Greenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH 1997</title>
		<meeting>SIGGRAPH 1997<address><addrLine>Los Angeles</addrLine></address></meeting>
		<imprint>
			<publisher>California</publisher>
			<date type="published" when="1997-08" />
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Steering image generation with wavelet based perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaddipatti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Machiraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yagel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="241" to="252" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A comparison of error indicators for multilevel visualization on nested grids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gerstner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rumpf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Weikard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EG/IEEE Data Visualization</title>
		<meeting>EG/IEEE Data Visualization</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="199" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Perceptually-driven radiosity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Hubbold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="141" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Computational image quality metrics: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ahumada</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SID International Symposium Digest of Technical Paper</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="305" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Simplified vision models for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ahumada</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SID International Symposium Digest of Technical Papers</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="397" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ray-based data level comparisons of direct volume rendering algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scientific Visualization: Overviews, Methodologies, Techniques</title>
		<editor>G. Nielson, H. Hagen, and H. Muller</editor>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bounds and error estimates for radiosity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Smits</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Greenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH 1994</title>
		<meeting>SIGGRAPH 1994</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="67" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The effects of a visual fidelity criterion on the encoding of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mannos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Sakrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="525" to="536" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visual perception in realistic image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mcnamara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chalmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trocianko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">State of the Art Reports</title>
		<meeting><address><addrLine>Interlaken, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An experimental environment of computer graphics imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Rushmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Torrance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="50" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The visible differences predictor: applications to global illumination problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Myszkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eurographics Rendering Workshop &apos;98</title>
		<meeting>Eurographics Rendering Workshop &apos;98</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="233" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Studies in comparative visualization of flow features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-G</forename><surname>Pagendarm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">H</forename><surname>Post</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scientific Visualization: Overviews, Methodologies, Techniques</title>
		<editor>G. Nielson, H. Hagen, and H. Muller</editor>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="211" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Competent, compact, comparative visualization of a vortical flow field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-G</forename><surname>Pagendarm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Walter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="142" to="150" />
			<date type="published" when="1995-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Vision Models for Target Detection and Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Peli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>World Scientific Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Comparing real and synthetic images: some ideas about metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rushmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Piatko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rust</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eurographics Rendering Workshop</title>
		<meeting>Eurographics Rendering Workshop<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-06" />
			<biblScope unit="page" from="82" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Structured spatial domain image and data comparison metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sahasrabudhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Machiraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Janus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Visualization</title>
		<meeting>IEEE Visualization<address><addrLine>San Francisco, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-10" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Data level comparison of wind tunnel and computational fluid data dynamics data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uselton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Visualization</title>
		<meeting>IEEE Visualization<address><addrLine>Research Triangle Park, NC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-10" />
			<biblScope unit="page" from="415" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Quantifying visualization for reduced modeling in nonlinear science: extracting structures from datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Zabusky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="61" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Data level comparative visualization in aircraft design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Trapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-G</forename><surname>Pagendarm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Visualization</title>
		<editor>R. Yagel and G. M. Nielson</editor>
		<meeting>IEEE Visualization</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1996-10" />
			<biblScope unit="page" from="393" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Digital Images and Human Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Waston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The modelling of dough mixing with free surfaces in two and three dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Webster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Sujatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Moving Boundaries VI: Computational Modelling of Free and Moving Boundary Problems</title>
		<editor>B. Sarler and C. A. Brebbia</editor>
		<imprint>
			<publisher>WIT Press</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="101" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Foundations for measuring volume rendering quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Uselton</surname></persName>
		</author>
		<idno>NAS-96-021</idno>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
