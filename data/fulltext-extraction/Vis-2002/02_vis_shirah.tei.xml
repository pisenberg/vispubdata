<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NASA&apos;s Great Zooms: A Case Study</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">W</forename><surname>Shirah</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Scientific Visualization Studio</orgName>
								<orgName type="institution">NASA Goddard Space Flight Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horace</forename><forename type="middle">G</forename><surname>Mitchell</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Scientific Visualization Studio</orgName>
								<orgName type="institution">NASA Goddard Space Flight Center</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">NASA&apos;s Great Zooms: A Case Study</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism -Color</term>
					<term>shading</term>
					<term>shadowing</term>
					<term>and texture; J.2 [Physical Sciences and Engineering]: Earth and atmospheric sciences visualization</term>
					<term>remote sensing</term>
					<term>renderman</term>
					<term>shader</term>
					<term>georegistration</term>
					<term>color matching</term>
				</keywords>
			</textClass>
			<abstract>
				<p>This paper examines a series of NASA outreach visualizations created using several layers of remote sensing satellite data ranging from 4-kilometers per pixel to 1-meter per pixel. The viewer is taken on a seamless, cloud free journey from a global view of the Earth down to ground level where buildings, streets, and cars are visible. The visualizations were produced using a procedural shader that takes advantage of accurate georegistration and color matching between images. The shader accurately and efficiently maps the data sets to geometry allowing for animations with few perceptual transitions among data sets. We developed a pipeline to facilitate the production of over twenty zoom visualizations. Millions of people have seen these visualizations through national and international media coverage.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The Scientific Visualization Studio (SVS) at the National Aeronautics and Space Administration's (NASA) Goddard Space Flight Center produces visualizations of Earth and space science data to explain NASA research and mission results. A recent series of visualizations consists of zooms from a global view of the Earth through finer and finer detail to a view of a particular landmark structure, such as the US Capitol Building.</p><p>What makes these visualizations unique is the use of actual cloudless satellite imagery in zooms so seamless that every frame is an accurate representation of the detail visible at a particular height. These animations illustrate the role resolution plays in remote sensing measurements by creating the illusion of a movie created from a single camera in space with a giant zoom lens (although the Earth is thankfully never free of clouds). The SVS has created these "great" zooms of about 26 different locations in the United States <ref type="bibr">[1]</ref>. This paper discusses the development of our initial zoom visualizations and the problems we encountered. Solutions are then identified, primarily in the form of a registration shader, followed by the production pipeline we created to produce the majority of the zoom visualizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">INITIAL EFFORTS</head><p>In the summer of 2000, Darrel Williams, the Landsat Project scientist, tasked the SVS to create a visualization of a dramatic zoom out from Washington DC to begin an Earth Science symposium [2] at the Smithsonian National Air and Space Museum. He provided four satellite images to illustrate some of the scales at which remote sensing data is acquired: The zoom would start with a tight view of the US Capitol using an NTSC-video resolution sub-image (720 by 486 pixels) from the IKONOS image. The point of view would move up, revealing more and more of the IKONOS image until the edges of the image were reached. The IKONOS image is about 10,000 pixels wide, so the effective resolution of the image on an NTSC screen would be 10,000 meters/720 pixels, or 14 meters per pixel. This is about the resolution of the Landsat image, so the transition to the Landsat image could be made at this point. Since each image had 15 to 17 times the resolution of the previous image in the sequence, appropriate data would be available for each transition all the way to a global view. The curvature of the Earth affects this calculation of resolution, but, since the Earth curves away from the viewer, less resolution is needed when the curvature becomes significant. As the animation scene was constructed, the following significant issues were encountered:</p><formula xml:id="formula_0">• A 4-</formula><p>• Registration -Each image was used as a texture map on a separate object, ranging from a sphere for the MODIS global composite to a rectangle for the IKONOS image. Despite reasonably accurate georegistration information, significant 'tweaking' was required to get the objects and the associated textures lined up using this multi-object approach. Accurate registration and texturing was critical. Texture opacity masks for the intermediate layers were carefully designed to force the transition from one image to the next to occur in regions where geophysical features were closely aligned.</p><p>• Numerical resolution -The ratio of numerical scales in this scene was about 4 Earth radii (20,000 kilometers) to 1 meter, which is roughly the precision limit of an IEEE 32-bit floating point number (2 24 ). Since all commercial rendering applications are currently single precision, numerical precision errors affected both accurate camera positioning and texture mapping at the closest zoom level. Our solution was to force a camera path to be parallel to an axis, so that only one position coordinate needed be computed.</p><p>• Color correction -Data from the satellite remote sensing instruments used here are collected as luminosity data in specified frequency bands, which are combined in an approximation of a photographic image. The lack of interinstrument calibration and/or simultaneous data acquisition forced us to perform all of the color matching among our images by hand, in Adobe Photoshop. The sensitivity of the human eye to variations in color made this step critical in our effort to create a perceptually seamless zoom.</p><p>Although the registration was crude and the color matching was marginal, the resulting animation [6] was very well received by the symposium audience. Its very success showed the high potential of a more precise and robustly designed sequence, one that could be replicated for various locations.</p><p>Goddard Space Flight Center coordinates a major release of information about Earth science research efforts on Earth Day every year. It was decided that zooms to major US cities would form the linchpin of the 2001 release [7], providing a local focus point for news organizations to discuss broader stories of Earth science research and remote sensing technology. This release prompted the development of a process for creating the zooms that led through the Earth Day release to several major event releases, including both the Super Bowl and the Olympics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE REGISTRATION SHADER</head><p>The critical element of our process that needed improvement was texture registration -the precise placement and layering of imagery onto a three-dimensional model of the globe. Without precise texture registration, the different satellite image layers could have visible seams and other artifacts. The software that we used for the initial zoom did not allow for accurate texture placement on arbitrary spherical segments.</p><p>Efficient access to textures at runtime was also necessary. The rendering software we used for the initial zoom loaded all textures into memory before rendering. Since we were dealing with very large textures, we needed software that loaded the texture pieces on-demand.</p><p>These requirements led us to look for rendering software that provides programmatic control over the renderer. Pixar's RenderMan software provides this control through Shading Language <ref type="bibr" target="#b0">[8]</ref>, allowing for the creation of procedural shaders. A procedural shader is a small program that is called every time a ray hits an object, and returns the color and opacity of that object. Shading Language also has a very efficient texture lookup function that reads MIP-mapped, tiled image files on-demand.</p><p>We developed a simple, elegant solution to our texture registration problem: a single registration shader that encapsulated the precise placement, layering, and blending of all four data sets using parameters that specified the longitude/latitude extents of each data set. The registration shader was applied to simple sphere geometry, then rendered.</p><p>The registration shader is passed four images with associated longitude/latitude extents. The images include embedded alpha channels used to smoothly blend between layers. When the shader is called at a particular point, the longitude and latitude are determined based on the (u,v) surface coordinates at that point. Texture calls are made for each layer which return the color and opacity for that layer. The colors are composited with the global image on the bottom and the 250-meter, 15-meter, and 1-meter images layered above in order.</p><p>The single registration shader, although elegant, had a serious problem. When the camera position was close enough to the sphere to be at the final zoomed in location (e.g., the US Capitol), there were numerous artifacts. We determined that we had run out of (u ,v) surface coordinate precision, and the coordinates being passed to the shader were not accurate enough for texture registration.</p><p>The surface coordinate precision problem was solved by placing a small patch in the exact position where the IKONOS data should be located. This gave the renderer a separate (u,v) coordinate space to correctly map the imagery. A simplified version of the registration shader was applied to this patch. This solution works because the curvature of the sphere in the very small region of the IKONOS patch has negligible curvature and the patch can be treated as a flat rectangle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THE PIPELINE</head><p>Once the registration shader had been developed, the process for creating the zooms became clear. A first render pass using the images with the registration shader would determine whether the registration was adequate for feature matching between images. Follow-up passes could then be repeatedly rendered using the registration shader as the images were color matched to each other until the color differences were imperceptible. Two primary steps precede rendering and color matching. First, data must be acquired for each level, and then the data must be transformed into color imagery that is registered to geographic coordinates. These four processes (acquisition, georegistration, color matching, and rendering) will now be described in further detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Acquisition</head><p>The first step in our pipeline is data acquisition. Having selected a zoom location, we needed to locate data at the various resolutions. There were several considerations when identifying which data sets to use. Our first priority was to obtain cloud-free imagery. This can be particularly challenging in certain regions (e.g., Seattle) and at sizes covering large areas (e.g., 250-meter data sets covering the entire East Coast). Our second priority was to obtain data sets that were acquired as close as possible to each other in time or at least relative to the season.</p><p>We reused the same 4-kilometer global data set from MODIS for most of the zooms. This presented some problems, primarily when creating zooms into northern cities where snow cover may have been different. The regional MODIS 250-meter data sets needed to be cloud free for about 1000 kilometers around the city. It proved to be challenging to find cloudless data sets covering this large an area for dates corresponding to the smaller scale data sets.</p><p>The local 15-meter data sets from Landsat 7 tended to be the easiest data sets to acquire. Landsat scenes cover small enough areas that locating cloud-free data was less difficult. In addition, Landsat has a long history of data sets to draw from with a robust and efficient ordering system. Space Imaging's IKONOS satellite provided the 1-meter data sets. These data sets tended to be more difficult to acquire. Since the area of IKONOS coverage is so small (roughly 10 km by 10 km), there are fewer occasions when data can be taken for a particular area. In addition, IKONOS was a relatively new satellite with fewer archival data sets to draw from.</p><p>A general problem that we had was finding data sets that were centered near our landmarks of interest. We needed to maintain the resolution in all directions to a certain point before transitioning to the next data set or blurring would result at the transition edges between data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Georegistration</head><p>Each data set consists of a series of 8-bit or 16-bit gray scale images in instrument specific frequency bands and a panchromatic image at a higher spatial resolution. The 16-bit images were rescaled to 8-bit since we did not need the higher color resolution, and 8-bit images can be manipulated with more image software. The panchromatic bands were used to sharpen the individual frequency channels to the higher resolution using Principal Component Substitution <ref type="bibr" target="#b1">[9]</ref>. In some cases, this method would alter the color balance of the resulting RGB images, but this was not a serious issue since significant color alteration would still be necessary to match the images to each other.</p><p>ERDAS Imagine was used to pan-sharpen IKONOS imagery to 1-meter resolution and Landsat imagery to 15-meter resolution. MODIS imagery was usually acquired at the pan-sharpened 250meter resolution. Imagine was also used to transform the imagery from the Universal Transverse Mercator projection to geographic coordinate-aligned rectangles and to perform the registration adjustments to achieve feature-level matching between pairs of images. The ability of Imagine to load multiple images at the maximum texture resolution of RenderMan (16,384 x 16,384 pixels) and display the images with correct geospatial registration was invaluable. The output from this phase was the four registered images and the associated longitude/latitude extents of the images for use in the rendering phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Color Matching</head><p>Color matching was the most time consuming and perhaps most important phase of our pipeline. Without extremely close color matching between data sets, our goal of a seamless zoom would be impossible due to the sensitivity of the human eye to variations in color. This phase was also unique in that it required artistic ability to complete the tasks.</p><p>We used Photoshop to perform color matching. Our general approach was to first make the 1-meter IKONOS image look as realistic as possible, particularly around the chosen zoom location. We also created an alpha mask for each image (except for the global image) so that the registration shader could blend each image into the next layer.</p><p>We next created a scaled-down version of the IKONOS image to overlay the Landsat 15-meter image in order to see how the Landsat color needed to be adjusted to match. Adjustment layers were our primary means of color matching <ref type="bibr" target="#b2">[10]</ref>. We most often used Color Balance, Hue/Saturation, Levels, Curves, and Brightness/Contrast. A common test we used when we thought we had a seamless transition between images was to ask people not involved in the project if they could detect any seams in the image.</p><p>We repeated this process with the MODIS 250-meter and MODIS 4-kilometer images. We often had problems once we reached the global 4-kilometer image in that colors were propagated up through the image layers causing regions or even the globe to appear unnatural in color. In these cases, we reworked the problem colors through all four layers. This process was slow, and it was exacerbated by the fact that image files were so large (about one gigabyte each). Simply saving and loading each file took about 30 minutes.</p><p>Once a good candidate set of images for each data set was color matched, those images were passed to the next phase of the pipeline: Rendering.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The beginning and end of the zoom sequence</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The Development Pipeline</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Many other people were involved in creating these visualizations. We would also like to thank Space Imaging Corporation, the USGS, and the Data Buy folks at NASA's Stennis Space Center.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Rendering</head><p>Alias|Wavefront's Maya three-dimensional animation software and Pixar's PhotoRealistic RenderMan renderer were used in the rendering phase. Maya was used for geometry and camera control, and RenderMan was used for rendering with the registration shader.</p><p>Controlling the motion of the camera (e.g., the current view) is very important. Initially, we used a key-framed motion spline created in Maya, but this motion had some visual inconsistencies. We discovered that for the velocity of the zoom out to appear constant and smooth over many orders of magnitude, the apparent angular velocity of the pixels at the edges of the frame must remain constant. This implies vertical motion that is exponential over time as in the trajectory function:</p><p>where H(t) is the height of the camera above the surface of the earth at time t, h 0 is the starting height of the camera at time t = 0, and h 1 is the ending height of the camera at time t = 1. For many of our animations h 0 is 0.00015 Earth radii and h 1 is 5.75 Earth radii. We also damped the beginning and end of the trajectory function for ease in/out effect Maya provides the tools to programmatically control the camera motion via expressions and the Maya Embedded Language (MEL) <ref type="bibr" target="#b3">[11]</ref>. Using MEL, we were able to program this exponential trajectory function for the camera directly to get very smooth camera motion.</p><p>The rendering phase often uncovered problems with color matching or georegistration that needed to be addressed. We returned to the appropriate phase in the pipeline to address these problems. Our iteration cycle was slow in part due to the need to preprocess the images using the txmake application, which generates optimized MIP-mapped versions of the textures for rapid access by the registration shader. This program could take over an hour to process an image file.</p><p>Linux-based 1.4 GHz processors were used to render the zooms. We only had three RenderMan licenses at the time, but the renderer is very efficient, and on three processors we were able to render one high quality NTSC-video resolution zoom in about 8 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We have presented the techniques used to create dramatic visualizations highlighting multiple resolutions of remote sensing data. Our initial efforts led to the development of a procedural registration shader. By employing this shader in a production pipeline, we have been able to create a series of highly successful visualizations in a reasonable time.</p><p>To date, we have produced 26 zoom visualizations into the following locations: All of these were rendered in NTSC-video resolution, and some were also rendered in HDTV resolution.</p><p>These visualizations received significant national and international television coverage during Earth Day 2001, Super Bowl XXXVI and the 2002 Winter Olympics. Millions of viewers have seen these visualizations and have, we hope, come to a better understanding of the role remote sensing imagery can play in their day-to-day lives.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Apodaca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gritz</surname></persName>
		</author>
		<title level="m">Advanced RenderMan, Part III</title>
		<meeting><address><addrLine>San Francisco, California</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Comparison of three different methods to merge multiresolution and multispectral data: Landsat TM and Spot panchromatic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Chavez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Sides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogrammetric Eng. Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="295" to="303" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Margulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Professional Photoshop</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2001" />
			<publisher>Wiley Computer Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Using Maya Expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Alias|Wavefront Inc</publisher>
			<pubPlace>Toronto, Canada</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
