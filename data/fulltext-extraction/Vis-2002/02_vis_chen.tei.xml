<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scalable Alignment of Large-Format Multi-Projector Displays Using Camera Homography Trees</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Chen</surname></persName>
							<email>chenhan@cs.princeton.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science HP Labs (CRL) and Computer Science Computer Science</orgName>
								<orgName type="institution" key="instit1">Princeton University The Robotics Institute</orgName>
								<orgName type="institution" key="instit2">CMU Princeton University Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
							<email>rahuls@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science HP Labs (CRL) and Computer Science Computer Science</orgName>
								<orgName type="institution" key="instit1">Princeton University The Robotics Institute</orgName>
								<orgName type="institution" key="instit2">CMU Princeton University Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><surname>Wallace</surname></persName>
							<email>gwallace@cs.princeton.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science HP Labs (CRL) and Computer Science Computer Science</orgName>
								<orgName type="institution" key="instit1">Princeton University The Robotics Institute</orgName>
								<orgName type="institution" key="instit2">CMU Princeton University Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
							<email>li@cs.princeton.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science HP Labs (CRL) and Computer Science Computer Science</orgName>
								<orgName type="institution" key="instit1">Princeton University The Robotics Institute</orgName>
								<orgName type="institution" key="instit2">CMU Princeton University Princeton University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Scalable Alignment of Large-Format Multi-Projector Displays Using Camera Homography Trees</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.3.3 [Computer Graphics]: Picture/Image Generation -Digitizing and scanning, Display algorithms, Viewing algorithms</term>
					<term>I.4.1 [Image Processing and Computer Vision]: Digitization and Image Capture -Imaging geometry, Camera calibration</term>
					<term>B.4.2 [Input/Output and Data Communications]: Input/Output Devices -Image display large-format tiled projection display, display wall, camera-projector systems, camera-based registration and calibration, automatic alignment, scalability, simulation, evaluation</term>
				</keywords>
			</textClass>
			<abstract>
				<p>This paper presents a vision-based geometric alignment system for aligning the projectors in an arbitrarily large display wall. Existing algorithms typically rely on a single camera view and degrade in accuracy 1 as the display resolution exceeds the camera resolution by several orders of magnitude. Naïve approaches to integrating multiple zoomed camera views fail since small errors in aligning adjacent views propagate quickly over the display surface to create glaring discontinuities. Our algorithm builds and refines a camera homography 2 tree to automatically register any number of uncalibrated camera images; the resulting system is both faster and significantly more accurate than competing approaches, reliably achieving alignment errors of 0.55 pixels on a 24-projector display in under 9 minutes. Detailed experiments compare our system to two recent display wall alignment algorithms, both on our 18 Megapixel display wall and in simulation. These results indicate that our approach achieves sub-pixel accuracy even on displays with hundreds of projectors.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Large format high-resolution display devices are becoming increasingly important for scientific visualization, industrial design and entertainment applications. A popular approach to building such displays is the projector array <ref type="bibr" target="#b6">[7]</ref> <ref type="bibr" target="#b3">[4]</ref>, where several commercially-available projectors are tiled to create a seamless, high-resolution display surface, (see <ref type="figure" target="#fig_0">Figure 1</ref>).</p><p>The projectors in the array require precise geometric alignment to prevent artifacts in the final display. This is typically done by manually aligning the projectors. Unfortunately, the process is laborintensive and time-consuming; furthermore, the display wall requires frequent re-alignment since the projectors shift slightly due to vibration and thermal flexing in the mounts. Given the increasing demand for large format display walls, alignment solutions must scale well to multi-projector arrays of arbitrary size. Recent advances in commodity graphics hardware have made it possible to pre-warp imagery in real time to correct misalignment in a tiled display wall. This enables software based solutions for multi-projector display alignment without compromising image quality. Several ideas for camera-based automation of projector array alignment have recently been proposed. Surati <ref type="bibr" target="#b10">[11]</ref> builds lookup tables that map pixels from each projector to points on the display surface; this is done by physically attaching a calibration grid (printed by a high-precision plotter) onto the surface. While this approach is adequate for a array of projectors, it scales poorly for larger displays since creating and accurately mounting an absolute measurement grid onto the display surface is infeasible. The PixelFlex system <ref type="bibr" target="#b12">[13]</ref> uses a single, wide field-of-view camera in conjunction with structured light patterns from each projector to determine camera-projector homographies, enabling automatic alignment of a reconfigurable display wall. These methods assume that the entire display surface is small enough to be completely visible in one camera's field of view. As display walls become larger, capturing a single camera image of the entire display surface becomes increasingly impractical: a single pixel in the camera maps to unacceptably large areas on the display surface once the display wall grows beyond a certain size.</p><p>This motivates approaches that can integrate information about the projector geometry from a set of camera images, each of which observe only a small portion of the display surface. Raskar et al. <ref type="bibr" target="#b7">[8]</ref> employ two calibrated cameras in conjunction with projected patterns to recover the 3-D model of a possibly non-planar projection surface. The need to calibrate camera views to the system makes this approach difficult to scale. Chen et al. <ref type="bibr" target="#b2">[3]</ref> use a pan-tilt-zoom camera to observe the individual overlap regions in the projector array. Information about local discontinuities, e.g. point-matches and line-matches across the seam, is acquired using an iterative process, and a large global optimization problem is constructed using this data. Simulated annealing is used to find the pre-warps that minimize discontinuity errors. The primary advan-1 Throughout the paper "accuracy" is intended to mean "local registration accuracy" unless otherwise stated. <ref type="bibr" target="#b1">2</ref> We use homography synonymously with collineation: a planar transformation which maintains collinear points. tage of their algorithm (referred to as SimAnneal in the remainder of this paper) is that, in principle, it scales well to large display walls since the uncalibrated camera can easily scan the overlap regions. However, simulated annealing converges slowly, particularly in high-dimensional parameter spaces. Furthermore, unless the initial manual alignment between projectors is good, the optimization algorithm can converge to an incorrect solution.</p><p>This paper presents a scalable approach to display wall alignment that is both more accurate and faster than existing approaches. It is motivated by the single-projector keystone correction system described in <ref type="bibr" target="#b9">[10]</ref>, adapted to employ images taken from multiple, uncalibrated camera views. Our approach efficiently scales to projector arrays of arbitrary size without sacrificing alignment accuracy. The experiments described in this paper were performed on an , 24-projector display with an effective resolution of pixels (see <ref type="figure" target="#fig_0">Figure 1</ref>).</p><p>The remainder of this paper is organized as follows. Section 2 details the camera homography tree algorithm for automatic display wall alignment. Section 3 describes our evaluation methodology, presents our approach for automatic measurement of alignment errors, and presents a simulator (validated on real data) that enables scalability experiments on very large format display walls. Section 4 presents experimental results investigating the accuracy, scalability and running time of our algorithm and comparing the approach to two recent systems, SimAnneal <ref type="bibr" target="#b2">[3]</ref> and PixelFlex <ref type="bibr" target="#b12">[13]</ref>. Section 5 summarizes the paper's contributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">DISPLAY WALL ALIGNMENT</head><p>Our display wall setup consists of 24 projectors, a cluster of workstations and a pan-tilt-zoom video camera. The Compaq MP-1800 microportable XGA-resolution DLP projectors are arranged in a 6 wide by 4 high array behind a rear-projection screen, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The projectors are mounted so that their display regions cover an region, generating an effective resolution of approximately 18 Megapixels. The projectors are controlled by a cluster of commodity PCs connected over a high-speed network. The pan-tilt-zoom NTSC-resolution camera is mounted on the ceiling in front of the projection screen.</p><p>To make our discussion general, we define the following notations. The term wall-(H,V) denotes a display wall array with projectors, arranged in a rectangular grid with projectors horizontally and projectors vertically. Each projector is assumed to have a resolution of ; thus a wall- <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b3">4)</ref> has an approximate resolution of . The term, cam-, denotes a set of camera poses and zoom lens setting, where an subset of the projector array is completely visible in each camera image. There are distinct camera views available as input. For instance, a cam-viewing a wall-(6,4) observes a set of projectors in each image; one could pan the camera to four horizontal and two vertical positions to obtain 8 different views of the display surface, each of which consists of a unique subset of projectors. Note that these views can be generated either from a single pan-tilt-zoom camera or from fixed cameras. Our algorithm works with either scenario. We further define the term cam-all to represent a single, wide-angle camera view that can see the entire display area at once, i.e. camall = cam-, where . For example, the cam-all for a wall- <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b3">4)</ref> is cam-.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Perspective Correction with 2-D Homographies</head><p>We assume that: the positions, orientations and optical parameters of the cameras and projectors are unknown; camera and projector optics can be modeled by perspective transforms; the projection surface is flat. Thus, the various transforms between screen, cameras, and projectors can all be modeled as 2-D planar homographies:</p><p>where and are corresponding points in two frames of reference, and (constrained by ) are the parameters specifying the homography. These parameters can be determined from as few as four point correspondences using standard methods. We employ the closed-form solution described in <ref type="bibr" target="#b9">[10]</ref>. It is summarized below.</p><p>Given feature point matches, , .</p><p>Let , and compute the eigenvalues of . is given by the eigenvector corresponding to the smallest eigenvalue.</p><p>Our system employs this technique to compute two types of homographies: camera-to-camera and projector-to-camera. Each is described in greater detail below, and illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. . This diagram shows the relationship between the various homographies described in the text. Our system's goal is to recover the homography ( ) mapping each projector , to the global reference frame. Although cannot directly be observed, it can be derived by composing , the homography between that projector and some camera view, and the chain of homographies connecting that view to the root of the homography tree. The geometric distortion of projected images is then corrected using a pre-warp of .</p><p>First, camera-to-camera homographies capture the relationship between different camera views of the display surface. Although each view typically observes only a few projectors, the system combines these views to generate a reference frame for the entire display surface (see Section 2.3). Conceptually, this is equivalent to automatically building a panoramic mosaic from a set of photographs. One cannot directly compute a homography between two camera views that do not overlap since they share no point corre-</p><formula xml:id="formula_0">18' 8' × 6000 3000 × 18' 8' × H V × H V 1024 768 × 6000 3000 × N N × N N × n v max H N -1 + 1 , ( ) max V N -1 1 , + ( ) × = 3 3 × 3 3 × n v M M × M max H V , ( ) = 6 6 × xw yw w         h 1 h 2 h 3 h 4 h 5 h 6 h 7 h 8 h 9         X Y 1         = x y , ( ) X Y , ( ) h h 1 … h , 9 , ( ) T = h 1 = n x i y i ( , ) X i Y i ( , ) { , } i 1 … n , , = A X 1 Y 1 1 0 0 0 X 1 x 1 - Y 1 x 1 - x 1 - 0 0 0 X 1 Y 1 1 X 1 y 1 - Y 1 y 1 - y 1 - X 2 Y 2 1 0 0 0 X 2 x 2 - Y 2 x 2 - x 2 - 0 0 0 X 2 Y 2 1 X 2 y 2 - Y 2 y 2 - y 2 - : • : • : • : • : • : • : • : • : • X n Y n 1 0 0 0 X n x n - Y n x n - x n - 0 0 0 X n Y n 1 X n y n - Y n y n - y n -                       = A T A h Projectors Display Surface Camera Views cami camj cam1 projk j P k iCj 1Ci R Pk RH1 P R k k P R k P j k P R k 1 -</formula><p>spondences. Therefore, our system builds a tree of homography relationships between adjacent views that spans the complete set of views; the mapping from any given view to the panoramic reference frame is determined by compounding the homographies along the path to the reference view at the root of the tree:</p><p>where is the homography mapping points from view to the global reference frame, are homographies connecting adjacent camera views and maps the root camera view to the global reference frame. <ref type="bibr" target="#b2">3</ref> Second, the projector-to-camera homographies transform each projector's area of projection into some camera's coordinate system. These homographies are determined as follows. Each projector k displays calibration slides with highly-visible features, whose locations are known in projector coordinates. By observing the locations of these features in the camera image j, we can determine the relevant projector-to-camera homography . Since we know the mapping between any camera j and the reference frame, this enables us to compute , the transform from projector k to the reference frame:</p><p>Note that expresses the geometric distortion induced by the projector's oblique placement. This distortion is removed by prewarping each projector k's output by .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sub-pixel Accuracy Feature Detection</head><p>Simple image processing techniques can typically locate features to the nearest pixel in an input image. However, since a single pixel in our camera images covers several projected pixels on the display surface, our application demands more sophisticated methods. Also, commodity video camera lenses usually exhibit noticeable distortions, making simple perspective camera models insufficient. We use the following five-parameter distortion model from <ref type="bibr" target="#b4">[5]</ref>:</p><p>where , and are the radial distortion coefficients, and the tangential distortion coefficients. These distortion parameters can be obtained via standard offline calibration procedures. We have also developed a method to automatically correct the distortion along with feature extraction.</p><p>The feature detection component of our system displays a sequence of calibration slides on the projectors that are visible in each camera view. The goal is to reliably identify point features in adjacent camera views that correspond to the same location on the physical screen. The standard approach would be to project a single known pattern, such as a checkerboard, from each projector and use the checkerboard's corners as features. We improve upon this by projecting pairs of patterns: a set of horizontal lines followed by a set of vertical lines. The intersections between these line sets can be determined with greater accuracy than standard corner detection. The details are described below.</p><p>For each camera view, horizontal lines are first displayed on a projector. <ref type="figure" target="#fig_2">Figure 3a</ref> shows an example of the captured image. Next, vertical lines are displayed on the same projector, as shown in <ref type="figure" target="#fig_2">Figure 3b</ref>. Note that these lines are "horizontal" and "vertical" only in the projector's own coordinate system. They may not be horizontal or vertical on the display surface nor in the camera image since the projector and camera orientations may be oblique. Each image is then processed as follows. We fit a quadratic function to the intensity values inside every and window in the image. A strong peak of the function under a window indicates that a line crosses through the window, and this provides a sub-pixel accuracy estimate of the line's local position, shown as blue dots in <ref type="figure" target="#fig_2">Figure 3c</ref> and 3d. The output of this procedure is a set of position estimates with floating-point precision along each visible line.</p><p>In the second step, the system determines the line equations that best fit the observed data in each camera image. Unfortunately, the observed lines are not precisely straight due to camera lens distortion. This is not easily corrected with offline calibration methods, because the camera distortion parameters change with zoom settings. This motivates the development of a novel online calibration method that combines distortion correction with line fitting. We use the sum of deviations of all points from the fitted line as the energy function. A non-linear optimizer is used to optimize the five-parameter distortion model to minimize the energy.</p><p>In the third step, the horizontal and vertical lines are intersected. This creates a set of accurate, stable point features for each camera view. A typical implementation employs calibration slides with five vertical and four horizontal lines, resulting in 20 point features per projector, as shown in <ref type="figure" target="#fig_2">Figure 3e</ref>. When the cam-configuration is used, features from four projectors are visible in each camera view, as shown in <ref type="figure" target="#fig_2">Figure 3f</ref>. These features are now used to compute the projector-to-camera and camera-to-camera homographies shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Computing projector-to-camera homographies is straightforward since the locations of the 20 features are known a priori in the projector's coordinate frame. The camera-to-camera homographies are determined using all of the features that are visible in overlapping camera views. For instance, when the camconfiguration is used, two common projectors are visible in adjacent camera views; this means that 40 feature points are available for the camera-to-camera homography calculation. When two camera views share no common projector, the homography relating them must be obtained indirectly, using a chain of known homographies, as described in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Camera Homography Trees</head><p>To accurately register camera views, we introduce the concept of a Camera Homography Tree, and an algorithm for optimizing it.</p><p>We call a Camera Homography Graph (CHG), where each vertex in V represents a camera view and an edge in E corresponds to a directly-computable homography between two views, i.e. an <ref type="bibr" target="#b2">3</ref> The transform ensures that the global frame axes are aligned with the display surface rather than the root camera view;</p><p>is computed by observing four known features on the display surface from any view.</p><formula xml:id="formula_1">H R j H R 1 C 1 i … C i j × × × = H R j j C s t H R 1 H R 1 H R 1 P j k P R k P R k H R 1 C 1 i … C i j P j k × × × × = P R k P R k 1 - x' x x k 1 r 2 k 2 r 4 k 3 r 6 + + [ ] 2p 1 xy p 2 r 2 2x 2 + ( ) + [ ] + + = y' y y k 1 r 2 k 2 r 4 k 3 r 6 + + [ ] 2p 2 xy p 1 r 2 2y 2 + ( ) + [ ] + + = r 2 x 2 y 2 + = k 1 k 2 k 3 , , ( ) p 1 p 2 , ( ) 9 1 × 1 9 × 2 2 × a d c b e f 2 2 × G V E , ( )</formula><p>edge connects two vertices only if they share at least one common projector. For a rectangular display wall, a CHG usually looks like a lattice. <ref type="figure" target="#fig_3">Figure 4</ref> shows a wall-(6,4) with cam-; 15 views are available, forming a lattice. Usually, a horizontal or vertical edge represents two strongly overlapping views, resulting in a better estimate of its homography; whereas a diagonal edge represents two weakly overlapping views, and thus a poorer estimate. As discussed in Section 2.1, as long as a CHG is connected, the homography between any two vertices can be computed by compounding a chain of homographies along a path connecting these two vertices. Ideally, the compound of homographies along any closed loop in an CHG should be identity; we call this graph a Consistent CHG. However, due to imperfection in the optics and limited resolution of the imaging device, this is usually not true; the result is an Inconsistent CHG. When multiple paths exist between two vertices, the calculated homography between these two vertices may depend on the choice of path. Clearly this needs to be remedied if we want to accurately register all camera views. Similar problems exist for 2-D image mosaicing. Shum et al. <ref type="bibr" target="#b8">[9]</ref> and Kang et al. <ref type="bibr" target="#b5">[6]</ref> have proposed methods for global registration. Their algorithms work on continuous-tone images, and have to extract features automatically. As mentioned before, we can detect features reliably with sub-pixel accuracy; this enables us to develop a novel algorithm that registers camera views precisely.</p><p>By definition, a tree is a connected graph without loops. Therefore, if a CHG is a tree, it is always consistent. Given a CHG , a Camera Homography Tree (CHT) is simply a spanning tree of G, where . In T, every pair of camera views is connected by a unique path. Although a CHT is consistent, it tends to be inaccurate when used directly -error in a single edge affects all homography paths containing that edge; also, being a subset of the original graph, only a portion of the feature correspondence information is utilized. We describe our method of constructing a initial CHT and optimizing the homographies along its edges to best represent the original CHG.</p><p>The goal of constructing the initial CHT is to minimize the path length from any vertex to the root, and also to minimize the path length between any adjacent camera views. To satisfy these crite-ria, we pick a vertex near the center of a CHG as the root, or reference view. A fishbone-shape tree is then constructed, with its "spine" aligned with the long side of the lattice, as shown in <ref type="figure" target="#fig_3">Figure 4</ref>. Each edge is initialized with the homography directly computed from common features visible in both camera views, as described in Section 2.1.</p><p>In the optimization stage, we iteratively refine the edges of a CHT to better represent the original CHG. In each iteration, the edges are updated in a bottom-up order. Each edge forms a cut set of T -when removed, T becomes a forest of two trees: and , where , , , , and</p><p>. An example of this is outlined in <ref type="figure" target="#fig_3">Figure 4</ref>. The initial homography along the edge is computed with features from the fourth projectors in the second and third rows, as shown in a darker shade. To refine this homography, we treat and as two CHT's and map features in each tree to the views of and . This gives us more common features between and , so we can compute a better homography for e. In this example, features from the entire fourth column of projectors, i.e. projectors with both dark and light shades in <ref type="figure" target="#fig_3">Figure 4</ref>, contribute to the refined homography. This process is continued until the variance of multiple samples of each point feature in the root view is below a threshold. We found that stable homography estimates are obtained after a small number of iterations. Details are available in <ref type="bibr" target="#b1">[2]</ref>.</p><p>This algorithm enables us to create an effective virtual camera with very high resolution from multiple uncalibrated low resolution views. With these techniques, our system achieves scalable subpixel alignment on very large display walls, as described in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EVALUATION METHODOLOGY</head><p>This section first proposes metrics for evaluating display wall alignment systems. It then introduces an automated vision-based system for measuring them. Finally, it details a simulator for evaluating the scalability of our algorithm on arbitrarily large display walls.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Metrics for Display Wall Alignment Systems</head><p>We use three metrics for evaluating the performance of a display wall alignment system: local alignment error, global alignment error, and running time.</p><p>Local alignment error quantifies the registration between adjacent projectors. Qualitatively, misalignment creates artifacts such as discontinuities and double-images in the overlap region (see <ref type="figure" target="#fig_4">Figure 5a</ref>). Quantitatively, the error can be characterized by the displacement between a point shown on one projector and the same point displayed by an adjacent projector. An appropriate measurement unit for this error is the average size of a pixel projected on the display wall. This unit is invariant to the physical dimensions of the display wall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Let</head><p>be the homography that maps point from the display surface into projector 's reference frame. Let be the alignment system's estimate for the inverse mapping. Due to alignment errors, . In other words, when projector attempts to illuminate point , it actually illuminates the point . Let be the set of all features, and be the set of all projectors. We define local error to be:</p><p>where is an indicator variable. I(.) = 1 if falls within the display frustum of projector and I(.) = 0 otherwise. This formula- </p><formula xml:id="formula_2">( ) v a v b T a T b G V E , ( ) T G E T , ( ) E T E ⊆ e v a v b , ( ) E T ∈ = T a G a E a , ( ) T b G b E b , ( ) v a G a ∈ v b G b ∈ G a G G b - = E a E T G a G a × ( ) ∩ = E b E T G b G b × ( ) ∩ = v a v b , ( ) T a T b v a v b v a v b H k P R k 1 - = p x y 1 , , ( ) T = k Ĥ k 1 - Ĥ k 1 -H k I ≠ k p p k Ĥ k 1 -H k p = Ω Φ E l I i p , ( ) I j p , ( ) p i p j - 2 ⋅ ⋅ i j ( , ) Φ Φ × ∈ ∀ ∑ p Ω ∈ ∀ ∑ = I i p , ( ) p i</formula><p>tion of the local error does not require knowledge of absolute points on the display wall, . It is sufficient to examine pairs of and and measure the relative distance between them. In the experiments described below, we obtain local error by displaying a grid pattern and measuring the projected discrepancy between grid points which are displayed by projectors in overlap regions.</p><p>Some alignment algorithms, such as SimAnneal explicitly observe point-and line-mismatches in the overlap regions and attempt to optimize pre-warp parameters to minimize this error. However, most algorithms, including ours, simply aim to register each projector to the global reference frame as accurately as possible, trusting that an accurate global registration will lead to small local errors. Global alignment error is a metric for measuring the overall registration of a display wall. A projector array with excellent local alignment may still exhibit alignment errors for two reasons: (1) the projected image may be globally warped so that its edges are not parallel to the sides of the display surface; (2) small errors in local alignment can accumulate as homographies are chained, resulting in non-linear distortions in the projected image. We define global alignment error to be the displacement between pixels in the projected image and their desired locations, as measured in the reference frame:</p><p>This global error metric requires knowledge of the absolute locations of points on the display surface, thus making accurate measurements of global alignment quite difficult. We approximate the global error by measuring the nonlinearity of a regularly spaced grid patter. Fortunately, the human visual system is tolerant of slight global misalignments while being very sensitive to local discontinuities. In practice, once the projected display is roughly aligned to the global coordinate frame, local errors dominate the user's perception of the display wall.</p><p>To be of practical value, a alignment algorithm must be fast as well as accurate. There are two components of running time: the time taken to acquire images, and the time needed for computation, including image processing and calculating homographies. We present timing results comparing our system to existing approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Automatic Measurement of Alignment Errors</head><p>Manual measurement of local and global alignment errors is a tedious, time-consuming process. Fortunately, we can employ the same camera hardware used for calibrating the display wall in evaluating its local alignment accuracy.</p><p>To measure local alignment error, each projector displays a set of calibration patterns. Unlike the patterns used during the calibration phase, these patterns are aligned (to the best of the system's ability) to the global reference frame. The camera captures detailed images of the seam regions where projection areas overlap. It records the displacement between a point displayed by one projector and the same global point displayed by other projectors at the seam. The average displacement over all seam regions on the display surface gives an estimate of local alignment error (in pixels). <ref type="figure">Figure 6</ref>. This graph plots the ground truth error (obtained from the simulator) against the observed error, as determined by our automated measurement system.</p><p>is the standard deviation of the noise model for feature detection. From this graph, we see that our automated measurement system slightly (but consistently) overestimates the error of the display wall alignment systems.</p><p>In principle, one must be cautious about using the same camera hardware and similar image processing algorithms, both for aligning the display wall and measuring its alignment accuracy. For this reason, we simulated the automatic measurement system in the display wall simulator (see Section 3.3). In this series of tests, we assumed that the noise in feature detection could be modeled using a zero-mean Gaussian distribution, . <ref type="figure">Figure 6</ref> plots the actual local error (ground truth available to the simulator) against the measured local error, for a range of noise models. A noise-free measurement process would obviously generate a straight line . We note that all of the curves lie above the line; this means that our automated measurement system is biased, and the bias is a consistent overestimate. Our experimental data indicates that our automated measurement system has pixels in both x-and y-direction for each feature detected. Since we use four points for each of the 38 seams on wall- <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b3">4)</ref>, the standard deviation on average local error estimates is and the 97% confidence interval is pixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Display Wall Simulator</head><p>To supplement our experiments on the 24-projector display wall we implemented a display wall simulator in Matlab. The primary function of the simulator is to investigate whether our camera homography tree algorithm scales well to very large format display walls, both in terms of alignment accuracy and running time. Additionally, the simulator may enable us to determine the components of a display wall system that are most likely to impact alignment accuracy. We broadly classify sources of alignment errors into four categories, each parametrized with one coefficient.</p><formula xml:id="formula_3">p p i p j B C A</formula><p>a) The uncalibrated setup; b) The result after single-view calibration: the average local error is 1.95 pixels; c) Sub-pixel accuracy achieved using homography tree algorithm cam-2×2: the average local error is 0.55 pixel. • Projector optics: We can expect errors to increase when the projectors' optics cannot be accurately modeled using a perspective projection model. We use the same five-parameter distortion model described in section 2.2 to simulate the projector optics. The Projector Distortion Factor, p, is coupled to both radial and tangential distortions, i.e. we define:</p><formula xml:id="formula_4">E g I k p , ( ) p p k - 2 ⋅ k Φ ∈ ∀ ∑ p Ω ∈ ∀ ∑ =</formula><p>The projectors in our system exhibit little distortion; we estimate , which corresponds to an average warp of 0.32 pixels at the projected image's edge. In simulations we use p from 0.01 to 0.04.</p><p>• Camera optics: Although our cameras exhibit significant distortion, especially in the widest-zoom setting, the effective distortion is greatly reduced after either offline camera calibration or our automatic distortion correction technique. However, these methods can not fully rectify the images, we model the residual distortion with a Camera Distortion Factor, c:</p><p>We estimate for our camera, which corresponds to an average warp of 0.23 pixels along the edge of the camera image. In the simulations, we use c from 0.00 to 0.05.</p><p>• Image processing: The simulator uses an abstract model for image processing. We assume that the system locates line features in the camera image, and that the position estimate for these features is corrupted with a zero-mean Gaussian noise. We define Image Noise Factor, n to be from 0.0 to 1.5, where corresponds to pixel error, or .</p><p>• Non-planar display surface: Our alignment system assumes that all transforms can be modeled using 2-D planar homographies -an assumption that relies on a planar display surface. The simulator models the shape of the display screen as a Gaussian surface parametrized on a single variable, the Screen Curvature Factor, , i.e. we define the screen as a surface:</p><p>where and are the width and height of the screen, and all units are in mm. In the experiments, we use s from 0.0 to 0.2, where corresponds to a 20 mm central bulge, which equals to the real measurement in our rear-projection screen.</p><p>We present some of the simulation results in Section 4.3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS</head><p>This section presents results from several series of experiments. Section 4.1 compares our approach to two existing display wall alignment algorithms. Section 4.2 examines how local error improves as the number of camera views is increased. Section 4.3 confirms that the camera homography tree algorithm remains accurate as the number of projectors in a display wall increases. These experiments on the 24-projector wall are further supported by simulation runs on very large format displays. Section 4.4 compares our algorithm's running time with existing approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparisons with Existing Techniques</head><p>Two recent systems, Chen et al.'s SimAnneal <ref type="bibr" target="#b2">[3]</ref> and Yang et al.'s PixelFlex <ref type="bibr" target="#b12">[13]</ref> were selected as suitable benchmarks for display wall alignment. We were able to obtain an implementation of the former for evaluation purposes, and were able to re-implement relevant portions of the latter algorithm from published details. The PixelFlex algorithm utilizes only a single camera view covering the entire rear-projection screen in our setup. <ref type="bibr" target="#b3">4</ref> The SimAnneal algorithm requires detailed views of inter-projector seams, and these images were collected automatically using a pan-tiltzoom camera. The same camera control software (with different parameters) was used in our system to collect the cam-views as input to the camera homography tree algorithm. Before presenting results, we briefly describe the image processing aspects for each algorithm.</p><p>SimAnneal uses a sequence of point and line feature correspondences wherever two or more projectors share a seam. The displacement between corresponding features displayed by different projectors provides an error metric that is minimized using simulated annealing. The implementation of SimAnneal we obtained assumes for its initial conditions that the homography between adjacent projectors can be adequately approximated by a simple translation and scale. This assumption is valid only when projectors are initially well-aligned; our uncalibrated setup, as seen in <ref type="figure" target="#fig_4">Figure 5a</ref>, exhibits significant error from rotation and perspective effects. As a result, SimAnneal performs poorly in our experiments, rarely achieving less than 12 pixels of local error after 500K iterations. For this reason, we report results from <ref type="bibr" target="#b2">[3]</ref>, where SimAnneal was evaluated on a much smaller wall- <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b1">2)</ref> display. We expect that with proper initialization, this value would represent a closer, but still optimistic estimate of SimAnneal's potential accuracy on a wall- <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b3">4)</ref>.</p><p>In the PixelFlex system, each projector displays an array of circles with a Gaussian intensity distribution, and the centroids of the observed Gaussians in the camera image provide a sub-pixel-accurate estimate of the feature locations <ref type="bibr" target="#b12">[13]</ref>. Our implementation of their algorithm required straightforward modifications to the camall version of our system: the main difference is the use of Gaussian rather than line features. <ref type="table">Table 1</ref> summarizes the results of our experiments. In the singleview (cam-all) configuration with no homography trees, our system's accuracy is comparable to the existing systems. We notice that cam-all performs slightly better than PixelFlex. The reason is that, under perspective projection, lines are invariant but Gaussians undergo asymmetric distortion -possibly inducing a bias in Pix-elFlex's estimate of feature location. When our system is able to take advantage of multiple camera views, e.g. in the camconfiguration, the result improves dramatically -the average local error is reduced by half, as shown in <ref type="figure" target="#fig_4">Figure 5c</ref>. <ref type="table">Table 1</ref>. Alignment results of various algorithms on wall- <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b3">4)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Improving Accuracy with More Camera Views</head><p>This experiment investigates the trade-offs between effective camera resolution and homography chain length. Higher effective camera resolution, achieved using a tighter zoom, enables the system to locate features in the calibration slides with greater accuracy. On the other hand, the smaller field-of-view implies that more camera shots are needed to span the complete display wall. <ref type="figure">Figure 7</ref> demonstrates that the camera homography tree algorithm does improve local accuracy. The single view approach, cam-all, exhibits slightly more than 1 pixel error on wall- <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b3">4)</ref>. Additional camera views improve accuracy, enabling us to achieve sub-pixel error  , which exhibits less than half the error of the best single camera view algorithm. <ref type="figure">Figure 7</ref>. This graph shows how the camera homography tree algorithm improves average local errors. A single camera view (cam-all), even with super-resolved feature detection, is unable to achieve sub-pixel alignment accuracy on a wall- <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b3">4)</ref>. These experiments were performed on our 24-projector display wall.</p><formula xml:id="formula_5">k 1 k 2 k 3 p 1 p 2 , ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Scalability</head><p>The previous experiment showed that the camera homography tree algorithm significantly improves accuracy on our current display wall hardware. The following two experiments investigate whether the multi-view algorithm continues to outperform single-view alignment techniques across different scales of displays. The first shows that the observed behavior is also true for smaller displays and the second indicates that our algorithm scales to very large format displays consisting of hundreds of projectors. <ref type="figure">Figure 8</ref> investigates the camera homography tree algorithm's behavior on display walls of different sizes. These experiments were performed on the 24-projector wall, using several rectangular sub-arrays of projectors ranging from wall- <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b1">2)</ref> to the complete display. The results confirm that the multiple view approach to display wall alignment scales well with projector wall size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Scalability Results on 24-projector Display Wall</head><p>We note that local error for cam-is higher than expected in the wall- <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b2">3)</ref> scenario. We believe that this may be due to screen curvature; initial simulation results support our hypothesis. We are conducting additional experiments to examine the issue. <ref type="figure">Figure 8</ref>. This graph shows how the average local error for different configurations of the camera homography tree algorithm scale with display wall size. Note that we achieve sub-pixel error rates even on the largest display wall. These experiments were performed on our 24-projector display wall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Scalability Results on Display Wall Simulator</head><p>To evaluate our algorithm's performance on very large displays, we run the simulator on the following display walls: wall-(H,V), where (H,V) ∈ {(2,2), <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b1">2)</ref>, <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b2">3)</ref>, <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b3">4)</ref>, <ref type="bibr" target="#b8">(9,</ref><ref type="bibr" target="#b5">6)</ref>, <ref type="bibr" target="#b11">(12,</ref><ref type="bibr" target="#b7">8)</ref>, (18,12), (24,16)}. <ref type="figure">Figure 9</ref> shows the expected local error versus the total number of projectors in a display. Each curve in the graph corresponds to a choice of cam-, where N∈{2, <ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr">18</ref>,24}. There is no benefit to using a wider field-of-view camera than necessary on a given display wall. Therefore, each cam-curve starts at the largest display wall that can be completely seen within a single view. By definition, the curve cam-all simply connects the starting data point of each cam-curve. Simulation parameters were selected to be similar to the current physical setup: , , , and each data point was generated by averaging the results of five runs. <ref type="figure">Figure 9</ref>. Semi-log plot of local alignment error for simulated display walls of various sizes. The horizontal axis gives the number of projectors comprising the display, i.e. h×v for Wall- <ref type="bibr">(h,v)</ref>. It shows that single-view alignment algorithms fail to achieve satisfactory accuracy, whereas the camera homography tree algorithm scales well.</p><p>As <ref type="figure">Figure 9</ref> shows, the simulation data on smaller display walls is consistent the experimental evidence presented before. This graph also shows simulations extended to very large scale displays. We make the following observations about <ref type="figure">Figure 9</ref>. First, the alignment error for a single-view algorithm (cam-all) grows almost linearly as projectors are added. Second, the curves for the tightlyzoomed camera configurations (e.g. cam-) are almost flat. This validates our earlier claim that the camera homography tree algorithm scales well with display wall size. Third, note that for a particular camcurve, the local error decreases as the number of projectors increases. This is because, on a larger display, each projector appears in more camera views. Our homography tree algorithm is able to utilize multiple views of point features to better refine the homographies. This results in improved accuracy. Finally, one can derive significant benefits from the camera homography tree algorithm even by chaining together a small number of views. For instance, on wall-(24,16), going from cam-(i.e. cam-all) to cam-cuts the local alignment error from almost 8 pixels to about 2.5 pixels.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Running Time</head><p>As mentioned earlier, there are two major components of the running time: the time required to collect the necessary images; and the time needed to process these images and calculate the homographies used to align the projectors. <ref type="table">Table 2</ref> presents the timing results. Our system is implemented in Matlab 6.0 with a moderate amount of optimization. Since our implementation of the PixelFlex algorithm uses the same code base, its running time is almost identical to cam-all. Therefore, it is not listed. It is clear that our system is fast: we can align wall- <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b3">4)</ref> in under 9 minutes. <ref type="table">Table 2</ref>. Comparison of running time (minutes) between SimAnneal and our system. Data column lists the time taken to collect images; Comp column is the computation time needed to calculate homographies from the data. SimAnneal was not evaluated on wall- <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b2">3)</ref>; timing information reported for wall- <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b1">2)</ref> in <ref type="bibr" target="#b2">[3]</ref> was used. Our system is faster by more than an order of magnitude on every display walls.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>Multi-projector display systems have become increasingly popular for a wide variety of applications. As display walls incorporate more projectors, it becomes necessary to develop automated approaches to help with designing, building and maintaining these systems. One key aspect to scaling tiled displays in both size and number is having a good automated alignment system. This paper describes a practical vision-based system for automatically calibrating such large format multi-projector displays. It incorporates an automated sub-pixel accuracy feature detection algorithm for tiled displays that simultaneously calibrates intrinsic camera parameters. It also includes an automatic vision-based system for measuring display wall alignment accuracy. A comprehensive series of experimental tests on a 24-projector wall demonstrate that our camera homography tree algorithm significantly improves local alignment accuracy by incorporating information from multiple, uncalibrated camera views. Our algorithm's accuracy exceeds that of existing solutions, and unlike those approaches, scales better (in both accuracy and speed) as projectors are added to the display system.</p><p>The system also includes a simulator tool. It helps system designers examine the impact of design decisions on the expected accuracy of a display wall. We have run many simulation tests, and the results (validated on real data) indicate that our approach is practical even for very large format displays. These simulation results would help a designer determine the quality and number of cameras needed, and the time necessary for aligning a display wall.</p><p>Our system is now in regular use at the Princeton Scalable Display Wall. The size of this display has warranted a new class of scalable alignment solutions, such as the one described here. We anticipate such solutions will increasingly be required by future displays.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The Princeton Scalable Display Wall is an display with an effective resolution of pixels. This behind-thescene photograph shows the array of 24 microportable projectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>Figure 2. This diagram shows the relationship between the various homographies described in the text. Our system's goal is to recover the homography ( ) mapping each projector , to the global reference frame. Although cannot directly be observed, it can be derived by composing , the homography between that projector and some camera view, and the chain of homographies connecting that view to the root of the homography tree. The geometric distortion of projected images is then corrected using a pre-warp of .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>An example of the image processing and feature extraction procedure of our system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>The Construction and Optimization of a Camera Homography Tree for Wall-<ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b3">4)</ref> with Cam-2×2 Configuration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Zoomed views of alignment errors on a wall-<ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b3">4)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>k 1 k 2 k 3 p 1 p 2 ,</head><label>2</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>System Number of Camera Views Local Error Avg (Max) Global Error Avg</head><label></label><figDesc>The best result is achieved by cam-</figDesc><table><row><cell>rates on a</cell><cell></cell><cell cols="4">display using only a</cell><cell>resolution</cell></row><row><cell>camera.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>N N ×</cell></row><row><cell></cell><cell>=</cell><cell>⁄</cell><cell>(</cell><cell>-</cell><cell>)</cell></row><row><cell>W s</cell><cell>H s</cell><cell></cell><cell></cell><cell></cell></row><row><cell>s 0.1 =</cell><cell></cell><cell>18' 8' ×</cell><cell></cell><cell></cell><cell>2 2 ×</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SimAnneal</cell><cell>152</cell><cell>1.35 (N/A)</cell><cell>N/A</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PixelFlex</cell><cell>1</cell><cell>1.73 (3.9)</cell><cell>1.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cam-all</cell><cell>1</cell><cell>1.19 (4.1)</cell><cell>1.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cam-2×2</cell><cell>15</cell><cell>0.55 (2.3)</cell><cell>1.8</cell></row><row><cell cols="2">18' 8' ×</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">This camera configuration was identical to the cam-all configuration in our algorithm.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ACKNOWLEDGEMENTS</head><p>The Princeton Scalable Display Wall project is supported in part by Department of Energy grant DE-FC02-99ER25387, by NSF Infastructure Grant EIA-0101247, by NSF Next Generation Software Grant ANI-9906704, by NCSA Grant ACI-9619019 (through NSF), by Intel Research Council, and by Intel Technology 2000 equipment grant. Thanks to Tat-Jen Cham, Gita Sukthankar, and Mark Ashdown for valuable feedback on the paper. Also, thanks for all the comments by the anonymous reviewers.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Lens Distortion for Close-Range Photogrammetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photometric Engineering</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Accurate Calculation of Camera Homography Trees for Calibration of Scalable Multi-Projector Displays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Cham</surname></persName>
		</author>
		<idno>TR-639-01</idno>
		<imprint>
			<date type="published" when="2001-09" />
		</imprint>
		<respStmt>
			<orgName>Princeton University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic Alignment of High-Resolution Multi-Projector Display Using an Uncalibrated Camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Housel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization</title>
		<meeting>IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large Format Displays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note>Guest editor introduction to special issue</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Open Source Computer Vision Library</title>
		<ptr target="&lt;http://www.intel.com/research/mrl/research/opencv/&gt;" />
		<imprint/>
	</monogr>
	<note>Intel Corporation</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Graph-Based Global Registration for 2D Mosaics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Pattern Recognition</title>
		<meeting>International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Building and Using a Scalable Display Wall System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daminakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Essl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Praun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shedd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzanetakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-Projector Displays using Camera-Based Registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Towles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fuchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization</title>
		<meeting>IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Panoramic Image Mosaics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<idno>MSR-TR-97-23</idno>
		<imprint>
			<date type="published" when="1997" />
			<pubPlace>Microsoft Research</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Smarter Presentations: Exploiting Homography in Camera-Projector Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stockton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mullin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A Scalable Self-Calibrating Technology for Seamless Large-Scale Displays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Surati</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Versatile Camera Calibration Technique for High Accuracy 3D Machine Vision Metrology using Off-the-shelf TV Cameras and Lenses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Robotics and Automation</title>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
	<note>RA-3(4)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pix-elFlex: A Reconfigurable Multi-Projector Display System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gotz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hensley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Towles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization</title>
		<meeting>IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
