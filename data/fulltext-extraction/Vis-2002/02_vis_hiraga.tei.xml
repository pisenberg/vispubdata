<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Case Study: A Look of Performance Expression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rumi</forename><surname>Hiraga</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bunkyo University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Case Study: A Look of Performance Expression</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.5.5 [Sound and Music Computing]: Systems; J.5 [Arts and Humanities]: Performing arts Music Performance</term>
					<term>Expressive Cue</term>
					<term>Performance Visualization</term>
					<term>Understanding Performance</term>
				</keywords>
			</textClass>
			<abstract>
				<p>For most of the time, we enjoy and appreciate music performances as they are. Once we try to understand the performance not in subjective terms but in an objective way and share it with other people, visualizing the performance parameters is indispensable. In this paper, a figure for visualizing performance expressions is described. This figure helps people understand the cause and position of the performance expression as it has expressive cues, which coincide with the cognitive meaning of musical performance, and not by using only MIDI parameter values. The differences we hear between performances are clarified by visualized figures.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>With performed music the resulting sounds are interpretation of a printed notation. Musical scores have been the basis of performances. Besides the usual staff notation, Karkoschka has introduced several new score types -some resemble a contour map, some like planisphere-which are used by modern composers <ref type="bibr" target="#b7">[8]</ref>. In his book, he says that music can even be performed from Kandinski's paintings.</p><p>If we want to understand the difference between more than two performances or why a note seems to have more impact, and share a clear understanding of the performance with other people on an objective basis, we need scientific rather than artistic performance visualization, since a picture is worth a thousand played notes and musical reviews.</p><p>Visualizing performance realizes the conversion from auditory to visual media. In this conversion, some musical information can be described by two different types of representations, while other information only by one. The purpose of performance-visualization research in general is to find the maximum overlap of performance and visualization data. The sharable information between auditory and visual media is different depending on which abstract level the musical information is described. The shared information in Figure 1 is described using MIDI <ref type="bibr" target="#b0">1</ref> .</p><p>In this paper, we propose a figure which visualizes a melody of a piano performance with its expressive cues (tempo, articulation, and sound level) that represents the musical meaning instead using MIDI parameter values. With this type of figure, especially people who are not well-trained in music can understand the cause and effect of a performance expression clearly. We think this figure <ref type="bibr" target="#b0">1</ref> Musical Instrument Digital Interface. A standard for representing digitized musical performance for compatibility among electronic instruments and for sharing data between computer systems. is usable in visual data mining of musical sound database. It also helps music novices to create a performance of their preference by giving them a visual music environment. In Section 2, we classify the music visualization and describe works related to this research. We refer to issues particular to music visualizations. In Section 3, a case of visualizing performance expressions is described. Finally, in Section 4, we summarize the current study and describe our future plans for using the visualization of performance expressions in actual applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MUSIC VISUALIZATION</head><p>While "music visualization" is restricted to visualizing music with computer graphics for non-artistic purposes, visualizing sound is already indispensable in research and applications. Besides drawing sound as waves, Pickover has developed an interesting and impressively symmetrical figure <ref type="bibr" target="#b11">[12]</ref>. Sobieczky visualized the consonance of a chord on the diagram based on a roughness curve <ref type="bibr" target="#b14">[15]</ref>.</p><p>User interface of Desk Top Music (DTM) software provides users with several types of visualized music figures. Usually, sequence software includes a staff notation and a piano-roll figure, as well as a table of MIDI events. Some introductory DTMs show icons of musical instruments and their players.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Related works</head><p>Although visualization has not been studied for music by many researchers so far, two types have been developed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Augmented score</head><p>Since conventional staff notation does not precisely represent the composers' intentions, Oppenheim proposed a tool for including it <ref type="bibr" target="#b10">[11]</ref>. Kunze proposed several three-dimensional figures for composers to use as a graphical tool when they write their music <ref type="bibr" target="#b8">[9]</ref>. Watanabe proposed a virtual score with focus+context to support music learning <ref type="bibr" target="#b5">[6]</ref>.   <ref type="bibr" target="#b2">[3]</ref> shows the resemblance between played notes by using musical-performance data. Miyazaki's comp-i enables users to generate music of their own by using a rich set of functions with a 3D performance-visualization interface <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Performance visualization</head><p>Hereafter in this paper, we use the term music visualization in a narrower sense meaning using computer graphics to depict music excluding music visualization for artistic purposes. Namely we restrict music visualization to visualize music data on a computer display for a scientific purpose, a method to aid musicians to create their music, or a method to help people understanding music with the visual aids. Above all, our research visualizes performance expression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Problems of visualizing music</head><p>Three issues need to be solved in our visualization of performance expressions.</p><p>1. Which abstract level of music should be used to visualize an expression.</p><p>Sequence software handles musical performance on a computer using MIDI base. Basic events of MIDI are Note On and Note Off which represent the start and end of a played note, respectively, and they have parameters for the pitch (which note to play) and the velocity (the volume of the note) in 128 levels along with the time interval from the previous MIDI event. MIDI based visualization is summarized as follows:</p><p>• Only MIDI parameter values are shown.</p><p>• Parameter values are independent and have no relationships with parameters of other MIDI events. • Musical structure is not considered.</p><p>Thus even if we hear accelerando in a phrase, we cannot find it easily by looking at onset time of each note. The impossibility of handling musical structure, such as a triplet or a measure, to say nothing of a motif or phrase, that we can understand by looking at scores, prevents users creating and judging a musical performance.</p><p>Despite MIDI data being not suitable for visualizing performance expressions, we do not know visualized figures based on the different abstract level of music.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">A small amount of data should represent an expression</head><p>A common problem in music-visualization systems, especially if composed music or MIDI data is manipulated, is, as Kunze indicated, the small amount of data as the object of visualization. This is completely the opposite problem in scientific visualization as well as most information visualization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">VISUALIZING PERFORMANCE EX-PRESSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Expressive cues and Chernoff's faces</head><p>After considering the music abstraction level in performance visualization, we decided on using expressive cues. They describe in qualitative music terminology (tempo, sound level, articulation, tone onsets and decays, timbre, IOI 2 deviations, vibrato, and final ritardando), which is quantifiable with the performance data.</p><p>Gabrielsson reported that listeners find basic emotions (anger, sadness, happiness, and fear) in human performances by intentionally controlling expressive cues <ref type="bibr" target="#b3">[4]</ref>. Then, Bresin showed a makeup experiment with his performance rendering system <ref type="bibr" target="#b0">[1]</ref>. We tried several figures to find a more suitable way to visualize expressive cues and came up with Chernoff's face <ref type="bibr" target="#b1">[2]</ref> as a candidate. Chernoff, a statistician, proposed using computer-drawn faces to visualize data in many dimensions. The advantages of using Chernoff's faces for performance visualization are as follows:</p><p>• Since human beings are familiar with faces, we can easily detect changes.</p><p>• A line of faces can show a time sequence (the change of expression over time).</p><p>• A face can express many dimensions (expressive cues) at once by changing parts of the face.</p><p>The third point, the spontaneous extension of facial dimensions is significant because performance visualization will include more and more expression cues in the future. Currently, 3D computer graphics cannot be extended intuitively to show many dimensions at a time.</p><p>The faces in <ref type="figure" target="#fig_2">Figure 2</ref> are examples of representing expressive cues. We modify the shape of parts of the original Chernoff face and omit some of them (eyebrows, for example). Each face corresponds to a note whose tempo, articulation, and sound level are visualized. The performed note sequence is from left to right.</p><p>Local tempo, calculated as IOI normalized by a note value <ref type="bibr" target="#b2">3</ref> , is mapped in the position of the eyeballs. The eyes look back <ref type="figure" target="#fig_2">(Figure 2, left-hand (2)</ref>) if the tempo is slower and look forward <ref type="figure" target="#fig_2">(Figure 2, right-hand (4)</ref>) if it is faster. The more extreme the eyemovements, the more the tempo deviates from the one specified on the score.</p><p>The contour of the face and the shape of the mouth both represent articulation. Articulation is calculated as the overlap/detachment of two consecutive notes divided by a note value. When a note is played rather in staccato then the mouth shape is narrow <ref type="figure" target="#fig_2">(Figure 2</ref> (2)), while in legato it is round <ref type="figure" target="#fig_2">(Figure 2 (4)</ref>).</p><p>In contrast to local tempo and articulation, the value of the sound level can be calculated in several ways. Though the sound level is described as the velocity value in MIDI parameters, the MIDI value does not correspond to a specific decibel. Currently, we express the sound level as the difference between two consecutive notes. If a note is played more strongly than the previous one, the nose is drawn as '&lt;' <ref type="figure" target="#fig_2">(Figure 2</ref> (2)), at the same velocity '|' <ref type="figure" target="#fig_2">(Figure 2</ref> (3)), and softer '&gt;' <ref type="figure" target="#fig_2">(Figure 2 (4)</ref>). Since the first note has no sound level to compare to, the first nose is always '|'. Faces <ref type="bibr" target="#b0">(1)</ref> and <ref type="bibr" target="#b2">(3)</ref> represent notes that are played with fidelity along a score with local tempo and articulation.</p><p>The visualized performance in <ref type="figure" target="#fig_2">Figure 2</ref> is artificially made. By merely looking at the figure we can see that the performance starts softer and lighter, and ends lengthy in this phrase. In this way, local tempo and articulation show the deviation of a performance from the score. Deviation is important not only in understanding and analyzing performance, but also for performance rendering by a computer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Case: Visualizing three performances of Etude 10-3 by Chopin</head><p>We used three performances of Etude 10-3 by F. Chopin (subtitled "Sadness" and widely known as "Farewell" in Japan). We asked a pianist, Chica, to play two performances according to the interpretations of two editions-Cortot and Paderewski. Because space is limitated, we can only show the first four measures of the melody <ref type="figure" target="#fig_3">(Figure 3</ref>). The slight difference of the editions gives musicians different feelings when they perform them. Chica finds that in the Cortot edition the sadness is like the end of "Gone with the wind", while Paderewski is sadness in calm. The other performance is played by another pianist who uses the Zenon edition (a Japanese score maker), which is based on the Peters edition.</p><p>The visualization of the three performances are shown in <ref type="figure" target="#fig_4">Figure 4</ref>. Each face represents a performed note in the melody. The vertical bar shows the bar in the score. From the figure, we can see that the two performances by Chica vary in tempo and articulation more than the third performance, which is rather regular. The move of sound level is almost the same in these performances. These explanations by looking at figures fit the listener's understanding.</p><p>The upper face line in <ref type="figure" target="#fig_5">Figure 5</ref> (1) shows the difference of the Cortot performance (PC) from the Paderewski one (PP). Different from <ref type="figure" target="#fig_4">Figure 4</ref>, the nose shows the difference in velocity between the two performances. By listening to them, we can recognize that (1) PP is played more slowly, and that (2) C-sharp in the third measure, which is the highest and the most impressive note in these measures is a hesitation in PP. The figure showing the differences <ref type="figure" target="#fig_5">(Figure 5</ref> (1)) indicates that the tempo differs regularly (PC is played faster). It also confirms the difference in expression of C-sharp in the third measure (fourth face), and can see that the difference lies in the articulation. In PC, G-sharp (the note prior to C-sharp) is played more lightly, or conversely, G-sharp is played longer and lengthy in PP. In the figure of difference, what we hear as "hesitation" is indicated at the position as the difference of articulation.</p><p>The lower face line in <ref type="figure" target="#fig_5">Figure 5</ref> (2) shows the difference of PC from the Zenon performance (PZ). PZ sounds louder and faster, which in the figure is indicated by the direction of the noses and eyeballs. We can tell that the third note in the second measure (Fsharp) in PC is distinctively a light performance (shorter and softer).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discussions</head><p>We showed the performance visualization in <ref type="figure" target="#fig_4">Figures 4 and 5</ref> to two users.</p><p>One is an amateur pianist. She recognized the deviation of local tempo and articulation of the performance from the score. Since deviation is important for understanding and analyzing performances, as well as for a performance created by computer, she thought she could get hints from the figures to generate a performance either by using DTM software or a performance rendering system 4 . She felt the change in sound level shown on the figure, which is almost the same as in the performances in <ref type="figure" target="#fig_4">Figure 4</ref>, was as she had heard. In the figure that shows difference ( <ref type="figure" target="#fig_5">Figure 5)</ref>, what she had heard as a "hesitation" was indicated at the position as a difference in articulation.</p><p>The other is also a music amateur who has studied music theory. He recognized the role of the musical notes and the musical struc- Though we visualized all notes uniformly in this figure, each note has a different role in a music. Depending on it can be visualized accordingly. Since musical structure is the nature of music, it will be a help to understand performance from visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSIONS AND FUTURE PLANS</head><p>We described visualizing performance expressions by using modified Chernoff faces. The performance expressions are not described in visualized by MIDI parameter values but by expressive cues that can explain them qualitatively musical meaning. The same shape can compare two performances. With the figures, we can cleary understand the tendency and slight differences of performances. This approach is a solution for handling the abstract level indicated in 2.2.</p><p>In the future we will focus on grouping information based on performance data, such as accelerando or diminuendo, clearly on the figure to enhance understanding of the performance. Another challenge is to introduce continuous data, such as that of the half-pedal on the discrete note icons. After all, since grouped information also describes continous change over a range, this issue is an inevitable problem.</p><p>We plan to include performance visualization in our case-based performance rendering project HHH <ref type="bibr" target="#b6">[7]</ref> so users can choose a case performance as the source of performance expression from a performance database at a glance and not through listening. This is the extraction of auditory data by using visual data mining. Another plan is to make use of the figure at the performance rendering concours Rencon <ref type="bibr" target="#b12">[13]</ref>. Rencon is a project to pursue evaluation methods for performance rendering systems in the style of music contest where clarifying the elements to be reviewed and their standards is important. The visualization is useful for sharing common understanding and coming to an agreement in judging performances which are subjective in nature.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Information overlap between auditory and visual media in the MIDI level</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>IEEE</head><label></label><figDesc>Visualization 2002 Oct. 27 -Nov. 1, 2002, Boston, MA, USA 0-7803-7498-3/02/$17.00 © 2002 IEEE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Sample faces Hiraga proposed simple figures to help analyzing musical performances [5]. Smith proposed mapping 3D graphics based on MIDI parameters [14]. Foote's checkerboard-type figure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Sample score (Etude10-3, F. Chopin)3. The perpetual problemDeveloping a figure style suitable for visualizing all music and that satisfies auditory and visual understanding at the same time is an infinite effort.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Three performances of Etude 10-3. (1) Cortot edition, (2) Paderewski edition, (3) Zenon</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Performance differences. (1) Cortot -Paderewski, (2) Cortot -Zenon ture.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Inter Onset Interval. The interval between the two Note On MIDI events.<ref type="bibr" target="#b2">3</ref> The length of a note specified on a score.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The output of a performance rendering system is a expressive performance as if it is played by a human player.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>This research is supported by The Ministry of Education, Culture, Sports, Science and Technology as Grant-in-Aid for Exploratory Research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Emotional coloring of computercontrolled music performances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bresin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Friberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Music Journal</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="44" to="63" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The use of faces to represent points in kdimensional space graphically</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chernoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="361" to="367" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Visualizing music and audio using self-similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Foote</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia99</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="77" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Emotional expression in music performance: Between the performer&apos;s intention and the listener&apos;s experience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gabrielsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Juslin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology of Music</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="68" to="91" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Visualized music expression in an object-oriented environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hiraga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Igarashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsuura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICMC</title>
		<meeting>of ICMC</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="483" to="486" />
		</imprint>
		<respStmt>
			<orgName>International Computer Music Association</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Music learning through visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hiraga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fujishiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WEB Delivering Music</title>
		<meeting>of WEB Delivering Music</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Next generation performance rendering -exploiting controllability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hirata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hiraga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aoyagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICMC</title>
		<meeting>of ICMC</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="360" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Das Schriftbild der Neuen Musik</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Karkoschka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966" />
			<publisher>Moeck Verlag</publisher>
			<pubPlace>Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">See: A structured event editorvisualizing compositional data in common music</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kunze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Taube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICMC</title>
		<meeting>of ICMC</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="63" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">comp-i: 3d visualization of midi datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Miyazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fujishiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2002-MUS-46</title>
		<imprint>
			<publisher>Information Processing Society Japan</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Compositional tools for adding expression to music</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oppenheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICMC</title>
		<meeting>of ICMC</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="223" to="226" />
		</imprint>
		<respStmt>
			<orgName>International Computer Music Association</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On the use of computer generated symmetrized dot-patterns for the visual characterization of speech waveforms and other sample data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pickover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="955" to="960" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<ptr target="http://shouchan.ei.tuat.ac.jp/-rencon/en/index.html" />
		<title level="m">RENCON</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A visualization of music</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">N</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization</title>
		<meeting>IEEE Visualization</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visualization of roughness in musical consonance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sobieczky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization</title>
		<meeting>IEEE Visualization</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
