<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cartographic Relief Shading with Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Jenny</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Heitzler</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilpreet</forename><surname>Singh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marianna</forename><surname>Farmakis-Serebryakova</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffery</forename><forename type="middle">Chieh</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenz</forename><surname>Hurni</surname></persName>
						</author>
						<title level="a" type="main">Cartographic Relief Shading with Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Fig. 1. Shaded relief of the Caucasus Mountains created with a neural network trained with a manual relief shading of Switzerland.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Cartographers commonly use shaded relief (or "hillshading") because it is an effective and aesthetic representation of terrain on maps. The main advantage of shaded relief is intuitiveness, as most map readers can easily and quickly interpret terrain elements ranging in size from large mountain ridges to small landforms, such as gullies or terraces. When drawing shaded relief manually, expert cartographers adjust the direction of illumination and modulate the brightness of individual terrain forms to visually accentuate the main terrain structures, while also making smaller, important details immediately recognisable <ref type="bibr" target="#b0">[1]</ref>. A shaded relief is used in conjunction with other map elements, rather than as a standalone feature. It creates the impression of a threedimensional surface on a flat, two-dimensional map and adds a pleasant aesthetic quality <ref type="bibr" target="#b1">[2]</ref>  <ref type="figure">(Fig. 2)</ref>. However, creating a highquality shaded relief requires specialised expertise and is immensely labour intensive, even when using digital image editing tools <ref type="bibr" target="#b2">[3]</ref>. While standard algorithms shade digital elevation models in a fraction of a second, the resulting images lack expressiveness. The structure of terrain is more difficult to perceive with digital shading than with carefully crafted manual relief shading. For this reason, professional cartography often still uses manually produced shaded relief. For example, Switzerland's national mapping agency swisstopo, whose maps are frequently considered the gold standard in topographic mapping <ref type="bibr" target="#b3">[4]</ref>, scanned and georeferenced their manually shaded relief images when transitioning to digital production.</p><p>Our goal is to accelerate the production of high-quality shaded relief and make the process more accessible to map authors to better enable and encourage them to include shaded relief art in their maps. We were inspired by recent neural networks for the generation of realistic yet artificial graphical objects with image-to-image translation <ref type="bibr" target="#b4">[5]</ref>, neural style transfer <ref type="bibr" target="#b5">[6]</ref>, image completion <ref type="bibr" target="#b6">[7]</ref>, and colourisation of grey images (e.g. <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>).</p><p>We create shadings from elevation models with a variation of a U-Net <ref type="bibr" target="#b9">[10]</ref>  <ref type="figure">(Fig. 1</ref>). U-Nets are a type of fully convolutional neural networks and were developed for image segmentation <ref type="bibr" target="#b10">[11]</ref>. We employ a modified U-Net architecture for neural end-to-end rendering of digital elevation models. We train neural networks with highquality manual shadings created by swisstopo cartographers. The neural networks are able to render shaded relief imagery that closely approximates the style and expressiveness of manual shading.</p><p>Our contributions include: (a) a neural network U-Net architecture for creating shaded relief imagery from digital elevation models; (b) insights on training neural networks with manual shadings; (c) methods for controlling illumination direction, level of detail, and height-dependent contrast of neural network shading; and (d) an evaluation of neural network shading with 18 experts, who provided very positive feedback and found that shaded relief created with our neural network method can reach the quality of manual shadings. <ref type="figure">Fig. 2</ref>. Shaded relief with contour lines to portray the third dimension of terrain on a topographic map (1:50,000, reduced size, © swisstopo). <ref type="figure">Fig. 3</ref>. Exemplary manual relief shading. Arrows indicate localised illumination direction. "NW" indicates a ridge oriented in the northwestern direction with illumination adjusted to the west. Area A (yellow outline) is shaded in brighter tones and area B (purple outline) in darker tones to accentuate the ridge between A and B. (Ticino area by Eduard Imhof and Heinz Leuzinger, from shadedreliefarchive.com <ref type="bibr" target="#b22">[23]</ref>).  It is important to note that the design principles require the cartographer to read and interpret a landscape, usually from elevation contours and morphological break lines indicating main ridges and valleys. When a landscape is shaded by multiple cartographers, the resulting images will likely vary based on personal style and interpretation of the design principles (for an example of different styles, see <ref type="bibr" target="#b23">[24]</ref>). Shaded relief is rarely produced manually nowadays because the process is an immense undertaking. Only a handful of specialists with the necessary training and artistic talent exist. For illustrations of their workflows, see <ref type="bibr">( [3]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• •</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2</head><p>Digital Relief Shading and Illustrative Shading Digital terrain shading of digital elevation models mainly uses variations of Lambert's diffuse reflection model. A large angular difference between the terrain normal and the illumination vector results in a dark value, while a small difference creates a bright value ( <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b29">[30]</ref>). To apply manual design principles to digital mapping, cartographers have developed algorithms for adjusting the illumination with the orientation of ridge and valley lines ( <ref type="bibr" target="#b30">[31]</ref>- <ref type="bibr" target="#b32">[33]</ref>), combining multiple shadings with varying illumination directions ( <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>), applying sky illumination models ( <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>), and adjusting illumination with surface orientation <ref type="bibr" target="#b37">[38]</ref>, curvature <ref type="bibr" target="#b38">[39]</ref>, or slope and elevation <ref type="bibr" target="#b39">[40]</ref>. A recent user study <ref type="bibr" target="#b40">[41]</ref> compared different relief shading methods and found that no single shading method works well for all major types of landforms (e.g. alpine mountains, folded and eroded mountains, V and U-shaped valleys, etc.), but the study found that for many landforms, the 151 participants preferred the clear sky shading proposed by Kennelly and Stewart <ref type="bibr" target="#b35">[36]</ref>. Besides cartographic shading algorithms, specialised generalisation operators exist for simplifying, smoothing, and enhancing features in elevation models that are applied before the shaded image is computed for maps at large scales (e.g. <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>) and small scales (e.g. <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>).</p><p>Illustrative visualisation is a group of varied non-photorealistic rendering techniques that emphasise specific structural aspects of measured, simulated or modelled data (see Lawonn et al. <ref type="bibr" target="#b45">[46]</ref> for a definition and review). The goal of illustrative visualisation is to communicate selected aspects more effectively than with conventional (photorealistic) shading and is often inspired by traditional illustration principles <ref type="bibr" target="#b45">[46]</ref>. The cartographic relief shading algorithms described above represent a particular type of illustrative shading. Exaggerated shading, an illustrative shading technique proposed by Rusinkiewicz et al. <ref type="bibr" target="#b46">[47]</ref>, was even inspired by Imhof's principles for relief shading. They modify the diffuse shading equation for different frequency bands, emphasising surface details at various scales. Light warping by Vergne et al. <ref type="bibr" target="#b47">[48]</ref> and light collages by Lee et al. <ref type="bibr" target="#b48">[49]</ref> are two examples closely related to cartographic shading as they adjust the lighting direction to object geometry. Also related to our work is the simple lit-sphere shading-by-transfer method by Sloan et al. <ref type="bibr" target="#b49">[50]</ref>. They use a shaded sphere as a lookup texture for shading.</p><p>Neural rendering is a varied and rapidly evolving field that combines generative machine learning with computer graphics. The overview by Tewari et al. <ref type="bibr" target="#b50">[51]</ref> identifies diverse applications of neural rendering, for example, view synthesis, facial and body re-enactment, or photo relighting. A group of neural rendering techniques process computer graphics output (such as depth map, normal map, per-pixel material parameters, diffuse rendering, etc.) to improve low-quality renderings <ref type="bibr" target="#b51">[52]</ref>, simulate ambient occlusion ( <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>) or shading, and other illumination effects <ref type="bibr" target="#b53">[54]</ref>. Our proposed method is inspired by these image-to-image translation operators; it transforms a scalar field with elevation values to a shaded image.</p><p>Neural style transfer is a popular type of neural rendering that uses a deep neural network to transfer the appearance of an image, building on concepts of content and style of images, which the inventors describe as follows <ref type="bibr" target="#b5">[6]</ref>: Content is directly defined by the feature representations of each layer of a neural network. Style is defined for each layer by the feature correlation of the vectorised feature maps as expressed by the Gram matrix. Style and content are typically obtained from two different images that are fed into the same (pre-trained) neural network (e.g. VGG <ref type="bibr" target="#b54">[55]</ref>). For transferring manual relief shading, the content image is a shading computed from an elevation model, while the style image is a manual shading of an arbitrary geographic area. To obtain the stylised shaded image, an image is initialised with random noise and then fed into the pre-trained neural network. Its pixels are iteratively optimised until losses in respect to style and content are minimised. Variations of this approach are discussed in two surveys ( <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref>).</p><p>Tom Patterson, a renowned expert in terrain mapping, applied neural style transfer for relief shading <ref type="bibr" target="#b57">[58]</ref>. First, he digitally shaded an elevation model, then used Prisma-a photo editing app-to apply an artistic style trained on paintings of rural life in India, which by happenstance, resulted in an improved, stylised shading. The neural style transfer was employed subtly for small changes. We experimented with neural style transfer and transferred hand-drawn relief shading to digital shadings computed from elevation models. While the manual style was closely replicated, we identified two problems: (1) neural style transfer could move or distort the geometry of terrain features, and (2) it occasionally omitted or invented terrain features that did not exist in reality. We were able to reduce these problems by tuning training parameters, but careful scrutiny of the resulting shading was required, and the problems could not be avoided entirely. We concur with Lawonn et al. <ref type="bibr" target="#b45">[46]</ref> that neural style transfer does not lead to satisfactory results for illumination transfer.</p><p>Summary: Despite the many attempts to improve terrain shading since the early days of computer cartography, relief shading created with current methods does not reach the high quality of manual shadings. For example, existing methods for adjusting the direction of illumination are either cumbersome and complicated to use, require considerable manual intervention, or do not provide satisfactory results. Illustrative visualisation techniques have been applied to terrain models, and the development of some illustrative methods was even inspired by cartographic relief shading. However, there is no digital method capable of applying the design principles developed for manual shading in a coherent and practical way. The main problems are inconsistency in accentuating large landforms, lack of automated tools for locally adjusting illumination directions, and absence of high-quality generalisation of terrain models for relief shading, particularly at medium and small scales <ref type="bibr" target="#b57">[58]</ref>. Neural style transfer is not applicable to cartographic terrain shading because mapping requires accurate positioning of all relevant terrain features and cannot accept the rendering of invented, non-existing features. Neural imageto-image rendering has proven successful in simulating illumination effects, but to our knowledge this has not been applied for transferring artistic relief shading for maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NEURAL NETWORKS FOR RELIEF SHADING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network Architecture</head><p>Our neural network architecture is an image-to-image generator that is trained with a manually produced shaded relief image and a digital elevation model of the same geographic area. The trained neural network is then applied to a digital elevation model of any geographical area to create a shaded relief. The design style and amount of generalisation of the manual shading used for the training are transferred to the new shading. Consequently, the neural network replaces commonly used shading models, such as Lambert's diffuse reflection, and also removes excessive details. Neural network shading does not explicitly model illumination direction, but instead learns how to illuminate the model from the manual shading. The input consists of a scalar field with terrain elevation that is normalised between 0 and 1. No other terrain attributes that could be derived from the elevation model are provided to the neural network, such as normal vectors, slope steepness, or slope orientation. When developing the network architecture, we experimented with a variety of convolutional neural network architectures. Unlike fully connected neural networks, convolutional neural networks can better exploit the spatial structure of images by training the weights of filter kernels rather than the weights used for linear combinations. We experimented with convolutional encoder-decoder networks as this family of architectures is well suited for transforming images, e.g. denoising and super-resolution <ref type="bibr" target="#b58">[59]</ref> or colourisation <ref type="bibr">([8]</ref>, <ref type="bibr" target="#b8">[9]</ref>). We also experimented with state-of-the-art ResNet <ref type="bibr" target="#b59">[60]</ref> and Inception <ref type="bibr" target="#b60">[61]</ref> modules within our architectures. To guide the training of our models, we investigated various loss functions: L1, L2, structural similarity index (SSIM <ref type="bibr" target="#b61">[62]</ref>), and perceptual loss <ref type="bibr" target="#b62">[63]</ref>.</p><p>Our final network is a U-Net <ref type="bibr" target="#b9">[10]</ref>. U-Nets use connected encoding and decoding layers and have proven to be particularly useful for image segmentation, e.g., in the context of biomedical images <ref type="bibr" target="#b9">[10]</ref> or geospatial raster data <ref type="bibr" target="#b63">[64]</ref>. A U-Net consists of a contracting path (or encoder) to analyse the spatial context of an image and an expanding path (or decoder) that allows for accurate localisation of predicted features. Along the contracting path, the input image is gradually down-sampled using max pooling layers to obtain a compact multiscale feature representation. The expanding path makes use of up-sampling layers to gradually reobtain the dimensions of the input image. By copying the feature matrix of an intermediate step of the contracting path to the corresponding step of the expanding path, the network gradually combines low-level and high-level feature representations. <ref type="figure" target="#fig_3">Figure 6</ref> shows an architectural schema of the U-Net variant used. The main differences between our architecture for neural network shading and the original U-Net architecture are as follows:</p><p>• The U-Net variant employs five instead of the standard four down-/up-sampling steps in each path. Our experiments have shown that other numbers of down-/up-sampling steps (e.g., four or six) also yielded satisfying results, but more than five steps did not result in visual improvements. Visual quality declined considerably with three or fewer steps. • Dropout is used at each level of the network with increasing dropout rates towards the innermost layers. <ref type="figure" target="#fig_3">Figure 6</ref> indicates dropout rates for each layer with dropout. The original U-Net architecture uses dropout only at the innermost layers. <ref type="figure" target="#fig_2">Figure 7</ref> illustrates that dropout for each layer is essential; when dropout is only applied at the innermost level, the resulting shading shows heavy, local disturbance ( <ref type="figure" target="#fig_2">Fig. 7</ref>, left). These artefacts are likely due to overfitting. Dropout effectively prevents overfitting by randomly supressing output from network units ( <ref type="figure" target="#fig_2">Fig. 7</ref>, centre). This can be interpreted as training a multitude of different "thinned" architectures whose averaged result is approximated during inference <ref type="bibr" target="#b64">[65]</ref>, yielding the desired smooth shading. • Weights are initialised via the method described by He et al.</p><p>[66] instead of sampling from a normal distribution. Networks initialised from a normal distribution showed a higher probability of failing to learn certain desirable shading characteristics. For example, in <ref type="figure" target="#fig_2">Figure 7</ref> (right), some slopes were depicted as plain grey areas without brightness modulation. This issue rarely occurs with He initialization, which is tailored to the initialization of ReLU layers and typically approaches better local optima than comparable methods <ref type="bibr" target="#b66">[66]</ref>. • Power-of-two image tiles are used to avoid cropping operations required by the original U-Net architecture at intermediate steps.</p><p>In <ref type="figure" target="#fig_3">Figure 6</ref>, the tile size is 256 × 256 pixels; the architecture can be adjusted to other power-of-two sizes.</p><p>As with the original U-Net architecture, ReLU layers <ref type="bibr" target="#b67">[67]</ref> are used as activations after each convolutional layer. However, in contrast to segmentation where thresholding is applied to determine the class a pixel belongs to, no such operation is necessary for the proposed approach since the final predictions already represent the desired grayscale values. Clipping is applied for the rare case of a value exceeding the range [0, 1].</p><p>Because elevation models can be large, a terrain model is split into regular tiles and the network is trained to shade a single elevation tile. We typically use square terrain tiles with a width of 256 pixels or 512 elevation values. A shaded output tile is smaller than the corresponding input terrain tile. The goal is to allow the network to "look beyond" the area of the shaded output tile and adjust grayscale values with landforms that are larger than the size of the output tile. This enables the network to adjust the direction of illumination with the orientation of major mountain ridges that are not included in the output tile, and adjust brightness for major landforms. In the <ref type="figure" target="#fig_3">Figure 6</ref> example, the cropped border is 50 pixels wide, resulting in output tiles of 156 × 156 pixels, covering 37% of the input terrain tile. When applying a trained network for shading an elevation model, the model is split into slightly overlapping tiles. The shaded output tiles are assembled into a final image using alpha blending to smooth differences in brightness along tile borders. An alternative to tiling is to modify the input and output layers of the network after training to render an entire elevation model in one go. It is possible to change the dimensions of the input and output layers because a U-Net only contains convolutional layers and therefore, only trains filter kernels, which do not depend on the dimensions of the input image. This alternative method is limited, however, by the size of available GPU memory.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data and Neural Network Training</head><p>The manual shaded relief used for training the neural networks was created for the Swiss national map series at scales between 1:25,000 and 1:1 million, of which the first map sheets were published in 1937. The multi-scale map series can be viewed online at https://map.geo.admin.ch. A small team of specialist cartographers at swisstopo created the shaded relief images mainly with airbrush tools ([68] <ref type="bibr" target="#b12">[13]</ref>). A shaded relief image was created for each of the 247 map sheets at 1:25,000, 78 sheets at 1:50,000, 23 sheets at 1:100,000, and fewer sheets for smaller scales. The shadings are available as digital images that align with digital elevation models <ref type="bibr" target="#b69">[69]</ref>. We trained neural networks with shadings at 1:50,000 and smaller scales. Cartographic generalisation during manual shading removed or strongly simplified small terrain features in maps at smaller scales. The shaded relief also shows local differences in style, contrast, and level of detail, which are the result of multiple cartographers contributing to this large work over many years. Consequently, the Swiss shaded reliefs contain many subjective decisions that were necessary to create this masterpiece. We digitally edited the manual shadings before using them for training ( <ref type="figure" target="#fig_4">Fig. 8)</ref>. Most lakes had not been filled with a continuous grey tone in the manual shadings because lakes are typically shown without shading on the final maps. Using Adobe Photoshop, we individually selected the bright lake spots and filled them with a consistent grey tone. For example, in the lower part of <ref type="figure" target="#fig_4">Figure 8</ref> (left), a north-facing mountain slope blends into the adjacent white lake. On <ref type="figure" target="#fig_4">Figure 8</ref> (right), the lake has been filled with a grey tone creating a clear distinction between the lake and the bottom of the neighbouring slope. We also homogenised the grey tone for flat areas, which varied among map sheets and caused trained neural networks to render plains with spotty brightness. For example, the flat area in the lower right corner of <ref type="figure" target="#fig_4">Figure 8</ref> (left) was corrected with the Photoshop brush tool because its brightness varied considerably. These manual corrections required roughly 25 hours of work for the 78 sheets at 1:50,000; however, it was only necessary to do one time before training the network.</p><p>Consistent and artefact-free shading of flat or nearly flat areas requires particular attention. In our training data, lakes and other flat areas exist at only a few altitudes (mainly between 200 and 570 meters in elevation). The networks initially produced random artefacts when asked to shade flat areas at some higher elevations. This is addressed by introducing artificial flat tiles placed at random elevations and corresponding shaded tiles with a continuous grey value. Alternatively, small sections of near-flat areas of the elevation model can be duplicated and then shifted vertically by a random amount. Training with shifted near-flat areas eliminates shading artefacts. When training the network, we also randomly shift the tiling origin of the input shading and input elevation model horizontally after a few epochs. Shifting the tiles reduces inconsistent spotty rendering of almost flat areas. Shifting every 25 epochs has proven successful. The loss function for training the network computed the mean squared error between the created shaded tile and the corresponding manual shading tile. It only considered the central area indicated by the red square in <ref type="figure" target="#fig_3">Figure 6</ref>.</p><p>The open-source TensorFlow library was used to model and train networks. Source images and terrain models for training always have the same number of pixels in width and height. They measured up to approximately 16,000 × 10,000 pixels. Training with an RTX 2080 GPU ran for 1,000 to 5,000 epochs, typically for one to five hours. We trained with a batch size of eight and used the Adam optimizer with values α = 0.001, β1 = 0.9, β2 = 0.999, and = 10 -8 as suggested by Kingma and Ba <ref type="bibr" target="#b70">[70]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Controlling Illumination Direction, Contrast, and Generalisation</head><p>Three aspects for cartographic shading can be controlled with neural network shading: the direction of illumination, the contrast of shading according to terrain type, and the amount of generalisation. Our neural network architecture does not itself provide explicit control of these parameters. Instead, the input terrain model is manipulated-rotated, vertically shifted, or filtered-to give users control over these aspects.</p><p>The global lighting direction can be altered by rotating the terrain model before shading it and then rotating the resulting shaded relief image back in the opposite direction. This can be useful for creating a shaded relief with southern illumination, which is occasionally used to imitate the natural illumination direction in the northern hemisphere. <ref type="figure" target="#fig_5">Figure 9</ref> shows an example shaded relief illuminated with standard north-western and alternative southern illumination. In Switzerland, the topography is largely characterised by three varying landscape types. The first is plains with large lakes and rolling hills that have elevations between approximately 300 and 1,000 meters (m). The second type are the limestone Jura Mountains with deep river valleys and both round as well as pointy crests reaching approximately 1,500 m. Lastly, the Alps with characteristic pointed peaks and extremely steep slopes reach altitudes of more than 4,500 m. The manual shaded relief shows these three landscape types with increasing contrast between bright illuminated and dark shaded slopes <ref type="figure" target="#fig_0">(Fig. 4)</ref>. We exploit the fact that the landscape types and contrast vary with elevation. Instead of using the full normalised elevation range between 0 and 1, the user can control the relative minimum and maximum. For example, when shading an alpine terrain, elevation values are normalised to the full range [0, 1]. However, when shading lowlands or rounded hills, the user can scale elevation values to [0, kmax] with kmax &lt; 1. <ref type="figure">Figure 10</ref> shows a shading of an almost flat lowland where kmax = 20%: The lowland is shown with appropriate smooth brightness transitions throughout instead of high-contrast craggy edges. Controlling the lower limit can also be useful when shading high alpine terrain without lowland, that is, [kmin, 1] with kmin &gt; 0. Controlling the elevation range allows for adjusting the shading style to fit the type of terrain, as well as shading adjacent terrain models with different landscape types (such as the lowlands and Himalayas in India) and then combining the shadings. <ref type="figure">Fig. 10</ref>. Flat lowland shaded with full elevation range (0 to 100%) creates overly strong contrast and alpine appearance (left). When scaling elevations to 0 to 20%, the landscape rounded by glacial erosion is appropriately represented (right, northernmost part of Poland).</p><p>The neural networks can create generalised shading by removing distracting detail much like in manual shading ( <ref type="figure" target="#fig_1">Figure 5</ref>). The amount of generalisation can be controlled by a pre-training and a posttraining method. The pre-training method encodes the generalisation in the neural network. For example, we trained three networks with a strongly generalised manual shading at a scale of 1:500,000 (sections are shown in <ref type="figure" target="#fig_0">Fig. 4</ref>) and three terrain models with cell sizes of 200 m, 100 m, and 50 m. The network trained with a terrain model with 200 m cell size learned to retain and accentuate details because it saw only low-frequency terrain information during training. The network trained with 50 m cell size saw high-frequency information during training and learned to discard details accordingly. <ref type="figure" target="#fig_6">Figure 11</ref> illustrates the increasing generalisation when the three networks shade the same elevation model with a cell size of 150 m.</p><p>The post-training method consists of down-sampling the resolution of the elevation model. Notably, sharp edges are retained, as networks learn to accentuate abrupt, high-frequency transitions <ref type="figure" target="#fig_7">(Fig. 12)</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION</head><p>To evaluate the performance of neural network shading, we implemented an application for rendering arbitrarily sized terrain models. We used the hardware-accelerated Apple Core ML framework <ref type="bibr" target="#b72">[72]</ref> and a 2019 laptop computer with an AMD Radeon Pro 5300M GPU. Rendering an elevation field with 5,000 × 5,000 values takes six seconds with an input terrain tile size of 256 × 156 × , and 20 pixels alpha blending between neighbouring tiles. Tile-based rendering is scalable to large elevation models and is only limited by the amount of available memory.</p><p>As a testimony to the quality of shading with neural networks, swisstopo decided to use the presented U-Net architecture to extend the area covered by their existing manual relief shadings at 1:200,000 and 1:500,000 scales.</p><p>Evaluation of a shaded relief image is difficult to quantify with quantitative metrics because a single ground truth reference image does not exist. As a result, loss function values or similar metrics are not indicative of a network's performance. Visual qualitative inspection is the preferred method for evaluating shaded relief <ref type="bibr" target="#b31">[32]</ref> and is also common for evaluating illustrative visualisation <ref type="bibr" target="#b45">[46]</ref> and "creative" convolutional neural networks generating art or photorealistic renderings. Subjects for human evaluation of new rendering methods are the authors (e.g. <ref type="bibr" target="#b73">[73]</ref>, <ref type="bibr" target="#b74">[74]</ref>), domain experts (e.g. <ref type="bibr" target="#b75">[75]</ref>, <ref type="bibr" target="#b76">[76]</ref>), or general users on online survey platforms (e.g. <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b77">[77]</ref>). We first compare neural network shading to other digital shading methods and then report on an evaluation with relief shading experts who compared neural network shading to manual shadings. <ref type="figure" target="#fig_8">Figure 13</ref> shows a Lambertian diffuse shading, a neural network shading, and a clear sky shading of an area that includes plains, mountains with an intricate structure, northwest trending mountain ridges (which align with the main illumination direction), and a coneshaped volcano. The neural network was trained with the 1:500k swisstopo shading <ref type="figure" target="#fig_0">(Fig. 4)</ref> with a cell size of 100 m, which resulted in a training image and a training elevation model of only 4,000 × 2,700 pixels (or approximately 150 tiles of 256 × ). The three shadings in <ref type="figure" target="#fig_8">Figure 13</ref> were created with an elevation model with a cell size of 180 m. The neural network shading shows terrain features at correct locations, which is relevant for aligning the shading with other map information, such as drainage networks and summit points. The network did not invent non-existing features or omit important features. The neural network successfully learned to replicate the design principles for terrain shading: the network clearly depicted the main landforms, accentuated high elevations with stronger contrast than low valleys, and locally adjusted the light direction. Illumination is rotated to the west for the northwest trending hills in the lower left corner. The same hills are difficult to perceive in the diffuse shading because the illumination direction aligns with the hill crests. On the diffuse shading, the rendition of the amalgamation of mountains at the centre is confusing and cluttered; the neural network shading more clearly shows the intricate shape of this terrain. The neural network also successfully generalised the terrain; it removed excessive details and accentuated main ridges and valleys, resulting in a terrain rendering that is easier to read and combine with other map information than the diffuse shading.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison with Digital Shading</head><p>Clear sky shading <ref type="bibr" target="#b35">[36]</ref> is a variation of ambient occlusion shading that was preferred over other relief shading methods in a recent user study <ref type="bibr" target="#b40">[41]</ref>. Before rendering the clear sky shading with the SkyLum software <ref type="bibr" target="#b78">[78]</ref>, we removed details from the elevation model in mountainous areas with line-integral convolution filtering <ref type="bibr" target="#b42">[43]</ref>, which retains edgy ridges. Clear sky shadings are considerably darker than diffuse and neural network shadings when applied with a recommended fivefold vertical exaggeration <ref type="bibr" target="#b78">[78]</ref>. In <ref type="figure" target="#fig_8">Figure 13</ref>, brightness was increased to approximate the appearance of the other shadings. Clear sky shading creates dark values for narrow valleys and bright lines for sharp ridges. It accentuates and exaggerates small details, resulting in an uneven noisy distribution of bright and dark areas; grey values mainly vary with narrowness of valleys. In comparison, neural network shading varies values according to the relevance and elevation of mountain ridges. Diffuse shading and clear sky shading do not adjust illumination for ridges and other landforms that align with the direction of illumination, as in the lower left corner of <ref type="figure" target="#fig_8">Figure 13</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with Manual Shading by Experts</head><p>An online survey collected feedback from expert cartographers to determine how they rate shadings created with neural networks, whether they prefer manual shading or neural network shading, and identify shortcomings of neural network shading. This study did not evaluate whether mountain ridges or other terrain features were easier to perceive in manual or neural network shading than with other digital shading methods. Participants: Eighteen experts (4 female, 14 male) participated; names are listed in the Acknowledgments section. Sixteen have extensive experience in creating high-quality shaded relief either manually or with digital tools, and two have published research results aiming at the digital automation of design principles for relief shading.</p><p>Setup: We asked experts to rate and comment on ten pairs of shadings ( <ref type="figure" target="#fig_0">Fig. 14)</ref> using an online survey. The two images in each pair showed shadings of the same geographic extent; one image was generated with a neural network and the other was drawn manually. The order of the two images within each pair was random and participants were not informed how the shadings were created. The two images were displayed side-by-side (except for the first pair, which was arranged vertically) with a total width of 1,000 pixels.</p><p>Tasks: For each pair, participants were asked to comment on strengths and weaknesses of both images and rate each image on a 5point Likert scale between poor and excellent. For each pair, participants also indicated which image was better for a topographic map, which image was more aesthetic, and which image was created manually. These questions were answered with one 5-point Likert scale per image pair. The possible answers were clearly left (1), rather left <ref type="bibr" target="#b1">(2)</ref>, no preference/I don't know <ref type="bibr" target="#b2">(3)</ref>, rather right <ref type="bibr" target="#b3">(4)</ref>, and clearly right <ref type="bibr" target="#b4">(5)</ref>. For reporting the results, we converted the answers for preference and aesthetics from numerical Likert values to corresponding categories: 1 = clearly manual, 2 = manual, 3 = no preference, 4 = neural, 5 = clearly neural. Answers to the question asking users to identify the manual shading were converted to 1 = clearly wrong, 2 = wrong, 3 = uncertain, 4 = correct, 5 = clearly correct. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data:</head><p>We selected high-quality manual shadings of geographic areas outside of Switzerland because the neural networks could have been overfitted and learned to replicate Swiss geography instead of creating generic relief shading. Our goal was to assess the applicability of neural network shading to other areas. We initially compiled 17 pairs of shadings and had two experts take the survey. We then removed the seven pairs that received the lowest scores for the manual shadings from the two experts in order to avoid comparing our neural network shadings to suboptimal manual shadings. The final set included shaded reliefs at various scales and terrain types. Seven manual shadings were created by the U.S. National Park Service for their park brochures <ref type="figure" target="#fig_0">(Fig. 14, #2</ref> and #5-10), two shadings (#3 Lorraine and #4 Lima) were from the Swiss World Atlas [79], and #1 Tutuila, American Samoa was created for another atlas <ref type="bibr" target="#b79">[80]</ref>. Most shadings are available through the Shaded Relief Archive <ref type="bibr" target="#b22">[23]</ref>. The pairs of shadings are included in the supplementary materials. For each manual shading, a neural network shading with a similar level of detail and overall appearance was created by controlling contrast and generalisation with the techniques described in Section 3.3. The default upper-left illumination direction was used for all shadings. Global brightness and contrast of neural network shadings were adjusted such that both images of a pair had a similar appearance.</p><p>Results: Experts spent considerable time answering the ten questions; the average time was very close to one hour (excluding five outliers who reported longer intermissions). <ref type="figure" target="#fig_1">Figure 15</ref> presents the expert ratings. The first column shows overall ratings for each shading ordered by pairs of manual and neural network shadings. While the experts agreed that most manual shadings that we selected were of fair, good, very good, or excellent quality, we were nevertheless surprised by how critically they rated the manual shadings. The manual shading of pair #10 Rocky Mountain National Park was an outlier, because it received five poor and four fair ratings. Some experts criticised its emphasis on the drainage network, lack of accuracy, and the stylised representation.</p><p>Ratings summed across all neural network shadings were: 15% poor or fair, 32% good, 43% very good, and 10% excellent. Most neural network shadings received similar ratings as their paired manual shadings, particularly maps showing mountain ranges resembling Swiss topography (#4, 6, 7), and/or maps at large scales (#1, 4). The neural network shading of pair #3 Lorraine, #5 Hot Springs, and #8 Black Canyon received more than two poor or fair ratings.</p><p>When asked to indicate "which of the two shaded reliefs is better for a topographic map", there were five pairs with the neural network shading preferred (#1, 2, 4, 7, 10), one pair with the manual shading preferred (#3), and four pairs without a clear preference <ref type="bibr">(#5, 6, 8, 9)</ref>. Experts preferred the neural network shading when they also rated the neural network shading higher (#1, 2, 4, 7, 10) and vice versa (#3). A recurring theme in comments was the more consistent generalisation of neural network shadings, which the experts appreciated. An example is the #5 Hot Springs pair: the manual shading received many fair ratings due to the less than perfect selection and representation of the multitude of small details.</p><p>Aesthetic preference ratings aligned with general preference ratings (compare the pairs of bars in the right column of <ref type="figure" target="#fig_1">Fig. 15</ref>). Across many pairs, experts commented positively on the more balanced, clear, and consistent appearance of neural network shading. For example, for the manual shading of #2 Crater Lake, some experts criticised the dark, "globby" and inconsistent look. At the same time some experts also liked when manual shadings highlighted landforms that were of particular interest, for example, the crater in #2 Crater Lake or the valley bottom of #8 Black Canyon. #3 Lorraine is the only pair of shadings where experts preferred the manual shading to the neural network shading. They commented that they liked the clear hierarchy of the large major landforms in the manual shading and criticised the lack of hierarchy and indifferent representation of the landforms in the neural network shading. Experts had difficulty identifying the manual shading for some pairs. For each pair, they answered the question "which of the two shaded relief images was created manually?" on a 1-5 scale where 1 = "clearly left", 2 = "rather left", 3 = "I don't know", 4 = "rather right", and 5 = "clearly right" ("left" and "right" referred to the two randomly ordered images). <ref type="figure" target="#fig_3">Figure 16</ref> shows that the manual shadings that were most difficult to identify were #1 Tutuila, #3 Lorraine, and #7 City of Rocks. The manual shading in #10 Rocky Mountain was most easily identified.  <ref type="figure" target="#fig_1">Figure 15</ref>).</p><p>The survey also asked experts how likely they would use neural networks for relief shading. Fourteen replied they were very likely or very interested in using it, one replied with "possibly", one was unlikely to use it, one is retired and no longer creates maps, and one did not provide an answer. Experts could also provide optional comments, some of which were enthusiastic in support of neural shading. The proposed U-Net architecture for image-to-image neural rendering successfully shades elevation models with the expressive Swiss-style relief shading. In most tested examples, the neural network shaded relief images accurately portray major landforms as well as small, relevant terrain features. We did not detect any invented, non-existing features in our test shadings.</p><p>Compared to the clear sky shading, our neural network test shadings show terrain with a more even grey value distribution and consistent generalisation. For example, the neural network shading in <ref type="figure" target="#fig_8">Figure 13</ref> locally adjusts the light direction to show all major landforms, resulting in a clearly structured rendering.</p><p>The 18 relief shading experts generally rated neural network shadings and the corresponding manual shadings similarly in overall quality, preference, and aesthetics. Only one of ten neural network shadings was rated lower than the corresponding manual shading. For many pairs, experts had difficulty identifying the manual shading from the neural network shading. Experts also liked the aesthetics of neural network shading and most commented positively about it.</p><p>The current neural network shading has limitations. There is an occasional lack of detail in flatter areas, when a neural network is applied to a terrain model that has a markedly different cell size than the model used for training. In <ref type="figure" target="#fig_2">Figure 17</ref> for example, a neural network trained with an elevation model with a cell size of 200 m is unable to accurately render a relatively flat elevation model with a cell size of 5 m (left). Details in the terrain are difficult to discern due to the lack of light modulation in flat areas. An elevation model with a cell size of 10 m results in fewer artefacts (centre), and with a cell size of 20 m (right), the terrain is accurately rendered albeit with stronger generalisation. This particular example neural network cannot handle elevation models with a cell size that is more than 10 times smaller than the cell size of the elevation model used for training. Some experts also criticised a shortage of details in flat areas in neural network shadings. For example, for #3 Lorraine <ref type="figure" target="#fig_0">(Fig. 14)</ref>, many experts commented that the characteristic flat-topped terrain features on the left side did not show up well and that the terrain features were not depicted with sufficient differentiation and seemed to be unconnected. We expect the reason for these shortcomings is that the manual shaded relief used for training the network did not contain similar features and was at a larger scale. In our tests, trained networks were able to successfully shade landforms that do not exist in Switzerland, such as a volcano <ref type="figure" target="#fig_0">(Fig. 14, #2</ref> Crater Lake) or high plateaus and canyons (#8 Black Canyon). However, further testing is required to ascertain whether every conceivable landform type can be depicted adequately.</p><p>Additionally, the networks are unable to locally adjust brightness to highlight specific features that are particularly relevant for a map. For example, the crater and canyon bottoms in #2 and #8 ( <ref type="figure" target="#fig_0">Fig. 14)</ref> are purposefully depicted with dark values on the manual shadings to emphasize these landforms, while the neural network shadings use a generic grey tone for all flat areas.</p><p>We initially thought very large manual shading images covering many map sheets would be needed for training the neural networks, but found that relatively small shadings were sufficient. For example, the network for creating the shading in <ref type="figure" target="#fig_8">Figure 13</ref> (centre) was trained with a shaded relief image of a single map sheet with 4,000 × 2,700 pixels. A constant tone needs to be applied evenly to all flat plains, otherwise speckles appear in the shading of flat areas; lakes and other waterbodies need to be shaded like plains or the trained network invents phantom water features; and contour lines, road networks, or other map features cannot be included in the training shading because the network will render these features at random locations.</p><p>Limiting the replicability of this research is the random initialisation of the network weights, which can influence the visual appearance of renderings. It is possible that two trainings with identical parameters and input data, but random initialisation produce networks that will create slightly different shadings. We found that He initialisation reduces this issue; however, we occasionally encountered networks that produced disturbing artefacts that disappeared when the network was trained again with a different set of random initial network weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">FUTURE WORK AND CONCLUSION</head><p>Our example renderings indicate that the neural networks learned to apply established design principles for manual cartographic relief shading including locally adjusting brightness and contrast to accentuate important terrain features. In our tests, the neural networks also successfully generalised the shadings by removing excessive terrain detail while retaining and accentuating important landform elements such as mountain ridges or valleys.</p><p>Users can adjust the contrast and appearance of the neural network shading according to the terrain type (between flat lowlands and alpine peaks) by adjusting the relative elevation range (see Section 3.2). We aim to automatically adjust the relative elevation range based on the terrain type. This would both eliminate required user input and enable the creation of detailed global neural network shadings for web maps.</p><p>We would also like to improve the rendering of flat areas and sharp ridges, which occasionally appear slightly blurred when compared to manual shading. The networks are not always successful if the cell size or terrain type differs considerably from the training data <ref type="figure" target="#fig_2">(Fig.  17)</ref>. We plan to develop neural networks that are more robust across a wider range of possible cell sizes. Recently introduced neural networks architectures, with more sophisticated connections such as stacked hourglasses <ref type="bibr" target="#b80">[81]</ref> or HRNet <ref type="bibr" target="#b81">[82]</ref> may help achieve these goals. We plan to extend our work to coloured relief shading by training networks on manual shadings that vary colour with elevation, exposition to illumination, and artistic interpretation of landforms.</p><p>With our application of deep learning to artistic relief shading, we hope to contribute to the curation of this cartographic expert knowledge and bring high-quality shading to the reach of modern mapmakers. We believe it is promising to explore the application of neural network rendering to other types of information visualisations and artistic illustration. While we optimised training and model parameters for a particular set of landforms and a specific style of cartographic relief shading, it remains to be seen how the proposed network architecture and training setup may be applied to other visualisations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 4 .</head><label>4</label><figDesc>Aerial perspective: contrast between illuminated and shaded slopes increases from lowland (left) to subalpine Jura Mountains (centre) and high alpine peaks (right, 1:500,000, © swisstopo).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 .</head><label>5</label><figDesc>Varying generalisation: sharp ridges are retained, but detail is reduced at small scales (Aletsch glacier, © swisstopo).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 7 :</head><label>7</label><figDesc>The effect of dropout and He initialisation on shading quality. Left: Local noise occurs when dropout is only applied to the inner three layers. Centre: Noise is removed when dropout is applied to all five layers. Right: Initialisation from normal distribution (instead of He, as in the centre image) results in a lack of brightness modulation on slopes, which appear as flat plains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>The U-Net architecture for inferring a shaded relief tile (right) from a digital elevation model tile (left).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Original manual shading (left) and shading for training with filled lakes and homogenised tone in flat areas (right, 1:50,000, Lake Lucerne, © swisstopo).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .</head><label>9</label><figDesc>Top-left (left) and bottom illumination (right, Sella Group, Italy).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 11 .</head><label>11</label><figDesc>Increasing generalisation by training networks with increasingly detailed terrain models (left: 200 m cell size, centre: 100 m, right: 50 m, Tyrol, Austria).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 12 .</head><label>12</label><figDesc>Increasing generalisation by reducing cell size of terrain models post-training (30 m, 60 m, 120 m from left to right). All figures are rendered with the same neural network (Valdez, Alaska [71]).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 13 .</head><label>13</label><figDesc>Standard diffuse shading (top), neural network shading (centre), and clear sky shading (bottom). Northern California, elevation model with 180 m cell size, filtered for clear sky shading.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 14 .</head><label>14</label><figDesc>Sections of the ten pairs of manual and neural network shadings for the expert evaluation. Full resolution images are included in the supplementary materials (manual shadings of #3 and #4 © Swiss Conference of Cantonal Ministers of Education (EDK), Swiss World Atlas).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 15 .</head><label>15</label><figDesc>Likert scale ratings (left), preference for a topographic map and aesthetics rating (right) by 18 experts on ten pairs of manual and neural network shadings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 16 .</head><label>16</label><figDesc>Identification of manual shadings by 18 experts (numbers as in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 17 .</head><label>17</label><figDesc>Poorly rendered flat areas (left and centre) when the rendering cell size (5 m, 10 m and 20 m from left to right, respectively) is drastically smaller than the training cell size (200 m, Massanutten Mountain, USA [71]).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>more aesthetic? neural 10 Rocky Mountain National Park, Colorado , USA</head><label></label><figDesc></figDesc><table><row><cell cols="6">Manual Neural 1 Tutuila, American Samoa Overall Rating Neural 9 Bandelier National Monument, New Mexico, USA More aesthetic Better More aesthetic Better Manual More aesthetic Neural Better 8 Black Canyon National Park, Colorado, USA Manual More aesthetic Neural Better 7 City of Rocks National Reserve, Idaho, USA Manual More aesthetic Neural Better 6 Great Basin National Park, Nevada USA Manual More aesthetic Neural Better 5 Hot Springs National Park, Arkansas, USA Manual More aesthetic Neural Better 4 Lima, Peru Manual More aesthetic Neural Better 3 Lorraine, France Manual More aesthetic Neural Better 2 Crater Lake National Park, Oregon, USA Manual More aesthetic Which is better/Manual Neural Better</cell></row><row><cell>poor fair good</cell><cell>very good</cell><cell>excellent</cell><cell cols="2">clearly manual manual</cell><cell>no preference</cell><cell>clearly neural</cell></row><row><cell cols="5">Which shading was created manually?</cell><cell></cell></row><row><cell>1</cell><cell></cell><cell></cell><cell>6</cell><cell></cell><cell></cell></row><row><cell>2</cell><cell></cell><cell></cell><cell>7</cell><cell></cell><cell></cell></row><row><cell>3</cell><cell></cell><cell></cell><cell>8</cell><cell></cell><cell></cell></row><row><cell>4</cell><cell></cell><cell></cell><cell>9</cell><cell></cell><cell></cell></row><row><cell>5</cell><cell></cell><cell></cell><cell>10</cell><cell></cell><cell></cell></row><row><cell cols="4">clearly wrong wrong uncertain</cell><cell>correct</cell><cell>clearly correct</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank the National Geographic Society for generously funding this research with an Exploration Grant. We also thank the following experts for participating in the evaluation: Jürg Gilgen </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Cartographic relief presentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Imhof</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982" />
			<publisher>De Gruyter</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The representation of topographic information on maps: The depiction of relief</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Collier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forrest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pearson</surname></persName>
		</author>
		<idno type="DOI">10.1179/000870403235002033</idno>
	</analytic>
	<monogr>
		<title level="j">Cartographic Journal</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="26" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Producing manual small-scale shaded relief</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th ICA Mountain Cartography Workshop</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Envisioning information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Tufte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Graphics Press</publisher>
			<pubPlace>Cheshire, CT</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.632</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5967" to="5976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.265</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2414" to="2423" />
		</imprint>
	</monogr>
	<note>2016-Decem</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Globally and locally consistent image completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ishikawa</surname></persName>
		</author>
		<idno type="DOI">10.1145/3072959.3073659</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automated colorization of a grayscale image with seed points propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Atiquzzaman</surname></persName>
		</author>
		<idno type="DOI">10.1109/tmm.2020.2976573</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46487-9_40</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9907</biblScope>
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4</idno>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention -MICCAI 2015: 18th International Conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
	<note>proceedings, part III</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298965</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Relief depiction: relief shading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Pearson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">History of Cartography</title>
		<editor>M. Monmonier</editor>
		<imprint>
			<publisher>University of Chicago Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1267" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-U</forename><surname>Feldmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oehrli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licht</forename><surname>Farbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Und Schatten</surname></persName>
		</author>
		<title level="m">Die Entwicklung der Reliefkartographie seit 1660. Murten: Cartographica Helvetica</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Education and cartography: cartographic textbooks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lobben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Meacham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">History of Cartography, Cartography in the Twentieth Century</title>
		<editor>M. Monmonier</editor>
		<imprint>
			<publisher>University of Chicago Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="343" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Swiss-style colour relief shading modulated by elevation and by exposure to illumination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jenny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hurni</surname></persName>
		</author>
		<idno type="DOI">10.1179/000870406X158164</idno>
	</analytic>
	<monogr>
		<title level="j">The Cartographic Journal</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="198" to="207" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Where is the sun?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="183" to="184" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Is light in pictures presumed to come from the left side?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">C</forename><surname>Mcmanus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Buckman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Woolley</surname></persName>
		</author>
		<idno type="DOI">10.1068/p5289</idno>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1421" to="1436" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Shape from shading: New perspectives from the Polo Mint stimulus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gerardin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Montalembert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mamassian</surname></persName>
		</author>
		<idno type="DOI">10.1167/7.11.13</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An empirical assessment of the impact of the light direction on the relief inversion effect in shaded relief maps : NNW is better than NW</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Biland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Çöltekin</surname></persName>
		</author>
		<idno type="DOI">10.1080/15230406.2016.1185647</idno>
	</analytic>
	<monogr>
		<title level="j">Cartography and Geographic Information Science</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="358" to="372" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Comparing the terrain reversal effect in satellite images and in shaded relief maps: an examination of the effects of color and texture on 3D shape perception from shading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Çöltekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Biland</surname></persName>
		</author>
		<idno type="DOI">10.1080/17538947.2018.1447030</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Digital Earth</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="442" to="459" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Aerial perspective for shaded relief</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jenny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Patterson</surname></persName>
		</author>
		<idno type="DOI">10.1080/15230406.2020.1813052</idno>
	</analytic>
	<monogr>
		<title level="j">Cartography and Geographic Information Science</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pondering the concept of abstraction in (illustrative) visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Isenberg</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2017.2747545</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2573" to="2588" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Shaded Relief Archive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jenny</surname></persName>
		</author>
		<ptr target="http://shadedreliefarchive.com" />
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automated reduction of visual complexity in small-scale relief shading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Leonowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jenny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hurni</surname></persName>
		</author>
		<idno type="DOI">10.3138/carto.45.1.64</idno>
	</analytic>
	<monogr>
		<title level="j">Cartographica: The International Journal for Geographic Information and Geovisualization</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="64" to="74" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Drawing color hillshade by hand</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<ptr target="https://youtu.be/" />
	</analytic>
	<monogr>
		<title level="m">North American Cartographic Society NACIS 2019</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Cartographic Perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tóth</surname></persName>
		</author>
		<idno type="DOI">10.14714/CP67.111</idno>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="19" to="28" />
		</imprint>
	</monogr>
	<note>Accidental cARTographer</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Analytical hill shading (a cartographic experiment)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yoéli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Surveying and Mapping</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="573" to="579" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The mechanisation of analytical hill shading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yoëli</surname></persName>
		</author>
		<idno type="DOI">10.1179/caj.1967.4.2.82</idno>
	</analytic>
	<monogr>
		<title level="j">The Cartographic Journal</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="82" to="88" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hill shading and the reflectance map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K P</forename><surname>Horn</surname></persName>
		</author>
		<idno type="DOI">10.1109/PROC.1981.11918</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Institute of Electrical and Electronics Engineers IEEE</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="14" to="47" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Computer-generated shaded-relief maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Batson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Eliason</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal Research U.S. Geol. Survey</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="401" to="408" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A model for automatic hill-shading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Brassel</surname></persName>
		</author>
		<idno type="DOI">10.1559/152304074784107818</idno>
	</analytic>
	<monogr>
		<title level="j">The American Cartographer</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="15" to="27" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improving the representation of major landforms in analytical relief shading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Marston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jenny</surname></persName>
		</author>
		<idno type="DOI">10.1080/13658816.2015.1009911</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Geographical Information Science</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1144" to="1165" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An interactive approach to analytical relief shading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jenny</surname></persName>
		</author>
		<idno type="DOI">10.3138/F722-0825-3142-HW05</idno>
	</analytic>
	<monogr>
		<title level="j">Cartographica: The International Journal for Geographic Information and Geovisualization</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1&amp;2</biblScope>
			<biblScope unit="page" from="67" to="75" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multidirectional, oblique-weighted, shaded-relief image of the Island of Hawaii</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mark</surname></persName>
		</author>
		<idno>92-422</idno>
	</analytic>
	<monogr>
		<title level="j">US Dept. of the Interior</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
	<note>US Geological Survey</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Three-dimensional terrain modeling with multiple-source illumination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Florinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Filippov</surname></persName>
		</author>
		<idno type="DOI">10.1111/tgis.12546</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions in GIS</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="937" to="959" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">General sky models for illuminating terrains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Kennelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Stewart</surname></persName>
		</author>
		<idno type="DOI">10.1080/13658816.2013.848985</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Geographical Information Science</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="383" to="406" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A uniform sky illumination model to enhance shading of terrain and urban areas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Kennelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Stewart</surname></persName>
		</author>
		<idno type="DOI">10.1559/152304006777323118</idno>
	</analytic>
	<monogr>
		<title level="j">Cartography and Geographic Information Science</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="36" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Changing the light azimuth in shaded relief representation by clustering aspect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Veronesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hurni</surname></persName>
		</author>
		<idno type="DOI">10.1179/1743277414Y.0000000100</idno>
	</analytic>
	<monogr>
		<title level="j">The Cartographic Journal</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="291" to="300" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Geomorphology terrain maps displaying hill-shading with curvature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Kennelly</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.geomorph.2008.05.046</idno>
	</analytic>
	<monogr>
		<title level="j">Geomorphology</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="567" to="577" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A GIS tool to increase the visual quality of relief shading by automatically changing the light direction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Veronesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hurni</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cageo.2014.10.015</idno>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Geosciences</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="121" to="127" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Comparison of relief shading techniques applied to landforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farmakis-Serebryakova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hurni</surname></persName>
		</author>
		<idno type="DOI">10.3390/ijgi9040253</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Geo-Information</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Models and experiments for adaptive computer-assisted terrain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weibel</surname></persName>
		</author>
		<idno type="DOI">10.1080/152304092783762317</idno>
	</analytic>
	<monogr>
		<title level="j">Cartography and Geographic Information Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="133" to="153" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Automated Swiss-style relief shading and rock hachuring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Geisthövel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hurni</surname></persName>
		</author>
		<idno type="DOI">10.1080/00087041.2018.1551955</idno>
	</analytic>
	<monogr>
		<title level="j">The Cartographic Journal</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="341" to="361" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Terrain Sculptor: generalizing terrain models for relief shading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Leonowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jenny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hurni</surname></persName>
		</author>
		<idno type="DOI">10.14714/CP67.114</idno>
	</analytic>
	<monogr>
		<title level="j">Cartographic Perspectives</title>
		<imprint>
			<biblScope unit="issue">67</biblScope>
			<biblScope unit="page" from="51" to="60" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Terrain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Guilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gaffuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jenny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Abstracting Geographic Information in a Data Rich World: Methodologies and Applications of Map Generalisation</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="227" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A survey of surfacebased illustrative rendering for visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lawonn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Preim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Isenberg</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.13322</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="205" to="234" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Exaggerated shading for depicting shape and detail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rusinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Decarlo</surname></persName>
		</author>
		<idno type="DOI">10.1145/1179352.1142015</idno>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2006 Papers, SIGGRAPH &apos;06</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1199" to="1205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Light warping for enhanced surface depiction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vergne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pacanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Granier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schlick</surname></persName>
		</author>
		<idno type="DOI">10.1145/1531326.1531331</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Geometry-dependent lighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varshney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="197" to="207" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The lit sphere: A model for capturing NPR shading from art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P J</forename><surname>Sloan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gooch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gooch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings -Graphics Interface</title>
		<meeting>-Graphics Interface</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="143" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">State of the art on neural rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.14022</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="701" to="727" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep CG2Real: Synthetic-to-real translation via image disentanglement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00282</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2730" to="2739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">DeepAO: Efficient screen space ambient occlusion generation via deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.1109/access.2020.2984771</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64434" to="64441" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep shading: convolutional neural networks for screen space shading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nalbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Arabadzhiyska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ritschel</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.13225</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="65" to="78" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015 -Conference Track Proceedings</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Neural style transfer: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<idno type="DOI">10.1109/tvcg.2019.2921336</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Neural style transfer: A paradigm shift for image-based artistic rendering?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Semmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Isenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Döllner</surname></persName>
		</author>
		<idno type="DOI">10.1145/3092919.3092920</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings -Non-Photorealistic Animation and Rendering</title>
		<meeting>-Non-Photorealistic Animation and Rendering</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Designing the Equal Earth physical map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Presentation at North American Cartographic Society NACIS 2019</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th Conference on Neural Information Processing Systems (NIPS 2016)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298594</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2003.819861</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46475-6_43</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision ECCV 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9906</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Cartographic reconstruction of building footprints from historical maps: A study on the Swiss Siegfried map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heitzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hurni</surname></persName>
		</author>
		<idno type="DOI">10.1111/tgis.12610</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions in GIS</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="442" to="461" />
			<date type="published" when="2020-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">56</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<ptr target="http://jmlr.org/papers/v15/srivastava14a.html" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: surpassing human-level performance on ImageNet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.123</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted Boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1123/jab.2016-0355</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on International Conference on Machine Learning</title>
		<meeting>the 27th International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Relief cartographers at swisstopo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jenny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Räber</surname></persName>
		</author>
		<ptr target="http://www.reliefshading.com/cartographers/swisstopo-cartographers/" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Swiss Map Raster</title>
		<ptr target="https://shop.swisstopo.admin.ch/en/products/maps/digital_maps/digital" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Elevation models for reproducible evaluation of terrain representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Kennelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cartography and Geographic Information Science</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Core ML: Integrate machine learning models into your app</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Apple</surname></persName>
		</author>
		<ptr target="https://developer.apple.com/documentation/coreml" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00244</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition CVPR</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2332" to="2341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">ArtGAN: Artwork synthesis with conditional categorical GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Aguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tanaka</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICIP.2017.8296985</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3760" to="3764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Combining silhouettes, surface, and volume rendering for surgery education and planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tietjen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Isenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Preim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seventh Joint Eurographics / IEEE VGTC conference on Visualization (EUROVIS&apos;05)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="303" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Real-time illustration of vascular structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dicken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Preim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Peitgen</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2006.172</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="877" to="884" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">CAN: Creative adversarial networks, generating &apos;art&apos; by learning about styles and deviating from style norms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazzone</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.07068</idno>
		<ptr target="http://arxiv.org/abs/1706.07068" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">SkyLum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Kennelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Stewart</surname></persName>
		</author>
		<ptr target="http://watkins.cs.queensu.ca/~jstewart/skyModels.zip" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Theroux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Wingert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981" />
		</imprint>
		<respStmt>
			<orgName>A Coastal zone management atlas of American Samoa. University of Hawaii Cartographic Laboratory, Department of Geography</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2020.2983686</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
