<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Retrieve-Then-Adapt: Example-based Automatic Generation for Proportion-related Infographics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyao</forename><surname>Qian</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhao</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Cui</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Guang</forename><surname>Lou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haidong</forename><surname>Zhang</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Zhang</surname></persName>
							<email>dongmeiz@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename><forename type="middle">S</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename><forename type="middle">C</forename><surname>Qian</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Retrieve-Then-Adapt: Example-based Automatic Generation for Proportion-related Infographics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">received xx xxx. 201x; accepted xx xxx. 201x.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Infographics, automatic visualization</keywords>
			</textClass>
			<abstract>
				<p>Figure 1. An example-based approach for automatic infographics generation. (a) The user information (top) and the example library (bottom) built by crawling infographics from the Internet. (b) The example (bottom) retrieved from the example library according to the query (top) generated from the user information. (c) The initial draft obtained by directly fitting user information into the design of the retrieved example. (d) The new infographic generated by adapting the design of the initial draft (e.g., enlarging the bottom text box).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Infographics <ref type="bibr" target="#b30">[31]</ref>, which often fuse text descriptions and graphic elements <ref type="bibr" target="#b27">[28]</ref> to convey the information, are widely used in advertisements, posters, magazines, etc. Compared with plain texts, infographics are obviously easier to capture viewers' attention and help them quickly understand complex information <ref type="bibr" target="#b0">[1]</ref>. However, creating a successful infographic is not an easy task which requires professional skills and tremendous time. Novice users will easily get lost in the vast amount of design choices. Even experienced designers have to go through many iterations of adjustment before proposing a final infographic <ref type="bibr" target="#b14">[15]</ref>.</p><p>To lower the authoring barrier for casual users, predefined blueprints are widely used to automate the design of infographics in both commercial software (e.g., Microsoft PowerPoint and Adobe Illustrator) and new research paradigms (e.g., Text-to-Viz <ref type="bibr" target="#b6">[7]</ref>). However, such blueprint-based approach has some limitations as well. First, as the number of predefined blueprints is limited, such approach may easily make the generated infographics limited and deja vu in terms of designs. Moreover, it is difficult to enrich the blueprint library with low efforts as designing blueprints is both complicated and laborious. For example, to build a useful template, designers need to consider numerous factors: what visual elements are allowed in the template; how they are spatially arranged; what is the size of each element; what is the font or font size; what is the ideal length for a text element in it, etc. These considerations are typically formulated as constraints in the template. Strict constraints can ensure good results, but often reduce the diversity of the results generated by the corresponding template. Loose constraints are more flexible, but can potentially yield strange or even wrong results. Therefore, designers need to thoroughly experiment and ensure the template can deliver good results with any valid input. On the other hand, there is a large collection of well-designed infographics on the Internet, called online examples for ease of reference. Currently, they are widely used as exemplars or inspirations for people who would like to create their own infographics. Compared to predefined blueprints, online examples are inherently rich in diversity and quantity. More importantly, design choices in these examples are generally appropriate and viable in practice, as they are made and endorsed by professionals.</p><p>Based on the above observations, we seek to generate infographics by automatically imitating online examples. However, there are two critical challenges when developing such an example-based approach for infographics. First, in a large collection of examples, how to find appropriate ones whose designs can be transferred to a specific piece of information. Second, even we find such a matching online example, it is unlikely that the information is a perfect fit in terms of every design aspect. For instance, the text length in the user information may be different with that in the example. To address these challenges, we propose a two-stage approach, called retrieve-then-adapt, to automatically convert a piece of information to its infographic equivalents.</p><p>In the retrieval stage, we build a retrieval system that indexes examples with their visual elements, such as charts, icons, and texts. Then, for a piece of information given by users, we need to effectively transform it to a concrete query to collect appropriate examples in the example library. Specifically, we first deconstruct the information and identify candidate visual elements that are applicable to individual information components. Then, we encode the visual elements into a valid query to retrieve examples that are composed of similar elements. Obviously, an information component may have multiple ways to present visually. Therefore, to ensure an authentic diversity that is consistent with the common practice of designers, we generate viable queries based on the distribution about visual elements observed in the crowdsourced examples. For example, if the proportion-related information is more frequently visualized using donut charts with icons than pure icons in the collected examples, our method will be more likely to issue a query about donuts with icons than pure icons, accordingly.</p><p>In the adaption stage, we address the second challenge by automatically adjusting the designs in retrieved examples to make them more suitable for user information. In most cases, the design of the retrieved example cannot be perfectly matched with user information in every aspect. Fortunately, it provides a good start point to customize and refine to achieve an appealing result. Specifically, we first directly adopt the design choices (e.g., position, color, font, etc.) from the example to create an initial draft for user information. Then, we propose a MCMC-like approach <ref type="bibr" target="#b28">[29]</ref> to iteratively make small changes to the design of the initial draft and gradually improve the visual quality over the iterations, until there is no better design can be proposed and a stable state is reached. To help evaluate the improvement after each iteration and ensure visual quality improves monotonically, we leverage recursive neural networks <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b31">32]</ref> to encode hierarchical structures of infographics into hidden vectors and build a scorer upon them to compare the visual qualities before and after a small change is applied.</p><p>To evaluate our approach, we collect a real-world dataset about proportion-related infographics from the Internet and implement our approach based on them. We present sample results to qualitatively demonstrate the performance of our approach. Moreover, we interview four experts and collect their comments for our generated infographics. Both sample results and expert reviews demonstrate that our approach can generate diverse infographics by imitating online examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Infographics</head><p>Research work on infographics mainly includes three aspects, i.e., effect, understanding and auto-generation. First, traditional studies focus on exploring what effects infographics have on humans. For example, Bateman et al. <ref type="bibr" target="#b2">[3]</ref> compare embellished charts with plain ones by measuring interpretation accuracy and long-term recall. Haroz et al. <ref type="bibr" target="#b10">[11]</ref> find that using pictographs can improve the performance of information exchanging in terms of memorability, reading speed, and engagement. In addition, many studies are conducted to understand the underlying message or the visual structure of an infographic. For example, Bylinskii et al. <ref type="bibr" target="#b3">[4]</ref> adopt OCR techniques to assign hashtags to infographics for information retrieval purpose. Lu et al. <ref type="bibr" target="#b24">[25]</ref> find 12 different Visual Information Flow (VIF) patterns in infographics.</p><p>Recently, there has been a growing interest in generating infographics. Many design tools <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35]</ref> have been developed to facilitate the creation of infographics in an interactive manner. However, these tools cannot completely automate the creation process. Users are still required to understand advanced operations and concepts in these tools, and use their design expertise to make choices among numerous visual elements and attributes. To further automate the process, Cui et al. <ref type="bibr" target="#b6">[7]</ref> propose generating infographics from natural language statements with predefined blueprints. However, predefined blueprints are expensive to create and easily homogenize designs. These limitations of predefined blueprints motivate us to consider leveraging online examples, which are easy to get and inherently diverse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Reusing Examples</head><p>Examples play an important role in design practice. On the one hand, experienced designers usually get inspirations from examples because they can offer rich design styles <ref type="bibr" target="#b14">[15]</ref>: color schemes, visual expression, etc. On the other hand, for novice users, it is far easier to adjust an existing example than to create a design from scratch <ref type="bibr" target="#b19">[20]</ref>. Such critical role of examples inspires researchers to investigate reusing examples to create new designs in many different tasks, such as web pages design and charts design. In web page design, Bricolage <ref type="bibr" target="#b18">[19]</ref> tries to match visually and semantically similar page elements between the content source and a web example, and then transfers the content of the source page to the best matching element in the example. Different from Bricolage that only considers one page, WebCrystal <ref type="bibr" target="#b4">[5]</ref> extracts and combines styling information from different existing websites. In chart design, much attention has been paid to converting existing charts into reusable style templates that can be easily applied to new data sources. Harper et al. <ref type="bibr" target="#b11">[12]</ref> present a pair of tools for deconstructing and restyling existing D3 visualizations. The deconstruction tool analyzes a D3 visualization by extracting the data, marks, and mappings between them. The restyling tool lets users modify the visual attributes of marks as well as the mappings from data to these attributes. The deconstruction approach is further extended to take into account non data-encoding marks and their visual attributes <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>All these existing approaches focus on how to reuse designs from existing examples. Our approach also falls into this category. However, we focus on a different type of data: infographics. The unique design space of infographics proposes two critical challenges besides design extraction, i.e., how to find appropriate examples for a specific piece of information and how to adapt the example's design to make it more suitable for the information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Automatic Layout Generation</head><p>Our adaption stage is inspired by existing automatic layout generation methods for traditional tasks, e.g., graphics design and indoor scene generation. Early approaches <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29]</ref> investigate Markov Chain Monte Carlo (MCMC) methods to generate layouts, which iteratively propose new layout and choose the better layout until no better layout can be proposed. The performance of those approaches is sensitive to layout evaluation metrics. However, these evaluation metrics are usually task-specific and heavily rely on human intuition, limiting them in modeling infographic design. Recently, deep neural networks have been explored for layout generations <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b37">38]</ref>. Those methods regard layout generation as a special case of image generation, and then leverage generative adversarial networks (GAN) <ref type="bibr" target="#b9">[10]</ref> or variational auto-encoder (VAE) <ref type="bibr" target="#b7">[8]</ref> to solve it. For example, LayoutGAN <ref type="bibr" target="#b21">[22]</ref> proposes a wireframe rendering layer to learn the alignment; READ <ref type="bibr" target="#b8">[9]</ref> leverages a recursive autoencoder to model highly structured layouts using less training data; ContentGAN <ref type="bibr" target="#b37">[38]</ref> synthesizes layouts by considering visual and textual semantics. Although those methods do not rely on handcrafted features, they often fail to achieve comparable performance as MCMC methods due to the limited training data.</p><p>The adaption stage in this work is similar to MCMC methods. However, instead of handcrafting evaluation metrics, we learn a recursive neural network from the example library about how to evaluate designs. … … <ref type="figure">Figure 2</ref>. Example for our setting of infographics generation, where the input is a proportion-related natural language statement (see (a)) and the output is an infographic representing the information (see (c)). In this setting, (a) the input is first analyzed to obtain (b) various candidates of visual elements, which are then used to synthesize the infographic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES</head><p>Our approach is highly related to Text-to-Vis <ref type="bibr" target="#b6">[7]</ref>. In that work, <ref type="bibr">Cui et al.</ref> first analyze and describe the text and visual spaces of proportionrelated facts. Then, a blueprint-based solution is built to bridge these two spaces, which automatically converts a proportion-related natural language statement to its infographic equivalents. According to their study, the motivation behind this input/output setting is two-fold. First, the natural language is the most common way to convey information. Second, the proportion-related infographic is the main category of infographics used in practice.</p><p>In this work, we adopt the same input/output setting of infographic generation ( <ref type="figure">Figure 2</ref>), but on top of which we build our novel examplebased approach. In addition to the input/output setting, we also adopt the input processing pipeline (i.e., (a) to (b) in <ref type="figure">Figure 2</ref>) to help prepare queries for examples. In this section, we briefly introduce these adopted algorithms and concepts that are used in the remainder of this paper. More details can be found in the work of Text-to-Vis <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Input Processing</head><p>In this work, we follow the same input processing pipeline of Text-to-Vis to collect candidates of visual elements, since this module is not the core interest of this work and existing algorithms can already provide sufficient information to support our example-based approach.</p><p>Specifically, given a input statement that contains proportion-related information (e.g., <ref type="figure">Figure 2</ref>(a)), a text analyzer, essentially a supervised CNN+CRF model, is utilized to split the statement into four different segments, namely, before, modifier, number, and after. In addition, two separate graphic generators are used to generate graphical elements. Specifically, an icon generator is used to extract representative icons according to the semantics of the input statement from a predefined icon library and a chart generator is used to generate pies, donuts, and bars according to the percentage value in the input statement. All these text segments and graphics are candidate visual elements for information components in the input statement, and then will be used to construct viable queries for retrieving compatible examples in the example library (see Section 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Visual Elements</head><p>Following the visual space described in Text-to-Vis <ref type="bibr" target="#b6">[7]</ref>, we rank and select the top ten frequently used visual elements in online examples.</p><p>Textual Elements. They are characterized by the contained information, which correspond with segments extracted by the text analyzer.</p><p>• Before. This refers to simple clauses in forms like "It is estimated that", "Compared with last month" or "By the year 2020,". They always appear at the beginning of the statement. • Modifier. This refers to adjunct words or phrases used before a number, such as "only", "more than", "around" or "nearly". • Number. This refers to the most important numerical information in a proportion-related fact. They have obvious textual patterns like "10%", "1 in 10" ,"1 out of 10" or "1/10". • After. This refers to the remaining texts after the number. For example, ". . . of companies will be using artificial intelligence for ONLY</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>20%</head><p>of retail products are purchased through direct sales reps</p><p>By the year 2020,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>60%</head><p>of companies will be using artificial intelligence for driving digital revenue. driving digital revenue" or ". . . of people would like to receive promotions on social media". • Statement. An input utterance may exist in the infographic as one text block sometimes. We consider the text block as a statement element because it dose not split the input utterance into meaningful segments. Graphical Elements. They are also critical parts in infographics, targeting attractive visual effects to emphasize the key points in the underlying information.</p><p>• Single icon. Elements of this type are semantically related to the input utterance. Since icons are not explicitly provide in the input, we use the aforementioned icon generator ( <ref type="figure">Figure 2</ref>) to collect meaningful icons to represent the input. • Donut/Pie/Bar. Elements of these types encode the numerical information of the input in a vivid way. In our implementation, they are programmatically generated according to the numerical information by chart generator <ref type="figure">(Figure 2</ref>). • Pictograph. Compared with the previous four elements, pictographs not only encode numerical information, but also emphasize the subject of the proportion-related fact. In our implementation, they are created with icon generator as well. Attributes of these textual elements and graphical elements are manually labeled in examples, and then used to index examples in the example library (see Section 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SYSTEM OVERVIEW</head><p>Formally, we denote the example library as D = {D m } M m=1 , which contains M infographics D m crawled from the Internet. The input is denoted by U, which is a proportion-related natural language statement like "More than 74% of users are female". Thus, our approach aims to automatically generate an infographic G from U, by imitating designs in D. <ref type="figure">Figure 1</ref> illustrates the workflow of our approach, which contains three key steps as follows.</p><p>Retrieval. The first stage is to find appropriate examples whose designs can be transferred to the input U. We address this challenge by building a retrieval system that takes U as the query to look up D and returns an appropriate exampleD ∈ D, i.e.,D = f retrieval (D,U). Specifically, we index each example by its visual elements, e.g., charts, icons, and texts. Then, we transform an input U to a concrete query in the same format of the example index. Finally, we retrieve examples based on the similarity between example indexes and queries <ref type="figure">(Figure 1(b)</ref>).</p><p>However, in many cases, U has multiple valid infographic designs. For example, it can be visualized either as statement+bar or state-ment+pictograph, which are different in terms of queries. To resolve this one-to-many mapping issue, we generate effective queries based on the distribution observed in the crowdsourced examples. Intuitively, we assume the example library indicates the likelihood of different design choices that are adopted in practice. This motivates us to generate queries by first learning a distribution about visual elements from the example library and then sampling from the distribution. In this way, we ensure that examples are retrieved based on the popularity of their design choices in the real world. Moreover, by sampling from the learned distribution multiple times, it is possible to obtain different design choices (i.e., different concrete queries for the "elements": "type": "Canvas", "built attribute": "position": "color": ["#FFFFFFFF "], "text attribute": { "Font": " ": " ":</p><p>{"type": "Single icon", "built attribute": "position": "color": ["#FF7F00", "#855E42"], "text attribute": { "Font": None, " ": " ":</p><p>{"type": "Number", "built attribute": "position": "color": ["#B873333"], "text attribute": {"Font": Font1, " ": " ":</p><p>: "type": "Canvas", "built attribute": "position": "color": ["#FFFFFFFF "], "text attribute": { "Font": " ": " ":</p><p>{"type": "Single icon", "built attribute": "position": "color": ["#FF7F00", "#855E42"], "text attribute": { "Font": None, " ": " ":</p><p>{"type": "Number", "built attribute": "position": "color": ["#B873333"], "text attribute": {"Font": Font1, " ": " ": … <ref type="figure">Figure 4</ref>. Illustration for the pipeline of example library construction.</p><p>same U), leading to improved diversity of generated infographics. <ref type="figure">Figure</ref> 1(b) shows a concrete query which contains visual elements "num-ber+after+donut+single icon" and the corresponding retrieved example. Initialization. We generate an initial draftG by directly applying the design of the retrieved exampleD to U, i.e.,G = f init D ,U . We apply positions, colors, and text-specified attributes to the visual elements specified by the query <ref type="figure">(Figure 1(c)</ref>). If the retrieval stage returns multiple examples, we generate one initial draft for each example.</p><p>Adaption. In many cases, directly applying the design may not work perfectly due to the mismatches between visual elements in the retrieved example and those used in the query. For example, text lengths or aspect ratios of icons may be slightly different, which may lead to a strange look inG, such as overlapping, excessive white space or improper font size <ref type="figure">(Figure 1</ref>(c)). We propose an adaption stage to refineG and make it more natural and suitable for the input U, i.e., G = f adaption G . Specifically, we propose a MCMC-like approach to adjust the design of the initial draftG iteratively. First, a candidate design is proposed by randomly making a small change to the position or size of a visual element in the current design. Then, we compare the visual qualities of the current and candidate designs and choose the better one as the starting point for the next round of iteration. However, evaluating designs is a nontrivial task as the criteria for a good design is intricate and hard to be quantified <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b25">26]</ref>. To address this problem, we train a recursive neural network from the example library to evaluate the change in the aforementioned iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXAMPLE LIBRARY CONSTRUCTION</head><p>The foundation of our work is a collection of online examples, which serves three purposes. First, we extract the distribution about visual elements from examples. Since there are no design preferences indicated in the input statement, we use the distribution to guide the query generation, so that the generated results have an authentic diversity consistent with real world examples. Second, we index these examples in a database to query and construct the initial draft for a given input. Third, we use the example corpus to train a recursive neural network that is used in the adaption stage to evaluate the change at every iteration.</p><p>In this work, we build an example library by crawling and processing infographic exemplars from the Internet <ref type="figure">(Figure 4</ref>). First, we collect a set of infographic sheets from the Internet. Then, an infographic sheet usually contains multiple infographic units <ref type="figure">(Figure 4(a)</ref>). Therefore, we crop the proportion-related units from each sheet. Finally, we label the visual elements and their attributes in the cropped infographics.</p><p>Collection. We search Google Image by using a primary keyword "infographic", as well as a secondary keyword indicating a topic to ensure the diversity of dataset. Specifically, we consider ten common topics, i.e., education, health, commerce, ecology, diet, sports, animals, wedding, technology, and medical. In total, we downloaded 1000 infographic sheets with 100 under each topic.</p><p>Cropping. Proportion-related infographics convey statistical information about how much a part occupies the whole, e.g., "More than 74% of users are female", which provides obvious indications for us to crop them from the downloaded sheets. Specifically, three coauthors review all the original sheets and crop proportion-related infographics from them independently. In total, we obtain 829 examples. Labeling. These examples are all stored as bitmap images. For each visual element, we manually label the following visual attributes:</p><p>• Element type t, which can be one of the visual elements introduced in Section 3.2, including before, modifier, number, after, statement, single icon, donut, pie, bar and pictograph. • Built-in attribute b, which is the length of characters for a textual element and the aspect ratio for a graphical element.</p><formula xml:id="formula_0">• Position g = (x l , y l , x r , y r )</formula><p>, which is the top-left coordinate and bottom-right coordinate for the element boundary. • Color c, where we label at most two colors for an element. Specifically, we label one outline color and one fill color for each icon to ensure that its style can be matched when new infographics are generated. Besides, we label the dominant font color when there are multiple font colors in one text box. • Text-specific attributes a, which includes the font and whether there is an italic and bold effect in the textual element 1 . Specifically, we label the dominant font type and font effect (e.g., bold) when there are multiple ones in one text box. We treat canvas as a special graphical element and label above attributes for it. Although there are other subtle visual attributes, we leave them for future work as essential visual attributes described above can already help us generate good infographics in many cases. We think there are several possible ways to effectively enrich visual attributes. For example, labeling process is quite simple and does not require design expertise, and thus it is possible to accelerate it by crowdsourcing. On the other hand, with the growth of computer vision techniques, current manual labeling process can be replaced by machines <ref type="bibr" target="#b5">[6]</ref>, which will make the example library construction even more scalable and effective.</p><p>Copyright is another big concern of this approach, as it is a violation of intellectual property rights to fully imitate an example without owning appropriate copyrights. To resolve this issue, we leverage two datasets in our implementation. One dataset of 50 infographics is created by our designers, which is used to retrieve examples for imitation. The other dataset of 829 infographics is collected from the Internet and used in the non-copyright related parts of our approach, e.g., extracting the distribution of visual elements and training a recursive neural network. The copyright issue is further discussed in Section 10.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RETRIEVAL</head><p>On top of the example library, we build a retrieval system to help find appropriate examples whose designs can be potentially adapted to the input statement given by the user. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Example Index</head><p>Intuitively, if the input U can be represented by an icon and a text statement, examples that exactly have one icon and one text box are more likely to be candidates for reuse. Moreover, if the text (or icon) of the input has a similar length (or aspect ratio) to that in an example, the design of this example will be ideal for reuse. This motivates us to index examples by visual element types t and their built-in attributes b, including the number of characters for textual elements and the aspect ratio for graphical elements. Formally, for each example D m ∈ D, its index I m can be denoted as</p><formula xml:id="formula_1">I m = {(t n m , b n m )} N n=1</formula><p>, where N is the number of visual elements in the example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Query Generation</head><p>As I m is composed of visual elements types and built-in attributes, we need to transform the input U to concrete queries of the same form. On the one hand, there are usually many valid ways to represent the same piece of information using infographics. For example, "More than 74% of users are female" can be visualized as statement + pie or statement+donut+icon. On the other hand, the example library indicates the likelihood of different representations adopted in the practice. Therefore, we propose to first learn a distribution about visual elements from the example library and then generate user queries by sampling from the distribution.</p><p>Learning distribution about visual elements. <ref type="figure" target="#fig_3">Figure 6</ref> shows design choices about visual elements. First, an infographic usually contains two main categories of visual elements, i.e., textual elements and graphical elements. Then, there are many design choices under these two categories (filled rectangles in <ref type="figure" target="#fig_3">Figure 6</ref>). For textual elements, there are two design choices, i.e., regarding the input statement as a whole and splitting the input statement into segments by semantics, denoted by statement and semantic segments in <ref type="figure" target="#fig_3">Figure 6</ref>. Note that semantic segments can be expanded to concrete visual elements, which may further yield valid combinations, e.g., number+after and before+number+after. For graphical elements, there are also multiple choices among the combinations over icon, bar, pie, donut and pictograph, e.g., bar+icon and donut+icon. Formally, for a given design choice about visual elements (u 1 ,...,u K ), we calculate its occurrence probability by</p><formula xml:id="formula_2">p(u 1 ,...,u K ) = #(u1,...,uK ) M</formula><p>, where # stands for the occurrences of (u 1 ,...,u K ) in D of size M, 1 ≤ K ≤ 7, and u k ∈ {statement, semantic segments, icon, bar, pie, donut, pictograph}.</p><p>Generating user queries. To ensure the diversity of generated infographics, for a given input U, we generate M queries by following steps. First, we sample M different design choices from p, where p is the learned distribution of design choices about visual elements. Second, we specify design choices with concrete visual elements and then get their built-in attributes. Specifically, if semantic segments exists in a query, we expand it to the corresponding visual elements, e.g., before, modifier, number and after (Section 3). If icon or pictograph exists in a query, we leverage the result of icon generator, which extracts representative icons according to semantics of the input statement. If pie, donut or bar exists in a query, we leverage the result of chart generator, which generates a specific chart according to the percentage in the input statement. Finally, we generate a set of queries</p><formula xml:id="formula_3">Q = {Q m } M m =1</formula><p>where each user query corresponds to one design choice about visual elements. Formally, we encode the user query Q m by visual element typest n m and built-in attributesb n m , i.e.,</p><formula xml:id="formula_4">Q m = {(t n m ,b n m )} N n =1</formula><p>, where N is the number of visual elements in the query. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Retrieving Strategy</head><p>We retrieve examples by comparing the distance between example indexes and user queries. Specifically, for each query Q m ∈ Q, we extract one exampleD ∈ D whose example index has the smallest distance to the query. This will result in a set of retrieved examples with size M . Concretely, for a given example index I m and a user query Q m , we first match their elements by type. Then, we calculate the distance between them by considering whether they have similar visual elements types and built-in attributes,</p><formula xml:id="formula_5">S (I m , Q m ) = ∑ n s n t n m , b n m ,t n m ,b n m , where s n = ⎧ ⎨ ⎩ 1, if t n m =t n m , |b n m −b n m | b n m , otherwise.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">INITIALIZATION</head><p>In the previous stage, we transform the input to a set of queries and retrieve one example for each query. In this section, for each retrieved example, we generate one initial draft by reusing its design.</p><p>Position. For a graphical element, the aspect ratio specified in the query is usually not the same as that used in the example. Therefore, if we directly apply positions (i.e., the top-left and bottom-right coordinates) of the one in the retrieved example, the new graphical element in the initial result will be easily distorted <ref type="figure" target="#fig_4">(Figure 7(b)</ref>). To avoid such distortion, we reuse the top-left coordinate used in the retrieved example, and recalculate the bottom-right coordinate by uniformly scaling the new graphical element to fit the space occupied by the old one <ref type="figure" target="#fig_4">(Figure 7(c)</ref>). For a textual element, the number of characters used in the user query is usually not the same as that used in the example either. If we do not recalculate the font size, there will easily be truncated text or large white space in the text box <ref type="figure" target="#fig_4">(Figure 7(b)</ref>). To tackle this problem, we recalculate the font size by making the font size as large as possible while maintaining the text information complete <ref type="figure" target="#fig_4">(Figure 7(c)</ref>).</p><p>Color/Text-Specified Attributes. For the other extracted design choices, e.g., color and font type, we directly reuse them for the visual elements specified by the corresponding query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">ADAPTION</head><p>In the previous stage, we generate an initial draft by reusing the design of the retrieved example. While the initial draft roughly conforms to design principles, spatial relationships between visual elements are not good enough in many cases. This is mainly caused by that builtin attributes in the query are often slightly different from that in the retrieved example. For instance, in <ref type="figure" target="#fig_4">Figure 7</ref>(c), there is an excessive white space to the right side of the icon. In addition, when the text of the query is placed in the same text box of the retrieved example, the font size is too small to read. Therefore, to ensure the quality of generated infographics, it is necessary to adjust spatial relationships between visual elements in the initial draft. In this section, we introduce an adaption stage, which is a MCMC-like approach, to solve this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">MCMC-Like Approach</head><p>As shown in <ref type="figure" target="#fig_4">Figure 7</ref>, despite imperfectness, the initial draft provides a good start point in the complex and huge design space. Intuitively, if we search around the initial draft in the design space, there stands a good chance of finding a better design. Actually, this simple intuition is consistent with the core concept of Markov chain Monte Carlo (MCMC) methods used in traditional tasks of layout generation (e.g., graphics design or indoor scene generation <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b36">37]</ref>). For these tasks, … <ref type="figure">Figure 8</ref>. An illustrative example for our MCMC-like approach. (a) The initial draft. (b) A candidate design proposed by enlarging the bottom text box in (a). It is accepted as the start point for the next round of proposal. (c) A candidate design proposed by moving the icon in (b). It will be accepted with a low acceptance probability as it causes incorrect overlap. If it is not accepted, the design in (b) will continue to be used as the start point for the next round of proposal.</p><p>MCMC methods start from a random layout, then iteratively propose a candidate layout according to a prior proposal distribution and accept the candidate with a certain probability. After Markov chain converges, an ideal layout that obeys the target distribution is naturally achieved.</p><p>Inspired by MCMC methods, we propose a MCMC-like approach to adjust spatial relationships in the initial draft by following three steps, where the last two steps are executed iteratively until there is no better proposal. <ref type="figure">Figure 8</ref> illustrates an example for our MCMC-like approach.</p><p>Set a Start Point. As the previous stage has generated an initial draft roughly conforming to design principles, we take the initial draft as the start point instead of randomly sampling one <ref type="figure">(Figure 8(a)</ref>).</p><p>Propose a Candidate Design. To adjust spatial relationships effectively, we propose candidate designs by making a small change to the current result (the initial draft for the first iteration). Spatial relationships often relate to two factors, i.e., positions and sizes of visual elements. Therefore, we design two simple dynamics corresponding to these two factors, i.e., position modification and size modification, and utilize them randomly to propose candidate designs.</p><p>• Position modification. This dynamic randomly chooses a visual element, and proposes a new positiong by adding a random variable δ g to the current position g (i.e.,g ← g + δ g) of the element, where δ g follows a bivariate normal distribution. • Size modification. This dynamic randomly chooses a visual element, samples a scale from a normal distribution, and uses the scale to resize the corresponding visual element. Compare Candidate and Current Designs. Once we propose a candidate design, we must determine whether to accept it for the next iteration. We adopt a similar mechanism to Metropolis-Hasting algorithm, where the candidate design is accepted with an acceptance probability α = min{1, d </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Recursive Neural Networks for Evaluation</head><p>The acceptance probability α relates to the score d (•), which reflects the quality of spatial relationships in a design. Typical MCMC methods use hand-crafted energy functions to evaluate spatial relationships in a layout. However, it is difficult to comprehensively and accurately represent complex spatial relationships in infographics by hand-crafted energy functions. To tackle this problem, we learn from the example library about spatial relationships evaluation. To learn such a mapping, we have to address two challenges. First, we should construct a training set with effective labels from the example library. The challenge is that the example library only contains labels for visual elements and attributes while our task requires labels reflecting the quality of spatial relationships in a design. Second, we should learn a good feature representation for spatial relationships in infographics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.1">Training Set Construction</head><p>As examples in our library are created by professionals, we naturally hypothesize that they have good spatial relationships. Besides, we assume that, if we make random perturbations on these examples, resulting designs have worse spatial relationships than original ones. The larger the perturbation is, the worse the spatial relationships are.</p><p>This provides us a good opportunity to construct training set effectively. Specifically, for each example D ∈ D, we generate a set of new designsD j ( j ≥ 1) through randomly choosing one visual elements and randomly changing the position or size of the labeled bounding boxes. Then, we construct training data by following two patterns.</p><p>• Take a pair of the original design and a perturbed design (i.e., D,D j or D j , D ) as the input, and (1, 0) or (0, 1) as the label. Here, original designs serve as good examples. This type of training data will help our model learn to differentiate ideal and bad spatial relationships, which are more common in late iterations.</p><p>• Take a pair of perturbed designs with different perturbation degrees (i.e., D i ,D j or D j ,D i ) as the input, and take (1, 0) or (0, 1) as the label. Here, the design with smaller perturbation degree is denoted asD i and serves as the good example. This type of training data will help our model learn to pick a better design when neither of them has ideal spatial relationships, which are more common in early iterations. In our implementation, we generate 20000 pairs for training and 2000 pairs for validation by following the above two patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.2">Model Architecture</head><p>The spatial relationships of infographics are inherently hierarchical. Taking <ref type="figure" target="#fig_7">Figure 9</ref>(a) as an example, its spatial structure can be represented by a tree, where the text box for number and the single icon are horizontally arranged and then these two elements and the text box for after are vertically arranged. On the other hand, recursive neural networks is a widely adopted and the state-of-the-art model to encode such hierarchical structures in existing works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23]</ref>. Therefore, we also leverage recursive neural networks, instead of convolutional neural networks <ref type="bibr" target="#b17">[18]</ref>, in our approach. <ref type="figure" target="#fig_7">Figure 9</ref> illustrates the model architecture. First, the tree builder transforms an infographic into a hierarchical tree in a top-down fashion. Then, the feature extractor, which is a recursive neural network, encodes the infographic into hidden vectors according to the hierarchical tree in a bottom-up manner. Finally, the scorer built upon those hidden vectors computes scores for spatial relationships in the infographic.</p><p>Tree Builder. It takes an infographic as input and outputs a hierarchical tree. Initially, the root node contains all the visual elements in the infographic. Then, we recursively split tree nodes until there is no node can be split according to following criteria.</p><p>• Horizontal divisibility. It means there exists a horizontal line that can divide visual elements in the current node into two groups. The current node is marked as a horizontal node and split into two child nodes, each of which contains a group of visual elements. If there is no such cutting line, we turn to the next criterion. • Vertical divisibility. Similar to the previous criterion, it means there exists a vertical cutting line that can divide visual elements in the current node into two groups. The current node is marked as vertical node and is split into two child nodes. If this criterion does not apply either, we turn to the last criterion. • Indivisibility. If there neither exists a vertical cutting line nor a horizontal cutting line, we mark this node as an indivisible node. Feature Extractor. It is a recursive neural network that encodes an infographic into hidden vectors according to the hierarchical tree.</p><p>To better encode different spatial relationships, we use four distinct layers in our recursive neural network, including box layer, horizontal layer, vertical layer and overlap layer. These layers are organized according to the hierarchical tree in a bottom-up manner <ref type="figure" target="#fig_7">(Figure 9(c)</ref>). Each of them is a two-layer perceptron although their weights are different. Inputs for these layers are as following:</p><p>• Box layer. It encodes the status of the leaf node in the hierarchical tree, i.e., the visual element. The input includes: 1) the element type, a one-hot encoding, 2) the position, a vector representing the top-left and bottom-right coordinates, and 3) the built-in attribute, a scalar indicating the aspect ratio or text length. • Horizontal/vertical/overlap layers. They encode horizontal, vertical, and overlap relationships between different nodes in the hierarchical tree, respectively. The input includes: 1) node representations for two child nodes, 2) the relation representation between two child nodes, which is a vector representing the offset of the right child relative to the left child, and 3) the cut ratio, a scalar indicating the position of the cutting line (only for the horizontal and vertical layer). Scorer. After the feature extractor, we get hidden vectors representing spatial relationships of the current design D and the candidate design D , respectively. Then, we concatenate them and feed them into the scorer to get the score for spatial relationships of the infographic. Specifically, the scorer consists of a fully connected hidden layer and a Softmax layer, and is jointly trained with the aforementioned feature extractor by minimizing the cross-entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">EVALUATION</head><p>We implement a prototype system targeting widely-used proportionrelated infographics. To demonstrate the system, we use 30 descriptions with average length of 74. We randomly pick 5 descriptions and generate 5 infographics for each of them. For the remaining descriptions, we generate 1 infographic for each of them. In this way, we generate 50 infographics in total and show them to the experts.</p><p>Our experiments are run on a CPU Windows server (Intel Xeon E5 2.6GHz). During the adaption, each initial draft is adjusted for 1000 iterations, taking 3.5s on average. Note that the adaption process is efficient because the initial draft provides a good starting point and only the inference step of the recursive neural network is used. <ref type="figure">Figure 10</ref> shows several typical samples from the 50 results. Note that if there is a pictograph, we will generate multiple candidate infographics by traversing the number of icons in the pictograph from 3 to 10, and then leverage the model introduced in Section 8.2 to choose the one that achieves the best score. For pies, donuts, and bars, we render them by referring to the percentage provided in the input statement and reusing the annotated positions and colors of the retrieved example. <ref type="figure">Figures 10(a)-(d)</ref> are all generated from the same input utterance "1 out of 3 patients have used a portal to connect with doctors." by using different queries. Specifically, we sample four different queries, including donut+single icon+number+after <ref type="figure">(Figure 10(a)</ref>), pictograph+number+after <ref type="figure">(Figure 10(b)</ref>), pie+single icon+statement <ref type="figure">(Figure 10(c)</ref>) and single icon+number+after <ref type="figure">(Figure 10(d)</ref>). We can observe that these generated infographics have different styles in terms of the choices for visual elements, the layout, the color and the font.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Sample Infographics</head><p>Figures 10(e)-(j) demonstrate that our approach provides great flexibility and diversity in dealing with different inputs. First, our approach can generate both concise and complex designs due to the natural diversity of examples. For example, <ref type="figure">Figure 10</ref>(e) concisely represents information in a simple up-down structure, while <ref type="figure">Figure 10</ref>(g)-(j) represent information by multiple text blocks and graphic elements in a complicated structure. Moreover, our approach is capable of highlighting the important information in various ways. For example, in <ref type="figure">Figure 10</ref> (e)-(j), number elements are set the biggest font size to attract visual attention, while in <ref type="figure">Figure 10</ref>(g)-(j), icons and charts are used simultaneously to emphasize the semantic meaning and numerical information. Furthermore, our approach can create different design styles by imitating color schemes of different examples. For example, although element types are almost the same in <ref type="figure">Figure 10</ref>(e) and (f), the former one shows a sense of mature and calm while the latter is more lively and vigorous. In addition, even some results have the same visual element combinations (e.g., <ref type="figure">Figure 10(a)</ref>, (g) and (h) all have donut+icon+number+after), they can have totally different layouts and colors if they are generated by imitating different examples. <ref type="figure">Figures 10(k)</ref>-(m) show generated compositional infographics with multiple proportional facts by imitating corresponding examples. To generate them, we run our approach twice. The first round generates individual proportion-related infographics by adapting an example retrieved according to element types and built-in attributes. The second round generates the compositional results by adapting an compositional example retrieved based on the number and aspect ratio of the individual infographics generated in the first round. These compositional infographics can facilitate effective comparison among multiple proportional facts and provide a richer representation.</p><p>Figures 10(n)-(o) compare the retrieved example, the initial draft generated by directly applying the retrieved example's design, and the final infographic after adaption. We observe that the initial draft roughly conforms to design principles while spatial relationships between visual elements are not good enough sometimes. For example, in <ref type="figure">Figure 10</ref>(n), the font size is a little small while in <ref type="figure">Figure 10</ref>(o), there is an excessive white space between the single icon and the after. After adaption, the spatial relationships in the initial draft are greatly improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Expert Review</head><p>To understand the effectiveness and usability of our method, we conducted an evaluation study with 4 designers. All of them have graduated (E1, E4) from or were enrolled (E2, E3) in professional schools of design disciplines. E1 and E4 are professional designers in a technology company. Both of them have around ten years of design experience in user interface, graphics, and video. E2 and E3 are senior graduate students and have been interns in design positions. Both of them have more than two years of design experience.</p><p>For each designer, we conduct a 60-minute interview. First, we use 10 minutes to introduce the background of this work and system workflow. Since our approach does not involve much interactions, we do not ask the designers to try out our system. Instead, we show them sample infographics generated by our approach, each containing a proportion-related natural language statement, a retrieved example, and a generated infographic. The statement and generated infographics are the input and output of our system. In particular, we show the designers the retrieved example side-by-side with the generated infographic, since we like to know their opinions about the resemblance between them. After that, a semi-structured interview was conducted to understand professional opinions on the usability and quality of our approach.</p><p>Overall, designers were very impressed by the convenience and intelligence that our approach provides to create infographics. For example, E1 mentioned that "The capability of automatically imitating existing online examples is really smart and amazing, because it is quite time-consuming for me to do it manually.", and E2 said that "By intelligently imitating online examples, this approach enables so many potential ways of presenting the user information, which cannot be finished by me in a short period. It does a great job."</p><p>Regarding how much assistance our approach can provide to users in real-world scenarios, designers thought at least two types of people would benefit from it. One is casual users with limited design expertise. From casual users' perspective, infographics are mainly used to enrich their reports or dashboards in informal communication. Designers believed that our generated results were good enough for them. For example, E3 mentioned that "The generated infographics can fully express the original information and can be used directly by casual users." </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>60%</head><p>of that data is filtered out before it reaches the brain <ref type="bibr">1 5</ref> students in private colleges and universities can hardly afford to complete college.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>83%</head><p>of offline consumers stated that rewards on offer are very important when they choose a current account or credit card of working women say they are concerned about losing their jobs before, during, and even after pregnancy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OVER</head><p>of the researchers in sciences who use the Internet agreed that Internet use had a positive impact on their study and research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Around</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Around</head><p>of the researchers in sciences who use the Internet agreed that Internet use had a positive impact on their study and research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">in 5</head><p>people live in rural areas of university students will buy new books.  <ref type="figure">Figure 10</ref>. (a)-(j) and (k)-(m) shows infographics with single and multiple proportional facts respectively. (n)-(o) shows a triple, including the retrieved example, the initial draft generated by directly applying the retrieved example's design and the final infographic after the adaption. For better visual appearance, embellished shapes (in (i) and (l)) and the color transparency of the icon (in (c) and (g)) are added by post-editing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>54%</head><p>E4 said "Compared to the uniqueness and creativity of the design, casual users usually prefer to follow popular designs, which is exactly what this approach provides." The other one is professional designers themselves. While our approach initially targets casual users, all the designers express the appreciation for the help our approach could provide for them as well. As admitted by all the designers, they had to immerse themselves with myriad examples to get inspirations during the design process. They usually login to multiple design websites and manually check which design satisfies their requirements. First, experts really appreciated the capability of automatically retrieving the examples which meet user requirements by our approach. "When I propose a design to a client, if no existing ones are referenced and attached, the proposal will be rejected with high probability. I have to manually search the Internet to find references. I think the retrieval stage in this approach can save my time on finding intent references", said one of the designers. E3 said, "Sometimes I even do not have clear thought about my designs. This approach can provide me various previews about different designs on my data. It is really informative." Moreover, designers thought the adaption stage by itself was very helpful. For example, E2 commented, "When I want to reuse some designs, I have to manually adjust visual elements to make it suitable for my data. It is amazing that the adaption stage can automatically do it. I think it would save much time if I could make adjustment on the automatically-adapted version provided by this approach.". In addition, designers further agree that the generated results could be used as their initial design drafts. E4 said, "Even from a professional point of view, it can definitely be used as an initial design draft for designers, which can save us lots of time and efforts.". In terms of the quality of our generated infographics, four experts rate the results separately and think that roughly 68% of them are reasonable. The comments of designers mainly center on three aspects. First, for most exhibited cases, the designers feel that they were flexible and diverse enough. Two designers commented regarding the element combination that "This approach provides the flexibility of composing various visual elements to create a complex infographic." (E4), and "overall it is very diverse, and each case shows a unique combination of elements." (E2). Another designer also commented on the layout of the generated infographics: "even though the element combinations are similar, it generates quite different infographics by laying elements out in different ways." (E1). Second, almost all designers had said that the important characteristics of the example were completely preserved by our approach. For example, E1 said that "the generated infographic looks very similar to that of the retrieved example. Only by scrutiny, could I tell the minor differences". Finally, all the designers thought that generated results accurately convey the original information. E2</p><p>commented that "The key information of user input, especially the numerical information, is correctly highlighted by using a larger font size or leveraging data driven graphics like bars, pies and donuts.".</p><p>We also receive suggestions that implied further necessary improvements to our approach. First, it is suggested to use icon combination instead of a single icon to represent the semantic meaning of the input statement. For example, when seeing a key phrase "phishing mail" in the input statement, E1 expressed the intent to compound a "fish" icon and an "email" icon together to convey such information. We think the latest work ICONATE <ref type="bibr" target="#b35">[36]</ref> can be integrated to our approach to improve performance. Moreover, designers also made suggestions on further adding a reference line in bar charts used in our results. "Although this is a trivial detail, the reference line makes multiple bar charts easier to be understood." said by one of designers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.1">Example-Based Infographic Generation</head><p>Creating infographics is non-trivial, which requires tremendous design expertise and is time-consuming. Therefore, automating the generation process of infographics is very meaningful and useful. Previous studies try to automatically generate infographics by using predefined blueprints. However, the deficiency of blueprints largely constrains the diversity of generated results. By contrast, there are enormous infographics on the Internet. This motivates us to consider example-based approach, i.e., generating infographics by imitating existing infographics. One concern for this approach is that crowdsourced examples might not always fit right design principles and thus may have negative influence on the generation performance. To alleviate such influence, in our implementation, we control the quality of crowdsourced examples by letting annotators discard the designs with obvious errors during the labeling process and collecting examples from professional websites.</p><p>Moreover, in real scenarios, both designers and casual users get inspired from existing infographics. For designers, they usually explore lots of online examples to learn advanced design principles and accumulate design materials, which will be used in their future designs. For casual users, who have no design background, they even have strong desires to directly reuse a favorite existing infographic on their own data. These scenarios make us believe that our example-based approach is useful for casual users and even potentially helpful for designers.</p><p>In addition, we aim to use the automatic generation of infographics as the assistance but not the replacement for humans. On the one hand, it is almost impossible to automatically generate a perfect infographic. On the other hand, the design process is subjective. Therefore, human interaction is indispensable during the design process. Our approach can generate editable infographics that roughly conform to design principles and meet user requirements, based on which people can easily get desirable infographics by polishing and fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.2">Copyright Issue</head><p>Copyright is a big concern for our example-based approach. It is a violation of intellectual property rights to fully imitate an example without owning appropriate copyrights. Practically, although designers explore online examples for inspirations, they hardly adopt every design aspect from a single example. During the interview, the designers mentioned that may largely reuse high-level designs, e.g., layouts and colors, but they will rethink details, e.g., shading and textures, based on their personal tastes. In this work, to demonstrate the capability of our approach, the algorithm is designed to imitate examples as much as possible. There are several solutions to deploy our technique in production. The simplest way is to directly use an example library with proper copyrights. As infographics are often easier to purchase than blueprints, it may still achieve a good diversity. In addition, as our approach contains two consecutive stages, it is also possible to allow users to directly provide an example with a correct copyright for the adaption stage. Another promising solution is to combine designs from multiple examples, instead of reusing every aspect from a single one. We believe this solution is more aligned with the common practice of designers, and plan to improve our approach in this respect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.3">Opportunities for New Usage</head><p>We think there are at least three opportunities for new usage.</p><p>From Proportion Infographics to Others. Although this work focuses on proportion-related infographics, it is possible to generalize our approach to more types of infographics, e.g., time-line, location, or process. In our approach, both the retrieval and adaption stage are proposed independent of the infographics type. Thus, it can be applied to other types of infographics once the visual elements are redefined and an annotated example library are provided.</p><p>Customized Searching Tool for Infographics. The retrieval stage can be extended to a customized searching tool for infographics. In practice, people often intend to find infographics with certain kinds of visual attributes. However, they seldom get satisfactory results by typing descriptions in traditional search engines, e.g., Google or Bing. For example, when people want to get inspiration from existing infographics about how to design infographics with unusual canvas size (like 5 : 1), traditional search engines can hardly provide related infographics. Thus, a customized searching tool, which indexes visual attributes of infographics, is of great value. The retrieval stage of this work has already indexed some essential visual attributes. A extended version of it can greatly improve the searching efficiency for infographics.</p><p>Automatic Layout Adaption Tool. Techniques used in our adaption stage can be potentially integrated into mainstream software to offer automatic refinement. To lower the authoring barrier, commercial software (e.g., PowerPoint or Adobe Illustrator) usually provide predefined blueprints to users. However, the user data usually cannot be perfectly fitted into these predefined blueprints, requiring tedious refinement after applying blueprints. If users can make edits based on designs that have been automatically adjusted by our adaption techniques, both of authoring efficiency and user experience can be improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.4">Limitations</head><p>There are several limitations of our approach. First, the implementation for our approach is limited to proportion-related infographics although it can be potentially generalized to other types of infographics. Second, when constructing the example library, visual elements and attributes can be further enriched to improve the performance of retrieval. For instance, various properties of charts can be added, e.g., the inner radius of donut charts or explode slices in pie charts. For another instance, embellished shapes can be added, e.g., stars, banners or speech/thought bubbles. Third, the current retrieval stage only considers attributes of candidate visual elements but ignores the user preference on visual styles (e.g., the concise style, the lively colors, or the landscape layout). In the future, we can index examples by their visual styles and improve the retrieval strategy to consider the consistence between the user preference and the example's visual style. Fourth, the adaption efficiency can be potentially improved by designing more efficient strategies to propose candidate designs. In addition, we only adjust spatial relationships as it is the most easily disturbed attributes after the initialization. Other attributes can also be adjusted to further boost the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">CONCLUSION AND FUTURE WORK</head><p>In this paper, we introduce retrieve-then-adapt, an automatic approach to generate infographics by imitating online examples. Specifically, we present a retrieval stage that indexes and retrieves online examples based on the element types and build-in attributes, and an adaption stage that automatically adjust spatial relationships by a MCMC-like approach to make the retrieved example's design more suitable for the user information. We demonstrate the expressiveness and usability of our approach through sample results and expert reviews. We believe this work opens up a new paradigm for automatically generating infographics, i.e., example-based approach. We plan to further extend this framework by considering other types of infographics, considering more design choices and mixing designs of different examples, in order to better meet users' growing demands.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Examples for visual elements in proportion-related infographics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 .</head><label>5</label><figDesc>(a)-(b) The distributions of element type and text length. (c)-(l) The position distributions of number, before, modifier, after, statement, single icon, bar, pie, donut, and pictograph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 4(c) shows a concrete labeling result for the infographic inFigure 4(b).Figure 5shows distributions of labeling results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Illustration for design choices about visual elements.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Example of initialization. (a) A retrieved example. (b) The initial draft generated by directly applying positions in the retrieved example. (c) The initial draft generated by scaling the graphical elements and recalculating the font size of textual elements.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(D ) /d (D)}. Here D and D stand for the candidate and current designs respectively, and d (•) measures the quality of the spatial relationships in a given design. Specifically, if spatial relationships in the candidate design are better than those in the current design, i.e., d (D ) &gt; d (D), we always accept the candidate design (see Figure 8(b)). Otherwise, we accept the candidate design with the acceptance probability of d (D ) /d (D) (see Figure 8(c)). The key challenge is evaluating spatial relationships of designs, i.e., estimating d (•).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Formally, we aim to learn a mapping h that takes a pair of designs (D , D) as the input and predict a pair of scores (d (D ) , d (D)) as the output, i.e., h : (D , D) → (d (D ) , d (D)). Then, (d (D ) and d (D)) will be used to calculate α.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Concrete model architecture for spatial relationships evaluation.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">in 5 smartphone owners check their phones within 15 minutes of waking up.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The font is recognized by using https://www.myfonts.com/WhatTheFont/.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>The authors wish to thank Yang Ou, Hoilan Wong, Ziyi Wang and Vicky Hua for attending the expert review and giving valuable comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Effects of infographics on student achievement and students&apos; perceptions of the impacts of infographics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Alrwele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Education and Human Development</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="104" to="117" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Coloring with words: Guiding image colorization through text-based palette generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bahng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the european conference on computer vision</title>
		<meeting>the european conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="431" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Useful junk? the effects of visual embellishment on comprehension and memorability of charts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bateman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Mandryk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gutwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Genest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcdine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI conference on human factors in computing systems</title>
		<meeting>the SIGCHI conference on human factors in computing systems</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2573" to="2582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Understanding infographics through textual and visual tag prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alsheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Madan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Webcrystal: understanding and reusing examples in web authoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Myers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards automated infographic design: Deep learning-based auto-extraction of extensible timeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="917" to="926" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Text-to-viz: Automatic generation of infographics from proportion-related natural language statements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-G</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05908</idno>
		<title level="m">Tutorial on variational autoencoders</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ben-Eliezer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Perel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Averbuch-Elor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.00302</idno>
		<title level="m">Read: Recursive autoencoders for document layout generation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Isotype visualization: Working memory, performance, and engagement with pictographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kosara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Franconeri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd annual ACM conference on human factors in computing systems</title>
		<meeting>the 33rd annual ACM conference on human factors in computing systems</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1191" to="1200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deconstructing and restyling d3 visualizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th annual ACM symposium on User interface software and technology</title>
		<meeting>the 27th annual ACM symposium on User interface software and technology</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Converting basic d3 charts into reusable style templates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1274" to="1286" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Aesthetic measures for automated document layout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Harrington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Naveda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Roetling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thakkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 ACM symposium on Document engineering</title>
		<meeting>the 2004 ACM symposium on Document engineering</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="109" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Getting inspired! understanding how and why examples are used in creative design practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Herring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krantzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Searching the visual style and structure of d3 visualizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1236" to="1245" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Data-driven guides: Supporting expressive design for information graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schweickart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dontcheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Popovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="491" to="500" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bricolage: example-based retargeting for web design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">O</forename><surname>Talton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Klemmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2197" to="2206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Designing with interactive example galleries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Brafman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Klemmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.09421</idno>
		<title level="m">Neural design network: Graphic layout generation with constraints</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Layoutgan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.06767</idno>
		<title level="m">Generating graphic layouts with wireframe discriminators</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Grains: Generative recursive autoencoders for indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Selecting semantically-resonant colors for data visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fortuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="401" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lanir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<title level="m">Exploring visual information flows in infographics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning layouts for single-pagegraphic designs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>O'donovan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1200" to="1213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Designscape: Design with interactive layout suggestions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>O'donovan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd annual ACM conference on human factors in computing systems</title>
		<meeting>the 33rd annual ACM conference on human factors in computing systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1221" to="1224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A classification of infographics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Purchase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Isaacs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bueti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hastings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kassam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Van Hoesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Theory and Application of Diagrams</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="210" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Human-centric indoor scene synthesis using stochastic grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5899" to="5908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Lyra: An interactive visualization design environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Satyanarayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="351" to="360" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Infographics: the new communication tools in digital age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">V</forename><surname>Siricharoen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The international conference on e-technologies and business on the web (ebw2013)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="169" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning</title>
		<meeting>the 28th international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adversarial colorization of icons based on contour and color conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="683" to="691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep convolutional priors for indoor scene synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ritchie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">70</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Infonice: Easy creation of information graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2018 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Iconate: Automatic compound icon generation and ideation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Herman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Echevarria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>To appear in the</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Image parsing with stochastic scene grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="73" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Content-aware generative modeling of graphic design layouts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">133</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
