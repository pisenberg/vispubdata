<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Chartem: Reviving Chart Images with Data Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayun</forename><surname>Fu</surname></persName>
							<email>fujiayun@hust.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Zhu</surname></persName>
							<email>binzhu@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Cui</surname></persName>
							<email>weiweicu@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Ge</surname></persName>
							<email>songge@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Wang</surname></persName>
							<email>wangyun@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haidong</forename><surname>Zhang</surname></persName>
							<email>haizhang@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Huang</surname></persName>
							<email>rayhuang@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyuan</forename><surname>Tang</surname></persName>
							<email>tangyuanyuan@hust.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Zhang</surname></persName>
							<email>dongmeiz@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojing</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename><forename type="middle">B</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Data Packaging • J. Fu, Y. Tang, and X. Ma are with Natl. Eng. Res. Ctr. for Big Data Tech. and Sys</orgName>
								<orgName type="department" key="dep2">Big Data Sec. Eng. Res. Ctr</orgName>
								<orgName type="department" key="dep3">School of Cyber Science and Technology</orgName>
								<orgName type="institution">Huazhong Univ. of Science and Technology</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Chartem: Reviving Chart Images with Data Embedding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">received xx xxx. 201x; accepted xx xxx. 201x.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>, and adjusts the opacity of the embedded patterns to produce an information-embedded chart image (e), which can be used as a regular chart since the embedding patterns are generally subtle. However, when needed, Chartem can recover the embedded information (f) for different uses by determining and decoding the background patterns.</p><p>Abstract-In practice, charts are widely stored as bitmap images. Although easily consumed by humans, they are not convenient for other uses. For example, changing the chart style or type or a data value in a chart image practically requires creating a completely new chart, which is often a time-consuming and error-prone process. To assist these tasks, many approaches have been proposed to automatically extract information from chart images with computer vision and machine learning techniques. Although they have achieved promising preliminary results, there are still a lot of challenges to overcome in terms of robustness and accuracy. In this paper, we propose a novel alternative approach called Chartem to address this issue directly from the root. Specifically, we design a data-embedding schema to encode a significant amount of information into the background of a chart image without interfering human perception of the chart. The embedded information, when extracted from the image, can enable a variety of visualization applications to reuse or repurpose chart images. To evaluate the effectiveness of Chartem, we conduct a user study and performance experiments on Chartem embedding and extraction algorithms. We further present several prototype applications to demonstrate the utility of Chartem.</p><p>Index Terms-Chart embedding, background embedding, data embedding, chart image, chart reuse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>As an effective and efficient means to convey quantitative information <ref type="bibr" target="#b37">[39]</ref>, charts have become an increasingly pervasive type of content widely adopted in newspapers, textbooks, websites, academic papers, etc. Nowadays, there are many tools, such as Excel, Tableau, and Power BI, to help users convert data into charts or graphs effortlessly. During the authoring process, a chart object is often created to maintain relationships between data and visual elements. After the authoring process, it is common to save the created chart as a bitmap image, for easy typesetting or sharing. In many cases, the resulting image is then disconnected from its chart object and becomes the only representation available for the underlying data. This may cause several issues in the long run. First, since the carried information and visual style are locked in a chart image, it is hard to reuse or repurpose the chart in the future. For example, if Alice wants to change the chart type or style for a different story or document, she often has to do it manually as a chart image is generally not machine readable. To assist with this task, many image-recognition-based techniques have been proposed to automatically recover data and visual design information from chart images <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">25]</ref>. However, this is still a relatively new research direction, and a robust solution that can accurately recover the full information of a chart image has not been accomplished yet due to diversity and complexity of chart content. Second, in many cases, the message conveyed by a chart is distilled from a bigger dataset via a series of aggregation and filtering operations. If a user likes to perform a different analysis on the same underlying dataset for a different purpose, it will be impossible since the information carried by the chart is limited and the original dataset is completely lost after the conversion.</p><p>In this paper, we present a novel approach to solving the above issues and unlocking chart images with more potential. Our solution, called Chartem, embeds the chart data or arbitrary data into a chart image when it is published. Once embedded, the information becomes an intrinsic part of the chart image. It can be retrieved, when needed, for further processing. For instance, if the information is the chart data, it can be directly used to generate a chart with a different style or type, or be analyzed for a different insight, etc. An overview of this process is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Unlike image-recognition-based techniques, the integrity of extracted information is verified in our method to guarantee completeness and accuracy of the restored chart object. As a result, our proposed method unlocks chart data accurately, which would open a great opportunity for reusing underlying data of chart images.</p><p>There are alternative information-carrying solutions, such as overlaying information on a chart image (e.g., QR code) or piggybacking the information into chart files. Our data-embedding approach has two advantages over them. First, our embedding patterns are not apparent or intrusive during chart reading, thus the same user experience of normal charts is preserved. Second, the embedded information stays with the chart even after the chart image is format-converted or screenshot.</p><p>Image data embeddings have been extensively studied for natural images. These methods have been built on the continuous-tone characteristics of natural images. Unlike natural images, chart images typically comprise homogeneous color regions, which makes naturalimage-based embedding techniques ineffective for chart images.</p><p>In Chartem, we present a novel data-embedding scheme specifically designed for chart images. It embeds arbitrary data into background regions of a chart image by slightly modifying the pixel values to form deliberated patterns to encode the data. Embedding patterns are faintly visible and do not incur adverse impact on chart reading. Chartem adopts a data segmentation mechanism as well as a design of synchronization marks to enhance the robustness of data extraction. The embedded data can be accurately extracted even if an embedded chart image undergoes typical image manipulations such as resizing, screenshot, compression, rotations, and brightness variations.</p><p>We present a user study to understand the impact of embedding patterns on human perception of charts and experimental evaluation of Chartem's performance. In illustrating potential utilization of Chartem, we present a prototype application as an Excel add-in to demonstrate how Chartem can be integrated into a chart authoring system to generate chart images with embedded data, followed by two prototype applications to show that Chartem can enable scenarios such as redesigning charts and reading charts to people with vision impairment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Parsing Chart Images</head><p>A huge amount of information is locked inside chart images and inaccessible to machines and visually impaired people <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20]</ref>. To overcome this limit, many computational methods have been proposed to interpret chart images based on OCR and image recognition techniques. From a given bitmap chart image, these methods attempt to extract information including chart type, underlying data, visual encodings, etc.</p><p>ReVision <ref type="bibr" target="#b23">[25]</ref> first uses a SVM model to detect chart type, then applies image processing techniques to locate the marks and recover data from bar and pie chart images. FigureSeer <ref type="bibr" target="#b24">[26]</ref> trains a CNN network for chart type classification and uses legend information for more accurate data extraction. DVQA <ref type="bibr" target="#b10">[11]</ref> employs a deep dual-network model to directly parse the data from bar chart images. Scatteract <ref type="bibr" target="#b6">[7]</ref> automatically restores the numerical values of data points from images of scatter plots. More recently, Choi et al. <ref type="bibr" target="#b5">[6]</ref> built a DNN-based automatic pipeline to extract data from chart images for reading to visually impaired people. Apart from chart data, there are research works focusing on chart design aspects. For example, Poco and Heer <ref type="bibr" target="#b19">[20]</ref> proposed a multi-stage pipeline, which combines ML and heuristics techniques, to automatically infer a visual encoding specification from a chart image. Poco et al. <ref type="bibr" target="#b20">[21]</ref> sought to extract color mappings from chart images. Besides images of standard charts, researchers have also investigated computational methods to parse images of infographics <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14]</ref>. Due to diversity and complexity of chart content, all these techniques support only a limited number of chart types (e.g., bar, pie, line, scatter charts). Moreover, they often cannot achieve sufficient accuracy of data extraction, especially when a chart has overlapped visual entities. Although some techniques, such as ChartSense <ref type="bibr" target="#b9">[10]</ref> and iVoLVER <ref type="bibr" target="#b15">[16]</ref>, adopt a mixed-initiative approach to improving data extraction accuracy with human interactions, they are not suitable for applications that require fully automated processing.</p><p>While sharing the same goal with these techniques, our work takes a completely different approach to unlocking chart images with more potential. Specifically, Chartem embeds data into background regions of a chart image. The embedded data can be extracted to support further processing. Compared with prior works on parsing content of chart images, our solution has several advantages. First, our solution is more robust and accurate as desired information is directly embedded into chart images. Second, since our solution does not depend on interpreting visual elements to decode information, it can be easily applied to different chart types. Third, the embedded data can be any digital information even not being presented on charts, so our solution can enable richer chart reuse applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data Embedding and Watermarking</head><p>Both watermarking and data embedding embed information into a host signal, typically audio, image, or video <ref type="bibr" target="#b26">[28]</ref>. While they share many common properties and requirements, watermarking and data embedding are targeted for different applications. Watermarking is generally used for tracking and copyright protection. A small number of bits are embedded, but the embedded data has to be very robust against all possible perceptual-quality-preserving manipulations including intentional attacks. Data embedding, on the other hand, generally embeds as much information as possible into a host signal, and the embedded data only has to survive the processing needed in its targeted applications. As a special type of data embedding, steganography aims to conceal the presence of a hidden message in a host signal <ref type="bibr" target="#b3">[4]</ref>. Embedding in steganography should be imperceptible and undetectable.</p><p>Data embedding embeds information into a host signal by modifying selected features of the host signal, while watermarking can either embed watermark in or superimpose an additive spread spectrum watermark on the host signal. We focus on embedding techniques used in watermarking and data embedding. Features selected to carry information can be pixels in the spatial domain or coefficients in a transform domain such as in the frequency domain or a wavelet-transform domain. Spatial-domain embedding is generally for host images, wherein the least significant bits (LSBs) of pixels are modified to carry information <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b38">40]</ref> or pixels are modified in pairs, with each pair carrying one bit information <ref type="bibr" target="#b36">[38]</ref>. Image steganography typically embeds in the spatial domain too, e.g., HUGO <ref type="bibr" target="#b18">[19]</ref>. Transform-domain embedding, on the other hand, can be applied to audios <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b33">35]</ref>, images <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b43">45]</ref>, and videos <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b43">45]</ref>, wherein middle and/or high frequencies in the frequency domain <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b28">[30]</ref><ref type="bibr" target="#b29">[31]</ref><ref type="bibr" target="#b30">[32]</ref><ref type="bibr" target="#b31">[33]</ref><ref type="bibr" target="#b32">[34]</ref><ref type="bibr" target="#b33">[35]</ref> or a wavelet-transform domain <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b43">45]</ref> are modified. Spatial-domain embedding is generally less robust than transform-domain embedding.</p><p>In addition to the above traditional approaches, deep learning has also been used for data embedding. An embedding network and an extraction network can be trained simultaneously to hide an image <ref type="bibr" target="#b0">[1]</ref> or arbitrary data <ref type="bibr" target="#b41">[43]</ref> into a host image for image steganography. They can hide a large amount of data into a host image, but small perturbations of a common image manipulation such as JPEG compression, screenshot, resizing, or rotation would render the hidden data unextractable. By incorporating, during training, various perturbations that an image may go through, hidden data is still extractable after JPEG compression and cropping <ref type="bibr" target="#b42">[44]</ref>, displaying and photographing <ref type="bibr" target="#b40">[42]</ref>, or printing and photographing <ref type="bibr" target="#b34">[36]</ref>, but the embedding capacity is significantly reduced, e.g., 56 bits for <ref type="bibr" target="#b34">[36]</ref>. The perceptual quality of images produced by these deep-learning-based methods is generally good, but the embedding residual can be perceptible in large low-frequency regions of a host image <ref type="bibr" target="#b34">[36]</ref>, and a sharp edge can be found blurred.</p><p>All the aforementioned watermarking and data-embedding methods are designed for natural host signals, e.g., natural or continuous-tone images. Unlike natural images, synthetic images such as chart images typically comprise homogeneous color components. Spatial-domainembedding methods used for natural images are generally ineffective for synthetic images since data embedding makes a homogeneous color region no longer homogeneous after embedding, resulting in perceptible embedding residual. Transform-domain-embedding methods are also ineffective since a homogeneous region has only energy around zero frequency. There is no middle or high frequency that can be modified to carry information. To address the unique characteristics of synthetic images, Masry <ref type="bibr" target="#b14">[15]</ref> proposed a watermarking scheme for map and chart images by modifying boundaries of homogeneous color components. Designed for watermarking applications, this method can embed only limited information, and thus is ineffective for data-embedding applications that we focus on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">QR Code</head><p>Since its introduction in 1994 by Masahiro Hara [23], the QR code <ref type="bibr" target="#b39">[41]</ref> has been widely used to carry information for various applications. The QR code is a machine-readable 2D barcode of black and white cells. Inspired by the QR code design, Chartem has borrowed some designs from the QR code, such as coarse and fine marks. On the other hand, our scheme differs from QR codes in several critical ways: our information carrying patterns are chart-dependent, faintly visible, and interleaved with foreground regions that vary from one chart to another. These key differences demand a very different approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CHARTEM</head><p>Chartem consists of two parts: an embedder to embed information into chart images, and an extractor to extract embedded information from chart images. Like traditional data embedding, Chartem faces three main conflicting requirements or challenges:</p><p>Perceptual quality. A chart embedded with information should not interfere with normal consumption of the chart.</p><p>Capacity. All desired information should be able to be embedded into a host chart image.</p><p>Robustness. The embedded information should be correctly extracted after targeted processing and distortions.</p><p>These requirements are inversely related. It is generally more robust when perceptual quality is lowered or capacity is reduced.</p><p>In viewing a chart, human's attention is typically focused on foreground components of the chart. To ensure perceptual quality, Chartem modifies only background pixels to carry information while keeping foreground components unchanged. To improve robustness, Chartem packages input data into segments. Each segment can independently determine if its extracted data is correct and complete. To balance capacity and robustness, Chartem uses fountain codes <ref type="bibr" target="#b12">[13]</ref> to generate extra segments to embed whenever there is extra capacity. Any set of recovered segments with the count equal to the number of segments the input data is partitioned into can virtually recover the whole embedded data. A chart image the extractor receives may have a different shape or size from its original chart image, unknown to the extractor. Chartem embeds two sets of synchronization marks, or simply marks, for the extractor to register a received chart image to its original chart image. <ref type="figure" target="#fig_0">Fig. 1</ref> includes a flowchart of the embedding process: Chartem detects the background of a chart image, embeds coarse and fine synchronization marks and bits of segments generated from packaging input data and fountain coding, and adjusts the visibility of generated embedded patterns via a weight. The resulting data-carrying chart image has the same size as and looks nearly identical to the original chart image. These processing steps will be described in detail in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Chartem Embedder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Background Detection</head><p>Background locations can be either passed to Chartem from a chart creation tool or detected by Chartem. For good usability, foreground components in a chart are typically visually distinctive from the background. This distinctiveness and the characteristics of chart images are exploited to detect the background of a chart image:</p><p>First Chartem groups pixels into clusters by classifying a new pixel into the cluster closest to it if the distance is within a threshold, determined a priori by the expected spread of background pixel values, or otherwise a new cluster. Each cluster maintains a histogram of its pixels and a reference value equal to the center of the histogram bin with the highest count. The distance of a pixel to a cluster is defined as the distance of the pixel's value to the reference value of the cluster, and the distance between two clusters is defined as the distance of their reference values. The reference value of a cluster is updated whenever the cluster adds a fixed number of new pixels.</p><p>Then Chartem labels one cluster as background and remaining clusters as foreground based on the cluster's size, spatial shape and location in the image, and distances to other clusters. The foreground is structurally dilated. Isolated foreground pixels and small background regions are removed. The resulting background is the background to find.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Coarse and Fine Synchronization Marks</head><p>After data embedding, a chart image may undergo size or shape changes such as scaling. The extractor needs to register a received chart image to its original size and shape before correctly extracting the embedded data. A general approach like in QR codes is to insert specially designed marks at a preset distance to register a received image to the desired size and shape. Unfortunately, this approach does not work for Chartem since marks can be inserted only into background regions of a chart image. Background regions vary from one chart to another. A fixed location may not be available for all chart images to embed a mark.</p><p>To address this problem, Chartem embeds two sets of synchronization marks: coarse marks for rough and fine marks for accurate estimation of transformation parameters. These transformation parameters are used to register a received chart image to its original shape and size. Both marks have unique patterns that can be easily identified. <ref type="figure" target="#fig_0">Fig. 1</ref>(A) and (B) show the coarse and fine mark patterns that Chartem uses, respectively. The coarse mark is a pattern of 9 by 9 logical bits, while the fine mark is a pattern of 7 by 7 logical bits. Their center ratios along both directions are 1:1:1:3:1:1:1 and 1:1:1:1:1:1:1, respectively.</p><p>In its basic setting, Chartem inserts at least three and up to four coarse marks at the corners of a rectangle within which data embedding occurs. The rectangle can contain foreground components. In such a setting, coarse marks indicate a bounding box of data-embedding regions. This setting is not necessary since Chartem uses start and end blocks to indicate where data blocks are located (Section 3.1.5). In a general setting, the rectangle can be anywhere in a chart image, as long as at least three coarse marks can be embedded at its corners. The rectangle should be large enough to reduce potential estimation errors at the extractor.</p><p>To embed fine marks, Chartem determines a grid of cell size h × v logical pixels and its bias so that more fine marks can be embedded at grid intersections in the background, where h, v ∈ A, and A is a set of admissible values for a grid. Chartem selects A = {56, 63}, which is designed to uniquely determine, after image registration with coarse marks, h or v of the grid used in a received chart image from two detected fine marks up to 4 cells apart. Chartem requires inserting at least three fine marks not aligned along a horizontal or vertical line. More embedded fine marks improve robustness since the extractor may miss some fine marks. In <ref type="figure" target="#fig_0">Fig. 1</ref>(c), six fine marks are embedded with a grid of 56 × 56 logical pixels: one circled by the right red circle, one on its left and two below it, and two more below the left red circle.</p><p>The synchronization marks are embedded into the background of a chart image first. They are embedded in the same way as embedding data, which is described in Section 3.1.5. Data is embedded into remaining background regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Data Packaging</head><p>Information to be embedded can be arbitrary data. Input data is first compressed losslessly to reduce its capacity requirement, and then prefixed with 2 bytes to represent the length of compressed data. The prefixed input data is then partitioned and packaged into segments. Each segment can be extracted and checked correctness independently. Such data packaging prevents error propagation from one segment to another and thus improves the robustness of extraction.</p><p>A chart image may have extra capacity after embedding the segments constructed from input data. In this case, we use fountain codes <ref type="bibr" target="#b12">[13]</ref> to generate an arbitrary number of segments to exhaust all the embedding capacity of the chart image. Fountain codes are a class of erasure codes that can generate a potentially unlimited number of segments from a set of source segments such that the source segments can be fully recovered from any subset of segments with the size equal to or slightly larger than the number of the source segments. The fountain coding <ref type="bibr" target="#b35">[37]</ref> used in Chartem preserves input data. As a result, we refer to a segment constructed from input data as a data segment and a segment generated …...  <ref type="figure">Fig. 3</ref>. A data block sequence: start and end blocks at both ends, and one or more data blocks in the middle. In a data block, there is a flip bit (a), and the remaining (b-i) bits are data bits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Start</head><p>from fountain codes as a fountain segment in this paper. A segment consists of header, payload, and parity, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. The header is composed of a marker to identify a segment header from a bitstream, a segment ID for the index of the segment among all distinct segments, a status bit to indicate if the segment is a data or fountain segment, and a CRC (Cyclic Redundancy Check) code to check if there is any error in the combination of the segment ID, the status, and the payload. Payload contains data from user data for a data segment or data generated by fountain codes for a fountain segment. Parity contains error correction parity to correct errors in the combination of the payload and the header excluding the marker in the header. In this setting, the total number of distinct segments is limited by the size of segment ID. When more segments can be embedded, Chartem reuses existing segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Data Blocks</head><p>Bits in segments are partitioned into data blocks, which are then organized into block sequences. Each block sequence consists of a start block, an end block, and one or more data blocks between them, as shown in <ref type="figure">Fig. 3</ref>, and is embedded into a continuous background region not taken by any synchronization mark. Each block comprises m × n such as 3 × 3 logical bits. The start and end blocks are special blocks with fixed bit patterns. They are used to identify a block sequence.</p><p>A data block contains a flip bit to indicate if the bits in the block have been flipped or not, and the remaining bits are bits from segments. When adding a data block to a block sequence, Chartem checks if the start or end block is replicated. If replication occurs, the newly added data block is flipped to eliminate the replication. In this way, there is no replication of the start or end block in any block sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.5">Embedding Data</head><p>To embed a block sequence, each logical bit in the block sequence is mapped into a preset block of, such as p × p, pixels. The block size is determined by targeted applications. Mapping to a large block of pixels increases robustness at the cost of reduced capacity. Based on the logical bit value, Chartem assigns each pixel in the block a color value slightly above or below the local average of background pixels. The gap between pixels representing 0 and 1 of a logical bit is a fixed factor times a weight. By adjusting the weight, we can adjust the perceptual quality of data-embedded chart images. The larger the weight is, the more visible the embedded patterns are. If logical bits are randomly distributed, this embedding process preserves the local average of background pixels. When moving up or down a specified distance from a local average results in a value outside the valid value range of pixels, Chartem shifts the assigned value back to the valid range while preserving the gap between pixels representing 0 and 1. After such shifting, the local average after embedding is slightly shifted from the original local average.</p><p>In the implementation of Chartem, we convert a chart image into the YUV color space, and embed information into Y-component while leaving the UV components unchanged.</p><p>For each foreground component, we secure a buffer region of a fixed width around the border of the foreground component as a transition region. No data is embedded into any transition region. After embedding block sequences to all available background regions, unused background pixels, such as those in transition regions or background regions too small to embed a block sequence, are assigned values in the same way corresponding to random logical bit values except that Chartem ensures no replication of the start or end block.  <ref type="figure">Fig. 4</ref>. A flowchart of the data extraction process. <ref type="figure">Fig. 4</ref> shows a flowchart of the data extraction process: the extractor detects background regions in a chart image, locates coarse and fine marks to execute coarse and fine registrations of the chart image, locates pairs of start and end blocks to identify block sequences and extracts bits from their data blocks, detects and validates each segment, performs fountain decoding, and validates the extracted data. To facilitate detection of coarse and fine marks and bit extraction, adaptive binarization is used to enhance embedding patterns. These steps will be described in detail in following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Chartem Extractor</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Detecting Background Regions</head><p>To detect background regions, the extractor applies the clustering method described in Section 3.1.1 to cluster pixels inside a sliding square window, typically of size in range <ref type="bibr" target="#b29">[31,</ref><ref type="bibr">51]</ref>, and then slides the window to cluster new pixels coming into the window. After clustering, it counts boundary pixels for each pair of clusters. If the maximum count normalized by the image size is above a preset threshold, we combine the two clusters of the pair into a single cluster and take it as a candidate for background. This occurs when the embedding weight is so large that pixels representing 0 and 1 are classified into two clusters. Since pixels carrying 0 and 1 interleave with each other, their clusters have significantly more boundary pixels than usual. The extractor then determines a cluster as the background and the remaining clusters as the foreground based on the cluster's size, spatial shape and location in the image, and distances to other clusters.</p><p>While embedder's background detection tries to exclude foreground pixels from the detected background to avoid touching foreground pixels during embedding, the extractor's background detection allows some foreground pixels in the detected background to avoid missing any embedded data. These foreground pixels and their impact will be removed in subsequent procedures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Adaptive Binarization</head><p>Binarization is needed in detecting coarse and fine marks and in extracting embedded bits. Chartem adopts an adaptive binarization scheme for potential lighting variation: It collects local distributions of background pixels to determine the size of an adapting window, with background pixels inside such an adaptive window being able to be considered as quasi-static. Then Chartem moves the window over the image: the window cannot cross any boundary of a large foreground region but can contain small foreground regions. At each position, Chartem collects the histogram of background pixels inside the window, removes pixels whose values significantly outside the expected range of background pixels, and applies the mode method <ref type="bibr" target="#b21">[22]</ref> to determine a threshold, which is robust to foreground pixels not excluded yet as long as their ratio to the background pixels in the window is small. The threshold is used to binarize the background pixels at the nominal center of the window, i.e., the center of the window at the position it should be located if there were no large foreground regions in the chart image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Detection of Marks and Image Registration</head><p>Coarse marks and fine marks are detected in the same manner. After binarization of pixels in background regions, Chartem scans background pixels to search for patterns that each matches the ratio of the mark to be detected both horizontally and vertically across the center block of the mark, with the outermost ring of the pattern, used as a guarding buffer, excluded. For the coarse and fine marks shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, the ideal ratio to match is 1:1:3:1:1 for the coarse mark and 1:1:1:1:1 for the fine mark. For each found pattern, Chartem checks if the exterior shape of each layer of the pattern is nearly a parallelogram. If they are, Chartem determines that the pattern is a mark to be detected.</p><p>Chartem detects coarse marks first. The center of an original coarse mark is black. Chartem first uses centers of detected coarse marks to determine black and white values of binarization. Then it uses detected coarse marks to roughly register the received chart image. Since coarse marks are arranged at corners of a rectangle at embedding, the detected coarse marks are used to determine a perspective transform to convert the received chart image into a rectangular shape. The horizontal and vertical scales are then estimated from each converted coarse mark, and their averages over detected coarse marks are calculated. The averaged horizontal and vertical scales are then combined with the perspective transform just applied to convert the received chart image into a chart image roughly like the original image.</p><p>Chartem then detects fine marks on the roughly registered chart image, determines the actual distances among detected fine marks, and uses them to evaluate a more accurate perspective transform to convert the received chart image into a chart image more accurately like the original image. To facilitate data extraction to be described next, Chartem actually registers a received image into a chart image of k times the size of the original image, i.e., one original pixel is equivalent to k × k pixels in the registered image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Data Extraction</head><p>After the fine registration, each logical bit corresponds roughly kp × kp pixels. Chartem first applies the adaptive binarization to convert background pixels of the registered image to bipolar -1 and 1. It then uses a template of kp × kp pixels, each of of value 1/(kp) <ref type="bibr" target="#b1">2</ref> , as a matched filter to scan all background pixels. In an ideal case, the matched filter produces 1 or -1, corresponding to logical bit 1 and 0, respectively, when the template is aligned with a block of pixels that represents a logical bit, and the matched filter's output is a maximum (or minimum) along one direction, either horizontal or vertical direction, if the logical bit is 1 (or 0) and its two neighboring logical bits on both sides along the direction are both 0 (or 1). These facts are exploited to detect values of logical bits and align them horizontally and vertically.</p><p>More specifically, Chartem applies a preset threshold to find all locations whose absolute value of the matched filter output is larger than the threshold. These locations are candidates of logical bit blocks. Chartem then locate extremums (maximums or minimums) along both directions and also along a single direction. To determine rows of logical bits, Chartem locates rows with the number of extremums along both directions and along the vertical direction above a preset threshold. These rows are determined to be rows of logical bits. Chartem then extends from these determined rows to determine other rows of logical bits, based on the fact that the vertical distance of a row is about rp pixels, fine-tuned with the locations of found extremums along both directions and along the vertical direction close to the row. Chartem determines locations of logical bits in each row in a similar way.</p><p>At the end of the above process, all logical bits are determined in background regions of the chart image. Then Chartem applies the patterns of start and end blocks to scan these logical bits to determine potential locations of start blocks and end blocks, and determine each matching pair of start block and end block that satisfies the conditions at embedding. Each pair determines a block sequence, wherein data blocks are determined, and raw bits are extracted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5">Unpacking Raw Data</head><p>At the end of the last step, a stream of raw bits is obtained. Chartem then applies the marker of a segment header as a matched filter to scan the raw bit stream to locate positions whose output is above a preset threshold. These positions are potential locations of segments. For each potential segment, Chartem applies error correction to decode the segment and then checks if CRC is correct. If both are successful, Chartem determines that a segment is found.</p><p>Once all segments are determined, Chartem checks segments with the same segment ID. If two segments with the same segment ID have conflict, the one with the worse match of the header marker is dropped. Then Chartem determines the largest segment ID of data segments and the smallest segment ID of fountain segments to determine a potential range for the number of data segments. Chartem tries each value in the range to fountain-decode the payload data from survived segments. If the decoding is successful, the embedded data is successfully extracted, and the prefix length is used to determine the size of the input data. The extracted input data is then decompressed and output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION</head><p>In our evaluation, we first carried out a user study to understand user perception of embedded patterns with different weights. We then conducted experiments to assess Chartem's performance on embedding capacity, extraction accuracy, and execution time. In addition, we present a set of example results as supplementary material, which demonstrate that Chartem can support a variety types of chart designs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">User Study</head><p>Chartem embeds data into background regions of a chart image by adjusting background pixel values to form certain patterns to carry desired information. As described in Section 3.1.5, Chartem uses a parameter, weight, to determine the value Chartem moves a background pixel from the local average to carry information. Weight determines the distortion our data embedding brings to a chart image or equivalently the visibility of our embedded patterns. The higher the weight is, the more visible the embedded patterns are to humans, and at the same time the more robust of the embedding. We conducted a user study to understand human's tolerance of embedding distortions and their impact on aesthetics. We recruited 22 participants (12 males and 10 females, 21-62 years old, average age = 30.5) from a technology company and a university. The participants included undergraduate and graduate students, professors, data analysts, researchers, program managers, software engineers and salespersons. They were all general users who had more or less experience of reading chart images to understand data in their daily work and study. None of the participants reported vision impairment in viewing the content on chart images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Stimuli and Procedure</head><p>We prepared a set of 12 chart images shown in <ref type="figure" target="#fig_2">Fig. 5</ref> for this user study. These chart images were selected from the Internet to have a variety of chart designs. Specifically, they are of four chart types: bar charts, pie charts, line charts, and scatter plots. We consider these chart types because they are the most frequently used ones in real world <ref type="bibr" target="#b8">[9]</ref>. For each chart type, we chose three chart images with different sizes and chart styles. For example, we selected both vertical and horizontal bar charts, included donut charts in addition to normal pie charts, and covered not only single-series line charts but also multiple-series ones. For each chart, we applied Chartem to create eight embedded chart images, each embedded with the same data but using a different embedding weight, as shown in <ref type="figure" target="#fig_3">Fig. 6</ref> for a test pie chart. Specifically, we selected eight levels of weight (i.e., <ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr">63</ref>, and 93). These weights were selected using the following criterion: an added weight should have perceptual difference from its adjacent weight when examined closely. All embedded chart images, along with their original images, are included as supplemental material.</p><p>Participants performed 12 trials. In each trial, participants were first presented an original chart image, followed by eight embedded chart images with different embedding weights. We asked participants to rate each embedded chart image according to how much the embedding patterns on background impact the overall aesthetic of the chart. Participants responded using a 5-point Likert-scale ranging from "High Impact" to "No Impact At All". To avoid potential bias, the eight embedded chart images were shown in a random order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">User Study Results</head><p>We received a total of 2,112 ratings (22 participants × 12 charts × 8 embedding wights). <ref type="figure" target="#fig_4">Fig. 7</ref> shows 95% confidence intervals of mean ratings for different chart types at each of the 8 embedding weights.</p><p>When weight increases, we observe a decreasing trend of ratings for all the chart types, which verifies our hypothesis that a higher weight leads to a lower acceptance by users. Specifically, the two highest weights (i.e., 93 and 63) are not acceptable by participants with ratings about 2.0 or below, while the three lowest weights (i.e., <ref type="bibr">6, 11, and 17)</ref> all receive a rating above 3.5, indicating that they are well accepted by participants. The other three weights (i.e., 22, 27, and 33) sit in the marginal zone. The ratings on a same weight vary slightly with different chart types. Generally, the ratings for pie and bar charts are higher than line and scatter charts. This rating difference can be explained by the distinct characteristics of different chart types: pie and bar charts typically comprise large foreground components that quickly attract human's attention and are easier to understand, resulting in less attention paid to the background when they are viewed. Line charts and scatter plots, on the other hand, typically comprise smaller and scattered foreground components that interleave much more extensively with the background. A reader generally needs to pay more attention to identify foreground components and understand the content of a chart of these types, making embedding patterns more distractive.</p><p>The rating results show that when an appropriate weight is applied, embedding distortions are totally acceptable to users, and do not impact their effectiveness of reading chart images. Based on the results, we further adopt 17, the highest among all acceptable weights, as the default weight value used for Chartem data embedding. With this setting, we hope to achieve a good balance that embedded patterns are easy to detect by machines while remaining non-intrusive to readers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation of Robustness, Capacity, and Runtime</head><p>We have implemented Chartem in C++ based on OpenCV <ref type="bibr" target="#b17">[18]</ref> to evaluate the robustness, capacity, and runtime on the same set of chart images used for the user study. Although these test charts all have a popular white background, Chartem works on any chart with a flat background of any color distinctly different from foreground components. We include embedding examples of chart images with color background in the supplemental material. The results and conclusions obtained in this subsection are generally valid.</p><p>In our experiments, the default weight obtained from the user study described in Section 4.1.2 was used, and the block size for each logic bit described in Section 3.1.5 was set to 2 × 2, i.e., p = 2. In addition, the header marker was set to 16 bits, Wirehair <ref type="bibr" target="#b35">[37]</ref> was used as fountain codes to recover data segments from extracted segments, and the Reed-Solomon error correction <ref type="bibr" target="#b22">[24]</ref> was selected for error correction within a segment, with 5 bits for a symbol, 21 data symbols, and 10 parity symbols. With this setting, the error correction can correct 5 erroneous symbols or 10 erasure symbols, payload is of 11 bytes (i.e. 21 × 5 − 8 − 8 − 1 = 88 bits), a segment has 171 bits (i.e., 21 × 5 + 10 × 5 + 16 = 171), and the combination of ID and CRC is 16 bits in total. If we use 8 bits for segment ID and 8 bits for CRC, Chartem supports 256 distinct segments, and the maximum number of bits for input data (after compression) is thus 2814 (= 256 × 11 − 2) bytes. If larger input data needs to be supported, we can increase the size of segment ID, at the cost of reduced error detection capacity by CRC. For example, if we use 10 bits for segment ID and thus 6 bits for CRC, 1024 distinct segments can be supported, and input data in this case can be up to 56318 bytes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Robustness</head><p>We first conducted experiments to evaluate Chartem's robustness after typical operations on chart images, including scaling, screenshot, rotating, JPEG compression, and brightness variations. In these experiments, we generated random bits to embed without using any error or erasure correction or data packaging (i.e., all 171 bits in each segment were randomly generated) and then compared extracted bits with the embedded bits to determine correctly extracted bits. We use recall and precision to measure Chartem's robustness. Recall is defined as the number of correctly extracted bits divided by the total number of embedded bits. Precision is defined as the number of correctly extracted bits divided by the total number of extracted bits. The total number of bits embedded into each chart image is listed as raw capacity in <ref type="table" target="#tab_3">Table 3</ref> and will be described in Section 4.2.2. A data-embedded chart image was first saved into the PNG format. Then we extracted the embedded bits from the saved image. This was to test the robustness when a data-embedded chart image has not gone through any distortion yet. All the test chart images got 100.0% for both precision and recall.</p><p>The next experiment was to randomly scale up a data-embedded image, screenshot the scaled image using Snipping Tool in Windows <ref type="bibr" target="#b16">[17]</ref>, and extract the embedded bits from the captured image. This was to mimic a typical process in which a user captures a digitally published chart image, which may be scaled during the capturing process or after being published. Columns 2 and 3 of <ref type="table" target="#tab_2">Table 1</ref> show the obtained precision and recall for the test images. They all have 100.0% precision, and their recalls are close to 100.0%.</p><p>To test robustness against rotations, an image was rotated at an angle either anti-clockwise (a positive angle) or clockwise (a negative angle), displayed on a screen, and screenshot. Extraction was then applied to the screenshot image. Columns 4 to 7 of <ref type="table" target="#tab_2">Table 1</ref> show the results for each test chart image after rotating ±30 • . Both precision and recall are close to 100.0% for each test chart image.</p><p>To test robustness against JPEG compression, we used popular image viewer software IrfanView <ref type="bibr" target="#b25">[27]</ref> to convert an image into a JPEG compressed image at different quality-factor values. <ref type="table">Table 2</ref> shows the resulting precision and recall for each test chart image after JPEG compression with the quality factor set to 80 (IrfanView's default value), 70, and 60. We can see from the table that the precision remains at about 95% or above while the recall decreases to around 80% or below when the JPEG compression's quality factor is lowered from the default 80 to 60. If the block size of a logic bit is increased from the current 2 × 2 to 4 × 4, at the cost of reduced capacity, the precision and recall are both above 90% for each test chart image except images L03 and S03 even when the quality factor decreases to 25. At block size 4 × 4, images L03 and S03 cannot embed any data since neither one has a sufficiently large background region to embed a single block sequence.</p><p>To test robustness against brightness variations, we conducted two experiments. In one experiment, we linearly compressed pixel values towards either 0 or 255 to leave enough room to respectively add 140 to or subtract 140 from each pixel to mimic brightening or darkening an image. We got 100.0% for both precision and recall for all the test chart images. In the other experiments, we compressed pixel values in the same way to leave a room of 140 to add to or subtract from each pixel, and then adjusted the value to add to or subtract from each pixel in a linear manner along either horizontal or vertical direction such that one side was 0 and the other side was 140. This was to mimic gradual brightness changes. All the test chart images got 100.0% for both precision and recall except S01 with 99.99% precision and 99.75% recall when the value subtracted from each pixel changed linearly along the vertical direction from 0 at the top and 140 at the bottom, and S02 with 100.0% precision and 98.58% recall when the value added to each pixel changed linearly along the vertical direction from 0 at the top and 140 at the bottom. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Embedding Capacity</head><p>Another important performance metric is the amount of arbitrary binary data that can be embedded into a chart image, i.e., embedding capacity, which is inversely related to robustness studied in Section 4.2.1: increasing robustness generally reduces capacity, and vice versa. Embedding capacity of a chart image depends on the size and the distribution of foreground components of a host chart image. <ref type="table" target="#tab_3">Table 3</ref> shows the size, background ratio defined as the total number of background pixels divided by the image size, raw capacity, and input data capacity for scaling up and then screenshotting and JPEG at the default quality for each test chart image. Raw capacity is the total number of bits of all segments, including header and parity bits, embedded into a chart image, while input data capacity is the maximum number of bytes of arbitrary input data that can still be correctly extracted after the targeted processing. For the specific setting of the experiments described at the beginning of Section 4.2, a segment contains 171 raw bits but only 11 bytes of payload for input data. The latter is much smaller than the former due to error correction and header information of a segment. The capacity in <ref type="table" target="#tab_3">Table 3</ref> is for arbitrary binary input data, which is after compressing user data in practical applications. The actual amount of user data that can be embedded into a chart image depends on the compressibility of the user data. For text input, lossless compression can typically reduce to half of the original size, and thus the amount of data to be embedded would be twice the capacity shown in <ref type="table" target="#tab_3">Table 3</ref>.</p><p>As a rule of thumb, the larger the size of a chart image, the more data the chart image can host. For chart images of the same size, the higher ratio of background regions to the image size, the more embedded data. Since a block sequence has to be embedded into a continuous background region and a block sequence has a minimum of 3 blocks, start and end blocks and at least one data block, a chart image with large background regions can embed more data than a chart image with scattered small background regions when they have the same image size and background ratio. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Runtime</head><p>To measure runtime, we ran Chartem with a single thread on an ASUS FL8000 laptop with Intel i7-8550U CPU @1.80GHz and 8GB memory running 64-bit Windows 10. <ref type="table" target="#tab_4">Table 4</ref> and <ref type="table">Table 5</ref> show the obtained overall runtime and its breakdown by major modules for both embedding and extraction, respectively. The second column in both tables is the runtime for background detection. In <ref type="table" target="#tab_4">Table 4</ref>, the third column is the runtime for finding an embedding rectangle and embedding coarse and fine synchronization marks, and the fourth column for packaging and embedding data. The overall embedding time ranges from 0.276s to 0.758s for the test chart images. In <ref type="table">Table 5</ref>, the third column is the runtime for the adaptive binarization, the fourth column for detecting coarse and fine synchronization marks and registering the image, and the fifth column for extracting and unpacking data. The overall extraction time ranges from 1.020s to 2.754s. For both embedding and extraction, an image of a larger size and with more background pixels generally has a longer overall runtime. We note that the current implementation has not been optimized for execution time. There should be a significant room to speed up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">SAMPLE APPLICATIONS</head><p>In this section, we demonstrate three sample applications that leverage Chartem to create and consume charts with embedded information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Chart Creation in Excel</head><p>To take advantage of Chartem's ability to embed and extract information, the first step is to build a convenient tool to help users embed information into normal charts. One obvious choice is to build a standalone tool to ask users to directly provide a chart image and embed information into it. However, we believe this is not ideal in terms of user experience, as users need to leverage a proprietary tool. Instead, we aim to integrate Chartem smoothly into the workflow of general users. As a result, we have built an Excel add-in for Chartem, as Excel is a powerful and widely used platform for analyzing data and creating chart visualizations.</p><p>Specifically, users may follow their normal workflow to analyze data and create chart visualizations accordingly with all the built-in functions in Excel. Once a chart is created <ref type="figure" target="#fig_0">(Fig. 8(a1)</ref>), users can directly click the Chartem button in the ribbon area. Then a side panel <ref type="figure" target="#fig_1">(Fig. 8(a2)</ref>) will appear to help users embed information into the chart. In this prototype application, we allow two types of information, both are optional. The first one is the data table itself. Since Excel maintains the data model that drives the chart visualization, we can directly collect the data table from the model, instead of parsing the chart image. The second one is a textual description. We allow users to directly provide it in the side panel. Finally, we allow users to customize the embedding weight, although a default value is provided. Once users complete the configuration, they can click the Embed button, and an embedded version of the chart is created and previewed in a pop-up dialog. Then, users can either save the chart as an image to the file system or copy it to the clipboard for use in other applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Customize Charts in PowerPoint</head><p>By design, PowerPoint is able to host charts generated by Excel. By doing so, backend chart models can be maintained and used to support a wide range of follow-up actions. For example, users can directly revise the backend data, so that the chart visualization can be updated automatically. In addition, users can also change the chart type or style to make it more consistent with the theme of presentation. However, in many cases when the chart visualization is imported as an image, all these possibilities are lost.</p><p>In the second sample application, we demonstrate how to leverage embedded information to empower chart images with the same flexibility in PowerPoint. Specifically, we have built a PowerPoint add-in to convert a chart image generated by the previous Excel add-in to an active chart object. For example, users can directly drag-and-drop a chart image into PowerPoint <ref type="figure" target="#fig_0">(Fig. 8(b1)</ref>). Then, to further customize the chart visualization, they can simply click the Convert to Chart button. Our backend service first tries to recover all essential information from the image, such as the backend data table and chart configurations. If all the information exist, our add-in will replace the inserted image with its equivalent of chart object <ref type="figure" target="#fig_1">(Fig. 8(b2)</ref>), so that users can take advantage of the built-in features to freely customize the chart as needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Voice-Over on Mobile Phones</head><p>During the creation process, we allow users to embed a free-form text into a chart image, which can be used to serve different purposes. For example, we can integrate additional description to elaborate the chart or essential description to help visually impaired users.</p><p>A voice-over mobile app is illustrated in <ref type="figure" target="#fig_5">Fig. 8(c)</ref>. In this hypothetical scenario, users may use the camera on a mobile phone to scan a chart image <ref type="figure" target="#fig_0">(Fig. 8(c1)</ref>). Then the mobile app will try to extract the textual information embedded in the image, and use a text-to-speech program to convert the description to an audio clip and play it back <ref type="figure" target="#fig_1">(Fig. 8(c2)</ref>). Currently, a desktop version of the application is implemented, in which a chart image is loaded as a image file instead of captured using cameras. However, we believe it is promising to directly recover embedded information using a camera and a mobile app.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION 6.1 Machine Friendly Charts</head><p>As chart data is accumulating rapidly on the Internet, it has become an important topic for machines to interpret chart images. There are several intriguing motivations behind this idea. For example, users may need to reprocess the data behind a certain chart, restyle or index a chart, etc. However, since charts are originally designed to be read by humans, they are not easy for machines to interpret. Many sophisticated approaches have been proposed to involve computer vision and machine learning techniques. Although they have achieved promising preliminary results, there are still a lot of challenges to overcome in terms of robustness and accuracy. In this work, we try to address this issue from the root, i.e., directly creating charts that are friendly to both humans and machines. Specifically, we do not aim to change human's reading experience. People can use charts in any ways that they are used to. On the other hand, we piggyback information that can be efficiently and accurately consumed by machines on top of chart images.</p><p>There are two unique advantages of this approach. First, since the extracted information can be guaranteed completeness and accuracy, it is more robust and direct compared with previous machine-learningbased approaches. Second, since machines do not rely on chart visuals to collect any information, this method works for different chart types. Therefore, it has the potential to be a new form of charts to replace existing chart visualizations, as it maintains the human experience while providing opportunities for more applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Opportunities for New Applications</head><p>As discussed in Section 1 and Section 2.2, there are several ways to embed information. For example, we may directly overlay information (e.g., QR code) on a chart, insert information into image files, encode information using the frequency domain, etc. Among all these candidates, we choose to embed information into the background area of a chart for two reasons. First, we aim to minimize interruptions to the reading experience. According to the user study reported in Section 4.1.2, most participants felt comfortable when reading the charts generated by our method, since chart backgrounds are generally not their foci and our embedding patterns are barely noticeable. Second, the information needs to be associated with chart images instead of files, since charts may be screenshot or saved in different formats.</p><p>In addition, since the embedded information is highly customizable, it can provide more flexibility to downstream applications. We have illustrated two examples in Section 5. However, we believe there are much more scenarios that can take advantage of this technique. For example, creators can embed encrypted confidential information into a chart. Then, only authorized users can use a mobile app to scan the chart and provide a password to decrypt the extracted information. We can also use the technique to enable AR-like experiences. For example, users may use a mobile device to see animations by pointing the camera at a chart with proper information embedded, which is a valuable complement to traditional static charts.</p><p>However, to make general users benefit from our approach, it is required to vastly distribute the chart embedder and extractor. Ideally, they can be integrated into mainstream software, as demonstrated in Section 5. Otherwise, charts with embedded information simply regress to normal charts without providing any benefits at all. Considering this situation, we believe machine-learning-based approaches are still prevalent and valuable for the foreseeable future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Limitations</head><p>Chartem makes charts accessible to machines, which may greatly expand the scope of applications. At the same time, it is also subject to several restrictions and limitations.</p><p>The first limitation is about embedding capacity. Chartem requires a minimum background size to embed information. A valid embedding pattern includes at least three coarse marks, three fine marks, and a block sequence of a minimum of 3 blocks (for data, start, and end, respectively). This requires a minimum background size to embed. When a chart image does not meet this minimum requirement, no data can be embedded at all. In addition, if background regions are too small, it may also fail to embed all desired data. There are several ways to address the insufficient capability issue. For example, it is possible to combine the technique used in <ref type="bibr" target="#b14">[15]</ref> by also embedding data at boundaries of foreground components to complement Chartem's background embedding. In addition, if the foreground also contains large areas of solid colors (e.g., in a typical treemap), we can also embed into foreground regions with a control of its embedding noise to make the perceptual quality acceptable for targeted applications. This foreground embedding complements Chartem's background embedding well and can significantly increase the embedding capacity. Finally, it is also possible to store the actual information in the cloud and embed the corresponding url address in the chart image instead. However, this approach requires Internet access when extracting information.</p><p>The second limitation is related to information robustness. Chartem embeds a bit by adjusting a block of pixels above or below the local average. Extracting the bit requires estimating the local average. To reduce estimation error, Chartem requires that background should be relatively smooth locally. If background of a chart image is not smooth, the estimated local average may be significantly affected by distortions to local pixels brought by an operation on the chart image such as scaling and thus inaccurate, hence damaging the information robustness. However, charts in the real world may have a much more complex or hostile background. For example, they may have noisy backgrounds or natural images as backgrounds, which are difficult for Chartem to embed information since Chartem is designed for charts with homogeneous backgrounds. For such complex backgrounds, traditional image-data-embedding methods can be adopted to embed information into background regions. Traditional image data embedding complements Chartem well for various backgrounds charts may use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We presented a novel solution, Chartem, to unlock information locked in charts, typically published in bitmap images that are unfriendly to machines, and enrich chart applications. Chartem is based on data embedding: chart data and information and/or generic data that enriches user experiences can be embedded into background regions of a chart image. Foreground regions are untouched to ensure a good perceptual quality after embedding yet maintain a large capacity and good robustness. Our user study and performance experiments indicate that data-embedded chart images are well accepted and Chartem is robust with relatively high capacity. We presented several prototype applications to demonstrate the utility of Chartem. In addition to extracting chart data and information to revive a chart image, Chartem opens the door for many more potential applications around chart images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Chartem Overview. Chartem embeds a piece of application-dependent digital information such as a chart specification (f) into a chart image (a). It detects background regions of the chart (b), embeds coarse marks (A), fine marks (B), and packaged information into the background regions (c and d)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The structure of a segment. Header consists of a marker (a), a segment ID (b), CRC (c), and a status bit (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Chart images used in our user study and experiments, including bar charts (B01-B03), line charts (L01-L03), pie charts (P01-P03), and scatter plots (S01-S03).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Embedding results with different weights. (a): original chart image; (b)-(i): data-embedded chart images with weight = 6, 11, 17, 22, 27, 33, 63, and 93, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Mean ratings and 95% confidence intervals per embedding wight and per chart type, calculated via bootstrap (1 = High Impact; 2 = Impact; 3 = Neural; 4 = Not Impact; 5 = Not Impact At All).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Sample applications: (a) A Excel add-in to help users embed information into typical charts; (b) A PowerPoint add-in to help users convert chart images into chart objects; (c) A voice-over mobile app that reads embedded information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Precision and recall in percentage (%) of test chart images(Fig. 5) after randomly scaling up and then screenshotting (S+S) and rotations</figDesc><table><row><cell>Chart</cell><cell>S+S Precision</cell><cell>Recall</cell><cell cols="2">Rotating 30 • Precision Recall</cell><cell cols="2">Rotating −30 • Precision Recall</cell></row><row><cell>B01</cell><cell>100.0</cell><cell>99.96</cell><cell>99.73</cell><cell>98.33</cell><cell>99.77</cell><cell>99.77</cell></row><row><cell>B02</cell><cell>100.0</cell><cell>99.97</cell><cell>98.25</cell><cell>94.21</cell><cell>99.72</cell><cell>97.03</cell></row><row><cell>B03</cell><cell>100.0</cell><cell>99.51</cell><cell>99.67</cell><cell>99.81</cell><cell>99.78</cell><cell>99.78</cell></row><row><cell>L01</cell><cell>100.0</cell><cell>99.96</cell><cell>99.59</cell><cell>99.60</cell><cell>99.71</cell><cell>99.71</cell></row><row><cell>L02</cell><cell>100.0</cell><cell>99.10</cell><cell>99.66</cell><cell>99.07</cell><cell>99.67</cell><cell>99.60</cell></row><row><cell>L03</cell><cell>100.0</cell><cell>99.36</cell><cell>99.39</cell><cell>98.67</cell><cell>99.34</cell><cell>99.30</cell></row><row><cell>P01</cell><cell>100.0</cell><cell>99.48</cell><cell>99.64</cell><cell>98.89</cell><cell>99.63</cell><cell>99.63</cell></row><row><cell>P02</cell><cell>100.0</cell><cell>99.94</cell><cell>99.72</cell><cell>99.75</cell><cell>99.73</cell><cell>99.73</cell></row><row><cell>P03</cell><cell>100.0</cell><cell>100.0</cell><cell>99.63</cell><cell>99.63</cell><cell>99.66</cell><cell>99.66</cell></row><row><cell>S01</cell><cell>100.0</cell><cell>99.98</cell><cell>99.27</cell><cell>99.27</cell><cell>99.35</cell><cell>99.35</cell></row><row><cell>S02</cell><cell>100.0</cell><cell>99.99</cell><cell>99.56</cell><cell>99.16</cell><cell>99.41</cell><cell>95.88</cell></row><row><cell>S03</cell><cell>100.0</cell><cell>99.90</cell><cell>99.60</cell><cell>99.60</cell><cell>98.41</cell><cell>99.48</cell></row><row><cell cols="7">Table 2. Precision and recall in percentage (%) of test chart images (Fig.</cell></row><row><cell cols="6">5) after JPEG compression with different quality-factor values</cell><cell></cell></row><row><cell>Chart</cell><cell cols="2">80 (default) Precision Recall</cell><cell>70 Precision</cell><cell>Recall</cell><cell>60 Precision</cell><cell>Recall</cell></row><row><cell>B01</cell><cell>99.16</cell><cell>97.18</cell><cell>97.30</cell><cell>92.29</cell><cell>95.07</cell><cell>88.89</cell></row><row><cell>B02</cell><cell>99.18</cell><cell>98.00</cell><cell>97.92</cell><cell>97.22</cell><cell>95.96</cell><cell>86.91</cell></row><row><cell>B03</cell><cell>99.10</cell><cell>95.09</cell><cell>97.08</cell><cell>91.88</cell><cell>94.97</cell><cell>81.58</cell></row><row><cell>L01</cell><cell>99.19</cell><cell>98.21</cell><cell>97.39</cell><cell>91.20</cell><cell>95.31</cell><cell>81.08</cell></row><row><cell>L02</cell><cell>99.24</cell><cell>97.66</cell><cell>97.56</cell><cell>92.58</cell><cell>95.22</cell><cell>81.59</cell></row><row><cell>L03</cell><cell>99.20</cell><cell>97.38</cell><cell>97.66</cell><cell>87.15</cell><cell>95.74</cell><cell>78.35</cell></row><row><cell>P01</cell><cell>99.23</cell><cell>99.07</cell><cell>97.76</cell><cell>94.02</cell><cell>95.66</cell><cell>84.41</cell></row><row><cell>P02</cell><cell>99.28</cell><cell>98.92</cell><cell>97.66</cell><cell>89.71</cell><cell>95.87</cell><cell>84.85</cell></row><row><cell>P03</cell><cell>99.20</cell><cell>94.26</cell><cell>97.48</cell><cell>93.83</cell><cell>95.14</cell><cell>87.60</cell></row><row><cell>S01</cell><cell>99.06</cell><cell>98.41</cell><cell>97.35</cell><cell>91.94</cell><cell>95.96</cell><cell>83.67</cell></row><row><cell>S02</cell><cell>99.25</cell><cell>96.59</cell><cell>97.56</cell><cell>92.05</cell><cell>98.25</cell><cell>82.36</cell></row><row><cell>S03</cell><cell>99.12</cell><cell>96.43</cell><cell>97.85</cell><cell>92.72</cell><cell>95.97</cell><cell>83.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>The image size, background ratio, and embedding capacity for each test chart image(Fig. 5) (where S+S means randomly scaling up followed by screenshotting and JPEG is of the default quality)</figDesc><table><row><cell>Chart</cell><cell>Size</cell><cell>Backgrd Ratio</cell><cell>Raw</cell><cell cols="2">Capacity Input Data (bytes)</cell></row><row><cell></cell><cell>(pixels)</cell><cell>(%)</cell><cell>(bits)</cell><cell>S+S</cell><cell>JPEG</cell></row><row><cell>B01</cell><cell>480 × 480</cell><cell>84.06</cell><cell>30911</cell><cell>1978</cell><cell>1945</cell></row><row><cell>B02</cell><cell>750 × 563</cell><cell>81.42</cell><cell>46375</cell><cell>2979</cell><cell>2946</cell></row><row><cell>B03</cell><cell>791 × 444</cell><cell>62.43</cell><cell>24871</cell><cell>1560</cell><cell>1505</cell></row><row><cell>L01</cell><cell>1000 × 600</cell><cell>89.78</cell><cell>60023</cell><cell>3859</cell><cell>3738</cell></row><row><cell>L02</cell><cell>700 × 525</cell><cell>94.44</cell><cell>46487</cell><cell>2935</cell><cell>2649</cell></row><row><cell>L03</cell><cell>600 × 467</cell><cell>81.19</cell><cell>21575</cell><cell>1351</cell><cell>1307</cell></row><row><cell>P01</cell><cell>619 × 591</cell><cell>90.54</cell><cell>53879</cell><cell>3430</cell><cell>3155</cell></row><row><cell>P02</cell><cell>805 × 511</cell><cell>71.54</cell><cell>51511</cell><cell>3309</cell><cell>2968</cell></row><row><cell>P03</cell><cell>721 × 786</cell><cell>46.88</cell><cell>34767</cell><cell>2231</cell><cell>2187</cell></row><row><cell>S01</cell><cell>674 × 424</cell><cell>89.52</cell><cell>22383</cell><cell>1428</cell><cell>1406</cell></row><row><cell>S02</cell><cell>900 × 604</cell><cell>88.48</cell><cell>75215</cell><cell>4827</cell><cell>4596</cell></row><row><cell>S03</cell><cell>595 × 404</cell><cell>77.55</cell><cell>10879</cell><cell>691</cell><cell>647</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Execution time (s) for embedding: overall and major modules</figDesc><table><row><cell></cell><cell>Chart</cell><cell cols="2">Backgrd Detection</cell><cell cols="2">Sync Marks</cell><cell>Data</cell><cell>Overall</cell></row><row><cell></cell><cell>B01</cell><cell>0.128</cell><cell></cell><cell cols="2">0.070</cell><cell>0.062</cell><cell>0.276</cell></row><row><cell></cell><cell>B02</cell><cell>0.258</cell><cell></cell><cell cols="2">0.151</cell><cell>0.120</cell><cell>0.552</cell></row><row><cell></cell><cell>B03</cell><cell>0.209</cell><cell></cell><cell cols="2">0.105</cell><cell>0.084</cell><cell>0.421</cell></row><row><cell></cell><cell>L01</cell><cell>0.317</cell><cell></cell><cell cols="2">0.142</cell><cell>0.144</cell><cell>0.636</cell></row><row><cell></cell><cell>L02</cell><cell>0.357</cell><cell></cell><cell cols="2">0.101</cell><cell>0.092</cell><cell>0.572</cell></row><row><cell></cell><cell>L03</cell><cell>0.324</cell><cell></cell><cell cols="2">0.103</cell><cell>0.065</cell><cell>0.519</cell></row><row><cell></cell><cell>P01</cell><cell>0.276</cell><cell></cell><cell cols="2">0.079</cell><cell>0.097</cell><cell>0.476</cell></row><row><cell></cell><cell>P02</cell><cell>0.355</cell><cell></cell><cell cols="2">0.067</cell><cell>0.110</cell><cell>0.558</cell></row><row><cell></cell><cell>P03</cell><cell>0.328</cell><cell></cell><cell cols="2">0.174</cell><cell>0.134</cell><cell>0.670</cell></row><row><cell></cell><cell>S01</cell><cell>0.306</cell><cell></cell><cell cols="2">0.066</cell><cell>0.070</cell><cell>0.463</cell></row><row><cell></cell><cell>S02</cell><cell>0.451</cell><cell></cell><cell cols="2">0.124</cell><cell>0.153</cell><cell>0.758</cell></row><row><cell></cell><cell>S03</cell><cell>0.239</cell><cell></cell><cell cols="2">0.097</cell><cell>0.050</cell><cell>0.408</cell></row><row><cell cols="7">Table 5. Execution time (s) of overall and major modules for extracting</cell></row><row><cell cols="7">data from chart images after scaling up by 20% and then screenshotting</cell></row><row><cell>Chart</cell><cell cols="2">Backgrd Detection</cell><cell cols="2">Binarization</cell><cell cols="2">Sync Marks</cell><cell>Data</cell><cell>Overall</cell></row><row><cell>B01</cell><cell></cell><cell>0.068</cell><cell cols="2">0.337</cell><cell cols="2">0.352</cell><cell>0.241</cell><cell>1.020</cell></row><row><cell>B02</cell><cell></cell><cell>0.243</cell><cell cols="2">0.611</cell><cell cols="2">0.595</cell><cell>0.396</cell><cell>1.876</cell></row><row><cell>B03</cell><cell></cell><cell>0.121</cell><cell cols="2">0.382</cell><cell cols="2">0.348</cell><cell>0.300</cell><cell>1.174</cell></row><row><cell>L01</cell><cell></cell><cell>0.172</cell><cell cols="2">0.925</cell><cell cols="2">0.900</cell><cell>0.635</cell><cell>2.666</cell></row><row><cell>L02</cell><cell></cell><cell>0.278</cell><cell cols="2">0.597</cell><cell cols="2">0.675</cell><cell>0.392</cell><cell>1.968</cell></row><row><cell>L03</cell><cell></cell><cell>0.421</cell><cell cols="2">0.410</cell><cell cols="2">0.413</cell><cell>0.257</cell><cell>1.529</cell></row><row><cell>P01</cell><cell></cell><cell>0.109</cell><cell cols="2">0.562</cell><cell cols="2">0.627</cell><cell>0.392</cell><cell>1.714</cell></row><row><cell>P02</cell><cell></cell><cell>0.249</cell><cell cols="2">0.502</cell><cell cols="2">0.553</cell><cell>0.385</cell><cell>1.716</cell></row><row><cell>P03</cell><cell></cell><cell>0.143</cell><cell cols="2">0.354</cell><cell cols="2">0.335</cell><cell>0.433</cell><cell>1.294</cell></row><row><cell>S01</cell><cell></cell><cell>0.334</cell><cell cols="2">0.454</cell><cell cols="2">0.388</cell><cell>0.296</cell><cell>1.498</cell></row><row><cell>S02</cell><cell></cell><cell>0.402</cell><cell cols="2">0.832</cell><cell cols="2">0.902</cell><cell>0.587</cell><cell>2.754</cell></row><row><cell>S03</cell><cell></cell><cell>0.435</cell><cell cols="2">0.353</cell><cell cols="2">0.217</cell><cell>0.207</cell><cell>1.234</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hiding images in plain sight: Deep steganography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2069" to="2079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Understanding infographics through textual and visual tag prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alsheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Madan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.09215</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lossless generalizedlsb data embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">U</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Tekalp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Saber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="253" to="266" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Digital image steganography: Survey and analysis of current methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cheddad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Condell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mc Kevitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal processing</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="727" to="752" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards automated infographic design: Deep learning-based auto-extraction of extensible timeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="917" to="926" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Visualizing for the non-visual: Enabling the visually impaired to use visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="249" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scatteract: Automated extraction of data from scatter plots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cliche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Madeka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="135" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lossless data embedding-new paradigm in digital watermarking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Advances in Signal Processing</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">986842</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Converting basic d3 charts into reusable style templates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1274" to="1286" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Chartsense: Interactive data extraction from chart images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 chi conference on human factors in computing systems</title>
		<meeting>the 2017 chi conference on human factors in computing systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6706" to="6717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dvqa: Understanding data visualizations via question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5648" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A novel high-capacity data-embedding system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Tewfik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2431" to="2440" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luby</surname></persName>
		</author>
		<title level="m">The 43rd Annual IEEE Symposium on Foundations of Computer Science</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
	<note>LT codes</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Madan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alsheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.10441</idno>
		<title level="m">Synthetically trained icon proposals for parsing and summarizing infographics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A watermarking algorithm for map and chart images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Masry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Security, Steganography, and Watermarking of Multimedia Contents VII</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">5681</biblScope>
			<biblScope unit="page" from="495" to="503" />
		</imprint>
	</monogr>
	<note>International Society for Optics and Photonics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ivolver: Interactive visual language for visualization extraction and reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">G</forename><surname>Méndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Nacenta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vandenheste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2016 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4073" to="4085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Use snipping tool to capture screenshots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Microsoft</surname></persName>
		</author>
		<ptr target="https://support.microsoft.com/en-us/help/13776/windows-10-use-snipping-tool-to-capture-screenshots" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Opencv Team</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Opencv</surname></persName>
		</author>
		<ptr target="https://opencv.org/" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Using high-dimensional image models to perform highly undetectable steganography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pevnỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Filler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Information Hiding</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="161" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reverse-engineering visualizations: Recovering visual encodings from chart images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Poco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="353" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Extracting and retargeting color mappings from bitmap images of visualizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Poco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mayhua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="637" to="646" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The analysis of cell images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Prewitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Mendelsohn</surname></persName>
		</author>
		<ptr target="https://www.qrcode.com/en/history/.Lastaccessed" />
	</analytic>
	<monogr>
		<title level="j">Annals of the New York Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1035" to="1053" />
			<date type="published" when="1966-04-28" />
		</imprint>
	</monogr>
	<note>23] QR Code.com. History of QR code</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Polynomial codes over certain finite fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the society for industrial and applied mathematics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="300" to="304" />
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Revision: Automated classification, analysis and redesign of chart images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chhajta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th annual ACM symposium on User interface software and technology</title>
		<meeting>the 24th annual ACM symposium on User interface software and technology</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="393" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Figureseer: Parsing result-figures in research papers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="664" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Irfanview graphic viewer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Skiljan</surname></persName>
		</author>
		<ptr target="https://www.irfanview.com" />
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note>version 4.54</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multimedia dataembedding and watermarking technologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Tewfik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1064" to="1087" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multiresolution video watermarking using perceptual models and scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Tewfik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Image Processing</title>
		<meeting>International Conference on Image Processing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="558" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Object-based transparent video watermarking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Tewfik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of First Signal Processing Society Workshop on Multimedia Signal Processing</title>
		<meeting>First Signal Processing Society Workshop on Multimedia Signal Processing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="369" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robust data hiding for images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Tewfik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Digital Signal Processing Workshop Proceedings</title>
		<imprint>
			<biblScope unit="page" from="37" to="40" />
			<date type="published" when="1996" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Transparent robust image watermarking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Tewfik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 3rd IEEE International Conference on Image Processing</title>
		<meeting>3rd IEEE International Conference on Image Processing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Data hiding for video-in-video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Tewfik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Image Processing</title>
		<meeting>International Conference on Image Processing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="676" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Audio watermarking and data embedding-current state of the art, challenges and future directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Tewfik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia and Security Workshop at ACM Multimedia</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">41</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Robust audio watermarking using perceptual masking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Tewfik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Boney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal processing</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="337" to="355" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Stegastamp: Invisible hyperlinks in physical photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2117" to="2126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Wirehair -fast and portable fountain codes in c</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Taylor</surname></persName>
		</author>
		<ptr target="https://github.com/catid/wirehair/" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Reversible data embedding using a difference expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on circuits and systems for video technology</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="890" to="896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">The visual display of quantitative information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Tufte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>CT</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A digital watermark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Van Schyndel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Z</forename><surname>Tirkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Osborne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 1st international conference on image processing</title>
		<meeting>1st international conference on image processing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1994" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="86" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Information technology automatic identification and data capture techniques qr code bar code symbology specification. International Organization for Standardization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wave</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">18004</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Light field messaging with deep photographic steganography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wengrowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1515" to="1524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cuesta-Infante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Veeramachaneni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.03892</idno>
		<title level="m">Steganogan: High capacity image steganography with gans</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Hidden: Hiding data with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="657" to="672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multiresolution watermarking for images and video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Q</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on circuits and systems for video technology</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="545" to="550" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
