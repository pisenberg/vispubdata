<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VisCode: Embedding Information in Visualization Images using Encoder-Decoder Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiying</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhui</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changbo</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Visualization</forename><surname>Retargeting</surname></persName>
						</author>
						<title level="a" type="main">VisCode: Embedding Information in Visualization Images using Encoder-Decoder Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Metadata 1. Url 2. Designer 3. Revision Log Information visualization</term>
					<term>information steganography</term>
					<term>autocoding</term>
					<term>saliency detection</term>
					<term>visualization retargeting</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Fig. 1. VisCode can be used in a variety of practical applications. The input is a static visualization image. According to the encoding rules, we can retarget the original visual style after performing the decoding operation. Visualization retargeting includes representation (a) and theme color (b). VisCode can be also applied to metadata embedding as shown in (c) and large-scale information embedding. After decoding the source code into a static image, we can quickly build a visualization application and apply a common interaction such as selection as shown in (d).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>For a long time, information visualizations were created by artists, designers or programmers, and most of them were disseminated in the form of bitmap images. Many researchers have performed many studies on specific aspects of the design of information representations, such as the visualization layout, the color theme, and more specific explorations. However, with the development of fast network technology, the spread of visualization images has encountered two problems. One is that the copyright problem for visualization has not been effectively solved. Most designers use explicit icons to present copyright information, but this method affects the aesthetics of a chart and can obstruct important information. Moreover, when a visualization is converted into a bitmap image, the data in the vi- sualization are converted into pixels, and the original data are completely lost. Although several researchers, such as Poco et al. <ref type="bibr" target="#b39">[40]</ref>, have applied a pattern recognition approach to visualization images to retarget the color theme, the effectiveness of data recognition is still related to the form of visualization. As an alternative approach, information steganography <ref type="bibr" target="#b12">[13]</ref> (a means of implicit information embedding) for visualization images has potential for copyright protection and source data preservation.</p><p>Information steganography has a very meaningful practical application in the field of visualization design. In many forms of information propagation, such as social networks, blogs, and magazines, bitmap images are used most frequently and do not require the support of an additional web server. After conversion into an image, the original code used to generate the image is lost, thus increasing the difficulty of modification and visualization redesign. Similar difficulties also arise when designers and programmers collaborate. However, if a designer could implicitly embed the key visualization code into an image during the visualization design period, then the dissemination and revision of the visualized information would not require two different types of media (image and text), which would help programmers integrate such visualizations into big data analysis systems. Another potential application is the visualization retargeting. Visualization retargeting is very useful for the creation of visualizations. On the one hand, it reduces the designer's workload. On the other hand, it broadens the artist's creative space. <ref type="figure">Fig. 1</ref> shows four practical applications of our work.</p><p>Information steganography is different from digital watermarking, which is a technique for hiding information in a piece of media (usually an image). As described in the book of Cox et al. <ref type="bibr" target="#b12">[13]</ref>, steganography has a long history. Researchers have proposed a variety of methods of embedding information in images for diverse steganographic applications. As surveyed by Ghadirli et al. <ref type="bibr" target="#b16">[17]</ref>, the techniques used in image steganography include chaos-based methods, permutation-based methods, frequency-domain methods, hashbased methods, and evolutionary methods, among others. The continuous development of steganographic technology and the recent emergence of deep learning technology have enabled the extension of image steganography into broader areas of application. To our knowledge, however, few researchers have studied how to embed information in visualization images. Related research has focused only on the recognition of visualization images <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b38">39]</ref>. Although many methods of steganography have been proposed for natural images, the features of data visualizations are quite distinct. First, most of these models are based on natural images and make use of rich and mature features, while visualization charts usually have clean backgrounds and clear visual elements, increasing the difficulty of encoding and decoding. Second, Bylinskii et al. <ref type="bibr" target="#b10">[11]</ref> demonstrated that the visual importance of visualizations depends not only on the image context but also on higher-level factors (e.g., semantic categories), which is different from that of natural images.</p><p>We present a novel information embedding approach for visualization images, called VisCode. VisCode is an end-to-end autoencoding method that implicitly encodes information into a visualization image with little visual error. First, we propose to use QR codes as a form of media in which to store information to increase the success rate of decoding. We create a dataset consisting of visualization chart images and QR codes to be used for autoencoder training. Second, we outline a deep neural network architecture based on saliency detection that is designed to perform stable information steganography for visualization images. Information in the form of QR codes is adaptively encoded into the visually nonsignificant areas of a visualization image by means of a saliency-based layout algorithm to reduce the visual traces of this encoding. We also report the successful application of the VisCode method to three practical application scenarios: metadata embedding, source code embedding, and visualization retargeting. Based on the embedded information, the designer can easily manage and modify a graphical chart produced using this method, for example, to generate different versions of the visual design. Additionally, our VisCode system can convert static visualization images into interactive images to facilitate more accurate visualization comprehension. Furthermore, our tool supports visualization retargeting and improvement in accordance with user preferences, such as modification of the color palette style and the form of visualization.</p><p>Our experimental results and evaluations show that VisCode has great potential for application in information visualization. In summary, our contributions include three aspects:</p><p>(1) We define the problem of information embedding in the context of information visualization. We verify the practical importance of this study in pioneering a new application domain. <ref type="bibr" target="#b1">(2)</ref> We outline a deep learning framework that can be used to implement personalized steganography for information visualization images. This framework can achieve high-quality large-scale information hiding. <ref type="formula" target="#formula_2">3</ref>We present a set of evaluations to demonstrate the effectiveness of our proposed framework from multiple perspectives, including user friendliness, indices of encoding quality and decoding success, steganography defense, and time performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Information Steganography</head><p>Information steganography, the art of concealing secret data within other carrier data, has numerous applications (e.g., digital watermarking, covert transmission, and access control). The most popular form of steganography is to embed secret information by slightly perturbing carrier images, videos, 3D models, texts, or audio data while ensuring that these perturbations are imperceptible to the human visual system. Instead of processing popular forms of media such as videos and images, COBRA <ref type="bibr" target="#b20">[21]</ref> encodes data into a special type of colorful 2D barcode to improve the efficiency of transmission between a smartphone screen and camera. Yang et al. <ref type="bibr" target="#b56">[57]</ref> outlined an effective steganography algorithm using a 3D geometric model as the carrier. Their algorithm hides information in the histogram of the vertex coordinates of the 3D model and can achieve robust performance. Jo et al. <ref type="bibr" target="#b26">[27]</ref> utilized light signals from displays and cameras to transmit information. Xiao et al. <ref type="bibr" target="#b55">[56]</ref> proposed an interesting method of directly embedding messages into the letters of text. The glyphs of fonts are perturbed to encode secret information, and this information is decoded by recognizing characters by means of an optical character recognition (OCR) library. However, this method has two limitations: different perturbation rules are needed when processing different font types, and sufficient resolution of the text document is necessary when decoding a message. Hota and Huang <ref type="bibr" target="#b3">[4]</ref> adopt an image watermarking technique to embed the information into a scientific visualization. It seems that their work is similar to VisCode. However, the variety of data types, representations, and propagations of the information visualizations pose new challenges. The steganography work of information visualization is different from scientific visualization. In summary, previous research on information steganography has investigated a variety of carriers, but there is less previous study on information steganography for information visualization. In the information visualization field, steganographic applications of personalized information visualization require a specially designed information steganography approach that is different from previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Image Steganography</head><p>Traditional Steganography Images are the most widely processed and exchanged information carriers in the steganographic domain. Early methods used spatial-domain techniques, such as bit plane complexity segmentation (BPCS) <ref type="bibr" target="#b27">[28]</ref> or least-significant-bit (LSB) substitution <ref type="bibr" target="#b32">[33]</ref>. These methods involve applying small variations to pixel bits, which are not visually obvious. However, they are easily detected by statistical analysis due to their fixed principles of operation. On the basis of digital signal processing theory, some approaches have been developed for embedding messages in the frequency domain. Almohammad et al. <ref type="bibr" target="#b2">[3]</ref> hid information based on the discrete cosine transform (DCT). To avoid detection through statistical analysis, many steganography algorithms define special functions to optimize the localization of embeddings. Highly undetectable steganography (HUGO) <ref type="bibr" target="#b36">[37]</ref> minimizes distortion by calculating a variable weight for each pixel based on high-dimensional image models. The Wavelet Obtained Weights (WOW) algorithm <ref type="bibr" target="#b23">[24]</ref> measures the textural complexity of different image regions with directional filters. Deep Steganography Recent studies have achieved impressive results by combining deep learning with steganography. Pibre et al. <ref type="bibr" target="#b37">[38]</ref> demonstrated that the results obtained from deep neural networks (DNNs) surpass the conventional results based on handcrafted image features. Several methods utilize a DNN as a component in combination with traditional steganography techniques <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b47">48]</ref>. More recently, end-to-end steganography networks have become popular. Inspired by autoencoders, Baluja et al. <ref type="bibr" target="#b4">[5]</ref> learned feature representations to embed a secret image into a cover image. Hayes et al. <ref type="bibr" target="#b21">[22]</ref> utilized adversarial training to generate steganography results. Some studies have also considered sources of corruption in digital transmission, such as JPEG compression <ref type="bibr" target="#b60">[61]</ref> and light field messaging (LFM) <ref type="bibr" target="#b52">[53]</ref>. Most of these models are based on natural images and make use of rich and mature features. We focus on encoding a large amount of data into images designed for visualization purposes while preserving their visual perceptual quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Retargeting of Information Visualization</head><p>Early work on information visualization focused on data representations. Tufte <ref type="bibr" target="#b48">[49]</ref> introduced the basic principles of quantitative chart design, emphasizing that the purpose of chart design is to allow users to quickly obtain rich and accurate information and, consequently, decoration that is not related to the data is unnecessary. Tufte's research illustrates the importance of data and the user's understanding of charts in information visualization. In contrast to Tufte, Wilkinson <ref type="bibr" target="#b53">[54]</ref> offered a method of describing charts at a higher level of abstraction. This abstract method involves categorizing charts based on the visual characteristics of the data and geometric figures to simplify the architecture of the drawing system. In recent years, scattering, network diagrams, regions, dynamics, metaphors, tools, and ( {"DataType": "CopyRightInfo", "CreatedDate": "2020-03-xx", "ModifiedDate": "2020-03-xx", "Owners": "xxx", "Size": "1.3MB", "DetailedData": [{"DataType": "LineChart", "DataIndex": 0, "Xaxis":{"Year": 1947}, " Yaxis":{"Value": 0} } …], … , }…) Information ( {"DataType": "CopyRightInfo", "CreatedDate": "2020-03-xx", "ModifiedDate": "2020-03-xx", "Owners": "xxx", "Size": "1.3MB", "DetailedData": [{"DataType": "LineChart", "DataIndex": 0, "Xaxis":{"Year": 1947}, " Yaxis":{"Value": 0} } …], … , }…)</p><p>Encoded Image . The main components of our VisCode system. First, the visual importance network (b) processes the input graphical chart (a), which can facilitate salient feature detection of the visualization and output a visual importance map. Next, an encoder network (c) embeds secret information in the graphical chart (a). The carrier chart image and the QR code image are embedded into vectors by the feature extraction process. Then, these two vectors are concatenated (as the yellow rectangle shown) and fed into the auto-encoding stage. The encoded image (d) is then sent to the user (e), and the user can send it to others by digital transmission. When a user wishes to obtain the detailed information hidden in the chart, the encoded image can be uploaded to the decoder network (f). After data recovery and error correction, the user receives the decoded information (g). other visualization techniques have been proposed, and the capabilities of information visualization are becoming increasingly close to ideal. With the development of big data and deep learning technologies, visualization retargeting approaches have been presented to aid in visualization redesign. Poco et al. <ref type="bibr" target="#b39">[40]</ref> outlined an approach for extracting color mappings from static visualization images. They evaluated their technique on a variety of visualization types. Later, an end-to-end system was proposed by Poco and Heer <ref type="bibr" target="#b38">[39]</ref> for extracting information from static chart images. Similar works include iVoLVER <ref type="bibr" target="#b31">[32]</ref> and Chart Decoder <ref type="bibr" target="#b13">[14]</ref>, and Enamul and Maneesh <ref type="bibr" target="#b15">[16]</ref> have similarly offered a deconstruction tool for extracting data and visual encodings from D3 charts. In addition to common usage, functionalities intended to support visually impaired users have also been considered; for example, Choi et al. <ref type="bibr" target="#b11">[12]</ref> presented a DNN-based method of extracting the features of a chart to help visually impaired users comprehend charts.</p><p>To the best of our knowledge, less previous work has addressed steganography in the context of information visualization. In this paper, we implement visualization retargeting from a novel perspective using information steganography.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OVERVIEW</head><p>An essential goal of data visualization is to improve the cognition and perception of data. Compared with the direct transmission of a large amount of raw data, images are more easily and widely processed carriers for disseminating visual information on the Internet. However, after the original detailed information (e.g., data points, axis text, and color legends) is converted into pixel values, it is difficult to reconstruct the original information from the resulting images. To solve this problem, we attempt to encode additional information into the carrier visual designs with minimal perturbation.</p><p>In this paper, we propose VisCode, a framework for embedding and recovering hidden information in a visualization image using an encoderdecoder network. <ref type="figure" target="#fig_1">Fig. 2</ref> shows the VisCode model, which has 3 main components:</p><p>• Visual Importance Network The preparation stage includes a visual importance network and a text transformation model. Since the semantic features of data visualizations are quite different from those of natural images, we use a network to model the visual importance of the input data graph. The resulting importance map is applied as a constraint in the encoder network. Accounting for error correction, we convert the plain message to be embedded into a series of QR codes instead of binary data.</p><p>• Encoder Network The encoder network embeds the message within the carrier image and outputs the resulting coded image. In this network component, the carrier chart image and the QR code image are embedded into vectors by the feature extraction process. Then, these two vectors are concatenated and fed into an autoencoder. The resolution of the output coded image is the same as that of the input carrier image.</p><p>• Decoder Network The decoder network retrieves the information from the coded image. Users can share coded images with others through digital transmission. After such a coded image is decoded, the final information is obtained from the decoded QR code image with error correction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Definition</head><p>Our image (also called the secret image). The output of the decoder network is I s , and T s = PostQR(I s ), where PostQR()is the process of converting a recovered QR code image into text form with an error-correction coding scheme, and T s is the result text. Formally, we attempt to learn functions Enc() and Dec() such that:</p><formula xml:id="formula_0">minimize I c − I c + α I s − I s (1)</formula><p>where Enc(I c , I s ) = I c and Dec(I c ) = I s represent the two functions of interest. α is a scalar weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODS 4.1 Dataset</head><p>Information Visualizations The features of data visualization images are quite different from those of natural images. Hence, to build our data visualization dataset for training, we used several popular chart and visualization libraries, such as D3, Echarts, AntV, and Bokeh, to generate carrier visual designs in 10 categories (e.g., "Bar Chart" or "Scatter Chart"). We also selected a subset of images from MASSVIS <ref type="bibr" target="#b7">[8]</ref>, a dataset of visualizations collected from a broad variety of social and scientific domains. The resolutions of these images are expressed as H ×W , where H,W ∈ [300, 3000]. We selected 1500 images and split them into 1000 images composing the training set and 500 images composing the test set. <ref type="figure" target="#fig_2">Fig. 3</ref> shows the distribution of this visualization dataset. QR codes It is necessary to use an error-correction coding scheme when recovering secret information from the coded images. Shannon's <ref type="bibr" target="#b44">[45]</ref> foundational work on coding theory for noisy channels has inspired many later studies. Reed et al. <ref type="bibr" target="#b41">[42]</ref> proposed a coding scheme based on a polynomial ring. Boneh et al. <ref type="bibr" target="#b5">[6]</ref> utilized the Chinese remainder theorem to decode Reed-Solomon codes in polynomial time. BCH codes <ref type="bibr" target="#b8">[9]</ref> are errorcorrection codes for binary group data. However, encoding binary data directly into carrier visual designs may result in obvious perturbations of the original appearance. <ref type="figure" target="#fig_3">Fig. 5</ref> presents an illustrative example comparing SteganoGAN <ref type="bibr" target="#b57">[58]</ref> and StegaStamp <ref type="bibr" target="#b46">[47]</ref>. SteganoGAN <ref type="bibr" target="#b57">[58]</ref> utilizes an adversarial training framework to embed arbitrary binary data into images based on Reed-Solomon codes. StegaStamp <ref type="bibr" target="#b46">[47]</ref> hides hyperlink bitstrings in photos using BCH codes. In <ref type="figure" target="#fig_3">Fig. 5</ref> Barcodes are one of the most popular ways to transfer text information between computing devices. In our implementation, we convert arbitrary binary data provided by users into two-dimensional barcodes (QR codes) as our error correction coding scheme. QR codes <ref type="bibr" target="#b51">[52]</ref> have essentially the same form as 2D images and therefore offer better visual effects than Reed-Solomon codes or BCH codes. QR codes have many advantages, such as a high capacity, a small printout size, and good dirt and damage resistance. We refer the reader to the ISO/IEC 18004 specification <ref type="bibr" target="#b0">[1]</ref> for more technical details of QR codes. For our QR code dataset, we chose "Binary" as the information type and randomly generated 1500 QR code images with various character lengths from 1 to 2900 and various error correction capability levels of 'Low' ('L'), 'Medium' ('M'), 'Quality' ('Q'), and 'High' ('H'). Raising this level improves error correction capability, but also increases the size of QR code when embedding the same amount of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Visual Importance Map</head><p>At a high level, the goal of the encoder network is to embed a large amount of information into a graphical chart while leaving the coded image perceptually identical to the original. A straightforward solution is to train a model to minimize the mean squared error (MSE) of the pixel difference between the original chart and the encoded chart <ref type="bibr" target="#b4">[5]</ref>. However, this metric is unreliable for assessing the visual quality of a chart <ref type="bibr" target="#b49">[50]</ref>. For example, the encoder may generate visible noise in important areas since the MSE metric weights all pixels equally. To solve this problem, we introduce a visual importance map as a perceptual constraint to preserve the visual quality of the carrier chart image when embedding information.</p><p>Although many methods of saliency prediction have been proposed for natural images, the features of data visualizations are quite distinct. The visual importance of visualizations depends not only on the image context but also on higher-level factors (e.g., semantic categories) <ref type="bibr" target="#b10">[11]</ref>. A visual importance map assigns different scores to different regions of an image, which can help to preserve the visual quality of the important content in the encoding stage.</p><p>Inspired by Bylinskii et al. <ref type="bibr" target="#b10">[11]</ref>, we build a prediction model based on the MASSVIS dataset with eye movement annotations <ref type="bibr" target="#b6">[7]</ref>. Instead of  leveraging an original fully convolutional network (FCN) architecture, we fine-tune a state-of-the-art salient object detection network, BASNet <ref type="bibr" target="#b40">[41]</ref>.</p><p>As shown in <ref type="figure">Fig. 4(a)</ref>, we use a network architecture the same as BAS-Net <ref type="bibr" target="#b40">[41]</ref>, based on U-Net <ref type="bibr" target="#b43">[44]</ref> with a multiscale residual refinement module (RRM). Our visual importance network receives a three-channel RGB chart image with a resolution of H ×W as input and outputs a one-channel H ×W visual importance map. Similar to ResNet <ref type="bibr" target="#b22">[23]</ref>, we use basic ResBlocks to obtain feature maps with different resolutions. Subsequently, we upsample the feature maps from the previous stage and concatenate them. The last feature map is then sent to the RRM for refinement. We refer the reader to the description of BASNet <ref type="bibr" target="#b40">[41]</ref> for more details.</p><p>The aim of the original BASNet is to detect salient objects, while our goal is to obtain a real-valued visual importance map describing the importance scores of all pixels. Our training process is based on the MASSVIS dataset <ref type="bibr" target="#b6">[7]</ref>. Accordingly, to obtain more accurate high-level semantic context information and low-level details, we use a hybrid training loss:</p><formula xml:id="formula_1">L prep = L bce + L ssim (2)</formula><p>where L bce represents the BCEWithLogits loss <ref type="bibr" target="#b14">[15]</ref> and L ssim denotes the structural similarity index (SSIM) loss <ref type="bibr" target="#b50">[51]</ref>. Given the ground-truth importance map at each pixel p, G p ∈ [0, 1], over all pixels p = 1,...,N, the BCEWithLogits loss is defined as:</p><formula xml:id="formula_2">L bce (G,V ) = − 1 N N ∑ p=1 (G p logV p + (1 − G p ) log(1 −V p ))<label>(3)</label></formula><p>where V p is the prediction of the visual importance network. The BCEWith-Logits loss is defined in a pixelwise manner and facilitates the semantic segmentation of all pixels. Let x = {x p |p = 1, 2,...,N} and y = {y p |p = 1, 2, ..., N} be two corresponding image patches extracted from the ground-truth importance map G p and the predicted result V p , respectively. Let μ x and μ y be the means of x and y, let σ 2</p><p>x and σ 2 y be their variances, and let σ xy be their covariance. The SSIM is defined as:</p><formula xml:id="formula_3">SSIM(x, y) = (2μ x μ y +C 1 )(2σ xy +C 2 ) (μ 2 x + μ 2 y +C 1 )(σ 2 x + σ 2 y +C 2 )<label>(4)</label></formula><p>where C 1 = 0.01 2 and C 2 = 0.03 2 are scalar constants. The SSIM loss is computed as L ssim = 1 − SSIM(x, y). This loss can help capture the structural information of the input chart image as the weights of the boundaries of visual elements increase. The hybrid loss presented here helps us preserve the multiscale features of the visual importance map. Since we aim to get a visual importance map that is similar to the distribution of ground truth with smooth gradient for pixels, we discard IoU loss used in BASNet <ref type="bibr" target="#b40">[41]</ref>. For evaluation, we use the same metrics and test dataset as in the report by Bylinskii et al. <ref type="bibr" target="#b10">[11]</ref>. Cross Correlation (CC) is a common metric used for saliency evaluation. Root-Mean-Square Error (RMSE) and the R 2 coefficient measure how correlated two maps are. We conduct an ablation study over different loss terms to demonstrate the effectiveness of the hybrid loss. The experimental results are shown in <ref type="table" target="#tab_4">Table 2</ref>. Higher CC values, lower RMSE values and higher R 2 values are better. As we can see, the hybrid loss outperforms others. <ref type="figure">Fig. 6</ref> demonstrates that the visual importance maps predicted by our network are more similar to the ground truth distribution than those of Bylinskii et al. <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Encoder and Decoder</head><p>The encoder network is designed to embed information within a carrier chart image while minimizing perturbations in the encoded image. The decoder network recovers the embedded secret information from an encoded image. Inspired by DeepSteg <ref type="bibr" target="#b4">[5]</ref>, we use a network structure similar to that of an autoencoder. As shown in <ref type="figure">Fig. 4(b)</ref>, the feature vectors of the input carrier chart image and the QR code image to be embedded are first extracted using 3 × 3 filters and 4 × 4 filters and the ReLU activation function <ref type="bibr" target="#b33">[34]</ref>. Then, these two vectors are concatenated and fed to the next 3 convolutional layers, followed by batch normalization (BN) <ref type="bibr" target="#b25">[26]</ref> and a ReLU activation function. After upsampling with 4 × 4 filters (stride = 2) and a ReLU function, the resolution of the output encoded image is the same as that of the input chart image. Then we use two convolution layers with 3 × 3 filters and a tanh function to obtain the final output encoded image.</p><p>The architecture of the decoder network ( <ref type="figure">Fig. 4c)</ref> is simpler than that of the encoder network. The encoded chart image is passed through a series of convolutional layers with 3 × 3 filters to produce the decoded QR code image as the final output. After error correction, we can obtain the reconstructed information in the same form as the original input message.</p><p>Different from DeepSteg <ref type="bibr" target="#b4">[5]</ref>, we introduce the visual importance map to supervise the encoder network. The encoder loss is defined as:</p><formula xml:id="formula_4">L Enc = V L mse = 1 N N ∑ p=1 (V p (I cp − I cp ) 2 )<label>(5)</label></formula><p>Here, V p is the predicted visual importance map, which can be regarded as a weight matrix; regions with higher visual importance scores will have higher weights. I c is the input carrier chart image, and I c is the output encoded image. N is the number of pixels. The decoder network is supervised with a cross-entropy loss. Thus, we define the joint loss as:</p><formula xml:id="formula_5">L joint = L Enc + αL Dec = V L mse (I c , I c ) + αL mse (I s , I s )<label>(6)</label></formula><p>where I s is the original input QR code image and I s is the reconstructed result of the decoder network. α is the weight of the decoder loss, which indicates a tradeoff between the visual quality of the encoded images and the decoding accuracy. Higher α results in higher decoding accuracy and decreased perceptual quality. α is set to 0.25 in our implementation. During the training process, we train the encoder network and decoder network simultaneously. That is, the joint loss incorporates both the visual quality of the coded image and the reconstruction error. In this study, our encoder-decoder network needed a smaller structure and fewer parameters to fit the dataset since our visualization data available for training were limited compared with natural images. We trained our encoder and decoder networks using the aforementioned dataset consisting of data visualizations and QR codes. During training, the images were resized to 256 × 256 with antialiasing. We used the Adam optimizer <ref type="bibr" target="#b28">[29]</ref> with an initial learning rate of lr = 1e −3 . We trained the encoder and decoder networks until the joint loss converged, without utilizing the validation set.</p><p>In the testing process, the encoder and decoder can be used separately. <ref type="figure" target="#fig_5">Fig. 8</ref> shows an example of results obtained using the encoder. The coded image looks identical to the original chart image since the perturbations in the coded image are unobservable to human perception. We enhance the intensity of the residuals <ref type="figure" target="#fig_5">(Fig. 8c</ref>) between the input chart image <ref type="figure" target="#fig_5">(Fig. 8a</ref>) and the output coded image <ref type="figure" target="#fig_5">(Fig. 8b)</ref> by a factor of 10 times to more clearly show the differences. The results demonstrate that our VisCode model can embed information while preserving the perceptual quality of the original chart image.  <ref type="figure">Fig. 7</ref>. Ilustration of the embedding region proposal algorithm. We localize the optimal embedding regions based on the visual importance map and avoid overlaps of the proposal region boxes. The position QR code makes the decoding process more efficient and accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Large-Scale Data Embedding</head><p>Our VisCode system is designed to take a chart image with a resolution of H ×W and arbitrary binary data as input, making it suitable for various actual scenarios. Embedding a large amount of information in an image is a challenging problem because it is easy to generate obvious noise or cause deviations in color or texture. To overcome this challenge, Wu et al. <ref type="bibr" target="#b54">[55]</ref> proposed learning the conditional probability distribution of the original image. However, this method may affect viewers' understanding of a chart since semantic information such as color maps and data points are particularly meaningful in data visualizations. Accordingly, we propose a novel approach for the large-scale embedding of information in regions to which the human visual system is insensitive.</p><p>Although arbitrary binary data can be accepted as input information, we focus on a text message to facilitate the explanation. Suppose that the user inputs text with a length of Len T characters. We first split the text into several blocks:</p><formula xml:id="formula_6">Num B = Len T /η , Len B = Len T /Num B , other blocks Len T mod Num B , last block<label>(7)</label></formula><p>where Num B denotes the number of blocks and Len B denotes the length of each block. Considering that there are various module configurations available for QR codes, which may influence the effects of the encoder-decoder network, we conducted tests with various mapping relations between the length of the text and the module configurations of QR codes. <ref type="table" target="#tab_3">Table 1</ref> shows the results. We use these mapping relations as criteria instead of performing dynamic calculations, which reduces the search time. The parameter η can be specified in accordance with the user's preference. A higher η corresponds to more text in each QR code. We set this parameter to 800 by default. After determining the resolution of each QR code, we attempt to localize the optimal regions of the carrier chart image in which to embed these QR codes. A straightforward solution is to randomly select several regions, but this may create discontinuities near the boundaries between modified and unmodified regions that will look disharmonious. Another brute-force solution is to list all possible regions and evaluate the effects of the encoderdecoder network on each one. However, this may incur a substantial time cost due to the large search space. Instead, inspired by region proposal techniques from the field of object detection (e.g., selective search <ref type="bibr" target="#b17">[18]</ref> and RPN <ref type="bibr" target="#b42">[43]</ref>), we generate bounding boxes based on the visual importance map instead of the original chart image. The corresponding problem can be formulated as follows. Given a chart image with a resolution of H × W and a text with a character length of Len T , we wish to find the Num B optimal regions with the smallest values in the visual importance map. The size of each region box depends on the mapping relations shown in <ref type="table" target="#tab_3">Table 1</ref>. We generate Num B QR code images containing the Num B blocks of text to be embedded. Given the visual importance map V p and the size of each region box, we use average pooling to calculate a value for each region box: <ref type="bibr" target="#b7">(8)</ref> where N b is the kernel size of the filter, which we regard as the size of the region box. Then, we sort the region boxes in increasing order of their values to obtain the list of candidate regions. We preserve the top left region box [(0, 0), (x rd , y rd )] for embedding the position information of the other Num B region boxes. A QR code containing the corresponding position information is also generated. To avoid overlap of the region proposal boxes, we adopt nonmaximum suppression (NMS) <ref type="bibr" target="#b34">[35]</ref> based on their intersection values. <ref type="figure">Fig. 7</ref> shows the key steps of the embedding region proposal algorithm. The detailed region proposal algorithm is presented in Algorithm 1. After determining the optimal regions, we crop the corresponding Num B + 1 patches of the input chart image and feed them into the encoder network with the Num B + 1 QR codes. In the decoding stage, we first extract the top left QR code with the position information from the coded chart image. Then, we locate other corresponding image patches and send them to the decoder network to retrieve the complete information. We utilize the position QR code instead of generating the visual importance map, which is more efficient and accurate.</p><formula xml:id="formula_7">R v = 1 N b Nb ∑ p=1 V p</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">APPLICATIONS</head><p>VisCode can be used for a number of applications, such as embedding metadata or source code in visualization design and visualization retargeting.</p><p>Experiments related to the envisioned applications were implemented on a PC with an Intel Core i7 CPU, an NVIDIA GeForce 2080 Ti GPU, and 32 GB of memory. The visualization images were generated using D3 <ref type="bibr" target="#b9">[10]</ref> and Echarts <ref type="bibr" target="#b29">[30]</ref>. The deep learning framework was implemented based on Pytorch <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Metadata Embedding of Visualization Design</head><p>Information visualization technology uses computer graphics and image processing techniques to convert data into graphics or images, allowing users to observe and understand data patterns more intuitively. Related to the processes of visualization design, display, and dissemination, there is usually considerable information that needs to be stored, such as the name of the designer, the design institution, the design time, the URL of the visualization web page, and historical revision information. Such related information for a visualization image is called metadata <ref type="bibr" target="#b18">[19]</ref>. VisCode provides strong support for information steganography in visualization design. First, our information encoding method is implicit and does not affect the original visual design. Sect. 6 will demonstrate the efficiency of our encoding method. Second, the proposed approach has a fast encoding speed and a high decoding success rate. Therefore, it is possible to effectively encode and decode the author information of visual works, the source website, and other such information. <ref type="figure">Fig. 9</ref> shows the application interface for embedding information in a visualization image. The input to the VisCode system consists of a visualization image and the corresponding metadata. These metadata may include website links, author names, institutions, and revision logs. The metadata are hidden in the visualization image. Obtaining the metadata through the VisCode decoding process is simple. Using this application, the designer of a visual chart can hide common text information in the chart. On the one hand, the design work is implicitly protected, and on the other hand, there is no need to maintain additional modification log files corresponding to the image. The ability to hide URL links in a chart also provides a convenient way to allow users to view web-based or interactive visualizations. As shown in <ref type="figure">Fig. 9</ref>, after the author hides the URL information in the image, a user who has obtained the image can use the decoding module of Vis-Code to obtain the URL and access the real-time interactive visualization corresponding to the static image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Source Code Embedding in a Visualization</head><p>Due to the limitations of network connections and Web servers, the presentation of dynamic web pages is not as flexible as that of static images. Therefore, the display of static images is a very important means of sharing information visualizations. However, static image display has many shortcomings compared with web-based visualization applications. First, a mature information visualization toolbox usually provides multiple interaction modes to help users further explore the contents of data, such as time period selection, data area selection, map level selection in map visualizations, and clicking and dragging operations. <ref type="figure" target="#fig_7">Fig. 10</ref> presents several cases of source code embedding in visualizations. The interactive information label display shown in <ref type="figure" target="#fig_7">Fig. 10(a)</ref> is a frequently used function for information visualization tasks. Second, since static images are mostly stored in bitmap format, when the resolution of an image is not high, it cannot clearly present information, as shown in <ref type="figure" target="#fig_7">Fig. 10(b)</ref>. Third, using the 3D form is a convenient way for the user to observe the patterns in different views. This is not possible with a static visualization image as shown in <ref type="figure" target="#fig_7">Fig. 10(c)</ref>. Fourth, in a visualization involving temporal data, animation is commonly used to visualize data in different periods represented by time steps, as shown in <ref type="figure" target="#fig_7">Fig. 10(d)</ref>. Hans Rosling's dynamic visualization of population and income throughout the world is a representative example <ref type="bibr" target="#b30">[31]</ref>. To extend the usage of VisCode, we present an application scenario that provides users with a robust source code embedding function. In this source code embedding scenario, we encode the source code data into a static visualization image. Because the VisCode framework supports very low loss of encoding quality and the encoding of large datasets, it can well support the encoding of source code in information visualizations. For the implementation of this application scenario, we first integrated common information visualization frameworks (such as D3 <ref type="bibr" target="#b9">[10]</ref> and Echarts <ref type="bibr" target="#b29">[30]</ref>) into the application and then built a functionality for encoding personalized visualization code into a specified image. When a user inputs an encoded image, the system decodes the source code from the image and dynamically generates and displays the visualization file, usually in the form of a web page. <ref type="figure">Fig. 11</ref>. A pipeline for collaboration between an artist and a developer using the VisCode system.</p><p>In addition, source code embedding through VisCode can facilitate collaboration between artists and developers working on visualization projects. In the production processes of large enterprises, artists usually use existing visualization design tools to create visualizations based on aesthetic considerations and then provide the renderings and source code generated by those design tools to developers. Because the interactions between these two roles are frequent and the visual design may undergo many revisions, it can be easy to confuse the design images and codes from different versions. The need to maintain or reconstruct the correspondence relationship between code files and images will increase the difficulty of file sharing. As an alternative approach, VisCode can effectively convert an image and its corresponding source code for a design into a single static image, thus reducing the occurrence of low-level errors. <ref type="figure">Fig. 11</ref> shows a pipeline for collaboration between an artist and a developer using the VisCode system. The artist can encode the source code into a static visualization image, and the developer can decode the source code and use it in a visualization application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Visualization Retargeting</head><p>In addition to source code embedding, another application of VisCode is to retarget a visualization. With the development of big data and deep learning technologies, visualization retargeting based on pattern recognition has gained the attention of researchers in recent years, as in the work of Poco et al. <ref type="bibr" target="#b39">[40]</ref>. Visualization retargeting is very useful for the creation of visualizations. On the one hand, it reduces the designer's workload. On the other hand, it broadens the artist's creative space. For some common chart types, existing research has yielded methods of extracting data from a visualization image and retargeting the visualization. For example, Poco et al. <ref type="bibr" target="#b39">[40]</ref> presented an approach for identifying values, legends, and coordinate axes from a bar chart. Similar work has also been applied for information extraction and retargeting based on color density maps. However, for many types of visualizations, such as network diagrams, such recognition is still difficult. Visualization retargeting via pattern recognition is not feasible for some visualizations if the visualization process is not irreversible. For example, even if the regions of a density map can be accurately identified, the original scattered data constituting the density map cannot be recovered. However, if VisCode is used to encode the original data into a static image, then for a visualization that is saved in a certain format, such as JSON, high-quality visualization retargeting can be achieved without pattern recognition. Our implementation includes representation retargeting and theme retargeting. First, we encode the original data of the visualization and the visualization type into a static visualization image in JSON format. After decoding, the data are displayed in a visualization of the specified type. For users, the input is an encoded static image, and the output is a visual effect of different styles, as shown in <ref type="figure" target="#fig_1">Fig. 12</ref>. We define this form of retargeting as representation retargeting. Second, we can implement different color themes for users to choose based on an existing representation. This capability of color theme switching is called theme retargeting. This application of VisCode can provide rich visual styles and color themes. It can also be used to enable deformations for the visualization of complex network diagrams from which information is difficult to extract, as shown in <ref type="figure">Fig. 1(a)</ref>. From an encoded color density map, the original scattered data can be extracted through the decoding function of VisCode. Based on these scattered data and the kernel density algorithm <ref type="bibr" target="#b45">[46]</ref>, a new density map with a personalized bandwidth can be generated. <ref type="figure" target="#fig_2">Fig. 13</ref> presents an example of the retargeting of a visualization of a color density map. <ref type="figure" target="#fig_2">Fig. 13(a)</ref> shows the input static image. <ref type="figure" target="#fig_2">Fig. 13(b)</ref> shows the extracted scattered data, and <ref type="figure" target="#fig_2">Fig. 13(c-d)</ref> show two personalized color density maps. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATION</head><p>We evaluate our VisCode model from three aspects: steganography indices, the evaluation of encoded image quality with the different number of message bits; steganography defense, the evaluation of decoding accuracy in various corruption scenarios through digital transmission; and time performance, the evaluation of embedding information and recovery time of different settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Steganography Indices</head><p>We used three metrics to evaluate the performance of our VisCode model: the peak signal-to-noise ratio (PSNR) <ref type="bibr" target="#b1">[2]</ref>, the SSIM <ref type="bibr" target="#b49">[50]</ref>, and the learned perceptual image patch similarity (LPIPS) <ref type="bibr" target="#b58">[59]</ref>. The PSNR <ref type="bibr" target="#b1">[2]</ref> is a widely used metric for evaluating image distortion. We computed the PSNR by comparing the coded image I c and the original chart image I c , each consisting of N pixels:</p><formula xml:id="formula_8">PSNR = 20 • (log 10 P max − log 10 1 N N ∑ p=1 (I cp − I cp ) 2 )<label>(9)</label></formula><p>where P max is the maximum possible difference between two pixel values, which we set to 1.0 in our experiment since the RGB values of our images are ranged from 0 to 1. Because the PSNR measures the distance between each pair of pixels independently, we also considered the SSIM to measure the structural similarity between two images. The SSIM is defined as shown in Equation 4. It is a patch-level perceptual metric. However, whereas both the PSNR and SSIM are low-level metrics, Zhang et al. <ref type="bibr" target="#b58">[59]</ref> have demonstrated that perceptual similarity is influenced by visual representations instead of being a special function. Accordingly, the LPIPS, which relies on deep features, can better reflect human judgments.</p><p>We construct a test dataset of 500 chart images with various resolutions and types, as mentioned in Sect. 4.1. We compare our method with StegaStamp <ref type="bibr" target="#b46">[47]</ref> and SteganoGAN <ref type="bibr" target="#b57">[58]</ref>. However, there are some limitations to these two methods. StegaStamp <ref type="bibr" target="#b46">[47]</ref> can embed only 100 bits data. SteganoGAN <ref type="bibr" target="#b57">[58]</ref> can embed small-scale data, but it will cost huge time when the number of bits increases to a threshold. Therefore, the two methods StegaStamp and SteganoGAN are not suitable for embedding large-scale information in data visualizations. For a fair comparison, we resize the chart images to the same resolution, and embed the same numbers of data bits into the test dataset. Higher PSNR, higher SSIM, and lower LPIPS are better. As shown in <ref type="table" target="#tab_6">Table 3</ref>, the comparison result demonstrates that VisCode can embed information in various types of chart images with better visual quality. Besides, we design a user study to evaluate the visual effects of the results by human judgments. We select 15 images from the test dataset as the input to generate steganographic results by 3 methods and recruit 30 users to choose the best one. VisCode achieves a mean proportion of being selected of 83.6%, while SteganoGAN achieves 15.1% and StegaStamp achieves 1.3%. Our method is rated higher than others.</p><p>Our VisCode system is designed to store an extensive amount of information. Embedding large data is challenging due to the tradeoff between the visual quality of encoded images and the amount of data. Therefore, we evaluate the quality of encoded images with different input text lengths. The resolutions of the test chart images are set to 1600 × 1600. <ref type="figure" target="#fig_10">Fig. 14</ref> reports the results of our steganography index evaluation, where X-axis represents the number of bits of input text, and Y-axis shows the average values of associated image quality metrics. Our method achieves 29.18 PSNR, 0.9873 SSIM, and 0.0042 LPIPS on average for embedding 204, 800 bits of information. It demonstrates that VisCode can encode large-scale data while preserving the perceptual quality of the visualizations.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Steganography Defense</head><p>If we use normal digital transmission without large deformations of the coded images, the decoding text recovery accuracy achieves over 90%. When users share encoded chart images through digital transmission, corruption, or attack may occur. Therefore, we analyze four typical scenarios: adding watermark, adjusting brightness, applying rotation and applying JPEG compression. For each image in the test dataset, we first generate the encoded image with text of 12, 000 bits and then apply the following operations: (1) Adding a visible watermark in the same position of the encoded image. (2) Adjusting ±10% brightness of the image. (3) Rotating the image 180 • clockwise. (4) Applying JPEG compression to the image with quality factor Q = 90. We use Text Recovery Accuracy (TRA) to assess the performance of our VisCode model in the decoding stage. TRA is defined as the proportion of recovered characters in all input characters. Since it is difficult to embed large-scale data in chart images using StegaStamp <ref type="bibr" target="#b46">[47]</ref> and SteganoGAN <ref type="bibr" target="#b57">[58]</ref>, we compare VisCode with other two methods LSB <ref type="bibr" target="#b32">[33]</ref> and DeepSteg <ref type="bibr" target="#b4">[5]</ref>. In this part, we maintain the diverse resolutions of the testing dataset from 300 × 300 to 3000 × 3000, as mentioned in Sect. 4.1. <ref type="table" target="#tab_7">Table 4</ref> shows the comparison result in steganography defense evaluation. LSB is a traditional steganography method using fixed principles, unable to the defense any perturbations. We find that the performance of VisCode in adjusting brightness, applying rotation and JPEG compression is close to DeepSteg <ref type="bibr" target="#b4">[5]</ref>, since the three manipulations apply changes on all pixels. Watermarks apply local changes to the image, which may destroy the important features of the image. In this case, VisCode outperforms DeepSteg <ref type="bibr" target="#b4">[5]</ref> by embedding information in insensitive regions.  <ref type="figure">Fig. 16</ref>. The sample steganographic images generated by VisCode and decoded results with various bits of data. The input chart image is with a resolution of 420 × 670. There are fewer visible artifacts in <ref type="figure">Fig. 16a</ref> than that in <ref type="figure">Fig. 16b</ref>. Moreover, <ref type="figure">Fig. 16c</ref> is decoded correctly, while embedding too much information may cause a high decoding error rate, as shown in <ref type="figure">Fig. 16d</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Time Performance</head><p>The encoder-decoder network is integrated into VisCode with several other processes for interactive applications. We evaluate time performance for VisCode system of different scales of input chart images and text in <ref type="figure" target="#fig_3">Fig. 15</ref>. X-axis represents the number of bits of input text, and Y-axis shows the average time (in seconds) taken by the complete encoding stage <ref type="figure" target="#fig_3">( Fig. 15a</ref>) and decoding stage <ref type="figure" target="#fig_3">( Fig. 15b</ref>). Lines of different colors represent different resolutions of the input chart images, ranging from 200 × 200 to 1600 × 1600. The environment configurations are introduced in Sect. 5.</p><p>In the encoding stage, we observe that the time performance is relatively stable for encoding different lengths of information to images with the same resolution. The complete encoding stage includes predicting the visual importance map of the input chart image, localizing the optimal regions, generating QR codes, sending corresponding image pairs to the encoder network, and obtaining the encoded chart image. When the resolution of images is constant, the increased time is mainly spent on generating QR codes, while the runtime of the encoder network is very fast. On the other hand, the leading cause of different time performance between images with diverse resolutions is the calculation of the visual importance map and the embedding region proposals. In the decoding stage, the time cost of the different resolution of images is similar owing to the fast decoder network, while larger message needs more time to convert QR codes to text with the error-correction scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We have proposed an approach for encoding information into a static visualization image. Our applications implemented with VisCode are designed to support metadata embedding, source code embedding, and visualization retargeting. We have designed an encoder-decoder network to effectively encode large amounts of information into visualization images such that the original data can be decoded with high accuracy. The presented applications and evaluations demonstrate that VisCode can offer intuitive and effective information embedding for various types of visualizations.</p><p>The current version of VisCode has certain limitations in addressing large deformations of the coded images. In addition, for a fixed visualization image size, there is an upper limit on the amount of information that can be embedded, which reduces the effectiveness of VisCode. If we embed too much information in a chart image, which corresponds to more text in each QR code and lower error correction capability, it may cause a high decoding error rate. With the same image size and encoded data bits, images with more complex texture and rich color content are easier to decode successfully.</p><p>In the future, we plan to propose a more flexible framework to ensure a high decoding success rate while adapting to visualization image deformation. Camera capturing and light-aware steganography, as shown in the work of Wengrowski and Dana <ref type="bibr" target="#b52">[53]</ref>, is a potential research direction that can extend the application domain of VisCode. We also plan to collaborate with designers and developers to obtain feedback in the actual design and manufacturing applications <ref type="bibr" target="#b59">[60]</ref> to improve the user experience of VisCode.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2</head><label>2</label><figDesc>Fig. 2. The main components of our VisCode system. First, the visual importance network (b) processes the input graphical chart (a), which can facilitate salient feature detection of the visualization and output a visual importance map. Next, an encoder network (c) embeds secret information in the graphical chart (a). The carrier chart image and the QR code image are embedded into vectors by the feature extraction process. Then, these two vectors are concatenated (as the yellow rectangle shown) and fed into the auto-encoding stage. The encoded image (d) is then sent to the user (e), and the user can send it to others by digital transmission. When a user wishes to obtain the detailed information hidden in the chart, the encoded image can be uploaded to the decoder network (f). After data recovery and error correction, the user receives the decoded information (g).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Distribution of different categories in our visualization dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Comparison of the encoded image quality with other methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Comparison of image quality before and after encoding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>1 :Fig. 9 .</head><label>19</label><figDesc>Initial version (Feb 02, 2020) v0.6: Data updated (Mar 01, 2020) v1.0: Release (Mar 14, 2020) Application interface for embedding information in a visualization image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .</head><label>10</label><figDesc>Three application scenarios involving the embedding of source code in visualizations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>4 Fig. 12 .</head><label>412</label><figDesc>Retargeting of the form of representation of information. The input visualization can be converted to other visualization forms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>2 Fig. 13 .</head><label>213</label><figDesc>Retargeting the visualization of a color density map. Scatter points are extracted from the input image and new colorful density maps are generated through the kernel density estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 14 .</head><label>14</label><figDesc>Steganography index evaluation demonstrates that VisCode can encode large-scale data while preserving the perceptual quality of the visualizations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 15 .</head><label>15</label><figDesc>Time performance comparison of encoding and decoding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>• Peiying Zhang, Chenhui Li, and Changbo Wang are with School of Computer Science and Technology, East China Normal University. Chenhui Li is the corresponding author. E-mail: chli@cs.ecnu.edu.cn.</figDesc><table /><note>Manuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication xx xxx. 201x; date of current version xx xxx. 201x. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org. Digital Object Identifier: xx.xxxx/TVCG.201x.xxxxxxx</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>, we can see that SteganoGAN may cause color variations in images with rich color content, while StegaStamp may generate visible noise when embedding information. Moreover, visualization charts usually have clean backgrounds and clear visual elements, which leads to a high decoding error rate in SteganoGAN. Due to these limitations, the two methods StegaStamp and SteganoGAN are not suitable for embedding information in data visualizations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Module Configurations of QR codes</figDesc><table><row><cell>Number of Characters</cell><cell>ECC Level</cell><cell>Resolution</cell></row><row><cell>1 ∼ 350</cell><cell>'High'</cell><cell>100 × 100</cell></row><row><cell>351 ∼ 1000</cell><cell>'Quality'</cell><cell>200 × 200</cell></row><row><cell>1001 ∼ 2000</cell><cell>'Medium'</cell><cell>200 × 200</cell></row><row><cell>2001 ∼ 2900</cell><cell>'Low'</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Visual Importance Map Comparison</figDesc><table><row><cell cols="2">Method</cell><cell>Dataset</cell><cell>CC ↑</cell><cell>RMSE ↓</cell><cell>R 2 ↑</cell></row><row><cell cols="2">Bylinskii et al. [11]</cell><cell></cell><cell>0.686</cell><cell>0.165</cell><cell>0.369</cell></row><row><cell>Ours</cell><cell>Lbce Lssim</cell><cell>MASSVIS</cell><cell>0.866 0.866</cell><cell>0.123 0.126</cell><cell>0.715 0.714</cell></row><row><cell cols="2">Lbce + Lssim</cell><cell></cell><cell>0.868</cell><cell>0.117</cell><cell>0.720</cell></row><row><cell>(a) Input chart</cell><cell cols="2">(b) Bylinskii et al.</cell><cell cols="2">(c) Ground Truth</cell><cell>(d) Ours</cell></row><row><cell cols="6">Fig. 6. Visual importance maps predicted by our network are more similar to</cell></row><row><cell cols="3">the distribution of ground truth.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Output: R o set = {R o1 , R o2 ,...,R ok }: a set of optimal embedding regions;</figDesc><table><row><cell>8:</cell><cell>for s region in R o set do</cell></row><row><cell>9:</cell><cell>if Intersection(c region, s region) &gt; 0 then</cell></row><row><cell>10:</cell><cell>con f lict = true</cell></row><row><cell>11:</cell><cell>end if</cell></row><row><cell>12:</cell><cell>end for</cell></row><row><cell>13:</cell><cell>if con f lict = f alse and cnt &lt;= Num B then</cell></row><row><cell>14:</cell><cell>R o set .append(c region), cnt = cnt + 1</cell></row><row><cell>15:</cell><cell>end if</cell></row><row><cell cols="2">16: end for</cell></row><row><cell cols="2">17: return R o set</cell></row></table><note>Algorithm 1 Embedding region proposal algorithm. Input: V : the visual importance map; Text: the user input text; η: the acceptable maximal number of characters in a single QR code;1: (W, H) = size(V ),2: Calculate Num B and Len B based on Equation 73: R o set = {}, cnt = 04: kernel size = map(Len B ) based on Table 15: Full set ← Apply average pooling on V6: sort(Full set ) in increasing order7: for c region in Full set do</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .</head><label>3</label><figDesc>Evaluation results of steganography indices</figDesc><table><row><cell>Method</cell><cell>Size</cell><cell>P ↑</cell><cell>100 bits S ↑</cell><cell>L ↓</cell><cell>P ↑</cell><cell>3200 bits S ↑</cell><cell>L ↓</cell></row><row><cell>StegaStamp</cell><cell></cell><cell>34.09</cell><cell cols="2">0.9312 0.0413</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SteganoGAN</cell><cell>400 × 400</cell><cell>35.26</cell><cell cols="3">0.9386 0.0247 35.19</cell><cell cols="2">0.9380 0.0249</cell></row><row><cell>VisCode</cell><cell>800 × 800</cell><cell cols="6">40.63 42.08 0.9990 0.0003 41.70 0.9987 0.0004 0.9959 0.0007 39.19 0.9930 0.0008</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .</head><label>4</label><figDesc>Evaluation results of steganography defense.</figDesc><table><row><cell>Method</cell><cell>Watermark</cell><cell>Brightness</cell><cell>Rotation</cell><cell>JPEG</cell></row><row><cell>LSB [33]</cell><cell>0.02%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell></row><row><cell>DeepSteg [5]</cell><cell>3.45%</cell><cell>32.75%</cell><cell>100%</cell><cell>9.8%</cell></row><row><cell>VisCode (Ours)</cell><cell>55.29%</cell><cell>32.79%</cell><cell>100%</cell><cell>10.2%</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors wish to acknowledge the support from NSFC under Grants (No. 61802128, 61672237, 61532002).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Information technology -automatic identification and data capture techniques -qr code bar code symbology specification</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">18004</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Stego image quality and the reliability of psnr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Almohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghinea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 2nd International Conference on Image Processing Theory, Tools and Applications</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="215" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">High capacity steganographic method based upon jpeg</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Almohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Hierons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghinea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third International Conference on Availability, Reliability and Security</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="544" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Embedding meta information into visualizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hiding images in plain sight: Deep steganography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2069" to="2079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Finding smooth integers in short intervals using crt decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boneh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="768" to="784" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Beyond memorability: Visualization recognition and recall</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Borkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bainbridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="519" to="528" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">What makes a visualization memorable</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Borkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2306" to="2315" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On a class of error correcting binary group codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Ray-Chaudhuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and control</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="68" to="79" />
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">D 3 data-driven documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bostock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ogievetsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2301" to="2309" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning visual importance for graphic designs and data visualizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>O'donovan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alsheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Madan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Annual ACM symposium on user interface software and technology</title>
		<meeting>the 30th Annual ACM symposium on user interface software and technology</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="57" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visualizing for the non-visual: Enabling the visually impaired to use visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="249" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bloom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kalker</surname></persName>
		</author>
		<title level="m">Digital watermarking and steganography</title>
		<imprint>
			<publisher>Morgan kaufmann</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Chart decoder: Generating textual and numeric information from chart images automatically</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Languages &amp; Computing</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="101" to="109" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A tutorial on the cross-entropy method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-T. De</forename><surname>Boer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kroese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Rubinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of operations research</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="67" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Searching the visual style and structure of d3 visualizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Enamul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1236" to="1245" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An overview of encryption algorithms in color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Ghadirli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nodehi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Enayatifar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Understanding metadata and metadata schemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Greenberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="17" to="36" />
		</imprint>
	</monogr>
	<note>Cataloging classification quarterly</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Evaluating &apos;graphical perception&apos;with cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Haehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="641" to="650" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cobra: color barcode streaming for smartphone systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th international conference on Mobile systems, applications, and services</title>
		<meeting>the 10th international conference on Mobile systems, applications, and services</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="85" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generating steganographic images via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Danezis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1954" to="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Designing steganographic distortion using directional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE International workshop on information forensics and security (WIFS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="234" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Artificial neural network for steganography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Husien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Badi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="111" to="116" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Disco: Display-camera communication using rolling shutter sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Principles and applications of bpcs steganography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kawaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">O</forename><surname>Eason</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">3528</biblScope>
			<biblScope unit="page" from="464" to="473" />
		</imprint>
	</monogr>
	<note>Multimedia Systems and Applications</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Echarts: A declarative framework for rapid construction of web-based visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual Informatics</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="136" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Three minutes with hans rosling will change your mind about the world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maxmen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature News</title>
		<imprint>
			<biblScope unit="volume">540</biblScope>
			<biblScope unit="issue">7633</biblScope>
			<biblScope unit="page">330</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">ivolver: Interactive visual language for visualization extraction and reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">G</forename><surname>Méndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Nacenta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vandenheste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2016 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4073" to="4085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Lsb matching revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mielikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE signal processing letters</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="285" to="287" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning (ICML-10)</title>
		<meeting>the 27th international conference on machine learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficient non-maximum suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neubeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th International Conference on Pattern Recognition (ICPR&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="850" to="855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Using high-dimensional image models to perform highly undetectable steganography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pevnỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Filler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Information Hiding</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="161" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep learning is a good steganalysis tool when embedding key is reused for different images, even if there is a cover sourcemismatch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pibre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pasquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ienco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chaumont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Imaging</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Reverse-engineering visualizations: Recovering visual encodings from chart images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Poco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="353" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Extracting and retargeting color mappings from bitmap images of visualizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Poco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mayhua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="637" to="646" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Basnet: Boundary-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7479" to="7489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Polynomial codes over certain finite fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the society for industrial and applied mathematics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="300" to="304" />
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell system technical journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="379" to="423" />
			<date type="published" when="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Density estimation for statistics and data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Silverman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>CRC press</publisher>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Stegastamp: Invisible hyperlinks in physical photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2117" to="2126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Automatic steganographic distortion learning using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1547" to="1551" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Tufte</surname></persName>
		</author>
		<title level="m">The visual display of quantitative information</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multiscale structural similarity for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thrity-Seventh Asilomar Conference on Signals</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1398" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Wave</forename><surname>Qr</surname></persName>
		</author>
		<ptr target="https://www.qrcode.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Light field messaging with deep photographic steganography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wengrowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1515" to="1524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The grammar of graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wilkinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Computational Statistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="375" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Stegnet: Mega image steganography capacity with deep convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Internet</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">54</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Fontcode: Embedding information in text documents using glyph perturbation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A 3d steganalytic algorithm and steganalysis-resistant watermarking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pintus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rushmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ivrissimtzis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1002" to="1013" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cuesta-Infante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Veeramachaneni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.03892</idno>
		<title level="m">Steganogan: high capacity image steganography with gans</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A survey of visualization for smart manufacturing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visualization</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="419" to="435" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Hidden: Hiding data with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="657" to="672" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
