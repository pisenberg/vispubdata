<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FemaRepViz: Automatic Extraction and Geo-Temporal Visualization of FEMA National Situation Updates Chi-Chun Pan</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasenjit</forename><surname>Mitrat</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Pennsylvania State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FemaRepViz: Automatic Extraction and Geo-Temporal Visualization of FEMA National Situation Updates Chi-Chun Pan</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>visual analytics</term>
					<term>geo-temporal visualization</term>
					<term>text processing</term>
					<term>knowledge discovery</term>
					<term>geospatial analytics Index Terms: H.4.2 [INFORMATION SYSTEMS APPLICA-TIONS]: Types of Systems Decision support;</term>
				</keywords>
			</textClass>
			<abstract>
				<p>An architecture for visualizing information extracted from text documents is proposed. In conformance with this architecture, a toolkit, FemaRepViz, has been implemented to extract and visualize temporal, geospatial, and summarized information from FEMA National Update Reports. Preliminary tests have shown satisfactory accuracy for FEMARepViz. A central component of the architecture is an entity extractor that extracts named entities like person names, location names, temporal references, etc. FEMARepViz is based on FactXtractor, an entity-extractor that works on text documents. The information extracted using FactXtractor is processed using GeoTagger, a geographical name disambiguation tool based on a novel clustering-based disambiguation algorithm. To extract relationships among entities, we propose a machine-learning based algorithm that uses a novel stripped dependency tree kernel. We illustrate and evaluate the usefulness of our system on the FEMA National Situation Updates. Daily reports are fetched by FEMARepViz from the FEMA website, segmented into coherent sections and each section is classified into one of several known incident types. We use ConceptVista, Google Maps and Google Earth to visualize the events extracted from the text reports and allow the user to interactively filter the topics, locations, and time-periods of interest to create a visual analytics toolkit that is useful for rapid analysis of events reported in a large set of text documents.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Successful crisis management requires rapid response. Unfortunately, first responders are overloaded with vast amounts of information that needs to be processed in a very short amount of time. Extracting required information totally automatically with high accuracy is not feasible (at least with today's technology). Therefore, semi-automatic methods are essential. Human beings can understand and analyze data more effectively if the data is presented in a visual mode. Ideally, the analyst or end-user should be presented with the automatically extracted information and the tool should interact with the end-user to enable the end-user to explore summaries of the data, correct extraction errors, select and examine articles of interest in more detail, etc. Such visual analytics tools can help first responders sift through large amounts of textual information fast in order to select texts reporting events of choice such that they can focus (drill down) into relevant articles and make decisions.</p><p>Spatial and temporal information are important attributes of data that must be recognized and treated as first-class objects. Such in-*e-mail: julianpan@psu.edu te-mail:pmitra@ist.psu.edu IEEE Symposium on Visual Analytics Science and Technology 2007 October 30 -November 1, Sacramento, CA, USA 978-1-4244-1659-2/07/$25.00 Â©2007 IEEE formation forms a vital component of decision-making. Essentially, end-users want to know what happened, when, and where. Firstresponders find such information crucial for decision making during emergency situations. For example, in 2004, an earthquake of magnitude of 9.3 Richter occurred in the Indian Ocean. The earthquake resulted in a series of dreadful tsunamis and took more than 300,000 lives from south Asia to east Africa. Most residents of the coastal areas did not have advance warning of the tsunami. A decision support system that can quickly process scientific reports and observations and generate an emergency alarm would be of immense value in such situations. Local television and radio stations could then broadcast warnings in the impacted countries. Decision makers in one country could promptly provide critical information to the emergency response agencies in other countries. Thousands of people could evacuate earlier and their lives could be saved.</p><p>Although, in this paper we do not provide an automated decision support system, we outline the architecture of a visual analytics tool that the end-user can use to make better decisions. The tool, FE-MARepViz, extracts and visualizes information from FEMA National Situation Updates (and could also be extended to handle other types of textual reports in the future) on a map and can show the progression of events over time. The FEMA reports were primarily weather reports but also had items related to bombings, chemical spills, etc. Our tool is especially useful when there are vast amounts of textual data that is impossible to process totally manually.</p><p>FEMARepViz is a hybrid information extraction system that extracts concept maps and spatial-temporal information from text documents using both rule-based and machine-learning methods. Daily reports are fetched from the FEMA website and automatically segmented into several incidents. Each incident then is classified into topics based on word frequency and tagged with timestamp and location names. The extracted information is stored in a repository and can be presented with visualization applications such as Google Earth. The system provides browsing and visualization of the incidents. Atop our extraction system, FactXtractor, like FEMARepViz, many other applications can be built such as emergency situation pattern analysis and real-time emergency monitoring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">FEMA National Situation Reports</head><p>The FEMA National Situation Updates contain information from a variety of sources including federal agencies, state and local government, and the news media. The reports are designed to provide in formation useful for emergency management planning and operational activities. Situation reports generally cover weather reports, earthquake activities, wildfire, and other incidents around the United States. The reports include location names indicating where the incidents happened. Sometimes persons or organizations involved in the incidents are also included in the reports. The richness of geographical information makes the FEMA National Situation Updates an excellent dataset for geo-temporal information analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Our Contributions</head><p>The key contributions of this paper are:</p><p>1. We design an information extraction system to automatically process text documents and create concept maps and geotemporal visualization. We demonstrate the usefulness of our system with the FEMA National Situation Updates.</p><p>2. We present a novel stripped dependency tree kernel for entity relation extraction.</p><p>3. We present a heuristic algorithm for location name disambiguation. We use simple rules and propose a clustering algorithm to determine the coordinates of locations.</p><p>The rest of the paper is organized as follows. The overview of system architecture is described in section 3 and technical details are presented in section 4. In section 2 we discuss related work. Finally we discuss future work in section 5 and conclude in section 6.</p><p>2 RELATED WORK RSOE HAVARIA AlertMap <ref type="bibr" target="#b3">[4]</ref> is a world-wide disaster information system. The system reports real-time event updates for a variety of incidents such as earthquake, active volcano, and tropical storms. However, unlike our system, RSOE HAVARIA AlertMap collects structured data from different data sources. Every event has been classified by its data source (such as European flu data from EISS and Volcano information from SWVRC) and come with detailed information such as timestamp and location coordinates. Although our system is focused on processing unstructured data, in the future, it can be enhanced to utilize information from structured data sources for, say identifying event co-reference.</p><p>HEALTHmap <ref type="bibr" target="#b1">[2]</ref> is a global disease information system that collects disparate data sources and provides a visualization of the current state of infectious diseases and their effect on human and animal health. The system gathers unstructured text from Google News, ProMED-maill, and alerts from the World Health Organi-zation2 and EuroSurveillance3. HEALTHmap processes text and extracts disease names and the location names appearing in the text. Locations are limited to countries and some major cities in certain countries. HEALTHmap is a customized system for a specialized domain of events, while our system is designed to process a broad range of events.</p><p>Zelenko, et al., <ref type="bibr" target="#b17">[18]</ref> proposed using tree kernels over shallow parsing trees to extract person-affiliation and organization-location relations. They created data samples by performing shallow parsing over sentences. Compared with deep parsing techniques, the results from shallow parsing are more reliable <ref type="bibr" target="#b18">[19]</ref>. The kernels then compare similarity between parse trees by recursively matching nodes between two parse trees starting from the root nodes.</p><p>In their experiments, kernel-based approaches have better performance than feature-based approaches with several different learning algorithms.</p><p>Culotta and Sorensen <ref type="bibr" target="#b10">[11]</ref> defined a slightly more general version of tree kernels than that proposed by Zelenko, et al., <ref type="bibr" target="#b17">[18]</ref> with a richer sentence representation. Their kernels are based on dependency trees instead of syntactic parse trees. Dependency trees include more information by considering syntactic relations between words. The experimental results show the dependency tree kernels have good precision but low recall on the ACE 2004 corpus. They combined a bag-of-words kernel into the dependency tree kernel to boost the performance. Harabagiu, et al., <ref type="bibr" target="#b12">[13]</ref> combined dependency trees with shallow semantic parsing and reported average Fl-score of 78.41% on ACE 2004 corpus <ref type="bibr" target="#b16">[17]</ref>. <ref type="bibr">Greenwood</ref> and Stevenson <ref type="bibr" target="#b11">[12]</ref> further extended the dependency tree kernels 2http://www.who.int/csr/alertresponse/en/ 3http://www.eurosurveillance.org/ by using linked chain patterns and structural similarity measurement. Their approach can improve the performance over iterations; nonetheless the maximum F-score is only 0.329 due to low recall.</p><p>Roth and Yih <ref type="bibr" target="#b14">[15]</ref> described a linear programming (LP) framework to detect named entities and entity relations. Unlike most NLP systems with pipelined architectures, their approach considers outcomes of different but mutually dependent classifiers simultaneously. The learning algorithm is a variation of the Winnow algorithm. They annotated the TREC data set with named entities and relations and then used the LP framework on it. Their results indicate that by optimizing the global interests instead of concentrating on task-specific constraints and accumulating errors within pipeline processes, the performance improved significantly.</p><p>Bunescu and Mooney <ref type="bibr" target="#b7">[8]</ref> proposed a shortest path dependency kernel which outperformed tree kernels on the ACE 2004 corpus. Their approach treats the directed dependency graph as an undirected graph and finds the shortest path between two entities. Then they compare the shortest path example with a Cartesian product kernel to compute the number of common features on the path. The key idea of the shortest dependency path is to consider only the information relevant to entity relations. The advantage of dependency path kernels is that they do not consider the depth of entity nodes, hence they yield better recall. However, their approach only considers the predicate-argument structures between words. Entities in sentences such as "While in Paris, John never visited the Louvre." may never be linked.</p><p>Our novel approach combines the advantages of dependency tree kernels and that of an information elimination technique similar to that proposed by Bunescu and Mooney <ref type="bibr" target="#b7">[8]</ref>. We propose to use stripped dependency trees. We show by empirical evaluation on the same data set used in <ref type="bibr" target="#b14">[ 15]</ref> that our approach has better precision and improved recall than the dependency tree kernel proposed by <ref type="bibr">Culotta [11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SYSTEM ARCHITECTURE</head><p>As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, our system has the following major components:(a)Text Extraction and Processing Module, (b) Disambiguation Module, and (c) Visualization and User Interaction Module.</p><p>The text extraction and processing module processes a set of documents and extracts named entities (like person, place, and organization names). It also segments a text into different contiguous segments having the same topic. The extraction module also detect events. For the current application, segmenting the document itself based on topics resulted in segregating the different events and we did not have to perform complex temporally-based event detection.</p><p>The disambiguation module is responsible for disambiguating named entities. For person names, the role of the disambiguation module is to establish which names refer to the same person, e.g., "Mohandas Gandhi" and "M.K. Gandhi", or if the same name actually refers to different people, e.g., two occurrences of "James Smith" may refer to different people in two different contexts. This module is also responsible for coreference resolution. The disambiguation module processes geographical named entities and using the information available about the contexts in which they occur, disambiguates the geographical named entity to an exact location. For example, it would generate an exact latitude and longitude for a reference to the town "Springfield" using the context information to derive which of the several "Springfield"'s in the U.S.A. (or beyond) it is.</p><p>The visualization module presents the information extracted from the text on a map for a particular time-point (or time-period). The user can interact with this module to filter the data being displayed based on topics, time periods, geographical locations, or people of interest. The user can provide feedback to correct incorrect items shown on the map, e.g., to move a displayed event from one location to another. The feedback provided by the user To provide the functionality of the three modules indicated above our system utilizes three main components: FactXtractor GeoTagger, and FEMARepViz. Every component is designed as a standalone web service to ensure system flexibility and interoperability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Information Extraction</head><p>FactXtractor is an information extraction web service for Named Entity Recognition (NER) and Entity Relation Extraction (RE). FactXtractor processes text documents using an open source text processing platform (GATE[l]) and identifies entity relations using Stripped Dependency Tree kernels. <ref type="figure">Figure 2</ref> shows the major steps of the processing flow. The input of FactXtractor is a document or a set of documents in plain text format. First, the text is processed by a shallow parser to get parts of speech (PoS) tags. In addition it identifies other linguistics features such as noun chunks and verb groups. In the second step, we use the Named Entity Tagger provided by GATE to extract named entities. Next, a deep parser processes the text to construct dependency trees. Syntactic relationships (subject-verb-object) can be easily extracted using dependency trees. FactXtractor extracts relationships using the Stripped Dependency Tree Kernels. The output of FactXtractor is a graph of the extracted entities and their relationships formatted in OWL <ref type="bibr" target="#b6">[7]</ref> (see <ref type="figure">Figure 7</ref>). This extracted graph can be visualized with Con-ceptVista4. We will discuss the methods used in FactXtractor in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Geographical Name Disambiguation</head><p>GeoTagger is a geocoding Web service. GeoTagger maps a location name that appears in text to its correct co-ordinates on a map GeoTagger uses the U S Geological Survey (USGS) Geographic Names Information System (GNIS)5 for U S locations National Geospatial-Intelligence Agency (NGA) GEOnet Names Server (GNS)T for locations outside U.S., and Google Map for global locations . We use Google Map as a secondary reference because GNIS and GEOnet contain many locations which are only used in local For example State College MS is listed in GNIS but is not listed in Google Map. If a location is listed in GNIS or GEOnet but not in Google Map, we do not consider it as a candidate 4available at httt Hwww geovista psu.edo/ ?edback.</p><p>z of system architecture <ref type="figure">Figure 2</ref>: Text Processing Flow for assigning location names. We will discuss the details in section</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>FEMARepViz is a visualization generation Web service for the FEMA Situation Reports. FEMARepViz processes situation reports using FactXtractor and GeoTagger. Processed reports are stored in a repository and can be retrieved by a Web interface. The output is a KML document that provides dynamic updates and interactive visualization. <ref type="figure" target="#fig_2">Figure 3</ref> illustrates the information processing flow and connections between the web services.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Named Entity Recognition</head><p>We use GATE[ 1] to extract named entities GATE is distributed with an Information Extraction component set called ANNIE AN-NIE contains a rule-based engine. ANNIE extracts nauned entities using gazetteers and language features like Part of Speech etc The named entities we extracted include person, location, organization, and time. <ref type="table" target="#tab_1">Table 1</ref> shows the performance evaluation of the GATE named entity extraction module on the CoNLL 2004 corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Entity Relation Extraction</head><p>We use an approach applying kernel methods on dependency trees for relation extraction. Kernel methods are widely used in a variety of machine learning problems with many popular algorithms such as Support Vector Machines <ref type="bibr" target="#b9">[10]</ref> (SVM) and Perceptron <ref type="bibr" target="#b5">[6]</ref>.    <ref type="bibr" target="#b10">[11]</ref>. A node in a dependency tree is the corresponding word or words in the original sentence. The dependence between words could be verb-subject, verb-object, verbadjective, etc. First we process each sentence by NLP tools to obtain the dependency information, named entities and word features such as PoS tags. We then generate a dependency tree for every pair of named entities in a sentence. A similarity score between two trees can be computed recursively from the roots of the trees (see <ref type="bibr" target="#b10">[11]</ref> for details).</p><p>One important observation of dependency trees is that if a node does not contain a descendant with a relation argument in its subtree, the node usually will not be involved in the relationship between entities. For example, in <ref type="figure">Figure 5</ref>, removing the node "a" from both trees will not change the fact that the two trees are similar to each other.</p><p>Based on this observation, we define the concept of stripped dependency tree (SDT) as follows: an SDT is a subtree of a dependency tree. Every node in an SDT has at least one descendant node that is a relation argument of a relation. We only consider binary relations in this paper. Hence, each relation has two relation arguments. For example, in a relation (Pittsburgh, located in, Pennsylvania), Pittsburgh and Pennsylvania are the relation arguments. Nodes are removed if they do not have a descendant with relation argument. Stripping a dependency tree can be done in 0(n) with a depth first search (DFS), where n is the number of nodes in the dependency tree. Because we have to perform a DFS to find the min- <ref type="figure">Figure 5</ref>: Dependency trees for "John bought a house near Seattle" and "Mary quickly found a car in Chicago". imum subtree that contains the specified entities, adding the node elimination will not increase the computational complexity. Note that there will be no non-matching nodes in the SDT. Hence, there is no difference between the contiguous tree kernel and the sparse tree kernel for SDT.</p><p>The advantages of SDTs are as follows:</p><p>1. The SDT is computationally efficient. We can use the continuous tree kernel algorithm to compute the result. An SDT usually has fewer nodes than the original dependency tree.</p><p>2. The matching process for SDTs will not be interrupted by ir- Some important information may be discarded when a dependency tree is converted to an SDT. For example, a sentence "John likes Mary." should not be matched with "John doesn't like Mary.". To handle this, and to improve the performance of the algorithm, we have made some enhancements; we describe them below.</p><p>We use MINIPAR to parse each sentence; we then generate a dependency tree for the sentence using output of MINIPAR. MINI-PAR can achieve about 88% in precision and 80% in recall for determining dependency relationships7. By linking each word with its head word of the MINIPAR output, dependency trees can be created for all the sentences. For each node, we assign a set of features generated by GATE. We used the features used by <ref type="bibr">Culotta and Sorensen [11]</ref>. To improve the performance, we added some additional features. They are orthography, word root, verb voice, verb negation, and synonyms obtained from WordNet. <ref type="table" target="#tab_4">Table 3</ref> lists all the features that we used in our experiments. Note that we represent WordNet hypemym and synonym set by their set-id. WordNet will give a list of hypernym sets ordered by estimated frequency and likewise for the synonym sets. We use both hypernym and synonym sets to capture the semantics of words.</p><p>We have implemented the tree kernel algorithms in Java and used it in an SVM. We augment the LibSVM <ref type="bibr" target="#b8">[9]</ref> implementation to use 7http://www.cs.ualberta.ca/ lindek/minipar.htm  <ref type="table" target="#tab_3">Table 2</ref> shows examples for each relation. Since our extractor already captured the Subject-Verb-Object structure using dependency trees, we only trained the SVM for the first four relations. We compared the tree-kernels results with the best results published in <ref type="bibr" target="#b14">[15]</ref> obtained using a linear programming algorithm. The F1 is computed by considering both precision and recall. Precision is the ratio of the number of correctly predicted positive answers to the number of total predicted positive answers. Recall is the ratio of the number of correctly predicted positive answers to the number 8available at ttp:H1l2r.cs.uiuc.edu/-cogcomp/Data/ER/conllO4.corp <ref type="figure">Figure 6</ref>: A fragment of the FEMA National Situation Updates with entities highlighted of positively annotated relations. We use the following equation to compute the F1 score.</p><p>2 * Prec. * Rec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F1</head><p>Prec. + Rec. <ref type="table" target="#tab_5">Table 4</ref> shows the performance comparison between F1 scores for both LP and SVM with tree kernels using 5-fold crossvalidation. The results indicate that tree kernel approaches outperform the linear programming approach for all relations tagged in the testing corpus. The tree kernel with SDT further improved the original dependency tree kernel described in [ 1]. As shown in 4, combining contiguous tree kernel with SDT did not improve the precision very much. The recall of the tree kernel with SDT is 3% to 8% better than the original dependency tree kernel. By reducing noise introduced by unnecessary nodes in dependency trees, the tree kernel performed better prediction for identifying relations between entities. We also experimented with different values for the decay factor A. Our observations are in line with those observed by Culotta and Sorensen <ref type="bibr" target="#b10">[11]</ref>. The performance did not vary much with different A values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Geo-Coding</head><p>Grounding a location name with a correct coordinate is challenging. Essentially, we want to disambiguate places with the same name such as "Springfield" to the unambiguous "Springfield, IL" and "Springfield, PA". If additional information is not available, the geo-coding for "Springfield" becomes arbitrary. In our system, we have implemented a dedicated component called GeoTagger that is used for geo-coding tasks.</p><p>GeoTagger takes a short text as input then determines a geographical scope for the text. The geographical scope of a text segment is determined based on the locations with higher certainty. We define geographical scope in four levels: World, Continent, Country, and Province. For each text segment, the largest geographical scope will be used to select candidate coordinates. For example, if "United States", "Pennsylvania" and "Georgia" are extracted from a text segment, then the geographical scope is determined at the "Country" level. Determining the geographical scope for a text can eliminate unlikely candidate locations. For example, if the highest geographical scope is "Country" and two countries, the "United States" and "Germany" have been extracted from the text, then only locations within these two countries will be considered in the disambiguation algorithm.</p><p>Even within a geographical scope, each location name could have several candidate locations. We perform geo-spatial disambiguation based on the following intuition: Location names that occur close together in the same document segment refer to places that are geographically close. For each location name, GeoTagger obtains the latitude-longitude of all possible places with the same location name, provided the place is within the geographical scope. GeoTagger then clusters locations such that each cluster contains only one occurrence of a location with a particular location name. That is, two Springfields cannot belong to one cluster, because the objective of clustering is to separate the possible locations and choose the location that is closest to the other (possibly non-ambiguous) location names.</p><p>The clustering algorithm works as follows. For each location name n in a set of location names N, we construct a list of latitudelongitudes of places with the name n. GeoTagger uses the k-means clustering algorithm with a parameter k to control the maximum number of clusters. The distance between two points is computed using the Euclidean distance between their coordinates. The result of the k-means algorithm is a one-to-one mapping between location names and their coordinates. The algorithm is listed in Algorithm 1. GeoTagger increments the value of k starting from k 2 until clusters are obtained that cover all location names in the set N such that no cluster contains two locations with the same name. Despite the simplicity of our geographical names disambiguation algorithm, the algorithm works reasonably well on the situation reports. However, geographical names that refer to large areas such as "Great Plains" or " The Mississippi River" may create difficulty since we only consider point-to-point distances. Extension of GeoTagger to handle non-point locations is left as future work. : The Concept map generated from the segment of the FEMA National Situation Updates shown in <ref type="figure">Figure 6</ref> 4.4 Document Segmentation and Topic Classification We use the n-gram language model described in <ref type="bibr" target="#b13">[14]</ref> for topic classification. The classification model estimates the maximum likelihood for a sequence of words by computing conditional probability of previous n -1 words. A category then can be decided by picking c* C C = tcl .ci .. , CC } that has the largest posterior probability</p><p>given the text with the following equation. where D is the text segment. We use the DynamicLMClassifier implementation in LingPipe <ref type="bibr" target="#b2">[3]</ref>. Text segments will be classified into one of nine categories that commonly appear in situation reports. The categories include disease, noticeable, snow, wildfire, earthquake, rain, thunderstorm, winter storm, and others. The noticeable category includes some uncommon incidents deserving of notice such as tornadoes, power plane exploding, and chemical leakage. 20 text segments are manually selected in each category as training corpus. Our preliminary evaluation indicates the topic classification model could achieve 93% accuracy. For text documents where the topics are not known or manually labeled training examples are not available, a clustering based topic detection [5] and text segmentation algorithm can be employed <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Geo-temporal Visualization</head><p>We provide the following visualization capabilities with FE-MARepViz.</p><p>First, the extracted graph between named entities that is generated as an OWL <ref type="bibr" target="#b6">[7]</ref> file can be displayed with ConceptVista ( <ref type="figure">Figure  7</ref>). In the named entity graph, entities are color-coded for their entity type. Links between entities indicate relations among them. The ConceptVista visualization is useful for users to grasp the key concepts (people/organization/location) within the reports without reading them.</p><p>Second, FEMARepViz visualizes the situation reports using Google Earth. From each report, events are detected using the segmentation algorithm mentioned above and geo-coded. Each event is represented using an icon that shows the type of event that occurred (using one icon for each of the known categories listed above). Geocoded incidents will be generated in Google Earth KML format and upload to Google Earth by a Network Link. If a particular event occurred in multiple places, the icon for that event is shown at all the locations on Google Earth. By using a right-click of the mouse, an user can see a summary of the event associated with an icon on a translucent pane that is hidden once the next icon is clicked.</p><p>Each situation report has a date associated with it. FEMARepViz extracts the first date entity in each report and takes it to be the date the report was published. A timestamp is added to each incident to indicate the temporal relationship between incidents. Our user interface has a time-slide wedge using which an user can change the timeline and examine the incidents that happened during the time period around the world <ref type="figure">(Figure 8)</ref>.</p><p>Third, Google Earth is used to display the extracted information similarly to Google Map. However, in this version, the user can use a frame at the left-hand side to select topics, people, location and time that is of interest. This allows the user to filter out the entire set of events and display only a set of events that are of interest to the user. Moreover, Google Earth allows users to play the dated events as an animation. The feature is useful for visualizing a sequence of events such as spreading of a wildfire or movement of a hurricane over different locations. casting modules are under development.</p><p>Periodic Incidents Forecasting: Periodic incidents can be predicted with time series analysis such as moving average or exponential smoothing. Incidents periodically appear in situation reports such as weather conditions are usually with seasonal factors. Hence, seasonal adjustment is necessary for forecasting. Using time series analysis we can predict the likelihood for periodic incidents in a region.</p><p>Correlated Incidents Forecasting: Some incidents may occur conditionally after other incidents. For example, a flood may happen in some region after heavy rain. Tornadoes may be caused by unusual heat or thunderstorms. Such correlation can be found in historical reports and used to forecast future incidents.</p><p>Although our relation extraction and geographical names disambiguation algorithms work reasonably well on the testing corpus and the situation reports, we will conduct more experiments to justify the significance of the performance improvement.</p><p>Moreover, we will add more data sources with varying reliability and types of events including ProMED Mail, Global Disaster Alert and Coordination System, news media, and Internet blogs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We present an architecture and an implementation of a system that can be used to automatically extract information from vast amounts of textual data and present the extracted information visually to end-users. We use FEMA National Situation Updates as our test bed to demonstrate the usefulness of our system. Our system, FE-MARepViz utilizes an entity-relationship extractor, FactXtractor, to extract named entities and links entities with syntactic and semantic relations. Named entity graphs can be created to visualize entity relations. Furthermore, we use GeoTagger to resolve geographical ambiguity and create geo-temporal visualizations with FEMARepViz and Google Earth.</p><p>Since there is no computational approach that can achieve human-level accuracy for complex information extraction tasks, visualization could be a solution to bridge the gap between fully automated extraction and search systems and manual data processing. We believe that our system can benefit users who have needs to analyze massive geo-temporal information in an efficient manner. We conjecture that our system architecture can form the basis for future visual analytics systems over text data, especially for information with important geo-spatial and temporal attributes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overviewx and the user's activities can be logged for future improvement of the extraction from text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Information processing workflow</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 :</head><label>1</label><figDesc>Algorithm for location selection Input Geographical scope GS, Set of location names N, Maximum number of clusters K, threshold w Output: Map of coordinations M begin M &lt;-4) for each n C N do L Coorsn &lt;-coordinates within GS match n for k = 1 to K do Arbitrarily select coorO E Coorsn -* M(n) Coorsn Coorsn -cooro} Dk = k-means(k,M) for each n E N do for each coori C Coorsn do if Replacing M(n) by coori reduce Dk then LM(n)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Evaluation of the GATE named entity extraction module</figDesc><table><row><cell></cell><cell cols="3">PER LOC ORG</cell></row><row><cell cols="3">Precision 0.89 0.81</cell><cell>0.65</cell></row><row><cell>Recall</cell><cell cols="2">0.74 0.79</cell><cell>0.69</cell><cell>adi Ilelse</cell></row><row><cell>F1</cell><cell>0.8</cell><cell>0.8</cell><cell>0.66</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Seaie</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>tboiight</cell></row></table><note>34ile -"-.S.. br.near A dependency tree represents of a parsed sentence and shows the relations between words</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Annotated relations in the training corpus.</figDesc><table><row><cell>Relations</cell><cell>Example</cell></row><row><cell>loc, locatedin, loc</cell><cell>(Seattle, locatedin, Washington)</cell></row><row><cell>per, work for, org</cell><cell>(Steve Jobs, work for, Apple)</cell></row><row><cell cols="2">org, orgBasedin, loc (Microsoft, orgBased in, Redmond)</cell></row><row><cell>per, live in, loc</cell><cell>(Bush, live in, D.C.)</cell></row><row><cell>per, kill, per</cell><cell>(Oswald, kill, JFK)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>List of feature assigned to each tree node. We use the CoNLL 04 corpus8 to evaluate the performance of tree kernels. The corpus consists of articles from several different sources such as WSJ and AP. There are 5925 sentences in the corpus. Among those sentences, 5336 named entities and 2040 binary relations are manually annotated. The annotated named entities include 1685 persons, 1968 locations, 978 organizations and 705 others. The relations between those named entities include 406 locatedin, 394 work for, 451 orgBasedin, 521 livein, and 268 kill.</figDesc><table><row><cell>Feature</cell><cell>Example</cell></row><row><cell>word</cell><cell>John</cell></row><row><cell>POS</cell><cell>NN</cell></row><row><cell>General POS</cell><cell>N</cell></row><row><cell>Entity type</cell><cell>Person</cell></row><row><cell>Relation argument WordNet hypernym</cell><cell>arg, 4274300</cell></row><row><cell>WordNet synonym</cell><cell>4274300</cell></row><row><cell>Orthography</cell><cell>upperlnitial</cell></row><row><cell>Chunk tag</cell><cell>NP</cell></row><row><cell>Word root</cell><cell>john</cell></row><row><cell>Verb voice</cell><cell>active</cell></row><row><cell>Verb negation</cell><cell>yes</cell></row><row><cell>the tree kernels.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison of relations classification results demonstrating improvement of our tree kernel + SDT method over the existing LP method[1 5] for the CoNLL 04 corpus. 64.0 58.9 78.6 58.4 67.0 79.7 61.5 69.4 work for 69.2 50.5 58.4 77.7 61.2 68.5 76.3 65.1 70.2 orgBased in 76.7 50.3 60.7 67.2 55.6 60.8 71.3 60.1 65.</figDesc><table><row><cell>Relations</cell><cell>LP</cell><cell></cell><cell>tree kernel</cell><cell></cell><cell cols="2">tree kernel + SDT</cell></row><row><cell></cell><cell>Prec. Rec.</cell><cell>F1</cell><cell>Prec. Rec.</cell><cell>F1</cell><cell>Prec. Rec.</cell><cell>F1</cell></row><row><cell>located in</cell><cell cols="6">54.5 2</cell></row><row><cell>live in</cell><cell cols="6">60.7 57.0 58.8 74.0 57.1 64.4 79.6 58.9 67.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Geo-temporal visualization with Google Earth from February 4th 2007 to February 6th 2007.</figDesc><table><row><cell>Figure 8:</cell></row><row><cell>I-ype-.i</cell></row><row><cell>Zilme</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.promedmail.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">FUTURE WORKIn the future, we plan to build statistical analysis modules on top of our information extraction system. Currently, two types of fore-</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Homeland Security Program, under the auspices of the Northeast Regional Visualization and Analytics Center (NEVAC). NVAC is operated by the Pacific Northwest National Laboratory (PNNL), a U.S. Department of Energy Office of Science laboratory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work was performed with partial support from the National Visualization and Analytics Center (NVAC), a U.S. Department of</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">GATE: General architecture for text engineering</title>
		<ptr target="http://gate.ac.uk" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="http://www.healthmap.org/" />
		<title level="m">HEALTHmap</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<ptr target="http://www.alias-i.comnlingpipe/" />
		<title level="m">LingPipe</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alertmap</forename><surname>Rsoe Havaria</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">On-line new event detection and tracking</title>
		<imprint>
			<date type="published" when="1998" />
			<publisher>ACM Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Theoretical foundations of the potential function method in pattern recognition learning. Automation and Remote Control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Braverman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">I</forename><surname>Rozoner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1964" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="821" to="837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Owl web ontology language reference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bechhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Van Harmelen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hendler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Horrocks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Patel-Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Stein</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A shortest path dependency kernel for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2005-10" />
			<biblScope unit="page" from="724" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">LIBSVM: a library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/cjlin/libsvm" />
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="273" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dependency tree kernels for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Culotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sorensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving semi-supervised acquisition of relation extraction patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Greenwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Information Extraction Beyond The Document</title>
		<meeting>the Workshop on Information Extraction Beyond The Document<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07" />
			<biblScope unit="page" from="29" to="35" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Shallow semantics for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Bejan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Morarescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence (IJCAI &apos;05)</title>
		<meeting>the Nineteenth International Joint Conference on Artificial Intelligence (IJCAI &apos;05)<address><addrLine>Edinburgh, Scotland, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1061" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Language and task independent text categorization with simple language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL &apos;03: Proceedings of the 2003 Conference of the North American Chapter ofthe Associationfor Computational Linguistics on Human Language Technology</title>
		<meeting><address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="110" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A linear programming formulation for global inference in natural language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-T</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2004</title>
		<meeting>CoNLL-2004</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Topic segmentation with shared topic detection and alignment of multiple documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;07: Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="199" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Ace 2004 multilingual training corpus. Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<pubPlace>Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Kernel methods for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zelenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richardella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal ofMachine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1083" to="1106" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Extracting relations with integrated information using kernel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL &apos;05: Proceedings of the 43rd</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m">Annual Meeting on Association for Computational Linguistics</title>
		<meeting><address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="419" to="426" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
