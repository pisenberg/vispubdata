<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Automated Multidimensional Detective</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfred</forename><surname>Inselberg</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tova</forename><surname>Avidan</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Tel Aviv University</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Senior Fellow San Diego SuperComputing Center, and Multidimensional Graphs Ltd, Raanana</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Automated Multidimensional Detective</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T18:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Automation has arrived to Parallel Coordinates. A geometrically motivated classifier is presented and applied, with both training and testing stages, to 3 real datasets. Our results compared to those from 23 other classifiers have the least error. The algorithm is based on parallel coordinates and : has very low computational complexity in the number of variables and the size of the dataset-contrasted with the very high or unknown (often unstated) complexity of other classifiers, the low complexity enables the rule derivation to be done in near real-time hence making the classification adaptive to changing conditions, provides comprehensible and explicit rules-contrasted to neural networks which are &quot;black boxes&quot;, does dimensionality selection-where the minimal set of original variables (not transformed new variables as in Principal Component Analysis) required to state the rule is found, orders these variables so as to optimize the clarity of separation between the designated set and its complement-this solves the pesky &quot;ordering problem&quot; in parallel coordinates. The algorithm is display independent, hence it can be applied to very large in size and number of variables datasets. Though it is instructive to present the results visually, the input size is no longer display-limited as for visual data mining. Motivation and the Algorithm T he display of multivariate datasets in parallel coordinates (abbr. k-coords) transforms the search for relations into a 2-D pattern recognition problem. Until now the discovery involved a skillful interaction between the &quot;detective&quot; and the data display; a process which was illustrated in the &quot;Multidimensional Detective&quot; [3]. It is not surprising that</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>the most persistent requests and admonitions were for tools which, at least partially, automate the Knowledge Discovery process.</p><p>Classification is a basic task in data analysis and pattern recognition and an algorithm accomplishing it is called a Classifier or Rule Finder <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. The input is a dataset P and a designated subset S. The output is a characterization, that is a set of conditions or rules, to distinguish elements of S from all other members of P the "global" dataset. The output may also be that there is insufficient information in the dataset to provide the desired distinction. As an example, a bank manager having data on all his customers (here this is the set P) may want a rule to distinguish the most profitable (or riskiest) customers (this would be the set S) from all others. Such a task, assuming that there is sufficient information in the dataset, can be assigned to a classifier. This paper consisting of the description and results from an automatic classifier based on k-coords is in a sense the sequel to <ref type="bibr" target="#b2">[3]</ref>.</p><p>With parallel coordinates a dataset P with N variables is transformed into a set of points in N-dimensional space. In this setting, the designated subset S can be described by means of a hypersurface which encloses just the points of S. In practical situations the strict enclosure requirement is dropped and some points of S may be omitted (in the lingo of Data Mining these points are called "false negatives"), and some points of P , S are allowed (these are the "false positives") in the hypersurface. The description of such a hypersurface is equivalent to the rule for identifying, within some acceptable error, the elements of S. This is the geometrical basis for the classifier presented here.</p><p>The algorithm accomplishing this entails: use of an efficient "wrapping" algorithm to enclose the points of S in a hypersurface S 1 containing S and typically also some points of P , S; so S S 1 , of course such an S 1 is not unique 1 , the points in P , S S 1 are isolated and the wrapping algorithm is applied to enclose them, and usually also a few points of S 1 , producing a new hypersurface S 2 with S S 1 , S 2 , the points in S not included in S 1 ,S 2 are next marked for input to the wrapping algorithm, a new hypersurface S 3 is produced containing these points as well as some other points in P , S 1 , S 2 resulting in S S 1 , S 2 S 3 , the process is repeated alternatively producing upper and lower containment bounds for S; termination occurs when an error criterion (which can be user specified) is satisfied or when convergence is not achieved. Basically, the "wrapping" algorithm is a fast way of producing a hypersurface enclosing tightly a given point set. The kind of surface produced is a convex-hull approximation. The efficiency of the version implemented here is due to the use of the k-coords representations of N-dimensional objects applied in the description of the resulting hypersurface <ref type="bibr" target="#b3">[4]</ref>. To summarize, initially the wrapping S 1 encloses all the points of S = S 0 . Then in the attempt to remove all extraneous points a cavity is created by the subsequent wrapping. Such cavities are generically denoted by S 2n for n = 1 ; 2; :::. Usually some of the points of S are enclosed in S 2n , so a correction follows with a S 2n+1 , the hypersurfaces with odd subscript, which enclose and add these points to the previous approximation for the enclosure of S. Such a correction may also add some points of P , S which need to subsequently removed, or better reduced, to provide an increasingly tighter bound. So the process entails bounding the designated set S alternately from above and below providing, in case of convergence, an increasingly better approximation for S. It can and does happen that the process does not converge when P does not contain sufficient information to characterize S. It may also happen that S is so "porous" (i.e. sponge-like) that an inordinate number of iterations are required.</p><p>At step r the output is the description of the set S r which consists of: a list of the minimum number of variables needed to describe S without loss of information. Unlike other methods, like the Principal Component Analysis (PCA), the classifier discards only the redundant variables. It is important to clarify this point. A subset S of a multidimensional set P is not necessarily of the same dimensionality as P. So the classifier finds the dimensionality of S in terms of the original variables and retains only those describe S. That is, it finds the basis in the mathematical sense of the smallest subspace containing S, or more precisely the current approximation for it. This basis is the minimal set M r of variables needed to describe S. We call this dimensionality selection to distinguish it from dimensionality reduction which is usually done with loss of information. Retaining the original variables is important in the applications where the domain experts have developed intuition about the variables they measure. The classifier presents M r ordered according to a criterion which optimizes the clarity of separation. This may be appreciated with the example provided in the attached figure, in addition, the current approximation of the rule stated in terms of conditions on the variables M r , which constitutes the description of the current hypersurface, is obtained.</p><p>So on convergence, say at step 2n, the description of S is provided as :</p><formula xml:id="formula_0">S S 1 , S 2 S 3 , S 4 ::: S 2n,1 , S 2n</formula><p>this being the terminating expression resulting from the algorithm. The implementation allows the user to select a subset of the available variables and restrict the rule generation to these variables. In certain applications, as in process control, not all variables can be controlled and hence it would be useful to have a rule involving such variables that are "accessible" in a meaningful way. There are also two options available : either minimize the number of variables used in the rule, or minimize the number of steps, in terms of the unions and (relative) complements, in the rule.</p><p>In the first case, when the first hypersurface S 1 is found, the variables occurring in its description are the minimum number of variables needed to describe S. From this point on the algorithm can be restricted to use only these. If convergence is achieved a rule involving this minimal set of variables is obtained; we fondly refer to this variation as Enclosed Cavities and abbreviate it by EC. By contrast, when the algorithm is allowed to operate on all the initially selected variables at each step, the number of operations in the terminating expression is reduced. This variation of the classifier is called Nested Cavities (abbr. NC). Clearly the minimal set of variables needed to specify S is not given by NC. In practice, the reduction in the number of steps between EC and NC turns out to be substantial.</p><p>It was already pointed out that, dimensionality selection involves finding the dimensionality, M, of the subset S in terms of the original variables. To illustrate, let us consider a set P in N-D and a subset S P which is contained in a p-dimensional (p N , 1) plane (called a p-flat) of N-D. This p-flat is positioned so that every one of it's points can be described as a linear combination of p variables with p being the minimum such number (i.e. basis). Then EC will return M = p. Now S is rotated so that minimally q p of the original variables are needed to describe every one of the points of S as linear combinations. In this case p q = M N. As an example, if N = 3 take P to be a cube with axes parallel edges, and S P a line segment which is parallel to one of the axes so that M = 1 . The line segment S is now rotated so that it is no longer parallel to any of the axes. In that case M = 2 if S is contained in a plane parallel to a principal plane or M = 3 if not. This shows that there is still room for dimensionality reduction methods like PCA to be applied after dimensionality selection. For this will result in new variables involving the minimum number of original variables. The prospect is certainly worth exploring. In the 3 cases presented next the dimensionality was lowered significantly not only by EC but also by NC to less than half and in one case to about 1/4 of the original variables.</p><p>One of the pesky problems in using parallel coordinates for viewing a specific dataset is to somehow find an axes permutation which is "good" (i.e. provides rich visual cues on what may be true or not) about the specific dataset. There is an inherent ordering emerging from dimensionality selection which, as we see below, answers this need well. This ordering is completely dataset specific. Further, since the algorithm is display independent there is no inherent limitation as to the size and number of variables in the dataset. The most significant limitation then in visual data mining is finally overcome. The visual aspects can now be used for displaying the result as well as exploring the salient features of the distribution of data brought out by the classifier. This is not the right forum to analyze the computational complexity and other intricacies of the algorithm. It is worth pointing out that achieving an "optimum", in the sense of minimizing the number of cavities, turns out to be an NP-complete problem. Still the next best thing is done here in terms of discovering the cavities in order of decreasing size. Other relevant aspects are:</p><p>an approximate convex-hull boundary for each cavity is obtained, utilizing properties of the representation of multidimensional objects in k-coords, a very low polynomial worst case complexity of ON 2 jPj 2 in the number of variables N and dataset size jPj is obtained; it is worth contrasting this with the often unknown, or unstated, or very high (even exponential) complexity of other classifiers, an intriguing prospect, due to the low complexity, is that the rule can be derived in near real-time making the classifier adaptive to changing conditions, the minimal subset of variables needed for classification is found, the rule is given explicitly in terms of conditions on these variables, in terms of included and excluded intervals, and provides "a picture" showing the complex distributions with regions where there is data and "holes" with no data; that can provide significant insights to the domain experts,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>T hree datasets, benchmarks in classification, are used to test the classifier. The results are then compared to those obtained with other well-known classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>On the classifiers</head><p>During 1990-1993, Michie, Spiegelhalter and Taylor <ref type="bibr" target="#b4">[5]</ref>, on behalf of the ESPRIT program of the European Union, made extensive studies of several classifiers applied to diverse datasets. About 23 different classifiers were applied to about 20 datasets for comparative trials in the StatLog project. This was designed to test classification procedures on large-scale commercially important problems in order to determine suitability of the various techniques to industry. There were three main types of classifiers used:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Extension to Linear Discrimination</head><p>This group includes algorithms which start with linear combinations and are followed by non-linear transformations of various sorts. Seven (7) different such classifiers were used named : Discrim, Logdisc, Quadisc, SMART, Backdrop, Cascade and DIPOL92.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Decision Trees and Rule-Based Methods</head><p>The 9 decision tree and rule-based methods used were : NewID, AC 2 , Cal5,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C4.5, CART, IndCART, Baytree, CN2, ITRule</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Density Estimates</head><p>These algorithms estimate probability density at each point, they were : Naive-Bay, CASTLE, ALLOC80, K-NN, RBF, Kohonen and LvQ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data, Results and Comparisons</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Satellite image data</head><p>Satellite data is used extensively in military, meteorological, earth resources planning and lots of other applications. The specific dataset used in Statlog is from a region in Australia. It consists of multi-spectral values and associated classification according to ground type and can be found in the Statlog ftp site. Each frame consists of four digital images of the same scene in different spectral bands, two in the visible and two in the near infra-red region. There are 36 variables (the attributes) and the class attribute for six (6) soil types i.e. the six classes to be characterized by the classifier(s). The data has 4435 samples(data items) for the training set and 2000 samples in the test set. By way of explanation, for validation the dataset is partitioned into training and testing subsets, the "popular" proportions being about 2/3 to 1/3. The rule is derived, by the classifier, on the the training set and tested on the remainder of the data; the error pertains to the false "positives" and "negatives". The important measure is the test error which should be as small as possible; to a lesser extent the difference between the train and test errors should be small. The comparative results are shown in <ref type="table" target="#tab_0">Table 1</ref> below.</p><p>By way of an example, a rule found by NC for one of the classes of this dataset required 23 out of the 35 attributes and was done in 4 iterations (i.e.only 4 hypersurfaces were needed). The class size was jSj = 479 (out of a total of 4435 items). It turns out that jS 1 j = 1291; jS 2 j = 831; jS 3 j = 162; jS 4 j = 143. Notice that,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="479">= jSj = 460 + 19 = jS 1 , S 2 S 3 , S 4 j</head><p>So in this case the rule found is "exact". Of course, this should not be taken literally since it depends on the actual data items used for training as can be seen from the test error. Not surprisingly the larger the datasets the more reliable are rules found and the closer are the training and testing errors. In a great many cases S 2 turned out to be the hypersurface requiring the largest number of variables for its definition. We conjecture that this is an indication of the existence of many "borderline" cases (i.e. close elements in the class S and it's complement) and it may suggest that the class definition may be "fuzzy".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vowel recognition data</head><p>This is an interesting problem in speech recognition where likely users are the physically handicaped, or those with "busy hands", or without keyboard access. The tasks may involve changing radio frequencies in airplane cockpits, asking for stockmarket quotations on the telephone etc.</p><p>The process involves digital sampling of speech then acoustic signal processing, followed by recognition of the phonemes, groups of phonemes and words. The goal here is a speaker-independent rule based on 10 variables of 11 vowels occurring in various words spoken (recorded and processed) by 15 British male and female speakers. Deterding <ref type="bibr" target="#b0">[1]</ref> collected this dataset of vowels and which can be found in the CMU benchmark repository on the WWW. There are 528 entries for training and 462 for testing. Three other types of classifiers were also applied to this dataset: neural networks and k-NN by Robinson &amp; Fallside <ref type="bibr" target="#b7">[8]</ref>, and Decision trees by Shang and Breiman <ref type="bibr" target="#b8">[9]</ref>. For the sake of variety both versions of our classifier were used and a somewhat different error test procedure was used. The results are shown in <ref type="table" target="#tab_1">Table 2</ref> and speak for themselves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monkey neural data</head><p>We have decided to include the result on this dataset due to its interesting and ..! The result shows that the data consists of two "banana-like" 2 clusters in 8-D one (the complement in this case) enclosing the other (class for which the rule was found). Note that the classifier can actually describe highly complex regions. It can build and "carve" the cavity shown. It is no wonder that separation attempts in terms of hyperplanes or nearest-neighbour techniques can fail badly on such datasets. The rule gave an error of 3.92 % using train-and-test with 66 % of the data for training) and impressed the Yale group -not an easy feat!</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary and Conclusions</head><p>The Nested Cavities (NC) with the smaller number of steps and the larger number of variables gives, not surprisingly, constistently better results than the Enclosed Cavities (EC) version of the classifier.</p><p>The larger the dataset the better the classification results.</p><p>The classifier works best with continuous variables though it can handle well a small (i.e. no more than 20 % of the total) number of categorical variables.</p><p>The geometric formulation combined with the results on the representation of multidimensional objects in k-coords gave a classifier with remarkably low computational complexity. This makes feasible the classification of truly large in size and number of variable datasets something we hope to test with suitable partners in the near future. The low complexity, enables the derivation of the rule in near real-time, and then apply it to incoming data, rendering the classifier adaptive to changing conditions. The rules provided are explicit, and "visualizable" and yield dimensionality selection which choses and orders the minimal set of variables needed to state the rule without loss of information. As it often happens such work raises new questions, on termination criteria, automatic approaches to overfiting, interpretation of the "geometry" of the dataset as described by the rule and others. Automation with of Knowledged Discovery with parallel coordinates in finally in sight. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Internet Repositories</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>RANK</cell><cell>CLASSIFIER</cell><cell cols="2">Error rate % Train Test</cell></row><row><cell>1</cell><cell>Nested Cavities (NC)</cell><cell>4.3</cell><cell>9.0</cell></row><row><cell>2</cell><cell>k-NN</cell><cell>8.9</cell><cell>9.4</cell></row><row><cell>3</cell><cell>LVQ</cell><cell>4.8</cell><cell>10.5</cell></row><row><cell>4</cell><cell>DIPOL92</cell><cell>5.1</cell><cell>11.1</cell></row><row><cell>5</cell><cell>RBF</cell><cell>11.1</cell><cell>12.1</cell></row><row><cell>6</cell><cell>ALLOC80</cell><cell>3.6</cell><cell>13.2</cell></row><row><cell>7</cell><cell>IndCART</cell><cell>2.3</cell><cell>13.8</cell></row><row><cell>8</cell><cell>CART</cell><cell>7.9</cell><cell>13.8</cell></row><row><cell>9</cell><cell>Backprop</cell><cell>11.2</cell><cell>13.9</cell></row><row><cell>10</cell><cell>Baytree</cell><cell>2.0</cell><cell>14.7</cell></row><row><cell>11</cell><cell>CN2</cell><cell>1.0</cell><cell>15.0</cell></row><row><cell>12</cell><cell>C4.5</cell><cell>4.0</cell><cell>15.0</cell></row><row><cell>13</cell><cell>NewID</cell><cell>6.7</cell><cell>15.0</cell></row><row><cell>14</cell><cell>Cal5</cell><cell>12.5</cell><cell>15.1</cell></row><row><cell>15 16</cell><cell>Quadisc AC 2</cell><cell>10.6</cell><cell>15.5 15.7</cell></row><row><cell>17</cell><cell>SMART</cell><cell>12.3</cell><cell>15.9</cell></row><row><cell>18</cell><cell>Cascade</cell><cell>11.2</cell><cell>16.3</cell></row><row><cell>19</cell><cell>Logdisc</cell><cell>11.9</cell><cell>16.3</cell></row><row><cell>20</cell><cell>Discrim</cell><cell>14.9</cell><cell>17.1</cell></row><row><cell>21</cell><cell>Kohonen</cell><cell>10.1</cell><cell>17.9</cell></row><row><cell>22</cell><cell>CASTLE</cell><cell>18.6</cell><cell>19.4</cell></row><row><cell>23</cell><cell>NaiveBay</cell><cell>30.8</cell><cell>28.7</cell></row><row><cell>24</cell><cell>ITrule</cell><cell cols="2">Failed Failed</cell></row></table><note>Summary of the StatLog results and comparison with the Nested Cavities (NC) classifier for the satellite image data. The error is averaged over the six classes.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Summary of classification results for the vowel dataset. unusual features. Here there are two classes to be distinguished consisting of pulses measured on two separate neurons in a monkey's brain (poor thing!). The experiment was conducted at the Yale Medical School and we received the data from Prof. R. Coiffman's group which has been working on this classification problem. There are 600 samples with 32 variables. Remarkably, convergence was obtained with only one iteration and dimensionality selection required only 8 of the 32 parameters. The resulting ordering shows a striking separation. In the attached figure the first pair of variables x 1 ; x 2 originaly given is plotted showing no separation. In the adjoining plot the best pair x 11 ; x 14 , as chosen by the classifier's ordering, shows remarkable separation. The discovery of this manually would require constructing and inspecting a scatterplot with 496 pairs .</figDesc><table><row><cell>Rank</cell><cell>Classifier</cell><cell>Testing Mode</cell><cell>Test Error Rate %</cell></row><row><cell>1</cell><cell>Nested Cavities (NC)</cell><cell>Cross validation</cell><cell>7.9</cell></row><row><cell>2</cell><cell>CART-DB</cell><cell>Cross validation</cell><cell>10.0</cell></row><row><cell>3</cell><cell>Nested Cavities (NC)</cell><cell>Train &amp; Test</cell><cell>10.5</cell></row><row><cell>4</cell><cell cols="2">Enclosed Cavities (EC) Cross validation</cell><cell>13.9</cell></row><row><cell>5</cell><cell></cell><cell>Train &amp; Test</cell><cell>13.9</cell></row><row><cell>6</cell><cell>CART</cell><cell>Cross validation</cell><cell>21.8</cell></row><row><cell>7</cell><cell>k-NN</cell><cell>Train &amp; Test</cell><cell>44.0</cell></row><row><cell>8</cell><cell>RBF</cell><cell>Train &amp; Test</cell><cell>46.5</cell></row><row><cell>9</cell><cell>Multi-layer perceptron</cell><cell>Train &amp; Test</cell><cell>49.4</cell></row><row><cell>10</cell><cell>Single-layer perceptron</cell><cell>Train &amp; Test</cell><cell>66.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>1. ftp.ics.uci.edu/pub/machine-learning-databases 2. ftp.ics.uci.edu/pub/machine-learning-databases/statlog 3. clyde.boltz.cs.cmu.edu/bench.html</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">To avoid unnecessary verbiage by a statement S j S k we also mean that the set of points enclosed in the hypersurface S j is contained in the set of points enclosed by the hypersurface S k .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">One observer suggested that this was due to the monkey's thinking of bananas during this fateful experiment ...!</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Speaker Normalization for Automatic Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Deterding</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
		<respStmt>
			<orgName>Cambridge University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Advances in Knowledge Discovery and Data Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">M</forename><surname>Fayad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Piatesky-Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Uthurusamy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>AAAI/MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Inselberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detective</surname></persName>
		</author>
		<title level="m">Proc. of IEEE Information Visualization &apos;97</title>
		<meeting>of IEEE Information Visualization &apos;97<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Comp. Soc</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="100" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Don&apos;t panic ... do it in parallel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Inselberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Comp. Stat</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="53" to="77" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Michie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Spiegelhalter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Taylor</surname></persName>
		</author>
		<title level="m">Machine Learning , Neural and Statistical Classification. Ellis Horwood series in AI</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning. McGraw-Hill</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">5 : Programs for Machine Leaarning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>C4</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Morgan Kaufman</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Dynamic Connectionist Model of Phoneme Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fallside</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 1st European Neural Network Conf. (nEURO)</title>
		<meeting>of 1st European Neural Network Conf. (nEURO)</meeting>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distribution based trees are more accurate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 1st Inter. Conf. on Neural Info. Proc. (ICONIP96)</title>
		<meeting>of 1st Inter. Conf. on Neural Info. . (ICONIP96)</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page">133</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
