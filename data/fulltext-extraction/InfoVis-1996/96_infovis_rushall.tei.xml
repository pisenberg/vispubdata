<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DEPICT: Documents valuated as Pictures Visualizing information Using Context Vectors and Self Organizing Maps</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Rushall</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">HNC Software, Inc</orgName>
								<address>
									<addrLine>5930 Cornerstone Court West</addrLine>
									<postCode>92121</postCode>
									<settlement>San Diego</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">R</forename><surname>Ilgen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">HNC Software, Inc</orgName>
								<address>
									<addrLine>5930 Cornerstone Court West</addrLine>
									<postCode>92121</postCode>
									<settlement>San Diego</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DEPICT: Documents valuated as Pictures Visualizing information Using Context Vectors and Self Organizing Maps</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T18:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>HNC Software, Inc. has developed a system called DEPICT for visualizing the information content of large tenual corpora. The system is built around two separale neural nemork methodologies: context vectors and self organizing maps. Context vectors (CVs) are high dimensional information representations that encode the semantic content of the textual entities they represent. Self organizing maps (SOMs) are capable of tranqorming an input, high dimensional signal space into a much lower (usually two or three) dimensional output space use@l for visualization. Neither process requires human intervention, nor an external knowledge base. Together, these neural network techniques can be utilized to automatically identi&amp; the relevant injormation themes present in a corpus, and present those themes to the user in a intuitive visual form.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years there has been an explosion in the amount of information available on-line. Much of this explosion has been fueled by the spectacular growth of the Internet and especially the World Wide Web. Along with this spectacular growth has come new challenges for effectively locating on-line information, especiaIIy when browsing rather than performing a directed search for a specific piece of information. Key word and linguistically based directed search engines offer some ability to present relevant information to the user. and have resulted in usefirl Internet products such as <ref type="bibr">Yahoo [l]</ref>, Lycos <ref type="bibr" target="#b1">[2]</ref>, and iUta Vistal31. However. these engines suffer from the fact that they require the user to specify a query of limited length and they offer no visual interface for browsing. To solve the problem of browsing the information space in order to find information of interest, new techniques for data retrieval and presentation must be developed, HNC has developed an underlying information representation technology and a concept for information visualization that can solve the problem of effectively browsing large textual corpora. As part of HNC's involvement in the ARPA sponsored TIPSTER program, we have developed a neural network technique that can learn word level relationships from free text. This capability is based upon an approach called context vectors which encodes the meaning and context of words and documents in the form of unil vectors in a high dimensional vector space. Furthermore, as part of HNC's involvement in the US intelligence community-sponsored PlOOO visualization effort, we have applied a secondary neural network process, the Self Organizing Map (SOM) <ref type="bibr" target="#b2">[4]</ref>, which uses the document context vectors to build a visual representation of the information content of the corpus. The combination of these technologies allows users to effectively browse the information space, to locate related documents, and to discover relationships between different themes in the information space.</p><p>The remainder of this paper is organized as follows. Section 2 presents an overview of both context vectors m d the Self Organizing Map. Section 3 presents the DEPICT system and presents the user interface, automatic region finding and region labeling, information retrieval and docmnent highlighting, and temporal analysis of the information space. Finally, Section 4 presents some concluding remarks and directions for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Technical Background</head><p>The DEPICT system is based on two technologies: context vectors and the SOM. Context vector technology was developed at HNC and has been demonstrated to be highly effective for such tasks as text retrieval (using a system called MatchPlusl [SI, text routing <ref type="bibr" target="#b4">[6]</ref>, and image retrieval <ref type="bibr" target="#b5">[7]</ref>. SOM technology was originally developed by T. Kohonen and has been used throughout the neural network community as a method for representing information in a manner suitable for visualization IS]. The following subsections present an overview of each of these technologies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Context Vectors</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Training the SOM</head><p>The key technical feature of context vector technology is the representation of terms, documents, and queries by high dimensional vectors consisting of real-valued numbers or components.</p><p>These vectors are constrained to be unit vectors in the high dimensional vector space. Since both terms (words or stems) and documents are represented in the same frame of reference, this allows several unique operations. All operations in MatchPlus are based 011 geometry of these high dimensional spaces <ref type="bibr">[9]</ref>. Specifically, closeness in the space is equivalent to closeness in SUbJeCt content. A neural network-based learning algorithm is designed to adjust word vectors such that [ems that are used in a similar context will have itectors that point in similar directions.</p><p>The determination of similar context is based upon the use of word stem co-occurrence statistics. Once trained, these word stem context vectors are used as building blocks for creating document and query context vectors. Specifically, context vectors for documents and queries are formed as the normalized weighted sum of the word stem context vectors for the word stems found in the document or query. This process results in the fact that context vectors for documents with similar subject content will point in similar directions. This vector representation of information content can thus be used for document retrieval, routing, document clustering (self organizing subject index) and other text processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Kohonen's Self Organizing Map</head><p>The concept of self organizing maps was first developed by Tuevo Kohonen in 1981 at the University of Helsinki. Kohonen demonstrated that a system could be taught to organize the data it was given, without the need for supervision or external intervention, through the use of competitive learning. Typically, the SOM consists of a collection of nodes arranged in a regular two dimensional grid. Each node corresponds to a cluster centroid vector for the high dimensional input vector space. A self-organizing training process is used to adjust the node vector components in an iterative fashion. Upon completion of training, the SOM node vectors have the property that node vectors that are close in the high dimensional vector space will be close in the two dimensional grid space, or *'map space." This training process is described in more detail below.</p><p>Assume the input space is comprised of N vectors, each of dimension n. Furthermore, assume a two dimensional array of "nodes" that will be trained to represent the N input vectors. Each of these nodes is also a vector of dimension n.  Before training, the node vectors are assigned random values. That is, a pseudo random number generator is used to assign each node in the map a random unit vector of dimension n. In the case where n is very large (-3oO), these initial conditions represent a quasiorthogonal state, i.e., each unit vector is approximately orthogonal to each other unit vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>@ @'@ @ @-</head><p>The SOM training algorithm is based on a simple, iterative comparison process, where the N input vectors are compared to each of the node vectors in the map. In essence, the node vectors are competing to have their values adjusted. The idea is to find the node vector that is "nearest" (in vector space) to the input vector. The node vector that is nearest is deemed the "winner" of this competition, and is rewarded by having its vector adjusted.</p><p>The adjustment comes in the form of moving the winning node vector in the direction of the input vector. The SOM extends this simple competitive learning process to include updates for the "neighbor" nodes to the winning node. These neighbor nodes are nodes that are close (in the map space) to the winning node. For example, in <ref type="figure" target="#fig_0">Figure 1</ref>, the neighborhood of node W2,3 is depicted, and is defined as those nodes that are within one row or one column of node W2,3. Neighborhoods can be larger or smaller; this is just one example. The updates for these neighbor nodes are smaller than the updates for the winning node, and the size of the neighbor node update is smaller for neighbors that are farther away (in map space) from the winning node. The inclusion of neighbor updates results in the organization of the information into a form suitable for visualization. The algorithm can be represented in pseudo-code as:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>For each input vector V Find node vector Wq that is closest to V Update Wid according to</head><formula xml:id="formula_0">Wid = Wij + a( V -W,)</formula><p>Find nodes that are close to node i,j in m p space For each of these "neighbor" nodes</p><formula xml:id="formula_1">( I w j j -V I &lt; I WkJ' V I vk, 1 ) UPdate wk,J according t0 wk,l= WkJ -k S@ v -W k J )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>where (0.0 S s I l . 0 ) End Loop over neighbor nodes End Loop over input vectors</head><p>The size of the adjustment, a, will determine how quickly the map space node vectors will converge to an accurate representation of the input space vectors. One loop through the input vector set is not sufficient to train the node vectors. It is necessary to perform this loop hundreds, and possibly thousands, of times before training is completed. The value of the parameter s for updating neighbor nodes is determined by a Gaussian function based on the nearness of the neighbor node to the winning node. Therefore, close neighbors will be updated, or adjusted, more than neighbors that are further away.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Win Frequency and Conscience</head><p>An ideal characteristic of this training is to have the map node vectors win the competition with equal probabilities. Unfortunately, this is not the case for the standard SOM algorithm. A consequence of this type of training is that some map nodes may never win the competition. This will result in a less useful representation of the input space. To eliminate this undesirable effect, DeSieno <ref type="bibr">[lo]</ref> has developed an improved competitive learning algorithm that makes use of the idea of "conscience''. The conscience mechanism allows nodes that are observed to rarely win the competition to subsequently win more often, and it prevents nodes that frequently win the competition from subsequently winning too often.</p><p>The conscience mechanism is employed as a second competition based on the outcome of the first competition described above in the self organizing algorithm. Before conducting the second competition, the conscience mechanism creates a bias factor for each node. The value of the bias is determined by the running statistics kept on the first competition. Nodes that normally lose the first competition are given favorable biases, and those that normally win are given unfavorable biases. The winner of this second competition is determined upon the basis of biased distance to the input vector. This biasing enforces an equiprobable winning distribution and results in a more useful clustering of the input information space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Computation Issues</head><p>Earlier we alluded to the fact that these algorithms are computationally intensive. This intensity depends upon three factors. One is the dimensionality of the vectors. Using higher dimensioned vectors (-1000) is possible, but adds to the timely computation problem. Likewise, the number of input vectors, as well as the number of map node vectors, will determine the scale of the problem. Fortunately, the nature of these algorithms is well suited for parallel processing architectures. Therefore, scalability of the algorithm depends on the number of processors that can be used to compute a solution.</p><p>HNC has developed a hardware architecture that is designed to handle neural networks, and in particular. the compute intensive processes they model. The hardware is a SIMD numerical array processor (SNAP), in essence a floating-point parallel array processor. The SNAP is ideally suited to determine the computationally intensive solution required by the §OM algorifhms.</p><p>Because of its availability to us. the SNAP was utilized for all SOM training. This provided us wiLh a much quicker turn-around time for evaluating the effectiveness of training runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DEPICT and Docuverse</head><p>The DEPICT system was developed as part of the IC PlOOO sponsored Docuverse research effort at HNC. Docuverse is an ongoing research and development effort with a goal of providing users with a tool to quickly and easily assess the information content of large textual corpora. DEPICT is based on the context vector technology foundation developed here over the past few years, and additionally provides a visual interface that allows the user to browse the information space in a visually appealing fashion. <ref type="figure" target="#fig_1">Figure 2</ref> presents the process by which a text corpus is transformed into some intuitive visual paradigm that users can easily relate to and understand. The initial neural network process is shown along the top part of <ref type="figure" target="#fig_1">Figure 2</ref>. The process flow indicates that some set of textual data, the training text, is used to obtain context vectors for the vocabulary set contained in the training text. This process involves a preliminary step of word "stemming" and stop list removal. Stemming is the process of representing similar word forms as the base form of the words (i.e. words like driver, driving, drives, driven, and drove are all stemmed to the word drive). Stop list removal refers to the removal of words with high frequency occurrence and little meaning in the training text (i.e. words like the, of, and, etc.). After preprocessing, context vectors for the remaining word stems are learned and stored into a database. At this point, the system is ready to process the user's desired corpus: Note that the corpus to be visualized does not need to be the same corpus that the system was trained with. It is helpful, however, if the training corpus is statistically representative of the corpora that will be visualized.</p><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, the control flow now switches to the middle part of the diagram, where the user identifies the corpus to be visualized. This text is preprocessed in the same exact manner as the training text was preprocessed. After this step, one pass through each document is all that is required to calculate a context vector for each document. The stem vectors learned from the training text are used to compute the context vectors for the user's text. These are stored in a document context vector database. It is these context vectors that are given to the self organizing map for visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Interface</head><p>The DEPICT interface presents the user with an array of nodes not unlike the array of nodes depicted in <ref type="figure" target="#fig_0">Figure 1</ref>. The size of the array is configurable by the user, but the default is a 20-by-20 array of nodes.</p><p>Recall that each of these 400 nodes has a context vector associated with it, and that the context vectors have been adjusted to represent the prevalent themes in the corpus. Therefore each node represents an information theme contained in the corpus, It is important to note that it is not necessary that each node have a different theme. Nodes can have similar themes, and in fact, the same theme if there is a relatively large amount of information pertaining to that particular theme within the corpus.</p><p>This discussion raises the question as to how the nodes reflect the amount of information present for the theme they represent. After experimenting with various paradigms such as color or icons, we've concluded that the size, or radius, of the nodes best conveys this information. Large nodes imply a relatively large number of documents for the given information theme. Small nodes imply a relatively small number of documents for the given information theme.</p><p>The way in which the system measures not only the amount of information for a theme, but also the similarity of themes, documents, words, or any free text, is through the use of the vector dot product operation. Recall that each of the aforementioned textual entities is associated with a context vector. Similar entities have context vectors that point in similar directions. The dot product for similar direction vectors will be close to 1.0, while dissimilar vectors will have dot products that are near zero.</p><p>Figure 3 depicts what we call the "corpus integral". This is the broad view of information content for the entire corpus.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>End loop over node vectors</head><p>The corpus integral is very useful in that the user knows, at a glance, which themes are present in the corpus. By "mousing" on a node (i.e. clicking the mouse button once on a node), a pop-up menu reveals, among other choices, the information theme the node represents.</p><p>The corpus that was used to generate the integral depicted in <ref type="figure" target="#fig_1">Figure 3</ref> is a set of over 17,700 documents. The documents are news reports taken directly off the AP News Wire during a four month span in 1990.</p><p>Other system capabilities, discussed in the sections below, allow the user to do a variety of information assimilation and information gathering tasks. For example, an undirected information search, commonly referred to as browsing, is made even easier when using the automatic region finding and labeling mechanism. Searching for specific information is also supported so that the user can request, in free text form, any desired information. A tool for visualizing information in the time domain is also provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Automatic Region Finding and Labeling</head><p>As we stated earlier, nodes that are near one another on the map have the property that they represent similar themes of information. We can exploit this fact to have the system automatically group the nodes into regions of similar themes. This can be thought of as a clustering of the clusters. Furthermore, the system will automatically generate an appropriate name, or label for the region. Consider <ref type="figure" target="#fig_1">Figures 4a and 4b</ref>. <ref type="figure" target="#fig_1">Figure 4a</ref>. The automatically generated regions <ref type="figure" target="#fig_1">Figures 4a and 4b</ref> show one of the many ways that a user can make use of the automatic region generation. They depict the option of selecting all regions found on the map. <ref type="figure" target="#fig_1">Figure 4a</ref> shows all the regions that were found by the system. The regions are outlines drawn around sets of nodes. <ref type="figure" target="#fig_1">Figure 4b</ref> shows a second dialog window that is used to display the labels for the regions. Although all the labels are not visible in the dialog, the region algorithm found a total of 84 distinct regions for the self organized map of the 17,700 AP News Wire documents. <ref type="figure" target="#fig_1">Figure 4b</ref>. The automatically generated labels.</p><p>Instead of showing all regions at once, a user can select regions of interest from either the map or the label dialog. If a user is interested in a particular area on the map, the region outlines for the nodes of interest can be toggled on or off by the pop-up menu provided on each node.</p><p>Alternatively, the user can peruse the list of region labels and select them directly from the list. This will draw the region outline on the map.</p><p>The algorithm for region finding and labeling is a two step process. The first step involves finding the regions. Initially, each node is given its own region. An iterative algorithm compares a region to every other region on the map. The comparison is a vector dot product operation. If the regions are similar enough (i.e., if the dot product between the two regions exceeds some threshold), the two regions are merged into one region. This process is repeated until no region combining occurs.</p><p>The next step involves finding an appropriate label for each region. First, a context vector is computed for the region. This is done by taking the weighted average of the context vectors belonging to the nodes in the region. This centroid region context vector is compared to all of the stem word context vectors in the vocabulary. The stem word vector that results in the highest dot product with the centroid region vector becomes the label for the region. Because of the stemming process, some of the stem words are truncated. Therefore, the user is given the ability to edit the labels to put them in correct grammatical form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Information Retrieval and Document Highlighting</head><p>DEPICT makes use of a rich set of information retrieval functionality.</p><p>This functionality was inherited from another system developed at HNC called MatchPlus <ref type="bibr" target="#b3">[5]</ref>.</p><p>MatchPlus focuses on information retrieval from large textual corpora.</p><p>When a user desires a more focused search for specific information, it is easily accomplished in a variety of ways. If a user has identified a map node representing an interesting theme of information, the user can select, from the node pop-up menu, an option to retrieve documents pertaining to the theme. The system uses the node's context vector to perform dot products with every document context vector in the corpus. The user is presented with a ranked list of the most relevant documents. The list is presented in a window with the document ID, the value of the dot product, and the first line of text in the document, <ref type="figure" target="#fig_1">Figure 5</ref> shows an example of the ranked list.  <ref type="figure" target="#fig_1">Figure 5</ref>. A ranked list of documents.</p><p>Alternatively, rather than using a node for a query, the user can type free text into a window and submit the free text as a query. The system converts the free text into a context vector, and the same retrieval process is performed. Yet another possibility is for the user to use an entire document as a query. Regardless of the method, the retrieval is done via context vector comparisons.</p><p>Once a ranked list of documents has been retrieved, the user can select any document from the list to view.</p><p>The document selected will appear in a separate window. along with a highlighting tool to furthcr examine the relevant parts of the document.</p><p>The highlight tool segments the document into 5-line paragraphs. The user is presented with another window containing a histogram, where each interval of the histogram corresponds to each of the paragraphs in the document, and the height of the interval corresponds to the dot product of the paragraph with the query that was issued. This tool provides the user with a tool to quickly and easily locate the most relevant portions of m y document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Temporal Analysis</head><p>With corpora comprised of periodically released information, it might be usem to visualize the information in the time domain. The DEPICT temporal analysis tool makes this possible. If documents contain a time stamp, the system can utilize this information so that user can see how information varies in the time domain.</p><p>The temporal analysis tool works as follows, The user defines a uniform time interval with which the system uses to segment the data (i.e. typical time intervals would be one hour, one day, or one week). The documents are segmented into consecutive time "slots" according to their time stamp. After each document is placed into the appropriate slot, a cumulative dot product operation is performed with the corpus integral. That is, for each time slot, the document's context vectors within that slot are used in a dot product operation with the corpus integral. The results are summed for each time slot. This operation shows how the information embodied in the documents in that time slot relate to the corpus as a whole, thereby creating a visual summary of the information content of b e documerils received during thal lime interval. This process results in a visual "information time series" which can be viewed in rapid succession. The results are viewcd on the SOM's main interface, just as the corpus integral is viewed. A separate dialog is used for controlling the temporal analysis. <ref type="figure" target="#fig_1">Figure 6</ref> shows the temporal analysis control interface. The user can "play" the time series, step through it interval by interval, or select any individual interval to view. The configure button allows the user to define the intervals. The time series of intervals are viewed on the SOM main interface. The results take on the same general look of the corpus integral, however the map will be representative of only those documents for that particular time interval. When using the temporal tool on the 17,700 AP News Wire documents, weekly cycles are easily identified. By stepping through the data in one day increments, the weekdays (Monday through Friday) are identified by three predominate regions pertaining to themes like hanks, stocks, world news, and taxation. The next two days, the weekend, show maps with two predominate regions, pertaining to themes like music, TV, movies, and various other entertainment themes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Summary</head><p>As we continue through the information age, tools such as DEPICT will no longer be considered luxuries, but rather necessities. We have developed DEPICT as merely a proof of concept system. We feel that this technology is far from realizing its full potential, As information visualization technology evolves and matures, so too will tools like DEPICT.</p><p>We are also in the process of exploring new ways to visualize this powerful information representation technology. One area of interest is developing three dimensional SUMS. The user will be presented with a spherical array of nodes, representing the "world" of information. Used in conjunction with flat maps, the</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc>depicts this arbitrary array of nodes, in this case, a 5-by-5 regularly spaced array of 25 nodes. Each node is uniquely identified by it's (ij) position. @ @ @ @ @ .............................. ..............................</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure I .</head><label>I</label><figDesc>An array of map nodes for self organization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>The process of transforming textual data into an intuitive, graphical visual.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>The corpus integral.The corpus integral is computed by summing the dot products of each document context vector with each node context vector. The summed dot products for each node are used to determine the size of the node.In pseudo-code, For each nude vector Wi,j Nodesizeid = 0 For each document vector V End loop over document vectors NOde-Sizeij /= number of document vectors Nodesizeu += Wu o V</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>The Temporal Analysis Control Interface</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>It is an attempt to graphically illustrate, through various sized information nodes, the entire set of prevalent themes contained in the corpus. Again, nodes that are large represent the themes that occur with the highest frequency and volume in the corpus, Nodes that are small, or not even visible (such as those in the upper left corner of the map), represent themes that occur with a much lower frequency and volume, relatively speaking............ ................ ..... . . e . . . . . ..... ...........</figDesc><table /><note>0 .* .......</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Dwument Soore Subjeats 2395 0.442 Christian Militia Regrow!Rival Christian Forces Battles Rage Between Ch Sniper Fire Persists in Aoun Calls Up Reservist 53 Dead, 133 Wounded in Mediating Committee Str Fighting Between Christ Rival Christian Forces Fighting i n East Beirut Rival Forces Exchange S Christian Militia Clamp Rival Christian Gunners fioun-s Forces Penetrate Rt Least Tu0 Killed As Aoun Gives Rival 72 Hou Christian Forces Duel w Christians Ask Hrami to Helicopter Base Falls t. Truce Holds in Mountain Renewed Fighting Thuart Aoun-s Forces Head into Aoun Forces Seize Strat,</head><label></label><figDesc></figDesc><table><row><cell>16507</cell><cell>0.442</cell></row><row><cell>16424</cell><cell>0.430</cell></row><row><cell>2755</cell><cell>0.429</cell></row><row><cell>539</cell><cell>0.427</cell></row><row><cell>11059</cell><cell>0.427</cell></row><row><cell>1893</cell><cell>0.427</cell></row><row><cell>16721</cell><cell>0.425</cell></row><row><cell>827</cell><cell>0.424</cell></row><row><cell>17302</cell><cell>0.424</cell></row><row><cell>2412</cell><cell>0.424</cell></row><row><cell>16259</cell><cell>0.423</cell></row><row><cell>13665</cell><cell>0.423</cell></row><row><cell>4350</cell><cell>0.423</cell></row><row><cell>10735</cell><cell>0.421</cell></row><row><cell>1940</cell><cell>0.421</cell></row><row><cell>13598</cell><cell>0.420</cell></row><row><cell>7069</cell><cell>0.417</cell></row><row><cell>1790</cell><cell>0.417</cell></row><row><cell>17409</cell><cell>0.417</cell></row><row><cell>3401</cell><cell>0.417</cell></row><row><cell>397</cell><cell>0.416</cell></row><row><cell>110</cell><cell>0.416</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>user would have a hierarchical SOM capable visualizing information at various levels of resolution.</p><p>It is clear that in terms of visualization, "one size fits all" does not apply. What is intuitively obvious to one user is unclear and convoluted to another. Realizing this, we have identified numerous browsing paradigms to appeal to a broader audience. Tools like the Virtual Reality Modeling Language (VRML) are well suited for use with this technology for visualizing information on the World Wide Web, and will aid us as we strive to improve information visualization.</p><p>DEPICT: Documents Evaluated as Pictures. Visualizing Information using Context Vectors and Self Organizing Maps ...........................................................................................................................</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Rushall, M. llgen</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!</forename><surname>Yahoo</surname></persName>
		</author>
		<ptr target="http://www.yahoo.cod" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="http://www.lycos.com/" />
		<title level="m">Lycos</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Self-organizing Maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Feedback and Mixing Experiments with MatchPlus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Gallant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Caid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings TREC-2 Conference</title>
		<editor>D. Harman</editor>
		<meeting>TREC-2 Conference<address><addrLine>Ed, Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">CONVECTIS: A Context Vector-Based On-Line Indexing System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Sasseen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Carleton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Caid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Dual-Use Conference</title>
		<meeting>IEEE Dual-Use Conference</meeting>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ImageEext Automatic Indexing and Retrieval System Using Context Vector Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Z</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPIE</title>
		<imprint>
			<biblScope unit="volume">2606</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kohonen</surname></persName>
		</author>
		<ptr target="http://websom.hut.fi/websom" />
		<title level="m">Statistics on Spheres</title>
		<imprint>
			<publisher>John Wiley and Sons</publisher>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adding a Conscience to Competitive Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Desieno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Neural Networks, I</title>
		<meeting>the International Conference on Neural Networks, I<address><addrLine>NY</addrLine></address></meeting>
		<imprint>
			<publisher>IEFE Press</publisher>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
