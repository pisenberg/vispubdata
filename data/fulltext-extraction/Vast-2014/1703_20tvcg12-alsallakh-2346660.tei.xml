<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visual Methods for Analyzing Probabilistic Classification Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Alsallakh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helwig</forename><surname>Hauser</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Miksch</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Rauber</surname></persName>
						</author>
						<title level="a" type="main">Visual Methods for Analyzing Probabilistic Classification Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TVCG.2014.2346660</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Probabilistic classification</term>
					<term>confusion analysis</term>
					<term>feature evaluation and selection</term>
					<term>visual inspection</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Fig. 1. Our visual analysis tools: (a) the confusion wheel shows sample-class probabilities as histograms colored by classification results, (b) the feature analysis view depicts feature distributions among selected samples, separated by their results, and ranked by a separation measure, (c, d) histograms and scatterplots reveal the separability of selected true and false classified samples by one or two features.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The performance of classifiers in terms of correct classification is a major factor in determining their applicability for a given problem. Significant advances in machine learning have led to the development of a variety of classifiers and to an improved understanding of their properties. Designing a classification algorithm for a given problem is usually an iterative process that involves several decisions and choices. These include choosing an appropriate classifier, parameter tuning of this classifier, feature selection, and possibly introducing specific extensions to the classifier in order to handle special cases or to incorporate domain knowledge. This process aims to optimize the performance of the classifier according to some measures such as error rate, cost, or risk. For each of the above-mentioned stages in the design process, machine-learning experts need to understand the data involved in order to make choices that increase performance. Providing tools that assist these experts in analyzing the data in relation to the classification performance enables valuable guidance for the design process <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>• Bilal Alsallakh, Allan Hanbury, Silvia Miksch, and Andreas Rauber are with Vienna University of Technology. E-mail: {lastname}@ifs.tuwien.ac.at • Helwig Hauser is with University of Bergen. E-mail: helwig.hauser@uib.no Visualization has played an important role in understanding and comparing classification algorithms and in improving their design (Sect. 2). In case of multi-class classifiers, the performance is usually reported by means of a confusion matrix that records for each class how many times its samples were confused for each other class ( <ref type="figure" target="#fig_1">Fig. 2a</ref>). Compared with overall performance measures, these matrices provide more details about the results and help in introducing appropriate adjustments to the classifier. Besides predicting the class for a given input sample, many multi-class classification algorithms compute likelihood scores for a sample being of each of the classes. Analyzing how these scores correlate with the classification error and data features is important to understand the behavior of such classifiers. Confusion matrices discard this information as they incorporate final classifier decisions only. This paper presents visual methods for analyzing classification results of a multi-class probabilistic classifier for a large number of labeled samples. After motivating this problems and identifying related tasks (Sect. 3-4), we describe how our methods allow analyzing classification results in context of class probabilities (scores) and data features. Our main contributions are:</p><p>• Involving class probabilities in the analysis of classification data by explicitly representing them as colored histograms.    <ref type="bibr" target="#b17">[18]</ref>, (b) ROC curves comparing five classifiers <ref type="bibr" target="#b12">[13]</ref>, (c) binary classification boundaries projected on two factors <ref type="bibr" target="#b24">[25]</ref>, (d) Class Radial Visualization <ref type="bibr" target="#b31">[32]</ref>.</p><p>Sect. 5 presents usage scenarios to demonstrate the applicability of our methods in analyzing and improving classification results. In Sect. 6 we discuss the advantages and shortcomings of our approach compared to previous work, and report expert feedback as well as practitioners experience in analyzing their classification data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>A variety of approaches and tools have been proposed to improve classification performance using visualization. They can be categorized into techniques that engage the user actively in building the classifier, and those that focus on retrospective analysis of the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Building Classifiers Interactively</head><p>Ware et al. <ref type="bibr" target="#b40">[41]</ref> argue that classifiers built by users can compete with automated techniques as the users can incorporate their domain knowledge in the classifier design. Several techniques have been proposed for interactively constructing specific classifiers such as ones based on linear discriminant analysis (LDA) <ref type="bibr" target="#b8">[9]</ref> or decision trees <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40]</ref>. Also, similar ideas were proposed for specific aspects of classifier design such as distance measures <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7]</ref> and feature selection <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>Certain techniques offer visual support for active learning, an approach which enables machine learning algorithms to query the user for the desired class of an unlabeled sample <ref type="bibr" target="#b32">[33]</ref>. This learning paradigm has been shown useful in several domains such as video analysis <ref type="bibr" target="#b15">[16]</ref> and document retrieval <ref type="bibr" target="#b14">[15]</ref> where the number of samples is very large, prohibiting a manual labeling beforehand <ref type="bibr" target="#b30">[31]</ref>.</p><p>Talbot et al. <ref type="bibr" target="#b34">[35]</ref> presented an interactive system to support ensemble learning, an approach to combine multiple classifiers to build one that is superior to its components. Their EnsembleMatrix technique visualizes the confusion matrices of the individual classifiers and allows combining these classifiers interactively with immediate update to the combined confusion matrix. Kapoor et al. <ref type="bibr" target="#b17">[18]</ref> developed ManiMatrix, a system to refine a classifier by means of simple interactions with the confusion matrix ( <ref type="figure" target="#fig_1">Fig. 2a</ref>). Reducing the tolerance for confusion between two classes triggers a search for new classification boundaries and updates the matrix interactively if a solution is found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">A Posteriori Analysis</head><p>Several visualization techniques were developed to help machinelearning experts analyze classification results a posteriori. These techniques are not tightly integrated with the classifiers, but are designed for post-mortem analysis, which makes them potentially classifieragnostic. We describe next three categories of these techniques according to the primary information they visualize.</p><p>Classifier performance: Receiver operating characteristic (ROC) curves <ref type="bibr" target="#b12">[13]</ref> and their variations <ref type="bibr" target="#b11">[12]</ref> are well-established methods for tuning, assessing, and comparing the performance of binary classifiers. A ROC curve plots the true positive rate against the false positive rate of a binary classifier for a varying discrimination threshold <ref type="figure" target="#fig_1">(Fig. 2b)</ref>. While ROC analysis can be extended to multi-class classifiers, it is still computationally exhaustive due to the large number of class combinations that need to be computed <ref type="bibr" target="#b19">[20]</ref>. Also, visualizing high-dimensional ROC spaces is challenging, with existing techniques being able to show only partial information about the classes <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data features:</head><p>These techniques are dedicated to analyze the influence of data features on the classification results. Anand et al. <ref type="bibr" target="#b2">[3]</ref> use a bubble chart to depict how the samples of a target class are distributed based on the values of one nominal and one numerical data features. Kienreich and Seifert <ref type="bibr" target="#b18">[19]</ref> use feature-class matrices to show how features correlate with classes. A number of techniques visualize the decision boundaries of a binary classifier in a multi-dimensional feature space <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b24">25]</ref>. Multiple scatter plots of the data features are used for this purpose <ref type="figure" target="#fig_1">(Fig. 2c)</ref>. A recent follow-up technique augments the data points with information about their distances to the decision boundary <ref type="bibr" target="#b25">[26]</ref>. This was shown useful for steering the classification model and identifying cost-changing data elements.</p><p>Class probabilities: Dedicated techniques have been proposed to visualize class probabilities in the case of probabilistic classification. Rheingans and desJardins used a heatmap to visualize the probability of a given class for each value combination of two features <ref type="bibr" target="#b28">[29]</ref>. They account for a larger number of features by creating a 2D projection of the feature space. Iwata et al. <ref type="bibr" target="#b16">[17]</ref> proposed a projection of class probabilities to visualize multiple classes in the same time. Projection-based techniques can preserve interesting structures in the high-dimensional space. Nevertheless, they might potentially result in complex visualizations that require good understanding of their properties and semantics to interpret correctly. Seifert and Lex <ref type="bibr" target="#b31">[32]</ref> proposed a simplified technique for visualizing class probabilities. It places the classes on a circle and depicts the samples as points in this circle based on their class probabilities ( <ref type="figure" target="#fig_1">Fig. 2d</ref>). These probabilities can be shown on demand for one sample as lines of varying thicknesses. The points are colored according to their predicted classes. In our work we also employ a radial layout for the classes, but use different visual abstractions and interactions as we explain next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MOTIVATION OF OUR WORK</head><p>When classifying samples using a probabilistic classifier, it is possible to infer for a wrongly-classified sample whether the actual class is the 2nd, 3rd or last guess (i.e. the rank of the actual class). The next chart shows a histogram of the ranks computed by a classifier for the actual classes of 10,992 samples. About 8.3% improvement on the classification rate is possible if the classifier would succeed on the 2nd guesses, e.g. by simple adjustments to the classifier or by using additional classification rules. On the other hand, 11.3% of the samples fail with low improvement chance with the current classifier. These samples are hard to separate from non-class samples. The histogram of actual class ranks does not provide actionable insight beyond indicating the amount of improvement potential. More detailed visualizations are needed to guide the users on how they can improve the performance. <ref type="figure" target="#fig_2">Fig. 3a</ref> shows a confusion matrix of the classification results described above. The size of a cell encodes the samples of its row class that are confused for its column class. The matrix is augmented with histograms of sample probabilities in each row and column. The row histograms represent false negatives (FNs), while the column histograms represent false positives (FPs) in the respective class. As we show in the next sections, this information is vital to understand the behavior of probabilistic classifiers. However, the matrix representation does not assign visual primacy to the histograms, which limits their usefulness. Moreover, it does not include information about correctly classified samples (true positive TPs and true negatives TNs) and how their probabilities are distributed, compared to misclassified samples. Finally, the information related to one class is scattered in multiple cells and two histograms in the respective row and column. We address these issues by employing alternative visual designs dedicated for analyzing probabilistic classification data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">OUR VISUAL ANALYSIS TOOLS</head><p>We propose a set of visualization tools that are integrated to analyze probabilistic classification data. The data encompasses:</p><p>• A set S of n labeled samples that are classified into m classes C = {c 1≤ j≤m }. • The actual label for each sample l a (s) ∈ C : s ∈ S.</p><p>• The predicted label for each sample l p (s) ∈ C : s ∈ S.</p><p>• The probability p i j for each sample s i ∈ S to belong to class c j ∈ C, as computed by the classifier.</p><formula xml:id="formula_0">• A set of l data features f 1≤k≤l (s i ) for each sample s i ∈ S, used</formula><p>by the classifier to compute the class probabilities.</p><p>The above information is available when classifying unknown samples, except for the actual labels l a . Therefore, l a -independent observations in the data can be reproduced during actual classifications.</p><p>Our tools aim to support the following analysis tasks:</p><p>• T1: Analyze the overall probability distribution of the samples to belong to a class (regardless to their actual classes). • T2: Compare the probability distribution of the samples according to their classification results (TPs, FPs, FNs, TNs) in a class. • T3: Find out FNs / FPs that have high / low probability. These samples are easier to improve than other FNs and FPs. • T4: Select samples confused between two classes, and analyze their probability distributions in these classes. • T5: Select samples by their class probabilities, classification results, or data features for further analysis. • T6: Find out if FPs / FNs at a certain probability range can be separated from TPs / TNs in that range by the data features.</p><p>All of these tasks involve the class probabilities, and some of them involve the data features as well. Matrix representations fall short of supporting these tasks, as they assign visual primacy to class confusions. Therefore, we propose two main visualizations that assign visual primacy to the probabilities (Sect. 4.1) or to the data features (Sect. 4.2). We show in the next sections how these views are suited for solving the above-listed tasks, and enable new insight in the classification results beyond the information available in the matrix representation. We illustrate our tools based on a UCI benchmarking dataset <ref type="bibr" target="#b4">[5]</ref> that contains 10,992 labeled samples representing pen-based handwritten digits. The samples have 16 data features that comprise the x and y coordinates of eight points sampled along the curve of each digit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Confusion Wheel</head><p>To assign visual primacy to the sample-class probabilities, we create histograms to show how they are distributed in each class (task T1). We employ the visual layout of Contingency Wheel++ <ref type="bibr" target="#b0">[1]</ref> which places these histograms in a ring chart whose sectors represent the classes c 1 ..c m <ref type="figure" target="#fig_2">(Fig. 3b)</ref>. In contrast to the matrix, this layout emphasizes the classes as primary visual objects with all information related to a class grouped in one place. This includes class probabilities and confusions with other classes as we explain next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Visualizing sample-class probabilities as histograms</head><p>For each class c j , the samples S are divided into four sets according to their classification results:T P j , FP j , T N j , and FN j . A histogram of the class probabilities is created for each of these sets using a uniform number of bins b, equal to n by default. The samples X jk aggregated in bin k in this histogram are a subset of the corresponding set X j , where X j is one of the four sets mentioned above:</p><formula xml:id="formula_1">X jk = {s i ∈ X j : (k − 1)/b &lt; p i j ≤ k/b}<label>(1)</label></formula><p>A closed interval [0, 1/b] is used for the first bin. The confusion wheel visualizes these histograms along the radial dimension in the respective sector, with the first bin placed next to the inner ring and the last bin b next to the outer ring. The user can select which histograms to include in the visualization (task T2). These histograms are stacked and centered in each sector to show the probability distribution of the respective samples. Color reveals the breakdown of these samples according to their classification results <ref type="figure" target="#fig_2">(Fig. 3b</ref>). All histograms have the same scale and the sectors are scaled to fit them.</p><p>By default, the confusion wheel filters out the bottommost bars of TNs having p i j ≤ 10%. Showing these samples is of marginal interest, as they usually do not compete with the winner class. Filtering out these samples increases the resolution of the other histograms that show more important information about TPs and misclassified samples. Likewise, it is also possible to filter out the topmost bars of TPs having p i j ≥ 90% to further increase the resolution.</p><p>A reference circle indicates the 50% probability level in each sector. All negatives are located below this line. It is also possible for positives to lie below this line: this happens, for example, when the highest three class probabilities for a sample are nearly equal.</p><p>The colored histograms give more information about the classifier performance than confusion matrices. For example, it is evident in <ref type="figure" target="#fig_2">Fig. 3b</ref> that classes c 4 and c 6 have the clearest discrimination, with the majority of positive samples (≥ 98%) in these classes being predicted with high probabilities (≥ 90%). The opposite holds for c 5 , where only 48% of its positive samples predicted with (≥ 90%) probabilities. Only 25% of these samples were classified correctly. The percentage information can be obtained interactively in a tooltip.A naïve Bayesian classifier is used to classify the data. <ref type="figure" target="#fig_3">Fig. 4</ref> shows three classes from the data depicted in <ref type="figure" target="#fig_2">Fig. 3b</ref>. It includes only misclassified samples (FPs and FNs) depicted at a higher resolution by filtering out TPs and TNs (task T3). The samples are colored according to their actual classes. For this purpose, a unique color is assigned to each class from an appropriate qualitative color scale. As a result, the FNs in each class are colored by the class color. The FPs are colored by their actual classes, showing which other classes were confused for this class, and at which probability. This reveals that samples confused for c 3 are mostly of classes c 5 and c 9 . Also, there is mutual confusion between c 5 and c 8 in the probability range ]0.2, 0.8]. A misclassified sample s is double coded in <ref type="figure" target="#fig_3">Fig. 4</ref> since it counts as a FP in l p (s) and as a FN in l a (s). This is indicated by the chords that represent class confusions are we explain next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Visualizing class confusions as chords</head><p>The confusion wheel depicts class confusions as chords between the sectors (task T4). A chord between two classes is depicted with a varying thickness: the thickness at sector j 1 is proportional to M j 2 j 1 , the number of elements of class c j 2 confused for c j 1 . Likewise, the thickness at sector j 2 is proportional to M j 1 j 2 . Hence, following the chords outgoing from a sector reveals for which other classes its false negatives are confused. Alternatively, the chord can be split into adjacent ones that show the confusion in each direction individually. The sectors are ordered so that thicker chords are made shorter, using an O(m 2 ) greedy algorithm <ref type="bibr" target="#b1">[2]</ref>. This reduces the visual ink and the clutter caused by chord crossings, resulting in a clearer visualization. Also, this reveals groups of classes that have more confusion among each other than with the other classes. For example, it is evident in <ref type="figure" target="#fig_2">Fig. 3b</ref> that the digits 1, 2 and 7 are often confused with each other, as their shapes are similar to some degree. Compared with the chords, a matrix representation is more accurate at showing class confusions and their distribution in the matrix. Nevertheless, a chord is better at showing the mutual confusion between a pair of classes and the asymmetry of this confusion, compared with two visually-separate cells in a matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Visualizing additional information about the samples</head><p>Instead of coloring the histograms by their classification results, a histogram bar can be alternatively colored by an attribute of the samples aggregated in it. For example, color can be used to compare the classification results against another classifier <ref type="figure" target="#fig_8">(Fig. 8</ref>). This shows for which samples the classification improved, worsened, or did not change in the depicted data (Sect. 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Feature Analysis View</head><p>Investigating the reason behind certain misclassifications and how to improve them depends heavily on analyzing how the data features are distributed among the affected samples. In particular, it is important to find out if certain features discriminate these samples from correctly classified samples. Therefore, we created a dedicated view that assigns visual primacy to the features by depicting how they are distributed in a selected subset of samplesŜ ⊆ S. As we did in the wheel view, we split the samples inŜ into the same four groups according to their classification result in a specific classc ∈ C selected by the user. To provide an overview first, we create up to four boxplots below each other for each data feature, showing how its values are distributed in each of the four groupsT P j ,FP j ,T N j ,FN j . If a group is empty, e.g. no FNs forc inŜ, no boxplots are created for it. The view shows boxplots of multiple data features ordered in a list ( <ref type="figure" target="#fig_4">Fig. 1b and 5g</ref>).</p><p>Typical feature analysis scenarios involve finding features that separate two main groups among selected samples:T P j fromFP j , or T N j fromFN j (task T6). This has two implications on our design: First, we used a minimal version of boxplots, showing the whole value range, the median Q2, and the inter-quartile range [Q1, Q3]. We do not show outliers as they are irrelevant for the separation task. Second, and more importantly, we rank the features f 1≤k≤l by their separation power of two of selected groups X 1 , X 2 from the above four groups. Several separation measures can be used for this purpose. One method is to use a significance statistic: for each feature f k , we compute Welch's t-statistic <ref type="bibr" target="#b10">[11]</ref> (which is used for Student's two-sample t-test with unequal sample sizes):</p><formula xml:id="formula_2">t k = mean( f k (X1)) − mean( f k (X2)) var 2 ( f k (X1))/|X1| + var 2 ( f k (X2))/|X2|<label>(2)</label></formula><p>We rank the features by the corresponding p-values, computed according to Student's t-distribution. This places features with more significant mean differences between X 1 and X 2 in the top of the list. Such features are more likely to separate the samples in both groups, assuming the values in these groups are normally distributed as in <ref type="figure">Fig. 1c</ref>. Boxplots show only summary information of feature distribution. To gain more details about it, the user can click on a feature's area, which shows a stacked histogram of its values, depicting breakdown of the samples into the multiple groups described above. It helps in better estimating the separability of the groups by the selected feature.</p><p>In many cases, higher mean difference between two groups does not mean better separability. An example is shown in <ref type="figure" target="#fig_4">Fig. 5i</ref>, where two groups have relatively closer means, and yet better separability by the respective feature than by other features. This often happens when one of the groups represents combined phenomena like TNs that belong to different actual classes. To account for such cases, we provide alternative separation measures to rank the features. Both χ 2 and K-S statistics <ref type="bibr" target="#b21">[22]</ref> are applicable generic measures to compare two empirical distributions without further assumptions. When defining additional classification rules (Sect. 5.4), it is important to identify ranges in feature distributions that have high separation. For this purpose, we use the following measure, based on the histograms h 1k and h 2k of a feature f k in X 1 and X 2 respectively:</p><formula xml:id="formula_3">F k = b h ∑ b=1 h 1k (b) • h 1k (b) h 1k (b) + h 2k (b)<label>(3)</label></formula><p>Similar to χ 2 , this measure is computed from binned distributions with an adjustable number b h of histogram bins. Instead of summing up deviations, it sums up the number of X 1 samples in each bin weighted by their retrieval precision, as with the F-measure <ref type="bibr" target="#b27">[28]</ref>.</p><p>To provide an overview of how much separation of X 1 from X 2 is possible using only one of the data features, we create a recallprecision graph in the top of the view <ref type="figure" target="#fig_4">(Fig. 5h)</ref>. This graph indicates for each precision level to retrieve X 1 , the largest recall rate possible.</p><p>In some cases, no single feature provides good separation of the groups. Therefore, we offer a scatterplot view of selected samplesŜ in the 2D space of two selected features, to check if these features offer better separation <ref type="figure">(Fig. 1d)</ref>. Automated and visual techniques can be employed to recommend scatterplots with best separation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>In our implementation, the user selects the scatterplot dimensions from two lists of features, ranked by their univariate separability.</p><p>The visual tools described so far show different aspects of classification data. In the next sections we show how these tools are integrated together and describe use cases of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The Interactive Exploration Environment</head><p>Analysis scenarios of classification results typically involve examining different pieces of the information to formulate and test hypothesis about the results, and to introduce improvements. Therefore, we developed an exploration environment that shows these pieces of information at different levels of detail using multiple views that are arranged and coordinated accordingly in the user interface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Summary views</head><p>These views show summary information about the classification results and performance. They assign visual primacy to the classification results, which are shown as secondary information in color in the other views. Each view highlights samples currently under selection. Three bar charts show breakdowns of the samples s ∈ S by their actual class l a (s) <ref type="figure" target="#fig_4">(Fig. 5a</ref>), predicted class l p (s) <ref type="figure" target="#fig_4">(Fig. 5b)</ref>, and classification correctness whether l p (s) = l a (s) or not <ref type="figure" target="#fig_4">(Fig. 5c</ref>).</p><p>In addition, two histograms show breakdowns of the samples by the probability of the predicted class p i j : l p (s i ) = c j ( <ref type="figure" target="#fig_4">Fig. 5d</ref>) and by the rank r(s i , l a (s i )) of the actual class l a (s i ) <ref type="figure" target="#fig_4">(Fig. 5d)</ref>, where:</p><formula xml:id="formula_4">r(s i , l j ) = |{1 ≤ j ≤ m : p i j &gt; p i j }| + 1<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Confusion wheel view</head><p>This is the central view in the user interface, showing aggregated information in more details than the summary views. Four checkboxes and two sliders at the top of this view enable quickly defining which samples to include. The sliders allow filtering TPs with high probability and FNs with low probabilities, to focus the analysis on more problematic samples to be depicted at a higher resolution (tasks T3).</p><p>Hovering the mouse over a visual element shows a tooltip with summary information about the samples that it represents <ref type="figure" target="#fig_6">(Fig. 6a</ref>). This encompasses, for example, recall and precision in a class, and the number of samples confused between two classes for a chord. More details about selected samples can be obtained in the detail views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Detail and feature analysis views</head><p>The detail views show more information about the samples selected in other views. The top area in these views show a textual description of current selection. A tabular list shows the data features as well as the predicted and actual classes for the selected samples <ref type="figure" target="#fig_6">(Fig. 6b)</ref>. The feature analysis view shows this information graphically, as explained in Sect. 4.2. Likewise, the probability view shows the class probabilities of the samples as a tabular list or as multiple histograms <ref type="figure" target="#fig_6">(Fig. 6c</ref>). The two tabular lists are synchronized: clicking on an sample in one view highlights it and ensures its visibility in both views. This enables a textual examination of the class probabilities of a certain samples. These probabilities are also depicted graphically as a star graph in the wheel view <ref type="figure" target="#fig_6">(Fig. 6a)</ref>. Also, when possible, a graphical representation of this sample can be shown in a dedicated view <ref type="figure" target="#fig_6">(Fig. 6d</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Interactive Queries on the Samples 1</head><p>By clicking on a bar in the summary views or in the wheel view, the respective subset E ⊆ S of samples is selected (task T5). The views are immediately updated to highlight the fractions of bars and chords that represent elements in E. These fractions in the wheel view retain their colors. The rest of the elements become uncolored.</p><p>Multiple bars can be selected at once in a histogram in the wheel view. The selection in The samples confused between two classes can be selected by clicking on the respective chord (task T4). This allows examining how these samples are distributed in the probability histograms of both sectors. The selection in <ref type="figure" target="#fig_6">Fig. 6a</ref> is defined by clicking on the chord between c 2 and c 7 . The views are updated to show the class probabilities and feature values of the samples in E. These samples can be examined individually by clicking on an item in these views <ref type="figure" target="#fig_6">(Fig. 6c</ref>). Finally, samples can be further selected based on their features or other attributes by selecting a specific value range in the respective view.</p><p>The subsets that correspond to the above-mentioned selection possibilities can be combined interactively using set operations as in <ref type="bibr" target="#b1">[2]</ref>. Specific keyboard modifiers allow specifying if the newly brushed elements should be added to, intersected with, or subtracted from the existing selection. This allows defining highly-expressive visual queries to select samples based on their classification results and probabilities in each class, and on their actual and predicted classes. For example, by clicking on c 3 in "predicted class" summary chart all positive samples in this class are selected (both TPs and FPs). This selection can be refined to TPs only by clicking on the respective bar in "actual class" while in set intersection mode. Also, certain FPs such as confusions with c 5 and c 9 can be filtered out by clicking on their bars in this chart while in set exclusion mode. We show in the next section how interactive selection of the samples supports several analysis scenarios of probabilistic classification data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">USAGE SCENARIOS</head><p>We demonstrate how our tools can be applied to analyze and improve classification results of the UCI "pen-based" dataset introduced in Sect. 4. For this purpose, we train several classifiers using the raw features 2 of 100 randomly-selected samples using the RapidMiner data- mining software <ref type="bibr" target="#b33">[34]</ref> (formerly YALE <ref type="bibr" target="#b23">[24]</ref>). We first show how interaction allows quick inspection of misclassified data. Then we show how the confusion wheel provides insight into classifier behavior, and enables comparing misclassified samples between two classifiers. Finally, we show how our tools help in defining additional classification rules to correct misclassified samples in a generalizable way. Besides demonstrating the standard features of our system, problem-specific features are introduced to support the last two use cases. Further examples with different data sets and classifiers are available at http: //www.cvast.tuwien.ac.at/ConfusionAnalysis/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Inspecting Misclassified Samples</head><p>The rich possibilities to select subsets of samples using the interactive exploration environment allow quick inspection of certain samples, e.g., to analyze the reason behind certain confusions. In <ref type="figure" target="#fig_6">Fig. 6</ref>, elements confused between c 2 and c 7 are selected by clicking on the respective chord. Inspecting these samples illustrates that people write the same digits in different ways, which requires increasing the training sample to match against using k-NN classifiers.</p><p>In many cases, erroneous labels or noisy data features are the reason behind classification errors. Using our detail views we were able to identify such cases in the UCI dataset. One example is a c 5 digit confused for c 8 because it is written in east Arabic numerals which have different shapes than Arabic numerals. Another example was a noisy sample which does not resemble any digit. Identifying and isolating such samples is important to introduce effective design improvements and to accurately evaluate and compare different classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Analyzing Classifier Behavior</head><p>Visualizing the probability histogram for each class and coloring these histograms by classification results reveal several patterns that explain the behavior of the classifier. <ref type="figure" target="#fig_7">Fig. 7</ref> shows this information for class c 5 using three different classifiers. With Neural Networks (NN), the classification accuracy increases proportionally with the probability of the predicted class <ref type="figure" target="#fig_7">(Fig. 7a)</ref>. This does not always apply to a Naïve Bayesian (NB) classifier, where some classes showing the opposite trend <ref type="figure" target="#fig_7">(Fig. 7b)</ref>. Also, though it varies between classes, the overall classification sharpness was higher for NB than NN, with 88.9% of all samples classified with ≥ 90% probability <ref type="figure" target="#fig_2">(Fig. 3b)</ref>, as opposed to 50.7% with NN. k-NN classifiers exhibit up to k peaks in the histograms at equidistant locations, when an appropriate number of bins b is used (Sect. 4.1). This is because k-NN classifiers perform weighted majority voting among the labels of the k nearest training samples to the samples being classified. If all k classifiers agree on the label c j for s i , p i j is equal (or very close to) 1 and the sample belongs to the outermost peak in the histogram. If none of the classifiers agree, the label c j of the closest training sample is predicted but with low probability, and the sample hence belongs to the innermost peak in the histogram of c j . In <ref type="figure" target="#fig_7">Fig. 7c</ref> there are two peaks, as k equals 2. In <ref type="figure">Fig. 1a</ref> and <ref type="figure" target="#fig_4">Fig. 5f</ref>, there are up to five peaks per sector, as k equals 5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison with Another Classifier</head><p>Classifier designers typically analyze the effect of using a different classification algorithm or changing certain parameters on the results. Besides a holistic measure of classification rate improvement, they often want to gain insight about the samples whose classification improved by this change, and the ones that worsened. A typical example is analyzing how the classes vary in their improvements, using twosided bar charts that depict improved and worsened samples for each class in different directions. Another example is analyzing which class confusions increased or decreased by the changes, using a suited matrix representation. These representations discard available information on class probability which offers new ways to improve the performance. To address this limitation, we offer a mode to color the samples in confusion wheel by their improvement status as illustrated in <ref type="figure" target="#fig_8">Fig. 8</ref>. The data is classified using a k-NN algorithm with k = 1 and k = 3. The histograms show the class probabilities computed with k = 3. TNs are not shown, as they are irrelevant for comparison on class level. Dark blue indicates misclassified samples in the depicted data that would improve when k = 1. Dark red indicates correctly classified samples that would worsen when k = 1. It is noticeable that k = 1 performs better than k = 3 as there is more dark blue than dark red (overall 6% improvement). We investigated the reason for that by checking samples that improved or worsened. <ref type="figure" target="#fig_8">Fig. 8a</ref> illustrates an example of a sample whose nearest neighbor is the correct class, but the 2nd and 3rd nearest are not. In this example k = 1 succeeds while k = 3 fails. <ref type="figure" target="#fig_8">Fig. 8b</ref> shows the opposite case: 2 out of 3 nearest neighbors have the correct labels, making k = 3 succeeds and k = 1 fails. In both cases, the sample is close to the outer boundary in c 2 as two of the three nearest neighbors agree on its label. <ref type="figure" target="#fig_8">Fig. 8c</ref> shows an interesting case which resembles <ref type="figure" target="#fig_8">Fig. 8a</ref>, with the only difference that the 2nd and 3rd neighbors are significantly far from the sample. This makes the classifier less confident about their votes, and hence predicting the answer at lower probability. Except for one sample, all 480 samples that fall in this probability range in this class would either improve or stay the same when k = 1. This suggests adding a rule to re-classify these samples, as we show next. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Defining Post-Classification Rules</head><p>In some cases, an existing classifier cannot be refined internally due to the lack of source code or expertise. Post-classification rules are one way to improve the performance in such situations by handling specific cases <ref type="bibr" target="#b38">[39]</ref>, incorporating domain knowledge <ref type="bibr" target="#b9">[10]</ref>, and rectifying systematic errors. Such rules are usually defined over the data features and are usually easy to understand and adapt especially if they are defined by domain experts. In case of probabilistic classification, a rule can specify Boolean conditions q(s i ) on the class probabilities p i j or ranks (Eq. 4) of the samples s i , in addition to conditions on their features f k (s i ). Both types of information are available at runtime as they do not involve the actual labels l a (s).</p><p>We consider three rules for correcting classification errors:</p><p>• Correcting false negatives: This rule intends to correct FNs of class c j by replacing their predicted classes with c j :</p><formula xml:id="formula_5">R FN j : q 1 (s i ) ∧ . . . ∧ q k (s i ) ∧ l p (s i ) = c j ⇒ l p (s i ) ← c j<label>(5)</label></formula><p>Samples that satisfy conditions q 1 ..q k are post-classified as c j . • Correcting false positives: This rule intends to correct FPs of class c j by replacing their predicted classes with the 2 nd guesses:</p><formula xml:id="formula_6">R FP j : q 1 (s i ) ∧ . . . ∧ q k (s i ) ∧ l p (s i ) = c j ⇒ l p (s i ) ← c j : r(s i , c j ) = 2<label>(6)</label></formula><p>It post-classifies a potential FP that satisfies its premise as the class c j ranked 2nd for this sample (Eq. 4). • Using another classifier: This rule intends to re-classify certain samples by using another classifier Cl z :</p><formula xml:id="formula_7">R Cl z : q 1 (s i ) ∧ . . . ∧ q k (s i ) ⇒ l p (s i ) ← Cl z (s i )<label>(7)</label></formula><p>It post-classifies a sample s i that satisfies its premise as the class Cl z (s i ) predicted by Cl z .</p><p>Our visual tools support in defining rules of the above types and in testing their actual improvement. For this purpose, the data set should be split into two parts: (1) training data that are loaded in the visualizations and used in defining the rules, and (2) validation data that are used to assess the actual improvement on unseen data. This is important to avoid dataset bias which occurs when defining rules that overfit the training data and fail to generalize to unseen data. Except for <ref type="figure" target="#fig_2">Fig. 3</ref>, the visualizations depicted in this paper use 80% of the UCI data introduced in Sect. 4. In the following we illustrate how potential improvements can be visually identified, and how the respective rules can be defined and validated on the remaining 20% of the data.</p><p>To improve misclassified samples in a class c j , the respective rule should define conditions on the samples that include as much of these samples as possible and in the same time exclude correctly-classified samples. This is important as applying the rule on the latter samples would worsen their classification. The wheel view gives an overview on how misclassified samples c j are distributed according to their probabilities. This makes it easy to spot probability ranges in c j having a significant number of these samples that are likely to improve by one of the rules. Typically, these samples interfere with correctly classified samples. In particular, the interference of TPs (green) with FPs (yellow) requires separation using further conditions on the data features, before applying R FP j . This interference is usually larger in the outermost bin(s) that are dominated by TPs, which suggests excluding these bins when defining this rule. Similarly, the interference of FNs (red) with TNs (grey) requires separation in order to apply R FN j . Also the innermost bin(s) need to be excluded when defining this rule, as their probability range is highly dominated by TNs.</p><p>To separate the interference in a potentially improvable probability range Q c j in c j , the user selects the samples in this range using brushing. The feature analysis view lists possible features that offer good separation, as explained in Sect. 4.2. After inspecting the feature histograms, the user can select a feature f k to define an improvement rule by double clicking on its histogram. This opens a dialog box which shows the histogram in higher resolution and allows selecting a specific value range Q f k for f k <ref type="figure" target="#fig_9">(Fig. 9)</ref>. The user selects a range that contains the majority of samples that need improvements (FPs or FNs) and excludes as much of the other samples as possible. The dialog also allows specifying which rule to apply to samples that fall both in Q c j and Q f k . The selected rule is externalized in text and applied to such samples both in the training dataset loaded in the visualization, and in the test dataset. The results in both cases are reported as the absolute number of samples that improved and worsened, and the total improvement on the classification rate. Changing the feature range Q f k automatically updates the results, to assist the user in choosing a robust range that performs well both in training and test datasets.</p><p>As example, in <ref type="figure">Fig. 1a</ref>, the analyst notices a large number of FNs in c 7 . She selects the respective probability range in c 7 <ref type="figure" target="#fig_4">(Fig. 5f</ref>) and finds that feature y 8 offers good separation of these FNs from the TNs in the value range [20%, 60%] <ref type="figure" target="#fig_4">(Fig. 5i</ref>). Therefore, she creates the following rule:</p><formula xml:id="formula_8">(0.1 ≤ p i7 ≤ 0.3) ∧ (20 ≤ y 8 (i) ≤ 60) ⇒ l p (s i ) ← c 7<label>(8)</label></formula><p>This rule improves 591 and worsens 13 samples in the loaded data, yielding a significant total improvement rate of 6.57%. Similar results apply to the testing data (141 improved, 3 worsened, 6.28% total rate) making the analyst accept this rule. She continues further to investigate the large number of FPs in c 1 by selecting the probability range [30%, 80%] and restricting the selection to samples with l p (s) = c 1 . She checks the features for separation but notice interference between FPs and TPs, even with the features with most separation power <ref type="figure" target="#fig_9">(Fig. 9a)</ref>. She selects the small range with as few TPs as possible, and applies rule R FP 1 which achieves 1.11% overall with 136 improved and 36 worsened samples in the training dataset. She rejects this rule and searches for more robust rules to improve these samples.</p><p>The scatterplot view allows identifying if two features can in combination achieve a good separation of interfering sample groups. As example, <ref type="figure">Fig. 1d</ref> illustrates how two features separate about 76% of FNs in c 8 that lie in probability range [10%, 30%] from the TNs, with only 13 TNs unseparated. Reclassifying these samples with R FN j yields 3.5% and 3.4% improvements on the training and test datasets. Dedicated algorithms are needed to rank the pairs of features by their separation power, and to recommend optimal region separations in a specific scatter plot. For the purpose of this use case, a manual search for such features is sufficient to illustrate the importance of visual inspection to assess their separation power.</p><p>Besides searching for separating features, it is possible to improve certain misclassifications using the results of another classifiers. As example, it is evident in <ref type="figure">Fig. a</ref> that the results for c 2 involve a large number of mixed misclassifications (FNs and FPs) especially in the probability range [30%, 70%]. We provide a separate view to check how other classifiers perform on these samples by selecting them <ref type="figure" target="#fig_9">(Fig. 9b)</ref>. This reveals that neural-networks-based classifier (NN) yields 7.31% improvement rate if applied to these samples, which suggests creating the following post-classifying rule (Eq. 7):</p><formula xml:id="formula_9">0.3 ≤ p i2 ≤ 0.7 ⇒ l p (s i ) ← l p NN (s i )<label>(9)</label></formula><p>Such combinations of results from multiple classifiers has been extensively researched in pattern-recognition and machine-learning literature <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b41">42]</ref>. Many of these techniques apply combination heuristic such as weighted majority voting in a holistic way to all samples. We illustrated that declaratively restricting such heuristics to certain samples yields better improvements, as this avoid impacting correct classifications among the remaining samples. Using visual inspection, we were able to outperform automated techniques for combining multiple classifiers such as majority voting <ref type="figure" target="#fig_9">(Fig. 9b)</ref>.</p><p>Post-classification rules should be defined carefully to avoid overfitting the data. First, the testing dataset should be representative and of sufficient size to warrant generalization. Second, the conditions used in these rules should be based on probability and feature ranges that have a sufficient number of samples to avoid creating rules specific to the training dataset. In fact, the distributions depicted in the probability histograms tend to be invariant among random subsets of sufficient sizes, if extreme values are discarded and avoided. Finally, defining several rules increases the overlap between their premises, the conflict in their actions, and the risk of over-fitting in general. It is important to select a small number of rules that exhibit robust improvement results, high precision, and few overlaps with each other. In general, classification errors should ideally be solved by improving the classification algorithm when possible, with help of the insights gained by the interactive visualizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>In this section we discuss the advantages and limitations of our tools and compare them with other techniques for visualizing probabilistic classifiers. We also report feedback and observations from classification experts and practitioners.</p><p>Scalability: The use of aggregated representations makes our tools highly scalable with the number of samples. For example, the histograms in the confusion wheel were able to handle datasets containing tens of thousands of samples. Furthermore, the filtering possibilities presented in Sect. 4.3 enable focusing on fine details contained in small subsets of these samples. Likewise, the boxplots and histograms in the feature analysis view scale well with the number of samples. Stacking the boxplots allow depicting summary information about 15-20 features at once. This is sufficient for our purposes, thanks to feature ranking which interactively places the most relevant features for the current analysis context at the top.</p><p>To ensure enough visibility of the information in each class, up to 20 classes can depicted at once as sectors in the wheel view. This limit is feasible for a wide range of classification problems that do not require a larger number of classes. In case of larger number of classes, a subset of them can be selected manually or automatically to be shown at once, such as the subset with the highest confusions between its classes.</p><p>Handling imbalanced data: Sometimes, classification data exhibit skewed distributions of the samples to the classes. This causes classes having large number of positives to occupy the majority of display area, possibly obscuring fine details in smaller classes. To handle such cases is to possible to make the sectors of equal sizes, and stretch the histogram to fit in these sectors using individual scaling factors. Arcs representing the same amount of samples can be drawn outside the sectors using different scales to indicate these scaling factors. Although this hinders comparing the histograms in absolute values, the shapes of the distributions and the proportions of misclassified samples are still comparable across the classes. Such relative comparisons are usually more relevant in analyzing the results than comparing absolute values between classes of significantly different sizes. Similarly, the class confusions can be normalized by the total number of samples in the respective classes. These relative confusions can be indicated in color or by adjusting the chord thicknesses to show relative instead of absolute confusions.</p><p>Some classifiers compute relatively low probabilities for the winning class, such as ones based on fuzzy logic. This limits non-empty histogram bars to the inner bins only, which lowers the visual resolution. It is possible in such cases to redefine the bins to cover the effective probability range at higher resolution and make the histogram span the whole sector area.</p><p>Comparison with previous work: Our tools combine different pieces of information that have been addressed individually in previous work, as presented in Sect. 2. Compared with existing techniques that visualize class probabilities such as Class Radial Visualization <ref type="figure" target="#fig_1">(Fig. 2)</ref>, the added-value in using the wheel metaphor lies in (1) aggregating the samples to analyze their probability distributions and to avoid clutter and ambiguity issues caused by individual points <ref type="bibr" target="#b31">[32]</ref>, (2) using color to show classification results of the samples next to each other and to reveal their separability, and (3) visualizing class confusions in a compact layout, thanks to the radial arrangement. This provides a rich overview of classification results that was not possible in previous techniques and enables new insight and tasks as we illustrate in the usage scenarios. Confusion matrices are better suited than the chords to gain more precise information about class confusions. Nevertheless, the chords reveal groups of classes having higher mutual confusions, emphasize the asymmetries in these confusions, and enable relating them to respective class probabilities.</p><p>The area-based nature of the histograms offers several possibilities for selecting and highlighting certain subsets of samples in confusion wheel. This is essential in a coordinated-multiple-view environment in order to formulate queries on the samples and to gain detailed insight into them in the feature analysis view. This environment enables a novel integration between probability-based and feature-based representations of classification data.</p><p>The visual analysis methods we propose are based on familiar visual representations such as boxplots and stacked histograms, as well as on the wheel metaphor <ref type="bibr" target="#b0">[1]</ref>. This metaphor is originally designed to visualize associations between entities and categories, and similarities between categories. By treating samples as entities and classes as categories, we were able to adapt this metaphor for classification data: Instead of associations and similarities, we compute probabilities and two-way confusions. Also, we introduced several changes to the visual encoding and different interactions to address the characteristics of classification problems.</p><p>Expert feedback: We analyzed classification data comprising about 40, 000 labeled samples provided by our industry partners. The data were classified in 10 classes using a fuzzy rule-based system designed by domain experts. We refined our design iteratively based on feedback from these experts, and on what information they wanted to analyze. After we explained the visualizations we created for their data, they were able to identify issues with their classification rules. For example, one class had a large number of FNs that were highly concentrated in the middle of the respective sector. Analyzing the reason for that revealed that the respective classification rule was assigned relatively low weight by the domain experts. Also, examining the features for selected FPs in a class revealed a rule that uses an inappropriate feature that was mistaken for the correct one: The values of this feature among these FPs should not appear among actual samples of the class, according to domain experts. These experts were able to refine their system accordingly. We did not have the possibility to quantify the improvement.</p><p>The informal feedback from our industry partners and from five other machine-learning experts confirms that our tools offers both a good overview of classification results and detailed information on demand. However, our visual metaphors need to be learned with enough examples and explanations before they can be interpreted correctly: One domain expert, asked for clarification on what the histograms in the wheel view mean, about 30 minutes after we started presenting our findings. To avoid such misunderstanding, the metaphor should be introduced part by part with sufficient examples, before discussing insights. In fact, our system is feature laden, and needs extensive learning of how these features work together. One machine-learning expert requested showing separate arcs to encode class confusions in both directions. Also, two machine-learning experts did not encourage using post-classification rules in general, as they could encourage overfitting the data. They suggested using the insight gained in improving the classifier design instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>The availability of class probabilities enables new possibilities to analyze the performance of probabilistic classifiers, beyond comparisons between predicted and actual classes. Common visual representations such as confusion matrices and ROC curves ignore class probabilities by assigning visual primacy to classification error in terms of false positives, false negatives, or class confusion. Assigning visual primacy to class probabilities or to the data features enables analyzing their influence on classification performance and performing further analysis tasks related to the data. We proposed a representation of probabilistic classification data by showing the probability distributions as stacked histograms in a radial layout and by coloring these histograms by the classification results of the samples. We showed how this representation lends itself to rich interactions to select samples based on their probabilities, and to perform further analysis of these samples based on their data features. We proposed intertwined automated and visual methods to analyze these features in a dedicated view and to rank them according to their separation power between true and false classifications among the selected samples. We presented several analysis scenarios that are possible using our visual tools. These include visual inspection and comparison of classification results, identifying performance problems, and interactive definition of post-classification rules to improve misclassified samples a posteriori. We demonstrated by that how exploratory analysis can reveal relevant patterns and correlations in classification data that are difficult to specify and identify automatically, and are usually compromised in holistic analysis methods. These insights are essential to introduce effective improvement to the classifier design that reduce the classification error in a generalizable way. Our future work aims to provide visual means to compare and combine the results of several classifiers, to support analyzing a large number of classes, and to explore hierarchical and multi-label classification results by means of similar interactive visualization methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>•</head><label></label><figDesc>Intertwined automated and visual methods to analyze data features in relation with classification results and probabilities, and to rank them by their separation of true and false classification. • An interactive exploration environment to analyze different aspects of probabilistic classification data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Techniques for visualizing classification results: (a) an interactive confusion matrix</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Visualizing classification results of 10,992 handwritten digits<ref type="bibr" target="#b4">[5]</ref> (a) using a confusion matrix augmented with histograms of sample probabilities in the respective rows and columns, (b) using the confusion wheel: Sectors represent digits with chords showing classification confusion between them. Histograms represent the probabilities of the samples in each class according to the color legend.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Filtering and coloring the samples in the confusion wheel, representing the same data as inFig. 3b. Only misclassified samples are shown (FPs and FNs), colored by their actual classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>The interactive exploration environment showing information about classification results, class probabilities, and feature distributions. The summary charts show breakdowns of the samples by (a) actual class, (b) predicted class, (c) classification correctness, (d) the probability of the predict class, (e) the probability rank of the actual class. The wheel view (f) shows the same data as inFig. ??b, classified using a k-NN classifier. Selected samples are highlighted in color. The feature analysis view (g) shows summary information data features of the selected samples, broken down according to their classification results, and the selection criteria in natural text. (h) A control panel to rank the features according to their separation power along with a recall-precision curve for possible separation. (i) A histogram of the top ranked feature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5</head><label>5</label><figDesc>is initially defined by brushing the range[0.1, 0.3] over the radial dimension in c 7 . This selects samples s i with p i,7 ∈ [0.1, 0.3].The selection is refined by excluding predicted samples in c 7 , focusing only on TNs and FNs. These samples are highlighted in other sectors to show their classification results in the respective classes. The feature analysis view is updated to show the feature distributions among these samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Obtaining details about elements interactively: (a) selecting samples confused between c 2 and c 7 . The list view show their (b) features and (c) class probabilities. One sample is selected for inspection by showing its graphical representation (d). Its class probabilities are highlighted both in the list view, and in the wheel view using arrows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Classification results in c 5 using (a) neural networks, (b) a naïve Bayesian classifier, and (c) a k-NN classifier with k = 2. TNs in the bottommost bars are filtered out. Individual histogram scales are used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Comparing the results of k-NN classifiers with k = 1 (encoded in color) against k = 3 (defining the histograms).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Defining post-classification rules: (a) checking separability between FPs (yellow) and TPs (green) according to a feature, (b) checking the performance of other classifiers on selected samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>1</head><label></label><figDesc>IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 20, NO. 1 ,</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>C 2 DE EM</cell><cell>BER 2014</cell><cell>703</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">2014 ate of D .</cell></row><row><cell>publication</cell><cell>11 Aug.</cell><cell>2014; date of current version</cell><cell>9 Nov.</cell><cell>2014.</cell></row></table><note>For information on obtaining reprints of this article, please send e-mail to: tvcg@computer.org. Manuscript received 31 Mar. 2014; accepted 1 Aug.Digital Object Identifier 10.1109/TVCG.2014.2346660</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The supplementary video illustrates some of the interaction possibilities<ref type="bibr" target="#b1">2</ref> For reproducibility and illustration purposes, we did not consider computing any additional features that could improve the performance.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement:</head><p>We thank Peter Filzmoser, Colin Johnson, and Ammar Shaker for feedback and proposals, and Bilal Esmael and Arghad Aranout from TDE Data Engineering for cooperation and discussions. This work was supported by the Austrian Federal Ministry of Science, Research, and Economy via CVAST, a Laura Bassi Centre of Excellence (project no. 822746).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reinventing the contingency wheel: Scalable visual analytics of large categorical data. Visualization and Computer Graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alsallakh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Aigner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Miksch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Gröller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2849" to="2858" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Radial sets: Interactive visual analysis of large overlapping sets. Visualization and Computer Graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alsallakh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Aigner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Miksch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2496" to="2505" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An L-infinity norm visual classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wilkinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Tuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="687" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Visual classification: an interactive approach to decision tree construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ankerst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<meeting>the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="392" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lichman</surname></persName>
		</author>
		<ptr target="http://archive.ics.uci.edu/ml" />
		<imprint>
			<date type="published" when="2014-03" />
		</imprint>
		<respStmt>
			<orgName>University of California, Irvine, School of Information and Computer Sciences</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Assisted descriptor selection based on visual comparative data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bremm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Landesberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schreck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="891" to="900" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dis-function: Learning distance functions interactively</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brodley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Visual Analytics Science and Technology (VAST)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="83" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Visual methods for examining SVM classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Caragea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wickham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Honavar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual Data Mining</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="136" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">iVisClassifier: An interactive visual analytics system for classification based on supervised dimension reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kihm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Visual Analytics Science and Technology (VAST)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Incorporating domain knowledge and spatial relationships into land cover classifications: a rule-based approach. International Journal of Remote Sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Daniels</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2949" to="2975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Probability and Statistics for Engineering and the Sciences. Cengage Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devore</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cost curves: An improved method for visualizing classifier performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Holte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="95" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An introduction to ROC analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fawcett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="861" to="874" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A novel scalable multi-class ROC for effective visualization and computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramamohanarao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Karmakar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Knowledge Discovery and Data Mining</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="107" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visual classifier training for text document retrieval. Visualization and Computer Graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Heimerl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2839" to="2848" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inter-active learning of ad-hoc classifiers for video visual analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hoferlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Netzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoferlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heidemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Visual Analytics Science and Technology (VAST)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="23" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Tenenbaum. Parametric embedding for class visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Iwata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ueda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stromsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2536" to="2556" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Interactive optimization for steering machine classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Human Factors in Computing Systems (CHI)</title>
		<meeting>the International Conference on Human Factors in Computing Systems (CHI)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1343" to="1352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visual exploration of feature-class matrices for classification problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kienreich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Seifert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Visual Analytics (EuroVA)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="37" to="41" />
		</imprint>
	</monogr>
	<note>The Eurographics Association</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving accuracy and cost of two-class and multi-class probabilistic classifiers using ROC curves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lachiche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Flach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Optimal combinations of pattern classifiers. Pattern Recognition Letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="945" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The Kolmogorov-Smirnov test for goodness of fit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Massey</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical Association</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">253</biblScope>
			<biblScope unit="page" from="68" to="78" />
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Guiding feature subset selection with an interactive visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bannach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ruppert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kohlhammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Visual Analytics Science and Technology (VAST)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="111" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">YALE: Rapid prototyping for complex data mining tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mierswa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wurst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klinkenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Scholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Euler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<meeting>the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="935" to="940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Visual exploration of classification models for risk assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Migut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Visual Analytics Science and Technology (VAST)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="11" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Visualizing multi-dimensional decision boundaries in 2d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Migut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Veenman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visualization of cluster structure and separation in multivariate mixed data: A case study of diversity faultlines in work teams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Metoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bezrukova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Spell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="117" to="130" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Evaluation: from precision, recall and f-measure to roc, informedness, markedness &amp; correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Powers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Technologies</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="63" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Visualizing high-dimensional predictive model quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rheingans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Desjardins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization</title>
		<meeting>IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="493" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A taxonomy of visual cluster separation factors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sedlmair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tatu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munzner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tory</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1335" to="1344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">User-based active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Seifert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Granitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining Workshops (ICDMW)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="418" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A novel visualization approach for data-miningrelated classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Seifert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lex</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Visualisation (IV), 13th International Conference</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="490" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Active learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Sciences Technical Report</title>
		<imprint>
			<biblScope unit="volume">1648</biblScope>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin-Madison</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pattern recognition engineering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kofler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Breuel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RapidMiner Community Meeting and Conference</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">EnsembleMatrix: Interactive visualization to support machine learning with multiple classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Talbot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Human Factors in Computing Systems (CHI)</title>
		<meeting>the International Conference on Human Factors in Computing Systems (CHI)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1283" to="1292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Combining automated analysis and visualization techniques for effective exploration of high-dimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tatu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Albuquerque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eisemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneidewind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Theisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Magnor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Visual Analytics Science and Technology (VAST)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="59" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Combining multiple classifiers by averaging or by multiplying? Pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Tax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Breukelen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Duin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1475" to="1485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">PaintingClass: interactive construction, visualization and exploration of decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Teoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<meeting>the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="667" to="672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Improving pixel-based vhr land-cover classifications of urban areas with post-classification techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Van De Voorde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">De</forename><surname>Genst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Canters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogrammetric Engineering and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">1017</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Baobabview: Interactive construction and analysis of decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Van Den Elzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Wijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Visual Analytics Science and Technology (VAST)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Interactive machine learning: letting users build classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ware</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. Journal of Human-Computer Studies</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="281" to="292" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Methods of combining multiple classifiers and their applications to handwriting recognition. Systems, Man and Cybernetics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krzyzak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="418" to="435" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Opening the black box of feature extraction: Incorporating visualization into high-dimensional data mining processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gruenwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1188" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
