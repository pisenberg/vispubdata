<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Visual Analytics Framework for Explaining and Diagnosing Transfer Learning Processes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arlen</forename><surname>Fan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingrui</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><forename type="middle">Reddy</forename><surname>Nelakurthi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Maciejewski</surname></persName>
						</author>
						<title level="a" type="main">A Visual Analytics Framework for Explaining and Diagnosing Transfer Learning Processes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Transfer learning</term>
					<term>deep learning</term>
					<term>visual analytics</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Source Model Target Model (2) Class-level Performance (2.1) High accuracies on both models (2.2) Big difference between the source and the target model accuracies on (2.3) t-SNE Projection Result (3.1) Rankings of Important Neurons (3) Network Relation View (3.3) Weight Visualization for Layer 7 (Less weights in the target model have correspondences in the source side) (4) Domain Discriminability scriminability (4.1) Neurons (4.2) Images (3.2) Many Target Weights with Source Correspondence (4.3) Domain-invariant Feature 4.3) 4 3) (4 (4 Domain-invariant Feature D i i i F C.1 C.2 Fig. 1. The visual analytics for transfer learning interface consists of four components: (A) statistical information summary, (B) the instance view, (C) the network relation view, and (D) the feature view. For the Office-31 dataset, (1) the prediction accuracy of the target model on the source dataset is lower than the source model; (2) for the target dataset, classes such as file cabinet and phone have a large performance gap between the source and the target models; (3) the neuron similarity matrices and the weights are presented; (4) some neurons have high domain discriminability, while Neuron #173 in Layer 5 is domain-invariant.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>• Y. <ref type="bibr">Ma</ref> Machine learning approaches have achieved outstanding success in various fields including image recognition <ref type="bibr" target="#b26">[28]</ref>, natural language processing <ref type="bibr" target="#b78">[80]</ref>, and question-answering systems <ref type="bibr" target="#b5">[7]</ref>. However, in realworld applications, the collected labels used to train machine learning models may become quickly outdated, and collecting new labels can be cost prohibitive <ref type="bibr" target="#b40">[42,</ref><ref type="bibr" target="#b53">55]</ref>. Furthermore, directly reusing expired or non-related labeled data from other domains may inject bias into the modeling process. To reduce the cost of labeling new instances and minimize the threat of bias, researchers have proposed transfer learning, a meta-strategy to relax the assumption of independent and identical distribution (i.i.d) and mitigate data insufficiency <ref type="bibr" target="#b52">[54]</ref>. From a non-technical perspective, transfer learning is inspired by the knowledge transfer phenomenon in humans <ref type="bibr" target="#b74">[76]</ref> where, for example, an experienced car driver can learn to operate boats with less effort by mapping the control mechanism, or a pianist may master other musical instruments faster than new learners. Transfer learning is seen as a promising approach in the deep learning community, where the high demand for labels and long training times of models can be bottlenecks for model deployment <ref type="bibr" target="#b66">[68]</ref>. By recycling labeled instances and pretrained networks from related data domains, model accuracy can be improved and training time reduced as compared to training neural networks with new instances from scratch <ref type="bibr" target="#b66">[68,</ref><ref type="bibr" target="#b77">79]</ref>. While there have been successful applications of transfer learning in deep neural networks <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b56">58]</ref>, understanding and interpreting the knowledge transfer processes is a critical step in the transfer learning process. Measuring the performance of the transferred model with conventional metrics is not enough to ensure that the knowledge transfer will be robust. Model developers need to understand what knowledge has been transferred from the old model and how the knowledge is reused in a particular form in the new model to boost its performance, which cannot be answered with simple statistical measures [1].</p><p>Given the recent success of visual analytics for explainable AI (XAI) <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b41">43]</ref>, we propose a visual analytics framework to explain and diagnose transfer learning processes in deep neural networks. The research challenges are summarized to characterize the exploration routines and essential knowledge users want to extract. Analytical tasks are derived in accordance with the requirements in a hierarchical manner and act as a guidance of the framework design. While existing visual analytics tools focus on individual models <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b71">73]</ref>, our approach addresses the task of revealing relationships between multiple models and utilizes a multi-faceted visualization scheme to present the transferred knowledge with respect to data, feature, and domain levels. Specifically, we propose an interactive interface consisting of two modules: descriptive visualization of statistical measures and detailed inspection of model behaviors. The statistical information of the models before and after transfer are revealed to provide an overall measure of how successful the transfer is, <ref type="figure" target="#fig_0">Figure 1 (A)</ref>. For detailed inspection components in <ref type="figure" target="#fig_0">Figure 1</ref> (B-D), the instance view depicts essential information on data distributions and predictions in the new model, the matrix-based network relation view is designed to reveal similarities and differences of the critical neural network components, and the feature view utilizes domain discriminability to support the exploration of shared knowledge hidden in the extracted features. To demonstrate our framework, we provide case studies and expert interviews in image classification tasks where a fine-tuning method is adopted on AlexNets. Our contributions include: • A visual analytics framework that supports the interpretation and diagnosis of the transfer learning processes in deep learning models; • A suite of visualization designs that illustrate the transferred knowledge from the data, the model, and the feature levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we review related work on model interpretation and comparison as well as visual analytics for deep learning.</p><p>Model-Agnostic and Model-Specific Analysis. A variety of work from the visual analytics community has focused on supporting model transparency for XAI. Previous work can be roughly classified into two main categories: model-agnostic design and model-specific design. Model-agnostic approaches consider the models to be black boxes where the internal learning processes are opaque to users. Here, the focus is on interpreting the models using the input data, output predictions, and performance measures, as such metrics are common across all classes of models. Manifold <ref type="bibr" target="#b80">[82]</ref>, ModelTracker <ref type="bibr" target="#b4">[6]</ref>, and Squares <ref type="bibr" target="#b58">[60]</ref> utilize the predictions of labels from classifiers as well as performance measures to support model comparison, performance debugging, feature importance analysis, and instance-level explanations. FairSight <ref type="bibr" target="#b1">[3]</ref>, FairVis <ref type="bibr" target="#b9">[11]</ref>, and the What-If Tool <ref type="bibr" target="#b75">[77]</ref> apply fairness metrics to evaluate whether algorithmic disparities of certain populations occur in the predictions. Prospector <ref type="bibr" target="#b25">[27]</ref> enhances the model interpretability by demonstrating feature importance with partial dependence and localized inspection. Model specific designs focus on improving model transparency by revealing the inner workings of models in a white-box manner. For example, tree-based models are considered to be highly-interpretable models because of the natural presentation of decision criteria. For lessinterpretable models, such as support vector machines <ref type="bibr" target="#b42">[44]</ref> and artificial neural networks <ref type="bibr" target="#b67">[69]</ref>, methods have been proposed to expose the core structures of the model (e.g., support vectors or neurons) or utilize interpretable surrogate models <ref type="bibr" target="#b48">[50]</ref>. Compared with the model-agnostic approaches, the strategy of "opening the black box" <ref type="bibr" target="#b49">[51]</ref> benefits the advanced users by providing insights into underlying patterns hidden inside the models. However, the strong bindings between the internal structures of models and the specialized visualization designs limit the universality of such model specific visual analytics systems <ref type="bibr" target="#b58">[60]</ref>. Model Comparison. In the predictive visual analytics pipeline <ref type="bibr" target="#b39">[41,</ref><ref type="bibr" target="#b41">43]</ref>, model comparison is essential for model selection, where the best result is selected from various predictions associated with multiple parameter settings <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b41">43]</ref>. Statistical charts, including line charts and scatterplots, have been applied to visualize model statistics and provide an intuitive comparison of different results <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b41">43]</ref>. Many visual analysis approaches provide a detailed comparative analysis of multiple predictions. Work by Spinner et al. <ref type="bibr" target="#b62">[64]</ref> summarizes typical tasks in XAI and proposes a framework for interactive machine learning.</p><p>In the model quality control stage, the model comparison component is implemented using different "explainers" to support comparative explanation. Visual analysis approaches in ensemble learning <ref type="bibr" target="#b37">[39,</ref><ref type="bibr" target="#b65">67,</ref><ref type="bibr" target="#b82">84]</ref>, Au-toML <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b72">74]</ref>, and choropleth classification under uncertainty <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b81">83]</ref> consider the comparison of individual ensemble members as the core analytical task. For topic modeling, Alexander and Gleicher <ref type="bibr" target="#b2">[4]</ref> explore the design space of task-oriented topic model comparison and derive comparison tasks based on their single-model counterparts. In data clustering, the DICON <ref type="bibr" target="#b10">[12]</ref> system adopts a glyph-based visualization design to interpret, evaluate, and compare high-dimensional statistical information of clusters. Pilhöfer et al. <ref type="bibr" target="#b55">[57]</ref> presents an algorithm to compare clustering results from different clustering models. Along with prediction comparisons, the INFUSE system <ref type="bibr" target="#b24">[26]</ref> addresses the need for comparative analysis in feature selection and provides a visual analytics system to guide the optimization of feature sets in classification tasks. Visual Analytics in Deep Learning. Given the current popularity of deep learning models, numerous visual analytics frameworks have also been proposed to improve the issues of low interpretability and transparency unique to the deep learning process. These visual analytics tools have been designed for deep learning experts and end users <ref type="bibr" target="#b19">[21]</ref> while tackling numerous real-world issues, such as model vulnerability <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b43">45]</ref>, data security <ref type="bibr" target="#b0">[2]</ref>, trust <ref type="bibr" target="#b31">[33]</ref>, and fairness <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b9">11]</ref>. Much of the previous visual analytics work for deep learning focuses on analyzing Convolutional Neural Network (CNN) structures in image classification <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b54">56,</ref><ref type="bibr" target="#b57">59,</ref><ref type="bibr" target="#b60">62,</ref><ref type="bibr" target="#b71">73,</ref><ref type="bibr" target="#b76">78]</ref> or Recurrent Neural Networks (RNN) and its variants in NLP tasks <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b47">49,</ref><ref type="bibr" target="#b63">65,</ref><ref type="bibr" target="#b64">66]</ref>. For unsupervised Generative Adversarial Networks (GAN), AEVis <ref type="bibr" target="#b33">[35]</ref>, GAN Lab <ref type="bibr" target="#b23">[25]</ref>, and GANViz <ref type="bibr" target="#b70">[72]</ref> support the exploration of how adversarial examples from the generative networks pass through the discriminative ones and trigger erroneous behaviors in the neurons which dramatically change the final predictions. Wang et al. <ref type="bibr" target="#b69">[71]</ref> propose DQNVis for the investigation of training dynamics and action outcomes in deep reinforcement learning models.</p><p>Along with learning tasks, visualization for deep neural networks also focuses on prediction performance, network structures, and the behaviors of intermediate hidden layers <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b73">75]</ref>. The performance diagnosis approaches for general black-box models can be applied to deep neural networks as well, and some work extends the black box methods by introducing instance checking components <ref type="bibr" target="#b22">[24]</ref>. For modelspecific inspection, network structures are illustrated using node-link diagrams <ref type="bibr" target="#b76">[78]</ref> where nodes represents layers, or important neurons, and edges represent the weights or filters that connect the layers or neurons <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b35">37]</ref>. Behaviors of hidden layers and neurons are sometimes embedded into the structure diagram, the activation map <ref type="bibr" target="#b60">[62]</ref>, the representative instances or features <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b47">49]</ref>, or the clusters of neurons <ref type="bibr" target="#b35">[37]</ref>.</p><p>While, the visual analytics community has developed a variety of methods for model interpretability, performance diagnosis, and model comparison, none of the aforementioned approaches have addressed the explainability and diagnosis of knowledge transfer between deep learning models. Recent works by Zeng et al. <ref type="bibr" target="#b79">[81]</ref>, Murugesan et al. <ref type="bibr" target="#b50">[52]</ref>, and Ma et al. <ref type="bibr" target="#b44">[46]</ref> are the closest in spirit to our work with respect to model comparison and knowledge transfer. However, our framework goes beyond simple comparisons between models and analysis of traditional transfer learning techniques on shallow learning models to further reveal the relationships of inheritance and knowledge reuse in deep neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BACKGROUND ON TRANSFER LEARNING</head><p>Before presenting our visual analytics framework, we first define the basic concepts of the transfer learning processes. Domain and Task. There are two core concepts in transfer learning: domain, and task <ref type="bibr" target="#b52">[54]</ref>.</p><p>• Given a set of data instances X = {xi|xi ∈ X , i ∈ [1, N]} in the feature space X , a domain D is a combination of X and the marginal probability distribution P (X). Take for example, a collection of news articles. We can consider the instances x to be the corresponding word count vectors of the articles, and the feature space X the vocabulary of all the articles. Due to variations of wording and expressions, the word count distribution is often unique for a specific news source, leading to different P (X) across publications. • The concept of task describes the supervision information and the model that fulfills a specific learning goal. Formally, a task T consists of two components: the label space Y which represents all possible labels for the instances, and a decision function</p><formula xml:id="formula_0">y = f (x) learned from the labeled instances {(yi, xi)|y ∈ Y, x ∈ X , i ∈ [1, N]}.</formula><p>In our news article example, the goal is to perform sentiment analysis, leading to a label space Y of two elements such that each label yi for xi can be either P ositive or Negative. The classifier f (•) is generated by fitting the labeled instances and can be further applied to unlabeled instances to predict their sentiment. In the conventional machine learning setting, the assumption should be held that unlabeled data comes from the same distribution as the training data and share the same set of class labels. Formally, by denoting the unlabeled instances as X = {x i |x i ∈ X , i = [1, M]} and their potential labels in the set of Y , we have X = X , P (X ) = P (X), and Y = Y, which leads to D = D and T = T . In the context of our sentiment analysis example, this requirement means that the unlabeled new articles should be collected from the same or similar websites with nearly identical patterns of word distributions and semantics, and the choices of class labels still remain either P ositive or Negative. Transfer Learning. In practice, the assumption of aligned domains and tasks for training and prediction stages may not hold. In our example, if the articles with sentiment labels are all retrieved from a political news channel, a sentiment classifier trained on these articles may generate biased predictions on sports news due to a potentially divergent vocabulary used to express positive and negative ideas, i.e., P (Xpolitics) = P (Xsports). There are other types of mismatches between the domains and tasks, such as varied numbers of unique words between news channels (Xpolitics = Xsports, namely, different feature spaces) or different concepts of classes (Ypolitics = Ysports).</p><p>The purpose of transfer learning algorithms is to handle the mismatching issue by learning a decision function f (•) in the new domain D and task T with the knowledge learned from the existing D and T , where D = D or T = T . In the transfer learning terminology, D and T are identified as the source domain Ds and the source task Ts, and the new domain and the task as the target domain / task (Dt and Tt), respectively. To facilitate the notations in the following sections, we further denote XD s and XD t as the source and the target dataset. Transfer Learning in Deep Neural Networks. Researchers and practitioners have identified two major issues that impact the efficiency of applying exising deep learning models to new problems <ref type="bibr" target="#b66">[68]</ref>, including the need of massive labeled instances for the new task and the prohibitive computational overhead. These two issues lend themselves well to the transfer learning paradigm. For the lack of labeled instances, the relaxation of being in the same domain enables the recycling of similar data sources via instance re-weighting. For models, various works <ref type="bibr" target="#b66">[68,</ref><ref type="bibr" target="#b77">79]</ref> tend to reuse learned parameters from existing models or utilize special network layers that extract common features. In our framework, we focus on the widely-used fine-tuning technique, which applies the trained network layer parameters in existing models to new ones as an initialization. For the type of models, we demonstrate our framework in the scenario of image classification where CNN-based neural networks are adopted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DESIGN OVERVIEW</head><p>Given the key features of transfer learning in deep neural networks, we have designed a visual analytics framework to explain and investigate knowledge transfer between neural network models. Here, we summarize the research challenges, and a set of analytical tasks are derived from the challenges and used to guide our framework design and development.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Research Challenges</head><p>To identify the research challenges in visual analysis of deep transfer learning process, we conducted a literature review on deep transfer learning <ref type="bibr" target="#b38">[40,</ref><ref type="bibr" target="#b52">54,</ref><ref type="bibr" target="#b56">58,</ref><ref type="bibr" target="#b66">68,</ref><ref type="bibr" target="#b74">76]</ref> and listed several requirements for explaining the knowledge transfer processes. The list was then refined through detailed discussions with our domain experts (who serve as co-authors). Furthermore, we organized the challenges in a structured manner based on the multi-level topology proposed by Brehmer et al. <ref type="bibr" target="#b8">[10]</ref>, and we identified two key challenges when analyzing the transfer processes. C1: Analytical Complexity. Inspired by the question of "why" a task is performed <ref type="bibr" target="#b8">[10]</ref>, analytical complexity describes how complicated the analysis is. We establish three levels of operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C1.1: Uncover the learned knowledge in the models in two domains.</head><p>To help make the transfer learning process more transparent, analysts need to understand how the models are trained in different domains. C1.2: Explore the similarities and differences between the source and the target model, i.e., "searching" the desired match/mismatch patterns. Interpreting the transfer learning processes requires analysts to not only explore individual models and the associated data domain, but also to perform an in-depth exploration of how similar the target model is to the source model. For example, analysts may need to investigate whether the patterns in the CNN's layer weights in the source model are still expressed in the target model, or whether a specific unlabeled instance receives the same prediction in both models. By comparing the similarities and differences between the source and the target model, analysts can gain insights into the underlying transfer mechanism. C1.3: Discover instances in the source domain that carry common knowledge. A key issue in transfer learning is to interpret the transferred knowledge from the source domain to the target domain. In instance-based transfer learning algorithms, source data instances carrying domain-invariant characteristics can be considered to be a form of externalization of the shared knowledge that supports instance-based interpretation. Revealing such instances can facilitate the understanding of the transferred knowledge by providing concrete examples. C2: Data Granularity. Another important factor is the hierarchy of data types involved in the transfer process, namely, the "what" in the task typology <ref type="bibr" target="#b8">[10]</ref>. Usually, the categorization is based on data types. In the context of interpretable machine learning <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b62">64,</ref><ref type="bibr" target="#b80">82,</ref><ref type="bibr" target="#b82">84]</ref>, the following data types are considered:   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analytical Tasks</head><p>We further distill the following tasks based on the research challenges of analytical complexity and data granularity. T1: Summarize the Model Performances. Summarizing data distributions and performance metrics is a fundamental prerequisite to start the analysis. Analysts may be interested in a coarse-grained comparison between the source and the target model, such as:</p><p>• What are the differences between the overall model accuracies evolved with the number of epochs? (C1.1, C1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">VISUAL ANALYTICS FRAMEWORK</head><p>Based on the identified research challenges and the analytical tasks, we propose a visual analysis framework, <ref type="figure" target="#fig_0">Figure 1</ref>, for analyzing transfer learning processes from the source to the target domains. The analytical tasks are mapped to two modules in the framework: Descriptive Visualization of Statistical Measures (T1). This module acts as an entry point for the entire analysis pipeline where descriptions of model performances are visualized, <ref type="figure" target="#fig_0">Figure 1</ref> (A). By examining the prediction performances of the source and the target models on different classes, analysts can get an overview of the models and identify salient patterns in the measurements, such as classes with the highest or lowest prediction accuracy. These classes can be addressed in the details-ondemand exploration. Detailed Inspection of Model Behaviors (T2, T3). We have designed three views to support details-on-demand exploration: 1. The Instance View <ref type="figure" target="#fig_0">(Figure 1 (B)</ref>) explains how the target model predicts data instances in selected classes (T2). A projection view is employed to show the separations among classes. <ref type="figure" target="#fig_0">(Figure 1</ref> (C)) utilizes a matrix-based visualization design to reveal similarities of model components including filters and weights (T3). 3. The Feature View <ref type="figure" target="#fig_0">(Figure 1 (D)</ref>) visualizes the domain discriminability of feature extractors in the target model (T3). A feature discriminability plot is proposed to visualize how much domainrelevant information is carried by the filters, indicating the extent of knowledge sharing in the filter level. <ref type="figure" target="#fig_0">Figure 1</ref> shows the interface where analysts can freely explore the results and switch between views. The visual elements share the same color encoding scheme where the red color represents the data instances and models in the source domain and the blue color is the target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The Network Relation View</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Statistical Analysis of Model Performances</head><p>The statistical measures provide an overview of how the source and the target models are trained and the predictions across all classes (T1). Additionally, the measures act as an entry point to reveal interesting clues and guide further detailed analysis. Overall Prediction Accuracies and Transferability Score. We collect accuracy values in each epoch from both models on three different datasets: 1) the source training data, 2) the target training data, and 3) the validation set from the corresponding domain. Hence, each model derives three time series along the evolvement of the training epochs, which are then visualized as lines in a line chart. As shown in <ref type="figure" target="#fig_0">Figure 1</ref> (A), the horizontal axis indicates the number of epochs, while the accuracy values, ranging from zero to one, are mapped to the vertical axis. The cross and circle symbols on the lines represent the values from the source model and the target model, respectively. The line colors correspond to the dataset origin of the accuracy series. The suffixes of the legend entries indicate the corresponding models of the accuracy series. Along with the detailed illustration of the accuracy values, we also provide a transferability score on the left side of the line chart to give an overall quality measure of the transfer process. The score is defined as the difference between the best target and source model accuracies on the target dataset. A positive score indicates a performance boost because of the higher accuracy for the target model, and vice versa for negative scores. Confusion Table. To visualize the class-level performances, we compute the confusion matrix of the trained model on the target validation set and list measures in the confusion table, <ref type="figure" target="#fig_0">Figure 1</ref> (A), comprised of five columns: 1) the class names; 2, 3) the accuracies of the two models on the target dataset; 4) the differences of the two accuracies on the same row, and; 5) the classes that the instances are misclassified into. Analysts can sort the rows based on the values in the column.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Instance Analysis</head><p>The instance view shows the data distributions of selected classes from the two domains and provides a detailed analysis of the relationships between instances with different attributes (T2). Instead of analyzing the original instances, i.e., raw pixels of the images, we focus on their embedding vectors extracted from the neural network layers. Thus, each instance is passed through the target model, and the activation vectors right before the fully-connected layers is used as the embedding vector. In our framework, we choose t-SNE <ref type="bibr" target="#b45">[47]</ref> to visualize the network embeddings of the corresponding instances.</p><p>In <ref type="figure" target="#fig_0">Figure 1</ref> (B), the instance view consists of three regions: a class selector, a projection scatterplot, and a detail view. Once the desired classes are selected, the details of the classes will be listed, and the projection result containing all the instances from the selected classes will be plotted. In the scatterplot, the colors of the glyphs indicate their class labels. To visualize the predictions made by the target model, the glyph borders for the mispredicted instances are set to dark gray. Similar to the encoding in the accuracy chart, we use crosses and circles for the instances from the source and the target domain, respectively. When hovering the mouse pointer on a glyph, the details of the corresponding instance are listed, including the original image, the domain of origin, the ground truth label, and the prediction made by the target model. The p-th target instance  </p><formula xml:id="formula_1">x D , c, p Layer i, Neuron j Neuron j Neuron N Neuron 1 2) Attribution (Layer Conductance) A (N rows × M columns) i D t , c x D t , c , 1 x D t , c , p Layer Conductance 3) Rank Each Column c, i, j a Neuron j Neuron N Neuron 1 x D t , c , 1 x D t , c , p i N ×N i i Neuron 1 Neuron j Neuron N</formula><formula xml:id="formula_2">x D t , c, 1 1 j i+1 x D t , c, p 0.2 0.1 0.4 0.0 0.9 0.7 2) Stack Convolution Results c,p,i,i+1 w (N rows, N ×N columns) i T, c i+1 N ×N i x D t , c, 1 1 j i+1 x D t ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Neural Network Component Analysis</head><p>Since the learned knowledge is carried in the neural network parameters and outputs, including weights and activations, revealing the core components in the network layers can help analysts understand the critical patterns captured by the models in each domain and how a model predicts a specific instance (T3). Furthermore, comparing the components between the source and the target models can assist the analysis of the relationships between the two domains (T3). We propose a network abstraction and comparison method as well as the network relation view, <ref type="figure" target="#fig_4">Figure 3</ref>, to support the exploration of the neural networks and the similarities between the two models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Network Abstraction</head><p>One issue in visualizing the network layers is the excessive number of neurons and weights inside and between layers. In the network abstraction stage, we focus on extracting the essential structures that best represent the learned patterns and knowledge hidden in the networks. This stage consists of two steps: 1) the extraction of important neurons, and 2) the extraction of valuable weights. Neuron Extraction <ref type="figure" target="#fig_4">(Figure 3 (A)</ref>). The purpose of extracting important neurons is to rank the neurons in the same layer by importance. <ref type="figure" target="#fig_4">Figure 3</ref> A -1) For the target dataset XD t , we denote the subset of data instances with the same class label c</p><formula xml:id="formula_3">as xD t ,c,p, p ∈ [1, MD t ,c],</formula><p>where MD t ,c represents the number of such instances. For the jth neuron in layer i with Ni neurons in total, we compute all the attribution values on all the MD t ,c data instances in class c, i.e., {xD t ,c,p|p ∈ [1, MD t ,c]}, resulting in an attribution vector ac,i,j for neuron j in layer i with the dimension of ND t . Usually, the attribution values are represented by using the activation values of the data instances on this neuron <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b35">37]</ref>. However, recent studies have identified limitations when directly applying activation values, especially on extracting important neurons <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b28">30]</ref>. To improve the ranking confidence, we use Layer Conductance <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b61">63]</ref> as the attribution of neurons. Since the Layer Conductance result for each neuron in convolutional layers is a 2-D matrix, we use the maximum value in the matrix to simplify the representation of attributions. <ref type="figure" target="#fig_4">Figure 3</ref> A -2) Then, we vertically stack the ac,i,j for all the Ni neurons in to a matrix Ac,i with a dimension of Ni × MD t ,c. Thus, each column represents the Layer Conductance attribution vector for an individual data instance from all neurons. <ref type="figure" target="#fig_4">Figure 3 A -3</ref>) Next, for each column, the neurons are ranked by the Layer Conductance values in ascending order, resulting in a ranking matrix A c,i . A neuron with a larger attribution value on a data instance receives a higher rank. Note that the tied values share an average ranking number. <ref type="figure" target="#fig_4">Figure 3 A -4</ref>) Finally, the ranking matrix A c,i is horizontally aggregated by summing up all the ranks in the same row, denoted as r c,i . In this way, we use r c,i as a representation of neuron importance in layer i on class c. To select a set of the most effective neurons, we select the neurons with the top-k largest aggregated ranks. In our framework, k is set to 10% of the total number of neurons in the layer.</p><p>Weight Extraction <ref type="figure" target="#fig_4">(Figure 3 (B)</ref>). Along with extracting the important neurons for each class, we also identify the links between consecutive layers which are frequently active. For layer i with Ni neurons and layer i + 1 with Ni+1 neurons, there are Ni × Ni+1 pair-wise weights between the two groups of neurons. We employ a strategy similar to the neuron selection procedure for selecting important weights:  <ref type="figure" target="#fig_0">, wc,p,i,i+1</ref>, that represents the activated levels on each weight. <ref type="figure" target="#fig_1">Figure 3 B -2</ref>) Under the vertical stacking scheme, we get a matrix containing MD t ,c rows and Ni × Ni+1 columns. Since the number of weights is relatively large, we apply a similar weight selection scheme as described in Hohman et al. <ref type="bibr" target="#b20">[22]</ref> to reduce the computational cost. For each row, we set the cells with the top-k weight values in the row to 1, and other cells to 0. In this way, we filter out the weights that are not important in classifying instances in class c. The matrix is further aggregated by adding up the replaced values vertically, bringing an importance vector wc,i,i+1 with Ni × Ni+1 values which record the counts of being important for each weight. <ref type="figure" target="#fig_4">Figure 3 B -3</ref>) Finally, the weights with the top-k importance values in wc,i,i+1 are used as the representative links between layer i and i + 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Network Comparison</head><p>By employing the neuron and weight extraction procedures to the source and the target models respectively, two sets of important neurons and weights can be extracted. To tackle the task of model comparison in T3, this step focuses on how to reveal the similarities and differences between the network components from the two models.</p><p>With respect to the neurons, we audit how similar the behaviors of the neurons are for the neurons in the two corresponding layers, <ref type="figure" target="#fig_4">Figure 3</ref> (C). In the neuron extraction step, we have described how to retrieve the attributions for each neuron on all the target data instances, namely, Ac,i = [ac,i,j], j ∈ Ni. We use AD s ,c,i and AD t ,c,i to represent the matrices from the source and the target models, respectively. We argue that two neurons from the two domains are considered similar once they have similar distributions of attribution values across all the target instances. Here, we use the cosine distances to measure the pairwise similarities of the neurons in AD s ,c,i and AD t ,c,i. The resulting similarity matrix Sc,i records how close the attribution behaviors are between the neurons from different models.</p><p>The similarities of weights are built upon the neuron similarity results. For each selected important weight wD t ,i,i+1 in the target model that connects the neuron n1 in layer i and n2 in layer i + 1, we first find the corresponding most similar neurons of n1 and n2 (denoted as n 1 and n 2 ) in the same layers in the source model by looking up the matrices Sc,i and Sc,i+1. Then, we see if the weight that connects n 1 and n 2 is in the important weight list in the source model. In this way, we can find whether an important weight in the target model is inherited from the source side and is valuable in making predictions. Weights in the Source Model <ref type="figure">Fig. 4</ref>. Visual design and layout of the neuron similarity matrices and the weights. (A) The matrices are placed in a zig-zag manner to allow the weight components to be placed in the adjacent corners between the two consecutive matrices. (B) The rows and columns of the similarity matrix are grouped into important and non-important neurons, resulting in four regions. (C) A similar grouping scheme is applied to the weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Visualization and Interactions</head><p>The network relation view is designed to visualize the extracted neurons and weights between the source and the target models. Neuron Similarity Matrix. This view leverages a diagonal layout where the similarity matrices are placed from the top left corner to the bottom right corner along the diagonal line, <ref type="figure">Figure 4 (A)</ref>. In each matrix, the similarity values are linearly scaled to the cell brightness, where zero is represented by the highest brightness and one the lowest. In our initial design, all the neurons are listed in the matrix without considering their importance. However, such a matrix may be too large to be displayed in the canvas. We improved the scalability by grouping the rows and columns based on their importance in the neuron extraction step, dividing the matrix into four regions, <ref type="figure">Figure 4</ref> (B). The top-left region shows the important neurons from both the source and the target models in the matrix form, while the other three regions are summarized into histograms of similarity value distributions. On the left and the top side of the matrices, the corresponding neuron indices are displayed with colors indicating the domain origins. Weight Visualization. In <ref type="figure">Figure 4 (A)</ref>, the order of the domain origins in two consecutive matrices (Layer 1 and 2) are switched alternatively, leaving the space of weights in the source and the target model on the corresponding adjacent corners. Following the same splitting strategy with the matrices, the weights are also summarized into four regions, <ref type="figure">Figure 4</ref> (C). In Region (1), the weights joining important neurons between both layers are rendered as an adjacency matrix. The rows and columns of the small squares are aligned with the corresponding important neurons in the similarity matrices. The brightnesses of the cells are mapped to the weight values, and the inner red dot indicates the existance of the corresponding important weight in the source model. Region <ref type="bibr" target="#b0">(2)</ref> and <ref type="formula">3</ref>  <ref type="figure">Figure 4</ref> (A)), there are no red dots in the squares or pie chart glyphs since only the correspondences from target network components are considered. Alternative Design. We also considered utilizing node-link diagrams where the neurons in different layers are encoded as columns of nodes with weight links between consecutive columns <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b35">37]</ref>. However, isolating the source and the target models into two diagrams weakens the direct comparison of the corresponding layers and weights in the two neural networks. Furthermore, visual clutter can be easily generated when large numbers of neurons and weights exist in the layers, whereas the matrix layout can increase readability by providing a non-overlapped representation of nodes and links <ref type="bibr" target="#b68">[70]</ref>. To simplify the displayed elements, two switches are employed to toggle the appearance of non-important regions in the similarity matrices and the source weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Feature Analysis</head><p>Deep neural networks are always considered as feature extractors that build new features from the original input space. As such, analysts often need to inspect what features are reconstructed in the hidden layers and how they are used in the classification process (T3). We have designed the feature analysis module to present the feature-related information and support the exploration of informative features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Domain Discriminability</head><p>Usually, how a feature can discriminate the instances from different domains plays an essential role in a successful transfer. The feature extractors should be carefully investigated to see if they can reconstruct important common patterns shared in both domains. Most of the feature-based transfer learning techniques seek to build new feature transformations upon the source domain to create such shared features.</p><p>To support the diagnosis of the transferred knowledge, we propose domain discriminability to measure whether a learned feature on the neuron is domain-invariant or domain-dependent. This measure is inspired by the widely-used A-distance <ref type="bibr" target="#b6">[8]</ref> which estimates the differences of two data distributions with a linear classifier between the two groups of data. The computation of the domain discriminability for neurons is described in Algorithm 1. First, we compute the Layer Conductance values for each selected neuron on the source and the target datasets, respectively (line 3-6 and 9-12). In the matrix A, each row represents the attribution values for an instance on the selected neurons. Then, a domain label list l is prepared to specify the domain origins of each row (line 7 and 13). Finally, a linear classifier C is trained on the stacked attribution matrix and the domain label list (line 15). Acting as a feature selection method, the coefficients from the trained C present how important a column in A, i.e., a selected neuron, can be in discriminating the domain origins of the data instances.</p><p>One potential limitation in computing the domain discriminability values is that the total number of neurons in the entire target neural network may be too large to train the domain classifier in an acceptable time. To reduce the computational cost, only the "important neurons" in each layer are selected as the input in Algorithm 1 since transfer methods often focus on the neurons with higher predictive power.  Domain Discriminability Plot. To provide a detailed view of how the domains are discriminated by the neurons, we use a linear projection method to show the decision boundary of the domain classifier C in Algorithm 1. First, a linear projection matrix WP ×2 is constructed as [ − → u , − → g ] ∈ RP ×2, where u in the first column is the feature weights of C, and g the first principal component on A. By projecting A onto a 2-D scatterplot with A • W , the decision boundary of C can be displayed on the horizontal direction, and the underlying patterns along the decision boundary can be revealed on the vertical axis. The domain origins for the rows in A are mapped to the shapes and colors where red cross represents the source instances and blue circles the target ones.</p><p>To further present the importance of each neuron, we draw the original axes in a biplot-like manner in the scatterplot. The lengths of the axes lines on the horizontal direction indicate their domain discriminability. We note that this may cause visual clutter of the axes lines when P , the number of neurons, is large. Thus, we only activate the top five neurons in the feature ranking list depending on the order selection, i.e., only the top five rows in W and the corresponding columns in A are considered. Additional neurons can be activated by clicking on the checkboxes in the corresponding rows of the feature ranking list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CASE STUDY AND EXPERT INTERVIEW</head><p>This section describes how our framework facilitates the understanding and exploration of transfer learning processes through applications in real-world datasets and feedback from domain experts. We implemented our framework with PyTorch for the deep learning library and React for the front-end framework. The data is transmitted in JSON format with RESTful APIs implemented with Flask. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Object Classification</head><p>Office-31 <ref type="bibr" target="#b59">[61]</ref> is a widely-used real-world dataset for demonstrating transfer learning algorithms. It contains 31 categories of images crawled from shopping websites and photo collections. In our experiment, we use the images from Amazon product pages ("amazon", 2817 images in total) as the source domain data, and photos from web cameras ("webcam", 795 images in total) as the target domain. For each dataset, we split the data into training and validation sets with the ratio of 85% : 15%, respectively. AlexNet <ref type="bibr" target="#b26">[28]</ref> was used as the backbone architecture for modeling, and it consists of five convolutional layers with the filter numbers of 96, 128, 384, 256, and 256, respectively. After AlexNet was trained on the amazon dataset, we adapt the model to the webcam data by using the trained parameters from the source domain AlexNet as an initialization. Analysis of the Model Statistics (T1). After the two models and datasets are loaded into our system, the brief summary of the model performances is depicted in <ref type="figure" target="#fig_0">Figure 1</ref>. We first check the accuracies on different datasets for the models. In the accuracy chart, <ref type="figure" target="#fig_0">Figure 1</ref> (1), we see that the source model performs well on the source dataset with high training and validation accuracies (≈ 0.98 and 0.8, respectively). However, the accuracy of the source model on the target dataset is ≈ 0.4, indicating a big difference in patterns between the amazon and the webcam dataset. By checking the accuracies of the target model, we discover that the performance on its own domain (webcam) is significantly higher than the source model, but it has a slightly worse accuracy on amazon than the source model. The performance drop may be due to the loss of some unique pattern extractors in the target model for the source domain, which are replaced by the new knowledge specifically for the target data. Instance-Level Inspection (T2). Apart from the overall accuracy analysis, we want to understand the prediction performance on the class and the instance levels. Here, we sort the third column of the class table to find the classes with the worst prediction accuracies by the target model, <ref type="figure" target="#fig_0">Figure 1</ref> (2). We find that some classes receive the same high accuracies from both models, such as bike and calculator (2.1), while classes like file cabinet and phone have the most diverse performances between the two models (2.2). We further examine their data distributions by activating bike and file cabinet in the instance view, respectively. In <ref type="figure" target="#fig_9">Figure 5</ref> (1), we observe that in the classes with similar performance on the two models, the images from the two domains share similar characteristics including visual angles and the appearance of the objects. However, the patterns significantly vary between the two domains, <ref type="figure" target="#fig_9">Figure 5</ref> (2). Besides inspecting the interplay between classes, we rank the class table by target accuracies. Although most of the classes show nearly perfect prediction performances, several classes still contain misclassified instances including file cabinet and phone. We select file cabinet and desktop computer in the instance view to show the erroneous predictions in file cabinet, <ref type="figure" target="#fig_0">Figure 1 (2.3)</ref>. In the t-SNE projection result, the distributions of the two classes are overlapped without a clear boundary, indicating a high chance of mis-classification. In-depth Exploration of the Models (T3). To reveal how the knowledge is shared in the classes with the same high accuracy or the diverse accuracy values, we further investigate the class bike and file cabinet in the network relation view and the feature view. After bike is selected in the network relation view <ref type="figure" target="#fig_0">(Figure 1 (3)</ref>), we check the neuron similarity matrices from the shallow layers to the deep ones. In the shallower layers, such as Layer 2, <ref type="figure" target="#fig_0">Figure 1 (3.1)</ref>, the indices of the important neurons from the target model are similar to the source side, such as neuron 50, 6, and 17. Additionally, many of the source neurons with the same indices are also the ones most similar to their target counterparts, indicating no major changes in the functions of the neurons in the target model. This phenomenon still exists in deeper layers such as Layer 7, where the first five neurons from the two models are the same. However, the distributions of important weights vary drastically when the layer depth increases. By observing the cells with central red dots and the pie glyphs between Layer 2 and 3, <ref type="figure" target="#fig_0">Figure 1 (3.2)</ref>, we see that most of the weights in the target model have corresponding important weights in the source model. This suggests that most of the patterns hidden in the network links are reused, which also matches the findings of Yosinski et al. <ref type="bibr" target="#b77">[79]</ref> that the shallow features are often reused in fine-tuning. However, the weight visualization shows a considerable change in the weights between Layer 6 and 7 where the proportions of the red arcs in most of the pie glyphs are much lower, <ref type="figure" target="#fig_0">Figure 1</ref> (3.3). We consider that although the neurons have similar rankings between both models, the weights are re-distributed between deeper layers in the target model in order to fit the new patterns in the target dataset. By inspecting the weights between Layer 2 and 3 in the class file cabinet, <ref type="figure" target="#fig_9">Figure 5</ref> (3), we find that the weight redistribution even occurs in the shallow layers. This indicates that the learned patterns diversify earlier between the source and the target model, resulting in the performance gap on the target dataset between the two models.</p><p>Finally, we explore the domain discriminability to find the common or unique features in the two domains. In the ranking result for bike ordered by the most domain-discriminative neurons, the top three features <ref type="figure" target="#fig_0">(Figure 1 (4.1)</ref>) are used to extract flat ellipses, which may be related to the tires. By further checking the instances in the domain discrimination plot, <ref type="figure" target="#fig_0">Figure 1 (4.2)</ref>, we observe that most of the bike images in the source domain (amazon, red crosses) are side-looking product profile images, while in the target domain (webcam, blue circles) the bikes are recorded at various camera angles, causing different shapes of tires in the photos. This indicates that the tire shape patterns from the source domain are not fully transferred to the target model. Meanwhile, some features with low discriminability ranks depict patterns of handlebars and frames (e.g., Layer 5, Neuron 173, <ref type="figure" target="#fig_0">Figure 1 (4.3)</ref>), which may be considered transferable between domains since these bike components exist in almost all photos in both domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Digit Recognition</head><p>In the second case study, we explore the knowledge transfer process between digit recognition datasets. We apply the Google Street View House Numbers dataset (SVHN) as the source domain, which contains natural scene photos of printed number labels. For the target domain, the MNIST dataset is used where the digits are in handwritten style and stored as grayscale images. We sampled 1500 instances per class in SVHN and 150 in MNIST, resulting in a source dataset with 15000 samples and a target dataset with 1500 samples in total. The purpose of downsampling is to mitigate scalability issues in computation and visualization, and these limitations are discussed in Section 7. We use the same AlexNet architecture for both models. Analysis of the Model Statistics (T1). After the results are loaded, the accuracy chart and the class table are shown in <ref type="figure" target="#fig_10">Figure 6</ref> (1). Similar to the lines in the first case, we find that the prediction accuracies of the two models show a diverse trend where each model performs well on its own domain but relatively poorly on the other domain. For the target model, this indicates that it has omitted some knowledge inherited from the source model and created new concepts specifically for the target dataset. In the class table, we find that some classes have a high accuracy in both models including 1 and 0. However, there is a big difference between the two accuracy values from the source and the target model as well as several misclassified instances in the class 8. Instance-Level Inspection (T2). Based on the findings in the class table, we want to examine the class of 8 in the instance view for more detailed explanations. In the t-SNE projection result for all the instances in SVHN and MNIST, <ref type="figure" target="#fig_10">Figure 6</ref> (2), we observe that most of the mispredicted source instances (crosses with bold borders) are heavily blurred, which can potentially be a domain discrimination factor since the strokes in the handwritten digit images are much crisper. As for the misclassified instances, we observe that some irregular patterns presented in these images have prevented them from being recognized correctly, such as extra-long strokes, the unnecessarily big opening on the top, and the cropped bottom circles. In-depth Exploration of the Models (T3). We perform a detailed diagnosis of 8 in the network relation view and the feature view. After checking the feature visualization of the target model neurons in the similarity matrix of Layer 1, <ref type="figure" target="#fig_10">Figure 6</ref> (3), we find that the extracted colors are slightly different between the target neurons and their most similar ones in the source model. Such differences become more significant in deeper layers where the corresponding neurons with the same indices in both models are not exclusively the most similar, such as the neuron 245 in Layer 7 whose most similar neuron in the source model is 54 instead of 245. Additionally, some neurons rank higher in the target model because of their unique characteristics to the MNIST dataset. For example, neuron 37 in the 5th place is used for extracting long free-style curves, which is common in the free handwritten images. However, its most similar counterpart, neuron 37 in the source model, receives a much lower ranking (rank 16). This suggests that the target model has adjusted the weights of several neurons to fit the new patterns emerging in the target data. In the feature view, <ref type="figure" target="#fig_10">Figure 6</ref> (4), it can be observed that the neurons for extracting circular shapes are the most domain-invariant features, indicating that in the class 8, the feature extractors for circles in the source model are reused in the target model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Expert Interviews</head><p>Our framework was further evaluated by two machine learning practitioners, E1 and E2, whose expertise is in deep learning for computer vision and medical image analysis. In each interview, we first introduced the background, the tasks, and the interface of our framework. The analytical workflow was then explained with the two datasets in the case study. The experts were allowed to explore the datasets freely during the interview process. Finally, we collected comments on the analytical workflow and the visual interface. The two interviews lasted approximately 1.5 and 1 hour, respectively.</p><p>The two experts agreed on the effectiveness of the workflow on analyzing transfer learning processes. They noted that it is an interesting aspect to investigate how the patterns learned in the source model are reused in the target model in an interpretable way. E1 commented that the multi-aspect analysis differs from conventional evaluation methods in deep learning where only statistical measures and data embeddings are considered. By examining the instances, network structures, and features in the coordinated views, analysts can gain insights into how the model parameters are reused in the target model and whether the learned patterns differ or were inherited from those in the source model. E2 pointed out that this framework can enable the diagnosis of per-class performance analysis and discover dominant features shared between the domains, which may further guide what types of existing labeled data can be reused in the current classification task.</p><p>Our framework received positive feedback on the visual representation and interactions in the views. The experts observed that the accuracy chart and the confusion table act as a proper entry point for the experts. E2 mentioned that "Checking accuracy values on the test datasets are common routines in our daily workflow. The per-class entries in the confusion table give me direct feedback on which classes receive bad performance, and I can continue investigating what is wrong in the detailed views." E1 addressed the usefulness of exploring model structures in the network relation view and the interplay between the detailed views. "The layout of neuron similarities and important weights provides an alternative way of understanding the network layers and the links between layers. I can easily find important neurons in a specific layer and locate corresponding similar ones in the other model, so I don't need to check the activation maps of neurons back and forth between the source and the target models. While linking the IDs of important neurons and the neurons in the feature ranking list, I could identify whether a layer prefers domain-invariant or -specific features."</p><p>The experts also offered suggestions on how to improve the usability of our framework. E2 discussed the feasibility of supporting other network types, such as recurrent neural networks and auto-encoders. E1 suggested providing functions to support the comparison of multiple source and target models. "We could investigate how the existing knowledge is preserved or ignored in different source-target pairs and various transfer learning settings simultaneously, which may help experts optimize the transfer strategies."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION AND CONCLUSIONS</head><p>In this paper, we present a visual analytics framework for inspecting and exploring transfer learning processes. To provide a comprehensive analysis of knowledge transfer between deep neural networks, we have identified a set of analytical tasks to guide our framework design. In the visual analytics framework, the relationships between the models in two domains are presented with a multi-aspect design including statistical information, instance-level analysis, and comparative analysis of neural network components. Analysts can check model performances, inspect data distributions between classes, and diagnose knowledge transfer between neurons and weights. We demonstrate the usability of our visual analytics framework through case studies and expert interviews. An implementation of our framework is released on Github 1 .</p><p>1 https://github.com/VADERASU/visual-analytics-for-deep-transfer-learning Scalability. We discuss the issues of scalability from three aspects: automated algorithms, visual presentation, and task generalizability. Automated Algorithms: Due to the massive computational cost of deep neural networks, the running time for the extraction and comparison procedures can be time-consuming. All instances from both domains should be passed to the two models for retrieving the Layer Conductance values on all network layers. For the two datasets in the case study, it took about 1.5 and 3.5 hours respectively to compute the Layer Conductance values, the aggregations, and the importance rankings. The cost will increase significantly in complex network architectures such as ResNet and Inception. A feasible way to reduce the overhead is to run the extraction algorithm on a selected subset of layers. Visual Presentation: In our design, the visual clutter occurs in the t-SNE projection and the domain discriminability plot when large amounts of instances are in the selected classes. For the overplotting issue, a future solution to further scale our design is to adopt sampling and aggregation methods to remove unnecessary points <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b46">48]</ref>. In the network relation view, the neuron similarity matrices can be very large if there are too many important neurons. We can apply a filter to each matrix to limit the number of visible important neurons on demand. Task Generalizability: We use the basic fine-tuning method as a representative of the transfer learning methods for deep neural networks to illustrate our framework. Our framework supports various deep transfer learning approaches once they share the same protocol with the access of datasets as well as supporting layer attribution computation in the models from the two domains. Partial transfers can also be supported where only a subset of layers is shared between two models. To some extent, our framework can be further generalized as a one-to-one model comparison tool for general model selection. Target Audience and Analysis Guidelines. The target audience for our framework are the machine learning practitioners and experts in different application domains where the transfer learning approaches are adopted in their daily workflow. For the practitioners, our framework can be used as a performance evaluation tool when target models are trained with the help of selected source domains. The insights gained from the visual exploration and inspection can further support the successive model selection. Similarly, the experts may also benefit from inspecting the knowledge transfer results when designing new transfer algorithms. To better use our framework, we suggest starting the analysis by observing the accuracy chart and the confusion table to identify the desired class (which can be seen in the "Analysis of the Model Statistics" step in both cases), followed by inspecting data distributions from two domains in the instance view. The inspection of neuron similarities and importance rankings should then be considered after the class is activated in the network relation view, together with exploring the domain discriminability and the feature rankings. As demonstrated in "In-depth Exploration of Models" about distinguishing between bikes with different tire shapes, Section 6.1, exploring the results in the network relation view and the feature view can benefit understanding the semantic differences for the same class between two domains. Future Work. In the future, we expect to explore different neuron and weight extraction criteria to evaluate different neuron attribution methods in terms of revealing model similarities. To facilitate the analysis between multiple domains, we plan to support the analysis of more than one source domain and introduce better measurements on the transferability of different source domains. Another promising extension is to enhance the usability of our framework in specific application scenarios such as object recognition and medical imaging.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>C2. 1 :</head><label>1</label><figDesc>Statistical Descriptions and Measures, including the distribution of data instances and measurement of model prediction performance; C2.2: Data Instances, including characteristics of specific data instances and their corresponding predictions in the models and domains; C2.3: Model Structures and Parameters, including the prediction mechanisms and features extracted by the neural networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>An overview of the research challenges, tasks, and framework. The framework consists of two modules: descriptive visualization of statistical measures and detailed inspection of model behaviors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 (</head><label>2</label><figDesc>M D , c instances in total)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>An illustration of the network abstraction and comparison procedures. (A) In each network layer, the important neurons are extracted based on the aggregated attribution (Layer Conductance) values on all the data instances. (B) Important weights are selected using a similar strategy. (C) Pairwise similarities between neurons in the same layers of the two models are computed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 B</head><label>3</label><figDesc>-1) First, for each target instance xD t ,c,p in the class c, we compute the weight values by convolving the weight kernels on the activation values from layer i, resulting in an Ni × Ni+1-dimensional vector</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>( 3 ) Same as Region 2 ( 4</head><label>324</label><figDesc>Number of weights connecting to the (non)important neurons of the adjacent layers(1) Weights (Important Neurons to Important Neurons) Weights (Non-important to Important) Boxplot: Distributions of weight values Pie glyph: Proportion of the weights that have corresponding weights in the source model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Interactions. The network relation view supports various interactions on the similarity matrices and the weights. Users can change the desired class and network layers in the title bar. Clicking on the indices will open a detail panel, Figure 1 (C.1), which shows a feature visualization [53] of the neuron and the top-5 most similar neurons in the other model. By clicking on a square, the details of the associated weight are shown in a pop-up panel including the indices of the connected neurons as well as the weight feature map, Figure 1 (C.2). The visual elements, including cells, boxplots, and pie glyphs, also provide pop-up details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>4 for j = 1 : P do 5 Ai 6 end 7 li 13 li+N</head><label>456713</label><figDesc>,j ← LayerConductance(nj, xD s ,i) ← 0 // Label 0 for source instances 8 end 9 for i = 1 : ND t do 10 for j = 1 : P do 11 Ai+N Ds ,j ← LayerConductance(nj, xD t ,i) 12 end Ds ← 1 // Label 1 for target instances 14 end 15 C = LinearSVM(A, l, 10-fold cross validation) 16 u ← [feature weights of C] // Use the weights from the classifier as domain discriminability values 5.4.2 Visualization and Interactions The design of the feature view, Figure 1 (D), consists of two components: the feature ranking list, and the domain discriminability plot. Feature Ranking List. The details are listed for the neurons involved in the computation of domain discriminability, including the indices and layers of the neurons, the feature visualization of the neuron, and a histogram of Layer Conductance values for all the instances in the source and the target datasets. To differentiate the instances from two domains, the distribution of source instances are placed above the horizontal axis, while the target distribution is at the bottom. The colors of the bars map to the corresponding domains as well. Analysts can select whether to sort the neurons based on their domain discriminability values in ascending or descending order.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 5 .</head><label>5</label><figDesc>The t-SNE projection results for (1) bike and (2) file cabinet are presented. (3) Many of the target weights between Layer 2 and 3 have no associated important source weights, suggesting a weight redistribution in the target model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 6 .</head><label>6</label><figDesc>The result of the digit recognition datasets. (1) The trends of the accuracies are similar to the result in the first case study.(2)For the target model, the misclassified instances in SVHN and MNIST datasets are presented. (3) Several neurons show changes of the learned patterns in the target model. (4) The most domain-invariant neurons (features) indicate that the circular patterns are shared between the two domains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, A. Fan and R. Maciejewski are with Arizona State University. E-mail: {yuxinma,afan5,rmacieje}@asu.edu. • J. He is with the University of Illinois at Urbana-Champaign. E-mail: jingrui@illinois.edu. • A. R. Nelakurthi is with Samsung Research America. E-mail: arunreddy.nelakurthi@gmail.com.</figDesc><table /><note>Manuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication xx xxx. 201x; date of current version xx xxx. 201x. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Thus, the vertical boxes show the weight value distributions connected with the same important neuron in Layer 2. The thicknesses of the rectangles in the boxes represent the number of weights. Pie chart glyphs are attached next to the boxes to show the proportions of weights with correspondence in the source model. For the weights connecting non-important neurons on both sides, we use a histogram in Region (4) to summarize the distribution of the weight values. Note that on the source model side (e.g., the bottom left corner in</figDesc><table /><note>summarize the weight values into boxplots by aggregating the weights on the side of the important neurons. For example, the left side of Region (2) is next to the non-important neurons in Layer 1, while the bottom side connects to the important neurons in Layer 2.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Algorithm 1 :</head><label>1</label><figDesc>Computing the domain discriminability values. Data: P selected neurons, {n1, n2, ..., nP }; the source dataset,XD s = {(xD s ,i, yD s ,i)|i ∈ [1, ND s ]}; the target dataset XD t = {(xD t ,i, yD t ,i)|i ∈ [1, ND t ]} Result: The domain discriminability values for the P neurons, {u1, u2, ..., uP } 1 A ← [empty matrix] (N Ds +N D t )×P // Attribution values 2 l ← [empty vector] // Domain labels 3 for i = 1 : ND s do</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning with differential privacy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mironov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Computer and Communications Security</title>
		<meeting>the ACM Conference on Computer and Communications Security</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="308" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fairsight: Visual analytics for fairness in decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1086" to="1095" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Task-driven comparison of topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="320" to="329" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Do convolutional neural networks learn class hierarchy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alsallakh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="152" to="162" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ModelTracker: Redesigning performance analysis tools for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chickering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Suh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Human Factors in Computing Systems</title>
		<meeting>the ACM Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="337" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">VQA: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Surveying the complementary role of automatic data analysis and visualization in knowledge discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bertini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lalanne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD Workshop on Visual Analytics and Knowledge Discovery Integrating Automated Analysis with Interactive Exploration</title>
		<meeting>the ACM SIGKDD Workshop on Visual Analytics and Knowledge Discovery Integrating Automated Analysis with Interactive Exploration</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="12" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A multi-level typology of abstract visualization tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brehmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munzner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2376" to="2385" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">FairVis: Visual analytics for discovering intersectional bias in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Á</forename><forename type="middle">A</forename><surname>Cabrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Epperson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morgenstern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Visual Analytics Science and Technology</title>
		<meeting>the IEEE Conference on Visual Analytics Science and Technology</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">DICON: Interactive visual analysis of multidimensional clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gotz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2581" to="2590" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ablate, variate, and contemplate: Visual analytics for discovering neural architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cashman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="863" to="873" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visual abstraction and exploration of multi-class scatterplots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1683" to="1692" />
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ReVACNN: Steering convolutional neural network via real-time visual analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD Workshop on Interactive Data Exploration and Analytics</title>
		<meeting>KDD Workshop on Interactive Data Exploration and Analytics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dhamdhere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12233</idno>
		<title level="m">How important is a neuron</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The state of the art in integrating machine learning into visual analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Endert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ribarsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Turkay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Nabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Blanco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rossi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="458" to="486" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Considerations for visualizing comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="413" to="423" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gamut: A design probe to understand how data scientists understand machine learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Head</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Deline</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Drucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Human Factors in Computing Systems</title>
		<meeting>the ACM Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pienta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2674" to="2693" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Summit: Scaling deep learning interpretability by visualizing activation and attribution summarizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1096" to="1106" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exploring the sensitivity of choropleths under attribute uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Maciejewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2576" to="2590" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ActiVis: Visual exploration of industry-scale deep neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kalro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Understanding complex deep generative models using interactive visual experimentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Horng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gan Lab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="310" to="320" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">INFUSE: Interactive feature selection for predictive modeling of high dimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bertini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1614" to="1623" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Interacting with predictions: Visual inspection of black-box machine learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Human Factors in Computing Systems</title>
		<meeting>the ACM Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5686" to="5697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">RetainVis: Visual analytics with interpretable and interactive recurrent neural networks on electronic medical records</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="299" to="309" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Influence-directed explanations for deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Test Conference</title>
		<meeting>IEEE International Test Conference</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A visual analytics system for multi-model comparison on clinical data predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fujiwara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Informatics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="122" to="131" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cluster-based visual abstraction for multivariate scatterplots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2531" to="2545" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Building trust in deep learning system towards automated disease detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">W</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9516" to="9521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">DeepTracker: Visualizing the training process of convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Analyzing the noise robustness of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Visual Analytics Science and Technology</title>
		<meeting>the IEEE Conference on Visual Analytics Science and Technology</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="60" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Analyzing the training processes of deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="87" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Towards better analysis of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="100" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Towards better analysis of machine learning models: A visual analytics perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Informatics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="56" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Visual diagnosis of tree boosting methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="163" to="173" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Transferable Representation Learning with Deep Adaptation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3071" to="3085" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Recent progress and trends in predictive visual analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Maciejewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Computer Science</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="192" to="207" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning under concept drift: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">4347</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The state-of-the-art in predictive visual analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Maciejewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="539" to="562" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">EasySVM: A visual analysis approach for open-box support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Maciejewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Tung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="161" to="175" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Explaining vulnerabilities to adversarial machine learning through visual analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Maciejewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1075" to="1085" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A visual analytical approach for transfer learning in classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">390</biblScope>
			<biblScope unit="page" from="54" to="69" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Splatterplots: Overcoming overdraw in scatter plots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mayorga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1526" to="1538" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Understanding hidden memories of recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Visual Analytics Science and Technology</title>
		<meeting>the IEEE Conference on Visual Analytics Science and Technology</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">RuleMatrix: Visualizing and Understanding Classifiers with Rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bertini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="342" to="352" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Opening the black box: Strategies for increased user involvement in existing algorithm implementations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Muhlbacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Piringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gratzl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sedlmair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Streit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1643" to="1652" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">DeepCompare: Visual and Interactive Comparison of Deep Learning Model Performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Murugesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="47" to="59" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schubert</surname></persName>
		</author>
		<title level="m">Feature visualization. Distill</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Transfer learning for wifi-based indoor localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for the Advancement of Artificial Intelligence Workshop</title>
		<meeting>the Association for the Advancement of Artificial Intelligence Workshop</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">DeepEyes: Progressive visual analytics for designing deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pezzotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Höllt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>Lelieveldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eisemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vilanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="108" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Comparing clusterings using bertin&apos;s idea</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pilh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gribov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Unwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2506" to="2515" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Transfusion: Understanding transfer learning for medical imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3342" to="3352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Visualizing the hidden activity of artificial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Fadel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Falcao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Telea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Falcão</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Telea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="101" to="110" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Squares: Supporting interactive performance analysis for multiclass classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amershi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="70" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="213" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Grad-CAM: Visual explanations from deep networks via gradientbased localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Computationally efficient measures of internal neuron importance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundaje</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.09946</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">explAIner: A visual analytics framework for interactive and explainable machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Spinner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schlegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Assady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schafer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Assady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1064" to="1074" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Seq2seq-Vis: A visual debugging tool for sequence-to-sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Behrisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="353" to="363" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Visual analysis of hidden state dynamics in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="667" to="676" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">EnsembleMatrix: interactive visualization to support machine learning with multiple classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Talbot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Human Factors in Computing Systems</title>
		<meeting>the ACM Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">1283</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A survey on deep transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Neural Networks</title>
		<meeting>the International Conference on Artificial Neural Networks</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Opening the black box -Data driven visualization of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-Y</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Visualization Conference</title>
		<meeting>the IEEE Visualization Conference</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="383" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Visual analysis of large graphs: State-of-the-art and future research challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Landesberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuijper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schreck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kohlhammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Van Wijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Fekete</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Fellner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1719" to="1749" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">DQNViz: A visual analytics approach to understand deep Q-networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="288" to="298" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">GANViz: A visual analytics approach to understand the adversarial game</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1905" to="1917" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">DeepVID: Deep visual interpretation and diagnosis for image classifiers via knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2168" to="2180" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">AtmSeer: Increasing transparency and controllability in automated machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Veeramachaneni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Human Factors in Computing Systems</title>
		<meeting>the ACM Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Visual genealogy of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">A survey of transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">The What-If Tool: Interactive probing of machine learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pushkarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolukbasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="56" to="65" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Visualizing dataflow graphs of deep learning models in tensorflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wongsuphasawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Smilkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Recent trends in deep learning based natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computational Intelligence Magazine</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="55" to="75" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">CNNComparator: Comparative analytics of convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Haleem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Plantaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop on Visual Analytics for Deep Learning (VADL)</title>
		<meeting>Workshop on Visual Analytics for Deep Learning (VADL)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Manifold: A modelagnostic framework for interpretation and diagnosis of machine learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Ebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="364" to="373" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Quantifying the visual impact of classification boundaries in choropleth maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Maciejewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="371" to="380" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">iForest: Interpreting random forests via visual analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="407" to="416" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
