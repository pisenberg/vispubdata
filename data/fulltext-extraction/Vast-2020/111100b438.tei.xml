<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DECE: Decision Explorer with Counterfactual Explanations for Machine Learning Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furui</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Ming</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huamin</forename><surname>Qu</surname></persName>
						</author>
						<title level="a" type="main">DECE: Decision Explorer with Counterfactual Explanations for Machine Learning Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Tabular Data</term>
					<term>Explainable Machine Learning</term>
					<term>Counterfactual Explanation</term>
					<term>Decision Making</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Fig. 1. The DECE interface for exploring a machine learning model&apos;s decisions with counterfactual explanations. The user uses the table view (A) for subgroup level analysis. The table header (A1) supports the exploration of the table with sorting and filtering operations. The subgroup list (A2) presents the subgroups in rows and summarizes their counterfactual examples. The user can interactively create, update, and delete a list of subgroups. The instance lens (A3) visualizes each instance in the focused subgroup as a single thin horizontal line. In the instance view (B), the user can customize (B1) and inspect the diverse counterfactual examples of a single instance in an enhanced parallel coordinate view (B2).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In recent years, we have witnessed an increasing adoption of machine learning (ML) models to support data-driven decision-making in var- ious application domains, which include decisions on loan approvals, risk assessment for certain diseases, and admissions to various universities. Due to the complexity of these real-world problems, well-fitted ML models with good predictive performance often make decisions via complex pathways, and it is difficult to obtain human-comprehensible explanations directly from the models. The lack of interpretability and transparency could result in hidden biases and potentially harmful actions, which may hinder the real-world deployment of ML models.</p><p>To address this challenge, a variety of post-hoc model explanation techniques have been proposed <ref type="bibr" target="#b5">[6]</ref>. Most of the techniques explain the model's decisions by calculating feature attributions or through case-based reasoning. An alternative approach for providing human-friendly and actionable explanations is to present users with counterfactuals, or counterfactual explanations <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b39">39]</ref>. The method answers this question: How does one obtain an alternative or desirable prediction by altering the data just a little bit? For instance, a person submitted a loan request but got rejected by the bank based on the recommendations made by an ML model. Counterfactuals provide explanations like "if you had an income of $40 000 rather than $30 000, or a credit score of 700 rather than 600, your loan request would have been approved." Counterfactual explanations are user-friendly to the general public as they do not require prior-knowledge on machine learning <ref type="bibr" target="#b3">[4]</ref>. Another advantage is that counterfactual explanations are not based on approximation but always give exact predictions by the model <ref type="bibr" target="#b26">[26]</ref>. Watcher et al. summarize the three important scenarios for decision subjects as understanding the decision, contesting the (undesired) decision, and providing actionable recommendations to alter the decision in the future <ref type="bibr" target="#b39">[39]</ref>. For model developers, counterfactual explanations can be used to analyze the decision boundaries of a model, which can help detect the model's possible flaws and biases <ref type="bibr" target="#b42">[42]</ref>. For example, if the counterfactual explanations for loan rejections all require changing, e.g., the gender or race of an applicant, then the model is potentially biased.</p><p>Recently, a variety of techniques have been developed to generate counterfactual explanations <ref type="bibr" target="#b39">[39]</ref>. However, most of the techniques focus on providing explanations for the prediction of individual instances <ref type="bibr" target="#b42">[42]</ref>. To examine the decision boundaries and analyze model biases, the corresponding technique should be able to provide an overview of the counterfactual examples generated for a population or a selected subgroup of the population. Furthermore, in real-world applications, certain constraints are needed such that the counterfactual examples generated are feasible solutions in reality. For example, one may want to limit the range of credit score changes when generating counterfactual explanations for a loan application approval model.</p><p>An interactive visual interface that can support the exploration of the counterfactual examples to analyze a model's decision boundaries, as well as edit the constraints for counterfactual generation, can be extremely helpful for ML practitioners to probe the model's behavior and also for everyday users to obtain more actionable explanations. Our goal is to develop a visual interface that can help model developers and model users understand the model's predictions, diagnose possible flaws and biases, and gain supporting evidence for decision making. We focus on ML models for classification tasks on tabular data, which is one of the most common real-world applications. The proposed system, Decision Explorer with Counterfactual Explanations (DECE), supports interactive subgroup creation from the original data-set and cross-comparison of their counterfactual examples by extending the familiar tabular data display. This greatly eases the learning curve for users with basic data analysis skills. More importantly, since analyzing decision boundaries for models with complex prediction pathways is a challenging task, we propose a novel approach to help users interactively discover simple yet effective decision rules by analyzing counterfactual examples. An example of such a rule is " Body Mass Index (BMI) below 30 (almost) ensures that the patient does not have diabetes, no matter how the other attributes of the patient change." By searching for the corresponding counterfactual examples, we can verify the robustness of such rules. To "flip" the prediction given in this example, the BMI of a diabetic patient must be above 30. Such rules can be presented to the domain experts to help validate a model by checking if they align with domain knowledge. Sometimes new insights are gained from the identified rules.</p><p>To summarize, our contributions include:</p><p>• DECE, a visualization system that helps model developers and model users explore and understand the decisions of ML models through counterfactual explanations.</p><p>• A subgroup-level counterfactual explanation method that supports exploratory analysis and hypothesis refinement using subgroup counterfactual explanations.</p><p>• Three use cases and an expert interview that demonstrate the effectiveness of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Counterfactual Explanation</head><p>Counterfactual explanations aim to find a minimal change in data that "flips" the model's prediction. They provide actionable guidance to end-users in a user-friendly way. The use of counterfactual explanations is supported by the study of social science <ref type="bibr" target="#b22">[23]</ref> and philosophy literature <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b29">29]</ref>. Wachter et al. <ref type="bibr" target="#b39">[39]</ref> first proposed the concept of unconditional counterfactual explanations and a framework to generate counterfactual explanations by solving an optimization problem. With a user-desired prediction y that is different from the predicted label y, a counterfactual example c against the original instance x can be found by solving</p><formula xml:id="formula_0">argmin c max λ λ ( f w (c) − y ) 2 + d(x, c),<label>(1)</label></formula><p>where f w is the model and</p><formula xml:id="formula_1">d(•, •)</formula><p>is a function that measures the distance between the counterfactual example x to the original instance x. Ustun et al. <ref type="bibr" target="#b38">[38]</ref> further discussed factors that affected the feasibility of counterfactual examples and designed an integer programming tool to generate diverse counterfactuals to linear models. Russell <ref type="bibr" target="#b31">[31]</ref> designed a similar method to support complex data with mixed value (a contiguous range or a set of discrete special value  <ref type="bibr" target="#b13">[14]</ref> proposed a general pipeline by solving a series of satisfiability problems. Most existing work focuses on the generation and evaluation of the counterfactual explanations <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">32]</ref>. Instead of generating counterfactual explanations, our work attempts to solve the question of how to convey counterfactual explanation information to a subgroup using visualization. Another focus of our work is to design interactions to help users find more feasible and actionable counterfactual explanations, e.g., with a more proper distance measurement suggested by Rudin <ref type="bibr" target="#b30">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Visual Analytics for Explainable Machine Learning</head><p>A variety of visual analytics techniques have been developed to make machine learning models more explainable. Common use cases for the explainable techniques include understanding, diagnosing, and evaluating machine learning models. Recent advances have been summarized in a few surveys and framework papers <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b34">34]</ref>.</p><p>Most existing techniques target at providing explainability for deep neural networks. Liu et al. <ref type="bibr" target="#b18">[19]</ref> combined matrix visualization, clustering, and edge bundling to visualize the neurons of a CNN image classifier, which helps developers understand and diagnose CNNs. Alsallakh et al. <ref type="bibr" target="#b2">[3]</ref> developed Blocks to identify the sources of errors of CNN classifiers, which inspired their improvements on CNN architecture. Strobelt et al. <ref type="bibr" target="#b36">[36]</ref> studied the dynamics of the hidden states of recurrent neural networks (RNN) as applied to text data using parallel coordinates and heatmaps. Various other work followed this line of research to explain deep neural networks by examining and analyzing their internal representations <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b35">35]</ref>.</p><p>The common limitation of these techniques is that they are often model-specific. It is challenging to generalize them to other types of models that emerge as machine learning research advances. In our work, we study counterfactual explanations from a visualization perspective and develop a model-agnostic solution that applies to both instance-and subgroup-levels.</p><p>The idea of model-agnostic explanation was popularized in LIME <ref type="bibr" target="#b28">[28]</ref>. Visualization researchers have also studied this idea in Prospector <ref type="bibr" target="#b15">[16]</ref>, RuleMatrix <ref type="bibr" target="#b25">[25]</ref>, and the What-If Tool <ref type="bibr" target="#b42">[42]</ref>. Closely related to our work, the What-If Tool adopts a perturbation-based method to help users interactively probe machine learning models. It also offers the functionality of finding the nearest "counterfactual" data points with different labels. Our work investigates the general concept of counterfactuals that are independent of the available dataset. Besides, we utilize subgroup counterfactuals to study and analyze the decision boundaries of machine learning models.</p><p>Our goal is to develop a generic counterfactual-based model explanation tool that helps users get actionable explanations and understand model behavior. For general users like decision subjects, counterfactual examples can help them understand how to improve their profile to get the desired outcome. For users like model developers or decisionmakers, we aim to provide counterfactual explanations that can be generalized for a certain group of instances. To reach our goal, we first survey design studies for explainable machine learning to understand general user needs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b42">42]</ref>. Then we analyze these user needs considering the characteristics of counterfactual explanations and identify two key levels of user interests that relate to counterfactuals: instance-level and subgroup-level explanations.</p><p>Instead of understanding how the model works globally, decision subjects are more interested in knowing how a prediction is made based on an individual instance, like their profile. This makes instance-level explanations more essential for decision subjects. At the instance-level, we aim to empower users with the ability to:</p><p>R1 Examine the diverse counterfactuals to an instance. Accessing an explanation of the model's prediction on a specific instance is a fundamental need. To be more actionable, it is often helpful to provide several counterfactuals that cover a diverse range of conditions than a single closest one <ref type="bibr" target="#b39">[39]</ref>. The user should also be able to examine and compare them in an efficient manner. The user can examine the different options and choose the best one based on individual needs. R2 Customize the counterfactuals with user-preferences. Providing multiple counterfactuals and hoping that one of them matches user needs may not always work. In some situations, it is better to allow users to directly specify preferences or constraints on the type of counterfactuals they need. For example, one home buyer may prefer a larger house, while another buyer only cares about the location and neighborhood.</p><p>Similar to the "eyes beat memory" principle, it is hard to view and memorize multiple instance-level explanations and derive an overall understanding of the model. Explaining machine learning models at a higher level than an instance can help users understand the general behavior of the model <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">25]</ref>. One of our major goals is to enable subgroup-level analysis based on counterfactuals. Subgroup analysis of the counterfactuals is crucial for users like model developers and policy-makers, who need an overall comprehension of the model and the underlying dataset. A subgroup also provides a flexible scope that allows iterative and comparative analysis of model behavior. At the subgroup-level, we aim to provide users the ability to:</p><p>R3 Select and refine a data subgroup of interest. To conduct subgroup analysis using counterfactual explanations, the users should first be equipped with tools to select and refine subgroups. Interesting subgroups could be those formed from users' prior knowledge or those that could suggest hypotheses for describing the model. For instance, a high glucose level is often considered a strong sign of diabetes. The user (patient or doctor) may be interested in a subgroup consisting of low glucose-level patients labeled as healthy, and see if most of their counterfactual examples (patients with diabetes) have high glucose levels. However, drilling down to a proper subgroup (i.e., an appropriate glucoselevel range) is not easy. Providing essential tools to create and iteratively refine subgroups could largely benefit users' exploration processes. R4 Summarize the counterfactual examples of a subgroup of instances. With a subgroup of instances, we are interested in the distribution of their counterfactual examples. Do they share similar counterfactual examples? Are there any counterfactual examples that lie inside the subgroup? An educator would be interested in knowing if the performance of a certain group of students can be improved with a single action. It is also useful for model developers to form and verify their hypothesis by investigating a general prediction pattern over a subgroup.</p><p>R5 Compare the counterfactual examples of different subgroups. Comparative analysis across different groups could lead to deeper understanding. It is also an intuitive way to reveal potential biases in the model. For instance, to achieve the same desired annual income, do different genders or ethnic groups need to take different actions? Comparison can provide evidence for progressive refinement of subgroups, helping users to identify a salient subgroup that has the same predicted outcome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">COUNTERFACTUAL EXPLANATION</head><p>In this section, we first introduce the techniques and algorithms that we use to generate diverse actionable explanations with customized constraints (R1, R2). Subsequently, we propose the definition of rule support counterfactual examples, which is designed to support exploring a model's subgroup-level behaviours (R3, R4, R5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Generating Counterfactual Examples</head><p>As introduced in Sect. 2.1, given a black box model f : X → Y , the problem of generating counterfactual explanations to an instance x is to find a set of examples {c 1 , c 2 , ..., c k } that lead to a desired prediction y , which are also called counterfactual examples (CF examples). The CF examples can suggest how a decision subject could act to achieve the user's targets. The problem we address in this section is how to generate CF examples that are valid and actionable. CF examples are actionable when they appropriately consider proximity, diversity, and sparsity. First, the generated examples should be proximal to the original instance, which means only a small change has to be made to the user's current situation. However, one predefined distance metric cannot fit every need because people may have different preferences or constraints <ref type="bibr" target="#b31">[31]</ref>. Thus, we want to offer diverse options (R1) to choose from and also allow them to add constraints (R2) to reflect their preferences or narrow their searches. Finally, to enhance the interpretability of the examples, we want the examples to be sparse, which means that only a few features need to be changed.</p><p>We follow the framework of DiCE <ref type="bibr" target="#b26">[26]</ref> and design an algorithm to generate both valid and actionable CF examples using three procedures. First, we generate raw CF examples by considering their validity, proximity, and diversity. To make the trade-off between these three properties, we optimize a three-part loss function as</p><formula xml:id="formula_2">L = L valid + λ 1 L dist + λ 2 L div .<label>(2)</label></formula><p>Validity. The validity term L valid ensures the generated CF examples reach the desired prediction target. We define it as:</p><formula xml:id="formula_3">L valid = k ∑ i=1 loss( f (c i ), y ),</formula><p>in which the loss is a metric to measure the distance between the target y and the prediction of each CF example f (c i ). For classification tasks, we only require that the prediction flips, and high confidence or possibility of the prediction result is not necessary. Thus, instead of choosing the commonly used L 1 or L 2 loss, we let loss be the ranking loss with zero margins. In a binary classification task, the loss function is loss(y pred , y ) = max(0, − y * (y pred −0.5)), in which the target y = ±1, and y pred is the prediction of the CF example by the model f (c), which is normalized to [0, 1].</p><p>Proximity. As suggested by the proximity requirement, we want the CF examples to be close to the original instance by minimizing L dist in the loss function. We define the proximity loss as the sum of the distance from the CF examples to the original instance:</p><formula xml:id="formula_4">L dist = k ∑ i=1 dist(c i , x).</formula><p>We choose a weighted Heterogeneous Manhattan-Overlay Metric (HMOM) <ref type="bibr" target="#b43">[43]</ref> to calculate the distance as follows:</p><formula xml:id="formula_5">dist(c, x) = ∑ f ∈F d f (c f , x f ),<label>(3)</label></formula><p>where</p><formula xml:id="formula_6">d f (c f , x f ) = |c f −x f | (1+MAD f )•range f if f indexes a continuous feature 1(c f = x f )</formula><p>if f indexes a categorical feature .</p><p>For continuous features, we apply a normalized Manhattan distance metric weighted by 1/(1+MAD f ) as suggested by Watcher et al. <ref type="bibr" target="#b39">[39]</ref>, where MAD f is the median absolute deviation (MAD) value of the feature f . By applying this weight, we encourage the feature values with large variation to change while the rest stay close to the original values. For categorical features, we apply an overlap metric 1(c f = x f ), which is 1 when c f = x f and 0 when c f = x f . Diversity. To achieve diversity, we encourage the generated examples to be separated from each other. Specifically, we calculate the pairwise distance of a set of CF examples and minimize:</p><formula xml:id="formula_7">L div = − 1 k k ∑ i=1 k ∑ j=i dist(c i , c j ),</formula><p>where the distance metric is defined in Equation 3.</p><p>To solve the above optimization problem, we could use any gradient-based optimizers. For simplicity, we use the classic stochastic gradient descent (SGD) in this work. As discussed in R2, we want to allow users to specify their preferences by adding constraints in the generation process. The constraints decide if and within what range a feature value should change. To fix the immutable feature values, we update them with a masked gradient, i.e., the gradient to the immutable feature values is set to 0. We also run a clip operation every K iteration to project the feature values to a feasible value in the range.</p><p>Sparsity. The sparsity requirement suggests that only a few feature values should change. To enhance the sparsity of the generated CF examples, we apply a feature selection procedure. We first generate raw CF examples from the previous procedure. Then we select the top-k features for each CF example separately with the normalized maximum value changes weighted by 1/(1 + MAD f ). At last, we repeat the above optimization procedure with only these k features by masking the gradient of other features. The generated CF examples are sparse with at most k changed feature values.</p><p>Post-hoc validity. In previous procedures, we treat the value of each continuous feature as a real number. However, in a real-world dataset, features may be integers or have certain precisions. For example, a patient's number of pregnancies should be an integer, and a value with decimals for this feature can bring confusion to users. Thus, we project each CF example c i to a meaningful onec i . Let the validity of projected CF examples,c i , exist as post-hoc validity. We design a post-hoc process as the third procedure to improve the post-hoc validity by refining the projected CF examples.</p><p>In each step of the process, we calculate the gradient of each feature to the loss L (Equation 2), grad i = ∇c i loss(c i , x), and update the projected CF example by updating the feature value with the largest absolute normalized gradient value j = argmax f ∈F (|grad</p><formula xml:id="formula_8">f i |): c j i,t+1 =c j i,t + max(p j , ε|grad j i |) sign(grad j i ),<label>(4)</label></formula><p>where p j notes the unit of the feature j and ε is a given hyperparameter, which usually equals the learning rate in the SGD process above. The process ends when the updated CF example is valid, or the number of steps reaches a maximum number, which is often set as the number of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Rule Support Counterfactual Examples</head><p>We first propose a subgroup-level exploratory analysis procedure for understanding a model's local behavior. Then we introduce the definition of rule support counterfactual examples (r-counterfactuals), which is designed to support such an exploratory analysis procedure. One of the major goals of exploratory analysis is to suggest and assess hypotheses <ref type="bibr" target="#b37">[37]</ref>. The exploration starts with a hypothesis about the model's prediction on a subgroup proposed by users. A hypothesis is an assertion in the form of an if-else rule that describes a model's prediction, e.g., "People who are under 30 years old and whose BMI is under 35 will be predicted healthy by the diabetes prediction model." No matter how the other features change (e.g., smoking or not), as long as the two conditions (under 30 years old with BMI under 35) hold, the person is unlikely to have diabetes. Each hypothesis describes the model's behavior on a subgroup defined by range constraints on a set of features ( <ref type="figure" target="#fig_0">Fig. 2A)</ref>:</p><formula xml:id="formula_9">S = D ∩ I 1 × I 2 × ... × I k ,<label>(5)</label></formula><p>where D is the dataset and I j defines the value range of feature j. The value range is a continuous interval for continuous features, and a set of selectable categories for categorical features.</p><p>The users expect to find out whether the model's prediction on the collected data conforms to the hypothesis and, more importantly, if the hypothesis generalizes in unseen instances. CF examples can be used to answer the two questions. Intuition suggests that if we can find a feasible CF example against one of the instances in the subgroup, the hypothesis might not be valid. For example, if we can find a person whose prediction for having diabetes can be flipped to positive but age &lt; 30 and BMI &lt; 35, the hypothesis that "people under 30 years old with BMI under 35 will be predicted healthy" does not hold. Otherwise, the hypothesis is supported by the CF examples.</p><p>For an invalid hypothesis, CF examples also suggest how to refine it. For example, if a CF example tells that "a 29-year-old smoker whose BMI is 30 is predicted as diabetic", it suggests that the user may narrow the subgroup to age &lt; 30 and BMI &lt; 30 or refer to other feature values (e.g., smoking ∈ {no}). With multiple rounds of hypothesis, users can understand the model's prediction on a subgroup of interest.</p><p>In our initial approaches, we find that unconstrained CF examples would overwhelm users due to the complex interplay of multiple features. Thus, we simplify the problem by only focusing on one feature at a time. This is achieved by generating a group of constrained CF examples called rule support counterfactual examples (rcounterfactuals). These are counterfactuals that support a rule. Specifically, with a given subgroup, we generate CF examples by only allowing the value of one feature j to change in the domain X j . In contrast, other feature values can only vary in the limited range, I j . For each feature j, we generate r-counterfactuals ( <ref type="figure" target="#fig_0">Fig. 2B</ref>) by solving:</p><formula xml:id="formula_10">r-counterfactuals j : argmin {ci} ∑ xi∈S L(x i , c i ),<label>(6)</label></formula><p>such that:</p><formula xml:id="formula_11">c i ∈ I 1 × I 2 × ... × X j × I n ,<label>(7)</label></formula><p>where the L is the loss function defined as in Equation 2. We generate multiple r-counterfactuals for each feature to analyze their effects on the model's predictions to the subgroup. To speed up the generation of CF examples, we adapt the minibatch method. As such, we are able to support the exploratory procedure for refining hypotheses. If, in all r-counterfactuals groups, every CF example falls outside the feature ranges, the robustness of this claim is supported by even potentially unseen examples-even when we intentionally seek negative examples that could invalidate the hypothesis, it is not possible to do so. Otherwise, users may refine or reject the hypothesis as suggested by the r-counterfactuals groups <ref type="figure" target="#fig_0">(Fig. 2C)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DECE</head><p>In this section, we first introduce the architecture and workflow of DECE. Then we describe the design choices in the two main system interface components: table view and instance view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Overview</head><p>As a decision exploration tool for machine learning models, DECE is designed as a sever-client application. To make the system extensible with different models and counterfactual explanation algorithms, we design DECE with three major components: the Data Storage module, the CF Engine module, and the Visual Analysis module. The former two are integrated into a web-server implemented using Python with Flask, and the last one is implemented with React and D3.</p><p>The Data Storage module provides configuration options so advanced users can easily supply their own classification models and datasets. The CF Engine implements a set of algorithms for generating CF examples with fully customizable constraints. It also implements the procedure for producing subgroup-level CF examples (as described in Sect. 4.2). The Visual Analysis module consists of an instance view and a table view. The instance view allows a user to customize and inspect the CF examples of a single instance of interest (R1, R2). The table view presents a summary of the instances of the dataset and their CF examples (R4). It allows subgroup-level analysis through easy subgroup creation (R3) and counterfactual comparisons (R5).</p><p>The instance view and table view complement each other as a whole in supporting the exploratory analysis of model decisions. As shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, by exploring the diverse CF examples of specific instances in the instance view, users can spot potentially interesting counterfactual phenomena and suggest related hypotheses. With hypotheses in mind, either formulated through exploration or prior experiences, users can utilize the r-counterfactuals integrated into the table view to assessing plausibility. After refining a hypothesis, users can then verify or reject it by attempting to validate the corresponding instances in the instance view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Visualization of Subgroup with R-counterfactuals</head><p>Verifying and refining the hypothesis on the model's subgroup-level behaviors is the most critical and challenging part of the exploratory analysis procedure. We focus on one feature at a time and refine the hypothesis achieved with r-counterfactuals (described in Sect. 4.2). For each r-counterfactuals group, we use a set of hybrid visualization to summarize and compare the r-counterfactuals and original subgroup's instance value on each focusing feature.</p><p>Visualize Distribution (R4). To summarize an r-counterfactuals group, we use two side-by-side histograms to visualize the distributions of the original group and counterfactuals group <ref type="figure" target="#fig_2">(Fig. 4A)</ref>. The color of the bars indicates the prediction class. Sometimes, the system cannot find valid CF examples, which indicates that the prediction for these instances can hardly be altered by changing their feature values with the constraints hold. In this case, we use grey bars to indicate their number and stack them on the colored bar. The two histograms are aligned horizontally, with the upper one referring to the original data and the bottom one referring to the counterfactuals group. For continuous features, a shadow box with two handles is drawn to show the range of the subgroup's feature value. To provide a visual hint for the quality of the subgroup, we use the color intensity of the box to signify the Gini impurity of data in this range. When users select a subgroup containing instances all predicted as positive, a darker color indicates that (negative) CF examples can be easily found within this subgroup. In this case, the hypothesis might not be favorable and needs to be refined or rejected. For categorical features, we use bar charts instead. Triangle marks are used to indicate the selected categories.</p><p>Visualize Connection. CF examples are paired with original instances. The pairing information between the two groups can help users understand how the original instances need to change to flip their predictions. In another sense, it also helps to understand how the local decision boundary can be approximated <ref type="bibr" target="#b26">[26]</ref>. Intuition hints that the feature with a larger change is likely to be a more important one. The magnitude of the changes also indicates how difficult the subgroup predictions are to flip by modifying that feature. We use a Sankey diagram to display the flow from original group bins (as input bins) to counterfactual group bins (as output bins) <ref type="figure" target="#fig_2">(Fig. 4B)</ref>. For each link, the opacity encodes the flow amount while the width encodes the relative flow amount to the input bin size. An alternative design is the use of a matrix to visualize the flow between the bins <ref type="figure" target="#fig_2">(Fig. 4D</ref>). Each cell in the matrix represents the number of instance-CF pairs that fail in the corresponding bins. However, in practice, the links are quite sparse. Thus, compared with the matrix, the Sankey diagram saves much space and also emphasizes the major changes of CF from the original value, so we choose the Sankey diagram as our final design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Refine Subgroup (R3).</head><p>As we have discussed in Sect. 4.2, a major task during the exploration is to assess and refine the hypothesis. What is a plausible hypothesis represented by a subgroup? A general guideline is: a subgroup is likely to be a good one if it separates instances with different prediction classes. The intuition is that if a boundary can be found to separate the instances, the refined hypothesis is likely to be valid. We choose the information gain based on Gini impurity to indicate a good splitting point, which is widely used in decision tree algorithms <ref type="bibr" target="#b4">[5]</ref>. The information gain is computed as</p><formula xml:id="formula_12">1 − (N le f t /N) • I G (D le f t ) − (N right /N) • I G (D right ),</formula><p>where D le f t , D right are the data including both original instances and CF examples, split into the left set or the right set. At first, we try to visualize the information gain in a heatmap lie between the two histograms <ref type="figure" target="#fig_2">(Fig. 4E)</ref>. Though such design has the advantage of not requiring extra space, we find that it is hard to distinguish the splitting point with the maximum information gain from the heat map. To make the visual hint salient, we visualize the impurity scores as a small Sparkline under the histograms. We use color and height to double encode this information. For continuous features, we enable users to refine the hypothesis by dragging the handles to a new range. And for categorical features, users are allowed to click the bars to update the selected categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Table View</head><p>The table view <ref type="figure">(Fig.1A)</ref> is a major component and entry point of DECE. Organized as a table, this view summarizes the subgroups with their r-counterfactuals as well as the details of instances in a focused subgroup. Vertically, the table view consists of three parts. From top to bottom, they are the table header, the subgroup list, and the instance lens. The table header is a navigator to help users explore the data in the rest of the table. The subgroup list shows a summary of multiple groups, while the instance lens shows details for a specific subgroup. The three components are aligned horizontally with multiple columns, each corresponding to a feature in the dataset. The first column, as an exception, shows the label and prediction information. Next, we explain the three parts and interactions that enable them to work together. <ref type="table">Table Header</ref>. The table header <ref type="figure">(Fig.1A1)</ref> presents the overall data distribution of the features in a set of linked histograms/bar charts. The first column shows the predictions of the instances in a stacked bar chart, where each bar represents a prediction class. Users can click a bar to focus on a specific class. We indicate false predictions by a hatched bar area and true predictions by a solid bar area. In each feature column, we also use two histograms/bar charts (introduced in Sect. 5.2) to visualize the distribution of the instances and CF examples. Users can efficiently explore the data and their CF examples via filtering and sorting. For each feature, filtering is supported by brushing (for continuous features) or clicking (for categorical features) on both histograms/bar charts. After filtering, that data is highlighted in the histograms while the rest is represented as translucent bars. To sort, users can click on the up/down buttons that pop-up when hovering on each feature.</p><p>Subgroup List. The subgroup list <ref type="figure" target="#fig_0">(Fig.1A2</ref>) allows users to create, refine, and compare different subgroups. Here, each row corresponds to a subgroup. The first column presents the predictions of the instances in the same design used in the table header. In other columns, each cell (i, j) presents a summary of the subgroup i with r-counterfactuals for the feature j (introduced in Sect. 5.2). The subgroup list is initialed with one default group, which is the whole dataset with unconstrained CF examples. Starting with this group, users can refine the group by changing ranges for each feature and clicking the update button. The users can also copy or delete an unwanted subgroup to maintain the subgroup list, which helps track explorations history. By aligning different subgroups together, users can gain insights from a side-by-side comparison (R5).</p><p>Instance Lens. When users click a cell in the subgroup list, the instance lens <ref type="figure" target="#fig_1">(Fig.1A3</ref>) presents details pertaining to each instance and CF examples. To ensure the scalability to a large group of instances, we design an instance lens based on <ref type="table">Table Lens [</ref> <ref type="bibr" target="#b27">27]</ref>, a technique for visualizing large tables. In the instance lens, we design two types of cells: row-focal and non-focal cells with different visual representations. A CF example shows minimal changes from the original instance. For numerical features, we use a line segment to show how the change is made. The x-position of the two endpoints encodes the feature value of the original instance and the CF example. The endpoint corresponding to the original instance is marked black. We use green and red to indicate a positive or negative change. For categorical features, we use two broad line segments to indicate the category of the original instance (in a deeper color) and the CF example (in a lighter color). Users can focus on a few instances by clicking them. Then the non-focal cells become row-focal cells where the text of the exact feature value is presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Instance View</head><p>The instance view helps users inspect the diverse counterfactuals of a single instance (R1). The inspection results can be used to support or reject their hypothesis formed from the table view. For users, mostly decision subjects instance view can be used independently to find actionable suggestions to achieve the desired outcome.</p><p>The instance view consists of two parts. The setting panel <ref type="figure">(Fig. 1B1)</ref> shows the prediction result of the input instance as well as the target class for the CF examples. In the panel, users can also set the number of CF examples they expect to find and the maximum number of features that are allowed to change to ensure the sparsity. The resulting panel <ref type="figure" target="#fig_0">(Fig. 1B2</ref>) displays each feature in a row. It allows users to input a data instance by dragging sliders to set values of each feature. The distribution of each feature value is presented in a histogram, which suggests the users compare the instance's feature values with the overall distribution of the whole dataset. For each feature, uses can add constraints (R2) by setting the value range of CF examples or locking the feature to prevent it from changing. The interactions help decision subjects to customize actionable counterfactual explanations for their scenarios.</p><p>After users input the instance, they click the "search" button in the panel. The system then returns all CF examples found in both valid and invalid sets. The CF examples, as well as the original instances, are presented in a set of polylines along parallel axes <ref type="figure" target="#fig_0">(Fig. 1B2)</ref>, where we use color to indicate their validity and prediction class. For valid CF examples and the original instance, we apply the same color use in the table view to show their prediction class, while invalid CF examples are presented in grey. Users can depict details of a valid CF example by hovering on the polyline. It is then highlighted, and the text of each feature value will be presented on the axis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATION</head><p>Next, we demonstrate the efficacy of DECE through three usage scenarios targeting three types of users: decision-makers, model developers, and decision subjects. We also gather feedback from expert users through formal interviews and interactive trails of the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Usage Scenario: Diabetes Classification</head><p>In the first scenario, Emma, a medical student who is interested in the early diagnosis of diabetes, wants to figure out how a diabetes prediction model makes predictions. She has done some medical data analytics before, but she does not have much machine learning knowledge. She finds the Pima Indian Diabetes Dataset (PIDD) <ref type="bibr" target="#b33">[33]</ref> and downloads a model trained on PIDD. The dataset consists of medical measurements of 768 patients, who are all females of Pima Indian heritage and at least 21 years old. The task is to predict whether a patient will have diabetes within 5 years. The features of each instance include the number of pregnancies, glucose level, blood pressure, skin thickness, insulin, body mass index (BMI), diabetes pedigree function (DPF), and age, which are all continuous features. The dataset includes 572 negative (healthy) instances and 194 positive (diabetic) instances. The model is a neural network with two hidden layers with 76% and 79% test and training accuracy, respectively.</p><p>Formulate Hypothesis (R3). Emma is curious about how the model makes predictions for patients younger than 40. Based on her prior knowledge, she formulates a hypothesis that patients with age &lt; 40 and glucose &lt; 100 mmol/L are likely to be healthy. She loads the data and the model to DECE and creates a subgroup by limiting the ranges on age and glucose accordingly.</p><p>Refine Hypothesis.</p><p>The subgroup consists of 167 instances, all predicted as negative. However, she notices that for most of the features, the range boxes are colored with a dark red <ref type="figure" target="#fig_3">(Fig. 5A)</ref>, indicating that CF examples can be found within the subgroup. This suggests that patients with a diabetic prediction potentially exist in the subgroup and implies that the hypothesis may not be valid. After a deeper inspection, she finds a number of CF examples <ref type="figure" target="#fig_3">(Fig. 5A1)</ref> with age &lt; 40. Then she checks each feature and tries to refine the hypothesis by restricting the subgroup to a smaller one. She finds that the BMI distribution of CF examples is shifted considerably to the right in comparison to the original distribution <ref type="figure" target="#fig_0">(Fig. 5A2)</ref>. This means that BMI needs to be increased dramatically to flip the model's prediction to positive. Thus, Emma suspects that the original hypothesis may not hold for patients with high BMI (obesity). She proceeds to refine the subgroup by adding a constraint on BMI. The green peek in the bottom sparkline <ref type="figure" target="#fig_0">(Fig. 5A2)</ref> suggests that a split at BMI = 35 could make a good subgroup. In the meantime, she discovers a similar pattern for DPF <ref type="figure" target="#fig_1">(Fig. 5A3)</ref>. She adds two constraints of BMI &lt; 35 and DPF &lt; 0.6, and then she creates an updated subgroup. After the refinement, Emma finds that most of the cells are covered by a transparent range box or filled with mostly light grey bars <ref type="figure" target="#fig_3">(Fig. 5B)</ref>. The light grey area represents the instances for which no valid CF examples can be found. One exception is the BMI cell <ref type="figure" target="#fig_3">(Fig. 5B1)</ref>, where a few CF examples can still be generated. To check the details, Emma focuses on this subgroup by clicking the "zoom-in" button. After the table header is updated, she filters the CF examples with BMI &lt; 35 by brushing on the header cell <ref type="figure" target="#fig_0">(Fig. 5B2)</ref>. She finds that all the valid CF examples have an extreme pregnancy value <ref type="figure" target="#fig_1">(Fig. 5B3)</ref>, which means patients who have had several pregnancies are exceptions to the hypothesis. However, such pregnancy values are very rare, so Emma updates the subgroup with a constraint of pregnancy &lt; 6, which covers most of this subgroup.</p><p>After the second update, the refined hypothesis is almost valid, yet some CF examples can still be found in the subgroup, which challenges the hypothesis <ref type="figure" target="#fig_3">(Fig. 5C1)</ref>. By checking the detailed instances of "unsplit" feature groups, she finds that all these valid CF examples have an unlikely blood pressure of 0. These blood pressure values are likely caused by missing data entries. She raises the minimum blood pressure value to a normal value of 40 and then makes the third update. The final subgroup consists of 86 instances, all predicted negative for diabetes, and all feature cells are covered with a totally transparent box, indicating a fully plausible hypothesis.</p><p>Draw Conclusions. After three rounds of refinement, Emma concludes that patients with age &lt; 40, glucose &lt; 100, BMI &lt; 35, and DPF &lt; 0.8 are extremely unlikely to have diabetes within the next five years, as suggested by the model. This statement is supported by 86 out of 768 instances in the dataset (11%) and their corresponding counterfactual instances. Generally, Emma is satisfied with the conclusion but concerned with something she sees in the blood pressure column. It seems that although all the bars turn grey, a clear shift between the two histograms exists <ref type="figure" target="#fig_3">(Fig. 5D1)</ref>, indicating that the model is trying to find CF examples with low blood pressures.</p><p>She is confused about why low blood pressure can also be a symptom of diabetes. Initially, she thinks it may be a local flaw in the model caused by mislabeled instances of 0 blood pressure. So she finds another model trained on a clean dataset and runs the same procedure. However, the pattern still exists. After some research, she finds out that a diabetes-related disease called Diabetic Neuropathy 1 may cause low blood pressure by damaging a type of nerve called 1 https://en.wikipedia.org/wiki/Diabetic neuropathy autonomic neuropathy. She concludes the exploratory analysis and gains new knowledge from the process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Usage Scenario: Credit Risk</head><p>In this usage scenario, we show how DECE can help model developers understand their models. Specifically, we demonstrate how comparative analysis of multiple subgroups can support their exploration.</p><p>A model developer, Billy, builds a credit risk prediction model from the German Credit Dataset <ref type="bibr" target="#b8">[9]</ref>. The dataset contains credit and personal information of 1000 people and their credit risk label of either good or bad. The model he trained is a neural network with two hidden layers, which achieves an accuracy of 82.2% on the training set and 74.5% on the test set. Billy wants to know how the model makes predictions for different subgroups of people (R5). Particularly, he wants to figure out if gender affects the model's prediction and the behavior of the model's predictions on different gender groups.</p><p>Billy begins the exploration by using the whole dataset and CF examples <ref type="figure">(Fig. 6A)</ref>. He notices that most of the CF examples with a flipped prediction as bad-risk have an extremely large amount of debt or debt that has lasted a long time <ref type="figure">(Fig. 6A1</ref>). This finding indicates that almost certainly, a client with an extremely large credit amount or duration would be predicted as a bad credit risk.</p><p>Select Subgroup (R3). He wants to learn if there are other factors that might affect credit risk prediction. So he creates a subgroup with credit amount &lt; 6000 DM and duration &lt; 40 months, containing a majority of the dataset. Then he selects the male group to inspect the model's predictions and counterfactual explanations. The male group <ref type="figure">(Fig. 6B</ref>) contains a total of 550 instances, and 511 instances are predicted to be good candidates. Billy checks the gender column and finds that a few CF examples are generated by changing the gender from male to female <ref type="figure">(Fig. 6B1</ref>). After Billy zooms in to the gender cell and filters the CF examples with gender = f emale <ref type="figure" target="#fig_0">(Fig. 6B2)</ref>, he finds that 22 instances have CF examples with gender changed. This indicates that gender may be considered in the prediction.</p><p>Inspect Instance.</p><p>Billy wants to figure out whether this pattern is intentional or accidental. He randomly puts one instance into the instance view.</p><p>Billy sets the feature ranges, credit amount &lt; 6000 and duration &lt; 40 to be the same as the subgroup's feature ranges. He clicks the search button to find a set of CF examples to probe the model's local decision <ref type="figure" target="#fig_1">(Fig. 6B3)</ref>. He finds that all valid CF examples alter the prediction from good credit candidates to bad credit candidates by either suggesting a worse job <ref type="figure">Fig. 6</ref>. Subgroup comparison on a neural network trained on German Credit Dataset. A. The whole dataset with CF examples where the distribution of the credit duration (A1) suggests that a long duration of debt will lead to a "bad" risk for all loan applicants. B. The male subgroup covers a majority of male instances, where the gender column (B1) suggests that a few CF examples are generated by changing the gender from male to female (B2). B3. Diverse CF examples against a sample male instance from B2, where all valid CF examples (orange lines) suggest either to change the gender from male to female or to degrade the job rank. C. The narrowed male subgroup, where a larger portion of the instances have CF examples that change their gender to female (C1). The CF examples are found far from the subgroup (C2). D. The contrast female subgroup against the male subgroup C, where CF examples can be found within the subgroup (D1).</p><p>(down by one rank) or changing the gender from male to female. He locks the job attribute and tries again. He finds that all CF examples suggest changing the gender from male to female. This indicates that gender affects the model's prediction in this instance and similar ones.</p><p>Refine Subgroup. Then Billy wants to find if there are any subgroups where gender has an enormous impact. He finds most of the male instances with unactionable CF examples that need to change their genders are from a subgroup of wealthy people, who have a good job (2-3), an above-moderate saving account balance, and apply for credit in order to buy a car <ref type="figure" target="#fig_0">(Fig. 6B2</ref>). Billy creates a wealthy subgroup and makes the update <ref type="figure">(Fig. 6C</ref>). Then he finds that compared with the former group, a larger portion of the instances have CF examples that change their gender to female <ref type="figure">(Fig. 6C1)</ref>. The major shift of gender in the CF examples implies that gender plays a more important role in the predictions of the wealthy subgroup.</p><p>Compare Subgroups (R5). To confirm this observation, Billy compares the male subgroup with another female subgroup, which has all of the same feature ranges except gender (R5). Billy finds that all 56 instances in the male subgroup are predicted as good credit candidates, and in the female subgroup, 29 out of 30 instances are predicted to be good credit candidates <ref type="figure">(Fig. 6D)</ref>. Then Billy compares the distribution of the CF examples against the two subgroups. In the male subgroup <ref type="figure">(Fig. 6C)</ref>, indicated by the transparent range boxes, all the valid CF examples are generated out of range. These CF examples support the conclusion that any potential male applicant within this subgroup is very unlikely to be predicted as a bad credit risk. In the credit amount column <ref type="figure">(Fig. 6C1</ref>), all the valid CF examples are found with credit amount &gt; 10000 DM. This means that if a man, who has a good job (2-3) and an above-moderate saving accounts balances, applies for a line of credit of 10000 DM with a term of fewer than 40 months, it is very likely that his request will be approved. However, in the female subgroup <ref type="figure">(Fig. 6D)</ref>, a large amount of CF examples are found within this group. In the credit amount column <ref type="figure">(Fig. 6D1)</ref>, CF examples can be found with credit amount &lt; 6000 DM. This indicates that a loan request of credit amount = 6000 DM from a woman in the same condition may be rejected.</p><p>Draw Conclusions. After the exploratory analysis, Billy concludes that gender plays an important role in the model's prediction for a subgroup of wealthy people defined above. In addition, the finding is supported by both instance-level and subgroup-level evidence. This is a strong sign that the model might behave unfairly towards different genders. Billy then decides to inspect the training data and conduct further research to eliminate the model's probable gender bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Usage Scenario: Graduate Admissions</head><p>In this usage scenario, we show how DECE provides model subjects with actionable suggestions. Vincent, an undergraduate student with a major in computer science, is preparing an application for a master's program. He wants to know how the admission decision is made and if there are any actionable suggestions that he can follow. He has downloaded a classifier, a neural network with two hidden layers trained on a Graduate Admission Dataset <ref type="bibr" target="#b0">[1]</ref> to predict whether a student will be admitted to a graduate program. He inputs his profile information and the model predicts that he is unlikely to be accepted. He wants to know how he can improve his profile to increase his acceptance chance.</p><p>He chooses DECE and focuses on the instance view. He inputs his profile information using the sliders. He finds that compared with other instances in the whole dataset, his cumulative GPA (CGPA) of 8.2 is average while both his GRE score, 310, and TOFEL score, 96, are below average. According to the provided rating scheme, he sets his undergraduate university rating, statement of purpose, and recommendation letter as 2, 3, and 2, respectively, which are all below average. Then he attempts to find valid CF examples. First, he goes through the attributes and locks the university rating since it is not changeable (R2). Then he sets the number of CF examples to 15 and clicks the search button to get the results <ref type="figure">(Fig. 7A)</ref>. He is surprised that so many diverse CF examples can be found (R1). However, he finds that over half of these CF examples have either a high GRE score (above 320) or TOEFL score (above 108), which would be challenging for him to achieve. Since he has already finished most of the courses in the undergraduate program, it would be difficult to boost his CGPA. So, considering his current situation, he locks the CGPA as well and tries to find CF examples with GRE &lt; 320 and TOEFL &lt; 100 (R2). In the second round of attempts, the system can still find a few valid CF examples (7B). He notices that all valid CF examples contain a GRE score above 315, which might indicate a lower bound of the GRE score initially suggested by the model. Also, all the valid CF examples suggest that a stronger recommendation letter would help.</p><p>Finally, Vincent is satisfied with the knowledge learned from the investigation and decides to pick a CF example to guide his application preparation, which suggests he increase his GRE score to 317, TOEFL score to 100, and obtain a stronger recommendation letter. <ref type="figure">Fig. 7</ref>. A student gets actionable suggestions for graduate admission applications. A. Diverse CF examples generated with university rating locked. B. Customized CF examples with user constraints, including GRE score &lt; 320, TOFEL score &lt; 100, and a locked CGPA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Expert Interview</head><p>To validate how effective DECE is in supporting decision-makers and decision subjects, we conduct individual interviews with three medical students (E1, E2, E3). All medical students have had a one-year clinical internship in hospitals. They have basic knowledge of statistics and can read simple visualizations. These (almost) medical practitioners can be regarded as decision-makers in the medical industry. Also, they have experience interacting with patients and can offer valuable feedback regarding the need from end-users.</p><p>The interviews were held in a semi-structured format. We first introduced the counterfactual-based methods by cases and figures without algorithm details. Then we introduced the interface of DECE using the live case example from Sect. 6.1. The introductory session took 30 minutes. Afterward, we asked them to explore DECE using the Pima Indian Diabetes Dataset for 20 minutes and collected feedback about their user experience, scenarios for using the system, and suggestions for improvements.</p><p>System Design. Overall, E1 and E2 suggested that the system is easy to use, while E3 had some confusion with the instance lens, where she mistakenly thought that the line in a non-focal cell encodes some range values. After shown with more concrete examples about how the non-focal cells and row-focal cells switch to each other, she got to understand. E1 mentioned that the color intensity of the shadow box in each subgroup shows cells in a very prominent way, suggesting a group of healthy patients have potential risks to have the disease.</p><p>Usage Scenarios. E1 suggested that the subgroup list in the table view helped her understand the potential risk level of her patients becoming diabetic, "so that I can decide if any medical intervention should be taken." However, both E2 and E3 suggested that the table view would be more useful in clinical research scenarios, such as understanding the clinical manifestations of multi-factorial diseases. "Doctors can then use the valid conclusions for diagnosis," E2 said. When asked whether they would recommend patients use the instance view to find suggestions by themselves, E2 and E3 showed concerns, saying that "the knowledge variance of patients can be very broad." Despite this concern, they mentioned that they would love to use DECE themselves to help patients find disease prevention suggestions.</p><p>Suggestions for Improvements. All three experts agreed that the instance view has a high potential for doctors and patients to apply it in clinical scenarios together, but some domain-specific problems must be considered. E1 suggested that we should allow users to set the value of some attributes as not applicable because "for the same disease, the test items can be different for patients." E2 commented that the histograms for each attribute did not help her much. She suggested showing the reference ranges for each attribute as another option.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION AND CONCLUSION</head><p>In this work, we introduced DECE, a counterfactual-based approach, to help model developers and model users explore and understand ML models' decisions. DECE supports iteratively probing the decision boundaries of classifiers by analyzing subgroup counterfactuals and generating actionable examples by interactively imposing constraints on possible feature variations. We demonstrated the effectiveness of DECE via three usage scenarios from different domains, including finance, education, and healthcare, and included an expert interview.</p><p>Scalability to Large Datasets. The current design of the table view can visualize nine subgroup rows or about one thousand instance rows (one instance requires one vertical pixel) on a laptop screen with 1920 × 1080 resolution. Users can collapse the features they are not interested in and inspect the details of a feature by increasing the column width. In most cases, about ten columns can be displayed in the table.</p><p>In addition to the three datasets used in Sect. 6, we also validated the efficiency of our algorithm on a large dataset, HELOC <ref type="bibr" target="#b6">[7]</ref> (N=10459). We used a 2.5GHz Intel Core i5 CPU (4 cores). Generating single CF examples for the entire dataset took around 16 seconds. We found that the optimization converges slower for datasets with many categorical features, such as the German Credit Dataset. To speed up the generation process, we envision future research to apply strategies from synthetic tabular data generation literature, such as smoothing categorical features <ref type="bibr" target="#b44">[44]</ref> and parallelism.</p><p>Generalizability to Other ML Models. The DECE prototype was developed for differentiable binary classifiers on tabular data. However, DECE can be extended for multiclass classification by selecting a target class manually or heuristically for each instance (e.g., the second most probable class predicted by the model). For non-differentiable models (e.g., decision trees) or models for unstructured data (e.g., image and text), generating good counterfactual explanations is an active research problem, and we expect to support this in the future.</p><p>R-counterfactuals and Exploratory Analysis. R-counterfactuals are flexible instruments to understand the subgroup-level behavior of an ML model. A general workflow for using it with DECE is as follows: 1) users start by specifying a subgroup of interest; 2) from the subgroup visualization, users can view the class and impurity distribution along with each feature; 3) users can then choose an interesting or salient feature and further refine the subgroup; 4) continue 2) and 3) until they get a comparably "pure" subgroup, which implies that the findings on this subgroup are salient and valid.</p><p>Improvements in the System Design. We expect to make further improvements to the system design in the future. In the table view, we plan to improve the design of subgroup cells for categorical features. One direction is to provide suggestions for an optimal selection of categories to refine the hypothesis. Besides, we plan to implement more interactions to support the "focus + context" display in the instance lens (e.g., focusing on multiple instances through brushing). In the instance view, we plan to provide users with different levels of interactions, e.g., allowing advanced users to directly manipulate the feature weights in the distance metric.</p><p>Effectiveness of Counterfactual Explanations. In this paper, we presented three usage scenarios to demonstrate how DECE can be used by different types of users. However, we only conducted expert interviews with medical students who can be regarded as decision-makers. To better understand the effectiveness of DECE and CFs, we plan to conduct user studies by recruiting model developers, data scientists, and layman users (for the instance view only). At the instance-level, we expect to understand how different factors (e.g., proximity, diversity, and sparsity) affect the users' satisfaction with the CFs. And it is interesting to know how well r-counterfactuals can support exploratory analysis for datasets in different domains at the subgroup-level.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>A simple exploratory analysis with r-counterfactuals. A. A hypothesis is proposed by selecting a subgroup. B. R-counterfactuals are generated against the subgroup. are instances within the subgroup, and are instances outside the subgroup. C. The hypothesis is refined to a new subgroup that excludes the previous CF examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>DECE consists of a Data Storage module, a CF Engine module, and a Visual Analysis module. In the Visual Analysis module, the table view and instance view together support an exploratory analysis workflow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Design choices for visualizing subgroup with r-counterfactuals. A-C. Our final choices. D. A matrix-based alternative design to visualize instance-cf connections. E. A density-based alternative design to visualize the distribution of the information gain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Understanding diabetes prediction on a young subgroup with a hypothesis refinement process. A. The initial hypothesis-"patients with Glucose &lt; 100 and age &lt; 40 are healthy"-is not valid as suggested by several CF examples found inside the range (A1). B. The hypothesis is refined by constraining BMI &lt; 35, and DPF &lt; 0.6, as suggested by the green peaks under the histograms (A2, A3). The hypothesis is still not plausible given the CF examples within the range (B1). After studying cell B1, all CF examples have an uncommonly high pregnancy value (B3). C. The user refines the hypothesis by limiting pregnancies &lt;= 6. All valid CF examples left in the subgroup have abnormal blood pressure (C1). D. After filtering out instances with an abnormal low (&lt; 40) blood pressure (D1), the final hypothesis is now fully supported by all CF examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>•</head><label></label><figDesc>Furui Cheng, and Huamin Qu are with Hong Kong University of Science and Technology. E-mail: {fchengaa, huamin}@ust.hk. • Yao Ming is with Bloomberg L.P. This work was done when he was at</figDesc><table /><note>Hong Kong University of Science and Technology. E-mail: yming7@bloomberg.net Manuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication xx xxx. 201x; date of current version xx xxx. 201x. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org. Digital Object Identifier: xx.xxxx/TVCG.201x.xxxxxxx</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank Dr. Panpan Xu for her insightful discussions on the subgroup-level analysis method and the system's design and for her support and assistance in writing.</p><p>This research was supported in part by Hong Kong Theme-based Research Scheme grant T41-709/17N and also a grant from MSRA.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A comparison of regression models for prediction of graduate admissions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Armaan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Antony</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Computational Intelligence in Data Science (ICCIDS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fairsight: Visual analytics for fairness in decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1086" to="1095" />
			<date type="published" when="2020-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Do convolutional neural networks learn class hierarchy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alsallakh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="152" to="162" />
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">it&apos;s reducing a human being to a percentage&apos; perceptions of justice in algorithmic decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Binns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Kleek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Veale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Lyngs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shadbolt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2018 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Classification and regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Olshen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Techniques for interpretable machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="68" to="77" />
			<date type="published" when="2019-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Explainable machine learning challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fico</surname></persName>
		</author>
		<ptr target="https://community.fico.com/s/explainable-machine-learning-challenge" />
		<imprint>
			<date type="published" when="2018-01-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Prince: Provider-side interpretability with counterfactual explanations in recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghazimatin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Balalau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Saha</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Web Search and Data Mining, WSDM &apos;20</title>
		<meeting>the 13th International Conference on Web Search and Data Mining, WSDM &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="196" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Statlog (german credit data) data set. UCI Repository of Machine Learning Databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hofmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gamut: A design probe to understand how data scientists understand machine learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Head</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Deline</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Drucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2019 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Visual analytics in deep learning: An interrogative survey for the next frontiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pienta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2674" to="2693" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">S ummit: Scaling deep learning interpretability by visualizing activation and attribution summarizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H P</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1096" to="1106" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Activis: Visual exploration of industry-scale deep neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kalro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H P</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Model-agnostic counterfactual explanations for consequential decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-H</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Barthe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Balle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Valera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="895" to="905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A workflow for visual diagnostics of binary classifiers using instance-level explanations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Swartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aphinyanaphongs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bertini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual Analytics Science and Technology (VAST), IEEE Conference on</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Interacting with predictions: Visual inspection of black-box machine learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2016 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5686" to="5697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Counterfactuals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Analyzing the training processes of deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="87" />
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Towards better analysis of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="100" />
			<date type="published" when="2017-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards better analysis of machine learning models: A visual analytics perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Informatics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="48" to="56" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Actionable interpretability through optimizable counterfactual explanations for tree ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Oosterhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Haned</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.12199</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Explainable reinforcement learning through a causal lens</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Madumal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sonenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vetere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Fourth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2493" to="2500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Explanation in artificial intelligence: Insights from the social sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">267</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Understanding hidden memories of recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Visual Analytics Science and Technology (VAST)</title>
		<meeting>IEEE Conference on Visual Analytics Science and Technology (VAST)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bertin</surname></persName>
		</author>
		<title level="m">RuleMatrix: Visualizing and understanding classifiers with rules. IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2019-01" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="342" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Explaining machine learning classifiers through diverse counterfactual explanations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Mothilal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2020 Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="607" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The table lens: merging graphical and symbolic representations in an interactive focus+ context visualization for tabular information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Card</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI conference on Human factors in computing systems</title>
		<meeting>the SIGCHI conference on Human factors in computing systems</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="318" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Why should I trust you?&quot;: Explaining the predictions of any classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Explaining explanation. Routledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-H</forename><surname>Ruben</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="206" to="215" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient search for diverse coherent explanations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="20" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Explanation by progressive exaggeration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pollack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Using the adap learning algorithm to forecast the onset of diabetes mellitus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Everhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Knowler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Johannes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Symposium on Computer Application in Medical Care</title>
		<meeting>the Annual Symposium on Computer Application in Medical Care</meeting>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="page">261</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">explainer: A visual analytics framework for interactive and explainable machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Spinner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schlegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Assady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1064" to="1074" />
			<date type="published" when="2020-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Seq2seq-vis: A visual debugging tool for sequence-to-sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Behrisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="353" to="363" />
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">LSTMVis: A tool for visual analysis of hidden state dynamics in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="667" to="676" />
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Exploratory data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Tukey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977" />
			<biblScope unit="volume">2</biblScope>
			<pubPlace>Reading, Mass</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Actionable recourse in linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ustun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Spangher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Counterfactual explanations without opening the black box: Automated decisions and the gdpr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wachter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mittelstadt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Harv. JL &amp; Tech</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">841</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dqnviz: A visual analytics approach to understand deep q-networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="288" to="298" />
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Ganviz: A visual analytics approach to understand the adversarial game</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1905" to="1917" />
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The what-if tool: Interactive probing of machine learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pushkarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolukbasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="56" to="65" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Improved heterogeneous distance functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Veeramachaneni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11264</idno>
		<title level="m">Synthesizing tabular data using generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
