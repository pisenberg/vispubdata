<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VATLD: A V isual Analytics System to Assess, Understand and Improve T raffic Light Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Gou</surname></persName>
							<email>liang.gou@us.bosch.com</email>
							<affiliation key="aff0">
								<orgName type="department">Robert Bosch Research and Technology Center</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lincan</forename><surname>Zou</surname></persName>
							<email>lincan.zou@us.bosch.com</email>
							<affiliation key="aff0">
								<orgName type="department">Robert Bosch Research and Technology Center</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxiang</forename><surname>Li</surname></persName>
							<email>nanxiang.li@us.bosch.com</email>
							<affiliation key="aff0">
								<orgName type="department">Robert Bosch Research and Technology Center</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hofmann</surname></persName>
							<email>michael.hofmann11@de.bosch.com</email>
							<affiliation key="aff0">
								<orgName type="department">Robert Bosch Research and Technology Center</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Kumar Shekar</surname></persName>
							<email>arvindkumar.shekar@de.bosch.com</email>
							<affiliation key="aff0">
								<orgName type="department">Robert Bosch Research and Technology Center</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Wendt</surname></persName>
							<email>axel.wendt@de.bosch.com</email>
							<affiliation key="aff0">
								<orgName type="department">Robert Bosch Research and Technology Center</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Ren</surname></persName>
							<email>liu.ren@us.bosch.com</email>
							<affiliation key="aff0">
								<orgName type="department">Robert Bosch Research and Technology Center</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><forename type="middle">• Kumar</forename><surname>{michael Hofmann</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Robert Bosch GmbH</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shekar</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Robert Bosch GmbH</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wendt</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Robert Bosch GmbH</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">VATLD: A V isual Analytics System to Assess, Understand and Improve T raffic Light Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">received xx xxx. 201x; accepted xx xxx. 201x.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Traffic light detection</term>
					<term>representation learning</term>
					<term>semantic adversarial learning</term>
					<term>model diagnosing</term>
					<term>autonomous driving</term>
				</keywords>
			</textClass>
			<abstract>
				<p>S0 Fig. 1. The VATLD user interface: (a) Summary and navigation of key performance statistics; (b) Visual landscapes of traffic lights (upper) and performance scores (lower) over the first two PCA components of semantic dimensions; (c) A live view to detect a traffic light from either real data or adversarial; (d) Ranked latent dimensions with information of semantics, performance and gradients.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Traffic light detection is an essential component in autonomous driving. It helps autonomous cars perceive driving environments by locating relevant traffic lights and also supports cars to make right decisions by recognizing the status of lights. The state-of-the-art traffic light detectors largely rely on deep Convolutional Neural Networks (CNNs) that have exhibited superior performance in many computer vision tasks such as image classification, object detection, semantic segmentation and so on. These detectors are usually trained upon general purpose object detectors (such as SSD <ref type="bibr" target="#b28">[29]</ref>, YOLO <ref type="bibr" target="#b34">[35]</ref> and Faster R-CNN <ref type="bibr" target="#b35">[36]</ref> locating and recognizing animals, person, vehicles, etc <ref type="bibr" target="#b25">[26]</ref>) and then fine tuned with domain-specific data (driving scenes with traffic lights) <ref type="bibr" target="#b2">[3]</ref> or combined with other prior knowledge about driving scenes, such as object distribution in a scene <ref type="bibr" target="#b32">[33]</ref>.</p><p>Despite the promising results of CNN based detectors, one concern is how to thoroughly assess, understand and further improve detector performance before they can be deployed to autonomous vehicles. The concern is two-fold: a) accuracy evaluation and improvement over massive acquired data (training and testing data); b) robustness evaluation and improvement over unseen data <ref type="bibr">(potential vulnerability)</ref>.</p><p>Firstly, it is a non-trivial task to assess model accuracy and understand when and why detectors tend to fail. The current evaluation and benchmark methods of model accuracy heavily rely on ag-gregated and over-simplified metrics, such as mAP (mean Average Precision) <ref type="bibr" target="#b25">[26]</ref>, and fail to provide interpretable and contextual information to understand model performance. Furthermore, although rising attention has been paid to the explainability of general CNNs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref> , the method of unveiling how CNN based detectors perform still lack of investigation. During a model building phase, interactive tools are desirable to support model assessing, understanding and improvement instead of a single accuracy number.</p><p>Another burning need is to identify a detector's potential vulnerability, and then assess and improve the robustness over potential vulnerable cases. Recently, the advance of adversarial attack and robustness research bears much potential to reveal the vulnerability in DNN (Deep Neural Networks) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b40">41]</ref>. Adversarial machine learning fools a classifier with small perturbation of inputs with the gradient information obtained from DNNs.</p><p>However, two challenges are on the horizon by applying current adversarial attack methods to understand, evaluate, and improve the robustness of detectors. First, most adversarial attack methods do not generate examples with meaningful changes. These methods aim to fool target models by adding imperceivable noises <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b40">41]</ref>, and therefore these noises do not provide physical meanings or semantics for human to understand model robustness, and also provide little guidance to improve robustness in the physical world. Also, the mechanism of understanding adversarial landscape and improving robustness of a model is desirable. For example, with current methods, we do not know what the common patterns of learned adversarial examples are, why they exist, and how to improve.</p><p>All told, to asses, understand and improve the performance of traffic light detectors, we need to overcome two hurdles of dissecting model accuracy over existing data, and also assessing and improving model robustness over unseen cases.</p><p>Aiming at above challenges, we propose a visual analytics system, VATLD, to assess, understand, and improve the accuracy and robustness of traffic light detectors in autonomous driving applications. Specifically, in this work, our contributions include:</p><p>• offering a novel visual analytics method to asses, understand and improve a detector's performance of both accuracy and robustness, guided by a semantic representation learning and a minimal human-in-the-loop approach. • adapting a representation learning method to efficiently summarize, navigate and diagnose the performance of detectors over large amounts of data. This method extracts low-dimensional representation (i.e. latent space) with disentangled intrinsic attributes (such as color, darkness, background etc.) of traffic lights and serves a fundamental representation for both human-friendly visualization and semantic adversarial learning. • proposing a new semantic adversarial learning algorithm to efficiently reveal the robustness issues of detectors without knowing detector parameters. The adversarial learning efficiently generates unseen test cases by searching the learned latent space, and the adversarial results are also interpretable with meaningful perturbation in the latent space. • demonstrating the effectiveness of model improvement strategies guided by visual analytics for real world problems, and illustrating a promising way of injecting human intelligence into models with few human interactions.</p><p>In sum, we present, VATLD, as the first research effort using a visual analytics approach to alleviate some trustworthy AI issues in autonomous driving applications. We hope this work can raise some discussion and open research opportunities for applying visual analytics and humanin-the-loop approaches to the domain of autonomous driving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Object Detection and Traffic Light Detection</head><p>Object detection is a fundamental computer vision task to locate and recognize the instances of visual objects from a certain class (such as humans, animals, vehicles, or others) in images. Riding the recent wave of deep learning, generic object detectors with state-of-the-art performance are mostly built upon powerful convolutional neural networks (CNN). CNN based detectors could be roughly categorized into two groups: two-stage and one-stage detectors. Two-stage detectors, such as R-CNN ("Region-based CNN") based detectors <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b35">36]</ref>, first propose a set of regions of interests (bounding boxes) by a searching method or a regional proposal network, and then use a classifier to recognize these regions. By contrast, one-stage detectors directly search and classify possible locations over one feature map (e.g. YOLO <ref type="bibr" target="#b34">[35]</ref>) or multi-scale feature maps (e.g. SSD <ref type="bibr" target="#b28">[29]</ref>, RetinaNet <ref type="bibr" target="#b24">[25]</ref>). Although these generic object detectors have achieved promising performance over multiple data-sets (e.g. coco dataset <ref type="bibr" target="#b25">[26]</ref>), specialized detectors are desired for various perception tasks in autonomous driving applications, such as detection for traffic lights, lanes, and pedestrians. Traffic light detector is a specialized detector locating traffic lights and recognizing their status (such as red, green and yellow). These detectors are usually built upon CNN based detectors and fine tuned with domain-specific data (driving scenes with traffic lights) <ref type="bibr" target="#b2">[3]</ref>, or combined prior knowledge about driving scenes, such as object distribution <ref type="bibr" target="#b32">[33]</ref>, and map information <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Assessing and Interpreting CNN based Detectors</head><p>A plethora of work has been proposed to assess and interpret generic CNN models in both machine learning and visual analytics communities. These works include model-specific and model-agnostic approaches. Model-specific methods attempt to reveal multiple levels of details learned in CNNs, such as neuron activation and filters <ref type="bibr" target="#b26">[27]</ref>, feature maps <ref type="bibr" target="#b40">[41]</ref>, concepts and feature attributions <ref type="bibr" target="#b1">[2]</ref>, while modelagnostic approaches employ intrinsic explainable models, such as decision trees, decision rules <ref type="bibr" target="#b30">[31]</ref>, linear models <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b42">43]</ref>, to mimic the prediction behaviors of CNNs. Surveys from both machine learning <ref type="bibr" target="#b38">[39]</ref> and visual analytics <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b46">47]</ref> offer more insights into this.</p><p>However, assessing and improving the performance of CNN based detectors, especially for traffic lights, is of scarce. Hoiem et al. <ref type="bibr" target="#b17">[18]</ref> conduct error analysis for generic object detectors by examining the influences of object characteristics (e.g. occlusion, size, aspect ratio, viewpoint etc.) on detection performance. However, these object characteristics are handcrafted and hard to scale. Vondrick et al. <ref type="bibr" target="#b41">[42]</ref> present a method to visualize detector features as natural images, but focusing on traditional HOG (Histogram of Oriented Gradients) detectors (non-CNN detectors). In short, these approaches lack of an intrinsic representation of objects to augment human's cognition for understanding and diagnosing model performance. Moreover, current methods are incapable of revealing and assessing the potential weakness of detectors to ensure the level of confidence that autonomous driving requires.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Adversarial Robustness</head><p>To tackle down the issue of potential weakness for CNN based detectors, recent developments in adversarial robustness research pointed some promising directions. The seminal work on adversarial attack <ref type="bibr" target="#b40">[41]</ref> offers us an efficient way to fail a classifier with minimal perturbation of inputs by following the ascending gradient direction of a model. According to the way of obtaining gradient information, adversarial learning methods can be grouped into white-box attacks <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b40">41]</ref>, using model parameters to calculate gradients, and black-box attacks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24]</ref>, with limited model queries to estimate gradients. In both approaches, model robustness can be measured by the minimal perturbation of the image pixels needed to fail the model. However, most adversarial attack methods do not generate examples with semantic meanings and limit their capability to generate actionable guidance for the model improvement over potential risks in the real world. Some initial research <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23]</ref> is conducted on the attacks in the physical world but it is hard to scale because of the scarce of available data with physical perturbation. Also, we lack of approaches for understanding and interpreting the semantics of adversarial examples. For example, it is not clear what the common patterns of the learned adversarial examples are and if these patterns are explainable or helpful to generate actionable insights in practice. In summary, to assess, understand and improve the performance of traffic light detectors, we aim to bridge the research gaps of dissecting the model accuracy over existing data, and also assessing and improving model robustness over unseen vulnerable cases in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BACKGROUND AND MOTIVATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">How a deep CNN based detector works</head><p>In this work, we use a Single Shot multiBox Detector (SSD) <ref type="bibr" target="#b28">[29]</ref> as an example because of its fast detection and high accuracy. With an image of driving scene as input, a SSD has two tasks: locating possible objects (traffic lights in this case) with a set of bounding boxes ({b i } : (Δc x , Δc y , w, h), Δc x , Δc y as the offsets to center coordinates, and w, h as box width and height), and recognizing object categories (red, green, yellow, off, and non-object/background) with confidence scores ({c i }, i as a category) for each predicted box, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>A SSD first passes an image thorough a back-bone CNN, such as ResNet <ref type="bibr" target="#b16">[17]</ref> and MobileNet <ref type="bibr" target="#b19">[20]</ref>, to extract base features (e.g. 38 × 38 with 512 channels in <ref type="figure" target="#fig_0">Fig. 2</ref>). These features are then converted into smaller size feature maps at different scales. For each cell on a feature map (e.g., a 3 × 3 with depth 256 has 3 × 3 = 9 cells), it makes k predictions of boxes with various aspect ratios, and p class scores for each box with a fixed 3 × 3 convolutional predictors. This prediction is applied to all feature maps at multiple scales to detect objects with different sizes. Finally, predictions with overlapping boxes are filtered by a non-maximum suppression method, and the outputs are ranked by maximal confidence scores of non-background classes.</p><p>To simplify the discussion in the paper, let's consider detector recognition to be a binary classification of traffic-light and non-traffic light (background), c(x) = c tra f f iclight . However, this is not a general limitation as the proposed method is model-agnostic and can thus be applied to any object detection network with multiple classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Metrics for Detector Accuracy</head><p>Average Precision (AP) is a well-adopted metric to evaluate detector accuracy by considering both localization and classification tasks <ref type="bibr" target="#b25">[26]</ref>. This metric is built up three metrics of IoU, precision and recall.</p><p>IoU (Intersection over Union) measures the overlapping between a ground truth box (GT, green) and a detection box (DT, red), as shown in <ref type="figure" target="#fig_1">Fig. 3-b</ref>. By setting a threshold of IoU (usually 0.5), we can decide if a detection is a True Positive (T P with IoU ≥ 0.5, i.e., correct detection) or a False Positive (FP with IoU &lt; 0.5, i.e., wrong detection), <ref type="figure" target="#fig_1">(Fig. 3-c)</ref>. If a ground truth does not have any detection, this is a False Negative (FN, i.e., missed detection). In detection application, True Negatives, namely no detection for background, are usually irrelevant for evaluation. <ref type="figure" target="#fig_1">Fig. 3</ref>-a shows an example. The given image has three ground truth boxes (GT i ) and a detector yields four detections (DT j ) with confidence scores larger than 0.5. With a IoU threshold of 0.5, we can know DT 1 and DT 2 are T Ps, and DT 3 and DT 4 are FPs. GT 3 has never been detected, namely FN.</p><p>Then, we can compute precision and recall: precision = T P/(T P + FP), recall = T P/(T P + FN). Finally, Average Precision (AP) is defined by the area under the precision-recall curve. For an IoU threshold of 0.5, it is written as AP@50. If averaging all AP values over different IoU thresholds or categories, we can obtain a mAP (mean AP). However, in practice, it is by far not enough to have a single aggregated metric, such as AP and mAP, to assess, understand and improve a detector.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Design Requirements</head><p>We have outlined two challenges on both accuracy and robustness to assess, understand and improve traffic light detectors. Targeting the two challenges, we distill two categories of design requirements for our visual analytic approach from the aspects of data and performance during several design iterations (see <ref type="table" target="#tab_0">Table 1</ref>).</p><p>On the data side, for both existing data and unseen data, we need:</p><p>• human-friendly data representation and summarization (RD1). This issue arises from the nature of high dimensionality and sheer volume of images. We need a representation to capture the intrinsic attributes of images in a lower dimension space and then summarize them in a human-friendly way <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. • efficient generation of unseen test cases (RD2). We seek for a method generating edge cases to probe model robustness. These test cases should be different from the imperceivable noises that learned from traditional adversarial approaches, and have semantic meanings to guide human to improve the robustness <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23]</ref>. For the performance of both accuracy and robustness, we require: • a contextualized understanding for model performance ( RP1 ). Instead of using an aggregated metric to evaluate models <ref type="bibr" target="#b17">[18]</ref>, we would like to put a single score into the contexts of various sizes, IoU thresholds and confident score ranges. • performance interpretation for both accuracy and robustness ( RP2 ). It is challenging to understand how the accuracy and robustness of detectors are impacted by different semantic characteristics of data <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b38">39]</ref>, including colors of traffic lights (red, green, yellow, off), illumination settings (sun glare or darkness in the tree), distances (large or small size object in the scene), or confusing background (similar but irrelevant objects). • injecting human intelligence for performance improvement with minimal human interaction ( RP3 ). The ultimate goal of model evaluation and interpretation is to improve its performance with human knowledge in the loop <ref type="bibr" target="#b11">[12]</ref>. Meanwhile, as we are working with domain experts, we found they have limited bandwidth to conduct in-depth exploration, but focus on key insights with few interactions <ref type="bibr" target="#b7">[8]</ref>. This calls for maximazing insight generation and injection with minimal interaction. The two aspects are inherently related: accuracy understanding relies on existing data (training/validation/testing data), and robustness evaluation counts on unseen data. In this work, we strive for a unified visual analytic approach to alleviate these issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THE FRAMEWORK OF VATLD 4.1 Framework Overview</head><p>We propose a framework, shown in <ref type="figure" target="#fig_2">Fig. 4</ref>, to support domain experts assess, understand and improve the performance of traffic light detectors.  The framework consists of four modules, including a data processing, b data representation and adversarial learning, c interactive visualization, and d VA (visual analytics)-assisted improvements. The framework starts with acquired data, including training and testing data. Suppose training data is large enough to cover most cases of traffic lights a detector may encounter, such as different colors (red, green, yellow), brightness, and symbols (circle or arrows). Let's consider these cases as "in-distribution", as shown in <ref type="figure" target="#fig_2">Fig. 4</ref>-a1 (Here, we only focus on the objects of traffic lights, not the whole scene images, because most detectors, as well as SSD, only look for local features to locate and recognize objects). However, in testing data, there may be some traffic lights with "shifted distribution" causing false or missing detection, such as brightness variations, shown in <ref type="figure" target="#fig_2">Fig. 4</ref>-a2 . In this work, we need to identify and understand these distribution shifts to diagnose detectors.</p><p>Meanwhile, there are still some unseen cases coming "out of distribution" <ref type="figure" target="#fig_1">(Fig. 4-a3</ref> ) that may fail a detector, and we need to generate and test them efficiently. These unseen cases may be associated with various factors, such as lighting, camera condition, and weather changes.</p><p>In the second module, two core learning components, disentangled representation learning and semantic adversarial learning, are introduced to augment our analysis and meet the requirements of RD1 and RD2 . Disentangled representation learning extracts intrinsic and interpretable attributes of traffic lights, such as colors, brightness, and background ( <ref type="figure" target="#fig_2">Fig. 4-b1</ref> ). This component first provides a human-friendly data presentation, and also offers a data space where an adversarial generation can efficiently search. Semantic adversarial learning learns prediction behaviors of a detector, and generates meaningful adversarial examples ( <ref type="figure" target="#fig_0">Fig. 4-b2</ref> ) on top of disentangled representation learning. After this, both acquired data <ref type="figure" target="#fig_1">(Fig. 4-b3</ref> ) and unseen data ( <ref type="figure" target="#fig_2">Fig. 4-b4</ref> ) are passed to the detector to obtain detection results.</p><p>The third module transforms detection results, as well as the metadata (object size, disentangled representation, gradients etc), into interactive and human-friendly visualizations <ref type="figure" target="#fig_1">(Fig. 4 -3a)</ref>, including multifaceted performance analysis and adversarial summarization and interpretation. The visualizations are designed to address performance analysis and interpretation requirements ( RP1 and RP2 ) for both accuracy over acquired data and robustness over unseen data.</p><p>Finally, with minimal human interaction from visual interface, actionable insights are derived to generate more data that attempt to "lift distribution up" via data augmentation ( <ref type="figure" target="#fig_2">Fig. 4-d</ref> ). This also enables us inject human intelligence to improve model accuracy and robustness, aiming at the requirement RP3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Disentangled Representation Learning (DRL)</head><p>Disentangled Representation Learning (DRL) is introduced to extract semantic latent representation of traffic lights (e.g. colors, background, rotation, etc.), shown in <ref type="figure" target="#fig_3">Fig.5</ref>-a and generate more data with controllable semantics for data augmentation. The semantic latent representation also serves as a cornerstone for both human-friendly data summarization (RD1) and semantic adversarial learning (RD2).</p><formula xml:id="formula_0">. o i Pretrained CNN Classifier Reconstruction Loss Perceptual Loss Prediction Loss y i (a) o′ i . y i Gradient Estimation o i z i z i Encoder e ϕ Decoder d θ o i x j x x x x x x x x x x x x x x x x j j j j j j j j j j j j j j j x x … ∇ z′ i = z i + ε i ∇ = Δf Δz Latent Loss (b)</formula><p>We adapted a state-of-the-art β -VAE <ref type="bibr" target="#b6">[7]</ref> with customized regularization of losses. Given an object image (o i ∈ O N , a traffic light with size N×N), β -VAE includes two components: an encoder, e φ , mapping an input (o i ) into a latent vector (z i ∈ Z D , D: latent dimension size), namely e φ : o i → z i , and a decoder (d θ ) converting a latent vector (z i ) into a reconstructed image (õ i ), namely</p><formula xml:id="formula_1">d θ : z i →õ i .</formula><p>In the vanilla β -VAE, it uses two losses to optimize the model including reconstruction loss (usually, a mean square error:</p><formula xml:id="formula_2">MSE = o i −õ i 2 )</formula><p>and latent loss (KL divergence:</p><formula xml:id="formula_3">D KL = D KL ((z|x) (z))).</formula><p>Here, we introduce two more regularization terms of prediction loss and perceptual loss <ref type="bibr" target="#b18">[19]</ref> to generate realistic images. We first train a CNN based classifier to predict traffic light colors. Then, we employ the pre-trained CNN classifier to predict the color (ỹ i ) of a reconstructed imageõ i , and also extract the feature maps from ConvNet layers (φ l (x) from the lth ConvNet layer) of both original (o i ) and reconstructed images (õ i ). Finally, we can obtain prediction loss (Cross-Entropy loss: CE(y i ,ỹ i )) and perceptual loss <ref type="bibr" target="#b18">[19]</ref> of feature maps between original and reconstructed images (</p><formula xml:id="formula_4">Σ L φ l (o i ) − φ l (õ i ) 2 ).</formula><p>The final loss term is the sum of all losses introduced above:</p><formula xml:id="formula_5">L = MSE(o i ,õ i ) + γ|D KL − C| + μCE(y i ,ỹ i ) + νΣ L φ l (o i ) − φ l (õ i ) 2 ,</formula><p>where γ,C are parameters to control disentanglement, and μ, ν are weights to control reconstruction quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Semantic Adversarial Learning</head><p>Built upon DRL, we propose a novel semantic adversarial learning (SemAdv) method to generate meaningful adversarial examples to test and interpret the robustness of a detector ( <ref type="figure" target="#fig_3">Fig.5-b</ref>). Two unique requirements should be met: a). Adversarial examples need to be meaningful (not imperceivable noise) and can guide us improve model robustness; b). The method should be applied to any detector without accessing model parameters, namely a model-agnostic approach.</p><p>The idea is that, given a traffic light object (o i ) from a driving scene (x j ) and a black-box detector ( f ), SemAdv needs to generate an adversarial object image, o i , to fool the detector, namely, f (o i→ x j ) &lt; 0.5. Here, f (o i→ x j ) is the detection score of traffic light o i re-inserted in the original scene x j . For detection results, P, with bounding boxes, {b} p , and confidence scores, {c} p , the f (o i→ x j ) is defined as:</p><formula xml:id="formula_6">f (o i→ x j ) = max{c d } if IoU(b i , b d ) ≥ 0.5, ∀d ∈ P 0 i fIoU(b i , b d ) &lt; 0.5, ∀d ∈ P<label>(1)</label></formula><p>The core step is to searching the object's latent space, Z D , to find a z i = z i + ε i with a minimal change to generate the adversarial o i . The algorithm is shown in algorithm 1.</p><p>We adapt a Fast Gradient Sign Attack (FGSM) based approach <ref type="bibr" target="#b15">[16]</ref> to obtain the minimal shift ε (Line 8-15 in algorithm 1). (FGSM is computationally efficient compared to other strong attack methods, e.g. PGD <ref type="bibr" target="#b29">[30]</ref>, by considering the high cost of gradient estimation needed.) The main idea is to obtain the gradient of a detector, ∇, and then iteratively push the latent vector, z i , to move along the ascending direction of the gradient, z t+1 i = z t i − ∇, until the reconstructed object fails the detector in the scene by f (o i→ x j ) &lt; 0.5.</p><p>To obtain the gradient without knowing detector parameters, we use a black-box gradient estimation approach by Natural Evolution Strategies (NES) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b45">46]</ref> (Line 2-7 in algorithm 1). Here, the gradient is approximated by the amount of output changes, Δ f , caused by input changes, Δz, namely, ∇ = Δ f /Δz. Finally, we constraint the change for each latent dimension smaller than a budget λ , namely ε i,d &lt; λ , ∀d ∈ Z D , to make sure a reconstructed image not goes unrealistic. </p><formula xml:id="formula_7">i = e φ (o i ) // Estimate gradient for z i 4 Sample σ 1 ,...,σ k ∼ N (0, I) for K times 5 Get perturbation: z ik = z i + δ σ k , ∀k 6 Generate perturbed objects:õ ik = d θ (z ik ), ∀k 7 Detection scores: f k = f (õ ik→ x j ), ∀k 8 Normalized scores:f k = ( f k − mean( f ))/std( f ), ∀k 9 Gradient estimation: ∇ = 1 Kδ Σ K k=1 σ kfk // Update z i by T steps 10 for t ← 0 to T do 11 z t+1 i = z t i − η∇ 12</formula><p>Constrain by budget:</p><formula xml:id="formula_8">z t+1 i = clip(z t+1 i , λ ) 13</formula><p>Generate a new object:</p><formula xml:id="formula_9">õ t+1 i = d θ (z t+1 i ) 14 if f (õ t+1 i→ x j ) &lt; 0.5 then // found an adversarial o i 15 o i =õ t+1 i 16</formula><p>Compute the change:</p><formula xml:id="formula_10">ε i = z t+1 i − z 0 i 17 return o i , ε i</formula><p>// Failed within T steps and modification budget λ</p><p>18 Set the change as the max budget:</p><formula xml:id="formula_11">ε i = λ 19 return o i , ε i Semantic Robustness.</formula><p>With SemAdv method, we can define a semantic robustness score of a detector by measuring how much minimal change in the latent space we need to make to fail the detector. For a specific object, o i , the semantic robustness is the average L1-norm of normalized minimal change in the latent space:</p><formula xml:id="formula_12">sr i = εi/std(Z) L1 λ D</formula><p>, where ε i /std(z) means that each dimension of ε i is normalized by the standard deviation of each dimension, * L1 is the L1-norm, λ is the modification budget and D is the dimension size. The semantic robustness of a detector, sr( f ), is defined as the average semantic robustness for all objects, O, in a dataset:</p><formula xml:id="formula_13">sr( f ) = Σ O i=1 sr(o i )/|O|, ∀i ∈ O.</formula><p>This semantic robustness is different from traditional adversarial robustness because it is meaningful and interpretable. We can intuitively know how much effort we need to change on each dimension with visual semantics, such as color, lightning, background, to fail a detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Data Extraction</head><p>We extract three types of data, including top-n detections, ground truth objects, and semantic adversarial objects, for analysis. We summarize the structure of data extracted for visual analytics in <ref type="table" target="#tab_2">Table 2</ref>. For each type of data, we collect performance information (bounding boxes, confidence sores, IoUs), data representation (image patches, latent vectors) and other meta-information (size, class, gradient, robustness). For unseen adversarial data, we only learn adversarials for the corresponding ground truth objects that have been successfully detected, namely f (o i→ x j ) &gt; 0.5 (Line 8-15 in algorithm 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">THE USER INTERFACE OF VATLD</head><p>Guided by above framework, we design and implement a visual analytics system, VATLD, to assess, understand and improve traffic light detectors. The system user interface consists of four views <ref type="figure" target="#fig_4">(Fig. 1)</ref>: a Multi-faceted Summary, b Performance Landscape-TileScape, c Driving Scene View, and d Semantic Representation View-hPCP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Multi-faceted Summary</head><p>This view offers a summary and navigation for key performance statistics ( <ref type="figure" target="#fig_4">Fig. 1-a )</ref>. The top of <ref type="figure" target="#fig_4">Fig. 1-a</ref> , summarizes the total number of traffic light objects in a dataset, the number of top detections, false positive (FP), false negative (including both never detected lights, FN-I, and confusing lights with low confidence scores, FN-II) and total adversarial. Also, bar charts in <ref type="figure" target="#fig_4">Fig. 1-a ,</ref> show the distribution of performance metrics (including IoU, confidence score, and robustness for valid detection) and meta-info (object size).</p><p>Furthermore, the metric charts are coordinated with each other to filter data and support multi-faceted performance analysis for accuracy and robustness in other views. This is designed to support contextualized understanding of performance, RP1 . For example, FP, FN-I and FN-II detections can be selected by setting different filtering conditions: FPs are detections with zero size (no ground truth), IoU larger than a threshold of 0.5, and confidence score larger than 0.5, namely, FP = {DT i |s i = 0 ∧ IoU i &gt; 0.5 ∧ c i &gt; 0.5}. This view also enables a user to quickly navigate to FP or FN visual summary by clicking the icons in the dashboard to apply pre-defined filtering conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance Landscape View: TileScape</head><p>We design a view of performance landscape, TileScape, to summarize visual characteristics and corresponding performance over tens of thousands of objects (design requirement RP2 ), shown in the panel b of <ref type="figure" target="#fig_4">Fig. 1</ref>. In TileScape, each cell (tile) is an aggregated bin of many detections which are similar in the semantic latent space learned from DRL <ref type="figure" target="#fig_5">(Fig. 6-a )</ref>. Each data-point is a detection, located in the latent space by its latent vector encoded from a detected image patch. Then, the data-points are aggregated into bins, called as tiles, according to a view range and bin size. Also, one object is selected as the representative one for each tile. Here, we use the object with median score from a bin. Additionally, in the background of TileScape, a contour density map shows the data distribution in the current space.</p><p>Each tile visually encodes one of four categories of object information: visual appearance (image patch), confidence score, robustness score and adversarial gradient direction. As shown in the upper part of <ref type="figure" target="#fig_4">Fig. 1b</ref> , each tile shows a detected image patch: either a ground truth (GT) object image when a detection (DT) is matched, or the image patch cropped from a driving scene by a DT bounding box when no matched GT object is found for the current DT. The tile can also encode detection accuracy and robustness scores with colored rectangles, shown in the lower part of <ref type="figure" target="#fig_4">Fig. 1-b</ref> . Users can change visual encoding for tiles via a drop-down menu in the panel.</p><p>Along the two axes of TileScape, we also design an aggregated image bars to show the data distribution along each axis, <ref type="figure" target="#fig_5">Fig. 6-b</ref> . The data is also binned and aggregated with the same approach described above for each axis. Each bar consists of a representative image, a rectangle and an arrow glyph with similar visual encoding: bar height indicates the number of data-points, bar color encodes the median score of either confidence or robustness, and an arrow shows gradient direction.</p><p>Users can zoom and pan TileScape, to observe the patterns at different levels of aggregation. Also, users can interactively select any latent dimensions or the PCA components of latent dimensions to lay out the TileScape view. By default, the first two PCAs of latent dimensions are used to show overall visual and performance patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Semantic Representation View: hPCP</head><p>To efficiently explore the semantic representation space, a hierarchical parallel coordinates plot, (hPCP), is designed as shown in <ref type="figure" target="#fig_4">Fig. 1-d</ref> . First, latent dimensions are hierarchically clustered with an agglomerative method to efficiently organize and navigate these dimensions ( <ref type="figure" target="#fig_5">Fig. 6-c )</ref>. The clusters are formed by a ward linkage <ref type="bibr" target="#b44">[45]</ref> that minimizes the variance of Euclidean distance among all latent object vectors within a cluster. Only top dimensions can be visible by applying a distance threshold. More dimensions can be shown by expanding subtrees. The first two PCA components of latent dimensions are also included to capture the dominate variance of all dimensions. They are organized as a special subtree in the root node.</p><p>An aggregated image bar plot is also used for each dimension to show the semantics captured by this dimension, shown in <ref type="figure" target="#fig_5">Fig. 6b</ref> . We first bin and aggregate the data over each dimension, and then use the same visual encoding for data distribution, performance (confidence and robustness) scores and gradient directions as in TileScape. The shown image here is a blending of five random images. Additionally, a gray area plot is introduced as background to compare data distribution with the forefront bar chart. We can use this to compare data distribution of testing and training data over each dimension.</p><p>A parallel coordinates plot is then applied upon the top dimensions. By connecting data coordinates across all visible dimensions with curved lines, it shows dimension correlation and data clusters. To reduce visual clutter, we do not show the lines of all data-points but only these of our interest via users' interaction. For example, hovering a bin on a dimension will show lines for the dat points in this bin.</p><p>Semantic interpretation by coordinating with TileScape. Two mechanisms are introduced to help users understand dimension semantics and also interpret their impact over performance ( RP3 ). TileScape → hPCP. Interaction from TileScape to hPCP enables users to interpret detector's performance with the semantics learned by latent dimensions. Users can use a lasso to select a group of bins of interest (e.g. low confidence scores) in TileScape, and corresponding lines and bins are highlighted in the latent space of hPCP.</p><p>A Rank-To-Interpret method is proposed to rank dimensions by their importance to separate the selection from other data points ( <ref type="figure" target="#fig_5">Fig. 6d )</ref>. The selected and unselected data are first marked with different labels for a target variable, and their latent vectors are used as features to estimate their mutual information (MI) towards the target variable <ref type="bibr" target="#b37">[38]</ref>. The dimensions are then ranked by their MI values and agglomeratively organized as a tree structure for hPCP. In this way, we can understand the top semantic dimensions explaining the selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Driving Scene View</head><p>The driving scene view offers a live detection result for a selected object <ref type="figure" target="#fig_4">(Fig. 1-c )</ref>. This view shows the ground truth box (green) and detected bounding box (red) in a driving scene image, and also scores of IOU, confidence and robustness ( <ref type="figure" target="#fig_4">Fig. 1-c1</ref> ).</p><p>This view also helps users understand adversarial samples and associated adversarial gradients by inserting different adversarial generations into its driving scene to examine detection results, shown in the bottom of <ref type="figure" target="#fig_0">Fig. 1-c2</ref> . The original object is in the middle (zero point) of the horizontal axis, and different samples are generated from this object by moving ascending (right) or descending (left) along the direction of adversarial gradient in the latent space. By examining generated images and their test results, we can see how an adversarial gradient changes object appearance and detection performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CASE STUDY AND EVALUATION</head><p>We closely worked with four domain experts from a function testing department for autonomous driving applications over one year. Two of the experts collaboratively contributed this work and the other two conducted model testing in their daily work. We together went through several design iterations and system refinements to alleviate their pain-points, namely how to generate actionable insights to assess, understand and improve traffic light detectors with minimal human-inthe-loop. The system has been successfully transferred and deployed in their testing platform. The following use cases and experimental evaluations were distilled from their routine practice of model testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">The Dataset and Detector</head><p>The experts suggested to use a public dataset, Bosch Small Traffic Lights Dataset <ref type="bibr" target="#b33">[34]</ref>, to study and benchmark system capability. The dataset includes 5093 training images (10756 annotated traffic lights) and 8334 test images (13486 annotated traffic lights). The baseline traffic light detector was provided by domain experts for case study purpose. The provided detector is based on SSD MobileNet V1 (see <ref type="figure" target="#fig_0">Fig. 2</ref> for model architecture) and achieves state-of-the-art accuracy with AP@IoU50 = 0.478 (compared with 0.41 published in <ref type="bibr" target="#b3">[4]</ref>). We ran the detector over both training and testing dataset and saved detection results for analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Representation and Adversarial Learning</head><p>We cropped 10683 valid traffic lights from the training dataset and then resized them into 64x64 to train the models of disentangled representation (DRL) and adversarial learning (SeADV). We only used training dataset for both models and reserved the test dataset for experiments.</p><p>For DRL, the network architecture is the same as β -VAE <ref type="bibr" target="#b6">[7]</ref> with latent dimension |Z| = 32, γ = 100,C = 20, μ = 4, ν = 1, learning rate 5e-5, 2K maximum iterations to increase capacity C, and 5k total iterations. With the pre-trained DRL model, we conducted semantic adversarial attack and generation against the base detector over all train objects. Among 7286 successful detected objects by the base detector, we obtained 6830 adversarial objects (93.7% success rate) and failed to attack 456 objects with the modification budget of two standard deviations over latent vectors, λ = 2 (other hyper-parameters in SeADV: η = 0.01, K = 512, δ = 0.5, and T = 500).</p><p>After model pretraining and data preprocessing, we extracted required data from both training and testing datasets to drive interactive visualization. The first two PCA components of both training and testing objects were projected with the same PCA model that was built upon the latent vectors of training objects. This kept the PCA views of training and testing data consistent and comparable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Visual Analysis Cases 6.3.1 Show me performance landscape</head><p>The experts were eager to see the overall accuracy landscape of the detector. They first examined the confidence landscape over test dataset, shown in <ref type="figure" target="#fig_7">Fig. 7</ref>. Intuitively, they had several observations.</p><p>First, there was a clear distribution imbalance of detection accuracy over different object colors, as shown in <ref type="figure" target="#fig_7">Fig. 7-a b</ref> . The semantics of color were captured by the first PCA component for latent dimensions, and shown as the x-axis in <ref type="figure" target="#fig_7">Fig. 7</ref>-b : right for red and left for green light (the color variance was learned from the data, and we didn't explicitly use color attributes). It was clear that the detector had worse performance over red lights compared with green ones.</p><p>Secondly, data distribution gap and low confidence score areas were observed. There was a large sparse area in the middle of <ref type="figure" target="#fig_7">Fig. 7</ref>-b , indicating testing data has less coverage in this area (compared with the background density map built from the training dataset). However, one interesting finding is that the detector still had low confidence score in the middle part even though the training data had dense coverage in this part. Also, another low score area is located on very right side, S1 in <ref type="figure" target="#fig_7">Fig. 7</ref>a . This area are mostly red lights that looked clear and bright, and it was quite interesting to see why the detector failed here.</p><p>Therefore, two following-up questions were asked by the domain experts: a) What factors lead the low performance areas? b) What actionable insights we can have to improve the accuracy of our interest?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">What confused the detector?</head><p>The experts were first interested in why the detector had low confidence score in the middle area where the training data already had good coverage. They turned to the training data by examining visual summary of objects (upper part in <ref type="figure" target="#fig_4">Fig. 1-b</ref> ) and confidence scores ( <ref type="figure" target="#fig_8">Fig. 8-a )</ref>. They observed that even for the training data, traffic light objects in the middle bottom part was quite challenging for the detector, as shown in yellow and red score bins. By looking and exploring the semantics of top dimensions in <ref type="figure" target="#fig_8">Fig. 8-b</ref> , they found the middle range of PCA0 explains the ambiguity of traffic lights caused low confidence scores (as PCA0 mainly captures the variations of colors), and the lower end of PCA1 shows dark traffic lights are difficult for the detector (as PCA1 mainly explains the variations from dark to bright). Therefore, they could conclude that the ambiguous and dark traffic lights explained the low confidence area in the middle part.</p><p>To investigate the low confidence area S1 in <ref type="figure" target="#fig_7">Fig. 7</ref>-a b , they ranked latent dimensions to show their importance contributing to this selection. As shown in <ref type="figure" target="#fig_7">Fig. 7</ref>-c , they observed that the left end of the 'DIM7', and 'DIM13' with red color contributed most to the selection S1. They also noticed that the selection on these two dimensions has very high brightness, and the distribution of high brightness in testing data (the colored bars in <ref type="figure" target="#fig_7">Fig. 7-c )</ref> is much larger than the distribution in training data (the gray area plot in <ref type="figure" target="#fig_7">Fig. 7-c</ref> . This explained the low confidence scores of S1 resulted from the under-representative high brightness red lights in the training dataset.</p><p>Further, they also observed one dimension, 'DIM28' among the top ranked dimensions, showed low confidence scores over yellow lights ( <ref type="figure" target="#fig_7">Fig. 7d )</ref>. After using 'DIM28' as x-axis to re-lay out the score summary, they can see an outlier cluster of yellow lights with low confidence scores, shown as S2 in <ref type="figure" target="#fig_7">Fig. 7</ref>-e . They ranked latent dimensions to explain these yellow lights selected in S2 , and found the dimensions of 'DIM28' and 'DIM7' explain the visual semantics of yellow lights with low confidence scores, as shown in <ref type="figure" target="#fig_7">Fig. 7</ref>-f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.3">How about potential failure cases?</head><p>The domain experts were curious about how robust the detector was and where potential failure cases might come from. They examined the visual summary of robustness scores for all training objects, the lower part in <ref type="figure" target="#fig_4">Fig. 1-b</ref> . Overall, the detector had better robustness over green lights on left compared with the red lights on right in the view.</p><p>Robustness was very low in the middle bottom part (selection S0 in <ref type="figure" target="#fig_4">Fig. 1b</ref> ). This part is mostly the dark and ambiguous traffic lights shown in the visual summary of training objects in <ref type="figure" target="#fig_4">Fig. 1-b</ref> . The domain experts then used latent dimensions to understand the low robustness area S0 . As they selected the low robustness area and then ranked the dimensions, the top dimensions 'DIM7' and 'DIM4' capture the most important visual traits of the low robustness area ( <ref type="figure" target="#fig_4">Fig. 1d )</ref>. It showed that the selected range of 'DIM7' represents color ambiguity and the one of 'DIM4' indicates dark color <ref type="figure" target="#fig_4">(Fig. 1-d1 )</ref>.</p><p>They further examined the adversarial gradient directions of top dimensions in the low robustness area. They could see that the adversarial gradients push the visual appearance of traffic lights to move towards ambiguous appearance over the dimension 'DIM7' (the arrows over bars pointing into the center in <ref type="figure" target="#fig_4">Fig. 1-d</ref> ). In the live detection view, <ref type="figure" target="#fig_4">Fig. 1</ref>c , they also tried to put various generated objects along the gradient direction to test detection results in the scene.</p><p>They concluded that the detector was sensitive in the dark and ambiguous lighting conditions and therefore showed low robustness in these conditions. By examining the original scenes for low robustness cases, the lighting conditions were caused by some factors like far-away distance, shadow by building and tree or occluded by other objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.4">Identifying noisy labels, confusing objects and more</head><p>The multi-faceted visual analysis enabled the experts to conduct more useful diagnosis about data quality and detector behaviors. For example, it helped the experts identify data with missing labels, as shown in <ref type="figure" target="#fig_9">Fig. 9</ref>-a3 by selecting FPs with high confidence scores ( <ref type="figure" target="#fig_0">Fig. 9-a1 a2 )</ref>. It also assisted users understand confusing objects for the detector, such as pedestrian sign and car rear lights in <ref type="figure" target="#fig_1">Fig. 9-b3</ref> , by selecting FPs with uncertain confidence scores ( <ref type="figure" target="#fig_0">Fig. 9-b1 b2 )</ref>. With various filtering combination in <ref type="figure" target="#fig_4">Fig. 1</ref>-a , the experts could conduct more analysis such as the impact of object size over performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Visual Analytics Assisted Improvements</head><p>With the insights from above analysis, domain experts developed three improvement strategies <ref type="table" target="#tab_3">(Table 3)</ref>. These strategies aimed at generating more data from training data based on the VA (Visual Analytics) insights, and augmenting model training to validate if we can improve the performance of either accuracy or robustness.  <ref type="table">Table 4</ref>. Accuracy improvement with distribution guided data augmentation, measured by AP@IoU50 (5 trials, larger is better). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.1">Distribution Guided Augmentation for Accuracy</head><p>This strategy is to generate data from training data with the insight of shifted data distribution learned in Sect. 6.3.2 to augment the detector's accuracy (noted as VA-Dist-Aug in <ref type="table" target="#tab_3">Table 3</ref>). The experts already understood that there were under-representative traffic lights with bright red and yellow color in training data. In this strategy, for each case of under-representative bright red or yellow color, the selected dimensions (e.g. 'DIM28' and 'DIM7' for yellow lights) were used to generate more training data. The training objects in the selected ranges (e.g. Then the sampled vectors were used to reconstruct object images. The generated object images are then blended back into original scene images as augmented data. 940 new objects were generated and used to fine tune the base detector along with original training data. <ref type="table">Table 4</ref> shows accuracy improvement with five trials. The overall accuracy was improved from 0.478 to 0.493, measured by AP@IoU50. The accuracy improvements over red and yellow lights were significant, although the green light accuracy had little drop. This indicates that the distribution guided data augmentation could help overall accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.2">Adversarial Guided Data Augmentation for Robustness</head><p>A common way to improve robustness is adversarial training, namely training models with adversarial data <ref type="bibr" target="#b15">[16]</ref>. With a similar idea, adversarial data were mixed with original training data and then used to fine tune the base model. To evaluate semantic robustness, the half of all adversarial objects was randomly selected as adversarial test data, and thus there was 6803/2 = 3402 adversarial testing objects (along their original scene images). The second half adversarial objects (3401) <ref type="table">Table 5</ref>. Semantic robustness and accuracy improvements with adversarial augmentation, measuredby AP@IoU50 (5 trials, larger is better). were used for training and validation. Two adversarial guided data augmentation were experimented: Glb-Adv-Aug and VA-Adv-Aug in <ref type="table" target="#tab_3">Table 3</ref>. In the Glb-Adv-Aug strategy, all 3401 adversarial objects were mixed with original training data for training and validation. The rationale for this strategy is that adversarials can expose potential weakness of a detector, and training with adversarials would help a model detect these adversarials.</p><p>In the VA-Adv-Aug strategy, more adversarials were generated to augment model robustness with VA insights that the dark ambiguous objects had low robustness. As the experts observed that the dark ambiguous objects were mostly explained by 'DIM7' and 'DIM4' <ref type="figure" target="#fig_4">(Fig. 1d )</ref>, they decided to generate 5 more adversarials for each object in the selected ranges of the two dimensions (i.e. DIM7 ∈ [−1, 1], DIM4 ∈ <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref>). Also, they observed the adversarial gradients over 'DIM7' and 'DIM4' tends to push towards homogeneous dark and ambiguous directions, and thus they needed to diversify adversarial generation by fixing the gradients of these two dimensions. To further diversify the generation, other random 3 dimensions were also fixed during the process of adversarial searching, namely</p><formula xml:id="formula_14">z t+1 i j = z 0 i j , j ∈ [d4, d7, d rnd1 , d rnd2 , d rnd3 ]</formula><p>in algorithm 1-line 10. In this way, 1242 adversarial objects were obtained and mixed with global adversarial objects (3401) for adversarial training and validation. <ref type="table">Table 5</ref> shows the performance improvement results with five trials. The results include two metrics of AP@IoU50 over adversarial test dataset for semantic robustness evaluation and the natural test dataset for accuracy evaluation. Both methods significantly improved semantic robustness with AP@IoU50 of 0.528 and 0.564 compared with base model of 0.368 over adversarial test data. Meanwhile, the two methods also improved the accuracy over the natural test data from 0.478 to 0.504 and 0.518. This indicates that adversarial guided data augmentation can improve both accuracy and robustness performance.</p><p>Moreover, the visual analytics guided method boosted both accuracy and robustness performance even further. With VA insights , the VA-Adv-Aug enhanced the semantic robustness accuracy from 0.528 to 0.564 and the natural accuracy from 0.504 to 0.518 on top of the Glb-Adv-Aug. The robustness improvement across different categories (off, green, yellow and read) were also observed. This validates our assumption that the VA insights can further improve the model performance of both accuracy and robustness with domain experts' knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Domain Experts Feedback</head><p>The system was deployed and used in the workplace of the domain experts. They also used the system to analyze in-production dataset and detectors. Here, we only reported the feedback that they had with above public data because of confidentiality issues.</p><p>Overall, domain experts were impressed by the "actionable insights" obtained and also felt excited about experiment results on model improvement by "injecting domain knowledge" with little human-in-theloop. For model validation, they found the tool's capability to aid them in targeting weak spots is "specifically useful", and visual summary was "the most useful feature" by considering the data magnitude they have. They were able to be"swiftly aware about" detectors' sensitive regions with "constrained adversarial attacks using the realistic augmentations". For robustness interpretation, the tool enabled them understand network weaknesses with "reasonable meaningful information", such as gradient directions and semantic dimensions.</p><p>They also suggested improvements from practical perspectives, such as "tracking quality trends" over different datasets and models, and "exporting and reporting" actionable insights. Also, they expressed some visualizations were "overwhelming" to take improvement actions upon, such as gradient maps. This calls for us to strengthen our design principles of "minimal human interaction" and augmenting human cognition with "interpretable representation learning".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION AND FUTURE WORK 7.1 What have we learned?</head><p>Practical implications for detector evaluation in autonomous driving. We learned some practical implications for both model evaluation and improvement in autonomous driving applications. One challenge in this domain is to deal with long-tail data distribution: although most cases could be covered by acquired dataset, rare cases comes from the long tail part <ref type="bibr" target="#b0">[1]</ref>. To efficiently retrieve and collect data for model evaluation and improvement, we can search and retrieve data with potential issues via our approach (e.g. high brightness and dark ambiguous objects for detectors), from a huge candidate dataset, or we can collect new driving scenes with such lighting conditions to improve models. This data-driven visual analytics approach bears much potentials for model understanding and improvement.</p><p>Augmenting human cognition with representation learning. Semantic representation learning shows promising capability of augmenting human cognition to understand complicated data and model space of deep neural networks. Some learning approaches, such as feature maps <ref type="bibr" target="#b40">[41]</ref>, concepts and feature attributions <ref type="bibr" target="#b1">[2]</ref>, directly extract interpretable representations from model spaces. In this work, semantic representation learning extracts human-friendly representations from data space, and then reveal feature contribution towards model predictions. Both approaches serve as more powerful technologies of understanding, interacting with and improving AI systems <ref type="bibr" target="#b8">[9]</ref>.</p><p>Minimal human-in-the-loop and maximizing actionable insights. We aim at utilizing minimal human interactions <ref type="bibr" target="#b7">[8]</ref> to inject domain knowledge into models during system design. Towards this end, two learning components of semantic representation and adversarial generation are introduced to complete heavy-lifting work: semantic representation extracting interpretable dimensions for human-friendly visualization and interaction, and adversarial generation reducing searching space to probe model potential weakness. Meanwhile, we also target maximizing actionable insights with minimal interaction (e.g. Rank-To-Interpret feature), but also provide rich interactive visualization to enable in-depth exploration on demand. However, it is a challenging task to balance the level of details (e.g. more details for adversarial searching) to be presented so that users can make sensible and reliable decisions with few interactions <ref type="bibr" target="#b39">[40]</ref>. This calls for more investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Limitations and future work</head><p>Extended to multi-object detection. Our current system aims at traffic light detection problem, namely a single object detector. However, the framework is also applicable to the problem of multi-object detections (e.g. person, car, truck, bike, rider, sign, etc). We plan to extend this wok to generic object detectors for autonomous driving.</p><p>Representation learning with powerful expressiveness. In spite of promising semantic interpretation in disentangled representation learning <ref type="bibr" target="#b6">[7]</ref>, we need non-trivial visual design, such as hPCP to associate semantics with latent dimensions (e.g. which dimensions explain object color or brightness). One possible remedy is to investigate representation learning methods of combining neural networks and symbolic learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">22]</ref>, by associating symbolic schema (e.g. decision tree, or rules) with latent representations.</p><p>Understanding detector beyond objects. The current method largely uses the visual semantics of objects to understand and improve object detectors, but leaves the context of driving scenes for future work. We can extend this approach by parsing driving scene semantics with additional representation learning components <ref type="bibr" target="#b47">[48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>In this work, we propose a visual analytic system, VATLD, to assess, understand, and improve the accuracy and robustness of traffic light detectors for autonomous driving. This approach is built upon a representation learning to augment human cognition with human-friendly visual summarization, and a semantic adversarial learning to expose interpretable robustness issues with minimal human interactions. We also demonstrate the effectiveness of performance improvement strategies derived with VATLD, and illustrate practical implications for real-world problems in production environments. We hope this work can capture a silver of ways of applying the approaches of visual analytics and human-in-the-loop to alleviate some trustworthy AI issues in autonomous driving domain.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>A Single Shot multibox Detector (SSD)<ref type="bibr" target="#b28">[29]</ref> for traffic lights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>An example of detector accuracy calculation. (a) A example with green boxes as ground truth (GT ), and red boxes as detections (DT ); (b) IoU calculation and (c) the confusion matrix with a IoU threshold;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>The VATLD framework. (a). Traffic light objects are first extracted from acquired data including both "in distribution" training data (a1) and testing data with possible "shifted distribution" (a2), and also unseen cases, "out of distribution" (a3), need to be generated and tested. (b). Disentangled representation learning extracts interpretable attributes (e.g. colors, brightness, and background) from training objects (b1); and semantic adversarial learning probes the prediction behaviors (i.e. gradients) of a black-box like detector, and generates meaningful adversarial unseen examples (b2). Both acquired data (b3) and unseen data (b4) are passed to the detector for detection results. (c). The results along with meta-data (object size, disentangled representation, gradients etc) are transformed into interactive visualization with multi-faceted performance analysis and adversarial summarization and interpretation; (d) With minimal human interaction, actionable insights are derived to "lift distribution up" via data augmentation and inject human intelligence to improve accuracy and robustness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>(a) Disentangled representation learning; (b) Semantic adversarial learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 1 :</head><label>1</label><figDesc>Semantic Adversarial Learning (SemAdv) Input: an object o i , from a scene x j , a detector f (x), an encoder e φ , a decoder d θ , learning rate η, perturbation sample size K, perturbation scale δ , maximal iteration steps T , and maximal modification budget λ Output: an adversarial object o i , and its modification ε i 1 if f (o i→ x j ) &lt; 0.5 then 2 return "Already failed. No need to attack!" 3 Encode object z</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>The design TileScape and hPCP: a) Tile aggregation; b) Dimension visual encodings; c) Hierarchical PCP; d) Rank-To-Interpret.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>hPCP → TileScape. Any dimension from hPCP can be selected as x or y axis for TileScape to examine what visual semantics are embedded in this dimension. Also by hovering or brushing the dimension bins in hPCP, corresponding tiles in TileScape are highlighted to show what visual feature the selected bins capture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>The detection confidence (accuracy) landscape and error diagnosing over test dataset. (a) Detection confidence summary over the first two PCA components of latent dimensions, and one low confidence area ('S1'); (b) Object visual summary over PCA components; (c) Ranked latent dimensions explain how the selection 'S1' is different from the rest, and the dimension 'DIM7', and 'DIM13' capture distribution shift of high brightness for red lights; (d) One top ranked dimension, 'DIM28', indicates low confidence scores over yellow lights; (e) Re-layout confidence summary with 'DIM28' as x-axis, and show an outlier cluster of yellow lights with low confidence score, 'S2'; (f) Top two ranked latent dimensions, 'DIM28' and 'DIM7', explain the visual semantics of yellow lights with low confidence scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>The visual summary for training data. (a) Detection confidence (accuracy) landscape (b) Top latent dimensions with semantics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Diagnosis for data quality and confusing objects. (a) Identifying mislabeled data (a3) by selecting the predictions without ground truth (zero size predictions in a1) and having high confidence scores (a2); (b) Understanding confusing objects (e.g. pedestrian sign and car rear lights in b3) by selecting the predictions without ground truth (b1) and having uncertain confidence scores (b2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>DIM28 ∈ [−3, −1.75] and DIM7 ∈ [−0.75, 0]) were first retrieved. For each retrieved object, k (k = 5 here) data points were uniformly sampled within the selected dimension ranges (e.g. [−3, −2.75,...,−2] for 'DIM28' and [−0.75, −0.6, ..., −0.15] for 'DIM7').</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>(N/A) N/A 0.752 (N/A) 0.560 (N/A) 0.599 (N/A) Glb-Adv-Aug 0.504 (0.014) N/A 0.732 (0.005) 0.682 (0.044) 0.600 (0.015) VA-Adv-Aug 0.518 (0.013) N/A 0.716 (0.023) 0.758 (0.022) 0.599 (0.017) Semantic Robustness Accuracy AP@.50IOU over Adversarials (5 Trials) Natural Accuracy AP@.50IOU (5 Trials)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Design requirements for VATLD</figDesc><table><row><cell></cell><cell>Data</cell><cell></cell><cell>Performance</cell></row><row><cell></cell><cell>Existing Data</cell><cell></cell><cell>Accuracy</cell></row><row><cell></cell><cell>Unseen Data</cell><cell></cell><cell>Robustness</cell></row><row><cell>Design Requirements</cell><cell>RD1 Representation/Summarization Human-friendly Data Semantic Edge Cases Efficient Generation for RD2</cell><cell>RP1 RP3 RP2</cell><cell>Contextualized Understanding Performance Interpretation Human-in-the-loop Improvement</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Data extracted for visual analytics in the framework Ci Box same as BBgti (only as GTi is detected)</figDesc><table><row><cell></cell><cell>Ground Truth -GTi</cell><cell>Top-N Detection -DTi</cell><cell>Adversarial -Advi</cell></row><row><cell cols="3">Bounding Box -BBgti, IoUi Conf. Score -Data Bounding Box -BBdti, Performance Image Patch -Oi, Same as Oi, Zi if matched,</cell><cell>Image Patch -O ' i,</cell></row><row><cell>Representation</cell><cell>Latent Vec -Zi</cell><cell>Odti, Zdti if no match</cell><cell>Latent Vec -Z ' i</cell></row><row><cell>Meta-Info</cell><cell>Size -Si</cell><cell>Same as Si if matched, Size = 0 if no match</cell><cell>Gradient -∇i, Robustness -sri</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Data augmentation strategies assisted by visual analytics</figDesc><table><row><cell>Strategies</cell><cell>VA Insights</cell><cell>Generation</cell><cell cols="4">Controlled Dimensions Cases</cell></row><row><cell>Distribution Guided Augmentation (VA-Dist-Aug)</cell><cell>Under-representative semantics (bright red and yellow) in training data</cell><cell>Uniformly upsampling in controlled dimensions</cell><cell>Bright red Yellow</cell><cell cols="2">DIM7: [-3, -1.2] DIM13: [-3, -1.8] 475 DIM28: [-3, -1.75] DIM7: [-0.75, 0] 465</cell><cell>940</cell></row><row><cell>Global Adversarial Augmentation (Glb-Adv-Aug)</cell><cell>Adversarials expose potential weakness</cell><cell>Globally generate adversarials</cell><cell cols="2">No dimension in control (use all latent dimensions)</cell><cell cols="2">6803 (3401)</cell></row><row><cell>VA Guided Adversarial Augmentation (VA-Adv-Aug)</cell><cell>Dark ambiguous objects have low robustness</cell><cell>Diversely generate adversarials w/ selected visual attributes</cell><cell cols="2">Dark ambiguous objects: DIM7: [-1, -1] DIM4: [1, 4]</cell><cell cols="2">1242</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Driverless Cars: 90 Percent Done, 90 Percent Left To Go?</title>
		<ptr target="https://www.forbes.com/sites/chunkamui/2018/02/28/driverless-cars-90-percent-done-90-percent-left-to-go" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Network dissection: Quantifying interpretability of deep visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings -30th IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>-30th IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3319" to="3327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A deep learning approach to traffic lights: Detection, tracking, and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Behrendt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Botros</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICRA.2017.7989163</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings -IEEE International Conference on Robotics and Automation</title>
		<meeting>-IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1370" to="1377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Git Repo for Bosch Small Traffic Lights Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Behrendt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Botros</surname></persName>
		</author>
		<ptr target="https://github.com/bosch-ros-pkg/bstld" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Besold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hitzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-U</forename><surname>Kuehnberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lowd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M V</forename><surname>Lima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>De Penning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pinkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zaverucha</surname></persName>
		</author>
		<title level="m">Neural-Symbolic Learning and Reasoning: A Survey and Interpretation</title>
		<imprint>
			<date type="published" when="2017-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bilal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2017.2744683</idno>
		<title level="m">Do Convolutional Neural Networks Learn Class Hierarchy? IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2018-10" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="152" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Understanding disentangling in β -VAE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1804.03599" />
		<imprint>
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The Nurnberg Funnel: Designing Minimalist Instruction for Practical Computer Skill</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Carroll</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Using artificial intelligence to augment human intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nielsen</surname></persName>
		</author>
		<idno>doi: 10.23915/ distill.00009</idno>
		<ptr target="https://distill.pub/2017/aia" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ShapeShifter: Robust physical adversarial attack on faster R-CNN object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cornelius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H P</forename><surname>Chau</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-10925-74</idno>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="52" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Visual Analytics for Explainable Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/MCG.2018.042731661</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="84" to="92" />
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The human is the loop: new directions for visual analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Endert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>North</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fiaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Andrews</surname></persName>
		</author>
		<idno>doi: 10. 1007/s10844-014-0304-9</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent Information Systems</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="411" to="435" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Black-box adversarial attacks with limited queries and information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">35th International Conference on Machine Learning, ICML 2018</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="3392" to="3401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fast R-Cnn</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.169</idno>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno>doi: 10. 1109/CVPR.2014.81</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2014-09" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2015 -Conference Track Proceedings. International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
	<note>3rd International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-12" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Diagnosing error in object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chodpathumwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-33712-325</idno>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">7574</biblScope>
			<biblScope unit="page" from="340" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep Feature Consistent Variational Autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qiu</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1704.04861" />
		<title level="m">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. arXiv</title>
		<imprint>
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Traffic light recognition in varying illumination using deep learning and saliency map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yoneda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mita</surname></persName>
		</author>
		<idno type="DOI">10.1109/ITSC.2014.6958056</idno>
	</analytic>
	<monogr>
		<title level="m">17th IEEE International Conference on Intelligent Transportation Systems</title>
		<imprint>
			<publisher>Institute of Electrical and Electronics Engineers Inc</publisher>
			<date type="published" when="2014-11" />
			<biblScope unit="page" from="2286" to="2291" />
		</imprint>
	</monogr>
	<note>ITSC 2014</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SCANViz: Interpreting the Symbol-Concept Association Captured by Deep Neural Networks through Visual Analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 13th IEEE Pacific Visualization Symposium</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>ICLR 2017 -Workshop Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">NAttack: Learning the distributions of adversarial examples for an improved black-box attack on deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">36th International Conference on Machine Learning, ICML 2019</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="6860" to="6870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Focal Loss for Dense Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="318" to="327" />
			<date type="published" when="2017-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Microsoft COCO: Common Objects in Context. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2014.471</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3686" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Towards Better Analysis of Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2016.2598831</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="100" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards better analysis of machine learning models: A visual analytics perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.visinf.2017.01.006</idno>
	</analytic>
	<monogr>
		<title level="j">Visual Informatics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="56" />
			<date type="published" when="2017-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno>doi: 10. 1007/978-3-319-46448-0 2</idno>
		<title level="m">SSD: Single Shot MultiBox Detector. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 9905 LNCS</title>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018 -Conference Track Proceedings. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">RuleMatrix: Visualizing and Understanding Classifiers with Rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bertini</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2018.2864812</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="342" to="352" />
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Interpretable and steerable sequence learning via prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.1145/3292500.3330908</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery Data Mining, KDD &apos;19</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery Data Mining, KDD &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="903" to="913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Traffic Light Recognition Using Deep Learning and Prior Maps for Autonomous Cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Possatti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guidolini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">B</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Berriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Paixao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Badue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliveira-Santos</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.2019.8851927</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Neural Networks</title>
		<meeting>the International Joint Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="2019-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Heidelberg Collaboratory for Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Processing</surname></persName>
		</author>
		<ptr target="https://hci.iwr.uni-heidelberg.de/content/bosch-small-traffic-lights-dataset" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Bosch Small Traffic Lights Dataset</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.91</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
	<note>2016-Decem</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2016.2577031</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Explaining the predictions of any classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939672.2939778</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>Augu</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
	<note>Why should i trust you?</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mutual information between discrete and continuous data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Ross</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0087357</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">87357</biblScope>
			<date type="published" when="2014-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Visual interpretability for deep learning: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1631/FITEE.1700808</idno>
		<imprint>
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Human-centered artificial intelligence: Reliable, safe &amp; trustworthy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
		<idno type="DOI">10.1080/10447318.2020.1741118</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="495" to="504" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations, ICLR 2014 -Conference Track Proceedings. International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Visualizing Object Detection Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-016-0884-7</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="145" to="158" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deepvid: deep visual interpretation and diagnosis for image classifiers via knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2019.2903943</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2168" to="2180" />
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dqnviz: A visual analytics approach to understand deep q-networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="288" to="298" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Hierarchical Grouping to Optimize an Objective Function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Ward</surname></persName>
		</author>
		<idno type="DOI">10.1080/01621459.1963.10500845</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">301</biblScope>
			<biblScope unit="page" from="236" to="244" />
			<date type="published" when="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Natural evolution strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Glasmachers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="949" to="980" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A survey of visual analytics techniques for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/s41095-020-0191-7</idno>
		<ptr target="https://doi.org/10.1007/s41095-020-0191-7" />
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
