<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attention Flows: Analyzing and Comparing Attention Mechanisms in Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">F</forename><surname>Derose</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayao</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Berger</surname></persName>
						</author>
						<title level="a" type="main">Attention Flows: Analyzing and Comparing Attention Mechanisms in Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>NLP</term>
					<term>Transformer</term>
					<term>Visual Analytics</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Fig. 1: Our approach supports the comparison of attention mechanisms in language models. We compare the BERT model (turquoise) and its fine-tuned counterpart (purple) tasked with determining question-answer pair validity (a). By selecting the word &quot;what&quot;, in contrast to BERT the fine-tuned model attends to the answer &quot;jacksonvillians or jaxons&quot; (c), with full sentence context shown in (b).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recent breakthroughs in natural language processing (NLP) have led to the development of models that yield significant performance gains across a wide variety of language understanding tasks. In particular, BERT <ref type="bibr" target="#b10">[11]</ref> -or Bidirectional Encoder Representations from Transformers -has demonstrated how Transformer models <ref type="bibr" target="#b48">[49]</ref>, pre-trained on unsupervised tasks from large-scale document corpora (e.g. Wikipedia), can be effectively fine-tuned for downstream supervised tasks. Remarkably, the pre-training tasks used in BERT, such as masked word prediction and next sentence prediction, at a glance appear quite different from the downstream tasks, such as question answering, textual entailment, and semantic equivalence of sentences <ref type="bibr" target="#b50">[51]</ref>. Although pre-training tasks encourage representations that capture lexical and syntactic reasoning <ref type="bibr" target="#b9">[10]</ref>, how these language models generalize to sen-tence understanding tasks remains unclear. This problem is important to model builders, as the relationship between pre-trained and finetuned models can help them design pre-training tasks. For instance, identifying differences between models and whether these differences are meaningful to the downstream task at hand, can help ensure that pre-training does not merely learn semantically-irrelevant data nuances that happen to be discriminative for the task <ref type="bibr" target="#b32">[33]</ref>. We see this problem as timely, as recent work has demonstrated the increasing importance of pre-training, resulting in numerous types of unsupervised and weaklysupervised tasks aimed at predicting text spans <ref type="bibr" target="#b21">[22]</ref>, entities <ref type="bibr" target="#b53">[54]</ref>, and word senses <ref type="bibr" target="#b26">[27]</ref>.</p><p>In this work, we seek to obtain a better understanding of BERT models and in particular, to understand the gap between pre-trained models and fine-tuned models. The problem of understanding BERT poses numerous challenges due to its inherent complexity. Specifically, sentence-contextualized word embeddings are learned across multiple layers, and each layer performs so-called self-attention, expressing a single word's output embedding as a convex combination of all input word embeddings. Further, self-attention is distributed amongst multiple attention heads that operate independently and collectively form a word's representation at a given layer. Recent work has addressed interpretability of self-attention mechanisms, demonstrated only on pre-trained models, showing their ability to capture dependency syntax <ref type="bibr" target="#b9">[10]</ref> and grammatical relationships <ref type="bibr" target="#b28">[29]</ref>. Yet these works are typically focused on analyzing a particular relationship known a priori and/or studying a single attention head or attention distributed over a given layer. In the context of fine-tuning, our object of study is the classification decision, e.g. does one sentence entail another? Answering this type of question requires a more holistic view of the model, and understanding how attention heads, across multiple layers, aggregate to form a single classification output.</p><p>We propose Attention Flows: a visual analytics approach to help interpret how classification outputs are formed in BERT models via the visual analysis of attention propagation. Our visualization design is classification-centric: in BERT, classification is performed with respect to a reserved classification token's embedding at the final layer of the model. Thus, the main objective of our visualization is to provide insight on how attention flows across words, both between sequences of layers and within attention heads, down to the eventual classification. We iteratively extract word dependencies between layers using selfattention, working backwards. We extract words at the last layer that most influence classification, and then for each of these words, we extract their respective dependencies from the previous layer, repeating this process throughout all network layers. We visually encode this information in a radial layout: each ring of the layout represents a layer, with the classification token output being at the center and shallower layers progressing outward, shown in <ref type="figure">Fig. 1</ref>. We encode both words and attention heads, where through a set of supported interactions, users can explore how attention flows into, and out of, words.</p><p>Critically, our visualization design naturally lends itself towards model comparison. We permit the visual comparison of attention flows between a provided pre-trained model and a fine-tuned model, so that the user can comprehend differences and similarities between the models. We show, through use cases and user feedback on a number of sentence understanding tasks <ref type="bibr" target="#b50">[51]</ref>, how our visualization highlights distinguishing factors between attention flows of pre-trained models and their fine-tuned counterparts. For instance, we find that question answer inference tasks lead to attention flows that target the "Five Ws" for information gathering, while for sentence paraphrasing tasks, we find that when one sentence does not paraphrase another, attention is focused around phrases that differentiate the sentences.</p><p>We summarize our contributions below:</p><p>1. We introduce a visualization design that supports a comprehensive understanding of self-attention in Transformer models, over multiple layers and attention heads. Our design enables inspection of how classification decisions are made in sentence understanding tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Our visualization supports the comparison of self-attention mech-</head><p>anisms between two models, focused on the differences and similarities amongst pre-trained and fine-tuned models.</p><p>3. Through use cases and user feedback, we show how our interface offers insight on changes in self-attention that are due to finetuning for inference tasks such as textual entailment, question answering, and sentence paraphrasing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Our work is related to the visual analysis and interpretability of deep learning, with an emphasis on NLP models. Here we discuss work related to deep networks for NLP, approaches to interpretability, visual analytics for interpreting deep learning, and comparative visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Language Modeling</head><p>Research in language modeling is focused on learning representations of linguistic elements, typically words and sentences, that capture syntactic and semantic properties suitable for higher-level language understanding. Neural language models, in particular, are focused on learning word <ref type="bibr" target="#b3">[4]</ref> and/or character <ref type="bibr" target="#b22">[23]</ref> level representations, where the task is to predict a probability distribution over words at a particular point in a sequence, conditioned on all prior words. It is common to use recurrent neural networks (RNNs) <ref type="bibr" target="#b33">[34]</ref>, and variants such as Long Short Term Memory (LSTM) <ref type="bibr" target="#b16">[17]</ref> or Gated Recurrent Units (GRUs) <ref type="bibr" target="#b7">[8]</ref>, for this task to learn contextualized word-level embeddings, given contextindependent embeddings as input, e.g. GLOVE <ref type="bibr" target="#b38">[39]</ref> or word2vec <ref type="bibr" target="#b34">[35]</ref>.</p><p>A key aspect of language models is that they do not require humanannotated data, but instead, they learn from document corpora where sentence-level information (e.g. sequences of words) is preserved.</p><p>Language modeling is usually seen as a pre-training process, where contextualized representations are used for downstream supervised NLP tasks, e.g. textual entailment <ref type="bibr" target="#b4">[5]</ref>, semantic role labeling <ref type="bibr" target="#b15">[16]</ref>, and named entity extraction <ref type="bibr" target="#b39">[40]</ref>. An alternative to recurrent models <ref type="bibr" target="#b40">[41]</ref> are Transformers <ref type="bibr" target="#b48">[49]</ref> for modeling sequential data. Transformers rely on a notion of selfattention: given input word embeddings combined with positional word encodings, the Transformer outputs a new representation of each word, in part, through a convex combination over all inputs. This convex combination -represented as a set of nonnegative weights over words that sum to 1 -assigns importance to words, namely the output embedding of a particular word is dependent on another word's input embedding if its attention weight is high. Further, it is common to employ multiple, independent forms of attention at a given layer through so-called attention heads. The approach of BERT <ref type="bibr" target="#b10">[11]</ref> has demonstrated how to use Transformers as language models, via solving the pre-training tasks of masked word prediction, as well as next sentence prediction. They demonstrated that, by fine-tuning such pre-trained Transformer models on supervised NLP tasks, strong improvements in performance over a variety of models can be obtained. This has motivated recent work in refining optimization procedures <ref type="bibr" target="#b31">[32]</ref> for Transformers, as well as designing different pre-training tasks, e.g. predicting text spans <ref type="bibr" target="#b21">[22]</ref>, entities <ref type="bibr" target="#b53">[54]</ref>, and word senses <ref type="bibr" target="#b26">[27]</ref>. The predominance of Transformers and BERT-based pre-training within NLP has motivated us to support a more detailed understanding of these models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model Interpretability</head><p>The successes of BERT have led to numerous works in attempting to understand why the model -specifically the Transformer model and its pre-training objectives -performs so well <ref type="bibr" target="#b43">[44]</ref>. Existing works are largely targeted at understanding the representations learned during pre-training. Specifically, recent works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b42">43]</ref> highlight how attention heads, across different layers, capture various dependency relations, e.g. prepositions and coreferent mentions. Lin et al. <ref type="bibr" target="#b28">[29]</ref> show how learned embeddings capture subject nouns and main auxiliaries, while attention captures subject-verb agreement and anaphora relations. Brunner et al. <ref type="bibr" target="#b5">[6]</ref> demonstrate that word embeddings in earlier layers tend to retain their identity, while deeper layers represent aggregated, abstract information. Other works have studied the differences in BERT between pre-trained models and their fine-tuned counterparts. For instance, Hao et al. <ref type="bibr" target="#b13">[14]</ref> visually illustrate the loss landscapes produced during finetuning, while other works inspect how syntactic relations change by inspecting BERT's attention patterns and embeddings <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b47">48]</ref>.</p><p>These methods can highlight relevant linguistic phenomena as highlevel summaries over a given dataset, typically through a pre-defined probing task (e.g. classifying dependency relations). However, there are two main limitations with these methods. First, they do not permit local explanations of individual instances, e.g. how can we understand the changes made from pre-training to fine-tuning on a given sentence pair? Secondly, the focus on individual attention heads, or layers, does not permit global interpretability of the model: the relationship between attention across multiple layers. These aspects of explainability are key for good user experiences in explainable AI systems <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28]</ref> and reflect objectives that we target in our visualization design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Visual Analytics for Interpreting Models</head><p>Within the visual analytics community, significant research has been devoted to analyzing and interpreting machine learning models, in particular deep learning methods, please see the survey by Hohman et al. <ref type="bibr" target="#b18">[19]</ref>. Although much work has been devoted to understanding convolutional neural networks for image classification <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b41">42</ref>], here we discuss techniques most relevant to our method, namely techniques for visually analyzing NLP models. In the context of RNNs, as previously discussed in Section 2.1, Strobelt et al. <ref type="bibr" target="#b46">[47]</ref> visualized hidden states as line marks plotted over a sequence, one per dimension, while Ming et al. <ref type="bibr" target="#b35">[36]</ref> seek to group hidden state activations via clustering. Other works are more task-specific, e.g. interactively exploring machine translation <ref type="bibr" target="#b45">[46]</ref> and inspecting classification decisions in natural language inference tasks <ref type="bibr" target="#b31">[32]</ref>, while Cashman et al. <ref type="bibr" target="#b6">[7]</ref> focus on understanding the training of recurrent networks through visualizing network gradients, and Gehrmann et al. <ref type="bibr" target="#b11">[12]</ref> support fine-grained model editing for abstractive summarization.</p><p>Other works have begun to address the visualization of attention mechanisms in language models. Some works have visually inspected attention for RNNs <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b45">46]</ref>, in particular, Choi et al. <ref type="bibr" target="#b8">[9]</ref> showed how visually encoding attention in an RNN-based sentiment classification task can help humans more efficiently and effectively annotate data. More recent work has started to consider the visualization of Transformer models <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b49">50]</ref>. These works are largely focused on visually analyzing individual attention heads <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b49">50]</ref>, or alternatively the aggregation of attention heads in a given layer <ref type="bibr" target="#b37">[38]</ref>, and provide support for querying sentences based on selected head embeddings <ref type="bibr" target="#b20">[21]</ref>. Rather than individually inspect attention heads or layers, our work aims to provide a more holistic view of self-attention in Transformers so that it is possible to visually analyze and compare models in terms of a particular downstream sentence understanding task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Comparative Visualization</head><p>A major emphasis of our work is on the visual comparison of language models and in particular, the comparison of graphs that result from their self-attention mechanisms. Visual comparison has been extensively studied within the visualization community, please see Gleicher <ref type="bibr" target="#b12">[13]</ref> for an overview. For visually comparing graphs, existing works have visually depicted similarities via graph merging and using color encodings <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">25]</ref>, while a larger design space -heatmaps, grouped bars node-link diagrams -has also been studied <ref type="bibr" target="#b0">[1]</ref>. Our design is inspired by these works, yet in our scenario the graphs from models are dynamic, updated in response to user interactions. Other works have considered the visual comparison of deep networks, specifically convolutional networks <ref type="bibr" target="#b52">[53]</ref> and recurrent models <ref type="bibr" target="#b36">[37]</ref>. However, these works are largely focused on comparing model performance, whereas our method is aimed at comparing how models reason over a given input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OVERVIEW AND TASKS</head><p>In this section we provide details on the Transformer model, BERT, and the set of tasks we aim to address through our visualization design. </p><formula xml:id="formula_0">A k 1 A k 2 A k 3 X k geegen X k+1 geegen X k when X k was X k gegeen X k the X k emperor Fig. 2:</formula><p>We illustrate the computation involved in self-attention for the sequence "when was gegeen the emperor". Here the word "gegeen" attends to the rest of the sequence in independent attention heads, producing a separate embedding for each head. These are combined to produce a new embedding for "gegeen" at the next layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A Summary of BERT</head><p>Our model of study is BERT <ref type="bibr" target="#b10">[11]</ref>, a Transformer model <ref type="bibr" target="#b48">[49]</ref> pre-trained to solve certain language modeling objectives. For clarity of notation, herein we refer to words as tokens, in order to encompass a larger set of inputs, e.g. punctuation, specialized symbols, as well as word pieces <ref type="bibr" target="#b51">[52]</ref>. The Transformer model relies on a notion of self-attention to compute contextualized token embeddings for a provided sequence of tokens. Context refers to a token's position, and other tokens that surround it, in a given sequence. More specifically, assume that we are provided a sequence of tokens</p><formula xml:id="formula_1">s = (w 1 , w 2 , ••• , w n ).</formula><p>The Transformer applies a stack of self-attention layers to a sequence, where the input of one layer is the output embedding of the previous. Assume that we are considering a layer l, we have an embedding vector</p><formula xml:id="formula_2">x l w ∈ R d for w ∈ s, d</formula><p>is the embedding dimension, and we denote the sequence matrix as X l ∈ R n×d , e.g. each row corresponds to a token's embedding. To obtain a new contextualized embedding in the next layer X l+1 , the Transformer (1) performs self-attention, one per attention head, (2) derives a new vector from each attention head, and (3) combines the per-head vectors into a single vector.</p><p>1. Self-attention: For a given attention head indexed by j, a dot product-based attention matrix is formed,</p><formula xml:id="formula_3">A l j = softmax 1 √ d X l Q l j X l K l j ,<label>(1)</label></formula><p>where Q l j , K l j ∈ R d×d are projection matrices from a d-dimensional space to a d -dimensional space where d &lt; d, and the softmax function is applied row-wise; each row sums to 1 and its entries are nonnegative.</p><p>2. Attention-based Vectors: A new embedding vector is formed for each token, and each attention head,</p><formula xml:id="formula_4">X l j = A l j X l V l j ,<label>(2)</label></formula><p>where V l j ∈ R d×d is a projection matrix, similar to Q and K. Given the properties of A l j , each token's output embedding is formed as a convex combination over the input token embeddings. Thus, an entry at row a and column b in A l j may be interpreted as how important token w b is to token w a . <ref type="figure">Fig. 2</ref> illustrates this computation, where each row corresponds to a head's self-attention for the given token "gegeen", depicting weights over tokens in the sentence.</p><p>3. Vector Aggregation: The embedding vectors are then concatenated over all heads:</p><formula xml:id="formula_5">X l cat = cat X l 1 , X l 2 , ••• , X l h W l , with W l ∈ R d×d</formula><p>. Note that if one token attends to another token over multiple attention heads, then it will have more influence on the output via this concatenation. This is depicted in <ref type="figure">Fig. 2 (right)</ref>, where we can see how "gegeen" aggregates different types of context, and "emperor" has less overall influence. After concatenation, layer normalization <ref type="bibr" target="#b2">[3]</ref> and residual connections <ref type="bibr" target="#b14">[15]</ref> are applied to the original and concatenated vectors, and last, a multi-layer perceptron (MLP) is applied to this result, followed by another application of layer normalization and residual connections.</p><p>BERT <ref type="bibr" target="#b10">[11]</ref> uses the Transformer model to learn contextualized embeddings that solve two unsupervised pre-training tasks. First, a masked token prediction task randomly replaces tokens in a sequence  <ref type="formula">12</ref>, and all extracted tokens ("not", "men", "?", "A" "Devo", "!") become new nodes for the prior layer (bottom).</p><p>with a unique "mask" token, and the model learns to predict a probability distribution that assigns high likelihood to the original tokens. Second, a next sentence prediction task aims to distinguish sentence pairs that are randomly sampled from sentence pairs that form a single contiguous sequence. The collection of projection matrices at all layers and attention heads, as well as MLPs, form the set of weights of the model to be learned during pre-training. Once completed, a downstream supervised sentence understanding task, e.g. question answer inference, may then be trained by fine-tuning the weights of the model. Typically, the process takes a reserved classification ([CLS]) token's embedding at the very last layer, which we denote x L CLS , and projects the embedding to a set of classification scores for prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Objectives and Tasks</head><p>The main focus of our work is in understanding how BERT performs classification when fine-tuned for NLP tasks, and thus, our starting point is the classification embedding x L</p><p>CLS . Yet, the complexity of BERT poses challenges for gaining insight, with components ranging from attention to contextualized embeddings to various learned transformations. In this work, we have chosen to study the self-attention mechanism, specifically the set of matrices A l j for all attention heads j and layers l, and how self attention organizes to form x L CLS . We view these matrices as key elements in understanding information flows in the Transformer model, e.g. how does one token influence another token across the layers of the network? Thus, in contrast to prior work that is more exploratory regarding self-attention <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b49">50]</ref>, our work seeks to analyze attention to help explain the classification decisions made by BERT. We note that self-attention is not the only way to quantify classification influence, as gradient-based attribution schemes are also commonly employed <ref type="bibr" target="#b44">[45]</ref>. However, for pre-trained models, taskspecific parameters attached to the <ref type="bibr">[CLS]</ref> token have yet to be updated as part of training, and thus gradients are not particularly meaningful. Simple model architectures for fine-tuning <ref type="bibr" target="#b10">[11]</ref> therefore suggest x L CLS , computed through self-attention, will remain important for prediction.</p><p>We thus identify two main objectives that we aim to address:</p><p>1. Understand how self-attention informs classification (O1).</p><p>We would like to understand how the model makes decisions via the words that it attends to, starting from the classification token at the very last layer and going backwards in layers.</p><p>2. Understand the refinement of self-attention due to finetuning (O2). We would like to assess what was learned by the Color visually encodes, for a given attention head, the number of tokens to which the classification token ("[CLS]") attends</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer 12</head><p>Layer 11</p><p>The position of the line encodes locations of tokens to which this particular token ("are") contributes attention, and height encodes the number of heads</p><p>In shallower layers, an attention head is divided up amongst tokens to encode counts unique to individual tokens model when fine-tuned for a specific task, in order to confirm that the model is learning relevant information in solving the task.</p><p>Note that (O2) is dependent on (O1): fine-tuning is tied with classification, thus it is challenging to identify changes in self-attention without an understanding of how self-attention is used to form classifications. We address these objectives via the following tasks:</p><p>1. Trace and query self-attention throughout the model (T1).</p><p>The complexity of self-attention requires user interactions that support the selection of tokens and attention heads over different layers in the model to understand how attention propagates forward (deeper in layers), as well as how attention dependencies form from shallower layers (O1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Discover attention functionality over layers and attention heads (T2).</head><p>An understanding of the model requires comprehending self-attention in aggregate and individual heads, discovered via user queries over interpretable units, e.g. input tokens (O1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>Compare self-attention of pre-trained and fine-tuned models (T3). To understand fine-tuning, it is necessary to assess similarities and differences between pre-trained and fine-tuned models. This should allow for (a) detailed comparisons, e.g. locating shared and distinct attention heads, and (b) global comparisons, e.g. tracing differences in attention flows across layers (O2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">VISUALIZATION DESIGN</head><p>Our visualization design is informed by the tasks identified in the previous section. Here we first discuss the information that we extract for our visualization, followed by a discussion of our design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Attention Graph</head><p>As discussed in Sec. 3.2, our main focus is on the analysis of selfattention. However, analyzing individual attention matrices, or even aggregate attention over a single layer <ref type="bibr" target="#b37">[38]</ref>, might not capture all of the information necessary to understand BERT's classification. Instead, in this work, we aim for a global view of attention -over all heads and layers -as a means of understanding how attention flows from tokens at arbitrary layers down to the classification ([CLS]) token. To this end, we would like to capture the prominent dependencies between tokens when computing contextualized embeddings from one layer to another. Specifically, for an attention matrix A l j , we construct a bipartite graph G l j , where an edge between tokens w a and w b is formed if A l j (a, b) &gt; τ for a threshold τ, a parameter that we allow the user to interactively modify. In this case, w b corresponds to this token's embedding at layer l, and such an edge indicates w b 's strong influence on the embedding for w a at layer l + 1. The graph is constructed iteratively, starting from the [CLS] token at the last layer, and working backwards in layers, please see <ref type="figure">Fig. 3</ref>.</p><p>Considering all bipartite graphs across all layers permits us to trace dependencies between tokens. However, what remains is a way to define such a dependency. Specifically, for tokens w b and w a , and subsequent layers l and l + 1, respectively, we consider two options:</p><p>1. w b is dependent on w a if there exists some attention head j at layer l that connects the tokens in its graph G l j . This scheme is enabled by default in our design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">A dependency only exists between two tokens if it passes through</head><p>a subset of attention heads. This is enacted via user interactions.</p><p>Given one, or a combination, of methods for constructing dependencies, we may then form a larger graph between tokens over all layers, where edges only exist across adjacent layers. We call this graph the Attention Graph, and it is the primary object that we analyze. The Attention Graph enables us to trace information flows between tokens, across layers, and amongst heads, addressing (T1). For instance, a token's embedding at layer l is unlikely to influence another token's embedding at layer m (m &gt; l) if no path exists between them. If a single path does exist, but it is a long path (e.g. over multiple layers), then the token's influence will be minimal, as it's identity is likely to be lost between layers <ref type="bibr" target="#b5">[6]</ref>. If multiple paths exist between the tokens, then this represents strong evidence of influence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Attention Overview</head><p>Our design, first, provides an overview of a model's self-attention mechanism, depicting a notion of classification influence for each token in the sentence at a given layer. To compute influence for a given token w at the last layer L, we record how many heads were used in attending to the [CLS] token, denoted as c L (w, [CLS]) for token w. For the previous layer L − 1 and token w, we gather attended tokens at layer L, denoted W L , and record the number of counts over these tokens:</p><formula xml:id="formula_6">c L−1 (w) = ∑ w ∈WL c L−1 (w, w ),<label>(3)</label></formula><p>where c L−1 (w, w ) indicates the number of heads used from w to attend to w . We iteratively apply this scheme, working backwards in layers, to define a count for any token w at layer l via c l (w). We then compute an influence score that summarizes all layers, from a given layer l:</p><formula xml:id="formula_7">I l (w) = 1 L − l + 1 L ∑ l =l α L−l c l (w),<label>(4)</label></formula><p>which averages the scores over all layers, applying an exponential decay to earlier layers as contextualized embeddings at these layers are unlikely to be as significant as embeddings at later layers, where we set α = 0.5. We find that different values of α do not impact the relative comparisons of models. In our Sentence view, we visually encode a token w's influence, given a user-selected layer l, by taking the ceiling of I l (w) and mapping this as a 5-circle rating directly above the token (see <ref type="figure">Fig. 1(a)</ref>). We clamp all influence scores to 5, as certain tokens, e.g. punctuation, can attend to a large number of tokens. Further, for model comparison, we map the circles to a dark orange color when both models share a certain amount of influence and use distinct colors when influence scores differ, e.g. if I l (w) = 2 for the pre-trained model and I l (w) = 4 for the finetuned model, then the first two circles will be orange, and the last two circles will be purple. This provides an at-a-glance comparison between the models in their self-attention influence on the classification score, addressing the overview aspect of task (T3-b), and can be used to identify tokens of interest for more detailed analysis, discussed next. <ref type="figure">Fig. 5</ref>: Our design allows the user to compare model attention: here the user clicks on "against", a token that shares influence between models. Inspecting its dependent tokens in previous layers, we observe commonalities (in orange) and tokens unique to the fine-tuned model (purple), e.g. "dollar". Attention heads for tokens are, further, split between models where appropriate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Attention Flows Design</head><p>The Attention Flow view uses the Attention Graph to depict selfattention across multiple layers and multiple attention heads, for a given sentence, as shown in <ref type="figure">Fig. 1(c)</ref>. Our visualization design depicts various aspects of the model: token dependencies, attention head information, and the capacity to visually compare models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Token Rings</head><p>For our central view, we deploy a radial layout, where each ring of tokens corresponds to a given layer -please see <ref type="figure" target="#fig_2">Fig. 4</ref>. The innermost ring includes only the [CLS] token, as this is the deepest layer before classification. The preceding ring contains tokens from the prior layer that have some influence on the embedding of the [CLS] token, namely, there exists some attention head that connects a given token and [CLS], as discussed in Sec. 4.1. This process is repeated, aggregating the previous layer's tokens that have some influence on the current layer's tokens, allowing us to trace token influence throughout the model (T1).</p><p>This design is intended to effectively utilize space in showing dependencies between tokens across multiple layers of the Transformer model. In practice, for the [CLS] token at the last layer (center of view), the number of tokens that it depends on from the previous layer is relatively small and thus, may be visually encoded in an annulus with small radii. The number of tokens in preceding layers tends to grow by a bounded amount, in practice by at most 5-10 tokens, and thus, a radial view scales well with the number of tokens in each layer. Each ring of tokens aims to capture the gist of the sentence at different levels of detail, depending on the layer, e.g. in <ref type="figure">Fig. 1</ref> the penultimate layer contains the tokens necessary to answer the question. To provide context with respect to the original sentence, we depict the positions of tokens found in each layer in an auxiliary radial view (c.f. <ref type="figure">Fig. 1(b)</ref>), where each token's position in the original sentence is encoded via angle. Further, to increase readability of tokens, we show as many tokens right side up as possible by having both sentences start from the same angle of the radial layout, with the first sentence going clockwise and the second sentence going counterclockwise until their meeting point (see <ref type="figure">Fig. 1</ref>), depicted as a thick black tick at each layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Attention Heads</head><p>The tokens within each ring represent dependencies that exist regarding some attention head. To provide more detailed information, we visually encode attention heads in two different manners: with respect to rows of a head's attention matrix and with respect to its columns. Recall that rows correspond to tokens that are performing attention and reflect what is output. Thus, adjacent to each token, we visually encode its attention heads as a set of 12 small glyphs (one for each head), where  the color encodes the number of tokens a head attends to in the previous layer, shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. We wrap the 12 head glyphs across 3 rows to optimally use space in the design. This design permits a more detailed assessment of token dependencies, e.g. determining if a set of heads collectively attend to a large number of tokens (T2).</p><p>Columns correspond to tokens that are being attended to in the input. We visually encode this information by mapping the relative positions of tokens at layer l +1 to sparklines under each dependent token at layer l, shown in <ref type="figure" target="#fig_2">Fig. 4 (right)</ref>. The location of the peak indicates whether the dependency occurs earlier (peaks left of center) or later in the sequence (peaks right of center), with a token attending to itself if the peak is in the center. The height of the peak encodes the number of attentive heads, clamped to 3, involved in the dependencies. This design allows the user to quickly assess whether self-attention is localized (near the center) or more global (a sparkline that is uniformly distributed).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Model Comparison</head><p>Our design supports a combined model display to facilitate comparison, please see <ref type="figure">Fig. 5</ref> for an illustration. We take the union of tokens attended to by each model and visually encode a token's detailed attention heads with colors that are model-specific, and shared between models (T3-a). If a head only has significant attention in BERT, we color the head glyph turquoise. If a head only has significant attention in the finetuned model, we color the head glyph purple. If a head has significant attention in the Attention Graphs of both models, we split the head glyph in half to indicated shared attention <ref type="figure">(Fig. 5</ref>). This color scheme is carried over to the Sentence Context view to create homogeneity and facilitate quick model comparisons, specifically, the parts of the sentence included in each Attention Graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Interacting with Attention</head><p>Our design supports a number of interactions to help users gain better insight into a model or models. <ref type="figure">Fig. 8</ref>: Selecting two tokens in non-adjacent layers highlights all tokens -and attention heads -through which attention flows, in this example from "demonstrate" to "light". We find tokens in the phrase "wireless power transmission" all share the same heads in the graph traversal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Exploring Attention Subgraphs</head><p>We allow the user to view subgraphs by selecting individual tokens at arbitrary layers. For a given selected token, we highlight tokens at subsequent layers that depend on the selection, as well as all tokens at prior layers that the selection depends on (T1), please see <ref type="figure" target="#fig_3">Fig. 6(a)</ref> for an example. A pair of tokens are considered dependent if there exists a path between them in the Attention Graph. By default, edges in the graph are formed if any attention head exists connecting a pair of tokens; for investigating specific heads, a user may select a head glyph for a given token, which limits connectivity to that specific head and given layer, shown in <ref type="figure" target="#fig_3">Fig. 6(b)</ref>. At all other layers, all heads are still used for determining paths.</p><p>Head selection is enacted via mouse hovering, as well as clicking, in order to freeze the current selection. Further, to select token sequences, we support brushing within layers, where we highlight the intersection of the attention graphs for the brushed tokens. The intersection is performed at each layer before continuing traversal at the next layer. This can be used to assess whether contiguous text phrases (e.g. named entities) have similar attention dependencies.</p><p>In the combined view, we offer a consistent form of hovering as with the single model view: hovering over a token will still highlight the attention traversal using the attention heads of that token. We perform traversals for the present attention heads of each model for the hovered token, union the resulting tokens, and highlight all tokens. We treat the attention graph of each model as separate entities, so the traversal for each model is computed separately (see <ref type="figure">Fig. 5</ref>). Thus, hovering over a token with attention heads from only one model will result in identical results to the single model view. This can be used to understand global similarities/differences between models (T3-b), e.g. for a token in a shallow layer, what tokens are dependent on it in deeper layers, and which models contain such dependencies. For a coherent user experience, we keep the color scheme uniform with the Sentence Context view and the head glyph coloring (pre-trainedturquoise, fine-tuned -purple, overlap -orange).</p><p>Hovering over head glyphs still limits traversals to that attention head, except the split head glyphs now consist of two separate hover targets. We allow users to both select individual models by hovering over the appropriate glyph, as well as to depict model comparisons via holding an appropriate keybinding when hovering over either glyph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Querying Head Functionality</head><p>In order to provide a more intuitive means of assessing head functionality, we allow the user to select tokens across layers, highlighting the set of heads through which attention flows (T2). Further, for tokens that are not in adjacent layers, we highlight intermediate tokens on the path between the selected tokens, along with their respective attention heads, as shown in <ref type="figure">Fig. 8</ref>. If multiple tokens are selected in either layer, <ref type="figure">Fig. 9</ref>: We showcase our interface for the task of recognizing textual entailment. By selecting the words "receive" and "go", we highlight how attention flows between the two sentences, aggregating information that is necessary to determine that "will receive" implies "proceeds go to", regarding "hepburn's family". the intersection path, or intermediate tokens that are attended to by all selected tokens, is shown.</p><p>In the combined model view, selecting tokens in different layers behaves consistently with the single model view. We traverse the attention graph for the BERT model and fine-tuned model separately, searching for paths through attention heads connecting the selected tokens with tokens in the path highlighted, with our consistent color scale according to which model's attention graph from which it was extracted. Paths are not required to join all selected tokens but only those included as nodes in their respective attention graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We showcase Attention Flows through a set of use cases and user feedback. Our focus is on highlighting model differences in BERT, e.g. what did fine-tuning learn for a particular task? To this end, our method is used on a benchmark set of supervised NLP tasks.</p><p>Our experiments are conducted in a similar manner to <ref type="bibr" target="#b10">[11]</ref>. We use the pre-trained, uncased BERT BASE model, a Transformer model with 12 layers and 12 attention heads per layer. Fine-tuning BERT is straightforward as task specific inputs to BERT, e.g. hypothesispremise pairs in entailment, are analogous to sentence pairs during pre-training (see Sec. 2.1). For fine-tuning, given a sentence understanding task, we introduce a linear layer for classification, which takes as input the [CLS] token embedding from the very last layer of the model. We represent the input sequence as a sequence pair, separated by a reserved [SEP] token, and use the final hidden vector x <ref type="bibr" target="#b11">12</ref> CLS ∈ R d , corresponding to a reserved [CLS] token, as an aggregate representation of the sentence <ref type="bibr" target="#b10">[11]</ref>. We compute a standard classification loss (cross entropy) using the introduced linear transformation W ∈ R k×d , where k is the number of labels, introduced in the fine-tuning classification layer. During training, we update both W and the Transformer's model weights, via stochastic gradient descent using the Adam optimizer <ref type="bibr" target="#b23">[24]</ref>. We obtain comparable development accuracy on all GLUE tasks to those reported in Devlin et al. <ref type="bibr" target="#b10">[11]</ref>, specifically, 91% for QNLI, 86.5% for MPRC, and 68.6% for RTE, tasks that we detail next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>The General Language Understanding Evaluation (GLUE) <ref type="bibr" target="#b50">[51]</ref> benchmark is a collection of sentence understanding tasks designed to evaluate the performance of NLP models. We evaluate our tool by in- <ref type="figure">Fig. 10</ref>: In this entailment example, the first sentence does not entail the second. Here, the user selects "qatar" in the entailed (a) and entailing (b) sentence to understand how attention flows from these words to "doha". The fine-tuned model attends to the past participle "located in" (a) that comprises the hypothesis, while the entailing (b) sentence shows attention that describes doha as the "capital city" of qatar. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Use Case: Textual Entailment</head><p>In <ref type="figure">Fig. 9</ref> we show an example of how to use Attention Flows to understand differences learned during fine-tuning for textual entailment. In this example, the first sentence entails the hypothesis found in the second, as "will receive the proceeds" implies "proceeds go to", both in reference to family. Hence, the user selects the token "go" at the  <ref type="figure">Fig. 11</ref>: Here we show examples for recognizing question answer pairings. In (a), the answer span ("more than 50%") attends to "what", followed by the full question span. Conversely (b), selecting "what year" shows a dependency for the answer span ("1950s") in the proceeding layer. We demonstrate our interface for recognizing that one sentence fails to paraphrase another. In (a), we find that selecting "said" has a dependency on the two distinguishing statements: "claims were preposterous" and "address the court". In (b), selecting one of the distinguishing text spans "looking to" shows attention flowing to the other distinguishing span: "solid chance to". penultimate layer and "receive" at an earlier layer, to understand the model's attention flows between these tokens. The proceeding layer from the "receive" selection highlights how both models attend to tokens that belong to the first sentence. However, at the next layer, we see that the fine-tuned model's attention crosses to the second sentence, picking up on the relevant text span to determine entailment. This layer, and the next, gathers the necessary tokens for the task ("family" then "hepburn"), before arriving at "go". Note that BERT's pre-trained attention contains heads for the aforementioned layer's distinguishing text span, but for this specific selection, only the heads that correspond to the fine-tuned model are enacted. Furthermore, we observe that the same head in the fine-tuned model is used for this text span.</p><p>In <ref type="figure">Fig. 10</ref> we show an example where the hypothesis in the second sentence does not lead to entailment. The user selects "doha" from the entailed sentence, and selects "qatar" from the entailed (a) and entailing (b) sentences to understand the relationship between these entities via attention flows from "qatar" to "doha". Here, we find that the finetuned model places more importance on the past participle "located in", compared with the pre-trained model, and thus "doha" is likely to be more informed by this description. Further, we find that "capital city" is attended to in the fine-tuned model, but not the pre-trained model, and this is a key phrase that determines the absence of entailment.</p><p>From these examples, and others, we make several observations. First, we find that the pre-trained model tends to focus on nouns ("hep-burn", "family", "sale"), while the fine-tuned model only does so if it is relevant to the task ("qatar" and "doha"). Further, we find that the fine-tuned model tends to focus on prepositions ("to", "from", "of"), verbs ("receive", "located in"), and possessive phrases. These parts of speech help establish relationships between nouns, e.g. named entities, and are common to the hypotheses involved in entailment, e.g. whether or not two entities are performing the same task. <ref type="figure">Fig. 11</ref> shows an example of model comparison for the task of question answer verification. In <ref type="figure">Fig. 11(a)</ref>, the user first selects the answer "more than 50%". Inspecting its dependent tokens, we see that the immediately preceding layer corresponds to the question of "what", and the layer further back highlights the remainder of the text span of the question. Hence, we see that the answer text span is able to gather the appropriate question information through multi-layer reasoning. <ref type="figure">Fig. 11(b)</ref>-left considers the converse: if we select the tokens that identify the question ("what year"), will this be made available to the answer span? Indeed, we find that the object of the answer in the next layer, "1950s", depends on "what year". On the other hand, the pre-trained model fails to attend to the answer across multiple layers, as indicated by most attention heads colored purple for the answer span. Further, on the right side of <ref type="figure">Fig. 11(b)</ref>, selecting the token "declined" in the answer shows that it depends on "drop in applications" from the question in preceding layers, and these phrases link the two sentences together in reference to "applications".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Use Case: Question Answer Verification</head><p>These two examples highlight a pattern in many cases of QNLI where the classification is positive (e.g. the second sentence does answer the question in the first sentence), namely that the question aspect of the first sentence (e.g. one of the "5 W's") serves as a bridge to the second sentence and frequently, the text span corresponding to the answer. We note that the answer spans are not technically required for answering the question, yet this information is nevertheless learned via fine-tuning, indicating that the model is not merely picking up on task-irrelevant details to solve the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Use Case: Paraphrasing Verification</head><p>Last, we analyze fine-tuning for the problem of paraphrase verification, please see <ref type="figure">Fig. 12</ref>. For the negative (e.g. not a paraphrasing) example in <ref type="figure">Fig. 12(a)</ref>, the user first selects the token "said", and we can observe that the fine-tuned model attends to what the person said in both sentences. Moreover, only the fine-tuned model attends to the tokens that differentiate the sentences, e.g. "claim were preposterous" and "address court". <ref type="figure">Fig. 12</ref>(b) highlights another negative paraphrasing example, where the user brushes the text span "looking to" in the second sentence. We find that the fine-tuned model attends to the text span in the first sentence from the proceeding layer -"solid chance to" -that distinguishes the sentences. Note, the pre-trained model does not attend to any tokens in the first sentence, instead expanding attention to nearby tokens. We also find that the [CLS] token for the pre-trained model tends to place importance on matched text spans between the sentences, e.g. "funny cide", "triple crown", whereas the fine-tuned model seeks relevant, distinguishing tokens. For instance, "horse racing" is not found in the first sentence, but this phrase does not distinguish the two sentences, and in fact, the pre-trained model places more importance on this phrase as shown in the sentence view.</p><p>For the MRPC task, our experiments show that, for negative samples, the fine-tuned BERT model attends to words that distinguish the two sentences while the pre-trained BERT model does not. Existing research <ref type="bibr" target="#b5">[6]</ref> claims that, because the BERT model computes word embeddings that are contextualized with regards to the sentence, the model will learn increasingly abstract representations as the sentence goes through deeper layers, and thus, word embeddings in the last few layers will lose their identity. However, our experiments show that, in the case of the sentence-pair paraphrasing task, the BERT model still retains the identities of the tokens in deeper layers, which enables the model to attend to words of the two sentences that distinguish them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">User Feedback</head><p>To verify the effectiveness of our design, we collected user feedback from two different groups. The first group consisted of experts in NLP, specifically three researchers within academia, where we aimed to answer the following: for domain experts, is our interface intuitive, easy to use, and helpful for model comparison? The second group consisted of a broader population, where we crowdsourced participants (total of seven) with a self-reported undergraduate education in Computer Science, aiming to answer the following: for potential non-experts, how effective is our interface for assessing differences between models?</p><p>To gather feedback, we directly recruited NLP experts, while we used the crowdsourcing platform Prolific 1 to recruit the broader audience. In both cases, we provided a brief explanation of the interface, provided a set of sentences from the above 3 tasks, and asked participants to freely use the interface. Participants were then asked to provided survey responses on: (a) their findings in using the interface, and (b) the usability of the interface. We summarize the group-specific findings:</p><p>NLP Experts: Overall, the NLP researchers enjoyed the visualization. One researcher mentioned that they understood the visual encodings "without having to read the instructions", and found the interface useful for identifying differences between pre-training and fine-tuning. Another researcher found the sentence summary useful for providing overviews but acknowledged that the central visualization was rather complex to understand. The third researcher liked how the interface compares "pre-trained and fine-tuned models via colors, it helps show patterns quickly", and expressed interest in using the interface for more general model comparisons, beyond fine-tuning.</p><p>Crowdsourced Participants: In using the interface, the participants all found the fine-tuned model attended to task-relevant details better than the pre-trained model, specifically, dealing with "medium-sized sentences", better handling second sentences in tasks "in terms of head attention counts", recognizing "importance of numbers", handling "verbs and objects in sentences", and handling actions where "someone was doing something". One participant thought, in contrast, that the pretrained model was "gibberish-ish all the way". Regarding usability, two participants mentioned that it took a while to understand, but eventually, became easy to use. One participant mentioned that toggling views made it easier to comprehend the main view, and "highlighting tokens was somewhat helpful in understanding the token selection process." Other participants, however, did find certain aspects difficult to use, with the purpose of certain interactions not being evident.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS AND FUTURE WORK</head><p>We introduced Attention Flows: a tool that supports exploration into how self-attention of Transformer models is refined during fine-tuning, and how self-attention informs classification decisions. Through use cases of our interface, we show how self-attention evolves to address task-specific details, while our user feedback validates the usability, and potential insights, provided by the interface.</p><p>We plan to explore several avenues of future work. In our design, we only consider self-attention matrices and do not consider other aspects of the Transformer model, such as contextualized embeddings. We plan on linking attention functionality with word embeddings to provide a more comprehensive view. Further, we plan to extend our approach to more general model comparison, e.g. models of different pre-training objectives <ref type="bibr" target="#b21">[22]</ref> or optimization schemes <ref type="bibr" target="#b31">[32]</ref>.</p><p>Our tool supports the comparison of a single sample across models, but a limitation with our current interface is how such samples are selected. We offer a simple interface for browsing samples according to the model's classification, yet a way to summarize and browse a collection of samples would be more useful for the end user. For future work, we intend to use our scheme for identifying attention heads as a means of querying sentence pairs that contain similar patterns in self-attention, in order to find and compare different samples. Such an interface can facilitate the investigation of existing benchmark datasets <ref type="bibr" target="#b50">[51]</ref>, ensuring that a user's discoveries holds across many samples, as a means to identify shortcomings in pre-training or fine-tuning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>At the last layer, the tokens that [CLS] depends on (c.f. Fig. 3) are encoded in a ring (left), where the attention heads for [CLS] are shown in the center. This design is carried over to previous layers (right), where per-token attention information is depicted, along with the positions of attended tokens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>Display of attention propagation while hovering over a token: (a) shows hovering over a single token and (b) shows hovering over a head glyph for propagation through only that head.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 :</head><label>7</label><figDesc>Head filtering: a user selects token "percentage" at a specified layer denoted L, and tokens "than", "50", &amp; "%" via brushing in the subsequent layer L + 1. For each selected token in layer L + 1, heads connecting to selected tokens in layer L have their glyphs highlighted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>vestigating self-attention on three of the nine sentence-pair, binary classification-based, GLUE tasks: RTE, MRPC, QNLI. RTE (Recognizing Textual Entailment) asks the model if the second sentence is an entailment of the first. For example, the sentence-pair [CLS]Mount Olympus towers up from the center of the earth. [SEP] Mount Olympus is in the center of the earth. [SEP] is classified positive since the second sentence can be inferred from the first sentence. QNLI (Question Natural Language Inference) asks the model to determine if the second sentence contains the correct answer to the question in the first sentence. For example, the sample [CLS] What percentage of this farmland grows wheat? [SEP] More than 50% of this area grows wheat. [SEP] is classified as positive. MRPC (Microsoft Research Paraphrase Corpus) asks the model to determine if two sentences have the same meaning. For example, the pair [CLS] It affected earnings per share by a penny. [SEP] The company said this impacted earnings by a penny a share. [SEP] is classified positive since the second sentence paraphrase the first.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Fig. 12: We demonstrate our interface for recognizing that one sentence fails to paraphrase another. In (a), we find that selecting "said" has a dependency on the two distinguishing statements: "claims were preposterous" and "address the court". In (b), selecting one of the distinguishing text spans "looking to" shows attention flowing to the other distinguishing span: "solid chance to".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Fig. 3: The Attention Graph is built by adding edges between tokens that contain high attention. The [CLS] token (top) is the sole node for the last layer</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Layer 12</cell></row><row><cell>[CLS] Q</cell><cell>: Are we not men ?</cell><cell>A</cell><cell>: We are Devo !</cell><cell>Head 1</cell></row><row><cell>[CLS]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>[CLS] Q</cell><cell>: Are we not men ?</cell><cell>A</cell><cell>: We are Devo !</cell><cell>Head 4</cell></row><row><cell>[CLS]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Layer 11</cell></row><row><cell>[CLS] Q</cell><cell>: Are we not men ?</cell><cell>A</cell><cell>: We are Devo !</cell><cell>Head 1</cell></row><row><cell>not</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>men</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>A</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Devo</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>!</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weighted graph comparison techniques for brain connectivity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Henry Riche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Isenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-D</forename><surname>Fekete</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI conference on human factors in computing systems</title>
		<meeting>the SIGCHI conference on human factors in computing systems</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="483" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Visual graph comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wohlfahrt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wurzinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th International Conference Information Visualisation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="62" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On identifiability in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brunner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wattenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cashman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mosca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12545</idno>
		<title level="m">Rnnbow: Visualizing learning via backpropagation gradients in recurrent neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Aila: Attentive interactive labeling assistant for document classification through attention-based deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2019 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">What does bert look at? an analysis of bert&apos;s attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="276" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visual interaction with deep learning models through collaborative semantic inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krüger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="884" to="894" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Considerations for visualizing comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="413" to="423" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Visualizing and understanding the effectiveness of bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05620</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep semantic role labeling: What works and what&apos;s next</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="473" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gamut: A design probe to understand how data scientists understand machine learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Head</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Deline</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Drucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2019 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visual analytics in deep learning: An interrogative survey for the next frontiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pienta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2674" to="2693" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Summit: Scaling deep learning interpretability by visualizing activation and attribution summarizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H P</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1096" to="1106" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">exbert: A visual analysis tool to explore learned representations in transformers models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hoover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.05276</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spanbert: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Visual summaries for graph collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Freire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Pacific Visualization Symposium (PacificVis)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="57" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Revealing the dark secrets of bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rumshisky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4356" to="4365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Padnos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shoham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05646</idno>
		<title level="m">Sensebert: Driving some sense into bert</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Questioning the ai: Informing design practices for explainable ai user experiences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gruen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Miller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.02478</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Open sesame: Getting inside bert&apos;s linguistic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Tana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Frankb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">241</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Towards better analysis of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="100" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Nlize: A perturbation-driven visual interrogation tool for analyzing and interpreting natural language inference models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Srikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pascucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-T</forename><surname>Bremer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="651" to="660" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Linzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3428" to="3448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Černockỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eleventh annual conference of the international speech communication association</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Understanding hidden memories of recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Visual Analytics Science and Technology (VAST)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deepcompare: Visual and interactive comparison of deep learning model performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Murugesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE computer graphics and applications</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="47" to="59" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sanvis: Visual analytics for understanding self-attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Visualization Conference (VIS Short Papers)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="146" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence tagging with bidirectional language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Power</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1756" to="1765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deepeyes: Progressive visual analytics for designing deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pezzotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Höllt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>Lelieveldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eisemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vilanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="108" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Visualizing and measuring the geometry of bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Viegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coenen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8592" to="8600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rumshisky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12327</idno>
		<title level="m">A primer in bertology: What we know about how bert works</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Is attention interpretable?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2931" to="2951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Seq2seq-vis: A visual debugging tool for sequence-to-sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Behrisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="353" to="363" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Lstmvis: A tool for visual analysis of hidden state dynamics in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="667" to="676" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">How does bert answer questions? a layer-wise analysis of transformer representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Aken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Löser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1823" to="1832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">A multiscale visualization of attention in the transformer model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05714</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="353" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Haleem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Plantaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05285</idno>
		<title level="m">Cnncomparator: Comparative analytics of convolutional neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ernie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07129</idno>
		<title level="m">Enhanced language representation with informative entities</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
