<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HypoML: Visual Analysis for Hypothesis-based Evaluation of Machine Learning Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianwen</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Alexander</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Pegg</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huamin</forename><surname>Qu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Min</forename><surname>Chen</surname></persName>
						</author>
						<title level="a" type="main">HypoML: Visual Analysis for Hypothesis-based Evaluation of Machine Learning Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Visual analytics</term>
					<term>model-developmental visualization</term>
					<term>machine learning</term>
					<term>neural network</term>
					<term>hypothesis test</term>
					<term>HypoML</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Fig. 1. There are many invariance problems in machine learning. Some are easy and others are hard. For example, one may wish to know if a classification model is rotation-invariant. If the model is not so, one may use another model that can detect the rotation angle of an object or perform some rotation regularization. The detected rotation angle or regularized image is a piece of extra information about a &quot;concept&quot; that the original model may or may not know. With HypoML, one can conduct a set of structured tests, obtained automated statistical and logical analysis of the results, and visualize the conclusions about the hypotheses related to the concept.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In computer vision, data mining, and machine learning (ML), a feature is a measurable variable that characterizes a particular kind of property or attribute of a data object (e.g., an image, a time series, a multivariate record, etc.). Many technical solutions in these fields heavily rely on model-developers' knowledge about various features and include human-centric feature engineering as a critical process in a model development workflow <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8]</ref>. On the other hand, from an AI perspective, it is desirable for ML to minimize the dependence on the human knowledge of potentially useful features <ref type="bibr" target="#b20">[21]</ref>. Technical solutions, such as deep learning, aim to achieve this objective.</p><p>Despite of the success of deep learning and other ML methods without the need for humans to specify features, there have been some concerns about whether the learned "useful" features contribute towards undesirable biases and may actually undermine model performance <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b41">42]</ref>. The concerns may be about the selection of some variation types (e.g., variables such as colors, patterns, dates, etc.) or some variation ranges (e.g., shades of yellow, circles, summer time, etc.). Inevitably, model-developers have been interested in what features may have been learned by an ML model and how these features influence the model performance. This interest is especially high with respect to deep neural networks (NN) for computer vision tasks. A class of visualization techniques, such as neuron activation plot, filter plot, gradient ascent plot <ref type="bibr" target="#b32">[33]</ref>, Deconvolution <ref type="bibr" target="#b39">[40]</ref>, and their variants, have been widely used by developers to observe neurons and analyze their learned features. Since a NN typically consists of a huge number of neurons, the visual observation may encounter several obstacles, including time demand for viewing all neurons that may reveal some features, subjectivity and memory limitation of an observer, and uncertainty about the semantic meaning of an observed feature. More importantly, while most modeldevelopers have a non-trivial amount of knowledge about features that are potentially useful or harmful, their initiatives are limited to searching for patterns in many thousands of neuron-based plots and speculating if a feature has been learned.</p><p>In this work, we propose a new visual analytics approach that enables model-developers to use their knowledge and initiatives in hypothesising and evaluating if any feature may be useful or harmful, if such a feature is learned by a model, and how it may affect a learned model.   <ref type="bibr" target="#b32">[33]</ref> can help model-developers observe the pattern that a specified neuron has learned. However, even a small CNN has a huge number of neurons waiting to be inspected, while many patterns shown are not semantically interpretable. Meanwhile, model-developers are often unable to determine whether a pattern is useful or harmful. The CNN and gradient ascent plots shown are from an experiment by the authors.</p><p>In particular, we outline a framework for testing such hypotheses systematically, and describe the underlying statistical and logical analysis for inferring conclusions about multiple hypotheses from multiple sets of testing results. Because many model-developers may not be familiar with or remember the underlying statistical and logical analysis, we develop a visual analytics tool, HypoML, for carrying out analysis as well as for depicting the flow of inference ( <ref type="figure" target="#fig_5">Fig. 1</ref>), facilitating rapid observation of the conclusions and the logical flow between the testing data and hypotheses. HypoML itself is independent of ML models or input data types, though we report only its application to CNN models for image classification in this work. We have made HypoML available as open-source software, a demo is available at https://hypoml.bitbucket.io/ and the source code is available at https://bitbucket.org/hypoML/hypoml.bitbucket.io.</p><p>The term "feature" typically implies a piece of information contained in the original input data. Since HypoML can also be used to test a hypothesis about a piece of information that is not included in the original data, we will use the term "concept-based hypotheses" to describe what to be tested with HypoML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>While machine learning (ML) has an important role to play in visualization and visual analytics <ref type="bibr" target="#b8">[9]</ref>, almost every aspect of ML processes can benefit from visualization as shown by a recently established ontology VIS4ML <ref type="bibr" target="#b29">[30]</ref>. In general, when model-developers observe some phenomena in an ML process, such as its training and testing data, results, the inner states of a model, and the provenance of the learning process, they acquire new information to inform their various decisions that affect the ML process. As demonstrated quantitatively by Tam et al. <ref type="bibr" target="#b34">[35]</ref>, a model-developer can contribute a huge amount of knowledge (measured in bits) to an ML process through the use of visualization. This work focuses on the evaluation stage of ML workflows, aiming to enable ML developers, who are able to hypothesize about a model intelligently, to evaluate their hypotheses rigorously using a VA tool.</p><p>Methods for evaluating ML models fall into two main classes: blackbox analysis and white-box analysis. Here we focus our review of the previous works on model evaluation that feature visualization techniques. More comprehensive surveys on using visualization for ML can be found in the works of Zhang and Zhu <ref type="bibr" target="#b42">[43]</ref> and Hohman et al. <ref type="bibr" target="#b11">[12]</ref>.</p><p>Black-box analysis enables users to investigate and evaluate ML models without knowing the internal working mechanism. Statistic metrics (e.g., accuracy, recall), ROC curve, and confusion matrices are widely-used black-box analysis and have commonly been provided as built-in functions in ML environments. To aid the aggregated statistical analysis, researchers recently proposed visualization techniques to support black-box evaluation of ML models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b40">41]</ref>. For example, Squares <ref type="bibr" target="#b27">[28]</ref> juxtaposes a set of histograms to present an instancelevel visualization for models in multi-class classification problems. Manifold <ref type="bibr" target="#b40">[41]</ref> employs a scatterplot-based visual technique to assist in the comparison between multiclass classifiers. Among black-box analysis, what-if analysis <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b36">37]</ref> is the most relevant to our study.</p><p>What-if analysis examines hypotheses about how perturbations to inputs affect the ML model outputs, and has been supported by visualization tools, e.g., What-if tool <ref type="bibr" target="#b36">[37]</ref>, Prospector <ref type="bibr" target="#b18">[19]</ref>, and GAMUT <ref type="bibr" target="#b10">[11]</ref>. These tools allow users to drill down into specific input data points, manipulate their feature values, and examine the effect of such manipulations. Our work focuses on statistically-meaningful "what-if" analysis.</p><p>White-box analysis, on the contrary, opens the black box and displays the internal states of ML models. A number of visualization tools have been proposed to support white-box analysis of different ML models, including MLP <ref type="bibr" target="#b26">[27]</ref>, CNNs <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>, deep generative models <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b35">36]</ref>, and RNNs <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b33">34]</ref>. Although these tools have utilized some of the most sophisticated visual representations and have assisted model-developers in evaluating, understanding, and explaining their models, comprehending a huge number of high-dimensional internal states is naturally challenging for humans.</p><p>In addition, researchers proposed techniques to summarize information about internal variables and present the summary information visually. Especially in computer vision, salience-based methods, such as CAM <ref type="bibr" target="#b43">[44]</ref>, Grad-CAM <ref type="bibr" target="#b30">[31]</ref>, and guided back propagation <ref type="bibr" target="#b32">[33]</ref>, identify discriminative regions in the input image and thus highlight important features for a certain prediction. However, these saliencebased methods can only offer explanations for specific predictions but cannot confirm whether or not a concept has been learned. To offer instance-independent explanation, Yosinski et al. employed gradient ascent plots <ref type="bibr" target="#b38">[39]</ref> to depict the patterns that an individual neuron has learned. <ref type="figure" target="#fig_1">Fig. 2</ref> illustrates a small selection of gradient ascent plots being observed in conjunction with a CNN. However, even for such a simple model, there are a huge number of neurons, it is impossible for model-developers to conduct a full examination. Moreover, the depicted pattern would provide largely a hunch, but not a proof whether a certain concept is useful or not to the classification task. Perhaps the most relevant to our work is TCVA <ref type="bibr" target="#b17">[18]</ref>, which learns human-friendly concepts from an already trained model and conducts hypothesis testing. However, TCVA requires a time-consuming process to label the concept across the whole dataset.</p><p>In this work, we propose a novel ML-testing framework that combines black-box and white-box analysis. Whether an ML model has learned a concept or feature is a typical "internal problem" that is to be investigated using white-box analysis. The new framework allows model-developers to investigate such "internal problems" in a manner of black-box analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CONCEPT-BASED TESTING OF ML MODELS</head><p>Let M be an ML model that transforms an input data d ∈ D to an output decision. A concept ξ is a variable that is not explicitly defined in D, but is hypothesized by an ML model-developer to be related to the quality of the output decision. <ref type="figure" target="#fig_3">Fig. 3</ref> shows several examples of concepts. We can observe that some concepts may be extracted from the original data objects using known techniques, while it may be almost impossible to infer some other concepts from the data objects.</p><p>As long as M has a finite number of constructs (e.g., neurons or  tree nodes) or receives input data with finite informative dimensions, there will always be some concepts that M cannot learn. Inevitably, most model-developers will ponder about some concepts in relation to a learned model M. Considering the examples in <ref type="figure" target="#fig_3">Fig. 3</ref>, one may ask: a. Would having an extended field of view be useful for recognizing an object captured from a less ideal viewing angle? b. Would another model for detecting an anomalous background or some scale inconsistency be useful to differentiate a toy from a real building? c. Would another model that is able to detect an object in an unusual position and estimate the rotation angle be useful to the recognition of the object? d. Would having additional information about a geographical context improve the accuracy of building recognition?</p><p>One can easily imagine many other questions about different types of extra information, such as different meta-data, multiple data capture modalities, and various pre-processing techniques. All these questions are essentially hypotheses. Just as in psychology, healthcare, social science, and many other disciplines, one can conduct experiments to evaluate such hypotheses. Indeed, one can test ML models against many thousands of data objects in comparison with tens of stimuli in typical empirical studies. However, since an ML experiment typically features many uncontrolled variables, having a lot of testing data does not imply that the testing result will be statistically meaningful.</p><p>Because model testing is a routine operation in ML, it is desirable to establish a structured method such that many model-developers can adopt the same method and produce comparable testing results. As the above definition of concept is relatively broad, developers of different ML models in various applications can benefit from open source or commercial software for supporting such a structured testing method. <ref type="figure" target="#fig_2">Fig. 4</ref> illustrates the framework for concept-based hypothesis testing. Given an ML process and a training and testing dataset, a modeldeveloper is interested to know how some extra information about a concept may affect the ML process and the learned model. The framework thus requires the developer to invoke two ML processes that receive two pieces of input data. As shown on the left of <ref type="figure" target="#fig_2">Fig. 4</ref>, both processes take the original training data D m as one piece of the data. For the other piece, one process takes random noise as its input, while the other takes extra data about a concept (denoted by the sign "+"). In general, the extra data for training M+ can be in any form, as long as there is a matching form of noise for training M. In practice, when testing a specific model M, it is cost-effective to fix the data structure of the + part, allowing M and M+ to have the same fixed model structure for testing many concepts. Such examples will be given in Section 6.</p><p>The framework then requires the model-developer to test each model with two runs using the exactly the same ML process and configuration. As illustrated in the middle column of <ref type="figure" target="#fig_2">Fig. 4</ref>, one testing run uses testing data D that does not have extra data, while the other run uses testing data D+ that include extra data. The two runs with M thus produce two sets of results, R M,D and R M,D+ , while the two runs with M+ produce R M+,D and R M+,D+ . Because evaluating an ML model typically involves testing many thousands of data objects, some computational analysis of the four sets of results will be necessary. HypoML is designed to automate the computational analysis. In particular, it provides statistical and logical analysis for evaluating a set of hypotheses. The statistical analysis is based on the well-established method for hypothesis testing, while the logical analysis is formulated in this work for reasoning about the intertwining relationships between 12 hypotheses and 6 statistical conclusions drawn from different pairs of results. To assist users in understanding such complex relationships, HypoML provides a purposely-designed visual representation, which enables users to trace the conclusion of each hypothesis to related statistical analysis and to the corresponding testing results.</p><p>The 12 hypotheses are listed on the right of <ref type="figure" target="#fig_2">Fig. 4</ref>. While the first a few are what many model-developers may hypothesize, the others are the "side products", about which the tests may possibly offer some inference. The primary goal of HypoML is to enable model-developers to evaluate their hypotheses rather than for a novice audience. For those model-developers who know their models and ML processes, it is not difficult to interpret these 12 hypotheses and selectively pay attention to a subset relevant to a particular model or a testing experiment.</p><p>The first two hypotheses, H 1 and H 2 , are about whether the concept concerned is useful (or harmful) to M+, and would be useful (or harmful) to M. Although the conclusions for these two hypotheses cannot in principle be both positive, each can also be inconclusive. We thus follow the convention of hypothesis testing by listing them as separate hypotheses, each can be independently confirmed, rejected, or unproven (inconclusive). We also anticipate that more testing and analysis methods may be developed in the future, which may support or reject those apparently-paired hypotheses asymmetrically. Having separate hypotheses will not hinder such advancement.</p><p>H 3 hypothesizes that model M has already learned the concept adequately, while H 4 hypothesizes that model M+ has learned the concept adequately. For H 3 , the adverb "adequately" implies that the concept can be learned by a model, such as M, without the need for any extra data about the concept. For H 4 , the adverb "adequately" implies that M+ would perform worse without the extra data of the concept. When the conclusion is in favor of M+, it suggests a potential for improving M, but should be interpreted that M+ has a better structure.</p><p>As shown in <ref type="figure" target="#fig_2">Fig. 4</ref>, model M is trained with the original data and random noise for the + part. Although the + part provides an extra signal channel, noise is not expected to provide extra information. Hence, M is not expected to be affected by the noise signal. In other words, there should be no significant difference between the testing results R M,D+ and R M,D . However, as a scientific exercise, one cannot take this assumption for granted due to some unexpected configuration or implementation errors. H 5 and H 6 are thus designed to examine if M is affected positively or negatively by the extra data during testing.</p><p>M+ is trained using the original data and the + part encoding the concept being tested. H 7 and H 8 are designed to test whether the extra information in the + part has a positive or negative effect on M+.</p><p>Meanwhile, as long as any part of an ML model can learn, directly or indirectly, from the information in both the original data and the + part, learning from one part of the data could be affected by the information in another part. Assume the model template for M and M+ is constructed based on an original model templatem without the extra data input. For M+, we define its M part as all components of the model (e.g., neurons and connections in NNs) belonging tom, and its + part as those added components (i.e., not inm). H 9 , H 10 , H 11 , and H 12 are for investigating the trade-off between the two parts of M+ in the development of its intelligence. Depending on the design of the model template or architecture, the M and + parts of M+ can be closely integrated or rather separate. When the two parts are more integrated, one should consider two parts as functional units rather than geometric or topological regions. For developers who know the structures of their models, it will not be difficult for them to interpret these hypotheses correctly. Similarly, we separate H 9 from H 10 , and separate H 11 from H 12 because of the inconclusive state in each case.</p><p>Evaluating hypotheses H 9 , H 10 , H 11 , and H 12 can also aid investigation into different strategies for information fusion in ML, ranging from multi-model decisions at higher layers (e.g., <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14]</ref>) to multi-sensor information fusion at lower layers (e.g., <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29]</ref>). We will show an example of instigating different fusion strategies in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">STATISTICAL AND LOGICAL REASONING OF HYPOTHESES</head><p>As shown in <ref type="figure" target="#fig_2">Fig. 4</ref> Given two sets of results, R a and R b , we assume that the tuples in the two lists are paired, i.e., the id entries are in the same order exactly. We can compare R a and R b with their accuracy, i.e., the average of correctness with uncertainty. As testing in ML often shows small variations of accuracy, it is necessary to measure the statistical significance, providing the probabilities of Type I and Type II errors. Although paired, one-tail t-test can be used for this purpose, HypoML uses paired, two-tail t-test in order to maintain consistent interpretation of p-values and prevent setting an overgenerous threshold p-value by mistake.</p><p>Let us introduce the following notation to denote the possible outcomes of the statistical analysis between R a and R b .</p><p>• R a R b -significantly lower than • R a R b -significantly higher than • R a ≈ R b -insignificant higher or lower than Clearly, reasoning about these relations is time consuming and error prone. In order to support the frequent analytical tasks of the developers <ref type="table">Table 1</ref>. The relations between statistical analysis and hypotheses.</p><formula xml:id="formula_0">• R a R b -R a R b or R a ≈ R b , but not R a R b . • R a R b -R a R b or R a ≈ R b ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis</head><p>Condition Hypothesis To simplify our logical formulae further, we will not explicitly list those inconclusive hypotheses. With this simplification, we can now express the inferences as a combined set of logical rules: </p><formula xml:id="formula_1">A 1 : R M+,D+ v. R M,D H 1 , H 2 , H 3 , H 4 , H 7 , H 8 A 2 : R M+,D+ v. R M,</formula><formula xml:id="formula_2">A 1 : Comparing R M+,D+ and R M,D may conclude: • R M+,D+ R M,D =⇒ (H 1 ) ∧ (H 4 ) ∧ (H 7 ) ∧ ⊥(H 2 ) ∧ ⊥(H 3 ) ∧ ⊥(H 8</formula><formula xml:id="formula_3">R M,D =⇒ (H 2 ) ∧ (H 4 ) ∧ ⊥(H 1 ) ∧ ⊥(H 3 ).</formula><p>Analysis A 2 cannot draw conclusions about H 5 and H 6 , but its conclusion may depend on them. In general, there is a common-sense assumption that neither H 5 nor H 6 is likely to be true.</p><p>A 2 : Comparing R M+,D+ and R M,D+ may conclude:</p><formula xml:id="formula_4">• R M+,D+ R M,D+ =⇒ (i) if ⊥(H 6 ) then (H 1 ) ∧ (H 4 ) ∧ (H 7 ) ∧ ⊥(H 2 ) ∧ ⊥(H 3 ) ∧ ⊥(H 8 ); or (ii) if (H 6 ) then (H 1 ) ∧ (H 4 ) ∧ (H 7 ) ∧ ⊥(H 2 ) ∧ ⊥(H 3 ) ∧ ⊥(H 8 ); or (iii) if (H 6 )</formula><p>. This offers an explanation but it is against a common-sense assumption that H 6 is unlikely to be true, and should be treated cautiously.</p><formula xml:id="formula_5">• R M+,D+ R M,D+ =⇒ (i) if ⊥(H 5 ) then (H 2 ) ∧ (H 4 ) ∧ (H 8 ) ∧ ⊥(H 1 ) ∧ ⊥(H 3 ) ∧ ⊥(H 7 ); or (ii) if (H 5 ) then (H 2 ) ∧ (H 4 ) ∧ (H 8 ) ∧ ⊥(H 1 ) ∧ ⊥(H 3 ) ∧ ⊥(H 7 ); or (iii) if (H 5 )</formula><p>. This offers an explanation but it is against a common-sense assumption that H 5 is unlikely to be true, and should be treated cautiously.</p><p>Because analysis A 3 does not compare M+ with M, the conclusion is limited to the context of M+. Mathematically, it is possible for A 3 to conclude that the concept is useful in the context of M+, while A 1 or A 2 concludes that the concept is harmful or is neither useful nor harmful. Considering this limitation, it is unsafe for this analysis to draw a conclusion about H 1 and H 2 . Meanwhile the analysis depends on the conclusions of H 1 and H 2 in a small way. </p><formula xml:id="formula_6">• R M+,D+ R M+,D =⇒ (i) if (H 1 ), then (H 7 ) ∧ (H 9 ) ∧ ⊥(H 8 ) ∧ ⊥(H 10 ); or (ii) if (H 1 ), then (H 9 ) ∧ ⊥(H 10 ); or (iii) if ⊥(H 1 ), then (H 12 ) ∧ ⊥(H 11 ). • R M+,D+ R M+,D =⇒ (i) if (H 2 ), then (H 8 ) ∧ (H 10 ) ∧ ⊥(H 7 ) ∧ ⊥(H 9 ); or (ii) if (H 2 ), then (H 10 ) ∧ ⊥(H 9 ); or (iii) if ⊥(H 2 )</formula><p>, then (H 10 )∧⊥(H 9 ). This conclusion is against a common-sense assumption that a useful concept normally should not affect the extra part of M+ negatively, and should be treated cautiously.</p><p>Analysis A 4 is relatively easy to reason, and it is useful for investigating if the part of model M+ for handling the original data D becomes less capable due to the training with extra information.</p><formula xml:id="formula_7">A 4 : Comparing R M+,D and R M,D may conclude: • R M+,D R M,D =⇒ (H 11 ) ∧ ⊥(H 12 ). • R M+,D R M,D =⇒ (H 12 ) ∧ ⊥(H 11 ).</formula><p>A 5 cannot draw conclusions about H 5 and H 6 , but its conclusion may depend on them. In general, there is a common-sense assumption that neither H 5 nor H 6 is true. </p><formula xml:id="formula_8">R M+,D R M,D+ =⇒ (i) if ⊥(H 6 ) then (H 11 ) ∧ ⊥(H 12 ); or (ii) if (H 6 ) then (H 11 ) ∧ ⊥(H 12 ); or (iii) if (H 6 )</formula><p>. This offers an explanation but it is against a common-sense assumption that H 6 is unlikely to be true, and should be treated cautiously.</p><formula xml:id="formula_9">• R M+,D R M,D+ =⇒ (i) if ⊥(H 5 ) then (H 12 ) ∧ ⊥(H 11 ); or (ii) if (H 5 ) then (H 12 ) ∧ ⊥(H 11 ); or (iii) if (H 5 )</formula><p>. This offers an explanation but it is against a common-sense assumption that H 6 is unlikely to be true, and should be treated cautiously.</p><p>Analysis A 6 is the only comparison that may inform the evaluation of H 5 or H 6 . In general, there is a common-sense assumption that neither H 5 nor H 6 is true if the model template or architecture was correctly defined, the correct ML method was followed, and the correct ML process was executed. When H 5 or H 6 is confirmed, it usually suggests some imperfection of the model template or learning process. Therefore the conclusions of A 6 should not be interpreted as their face values. However, the evaluation of H 5 nor H 6 is necessary since A 2 and A 5 depend on them.</p><formula xml:id="formula_10">A 6 : Comparing R M,D+ and R M,D may conclude: • R M,D+ R M,D =⇒ (H 5 ) ∧ ⊥(H 6 ); • R M,D+ R M,D =⇒ (H 6 ) ∧ ⊥(H 5 ).</formula><p>Because of the dependency among the six sets of analysis, the computation of the logical inference must follow an appropriate order, which is summarized as follows: STEP 0: Initialize the indicator of each hypothesis to 0. Clearly, given A 1 ∼ A 6 , it is not feasible for a user to perform rigorous reasoning defined by these logical inference rules. Hence it is necessary to introduce visualization to aid the automated analysis. It is helpful for model-developers to make quick observation about the analysis and conclusions. It will also be useful for the modeldevelopers to convey the outcomes of the test to other stakeholders, such as fellow model-developers and sometimes users of the ML models being evaluated. It can be difficult for some model-developers and many of ML users to remember and reason the complicated relationships among experiment results, statistic and logical analysis, and multiple hypotheses. Therefore, an effective visual representation is necessary. The bipartite graph shown in <ref type="figure" target="#fig_4">Fig. 5</ref> is a straightforward solution but it exhibits several shortcomings that hinder efficient information acquisition and effective information dissemination.</p><p>One main shortcoming is the cluttered links between the six statistical comparisons and the twelve hypotheses (i.e., middle of <ref type="figure" target="#fig_4">Fig. 5</ref>). These links have no obvious or memorable structures and are difficult to track by eye. One can add additional visual encoding to these links to depict three types of conclusions (i.e., reject, support, unproven) and conditional dependency. However, such encoding would further worsen the cluttering of the bipartite graph. To address this issue, we designed a matrix-based visualization for HypoML as shown in <ref type="figure" target="#fig_8">Fig.  6(a)</ref>, where four types of icons (a2) are introduced to indicate reject, support, unproven, and conditional dependency.</p><p>The second shortcoming is that displaying those numerical values in <ref type="figure" target="#fig_4">Fig. 5</ref> incurs a fair amount of cognitive load upon users for visualization tasks other than value retrieving. Since it is necessary to display them for the value retrieving task, we introduce additional visual encoding to ease other tasks. With the four accuracy values, one visualization task is to compare them. Using positions can significantly reduce the cognitive load for such a task <ref type="bibr" target="#b6">[7]</ref>. As shown in <ref type="figure" target="#fig_8">Fig. 6(c)</ref>, we use the positions of circles to indicate the accuracy values and the lines to indicate the 95% confidence interval.</p><p>With the six p-values, one visualization task is to identify those clearly below or above the threshold. Another is to use a specific value as a pivot point when tracking from hypotheses to testing results or vice versa. We decided to add a glyph to each p-value to ease these tasks as the visual encoding can aid memory recall <ref type="bibr" target="#b3">[4]</ref>. As shown in <ref type="figure" target="#fig_5">Fig. 6(b1)</ref>, we considered several alternative designs. With one design option, the area of a circle is used to encode the level of statistical significance, i.e., the inverse of a p-value. The lower the p-value, the more significant the difference and the larger the circle. However, the initial user feedback suggested that this design was "confusing" due to the reverse encoding. With another design option, the p-value is encoded using the area of an orange circle, which is inside a large blue circle of a fixed size. While this design enables direct observation of statistical significance through the blue area as well as the p-value through the orange area, it was found to be "unintuitive" for those who p-value thr 0.05</p><formula xml:id="formula_11">1/p p p alternative design</formula><p>The difference is statistically significant insignificant b1 b2</p><p>Experiment Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistic Comparison b c H1</head><p>The concept is useful to M+ and would be useful to M</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H2</head><p>The concept is harmful to M+ and would be harmful to M</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H3</head><p>M has already learned the concept adequately</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H4</head><p>M+ has learned the concept adequately</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H5</head><p>The extra information in D+ has a positive effect on M</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H6</head><p>The extra information in D+ has a negative effect on M</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H7</head><p>The extra information in D+ has a positive effect on M+</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H8</head><p>The extra information in D+ has a negative effect on M+</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H9</head><p>Learning with Dm+ affects the extra part of M+ positively</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H10</head><p>Learning with Dm+ affects the extra part of M+ negatively  were unfamiliar with the definition of p-value. We finally settled down on the third design based on the illustration widely used for explaining statistical hypothesis testing. In this design, the whole shape represents a normal distribution and the area in orange coarsely encodes the pvalue. The normal distribution curve can quickly remind users of the meaning of p-value.</p><p>The third shortcoming is that while depicting the reasoning flow from data to conclusion as in <ref type="figure" target="#fig_4">Fig. 5</ref> correctly represents the temporal order of the computation, it would slow users down when they wish to find out the conclusions quickly. We thus reverse the order of the workflow in both the vertical and horizontal versions of the visual user interface (see <ref type="figure" target="#fig_5">Fig. 1 and Fig. 6</ref>). The horizontal design is more suitable for wide-screen displays, while the vertical design can be used on portable devices and high-resolution monitors. Users may benefit from having both designs available. In addition to these two versions, we considered another two less-integrated layouts. In Appendix B, we provide more detailed analysis of the four layouts, including independent feedback from ML specialists.</p><p>The HypoML interface was designed and developed by following an iterative design process with regular discussions with ML developers within the team and beyond. From the discussions, we discovered that most users would prefer to observe the conclusions of the hypotheses as soon as the testing results were loaded into HypoML. They could then decide whether it would be necessary to track back to the statistical comparison and experiment results for detailed reasoning. We also discovered that double encoding used for the p-value and hypotheses had enhanced users' perception of the information and enable them    <ref type="figure">Fig. 8</ref>. Three approaches to experiment design: (a) using features extracted from the source data as the + data; (b) using extra information as the + data; and (c) assuming that the source data has already been processed and using the source data as the + data.</p><p>to switch between overview (through visual encoding) and details on demand (through numerical values) rapidly by simply changing their visual attention. While each p-value is already encoded using the glyph and numerical value, we further encode it through its links with the testing results. The link style (i.e., solid or dashed) shows whether the difference between two sets of results is statistically significant or not ( <ref type="figure" target="#fig_1">Fig. 6(b2)</ref>). While the decision state of a hypothesis is already encoded using icons in the matrix, we doubly encode it using black and two grey-scale values to the levels of support to the hypothesis <ref type="figure" target="#fig_5">(Fig. 6(a1)</ref>). The black color draws users' attention quickly to those hypotheses that have been confirmed.</p><p>HypoML supports a set of interactions. Users may modify the threshold of p-value, which dynamical updates of the whole visualization and may lead to changes in the conclusions of the hypotheses. By hovering on a p-value, users can highlight the two corresponding sets of results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">USE CASES</head><p>The use cases reported in this section is primarily for testing HypoML to see if HypoML can make correct transformation from four sets of results R M,D , R M,D+ , R M+,D , and R M+,D+ to visual representations of the conclusions about 12 hypotheses. The examples shown are not intended to establish the truth about the goodness of any particular ML technique, but to demonstrate the practical uses of HypoML. If a developer suspects an ML model or a training dataset may have a shortcoming, HypoML can help the developer confirm or reject such a hypothesis. With convolution neural networks (CNN), a common wisdom is that the deeper and the larger a CNN is, more likely a concept will be learned by the CNN. When our tests show that a particular CNN model has not learned a concept adequately, it does not necessarily mean that a more complex CNN model would not be able to learn the concept either. This is indeed what testing is for in software engineering. One common goal of ML testing is to discover the shortcomings of a model, which could be due to a particular feature or a dataset. Experiment Design. We used two datasets: Fashion MNIST <ref type="bibr" target="#b37">[38]</ref> and CIFAR-10 <ref type="bibr" target="#b19">[20]</ref>. We adopted CNN classification models from the Keras documented examples <ref type="bibr" target="#b5">[6]</ref>. All models were specified using Keras and Tensorflow in Python, and trained and tested using the Google Colaboratory server. In order to make testing results comparable across different tests, we used the same data template for testing each of the two models. Because the lower-layer of a CNN model depends on the template of input data, using the same data template also made the processes for testing multiple concepts more cost-effective.</p><p>One key step in the experiment design is to find extra data that encodes the testing concept. As shown in <ref type="figure">Fig. 8</ref>, HypoML supports three ways of constructing such extra data. (a) The extra data can be obtained from the source data using an external model or a pre-processing algorithm. For example, humans can often recognize objects drawn as outline sketches. One may hypothesize that outline features may be helpful, and pre-process the source dataset D src to generate D ftr . (b) The extra data can be obtained from an external dataset. For example, humans can estimate the size of an object being observed and the estimation can be used to ascertain a recognition decision. One may hypothesize that it may be useful to add extra size information D info . (c) The extra data can be obtained, indirectly, by simulating the raw, unprocessed data. In many cases, the source data D src has been manipulated using an unknown process P, e.g., rotation regularization. One may suspect the impact of P and decides to simulate the unprocessed data, yielding D srd . One can test the impact of P by setting D srd as the D part shared by M and M+ and the original D src as the + part. Note that the role of D src has changed.</p><p>Another key step is to combine the D and the + parts of the data. As we use a fixed data template to test many concepts in relation to a dataset, we generated data samples in a way similar to mosaic augmentation <ref type="bibr" target="#b2">[3]</ref>, a technique widely used in computer vision. As shown in <ref type="figure" target="#fig_9">Fig.7</ref>, for the Fashion MNIST dataset, we used a template with 2×2 placeholders. The top-left quadrant holds the source data D, while the other three quadrants hold the + data representing up to three concepts. For the CIFAR-10 dataset, we used a template with two placeholders, the left for D and the right for +. A placeholder is filled with noise if it is not used for either D or +. Note that we can use other methods, such as concatenation, to combine the D and + parts.</p><p>Testing Concepts. We have conducted some 15 tests. Three examples are given in this section. We detail three further examples in Appendix C, where we also demonstrate the use of experiment design (b) in <ref type="figure">Fig. 8</ref>, and compare its use with experiment design (c). Average Luminosity. Let us first consider a simple hypothesis, for which the conclusion is relatively easy to anticipate for most models. Given an object in an image, its average luminosity is one of its simplest features. In general, a CNN is expected to learn features about some aggregated properties (e.g., mean, median, or mode). To test this hypothesis, we can use the experiment design in <ref type="figure">Fig. 8(a)</ref>. As shown in <ref type="figure" target="#fig_11">Fig. 9</ref>, we introduced the average intensity value of an object as a single-colored square in the upper-right quadrant. We then trained two models M and M+, and tested each of them using two datasets D and D+ according to the workflow in <ref type="figure" target="#fig_2">Fig. 4</ref>.</p><p>From the four sets of testing results, HypoML carries out statistical and logical analysis and displays the results. As shown in <ref type="figure" target="#fig_11">Fig. 9</ref>, the analysis indicates that most hypotheses are unproven. Although R M+,D+ has the highest accuracy, it is statistically insignificant to support or reject most hypotheses. This example shows the importance of statistical inference. The only hypothesis that has been confirmed is H 9 , i.e., learning with D m + affects the extra part of M+ positively. However, this does not translate to a confirmation of H 7 about the overall positive impact to M+. By observing the details about howH 9 was confirmed, we can see that it is confirmed only within the context of M+, without involving any tests about M. Rotation Regularization. In computer vision, there are many desired invariance properties for a model. Among them rotation-invariance is relative difficult to achieve. The original images in the Fashion MNIST dataset feature all fashion objects in an upright position. This naturally leads to a speculation that a trained model may not be rotation invariant. One possible way to address the need for rotation-invariance is to train a model with images featuring randomly rotated objects, which is widely employed in data augmentation techniques <ref type="bibr" target="#b31">[32]</ref>. Since the source data is already pre-processed with an unknown rotation-regularization P, we adopt the experiment design in <ref type="figure">Fig. 8(c)</ref>, to see if P is beneficial.</p><p>To simulate the real data, we applied random rotation (as P −1 ) to each image in the training and testing data, and placed the randomly-rotated image in the placeholder for D. As illustrated in <ref type="figure" target="#fig_9">Fig. 7</ref>, we simply reused the original upright images (as the results of P), and placed them in one of the placeholders for +.</p><p>The testing results are shown in <ref type="figure" target="#fig_5">Fig. 1</ref>, from which we can observe that six hypotheses have been confirmed. is, if the extra information is unavailable, M+ performs worse than M, which has not learned with the extra information.</p><p>While it is inspirational to train a CNN with rotation-invariance capability, this test indicates that an object detection model can benefit from the extra information generated by another model that can detect the rotation angle or perform rotation regularization. HSV Color Space. In computer vision, there is an unsettled debate about which color space may be the best for the images to be processed by CNNs. Gowda and Yuan conducted a series of experiments, showing that images encoded using several color spaces may improve the accuracy of a classification model <ref type="bibr" target="#b9">[10]</ref>. For example, they showed that mean accuracy of RGB+HSV (81.42%) is higher than that of RGB (78.89%), suggesting that HSV may be useful in addition RGB.</p><p>The comparison between RGB and RGB+HSV fits perfectly with the HypoML platform as we can place RGB in the D placeholder and HSV in the + placeholder. The comparison thus becomes a hypothesis test about whether HSV is useful as an extra concept.</p><p>Using the CIFAR-10 dataset <ref type="bibr" target="#b19">[20]</ref>, we trained models M for RGB only and M+ for RGB+HSV. Following the design outlined by Gowda and Yuan <ref type="bibr" target="#b9">[10]</ref>, we merged the network for RGB and that for HSV/noise with an ADD function at the top activation layer. As shown in <ref type="figure" target="#fig_5">Fig.  10(a)</ref>, we obtained mean accuracy of R M+,D+ (RGB+HSV) as 81.04% and that of R M,D (RGB) as 80.57%. However, the t-test shows that the comparison between R M,D and R M+,D+ is statistically insignificant (p = 0.43). The concept of HSV is neither useful or harmful.</p><p>We also tested other ways of integrating the two networks. For example, using a MAX function at the the top activation layer, merging them at a maxpool layer in the middle of the two networks, and merging them at a maxpool layer in the lower part near the data inputs. As shown in <ref type="figure" target="#fig_5">Figs. 10(b,c,d</ref>), none of the tests resulted is statistically insignificant R M,D R M+,D+ or R M,D R M+,D+ . Hence, with the CNN models that we used for testing RGB vs. RGB+HSV, the hypothesis that HSV may be useful in addition to RGB has not been confirmed. Effects of Componentization. Recall the early discussion about interpreting H 9 , H 10 , H 11 , and H 12 in Section 3. <ref type="figure" target="#fig_5">Fig. 10</ref> exemplifies the different ways of merging the two networks, i.e., the M part and the + part, into M+ has different impact on H 9 , H 10 , H 11 , and H 12 .</p><p>When two networks are merged at an upper layer, M+ can be seen as highly componentized. When they are merged at a lower layer, they can be seen as highly integrated. In the highly integrated case, as shown in <ref type="figure" target="#fig_5">Fig. 10(d)</ref>, H 9 and H 12 were confirmed. However, the boundary between the two parts is not clearly defined, the conclusions about learning with extra information impacts the M part negatively and the + part positively should be treated with care. We can observed the changes when the merge takes place in the middle. As shown in <ref type="figure" target="#fig_5">Fig.  10(c)</ref>, none of H 9 , H 10 , H 11 , and H 12 is confirmed. Interestingly, H 9 is unconfirmed as R M+,D+ ≈ R M+,D . This means that the + part of M+ did not really learn much from HSV data.</p><p>When the two networks are merged at the top activation layer, the conclusions of H 9 , H 10 , H 11 , and H 12 are much more meaningful. Because ML developers are expected to know how the two networks are merged, we believe most ML developers can interpret these hypotheses correctly in conjunction with their knowledge about their CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">INDEPENDENT FEEDBACK</head><p>The VA technique proposed in this work consists of several components, which are new and are expected to take some time to be adopted by the wide ML community. These components include (i) the overall testing method, (ii) the logical inference rules, and (iii) the visual design of the HypoML tool. As shown in Section 6 and Appendix C, the author team, which includes ML specialists, have used HypoML extensively. This allowed the team to validate the correctness of (i) and (ii), and evaluate (iii). The numerical values, the computed inference results, and the visual summary of the these shown in Section 6 and Appendix C provide objective evidence to demonstrate the usefulness of this new VA technique and the HypoML tool.</p><p>To complement the objective evaluation in Section 6 and Appendix C, we sought independent feedback from four ML specialists who are not part of the author team. They include a a faculty member with 20 years of ML research experience, b a senior industrial researcher with more than five years of ML experience, c an industrial researcher and a doctoral student with about five years of ML experience, and d a doctoral student with about four years of ML experience. Appendix D details the interview procedure and questions. We organized our discussions around the following topics:</p><p>Existing Methods for Considering Features. Three specialists ( a , b , c ) use confusion matrix to observe mistakes made by learned models. b sometimes browses mistakes made by a model one by one. c , d sometimes view activation or similar plots, hoping to find interesting patterns.  publications and competitions. Reflection: All these methods can stimulate hypotheses about features. HypoML completes these methods by offering a structured method for evaluating hypotheses. Necessity of Hypothesis Testing. When facing a question about how much difference (e.g., accuracy 84.9% 84.8%), all four specialists stated no easy answer as the difference may be caused by some unknown facts. b , d considered that it may depend on how difficult the task is, or the size of the testing data. b added: "For industrial problems, we do not always have big dataset, and hypothesis testing will be useful." c reasoned: "According to information theory, I know hypothesis testing is needed even for big datasets." None of the interviewees were aware of anyone using hypothesis testing to evaluate ML models. Reflection: Most ML developers are cautious about some differences when comparing models, though some may not be aware that testing with a big dataset can still yield results that are statistically insignificant. Statistical hypothesis testing provides the subsequent logical influences with a standardized and consist basis.</p><p>Usefulness of Feature-based Hypothesis Testing. All four specialists confirmed that they had hypotheses all the time, e.g., about the influence of background, size, color, position, especially when a model exhibited unexpected errors. To evaluate a hypothesis, they would make modification to the model structure, training data, or some training parameters, and then compared the performance of the old and new models. d commented: "For hypotheses about features, it is not always easy to change a model structure or find new training data." a added: "For many medical imaging applications, we use both 'bag of features' and CNN models. One always has difficulties with some CNN models, and I can think about testing some CNN models against some features now. I should ask my students to download the software." c concluded: "I have worked with models with image and text inputs, I know HypoML will work for these applications." b reasoned: "[The workflow] from the model results to statistical comparison and then to hypothesis logic works for me." d indicated: "I would like to use the visualization to show the results to non-expert model users." Reflection: The feedback shows that ML specialists can anticipate the potential uses of HypoML, and appreciate that designing a test for a feature will be easier than modifying a model structure and acquiring new training data. This is similar to education where examinations are used to find out any missing knowledge and inform the modification course syllabuses or teaching methods.</p><p>HypoML Visual Designs. We also asked the four ML specialists to comment on the four visual designs mentioned in Section 5. In general, they considered that they could work with all four visual designs, and there was slightly more preference for the vertical design shown in <ref type="figure" target="#fig_8">Fig.  6</ref>, followed by the horizontal design in <ref type="figure" target="#fig_5">Figs. 1 and 9</ref>. Further details of the four ML specialists' comments can be found in Appendix B.</p><p>Suggestions for Future Work. d suggested: "I would like to test how independent or integrated two features are. I wonder if I need to change the logical influences rules." c mentioned: "I am working with graph inputs. I may need to change the two parts of the inputs." d indicated: "I have other types of hypotheses beyond the 12 listed."</p><p>Reflection: It is encouraging to see that ML specialists are already thinking about extending various components of HypoML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSIONS</head><p>In this paper, we propose a novel testing framework to aid the evaluation of ML models. In particular, this framework tests a set of hypotheses about a concept, checking whether extra information about the concept can benefit an ML model, and if so, how the extra information affects the model. The testing framework is underpinned by statistical analysis of the experiment results as well as logical inferences about the relations between six statistical conclusions and twelve hypotheses. Through an implementation of this framework HypoML, we demonstrate that with a purposely-designed visual representation, model-developers can visualize the conclusions about the twelve hypotheses as soon as the four sets of testing result data become available. This approach complements the traditional way of observing various plots for monitoring neuron activities, such as activation plots and gradient ascent plots. Model-developers, who observe any interesting patterns or failed to find desired patterns, can now formulate a concept-based hypothesis and carry out a structured test to evaluate their hypotheses.</p><p>We recognize that HypoML is only one of the many steps towards an ultimate goal of developing a powerful testing suite for evaluating, understanding, and explaining ML models. There is a need for further theoretical developments, including, e.g., formulating more complex logical inference for sub-group, multi-model and multi-concept analysis of the testing results, designing an advanced user interface for supporting detailed observation of sub-group analysis, and integrating with other visualization techniques for observing, understanding, and explaining ML models. While HypoML allows users to answer new questions about ML models, it also demands extra computational cost for model training and testing. Efficient models such as MobileNet <ref type="bibr" target="#b12">[13]</ref> can be used to reduce such cost. However, further investigation is necessary for gaining a full understanding in this respect.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Gradient ascent</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>An illustration of the structured testing method proposed in this work. The concept to be tested is encoded as extra information to accompany the original data. Two models, M+ and M, are trained with and without extra information. Both models are then tested using two different types of testing data, one with extra information and one without. The four sets of results are then analyzed by the HypoML tool against 12 hypotheses. HypoML presents the analytical conclusions using visualization as shown inFig. 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>A 3 :</head><label>3</label><figDesc>Comparing R M+,D+ and R M+,D may conclude:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>A 5 :</head><label>5</label><figDesc>Comparing R M+,D and R M,D+ may conclude: •</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>STEP 1 :</head><label>1</label><figDesc>Compute the six comparative values, i.e., A 1 , A 2 ,...,A 6 , in terms of , , and ≈, based on statistical analysis. STEP 2: Compute the logical inference (i.e., in terms of , ⊥, ) based on A 1 , A 4 , A 6 . For each true statement, i.e., (H i ), add +1 to the indicator of H i . For each false statement, i.e., ⊥(S), add −1 to the indicator of H i . STEP 3: Compute the indicators based on A 2 , A 5 . STEP 4: Compute the indicators based on A 3 . STEP 5: Then display each indicator based on positive or negative values. HypoML displays each hypothesis according to its indicator in three states: &gt;0 (confirmed), 0 (unproven), &lt;0 (rejected).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>H 1 H 2 3 H 4 H 5 H 6 H 7 H 8 Fig. 5 .</head><label>123456785</label><figDesc>The concept is useful to M+ and would be useful to M The concept is harmful to M+ and would be harmful to M H The extra information in D+ has a positive effect on M The extra information in D+ has a negative effect on M The extra information in D+ has a positive effect on M+ The extra information in D+ has a negative effect on M+ The analytical workflow from testing results to statistical analysis and then logical inference of hypothesis. As a basic visual design, it has a number of shortcomings.5 VISUAL ANALYSIS OF HYPOTHESESFig. 5 shows a typical workflow of the proposed hypotheses testing. To start with, model-developers conduct experiments and obtain four sets of results, i.e., R M,D , R M,D+ , R M+,D , and R M+,D+ . HypoML then performs six sets of statistical analysis by comparing each pair of the results. Based on the statistical analysis, HypoML makes logical inference about the twelve hypotheses, deciding whether a hypothesis should be supported, rejected, or inconclusive.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>The vertical version of HypoML interface depicts the three phases of a hypothesis test: (a) at the top, conclusions of the 12 hypotheses, which allow users to decide whether to track back to the statistical inference and experiment results for detailed reasoning; (b) in the middle, six sets of statistic analysis for comparing model testing results, where each p-value is doubly encoded using glyph patterns (b1) and link styles (b2); (c) at the bottom, four sets of model testing results. A horizontal version, which is more suitable for wide-screen monitors, is shown inFig. 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 .</head><label>7</label><figDesc>Samples of the training data for testing the concept of rotation correction. For each sample, the left image shows the original object. The middle image shows the corresponding stimulus in the testing dataset D, where the object has been arbitrarily rotated. The right image shows the stimulus in the testing dataset D+ where the rotated object is accompanied by an up-right view of the object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 9 .</head><label>9</label><figDesc>Testing the combined concept of average intensity. For each sample, the stimulus in D contains an original object. The stimulus in D+ contains an extra piece of information about the average intensity of the object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>They indicate: • H 1 : The concept of rotation regularization is useful to M+ and would be useful to M. • H: M+ has learned from the concept of rotation regularization adequately. • H 6 : The extra information in D+, when it is fed to M, has a negative effect on M. Although M has only learned from noise the upper-right quadrant of the stimuli, when non-noise information appears in that area, it still affects M, in a negative way. • H 7 : The extra information in D+ (upper-right quadrant) has a positive effect on M+. • H 9 : Learning with D m + affects the extra part of M+ positively. This is somehow anticipated because H 1 is confirmed. • H 12 : Learning with D m + affects the M part of M+ negatively, that</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>d mainly use summary statistics as required by R(M+, D+) R(M+, D) R(M, D+) R(M, D) R(M+, D+) R(M+, D) R(M, D+) R(M, D) (a) Merging at an upper activation layer with an ADD function (b) Merging at an upper activation layer with a MAX function R(M+, D+) R(M+, D) R(M, D+) R(M, D) R(M+, D+) R(M+, D) R(M, D+) R(M, D) (c) Merging at a middle maxpool layer with a MAX function (d) Merging at a lower maxpool layer with a MAX function</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 10 .</head><label>10</label><figDesc>Using HypoML to compare two models: M for RGB only and M+ for RGB+HSV. This enables the evaluation of the hypothesis if HSV is useful in addition to RGB, while observing the effect of different types of integration. Purple circles are zoomed-in views of the model results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>• QianwenWang  and Huamin Qu are with Hong Kong University of Science and Technology, Hong Kong, China. Emails: qwangbb@connect.ust.hk, huamin@cse.ust.hk. • William Alexander, Jack Pegg, and Min Chen are with University of Oxford, UK.</figDesc><table /><note>E-mails: walexander1997@gmail.com, jack.p3gg@gmail.com, min.chen@oerc.ox.ac.uk Manuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication xx xxx. 201x; date of current version xx xxx. 201x. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org. Digital Object Identifier: xx.xxxx/TVCG.201x.xxxxxxx</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>, HypoML receives four sets of results, namely R M,D , R M,D+ , R M+,D , and R M+,D+ . Each set of results is a list of tuples, each of which consists of • id -the unique identifier of a data object. The data object may be an image, a feature vector, a multivariate data record, or a more complex data record. • ground truth -a ground truth label, which can be a nominal value, an integer, a real number, a range, or a data record of a more complex data type (e.g., a time series). • ML label -a label generated by an ML model. The label must be of the same data type as ground truth. • ML uncertainty -an optional value indicating the uncertainty estimated by an ML model equipped with a self-assessment capacity. • correctness -This is a value in the range of [0, 1] with 1 indicating absolutely correct, and 0 indicating absolutely incorrect. The value is mostly computed based on ground truth and ML label using a user-defined function such as accuracy, recall, precision. • correctness with uncertainty -This is used by the statistical analysis and is defined as ML uncertainty × correctness.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>but not R a R b . With four sets of results, there are six pairs of statistical comparison, which are labelled as A 1 , A 2 ,...,A 6 . Each analytical conclusion A i may support or reject some of the 12 hypotheses H 1 , H 2 ,...,H 12 , but not all. For example the analysis A 1 , which compares R M+,D+ and R M,D , can inform the evaluation of H 1 and H 2 . If R M+,D+ is statistically better than R M,D , i.e., R M+,D+ R M,D , we can draw a conclusion that A 1 supports H 1 and rejects H 2 . If R M+,D+ R M,D , A 1 supports H 2 and rejects H 1 . If R M+,D+ ≈ R M,D , A 1 returns an unproven (inconclusive) verdict about H 1 and H 2 . We can observe that A 1 can also inform the evaluation of H 3 , H 4 , H 7 , and H 8 . A 2 , which compares R M+,D+ and R M,D+ , can inform the evaluation of H 1 , H 2 , H 3 , H 4 , H 7 , and H 8 , but it can only do so subject to that some other hypotheses have already been confirmed or rejected. Table 1 summaries the relations between the six sets of statistical analysis A 1 , A 2 ,...,A 6 and the 12 hypotheses H 1 , H 2 ,...,H 12 . Since the analysis of A i and H i depends only on the four sets of results, HypoML can be used to test any ML model with any input data type.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>D+ H 5 , H 6 H 1 , H 2 , H 3 , H 4 , H 7 , H 8 A 3 : R M+,D+ v. R M+,D H 1 , H 2 H 7 , H 8 , H 9 , H 10 , H 11 , H 12 A 4 : R M+,D v. R M,D H 11 , H 12 A 5 : R M+,D v. R M,D+ H 5 , H 6 H 11 , H 12 A 6 : R M,D+ v. R M,D H 5 , H 6 in testing their ML models, we formulated a set of logical inference rules following careful reasoning about the causal relations among different conclusions of statistical analysis and different hypotheses. This allows HypoML to provide automated logical analysis as well as statistical analysis. To help describe the logical analysis, we employ some additional notations. detailed explanations of the six sets of logical influence rules. For the self-containment of the main text, we list these rules with brief explanations highlighting the major considerations. The statistical analysis A 1 reflects the conventional comparison between R M+,D+ and R M,D . If the t-test shows R M+,D+ &gt; R M,D is statistically meaningful, we can infer that H 1 , H 4 , H 7 are confirmed, and H 2 , H 3 , H 8 are rejected. If the t-test shows R M+,D+ &lt; R M,D is statistically meaningful, we can infer the opposite conclusions for H 1 , H 2 , H 3 , and H 4 , while H 7 and H 8 are inconclusive.</figDesc><table><row><cell>They are:</cell></row><row><cell>• (S) -The statement S is true.</cell></row><row><cell>• ⊥(S) -The statement S is false.</cell></row><row><cell>• (S) -The statement S is unproven.</cell></row><row><cell>• ∧ -Logical conjunction.</cell></row><row><cell>• ∨ -Logical (inclusive) disjunction. Appendix A provides</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>). This reads as H 1 , H 4 , and H 7 are all true, and H 2 , H 3 , and H 8 are all false.</figDesc><table /><note>• R M+,D+</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>A part of this work has received funding from the European Union's Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement No 822214.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Introduction to Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Alpaydin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
	<note>2nd ed.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Modeltracker: Redesigning performance analysis tools for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chickering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Suh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 33rd Annual ACM Conference on Human Factors in Computing Systems</title>
		<meeting>33rd Annual ACM Conference on Human Factors in Computing Systems<address><addrLine>Seoul, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="337" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Yolov4: Optimal speed and accuracy of object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><forename type="middle">M</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10934</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An empirical study on using visual embellishments in visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Borgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdul-Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Floridi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2759" to="2768" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Sensors: Special Issue on Recent Advances in Artificial Intelligence and Deep Learning for Sensor Information Fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-C</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-H</forename><surname>Tseng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://keras.io/examples/mnist_cnn/" />
	</analytic>
	<monogr>
		<title level="j">Keras CNN examples</title>
		<imprint>
			<biblScope unit="page" from="2019" to="2031" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graphical perception: Theory, experimentation, and application to the development of graphical methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Cleveland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcgill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">387</biblScope>
			<biblScope unit="page" from="531" to="554" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Machine learning, neural, and statistical classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">433</biblScope>
			<biblScope unit="page" from="436" to="439" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The state of the art in integrating machine learning into visual analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Endert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ribarsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Turkay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Nabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Blanco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rossi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="458" to="486" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ColorNet: Investigating the importance of colorspaces for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Gowda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asian Conference on Computer Vision</title>
		<editor>C. Jawahar, H. Li, G. Mori, and K. Schindler</editor>
		<meeting>Asian Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">GAMUT: A design probe to understand how data scientists understand machine learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Head</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Deline</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Drucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM CHI conference on human factors in computing systems</title>
		<meeting>ACM CHI conference on human factors in computing systems</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SUMMIT: Scaling deep learning interpretability byvisualizing activation and attribution summarizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H P</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hierarchical multi-modal fusion FCN with attention model for RGB-D tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ActiVis: visual exploration of industry-scale deep neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kalro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Understanding complex deep generative models using interactive visual experimentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gan Lab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="310" to="320" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Avoiding discrimination through causal reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kilbertus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rojas-Carulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Parascandolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 31st International Conference on Neural Information Processing Systems</title>
		<meeting>31st International Conference on Neural Information essing Systems<address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="656" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sayres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 35th International Conference on Machine Learning</title>
		<meeting>35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Interacting with predictions: Visual inspection of black-box machine learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM CHI Conference on Human Factors in Computing Systems</title>
		<meeting>ACM CHI Conference on Human Factors in Computing Systems<address><addrLine>San Jose, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5686" to="5697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<ptr target="https://www.cs.toronto.edu/˜kriz/learning-features-2009-TR.pdf" />
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deeptracker: Visualizing the training process of convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Analyzing the training processes of deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="87" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Towards better analysis of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="100" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10777</idno>
		<title level="m">Understanding hidden memories of recurrent neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deepeyes: Progressive visual analytics for designing deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pezzotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Höllt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>Lelieveldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eisemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vilanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="108" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visualizing the hidden activity of artificial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Fadel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Falcao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Telea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="101" to="110" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Squares: Supporting interactive performance analysis for multiclass classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="70" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Combining time-series and textual data for taxi demand prediction in event areas: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Markou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="120" to="129" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">VIS4ML: An ontology for visual analytics assisted machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sacha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="385" to="395" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Grad-CAM: Visual explanations from deep networks via gradientbased localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Best practices for convolutional neural networks applied to visual document analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Steinkraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th International Conference on Document Analysis and Recognition</title>
		<meeting>7th International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<title level="m">Striving for simplicity: The all convolutional net. arXiv 1412</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6806</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">LSTMVis: A tool for visual analysis of hidden state dynamics in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="667" to="676" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An analysis of machine-and human-analytics in classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K L</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="80" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ganviz: A visual analytics approach to understand the adversarial game</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1905" to="1917" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The what-if tool: Interactive probing of machine learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pushkarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolukbasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="56" to="65" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno>arXiv 1708.07747</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Understanding neural networks through deep visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 32nd International Conference on Machine Learning</title>
		<meeting>32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th European Conference on Computer Vision</title>
		<meeting>13th European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Manifold: A modelagnostic framework for interpretation and diagnosis of machine learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Ebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="364" to="373" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Achieving non-discrimination in prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 27th International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>27th International Joint Conference on Artificial Intelligence (IJCAI)<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3097" to="3103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Visual interpretability for deep learning: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1631/fitee.1700808</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Information Technology &amp; Electronic Engineering</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="39" />
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
