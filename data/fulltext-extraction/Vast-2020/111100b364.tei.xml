<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CNNPruner: Pruning Convolutional Neural Networks with Visual Analytics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junpeng</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han-Wei</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixin</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guihua</forename><surname>Shan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghua</forename><surname>Lu</surname></persName>
						</author>
						<title level="a" type="main">CNNPruner: Pruning Convolutional Neural Networks with Visual Analytics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>visualization</term>
					<term>model pruning</term>
					<term>convolutional neural network</term>
					<term>explainable artificial intelligence</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Fig. 1. CNNPruner: (a) the Tree view helps to track different pruning plans; (b) the Statistics view presents model-critic statistics to monitor the pruned models; (c) the Model view enables users to interactively conduct the pruning with informative visual hints from different criteria; (d) the Filter view presents details of individual filters for users to investigate and interactively prune them.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Convolutional neural networks (CNNs) have demonstrated extraordinarily good performance in many applications, such as image classi- fication, object detection, and speech recognition <ref type="bibr" target="#b7">[11,</ref><ref type="bibr" target="#b16">20,</ref><ref type="bibr" target="#b17">21,</ref><ref type="bibr" target="#b30">34,</ref><ref type="bibr" target="#b31">35]</ref>. The recent improvements of CNNs' performance are often at the cost of model sizes. It becomes increasingly more common now to see models with hundreds of layers and millions of parameters. For example, VGG-16 <ref type="bibr" target="#b31">[35]</ref> is a commonly used model for classification tasks. It has ∼138.3 millions of parameters, and one forward pass of the model needs ∼15.5 billions of floating-point operations. The ever increasing size of CNNs, however, also prevents them from being widely deployed to devices with limited resources, e.g., mobile/embedded devices.</p><p>Model pruning, i.e., compressing models' size by removing decisionirrelevant or less important parameters, aims to solve this problem. The idea can be traced back to the 1990s, when LeCun et al. <ref type="bibr" target="#b19">[23]</ref> first improved the efficiency of their neural networks by removing unimportant model parameters (weights) based on information theory metrics. In general, model pruning algorithms can be divided into structured prun- ing and unstructured pruning <ref type="bibr" target="#b13">[17]</ref>. Compared to unstructured pruning, which requires support from additional hardware to achieve excellent performance, structured pruning has gradually dominated the recent developments and has become a hot research topic. Most notably, filter pruning is an effective structured pruning method, which directly prunes filters that are less relevant to the prediction outcomes to reduce models' size. There exist three key steps in a typical filter pruning pipeline: 1) filters evaluation; 2) filters pruning; 3) model fine-tuning. Frequently, the pipeline is executed in an automated yet iterative manner <ref type="figure" target="#fig_0">(Fig. 2)</ref>, where the filters are removed based on hard thresholds, and the models are pruned multiple times to achieve the desired compression goal, without significantly compromising the models' accuracy.</p><p>Nevertheless, existing automated CNN pruning solutions lack the flexibility to optimally balance the trade-off between pruning efficiency and prediction accuracy. The automated pruning usually removes a fixed number or a fixed percentage of convolutional filters in each pruning. If too many filters are removed in one pruning iteration, the model will be severely damaged and difficult to recover. Conversely, if too few filters are deleted, the effectiveness of pruning will be significantly undermined. In practice, the degree of "over-parameterization" is related to the corresponding CNN's size and the type of computer vision tasks, and in turn, different models have different abilities to recover from the damage caused by filter removal. Using a fixed numerical threshold as the criterion to remove filters in each pruning iteration ignores the characteristics of each model and may not lead to the optimal pruning solution. Moreover, the complicated interplay between the stages of filter pruning and model fine-tuning makes this process difficult to control, i.e., the automated pruning focuses solely on the accuracy of the pruned model, but pays little attention to the intermediate state changes. As a result, the anomalies accumulated in the iterative pruning process may get enlarged, which will affect the pruning efficiency and eventually impact the accuracy of the pruned model. Focused on the above challenges, we propose CNNPruner, a visual analytics system to help deep learning experts create interactive pruning plans and evaluate the pruning process. CNNPruner contains four main visualization components ( <ref type="figure">Fig. 1</ref>): (a) the Tree View helps to overview and track the altered models from iterative pruning stages; (b) the Statistics View presents the loss/accuracy fluctuation, model recovery capability, and recovery cost to help users adjust pruning strategy in time; (c) the Model View, facilitated with two new metrics of instability and sensitivity, evaluate the importance of different CNN filters and enables users to interactively create pruning plans; (d) the Filter View reveals the roles that different filters played in the prediction process and helps users interpret and prune the CNN model. We conducted case studies with CNNPruner on CNNs of real-world sizes to validate its effectiveness. To sum up, the contributions of our work are:</p><p>• We design and develop a visual analytics system to help deep learning experts progressively analyze the CNN pruning process and introduce interactive intervention to the process on-demand.</p><p>• We introduce two metrics (instability and sensitivity) to assist model designers in better estimating filters' importance before pruning, and three criteria (recovery capability, loss fluctuation, and recovery cost) to evaluate the pruned model.</p><p>• We examine data instances, where the intermediate pruned models behave differently, to study the critical filters and interpret the internal working mechanism of CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>In this section, we introduce the concept of model pruning and its development in the field of deep learning. Also, we review the visual analytics works in interpreting and diagnosing deep learning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model Pruning</head><p>Model pruning compresses deep learning models by removing less important parameters and seeks a trade-off between model size and prediction accuracy. To date, many works have achieved good performance in neural network pruning, and we can roughly divide them into two categories <ref type="bibr" target="#b13">[17]</ref>, weighted pruning and filter pruning.</p><p>Weight pruning is an unstructured pruning method that deletes weights or compresses weights in a filter. Han et al. <ref type="bibr" target="#b11">[15]</ref> proposed a method to reduce the model size by removing the unimportant connections. They applied this method to CNN models that are trained for the ImageNet dataset, and reduced parameters about 89% for the AlexNet model and about 92% for the VGG-16 model. Han et al. <ref type="bibr" target="#b10">[14]</ref> used weight sharing on the basis of removing the unimportant connections <ref type="bibr" target="#b11">[15]</ref>, and employed the Huffman encoding to compress the weight to maximize the compression rate. Their experiment shows that through the compression of the VGG-16 model, the method can reduce the memory consumption from 552 MB to 11.3 MB without compromising the accuracy. Carreira-Perpinan et al. <ref type="bibr" target="#b2">[6]</ref> proposed a method to find unimportant weights by minimizing loss changes while compressing those weights. Some other studies have achieved good results through weight pruning <ref type="bibr" target="#b4">[8,</ref><ref type="bibr" target="#b9">13,</ref><ref type="bibr" target="#b12">16,</ref><ref type="bibr" target="#b33">37,</ref><ref type="bibr" target="#b34">38,</ref><ref type="bibr" target="#b43">46]</ref>, but weight pruning may cause unstructured sparsities and requires support from additional hardware to achieve excellent performance.</p><p>Filter pruning is a structured pruning method that directly removes convolutional filters from CNNs. Luo et al. <ref type="bibr" target="#b24">[28]</ref> proposed a framework named ThiNet to help the user identify the unimportant filters by computing the statistical information of adjacent layers. Li et al. <ref type="bibr" target="#b20">[24]</ref> proposed an acceleration method for CNNs by removing the filters and their feature maps, which could reduce the computation to 34% of the VGG-16 model. Molchanov et al. <ref type="bibr" target="#b27">[31]</ref> proposed a new Taylor expansion criterion to find the filters which have little influence on the loss value and remove them to reduce the model size. Some other pruning studies along this line also show good pruning results <ref type="bibr" target="#b5">[9,</ref><ref type="bibr" target="#b13">17,</ref><ref type="bibr" target="#b14">18,</ref><ref type="bibr" target="#b23">27,</ref><ref type="bibr" target="#b39">43]</ref>. Filter pruning keeps the regular structure of the model but significantly reduces the calculation and storage cost, making it a popular solution for model compression. We focus on this approach in this work as well.</p><p>Most of the aforementioned model pruning studies focus on proposing new pruning criteria and use a small fixed numerical threshold to determine the number of filters to be removed. This is because they mainly focus on the accuracy of the final model but concern less on the intermediate pruning process. The small number of removal makes the model recover easily from the pruning and often leads to an optimal pruning result. However, it prolongs the pruning process, as more pruning iterations will be needed to achieve the compression goal. The process is usually not efficient and may incur a higher computational cost to perform fine-tuning in each pruning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Visual Analytics for Deep Neural Networks</head><p>Based on the taxonomy from <ref type="bibr" target="#b3">[7,</ref><ref type="bibr" target="#b22">26,</ref><ref type="bibr" target="#b41">44]</ref>, the visualizations for deep neural networks (DNNs) interpretations can roughly be categorized into three groups, targeting on model understanding <ref type="bibr" target="#b15">[19,</ref><ref type="bibr" target="#b22">26,</ref><ref type="bibr" target="#b25">29,</ref><ref type="bibr" target="#b37">41]</ref>, model debugging <ref type="bibr" target="#b21">[25,</ref><ref type="bibr" target="#b28">32,</ref><ref type="bibr" target="#b29">33,</ref><ref type="bibr" target="#b38">42,</ref><ref type="bibr" target="#b42">45]</ref>, and model refinement <ref type="bibr" target="#b1">[5,</ref><ref type="bibr" target="#b36">40]</ref>.</p><p>To understand a DNN model, researchers usually use visualization techniques to show the internal structure and state information of the model. For example, CNNVis <ref type="bibr" target="#b22">[26]</ref> uses directed acyclic graphs to formulate the model architecture and help domain experts understand CNN through visualization. GANViz <ref type="bibr" target="#b37">[41]</ref> helps the user understand the model by visualizing and comparing the internal model states (i.e., hidden activations) of the generative adversarial networks (GANs) <ref type="bibr" target="#b8">[12]</ref> over the training process. GAN Lab <ref type="bibr" target="#b15">[19]</ref> is an interactive visualization tool for non-experts to learn the GAN models, and it significantly reduces the difficulty of understanding complex generative neural networks by using visualization techniques.</p><p>To debug/diagnose a DNN model, researchers usually define some visual evaluation methods to assist in the analysis of the model. For example, DeepEyes <ref type="bibr" target="#b28">[32]</ref> helps the user diagnose a CNN model by visualizing the convolutional layers and convolutional filters. Based on the active level of different filters, this system improves the efficiency of model design by optimizing the network structure. DGMTracker <ref type="bibr" target="#b21">[25]</ref> monitors and diagnoses the training process of deep generative models through the visualization of a large amount of time-series information over time.</p><p>For model improvement, researchers usually use visualizations to help users identify the weakness of the model. For example, DQNViz <ref type="bibr" target="#b36">[40]</ref> exposes the details in the training process of deep Q-networks <ref type="bibr" target="#b26">[30]</ref> and uses visualization techniques to extract useful patterns of the model to better control the training. Blocks <ref type="bibr" target="#b1">[5]</ref> uses visualization techniques to analyze the impact of class hierarchy on the training of CNN models. Using the analysis results, the tool can accelerate model convergence and alleviate the problem of overfitting.</p><p>These studies have proved the effectiveness of visualization and visual analytics in the machine learning filed. Our work focuses on CNN model pruning and uses visualization to help deep learning experts to better understand and improve the pruning process of CNN models. We believe that with visualization and visual analytics, our system can effectively improve the efficiency of model pruning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BACKGROUND AND CONCEPTS</head><p>This section introduces the basic concepts of model pruning and a stateof-the-art filter visualization technique. Following them, we introduce the metrics used in this work and propose a novel evaluation concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pruning CNN Filters with Taylor Expansion</head><p>This section describes the details of each step in the filter pruning process and introduces the Taylor expansion based filter evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Taylor Expansion Criterion</head><p>Our work uses the Taylor expansion criterion <ref type="bibr" target="#b27">[31]</ref> for filter pruning. Its idea is to remove filters and check how significant the removal will impact the loss function, i.e., examine the importance of filters by perturbation. The resulting importance values can then be used to prioritize filters during pruning. Mathematically, this process can be denoted as:</p><formula xml:id="formula_0">ΔL( f i ) = |L(D, f i = 0) − L(D, f i )| (1)</formula><p>where D is the training data, L() is the loss function, and f i is the output (i.e. feature map) produced from filter i, and L(D, f i ) is the loss before any model perturbation. L(D, f i = 0) is the loss when f i is removed. Physically removing individual filters and recomputing the loss for each removal is computationally expensive. But, the process can be approximated through Taylor expansions, as demonstrated in <ref type="bibr" target="#b27">[31]</ref></p><formula xml:id="formula_1">, i.e., L(D, f i = 0) = L(D, f i ) − ∂ L ∂ f i f i (2)</formula><p>ΔL( f i ) can then be transformed as follows:</p><formula xml:id="formula_2">ΔL( f i ) = |L(D, f i ) − ∂ L ∂ f i f i − L(D, f i )| = | ∂ L ∂ f i f i |<label>(3)</label></formula><p>In Equation 3, we need to calculate the product of the feature map and the gradient (the loss function w.r.t. to the feature map) to get the estimated cost of removing the corresponding filter, and this value can be calculated through back-propagation. After the calculation, l2−normalization is used to normalize the set of ΔL values resulted from removing individual filters. With the normalized importance values, we can prioritize all filters and prune the less important ones. We call this process of choosing a proper importance criterion to prioritize filters and decide the number of less important ones to remove as a pruning plan. Our objective is to derive efficient and effective pruning plans through interactive visual analytics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Model Fine-Tuning</head><p>After removing the less important filters, the model structure is slightly damaged, and its accuracy will drop. To recover the accuracy, we need to retrain the model using the training dataset. As most of the important filters are still retained in the model, the original accuracy can be recovered with a few numbers of training epochs. This process, i.e., retrain the CNN model to recover its accuracy, is called fine-tuning.</p><p>As described in <ref type="figure" target="#fig_0">Fig. 2</ref>, filters evaluation, filters pruning, and finetuning constitute one pruning iteration. Repeating the process multiple times, we can generate the pruned CNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Filter Visualization Through Guided Back-Propagation</head><p>Our primary goal in this work is to remove less important filters. Therefore, we need a proper filter visualization technique to reveal what individual features have been captured and to verify their importance. Guided back-propagation <ref type="bibr" target="#b32">[36]</ref>, as one of the state-of-the-art filter visualization technique, is adopted in our work.</p><p>Given an input image, this algorithm first performs a forward pass to the target network-layer. It sets all activations of that layer to zero, except the one extracted by the filter that we want to analyze. Next, the algorithm propagates the non-zero activations back to the input image to highlight what was extracted by the corresponding filter. Therefore, the resulting filter visualization image will have the same size as the input image and highlight what individual filters have captured. We adopt this filter visualization technique, as it can work well in interpreting filters in deeper CNN layers <ref type="bibr" target="#b32">[36]</ref>. It has also been adopted by multiple other model interpretation works <ref type="bibr" target="#b36">[40]</ref>. <ref type="figure" target="#fig_1">Fig. 3</ref> shows some filter visualization examples through this guided back-propagation technique. Four filters from Layer 0 of a 6-layer CNN is visualized, when taking a mountain image as input. From the highlighted regions in the filter visualization result, Filter 0 and Filter 3 capture the silhouette features of the mountain, whereas Filter 1 and Filter 2 capture the texture features of the mountain. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sensitivity and Instability</head><p>Based upon the Taylor expansion algorithm explained in Sect. 3.1.1, we define one criterion and propose a new metric as another criterion for filter pruning in our work.</p><p>The Sensitivity of a filter reflects the filter's impact on the model's loss when being removed. It is calculated using L2-normalized ΔL (Equation 3). A filter with a lower Sensitivity value should be removed first to reduce the impact on the model. Notice that repeating the sensitivity calculation for the same filter multiple times may result in different sensitivity values of the filter, due to the randomness inherited from the statistical parameter update process. Specifically, the updates of CNN model parameters are often in the unit of data batches. Feeding data batches into a CNN with shuffled orders will result in different parameter update orders and scales. The impact of this randomness is usually marginal to important filters, as their sensitivity values are always large. However, for less important ones, their sensitivity values are minimal and can be easily influenced by this randomness. Therefore, for these less important ones, their sensitivity orders may be very different from different calculations.</p><p>We introduce the metric Instability to accommodate the above issue, which is defined as the mean absolute deviation of the filters' ranks from different calculations, i.e.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instability( f j</head><formula xml:id="formula_3">) = ∑ n i=0 |(Rank i ( f j ) − Rank( f j ))| n (4)</formula><p>where n is the total time that we computed the sensitivity for individual filters, Rank i ( f j ) is the ranking of the jth filter in the ith computation, and Rank( f j ) is the average rankings for filter j. The instability of a filter reflects the uncertainty of the removal order, and often, the filter with a higher instability indicates it is less important. We set n=5 in this work, but it can be adjusted on-demand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Degenerated Instances and Improved Instances</head><p>Each pruning iteration improves and degenerates the model a little bit, and its prediction accuracy also changes, i.e., some data instances in the test data have different predictions results from the original and the pruned model. To better index the subset of instances with different predictions from the two models, we define the following two concepts: Degenerated Instances are images that are correctly predicted in the original model but incorrectly predicted by the pruned model, i.e., the pruning hurts the model's recognition ability on these images. Improved Instances are images that are incorrectly predicted by the original model but correctly predicted by the pruned model, i.e., the pruning improves the model's recognition ability on these images. The test dataset used by a CNN model usually contains many images, and it is difficult to analyze the effect of the pruning on every single image. The degenerated instances and improved instances help users to quickly locate the analysis target from the massive images, which improves the analysis efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DESIGN REQUIREMENT</head><p>We worked with a couple of deep learning researchers and had some discussions/interviews with them during the system design stages. Also, we investigated the related works on model pruning to identify the challenges that deep learning experts are facing with. From these discussions and literature reviews, we found that proposing effective pruning criteria is an important research topic, and the criteria are often evaluated by the accuracy of the pruned model. Based on different criteria, people often use a fixed number or a mathematical formula to decide the amount of filters to be removed in each pruning iteration, which lacks flexibility and is usually not efficient. For example, a small removal count is often used to guarantee the model's recovery capability. However, the small number often leads to more pruning iterations, which inevitably prolongs the pruning process, costing more computing resources for model fine-tuning. Additionally, we noticed that even if the original and pruned models have similar prediction accuracy, their recognition power for different classes may be very different. Revealing these details, along with other model-level details (e.g., model architecture evolution, recovery capabilities from pruning) are very important to understand the pruning process. Through the responses from the experts and our studies of the existing works, we have identified the following design requirements for CNNPruner.</p><p>• R1: Display different levels of information about the CNN models during pruning. Many intermediate CNN models are generated in the iterative process of model pruning, and our system needs to track and display the details of those models. Displaying these model information is the basis for understanding and exploring the pruning process, which requires CNNPruner to: • R2: Interactively analyze and decide the number of filters to be removed in each pruning iteration. After each pruning, the model needs to be fine-tuned, and its prediction accuracy will change. The experts want to minimize the computational cost for fine-tuning but restore the accuracy as much as possible. Therefore, they expect CNNPruner to help them analyze the impact of pruning and select the appropriate removal amount in each pruning. We, therefore, design CNNPruner to be able to:</p><formula xml:id="formula_4">-</formula><p>-R2.1: estimate the influence of a pruning plan on the model before the pruning actually happens (i.e., pre-estimation). -R2.2: evaluate the quality of the pruning plan and the pruned model after each pruning (i.e., post-evaluation).</p><p>-R2.3: assist the user in better selecting or optimizing the number of filters to be removed in each pruning iteration.</p><p>• R3: Understand model pruning process and refine the pruning plan. The convolutional filters are the basic units to be removed in each pruning. The in-depth analysis of them can help the user better understand the pruning process and identify the abnormal changes of the accuracy values for different classes of the studied dataset. Therefore, CNNPruner needs to be able to:</p><p>-R3.1: visualize the filters of interest and help the user understand the roles that different filters played during pruning. -R3.2: interactively refine the pruning plan by adding or removing filters to be pruned to reduce undesired changes of the model over the pruning. <ref type="figure" target="#fig_3">Fig. 4</ref> shows the architecture of CNNPruner, which contains a back-end powered by PyTorch <ref type="bibr" target="#b0">[4]</ref>, and a web-based front-end for visualization and interaction. We use the Flask [3] library to support the communication between the back-end and the front-end. CNNPruner takes a pre-trained CNN model as input and outputs the pruned model. Users can flexibly interact with the four visualization components from the font-end to complete the above process. In detail, the Tree view layouts the pre-trained (tree root) and post-pruned CNN (tree leaf), as well as all intermediate pruned models through a tree structure (R1.1). An estimator (R2.3) is equipped in this view to help users estimate a proper number of filters to be removed between adjacent tree nodes (i.e., CNN models). The Statistics view ( <ref type="figure">Fig. 1b)</ref> shows the evolution of the model's statistics over the process of pruning (R1.2), where users can evaluate the pruning scheme through these statistics (R2.2). The Model view <ref type="figure">(Fig. 1c</ref>) presents the internal structure and the filter attributes of a selected tree node (i.e., a CNN model) from the Tree view (R1.3). It is the main component that allows users to interactively prune the selected model and provides immediate feedback to the pruning operation to guide users towards an optimal pruning plan. The Filter view ( <ref type="figure">Fig. 1d</ref>) presents details of the individual filters to help users interpret them and interactively refine the pruning plan (R3.1, R3.2). All of the four visualization components are coordinated, and they work together to meet the objective of helping experts understand, diagnose, and refine the pruning process of CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">SYSTEM OVERVIEW</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">VISUAL ANALYTICS SYSTEM: CNN PRUNER</head><p>CNNPruner <ref type="figure">(Fig. 1)</ref> is composed of four visualization components, demonstrating different levels of CNN information and the pruning process. We provide the details of individual components in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Tree View</head><p>The Tree view provides an overview of the iterative model pruning process (R1.1). The root and leaves of the tree are the original and the pruned models respectively. Each branch of the tree (connecting the root to a leaf) chains a sequence of intermediate models from the iterative pruning process. For each node, we use two horizontal filled rectangles to denote the corresponding model's prediction accuracy and compression ratio, and one vertical rectangle to display the model ID. The system automatically generates the ID, and the ID of the root model is 0. The vertical position of a node is decided by the number of filters in the corresponding model (see the left vertical axis). The edges connecting a pair of parent-child nodes represent the fine-tuning process, where we use gray and purple lines to indicate if the finetuning process converged or not after reaching the user-specified stop conditions (e.g., the maximum number of epochs). The Tree view can track the whole pruning process, and each node in the tree represents a model. When the mouse hovers over a node, a prompt bar will appear to show the storage size of the corresponding model. The user can click on individual nodes of the tree to update the data displayed in other views. The node with the red border is the currently selected one.</p><p>The Tree view is also equipped with a pruning estimator, to help users balance the trade-off between model size and prediction accuracy (R2.3). It estimates the number of filters in a model and the model's prediction accuracy, by linearly interpreting these values from a pair of most adjacent nodes from the tree. This rough linear estimation works well based on our experiments, and we expect more sophisticated interpolation algorithms to yield better results. The pruning estimator node is only visible when users pressing the black box icon on the top right corner of the tree view, and users can flexibly drag it vertically to generate the estimations dynamically.</p><p>This view has two important parameters to be configured before any pruning (using the buttons on the top of this view). One is the pruning mode, which could be automated or manual pruning. The other is the termination criteria for fine-tuning. They are explained as follows: Auto/Manual-Pruning. For auto-pruning, users specify a fixed ratio of the filters to be removed, e.g., 1/2, 1/3, or 1/4 of the total amount, and CNNPruner will iteratively remove the specified amount of filters (based on the pruning criteria) and fine-tune the model. The iterative process runs until the pruned model fails to meet the desired need (e.g., the prediction accuracy no longer meets the requirement). This process is automated but lack of pruning flexibility. Conversely, in manualpruning, users can flexibly specify the number of filters to be removed (based on their distribution) in individual pruning iterations. Termination Criteria for Fine-Tuning. CNNPruner has three termination criteria to finish the fine-tuning process, (1) Delta Loss, i.e., the change of loss values, (2) the Target Accuracy, and (3) the Maximum Epoch. The fine-tuning will be terminated if any of them is met.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Statistics View</head><p>The Statistics view <ref type="figure">(Fig. 1-b</ref>) is used to display detailed statistical information of the CNNs (R1.2). When the user selects a model in the Tree view, the system will find a path from the current node to the root node where the models along the path form a pruning process. This view shows statistical information from multiple dimensions over the pruning process. There are five components in this view.</p><p>Confusion Matrix. <ref type="figure">Fig. 1-b1</ref> shows the confusion matrix of the current model (R1.2). Diagonal cells of the matrix represent the accuracy of true-positive instances (i.e., the percentage of correctly predicted images in one category). Non-diagonal cells represent the percentage of incorrectly predicted images. Values (of the cells) from small to large are mapping to colors from light blue to dark blue. Clicking any cell of the matrix will show a line-chart presenting the value changes for the corresponding cell during the pruning process (i.e., X-axis is the pruning iterations, Y-axis is the cell values from different iterations). The line-chart reflects the model's prediction power for a particular category across the pruning process.</p><p>Recovery Capability. This sub-view ( <ref type="figure" target="#fig_0">Fig. 1-b2</ref>) reveals the model's recovery capability after each pruning (R1.2), i.e., how difficult it is to learn the prediction power back over the fine-tuning process. The X-axis is the model ID, and it represents different pruning iterations, whereas the Y-axis denotes the model's prediction accuracy from individual iterations. The gray-curve connects the model's final accuracy values after the individual fine-tuning process. The rectangular color stripe at each iteration shows the distribution of the accuracy values from different epochs of the corresponding fine-tuning. A longer stripe indicates a more significant accuracy change before and after the finetuning. If the pruned filters have little effect on the model, the recovery region will be very short, and the accuracy fluctuation will be very small (i.e., a short strip with dark blue color). The information from this sub-view is an important criterion to evaluate the pruning plan.</p><p>Loss Fluctuation. The Loss Fluctuation sub-view shows the loss changes in the process of fine-tuning (R1.2). The X-axis in the chart is the model ID, and the Y-axis is the loss value. The curve between the two IDs represents the fluctuation of the training loss in the fine-tuning process (between two models). The importance of a filter is estimated based on how significantly the loss will change when removing it.</p><p>The loss values, quantifying the inconsistency between the predicted label and the true label, can effectively monitor the model's evolution. If our pruning plan is good enough, the impact on the loss will be small. Therefore, the fluctuation of loss is another important criterion to measure the pruning plan, and this sub-view helps users analyze the fine-tuning process by displaying the loss fluctuation. Recovery Cost. The Recovery Cost sub-view shows the number of epochs in the fine-tuning process through a bar chart (R1.2). The X-axis of the chart is the model ID, and the Y-axis is the epoch count. If the pruning plan has little effect on the model, then only a small amount of training epochs is needed in the fine-tuning process to recover the accuracy. Conversely, if over-pruning happens, even with a lot of training epochs, it is still difficult to recover the accuracy. Therefore, the recovery cost is also a criterion to evaluate the pruning plan. Through this sub-view, the user can have an intuitive understanding of the recovery cost in the pruning process.</p><p>Parameters and Computation. This sub-view displays the reduced amount of the model parameters and the computational cost. As is shown in <ref type="figure" target="#fig_4">Fig. 1-b5</ref>, the line chart displays the reduced number of parameters, and the histogram displays a reduced amount of computations (R1.2). The pruning process removes filters from the network, thus reducing the size of parameters. Meanwhile, the size of the parameters is proportional to the amount of computation in the model. By calculating the amount of computation for the process of one image in the test dataset, users can estimate the running efficiency of the model in mobile/embedded devices. The user can verify if the pruned model meets the computation requirements or not through this chart.</p><p>All sub-views, except the Confusion Matrix, can be scaled horizontally to take the full space of the Statistics view (by double-clicking the corresponding sub-view). This interaction helps to scale the system when the pruning process is long or involves many pruning iterations. It also reduces the information that users need to watch at once, helping them to better focus on a single metric at a time (rather than overwhelmed by all five statistical metrics).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Model View</head><p>The visualization of the internal information of a CNN model can help users understand the state of CNN and make proper pruning plans. As shown in <ref type="figure">Fig. 1-c</ref>, we designed the Model view to display the architecture of the studied CNN model <ref type="figure">(Fig. 1-c1</ref>), the evaluation of filters from the model <ref type="figure" target="#fig_0">(Fig. 1-c2</ref>), and the pruning plan ( <ref type="figure" target="#fig_1">Fig. 1-c3)</ref>.</p><p>The architecture of the model selected in the Tree view is displayed in <ref type="figure" target="#fig_0">Fig. 1-c1 (R1.3, R2</ref>.1). Each box in the architecture diagram represents a layer of the model. Different colors represent different types of layers. In particular, we use the red box to represent the deleted filter, and the width of this box to denote the percentage of the deleted filters in the current convolutional layer. The height of each box is proportional to the size of the feature map. The number on the box is the number of filters in the corresponding convolutional layer.</p><p>The visualization of filter evaluation is shown in <ref type="figure" target="#fig_0">Fig. 1-c2</ref>, which consists of a radar plot and a bubble plot <ref type="figure" target="#fig_0">(R1.3, R2.1</ref>). The radar plot shows the impact of the pruning plan on the current model. There are three dimensions of information in the radar chart, namely, the number of filters, the remaining sensitivity percentage, and the remaining instability percentage. The remaining percentage means the ratio of the metric between the model after this pruning iteration and the current model. The bubble plot on the right shows the sensitivity and instability of each filter in each layer. In the bubble plot, each bubble represents a filter, and different layers have different colors. The X-axis represents the sensitivity value, and hence the bubbles closer to the right are the filters with more impact on the loss (i.e., important ones that should not be pruned). The size of the bubbles represents the corresponding filters' instability, i.e., bigger ones correspond to larger values.</p><p>The pruning plan is shown in <ref type="figure" target="#fig_1">Fig. 1-c3</ref>, and it shows the indices of filters that each layer will be removed (R2.1). Each circle represents one filter, and the number on the circle is the index of the filter. The circles of different layers use different colors, which are consistent with the bubble plot above. The multi-color line under the circles is an overview of the number of filters to be removed in the pruning plan. Different colors represent different convolutional layers, and the length of the color segment represents the percentage of the removed filters in the corresponding convolutional layer. This view displays information of the model selected from the Tree view. The icons (i.e., the layer legends) on the right of the model architecture support the filtering of different layers. For example, when clicking the icon for convolutional layers, other layers, e.g., pooling and linear layers, will become transparent to help users better focus on the layers in the analysis. There is a vertical slider in the bubble plot, and users can drag it to specify the pruning threshold. The bubbles on the left of the slider are shown in the pruning plan and represent the filters that will be removed in the current pruning. Meanwhile, the radar plot on the left shows the influence of pruning on the number of filters, the sensitivity, and the instability (R2.1). Dragging the slider will also change the width of the red boxes in the model architecture diagram and the proportion of different colors in the multi-color segment of the pruning plan (R2.1). Additionally, the system provides a set of buttons on the right of the bubble plot to help users quickly move the slider to certain positions. Users can scale the bubble plot horizontally along the sensitivity axis to reduce the occlusion between bubbles. They can also switch among different convolutional layers in the Filter view through the convolutional buttons between the radar plot and the bubble plot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Filter View</head><p>The Filter view allows the user to conduct an in-depth analysis of a specific convolutional layer (R3.1, R3.2). As shown in <ref type="figure">Fig. 1-d</ref>, this view consists of a scatter plot and a filter visualization matrix. The points in the scatter plot represent the degenerated and the improved instances in the test dataset, and the color represents the category of the exemplars. We use the t-SNE <ref type="bibr" target="#b35">[39]</ref> algorithm to process the image instances and display them in the scatter plot. Our system uses the degenerated and improved instances to distinguish sensitive images, which efficiently narrows down the analysis scope. The selected image in the middle of the Filter view shows the point that the user clicked in the scatter plot. There are two lines of texts at the bottom of the image. The first line shows the image name and its true label. The second line shows the labels of the image before and after the pruning, separated by an arrow. In the filter visualization matrix, each item represents a filter, and the items with red borders will be deleted in current pruning. The image in each item is the visualization of the filter. The area chart on the top right of the item shows the distribution of pixel values of the filter visualization images. The blue and green bar below the area chart represent the sensitivity and instability of the filter, respectively.</p><p>When the user selects a node in the Tree view, the system retrieves the degenerated and improved data instances according to the selected node and its child node. The user can switch the displayed convolutional layer in the Layer view by clicking on the convolutional buttons in the Model view (between the radar plot and the bubble plot). The scatter plot supports the filtering of different types of data instances through the icons on the upper right corner. After the user clicks one point in the scatter plot, the selected image and the matrix view on the right will be updated accordingly to reflect the selection. From the matrix view, the user can double-click any item to add/delete the corresponding filter to/from the current pruning plan.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CASE STUDIES</head><p>In this section, through three cases we present how CNNPruner can assist pruning, improve pruning efficiency, and optimize pruning plans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Automated Pruning: A Guiding Example with MNIST</head><p>The MNIST dataset <ref type="bibr" target="#b18">[22]</ref> is a commonly used classification dataset. It contains 60,000 images for training and 10,000 images for testing. We train a two-layer CNN to perform this classification task. There are 32 filters in the first convolutional layer and 64 filters in the second. The network structure is shown in <ref type="figure" target="#fig_5">Fig. 6-c1</ref>. The accuracy of the model is 98.74%, and its size is 240KB.</p><p>The goal of pruning in this case study is to obtain a pruned CNN with an accuracy of more than 98.50%. The system supports automated pruning and manual pruning. For this simple CNN, we believe automated pruning is enough to meet the compression goal.</p><p>After loading the model to CNNPruner, we need to set some necessary parameters before the pruning. First, we configure the dataset parameters to tell the system where the dataset is. Then, we set the fine-tuning parameters (i.e., set the Delta Loss to 0.000001, the Target Accuracy to 98.50%, the Maximum Epoch to 30, the Learning Rate to 0.001, the Optimizer to Adam, and the Batch Size to 100). For the setting of the Delta Loss, the Learning Rate, the Optimizer, and the Batch Size, we suggest using the same parameters as the model training stage. The Delta Loss, the Target Accuracy, and the Maximum Epoch determine the termination of the fine-tuning process.</p><p>There is only one node in the Tree view after setting the above parameters. By selecting this root node, we can observe the sensitivity and instability distribution of the model in the Model view <ref type="figure" target="#fig_4">(Fig. 5</ref>). For one pruning iteration, we want to minimize the impact of sensitivity while maximally decreasing the instability and the number of filters. From the estimated pruning results (in the radar chart), we see that removing one-third of the filters will preserve 96% of the sensitivity, and reduce 38% of the instability. We, therefore, believe we can use the 1/3 auto-pruning strategy for this case and set the corresponding auto-pruning parameters. Using the auto-pruning button in the Tree view, we automatically prune the model and generate a pruning tree. <ref type="figure" target="#fig_5">Fig. 6</ref>-a is the pruning tree for this auto-pruning process. It shows that the number of CNN filters is reduced from 96 to 10 after six pruning iterations. The prediction accuracy changes marginally in the first five iterations, and the fine-tuning process converges well. The pruned model from the sixth iteration failed to meet our requirement (i.e., the accuracy dropped to 98.07%&lt;98.50%), and the fine-tuning process did not converge (indicated by the purple color of the line). <ref type="figure" target="#fig_5">Fig. 6</ref>-b presents the statistics from CNNPruner for further analysis of the auto-pruning process. <ref type="figure" target="#fig_5">Fig. 6-b1</ref> shows the recovery ability and the volatility of the six pruned models. As demonstrated by the short and light blue bars, the "damage" introduced by the first three pruning operations is small, and the pruned models can easily recover from it. Starting from the fourth iteration, the resilience of the model decreases, and the accuracy fluctuates more significantly. <ref type="figure" target="#fig_0">Fig. 6-b2</ref> shows the model's loss function in the six fine-tuning iterations, which can reduce to the same level after individual fine-tuning iterations. For Model 6, pruning has a large impact on the loss, and it cannot recover the accuracy, even after 30 epochs re-training. Therefore, we think that the parameters of Model 6 are not enough to support the original accuracy. From the statistics in <ref type="figure" target="#fig_5">Fig. 6-b1</ref> and 6-b2, we believe Model 5 is the best candidate model to meet the compression goal. <ref type="figure" target="#fig_1">Fig. 6-b3 and 6-b4</ref> show that the number of operations in one forward pass of Model 5 is ∼3 megaFLOPS, and the number of parameters is ∼6.3 thousand. Therefore, the pruning reduced the model's size by 87.58%, and the computation cost by 97.12%. The final pruned model (Model 5) is shown in <ref type="figure" target="#fig_0">Fig. 6-c2</ref>, which has five and nine filters in the first and second convolutional layer respectively. With CNNPruner, we can reveal model-pruning details, such as model convergence, model accuracy, recovery ability, loss fluctuation, and recovery cost. These details can help the user better understand the state changes of the model in the pruning process and evaluate the fine-tuning process.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Manual Pruning: Flexible Intervention over Pruning</head><p>Our second study presents the case of using the Cat&amp;Dog dataset [1] to interactively achieve a pruning goal. The Cat&amp;Dog dataset contains 25,000 images of cat or dog (the two classes). We randomly select 10,000 cat images and 10,000 dog images as the training dataset. The rest of the images are used for testing. A CNN with six convolutional layers is trained to differentiate cats from dogs, and its structure is shown in <ref type="figure" target="#fig_6">Fig. 7</ref>. The original well-trained model before any compression can achieve a prediction accuracy of 92.76%. The model contains 2200 filters, which has 6.88 million parameters with a size of 26.30 MB. A single forward pass of the CNN needs 4.6 GFLOPs operations.</p><p>The desired pruning goal is to maximally shrink the model while maintaining the prediction accuracy to be above 92.5%. CNNPruner can help the user choose the optimal pruning solution by analyzing the pruning process and revealing the pruning details, so as to improve the pruning efficiency and ensures the accuracy of the pruned model. To demonstrate this, we use manual+estimator pruning in this study, which includes two major stages. The first stage relies on statistical information and immediate visual feedback from the system to remove the filters. The second stage uses the estimator to remove filters interactively in much finer granularity. In addition, this section also compares the manual+estimator pruning with the automated only pruning and automated+estimator pruning to show its advantages.</p><p>Stage 1: Rough-Pruning with Interactive Estimation of Thresholds <ref type="figure" target="#fig_0">(R2.2, R2.3)</ref>. After setting the dataset parameters and fine-tuning parameters, we use the bubble plot in the Model view to interactively probe and determine the number of filters to be removed <ref type="figure" target="#fig_7">(Fig. 8</ref>). As shown in <ref type="figure" target="#fig_7">Fig. 8-b</ref>, removing 50% of the filters does not seem to significantly impact the model's sensitivity (change by 6%) and instability (change by 85%). Therefore, we decided to remove 50% of the filters. After one round of fine-tuning, we get Model 1 and the statistical information corresponding to this model, as shown in <ref type="figure" target="#fig_7">Fig. 8</ref>-e. These statistics reflect the difficulty level of the fine-tuning process. For example, although the accuracy of Model 1 meets the requirements, the accuracy fluctuated significantly over fine-tuning (reflected by the long strip in <ref type="figure" target="#fig_7">Fig. 8-e1)</ref>. Also, the model's training loss reduced a lot over the fine-tuning process <ref type="figure" target="#fig_0">(Fig. 8-e2</ref>). With these observations, we decided to remove fewer filters in the next iteration to guarantee a quick recovery. Note that, if the pruned model cannot be recovered after pruning 50% of the filters, we should restart again from the root node.</p><p>In the second pruning iteration, we decided to delete 25% of the filters (based on our observations of Model 1's statistics). As expected, the accuracy fluctuation and the training loss changed much less in the pruning from Model 1 to Model 2 (i.e., the second pruning did not damage the model as significantly as the first pruning iteration).</p><p>We repeat the above pruning process with on-demand humaninterventions until the model no longer meets the requirements. Over this iterative process, we get a pruning tree, as shown in <ref type="figure" target="#fig_7">Fig. 8</ref>-a. With the pruning process going forward, the instability of the model gradually decreases (i.e., from <ref type="figure" target="#fig_7">Fig. 8</ref>-b, c, to d, the instability changes from 15%, over 66% to 73%). Meanwhile, the accuracy fluctuation becomes more and more violent (i.e., from Model 2 to Model 8, the range changes from 92%∼94% to 80%∼93%), the training loss changes become larger, and the number of required epochs for model recovery increases. As more and more parameters being removed, the overparameterization of the model becomes less severe. In addition, using this progressive pruning and evaluation is conducive to manage the model's state change in real-time. Furthermore, users of CNNPruner can directly control the pruning strategy to improve pruning efficiency and prevent the model from being excessively damaged.</p><p>Stage 2: Fine-Pruning with a Real-Time Estimator (R2.3). From the pruning tree obtained in the first stage ( <ref type="figure" target="#fig_7">Fig. 8-a)</ref>, we can see that the number of filters in the target model should be between that of Model 7 and Model 8. At this stage, the estimator of CNNPruner can be used to help the user better estimate the number of filters to be removed next. In the first estimation, the target number of filters given by the estimator is 182. Therefore, we prune Model 7 to Model 9, i.e., removed 19 (201-182) filters. Using the estimator again, we find that the number of filters in the target model is 174 ( <ref type="figure" target="#fig_8">Fig. 9)</ref>. At this time, the gap of filter numbers between the target model and the current model is only 8, so we decided to terminate the pruning.</p><p>The pruning process reduced the storage of the model from 26.30 MB to 188 KB. The accuracy of the final pruned model is 92.64% (92.96% for the cat and 92.32% for the dog, <ref type="figure" target="#fig_8">Fig. 9</ref>). The accuracy is reduced by 0.12% compared with the root model. The parameters of the model are reduced by 99.44%, and the computation needed for processing an image is reduced by 98.58%.</p><p>Comparison of three pruning strategies. To highlight the pruning efficiency of the manual+estimator pruning, we compare it with another two pruning strategies, i.e., the automated pruning and auto-mated+estimator pruning, as shown in <ref type="figure" target="#fig_9">Fig. 10</ref>. The automated pruning in <ref type="figure" target="#fig_9">Fig. 10</ref>-a uses the 1/2 auto-pruning plan, i.e., removing half of the filters in each pruning iteration. From the result, we can see that model  ters in each pruning, we will get a better result, but it will also increase the pruning iterations, costing more computing resources and making the pruning less efficient. Therefore, automated pruning is inflexible and difficult to achieve the best performance. The automated+estimator pruning in <ref type="figure" target="#fig_9">Fig. 10</ref>-b contains two stages. The first stage uses the 1/2 auto-pruning plan and the second stage uses the estimator for finer granularity pruning. From the result, we can see that the estimator provides guidance for fine-pruning to help the user get an optimal model. But the large range between Model 3 and Model 4 is not preferable to the second stage of estimation, as it may affect the estimator's performance. Besides the pruning strategy in <ref type="figure" target="#fig_9">Fig. 10</ref>-b used about 21% ((110 − 91)/91, please check the total epoch numbers) of additional time than that of the strategy in <ref type="figure" target="#fig_9">Fig. 10</ref>-c (manual+estimator pruning). From these comparisons, we can clearly see how human intervention in the pruning process can help improve the pruning efficiency.</p><p>As shown in <ref type="bibr" target="#b6">[10]</ref>, there should be an optimized sparse sub-network structure in a complex DNN, which can use fewer parameters to get the same accuracy. Model pruning is an effective way to find this kind of sparse sub-network structure. Our system targets to detect whether the sub-network has been damaged or not during pruning, and in turn, improve the effectiveness and efficiency of model pruning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Diagnose the Pruning Process</head><p>Our third study presents the case of using an image dataset of nature scenes [2] to diagnose the pruning process. The dataset contains 17,034 images in 6 classes, 14,034 for training, and 3,000 for testing. The 6 categories are: 'buildings', 'forest', 'glacier', 'mountain', 'sea', and 'street'. Example images from individual classes are shown in <ref type="figure" target="#fig_10">Fig. 11</ref>. A CNN classifier with six convolutional layers is used in this case, and its structure is shown in <ref type="figure" target="#fig_0">Fig. 12-b</ref>. The original well-trained model before any compression can achieve a prediction accuracy of 86.10%. Our pruning goal is to maximally shrink the model while maintaining the prediction accuracy at above 85.00%. We used CNNPruner to prune the model and got the pruning tree in <ref type="figure" target="#fig_0">Fig. 12</ref>-a. After pruning, we reduced the number of filters in the model to 130, and the changes in the model structure are shown in <ref type="figure" target="#fig_0">Fig. 12-b,c,d</ref>. Model 6 is our final pruned model, and its accuracy is 85.16%. By analyzing the confusion matrix, we found the model's recognition accuracy for 'buildings' dropped sharply from Model 4 to Model 6 (see <ref type="figure" target="#fig_0">Fig. 12-e2)</ref>.</p><p>It is worth mentioning that a model's recognition power for different classes may not be equally important in various tasks. For example, in autonomous driving, recognizing pedestrians around a car is far more important than recognizing the mountains several miles away. Therefore, in some model pruning tasks, domain experts care more about maintaining models' recognition power for certain classes. In this case, we use CNNPruner to present an in-depth analysis of the abnormal changes of the accuracy value, and demonstrate how the system can help to refine the pruning plan to reduce its impact.</p><p>Refining Pruning Plan (R3.2). CNNPruner can be used to secure the prediction accuracy for the 'buildings' class, while maximally compressing the model. From Model 4 to Model 6, the model's overall accuracy descends by 0.3%, resulting in 168 degenerated images and 159 improved images. 40 out of the 168 degenerated images and 10 out of the 159 improved images have the true label 'buildings'.</p><p>We analyze the degenerated 'buildings' instances to find out why pruning affects the recognition of this particular class. <ref type="figure" target="#fig_1">Fig. 13</ref> shows two degenerated instances of the class 'buildings'. From the filter visualization matrix, we can see that the system deletes the filters that have the lowest sensitivity and highest instability, i.e., Filter 0 and Filter 5 (see the blue and green bar on the right of the filter visualization). However, to the class of 'buildings', the features captured by these two filters are not the least important. The area chart in the upper right of the filter visualization displays the distribution of pixel values for the filter visualization image (feature map). In general, the more concentrated the distribution is, the sharper the features are extracted. Comparing the eight distributions, Filter 1 and Filter 6 are the least important ones (for 'buildings'). The pixel value distributions for these two filters are more chaotic than others, and there are more noises in the corresponding feature maps. The decision of deleting Filter 0 and 5, rather than Filter 1 and 6, reduces the model's power in recognizing 'buildings', which is hard to recover from the subsequent fine-tuning process.</p><p>Based on the above observation, we decided to refine the pruning plan by removing Filter 1 and 6, but keep Filter 0 and 5. We set up a new branch from Model 4 and pruned it with the refined plan to get Model 7, and the result is shown in <ref type="figure">Fig. 1</ref>. The accuracy of Model 7 is 85.40% and 87.41% for the class 'buildings'. Therefore, our system optimized the pruning plan through this in-depth analysis of the filters.</p><p>To avoid the influence of randomness introduced during the fineturning process, we repeated the pruning multiple times to validate if our refined pruning plan is indeed better. Specifically, we pruned Model 4 20 times, 10 times of which used the original plan, the other 10 times used the refined plan. After pruning, we got 20 pruned models.   Their statistics are shown in <ref type="table" target="#tab_1">Table 1</ref>. From the table, we can see that the refined pruning plan can effectively reduce the decreasing trend of the accuracy of class 'buildings' when pruning Model 4.</p><p>Interpreting Pruning Process (R3.1). From Model 4 to Model 6, the accuracy for the class 'mountain' increased by 11.62% <ref type="figure" target="#fig_0">(Fig. 12-e3</ref>), resulting in 20 degenerated instances and 81 improved instances for this class. With CNNPruner, we can interpret what has contributed to the model improvement over the pruning. As shown in <ref type="figure" target="#fig_3">Fig. 14,</ref> we selected some images to analyze why the pruning plan improved the accuracy of 'mountain'. The image in <ref type="figure" target="#fig_3">Fig. 14-a</ref> was mis-classified as 'sea' by the model initially. The pruning removed Filter 5, which extracted the majority of the pixels for 'sea' in the image. As a result, the pruned model believes the image is more like a 'mountain', rather than 'sea'. Similarly, in <ref type="figure" target="#fig_3">Fig. 14-b</ref>, Filter 5 mostly extracted the glacier features, which is probably why the image was mis-classified as 'glacier' before pruning. Removing these noisy features makes the model concentrate more on the mountain and generate the correct prediction of 'mountain'. Identify Confusing Images. Additionally, from the investigations with the degenerated image instances (from Model 4 to Model 6) with CNNPruner, we also found images with improper labels. For example, the image in <ref type="figure" target="#fig_4">Fig. 15</ref> is one of the degenerated instances with the true label 'buildings'. The original image contains both street and buildings, and the street takes a major portion of the image. Although the image is labeled as 'buildings', we feel 'street' is more proper for it. As this image only confuses the model, we recommend removing it from the test dataset, which can make our model evaluation more objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">DISCUSSION AND DOMAIN EXPERTS' FEEDBACK</head><p>We conducted open-ended interviews with two machine learning experts (E 1 , E 2 ) to discuss the strengths, weaknesses, and potential extensions of CNNPruner. The experts' research interests are accelerating deep neural networks, and model pruning is an important portion of their research works. We first introduce the design goal of CNNPruner and the individual visualization components to them (in about 30 minutes). With the experts' background on model pruning, they can quickly pick up the domain-related concepts and understand the functionality of individual components, though it still took them some time to get familiar with the visualization and interaction of the system (about 60 minutes). We then go through the cases presented in the Case Studies and ask them to freely play with the system and provide feedback.</p><p>In general, both experts felt positive on CNNPruner, and they believed that the model pruning process can be clearly and intuitively presented through visualization techniques. E 1 likes the Tree view the most, as it can reveal the evolution of the pruned model quickly and allow users to reprocess the pruning interactively. The estimator in the Tree view was very interesting to him, and he agreed that it could effectively help users determine the pruning depth in the last pruning stage. E 2 appreciated the progressive pruning method proposed in CNNPruner. Through the proposed criteria (i.e., recovery capability, loss fluctuation, and recovery cost), domain experts can evaluate the pruning process more objectively. Both E 1 and E 2 were glad to see the effectiveness of the Filter view in interpreting CNNs and refining pruning plans. With the existing techniques, it is still hard for them to thoroughly understand the model pruning process from numerical statistics only. CNNPruner provides a practical way for them to interpret individual filters visually and understand their roles over the pruning process. Moreover, both experts agreed that the concepts of degenerated and improved instances are beneficial in effectively identifying images of interest.</p><p>The experts also pointed out several limitations of CNNPruner, as well as some improvements that can be applied in the future. For example, E 1 mentioned that for models with many classes, the Confusion Matrix view may not scale well. We plan to improve this view by supporting the filtering of different classes in the future. Also, the experts provided their domain feedback on how we can proceed further along this research direction. E 1 suggested that we can extend model pruning to fully connected layers, as the parameters from these layers can take a considerably large portion of the networks in many scenarios. E 2 recommended us to enhance the system by supporting the comparisons of different pruning criteria. As model pruning is still a fast-growing topic, he believed more and more criteria will be proposed. With our system, researchers can more intuitively compare different pruning plans, which in turn, will help them optimize the pruning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSION</head><p>In this work, we proposed CNNPruner, a visual analytics system to help machine learning experts to understand, diagnose, and refine the CNN pruning process. CNNPruner contains four visualization components that work together to reveal model details on different levels over the iterative pruning process. Two criteria and three metrics are used in CNNPruner to estimate filters' importance before pruning and evaluate the pruned model's quality after pruning. Both the pre-estimation and post-evaluation facilitate users to make and refine their pruning plans. Moreover, the capability of CNNPruner in thoroughly examining the degenerated and improved data instances within one pruning iteration plays an essential role in interpreting and diagnosing the pruned model. Through multiple case studies on CNN models with real-world sizes, we validated the effectiveness of CNNPruner.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>The iterative model pruning process of CNN models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>An example of filter visualization. The input is a mountain image. Filter 0 and Filter 3 capture the silhouette features of the mountain, whereas Filter 1 and Filter 2 capture the texture features of the mountain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>R1.1: track the intermediate models generated over the pruning process and index the models effectively. -R1.2: display the states of the pruned models and monitor the evolution of these states over the pruning process. -R1.3: visualize the internal structure of a selected CNN model (e.g., the original/intermediate/final pruned model) and its filters' attributes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>The architecture of CNNPruner, including a back-end powered by PyTorch and a web-based font-end visualization interface.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>The sensitivity and instability distribution of the root model. The radar chart shows the influence of removing one third of the filters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>The result of CNN pruning. The system executed six prunings to get six models. The Statistics view shows the information for Model 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>The Cat&amp;Dog dataset and the CNN model architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>The first stage of manual pruning. The Statistics view is the information corresponding to Model 8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>The second stage of manual+estimator pruning. The Statistics view is the information corresponding to Model 9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Comparison of three pruning strategies, (a) automated pruning, (b) automated+estimator pruning, (c) manual+estimator pruning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 .</head><label>11</label><figDesc>Example images from the scene classification dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 .</head><label>12</label><figDesc>The result after model pruning. (e2) The accuracy changes for the class 'buildings'. (e3) The accuracy changes for the class 'mountain'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 13 .</head><label>13</label><figDesc>Examples of degenerated instances from Model 4 to Model 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 14 .</head><label>14</label><figDesc>Examples of the improved instances from Model 4 to Model 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 15 .</head><label>15</label><figDesc>The confusing image example from Model 4 to Model 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>• GuanLi, Kaixin Chen, Guihua Shan, and  Zhonghua Lu are with Computer Network Information Center, Chinese Academy of Sciences. They are also with University of Chinese Academy of Sciences. E-mail:{liguan, sgh, zhlu}@sccas.cn, chenkaixin@cnic.cn. • Junpeng Wang is with Visa Research. E-mail: junpeng.wang.nk@gmail.com. • Han-Wei Shen is with The Ohio State University. E-mail: shen.94@osu.edu. • Guihua Shan is the corresponding author. Manuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication xx xxx. 201x; date of current version xx xxx. 201x. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org. Digital Object Identifier: xx.xxxx/TVCG.201x.xxxxxxx</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>The statistics of two pruning plans (averaged over 10 runs).</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Original Plan Refined Plan</cell></row><row><cell></cell><cell>Degenerated</cell><cell>158.3</cell><cell>157.0</cell></row><row><cell>All Categories</cell><cell>Improved</cell><cell>149.7</cell><cell>152.1</cell></row><row><cell></cell><cell>Accuracy</cell><cell>85.18%</cell><cell>85.30%</cell></row><row><cell></cell><cell>Degenerated</cell><cell>35.1</cell><cell>20.2</cell></row><row><cell>'buildings' Only</cell><cell>Improved</cell><cell>12.1</cell><cell>17.8</cell></row><row><cell></cell><cell>Accuracy</cell><cell>81.46%</cell><cell>86.17%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">is the final pruned model, and the results are worse than the other two strategies. If we use a smaller removal number, e.g., removing 1% fil-</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported by the Strategic Priority Research Program of the Chinese Academy of Sciences, grant No. XDA19080102. The work was started at The Ohio State University when Guan was visiting the GRAVITY research group. The authors would like to thank all GRAVITY members for their suggestions and insightful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pytorch</surname></persName>
		</author>
		<ptr target="https://pytorch.org" />
		<imprint>
			<biblScope unit="page" from="2020" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Do convolutional neural networks learn class hierarchy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bilal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="152" to="162" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">learning-compression&quot; algorithms for neural net pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Carreira-Perpinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Idelbayev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8532" to="8541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Visual analytics for explainable deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="84" to="92" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to prune deep neural networks via layer-wise optimal brain surgeon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4857" to="4867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Coreset-based neural network compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="469" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2019 : 7th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR &apos;14 Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dynamic network surgery for efficient dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;16 Proceedings of the 30th International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1387" to="1395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2016 : International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;15 Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Soft filter pruning for accelerating deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI 2018: 27th International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2234" to="2240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Filter pruning via geometric median for deep convolutional neural networks acceleration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4340" to="4349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Channel pruning for accelerating very deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1398" to="1406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gan lab: Understanding complex deep generative models using interactive visual experimentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H P</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Viegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="310" to="320" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of The ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intelligent Signal Processing</title>
		<imprint>
			<biblScope unit="page" from="306" to="351" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Optimal brain damage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="598" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pruning filters for efficient convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2017 : International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Analyzing the training processes of deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="87" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards better analysis of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="100" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning efficient convolutional networks through network slimming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2755" to="2763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Thinet: A filter level pruning method for deep neural network compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5068" to="5076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rulematrix: Visualizing and understanding classifiers with rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bertini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="342" to="352" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pruning convolutional neural networks for resource efficient inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2017 : International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deepeyes: Progressive visual analytics for designing deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pezzotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hollt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>Lelieveldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eisemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vilanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="108" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Squares: Supporting interactive performance analysis for multiclass classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="70" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Conversational speech transcription using context-dependent deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="437" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2015 : International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Striving for simplicity: The all convolutional net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (workshop track)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Convolutional neural networks with low-rank regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2016 : International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Clip-q: Deep network compression learning by in-parallel pruning-quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7873" to="7882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dqnviz: A visual analytics approach to understand deep q-networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="288" to="298" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ganviz : A visual analytics approach to understand the adversarial game</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1905" to="1917" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deepvid : Deep visual interpretation and diagnosis for image classifiers via knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2168" to="2180" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Nisp: Pruning networks using neuron importance score propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9194" to="9203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A survey of visual analytics techniques for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Manifold: A modelagnostic framework for interpretation and diagnosis of machine learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Ebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="364" to="373" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Accelerating very deep convolutional networks for classification and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1943" to="1955" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
