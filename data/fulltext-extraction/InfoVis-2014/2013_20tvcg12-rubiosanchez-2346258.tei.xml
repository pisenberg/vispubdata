<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Axis Calibration for Improving Data Attribute Estimation in Star Coordinates Plots</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Rubio-Sánchez</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Sanchez</surname></persName>
						</author>
						<title level="a" type="main">Axis Calibration for Improving Data Attribute Estimation in Star Coordinates Plots</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TVCG.2014.234625</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Star Coordinates</term>
					<term>RadViz</term>
					<term>Biplots</term>
					<term>Axis calibration</term>
					<term>Attribute value estimation</term>
					<term>Data centering</term>
					<term>Orthographic projection</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Star coordinates is a well-known multivariate visualization method that produces linear dimensionality reduction mappings through a set of radial axes defined by vectors in an observable space. One of its main drawbacks concerns the difficulty to recover attributes of data samples accurately, which typically lie in the [0,1] interval, given the locations of the low-dimensional embeddings and the vectors. In this paper we show that centering the data can considerably increase attribute estimation accuracy, where data values can be read off approximately by projecting embedded points onto calibrated (i.e., labeled) axes, similarly to classical statistical biplots. In addition, this idea can be coupled with a recently developed orthonormalization process on the axis vectors that prevents unnecessary distortions. We demonstrate that the combination of both approaches not only enhances the estimates, but also provides more faithful representations of the data.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Exploratory data analysis is an approach aimed at obtaining overviews of data sets, and consists of an integral component of a data mining process. Many of the techniques generate visual representations of data sets, where users are integrated into the analysis process with the goal of combining their perceptual capabilities, flexibility, or domain knowledge with the computational power of today's computers.</p><p>Multivariate visualization techniques can be categorized according to different criteria, including data type, the graphical objects and layouts that compose the plots, or forms of interaction <ref type="bibr" target="#b15">[16]</ref>. Some are capable of displaying high-dimensional data without any loss of information (e.g., scatterplot matrices, parallel coordinates <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b10">11]</ref>, or the table lens <ref type="bibr" target="#b18">[19]</ref>), which show exact attribute values of data samples directly on the plots. In contrast, in this paper we focus on methods based on radial axes (see <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4]</ref>) that generate transformations of data onto an observable display, where information is inevitably lost in the dimensionality reduction process. For these methods users generally cannot determine precise values of data samples by just observing their low-dimensional representations. Thus, users can only recover original data values visually to a certain extent. In this paper we refer to this process as attribute (or data value) estimation. Although these methods are useful for exploratory purposes in the overview phase of the well-known information seeking mantra <ref type="bibr" target="#b20">[21]</ref>, the differences between the true and estimated data values may cause misinterpretations.</p><p>In this paper we study attribute estimation accuracy in Star Coordinates (SC) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b17">18]</ref>, which is acknowledged to be one of the method's major drawbacks <ref type="bibr" target="#b4">[5]</ref>. Our goal is to provide an indepth analysis of several operations that allow to obtain more accurate estimates, and help users compare, group, examine, and understand the data quickly, without the need to inspect numerical values directly. In particular, we show that the accuracy of the estimates can be enhanced by reading off data values through projections of embedded points onto "calibrated" (i.e., labeled) axes, similarly to classical statistical biplots. When using this approach we prove that estimation accuracy can be increased considerably by centering the data. Thus, the usual preprocessing operation that normalizes the data to lie in the [0,1] interval is not appropriate for estimating attribute values through projections onto calibrated axes. Additionally, it has been shown recently that the axis vectors can be modified or restricted in order to</p><p>• Manuel Rubio-Sánchez is with URJC. E-mail: manuel.rubio@urjc.es.</p><p>• Alberto Sanchez is with URJC. E-mail: alberto.sanchez@urjc.es.</p><p>generate orthographic projections in SC plots <ref type="bibr" target="#b16">[17]</ref>, which also helps to reduce estimation errors, although at a lesser extent than the centering approach. We demonstrate the accuracy improvement through a comparative study that includes models of how users estimate data values in SC and RadViz <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b2">3]</ref>, which is another popular method based on radial axes. Finally, we show that the combination of the centering and the orthographic projections not only enhances the estimates in SC, but also provides more faithful representations of the data.</p><p>The rest of the paper is organized as follows. Section 1 reviews the related work. Section 2 describes the theory underlying the proposed approach, while Sec. 3 presents experimental results. Finally, Sec. 4 provides a discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">RELATED WORK</head><p>Star coordinates (see <ref type="figure" target="#fig_0">Fig. 1</ref>) is a simple, efficient, and well known interactive method for multivariate data visualization that can be used for exploratory purposes including cluster analysis, outlier and trend detection, or decision making tasks. The method generates linear dimensionality reduction mappings from an n-dimensional data space to a lower m-dimensional observable space (m ≤ 3) in order to represent the data graphically. In particular, it constructs plots through a set of m-dimensional vectors v i , for i = 1, . . . , n, with a common origin point that represent radial axes, where v i is associated with the i-th data variable. The low-dimensional embedding p ∈ R m of a data sample x ∈ R n is simply a linear combination of the vectors v i , where the linear coefficients correspond to the variable attributes of x. Formally:</p><formula xml:id="formula_0">p = x 1 v 1 + x 2 v 2 + • • • + x n v n = V T x,<label>(1)</label></formula><p>where V is the n × m matrix whose rows are the vectors v i . The interpretation of the axis vectors is straightforward: the orientation determines the direction in which a variable increases, and the length specifies the amount of contribution of a particular variable in the resulting visualization, given that all variables have a similar scaling. Any dimensionality reduction process naturally entails a loss of information. Thus, in SC users will not be able to recover attributes of data samples perfectly, given their embedded points and the axis vectors. In addition, estimating attribute values is particularly difficult in SC <ref type="bibr" target="#b4">[5]</ref>. When thinking of the linear combination, the estimates of attribute values are not independent of each other, since assigning some particular value to a variable affects the possible values of the estimates for the rest of the variables. In this regard, the estimation process can be understood graphically as building a "path" from the origin to a known p, by concatenating scaled axis vectors of lengthx i • v i , wherex i is the estimate of x i , similarly to the SC mapping procedure, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Thus, fixing a particular estimate (which defines the length of a segment) affects the values of the rest. When estimating attributes through this approach users must therefore consider all of the </p><formula xml:id="formula_1">Sfrag replacements v 1 v 2 v 3 v 4 p = 0.25v 1 + 0.5v 2 + 0.75v 3 + v 4</formula><p>x = (0.25, 0.5, 0.75, 1) possible estimates simultaneously, which is complicated, especially as the number of variables increases. In order to mitigate this issue, we focus on optimizing the estimates when they correspond to orthogonal projections of embedded points onto axes, similarly to biplots <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b7">8]</ref>. These classical statistical visualizations essentially form scaled versions of the principal component analysis (PCA) plot, representing the best rank m approximation of the data in a least squares sense (see <ref type="bibr" target="#b12">[13]</ref>). One of the most interesting properties of biplots is the ability to read off estimates of data attributes by projecting embedded points orthogonally onto adequately calibrated axes, as shown in <ref type="figure" target="#fig_3">Fig. 2</ref>. In biplots, the estimatex i for the i-th attribute is defined as a dot product:</p><formula xml:id="formula_2">x i = v T i p = v T i p v i v i .</formula><p>Thus, the (orthogonal) scalar projections must be multiplied by the lengths of the axis vectors, where the distance between consecutive integers on the i-th axis is 1/ v i . Finally, the estimatex of a data sample is therefore:x = Vp.</p><p>In Section 3.1 we describe a model of how users typically estimate values in SC. Another well known technique related to SC is RadViz, where the vectors v i define anchor points of springs in a physical model rather than axes (see <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b2">3]</ref>). In particular, it generates nonlinear projections according to:</p><formula xml:id="formula_4">p = ∑ n i=1 x i v i ∑ n i=1 x i = V T x 1 T x ,<label>(3)</label></formula><p>where the data is normalized so that the range of each variable is the [0, 1] interval. Nevertheless, <ref type="bibr" target="#b2">(3)</ref> shows that RadViz is equivalent to SC if each data sample is also processed so that the sum of its elements is equal to 1. Note that this last normalization step produces nonlinear mappings of the data onto an observable display, since it applies a linear mapping (SC) to points projected nonlinearly onto the unit (n − 1)-simplex (1</p><formula xml:id="formula_5">T x = 1). −3 −2 −1 0 1 2 3 4 5 −1.5 −1 −0.5 0 0.5 1 1.5 1 −1 1 −1 1 −1 1 −1 replacements v 1 v 2 v 3 v 4x 1x 2 x 3x 4 p p 1 p 2 Fig. 2.</formula><p>Estimation of data values through projections in biplots. The estimatex ∈ R 4 of a data sample x ∈ R 4 that has been embedded onto p ∈ R 2 , is found by projecting p onto adequately calibrated axes associated with the variables, and defined by the axis vectors v i .</p><p>In RadViz the anchor points are usually arranged in a regular pattern, although it is possible to update their location interactively. The embedded points p will lie inside the convex hull defined by the anchor points, since the former are convex combinations of the latter. In regular patterns the method has the following properties: (1) the larger x i is in comparison with the rest of values, the closer p will be to the anchor point v i , (2) samples whose i-th element is the only nonzero one are mapped to v i , (3) samples whose data attributes are all the same get mapped to the origin, and (4) when the embedding p is located on an edge of the convex hull between v i and v j , then the attributes for the remaining variables will be 0. In Section 3.2 we describe a model of how users can estimate values in RadViz that is consistent with these properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">CALIBRATION AND OPTIMAL ESTIMATES</head><p>The estimation error associated with approximating data attributes through projections onto calibrated axes as in biplots, through (2), can be defined as:</p><formula xml:id="formula_6">ε = x − x = Vp − x ,</formula><p>where • is some vector norm (in this paper we use the Euclidean norm or its square). For SC, the estimation error is therefore:</p><formula xml:id="formula_7">ε = VV T x − x ,<label>(4)</label></formula><p>due to (1). The following subsections describe approaches that minimize this error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data centering</head><p>When using (4) the data preprocessing approach turns out to be fundamental in order to estimate values accurately across an entire data set. Firstly, note that, due to (2), the estimatex of a data sample is a linear combination of the columns of V. Therefore, it lies on the range or "column space" of V, which is the subspace spanned by the columns of V. In this paper it is a plane embedded in R n since we only consider the case when m = 2. In the remainder of the paper we refer to this subspace as R(V), where R denotes range. Additionally, the distance between x and its estimatex is precisely the estimation error. Thus, it is also related to the distance from x to R(V), where it is apparent that the data should lie close to that subspace in order to obtain accurate estimates (see <ref type="figure">Fig. 3</ref>). The following results show that, in order to minimize the sum of squared estimation errors for an entire data set, the data can be shifted so that its new mean lies in R(V), which may as well be the origin. Thus, centering the data will allow to obtain more accurate estimates in general. </p><formula xml:id="formula_8">Proposition 1. Let V</formula><formula xml:id="formula_9">N ∑ i=1 Vp i − (x i − t) 2 ,<label>(5)</label></formula><p>PSfrag replacements</p><formula xml:id="formula_10">R(V) x ō x s</formula><p>Original data Shifted data <ref type="figure">Fig. 3</ref>. Consider a particular configuration of axis vectors determined by V. In that case the original data (with meanx o ) should be shifted so that its new mean (x s ) lies on R(V) in order to obtain more accurate estimates through projections, since estimation errors are related to distances from points to R(V). The shifting operation reduces these distances on average, and therefore the estimation errors. is minimized if t =x + z, wherex is the sample mean of the data, and z is any vector in the range of V.</p><formula xml:id="formula_11">p p x x VV T V ⊥ V ⊥ † = V ⊥ V ⊥ T xx R(V) R(V) = R(V ⊥ ) R 2 R 2 V † V ⊥ † = V ⊥ T e 1 e 2 e 1 e 2 v 1 v 2ṽ ⊥1 v ⊥2 (a) (b) Isometries R n R n ε = x − x ε = x − x</formula><p>Proof. The optimal shift t of the data is found by minimizing <ref type="bibr" target="#b4">(5)</ref>.</p><p>Since p = V T x in SC, it follows that p i = V T (x i − t) for the i-th data point. Thus, the function to be minimized is:</p><formula xml:id="formula_12">f (t) = N ∑ i=1 VV T (x i − t) − (x i − t) 2 = N ∑ i=1 (I − VV T )t − (I − VV T )x i 2 = N ∑ i=1 Bt − c i 2 = N ∑ i=1 t T B T Bt − 2t T B T c i + c T i c i = Nt T B T Bt − 2t T B T N ∑ i=1 c i + N ∑ i=1 c T i c i ,</formula><p>where I is the identity matrix, B = (I − VV T ), and c i = Bx i . Taking derivatives with respect to t and setting them to 0:</p><formula xml:id="formula_13">∂ f ∂ t = 2NB T Bt − 2B T N ∑ i=1 c i = 0, 2NB T Bt − 2B T B N ∑ i=1 x i = 0, B T Bt − 1 N B T B N ∑ i=1 x i = B T B(t −x) = 0. Thus, t =x + z, where z ∈ R(V), since R(V) = R(VV T ) = R(I − VV T ) ⊥ = N (I − VV T ) = N (B),</formula><p>where N denotes nullspace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corollary 1.</head><p>In SC the data can be centered in order to minimize the sum of squared estimation errors for all of the points in a data set when V has full column rank.</p><p>Proof. From Prop. 1 it follows that t can bex, since we can choose z = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Orthonormalization of the columns of V</head><p>Recently, Lehmann and Theisel <ref type="bibr" target="#b16">[17]</ref> proposed to restrict SC to orthographic projections in the context of interaction in order to improve the quality of the visualizations. The idea consists of replacing the columns of V by an orthonormal set of vectors that span the same subspace as the original ones. This generates a new set of axis vectors that produces a similar plot, where the estimates for each data sample are optimal, and which leads to more faithful representations of the data. In this section we review the approach but focusing on enhancing attribute estimation accuracy. Firstly, note that the SC mapping (see (1)) can be decomposed into two separate linear transformations:</p><formula xml:id="formula_14">p = V T x = [V † ][VV T ]x = V †x ,</formula><p>where † denotes the Moore-Penrose pseudoinverse (we assume that V has full column rank, where</p><formula xml:id="formula_15">V † = (V T V) −1 V T ).</formula><p>Geometrically, the idea is illustrated in <ref type="figure" target="#fig_1">Fig. 4a</ref>. Thus, we can interpret that SC first projects data samples onto R(V), through VV T , where the resulting points constitute the n-dimensional estimates. In order to complete the process, the second mapping defined by V † simply rotates and scales the estimates in R(V) in order to represent them in the observable space (R m ). In particular, the column vectors of V, denoted asṽ i , for i = 1, . . . , m, are transformed into the vectors of the standard basis (e i ) of the observable space.</p><p>Since the final plot can be interpreted as a simple linear transformation of the information on R(V), we can substitute the column vectors of V by an orthonormal set that spans the same subspace, as shown in  <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref>, shown as a gray area. The points have been colored according to protein content, where the color bar indicates the particular coding. Note that lighter points, with higher values, are located towards the right, in the direction of the axis vector. Additionally, since the estimates for protein are the (orthogonal) projections of the points onto its labeled axis, ideally they should be located inside the gray area. In (a) the plot uses data in [0,1] (not centered) and a configuration of axis vectors that does not generate orthographic projections, where the estimates are highly inaccurate. In (b) the columns of V form an orthonormal set, which has a positive effect on the accuracy. In (c) the vectors do not produce orthographic projections but the [0,1] data has been centered, which allows to reduce the squared estimation errors considerably. Finally, in (d) both approaches are combined to reduce the attribute estimation error even further, where almost every point lies in the <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref> interval.</p><p>through the Gram-Schmidt orthonormalization procedure ( <ref type="bibr" target="#b16">[17]</ref> also proposes a gradient descent based algorithm which essentially mini-</p><formula xml:id="formula_16">mizes V T V − I 2</formula><p>Fro , where Fro denotes the Frobenius norm). This defines an alternative configuration of axis vectors, but has several benefits that we describe below.</p><p>On the one hand, recall that the estimation errors are the distances from x tox. Thus, these are minimized for individual points ifx is the orthogonal projection of x onto R(V). In SC the optimal estimates are achieved if and only if VV T is an orthogonal projection matrix, and this occurs if and only if the columns of R(V) form an orthonormal set of vectors (i.e., if and only if V T V = I). If the column vectors of V do not form an orthonormal set the projections are oblique and can lead to large estimation errors.</p><p>On the other hand, when using V ⊥ dot products, and therefore distances and angles, between points that lie on R(V) are preserved exactly on the low dimensional display, due to the following result. Proof. Consider two points p, q ∈ R m , and their images Vp, Vq ∈</p><formula xml:id="formula_17">R(V), then: p, q = p T q = p T V T Vq = Vp, Vq ⇔ V T V = I,</formula><p>where •, • denotes dot product. Similarly, the linear mapping from R(V) to R m defined by V † will also be an isometry:</p><formula xml:id="formula_18">Vp, Vq = p, q = V † Vp, V † Vq ,</formula><p>if and only if V defines an isometry from R m to R(V), which happens if and only if V T V = I, as proved above.</p><p>Finally, we provide a result that shows that the lengths of the axis vectors are at most 1 when the columns of V form an orthonormal set of vectors: Proposition 3. Let V be an n×m matrix, where n ≥ m, whose columns form an orthonormal set of vectors. The Euclidean norm of any row of V will be at most 1.</p><p>Proof. Firstly, the entries of V are constrained since the Euclidean norms of the columns are equal to 1. Additionally, since V has rank m, in order to find the largest squared values for the entries of some rows we can assume that all of the entries of n − m rows of V are 0. In this optimal scenario, the remaining m rows form a matrix R which is orthogonal, where R T R = RR T = I, which implies that the rows of R have unit Euclidean norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Benefits of axis calibration for data analysis</head><p>The new visualizations with centered data and V ⊥ provide more faithful representations that can be useful in tasks where users need to approximate data values quickly. <ref type="figure" target="#fig_2">Figure 5</ref> shows the benefit on estimation accuracy of combining both approaches, on configurations of five variables of the breakfast cereal data set available in lib.stat.cmu.edu/datasets/1993.expo/. For reference, we have labeled (calibrated) the axis associated with protein content with the original values (in grams), which lie in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref>, and have shaded the area corresponding to such interval. Thus, if an embedded point falls outside of the gray area its orthogonal projection onto the protein axis will result in a value that is outside of <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref>. Lastly, we have colored the embedded samples according to their protein content (the color bars in the middle show the particular color coding).</p><p>In <ref type="figure" target="#fig_2">Fig. 5a</ref> the plot is generated with data normalized to lie in [0,1], and is therefore not centered. Regarding the configuration of axis vectors, when they are regular, as in the graphics, they only provide orthographic projections when the lengths of the axis vectors are 2/n (where n is the number of variables) <ref type="bibr" target="#b16">[17]</ref>. In <ref type="figure" target="#fig_2">Fig. 5a</ref> their length is 1, and therefore V is not an orthogonal matrix. The estimates associated with the resulting plot are therefore highly inaccurate, where more than half of the points are not even in the gray area (note that some have large negative values). In <ref type="figure" target="#fig_2">Fig. 5b</ref> the lengths of the axis vectors are 2/5, which cause the columns of V to form an orthonormal set of vectors. This modifies the scaling on the axes, which has a positive effect on the estimation accuracy, even though the number of points that fall inside the gray area is the same as in <ref type="figure" target="#fig_2">Fig. 5a</ref>. In <ref type="figure" target="#fig_2">Fig. 5c</ref> the vectors have unit length but the data (previously in [0,1]) has been centered, which reduces the squared estimation error considerably. Finally, in <ref type="figure" target="#fig_2">Fig. 5d</ref> we have used this centered data and an orthogonal matrix, where most of the points fall inside the gray area, and provide the lowest squared estimation errors.</p><p>In addition, when users inspect data for decision making a commonly used approach consists of pointing vectors associated with desirable features in a similar direction, while pointing undesirable vari-  ables in the opposite direction. <ref type="figure" target="#fig_4">Figure 6a</ref> shows a configuration of axis vectors used to characterize healthy vs. unhealthy breakfast cereals, similar to that in <ref type="bibr" target="#b23">[24]</ref>. The result of using V ⊥ with centered data, illustrated in <ref type="figure" target="#fig_4">Fig. 6b</ref>, shows that the original configuration unnecessarily stretched the plot along the horizontal axis, introducing distortions in the visualization. In practice, the data samples will not lie on R(V) exactly, causing nonzero estimation errors that can be visualized in order to examine which data samples are being represented well for the particular set of axis vectors. When they are large they provide warnings that indicate the need to examine data samples carefully before making decisions. However, if V ⊥ is used with centered data analysts can be confident not only about interpreting values of points that have been represented accurately, but also regarding distances and angles between them. This is illustrated in <ref type="figure" target="#fig_5">Fig. 7</ref> through an example involving three breakfast cereal variables. In <ref type="figure" target="#fig_5">Fig. 7a</ref> the points are depicted as usual, where the color coding corresponds to sugar content. In <ref type="figure" target="#fig_5">Fig. 7b</ref>, we have decreased the size of the dots corresponding to samples for which the estimation errors are large. This operation simply shows samples that have not been represented well as very small dots, since they can be misleading. Notice that healthy cereals with high potassium values and low sugar and sodium have practically disappeared from the plot. This is useful for decision making, where the errors are warning us that the data set does not contain cereals with that combination of features. In addition, we can be very confident about the estimated values of the remaining larger dots, where the distances between them are almost identical to the ones in the data space.  Lastly, <ref type="figure" target="#fig_7">Fig. 8</ref> shows an example with a 3-dimensional toy data set containing six small clusters. The particular choice of axis vectors results in the range of V passing through only two of the clusters, as shown in <ref type="figure" target="#fig_7">Fig. 8a</ref>. Therefore, the points in those clusters are represented well, and analysts can be confident about correctly interpreting and comparing their values. Furthermore, the relative distances between those points are also better represented. The result of the method's linear mapping is shown in <ref type="figure" target="#fig_7">Fig. 8b</ref>. In order to focus on the points that have been represented well, those with larger estimation errors have been displayed with smaller dots. Thus, it is important to note that the approach is effectively revealing the loss of information associated with the ability to recover original data values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PSfrag replacements</head><formula xml:id="formula_19">v X v Y v Z (b)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EVALUATION OF THE ESTIMATION ACCURACY</head><p>In this section we evaluate the accuracy of the estimates associated with the approaches on real data sets, and describe models of how users estimate data values in SC and RadViz. In particular, Tab. 1 summarizes the methods' approaches and estimation strategies used in the experiments.</p><p>The user study involved 10 computer science professors (all male, with average age 37) and 10 graduate students (with average age 28, where only 2 were female), at Universidad Rey Juan Carlos, in Madrid, Spain. The participants did not have a previous background in SC nor in RadViz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Star Coordinates</head><p>Before performing estimation experiments we explained to them SC in detail. We made special emphasis on understanding the linear combination geometrically as a weighted sum of vectors, which could be interpreted as a path from the origin to the low-dimensional point p, as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>For this purpose, we developed a tool to teach the method prior to performing the estimation experiments. Given a configuration of axis vectors, and a low-dimensional point p corresponding to a particular data instance x from the breakfast cereal data set (with values in [0, 1]), it allowed users to visualize and construct a path from the origin to p by updating the coefficients associated with the axis vectors. These values would represent possible estimates for the data sample x that had been mapped onto p by SC. At the end of the process the true values of x were revealed to the users. One of our goals consisted of showing that there is usually an infinite number of possibilities to choose the coefficients. Additionally, by using the cereal database, which is not sparse, we intended to show that in most cases the actual values of x were not 0 (i.e., most variables contributed a segment to the path). In total, each participant estimated values of nine data entries in this training stage: 3 (for sets of 3, 6, and 9 variables) × 3 instances.</p><p>Afterwards we measured the participants' ability to estimate data attributes in SC without the aid of calibrated axes. In a first experiment we showed users the embeddings of randomly chosen data samples from the breakfast cereal database and the wine data set in <ref type="bibr" target="#b5">[6]</ref>, together with configurations of axis vectors (V) whose directions were random and lengths were drawn from a uniform U [0.5, 2.5]. This particular choice implies that the longest vector will be at most 5 times larger than the smallest. On the one hand, we guarantee that the variable with the smallest vector will still have a relevant effect on the plot, since the contribution of a variable is proportional to the length of its axis vector. On the other hand, we chose the upper limit of 2.5 in order to expect a notable reduction of the estimation errors when carrying out the orthonormalization process. Note that V⊥ could be quite different than V, since the length of its axis vectors can be at most 1, due to Prop. 3.</p><p>The experiments considered configurations of 3, 5, 7, and 9 variables, also chosen at random from the data sets. Finally, we asked the participants to estimate the value of only one of the attributes (also selected at random). In total, every participant estimated 40 values: 5 attribute values × 2 data sets × 4 (for sets of 3, 5, 7 and 9 variables). It is worth mentioning that we encouraged the participants to use a fast and intuitive estimation strategy that would not involve complex calculations, since otherwise users could simply look up the values of the data quicker on the graphical interface (e.g., in a table). This allowed them to report values typically in under 15 seconds.</p><p>Finally, we compared the results with those relative to automatic strategies based on projections onto calibrated axes (see Tab. 1). <ref type="figure">Figure 9</ref> shows average estimation errors for the user (SC User ), and automatic estimates (see the supplemental material for separate results on each data set). The reported measure for a certain number of variables n is:</p><formula xml:id="formula_20">υ(n) = 1 P • D • M P ∑ p=1 D ∑ d=1 M ∑ i=1 |x i,d,p − x i,d,p |,</formula><p>where P = 20 is the number of participants, D = 2 is the number of data sets, M = 5 is the number of values to estimate, x i,d,p is the ith value to be estimated for the d-th data set and p-th participant, and finallyx i,d,p is the corresponding estimate. It is apparent that, although the orthonormalization process has a positive effect, the estimates based on projections onto calibrated axes are only appropriate when the data is centered. In that case the approach clearly provides more accurate estimates than the users'. Without the aid of the calibrated axes the estimation task requires carrying out complex calculations that are generally inaccurate and too time consuming.</p><p>While in the first experiment users only had to estimate one value per data instance, we carried out another study where we asked the participants to estimate all of the attributes. Our goal was not only to evaluate the accuracy of users' estimates, but also to analyze their strategies.</p><p>In this new experiment we showed regular (with unit vectors) and random configurations of axis vectors (again with random directions  <ref type="figure">Fig. 9</ref>. Average estimation errors υ(n) obtained in experiments with users for SC, and the automatic estimates onto calibrated axes. Projecting points onto calibrated axes allows to obtain more accurate results, but the data must be previously centered. , and the embedding of a randomly chosen data sample x from the breakfast cereal database, where the variables were normalized to lie in [0,1]. The experiments considered sets of 3, 6, and 9 variables, also chosen at random from the database. Finally, we asked the participants to estimate the attributes of x. In total, every participant estimated the variable values of 12 data instances: 2 (for regular and random configurations) × 3 (for sets of 3, 6, and 9 variables) × 2 instances. At the end of the experiment we asked the participants to explain their strategies. We identified one main approach, used by 13 participants (65%), where they searched for estimates (i.e., the coefficients of the linear combination) that minimize the length of the path from the origin to p by using the least number of variables as possible (typically, only two or three), while satisfying the linear combination equation <ref type="formula" target="#formula_0">1</ref>, as well as the constraints associated with the data normalization. Thus, many of the reported estimates were 0, despite our efforts in the training stage to show that the solutions were usually not sparse. This simple approach arises not only due to the difficulty to estimate values, but also to the necessity to visualize them quickly. For this strategy users needed about 20 seconds to report the nonzero estimates. Note that the particular scale of the axis vectors does not affect the user estimates. In other words, we would have obtained identical results had we chosen a uniform U [0.2, 1] for the lengths of the axis vectors. Lastly, we did not identify other common strategies.</p><p>In order to extrapolate the results to more data we modeled the user strategy, denoted as SC M , with the following convex optimization problem:</p><formula xml:id="formula_21">minimize x ∈ R n Dx 1 subject to V T x = p, 0 x 1,<label>(6)</label></formula><p>where D is diagonal matrix in which d i,i = v i , and denotes vector componentwise inequality. Note that Dx 1 corresponds to the length of the path. Thus, the optimization problem models the fact that users generally search for solutions where the path associated with the linear combination is shortest. Moreover, the 1 norm is appropriate for obtaining solutions that use only a few variables (nonzero coefficients), since it usually produces sparse solutions, i.e., many values ofx will be 0, which is equivalent to not considering a variable in the linear combination. The supplemental material includes an alternative formulation (as a linear program) <ref type="bibr" target="#b0">[1]</ref>, and shows that the solutions are sparse. <ref type="figure" target="#fig_0">Figure 10</ref> analyzes the validity of model SC M by examining the discrepancy between the model and user estimates (only for the 13 participants who used the associated strategy), and evaluating their average estimation errors. In <ref type="figure" target="#fig_0">Fig. 10a</ref> the estimates of the model (x Model ), are very close to the ones provided by the users (x User ). Thus, the average squared estimation errors for SC User and SC M are also very similar. In <ref type="figure" target="#fig_0">Fig. 10b</ref> the average estimation errors for random configurations are also very close, despite a higher discrepancy ( x User −x Model 2 ) between the users' and the model's estimates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">RadViz</head><p>In addition, we studied possible ways users estimate values in RadViz. Since the method was new to the participants we explained to them its physical spring model and described its properties, as mentioned in Sec. 1. ). Therefore, the average squared estimation errors for SC User ( x −x User 2 ) and SC M ( x −x Model 2 ) are also very similar. For random configurations there is more discrepancy between the model and the user estimates, but the average estimation errors are nevertheless still very similar.</p><p>Firstly, we carried out an experiment where, given a regular configuration of five variables (with anchor points distributed uniformly on the unit circle), and a point p inside the pentagon formed by the anchor points, users were asked to intuitively choose all of the values of some possible instance that would have been mapped onto p. In particular, each participant chose the five attribute values associated with five points. Let IC denote these "intuitive choices".</p><p>Afterwards, we asked the participants to obtain a formula or algorithm for visualizing values quickly, which would be consistent with the RadViz properties. They were given 24 hours to come up with a strategy. The following day only two professors and one student reported a consistent strategy, which was identical. In particular, they formed estimates by tracing a line segment from the anchor v i to p, and finally reaches the convex hull of the anchors at a point c. The i-th estimatex i is then defined as:</p><formula xml:id="formula_22">x i = c − p / c − v i .<label>(7)</label></formula><p>This approach, denoted as RadViz M , is illustrated in <ref type="figure" target="#fig_0">Fig. 11</ref>.</p><p>Other users reported strategies that were consistent with every property but the 4-th, when p is on an edge of the convex hull of the anchor points. For instance, 5 participants usedx i = c − p /d, where d = 2 v is the diameter of the circumference on which the anchor points lie. However, these users corrected their strategy to (7) when notified that their initial solution was inconsistent. Other solutions involved ratios of areas, or the ratio between the distance from anchor i <ref type="figure" target="#fig_0">Fig. 11</ref>. Estimation strategy employed by users in RadViz for individual variables that is consistent with the method's properties.</p><formula xml:id="formula_23">PSfrag replacements v i c px i = c − p / c − v i</formula><p>to p and the sum of the distances from p to the rest of the anchors. Finally, we compared the intuitive choices IC before the participants thought of a strategy, to those produced by the RadViz model for the same settings used in the initial experiment. <ref type="figure" target="#fig_0">Figure 12</ref> shows a histogram of the differences between the estimates. Most of the discrepancies have an absolute value less than 0.1, which indicates that RadViz M approximates the users' intuitive estimates fairly well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison among models</head><p>We have empirically compared the estimation accuracy of the described approaches by carrying out a simulation involving the breakfast cereal data set, the Breast Cancer Wisconsin (Diagnostic), Parkinsons, and Wine data sets available in <ref type="bibr" target="#b5">[6]</ref>. The experiment measured average estimation errors for configurations of axis vectors where the variables were selected at random. In particular the reported measure for a data set is:</p><formula xml:id="formula_24">δ (n) = 1 n • T T ∑ t=1 x t − x t ,</formula><p>where n is the number of variables, T = 2000 is the number of trials, x t is the randomly chosen sample from the data set on the t-th trial, andx t is the corresponding vector of estimates for such sample. <ref type="figure" target="#fig_0">Figure 13</ref> shows results on random configurations of axis vectors. The vectors' angles were chosen at random, while their lengths followed a uniform U [0.5, 1] distribution. The figure shows that centering the data (PRc) has a greater positive effect on estimation accuracy than the orthonormalization process alone (PR⊥). However, both approaches combined lead to the best results (PR⊥c). The results are poor for SC M , PR, and even PR⊥. In this regard, notice that the differences between V and V ⊥ are relatively small, since the lengths of the axis vectors are at most 1 (see Prop. 3). Thus, the results for PR and PR⊥, and for PRc and PR⊥c, are similar. Finally, RadViz M can achieve, at most, the performance of PRc. It is important to note that, since the axis vectors define the anchor points, in this setting these are not arranged in a regular pattern. <ref type="figure" target="#fig_0">Figure 14</ref> shows results on regular orthonormalized configurations of axis vectors. In these examples it is apparent that the centering is crucial for obtaining accurate estimates. In this case the anchors for RadViz are arranged in a regular pattern, which has a positive effect on the estimation accuracy of RadViz M . <ref type="figure" target="#fig_0">Figure 15</ref> shows results on PCA configurations of axis vectors. In these cases the columns of V, which are the eigenvectors of the data's covariance matrix, form an orthonormal set, and therefore generate orthographic projections in SC. The examples also show the importance of centering regarding estimation accuracy. In all cases <ref type="figure" target="#fig_0">(Fig. 13, 14</ref>, and 15), the most accurate estimates are obtained by PR⊥c (orthonormalized vectors with centered data), as predicted by the theoretical analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DISCUSSION</head><p>In this paper we have addressed one of the main drawbacks of SC, namely, the difficulty to estimate data attributes, providing a new approach to mitigate the issue where estimates are computed through projections onto calibrated axes, similarly to statistical biplots. However, the data and axis vectors must satisfy certain conditions in order to obtain accurate estimates. In particular, the columns of V must form an orthonormal set of vectors, and the data must be centered.</p><p>Specifically, the estimates after centering the data can be considerably more accurate than those provided by orthonormalizing the columns of V, showing that normalizing the data to lie in [0, 1] is a poor choice regarding estimation accuracy when using projections onto calibrated axes. However, both approaches can be combined to enhance the estimates even further, where the orthonormalization provides several benefits, since it allows to represent the data more faithfully. Additionally, we have also shown that the estimates in our experiments with V ⊥ and centered data are more accurate than those obtained through the SC and RadViz estimation models.</p><p>Another important contribution is a detailed geometrical description of the decomposition of the dimensionality reduction mapping, which explains the optimality of the approaches, why distances in R(V) are preserved on the display, and how to understand plotted information and estimation errors.</p><p>The axis calibration also allows to visualize the errors and interpret them geometrically. This is useful for data analysis since it provides a measure of goodness of a plot (see <ref type="bibr" target="#b1">[2]</ref>). In particular, the errors can be used to discard or highlight data samples that have not been represented well and can be misleading, which can enhance decision support tasks. In addition, they can improve our interpretation of the distances between the embedded samples.</p><p>Naturally, since there is a loss of information in any dimensionality reduction process, our goal is not to recover exact values, but to obtain better estimates for exploratory purposes. Note that the ability to represent an entire data set accurately depends on how well R(V) fits the data. Unless the points lie close to an m-dimensional linear manifold, and the axis vectors are chosen so that R(V) is aligned with it, the estimates for some points will inevitably be inaccurate. In this regard, the configuration of axis vectors for which R(V) fits the data optimally (i.e., minimizes the sum of squared estimation errors over every sample) leads to the PCA plot. However, while in PCA the axis vectors are fixed, SC allows to manipulate them interactively, which is useful in other tasks such as searching for cluster structure, outlier detection, or searching for data with particular characteristics.</p><p>Finally, these improvements on attribute estimation are focused on addressing one of the major goals of data visualization, which is to provide meaningful representations of data that will allow analysts to extract knowledge visually, without the need to inspect concrete values in tables.  <ref type="figure" target="#fig_0">Fig. 13</ref>. Average estimation error δ (n) obtained in a simulation for random configurations of axis vectors.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Diagram of Star Coordinates' linear mapping. A 4-dimensional data sample x = (0.25, 0.5, 0.75, 1) gets mapped onto the 2-dimensional point p by adding scaled versions of the vectors v i , where the scaling factors are the values of x associated with each variable. The process can be understood graphically as building a "path" from the origin to p.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Two-step interpretation of the linear mapping defined by SC. Data samples are first projected onto R(V) through VV T , and afterwards mapped onto the observable space (R 2 ), through V † . Roughly speaking, this last operation performs rotations and scalings in order to alignṽ i with e i . Ifṽ 1 andṽ 2 (the columns of V) do not form an orthonormal set, as shown in (a), the first projection is oblique leading to larger estimation errors (ε = x − x ), while distances and angles in R(V) are not preserved in R 2 . In (b),ṽ ⊥1 andṽ ⊥2 form an orthonormal set, where not only is the estimatex optimal (since it is the orthogonal projection of x onto R(V)), but distances and angles in R(V) are preserved in R 2 due to the isometries between both spaces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 4b, where V ⊥ denotes this new orthogonal matrix of axis vectors (thus, V T ⊥ V ⊥ = I), and whereṽ ⊥i , for i = 1, . . . , m, are the corresponding column vectors. It is straightforward to compute V ⊥ , for example,Squared estimation error for protein = 1.66 Total squared estimation error = 11.7 (d) Comparison of methods. The graphics show SC plots of a breakfast cereal data set containing five variables, where the axis associated with protein content has been labeled with original values that lie in the interval</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Proposition 2 .</head><label>2</label><figDesc>The linear mapping from the observable inner product space R m to R(V), defined by V, as well as the transformation from R(V) to R m , defined by V † , will be isometries if and only if the columns of V form a orthonormal set of vectors (i.e., if and only if V T V = I).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Avoiding unnecessary distortions. In (a) the axis vectors are chosen in order to characterize health vs. unhealthy cereals according to protein, vitamin, sugar, and fat content, where healthy cereals should lie on the right part of the plot. The normalization procedure used in (b) shows that pointing healthy and unhealthy variables in opposite directions as in (a) only stretches and distorts the plot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Visualizing error information. A SC plot for 3 (standardized) breakfast cereal variables is shown in (a) as usual, for an orthonormalized configuration of axis vectors, where the colors represent sugar content. In (b), the samples for which the estimation errors are large are depicted as very small dots. In contrast, we can not only estimate values accurately for the remaining large dots, but the distances between them are almost identical to the ones in the data space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Error information in cluster analysis. Image (a) shows an example with a 3-dimensional toy data set consisting in six small clusters centered at the vertices of an octahedron:(−1, −1, 0), (−1, 1, 0), (1, −1, 0), (1, 1, 0), (0, 0, 1) and (0, 0, −1). The range of V only passes through two of the clusters, centered at (1, −1, 0) and (−1, 1, 0), whose samples are represented well in the low-dimensional plot. The result of the linear mapping is shown in (b), where the points with larger estimation errors are depicted with smaller dots. Thus, analysts should be cautious when interpreting and comparing values or distances between those points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Validity of SC M for (a) regular, and (b) random configurations of axis vectors. In (a) the estimates of the model are very close to the ones provided by users ( x User −x Model 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>FrequencyFig. 12 .</head><label>12</label><figDesc>Histogram of differences between the intuitive choices of values (IC) in the first RadViz user experiment, and the estimates generated by RadViz M for the same settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 14 .</head><label>14</label><figDesc>Average estimation error δ (n) obtained in a simulation for regular configurations V⊥ of axis vectors, which lead to orthographic projections in SC. Note that in this case PR = PR⊥, and PRc = PR⊥c.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 15 .</head><label>15</label><figDesc>Average estimation error δ (n) obtained in a simulation for PCA configurations V⊥ of axis vectors, which form orthographic projections in SC. Note that in this case PR = PR⊥, and PRc = PR⊥c.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Summary of the Methods' Approaches and Estimation Strategies Used to Gather Results.</figDesc><table><row><cell>Result notation Method Data normalization Axis vectors Estimation strategy SC User SC [0,1] V Reported by users SC M SC [0,1] V Modeled by (6) RadViz M RadViz [0,1] V Modeled by (7) PR SC [0,1] V Projections onto calibrated axes PR⊥ SC [0,1] V⊥ Projections onto calibrated axes PRc SC Range 1, centered V Projections onto calibrated axes PR⊥c SC Range 1, centered V⊥ Projections onto calibrated axes</cell></row><row><cell>and lengths drawn from a uniform U [0.5, 2.5])</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported by projects URJC-CM-2010-CET-5185 of the Madrid Regional Authority, TIN2011-29542-C02-01 of the Spanish Ministry of Science and Innovation, the Cajal Blue Brain Project, and the Human Brain Project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Convex Optimization. Cambridge University Press</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Top 10 unsolved information visualization problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Graph. Appl</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="12" to="16" />
			<date type="published" when="2005-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Properties of normalized radial visualizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Daniels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">G</forename><surname>Grinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Glidden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Visualization</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="273" to="300" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Uncovering strengths and weaknesses of radial visualizations-an empirical approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Diehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="935" to="942" />
			<date type="published" when="2010-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey of radial methods for information visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Livnat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Riesenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="759" to="776" />
			<date type="published" when="2009-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asuncion</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The biplot graphic display of matrices with application to principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Gabriel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="453" to="467" />
			<date type="published" when="1971-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Understanding Biplots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gardner-Lubbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Le Roux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Greenacre</surname></persName>
		</author>
		<title level="m">Biplots in Practice. BBVA Foundation</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">DNA visual and analytic data mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Grinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Marx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stanley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th conference on Visualization &apos;97, VIS &apos;97</title>
		<meeting>the 8th conference on Visualization &apos;97, VIS &apos;97<address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="437" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Parallel Coordinates: Visual Multidimensional Geometry and Its Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Inselberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Springer-Verlag New York, Inc</publisher>
			<pubPlace>Secaucus, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Parallel coordinates: a tool for visualizing multi-dimensional geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Inselberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dimsdale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st conference on Visualization, VIS&apos;90</title>
		<meeting>the 1st conference on Visualization, VIS&apos;90<address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1990" />
			<biblScope unit="page" from="361" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Principal component analysis. Springer series in statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">T</forename><surname>Jolliffe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Star coordinates: A multi-dimensional visualization technique with uniform treatment of dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kandogan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Information Visualization Symposium, Late Breaking Hot Topics</title>
		<meeting>the IEEE Information Visualization Symposium, Late Breaking Hot Topics</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="9" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visualizing multi-dimensional clusters, trends, and outliers using star coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kandogan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining, KDD&apos;01</title>
		<meeting>the seventh ACM SIGKDD international conference on Knowledge discovery and data mining, KDD&apos;01<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="107" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Information visualization and visual data mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2002-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Orthographic star coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Theisel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2615" to="2624" />
			<date type="published" when="2013-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visualizing high density clusters in multidimensional data using optimized star coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">V</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Linsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="655" to="678" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The table lens: merging graphical and symbolic representations in an interactive focus+context visualization for tabular information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Card</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference companion on Human factors in computing systems, CHI&apos;94</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="318" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visualization of high dimensional data using an automated 3d star coordinate system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Shaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yeasin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International joint conference on neural networks</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1339" to="1346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The eyes have it: A task by data type taxonomy for information visualizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1996 IEEE Symposium on Visual Languages</title>
		<meeting>the 1996 IEEE Symposium on Visual Languages<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="336" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Interacting with parallel coordinates. Interacting with Computers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Siirtola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-J</forename><surname>Räihä</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1278" to="1309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A clustering-oriented star coordinate translation method for reliable clustering parameterization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Pacific-Asia conference on Advances in knowledge discovery and data mining, PAKDD&apos;08</title>
		<meeting>the 12th Pacific-Asia conference on Advances in knowledge discovery and data mining, PAKDD&apos;08<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="749" to="758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dust &amp; magnet: multivariate information visualization using a magnet metaphor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Melton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Jacko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Visualization</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="239" to="256" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">HOV 3 : An approach to visual cluster analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Orgun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Data Mining and Applications</title>
		<meeting><address><addrLine>Berlin / Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">4093</biblScope>
			<biblScope unit="page" from="316" to="327" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
