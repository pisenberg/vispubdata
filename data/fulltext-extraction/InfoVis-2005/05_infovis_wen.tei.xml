<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T18:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>intelligent multimodal interfaces</term>
					<term>visual context management</term>
					<term>automated generation of visualization</term>
					<term>visual momentum</term>
				</keywords>
			</textClass>
			<abstract>
				<p>We are building an intelligent multimodal conversation system to aid users in exploring large and complex data sets. To tailor to diverse user queries introduced during a conversation, we automate the generation of system responses, including both spoken and visual outputs. In this paper, we focus on the problem of visual context management, a process that dynamically updates an existing visual display to effectively incorporate new information requested by subsequent user queries. Specifically, we develop an optimization-based approach to visual context management. Compared to existing approaches, which normally handle predictable visual context updates, our work offers two unique contributions. First, we provide a general computational framework that can effectively manage a visual context for diverse, unanticipated situations encountered in a user-system conversation. Moreover, we optimize the satisfaction of both semantic and visual constraints, which otherwise are difficult to balance using simple heuristics. Second, we present an extensible representation model that uses feature-based metrics to uniformly define all constraints. We have applied our work to two different applications and our evaluation has shown the promise of this work.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>To support context-sensitive information access and exploration, we are building an intelligent multimodal conversation system, called Responsive Information Architect (RIA). Specifically, RIA allows users to express their information requests in context using multiple modalities, including natural language, GUI, and gesture. Moreover, RIA dynamically creates a tailored response, including visual and spoken outputs ( <ref type="figure">Figure 1)</ref> 1 . RIA is embodied in two applications: a real-estate application that helps users to search for residential properties and a hospitality application that aids users in finding hotels and relevant amenities (e.g., restaurants).</p><p>Based on the understanding of a user request <ref type="bibr" target="#b4">[5]</ref>, RIA automatically creates its response in three steps. First, RIA decides the type of the response. In the case of U1 <ref type="figure">(Figure 1</ref>), RIA decides to present the requested houses directly. Depending on the context, RIA may formulate different types of responses. For example, if the retrieved data set is very large it may ask the user to supply additional constraints. Second, RIA determines the data content of the response <ref type="bibr" target="#b19">[21]</ref>. In responding to U1, RIA selects a subset of house attributes such as the price and style. Third, RIA designs the form of the response using suitable media and presentation techniques <ref type="bibr" target="#b20">[22]</ref>. For example, in R1 (in <ref type="figure">Figure 1</ref>) RIA uses graphics to display house data on a map and speech to summarize the number of retrieved houses.</p><p>To handle a follow-up request like U2 <ref type="figure">(Figure 1</ref>), RIA creates a response R2, which emphasizes the cheapest house while preserving most of the existing visual context provided by R1. In contrast, for request U2', RIA zooms in on the cheapest house and simplifies the display of other information (R2'). Here we use the term visual context to refer to a visual scene that a user perceives when issuing a query. Without integrating new information into an existing scene, a user may have difficulty in comparing and combining information. For example, if R2 displays only the cheapest house, the user may not easily relate this house to others retrieved earlier.</p><p>As demonstrated by this example, RIA tailors its response to a user request at run time. Our focus here is on visual context man-U1 Text: Find homes between $500k and $600k with at least 0.3 acre in central Westchester.</p><p>U2 Text: Which is the cheapest? U2' Text: Just the cheapest R1 Speech: I found 8 houses meeting your criteria. R2 Speech: Here is the cheapest one. R2' Speech: Here it is. <ref type="figure">Figure 1</ref>. A recorded user-RIA conversation fragment: U1-U2 are user inputs, R1-R2 are corresponding RIA-generated responses.</p><p>1 All examples used in this paper are working examples created by RIA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>An Optimization-based Approach to Dynamic Visual Context Management</head><p>Zhen Wen Michelle X. Zhou Vikram Aggarwal IBM T. J. Watson Research Center Hawthorne, NY 10532 {zhenwen, mzhou, aggarwal}@us.ibm.com</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IEEE Symposium on Information Visualization 2005</head><p>October 23-25, Minneapolis, MN, USA 0-7803-9464-X/05/$20.00 ©2005 IEEE.</p><p>agement, a process that dynamically determines how to incorporate newly requested information into an existing visual context so that users can comprehend all relevant information as a coherent whole. More precisely, visual context management is a process that derives a set of visual transformations, which updates an existing scene to incorporate new information. To obtain R2 in <ref type="figure">Figure  1</ref>, RIA updates R1 by adding details to the cheapest house and simplifying other retrieved houses.</p><p>Since it is very difficult to predict how a user-RIA conversation would unfold, it is impractical to plan all possible visual context transformations a priori. To incrementally present diverse, unanticipated information introduced during a conversation, we model visual context management as an optimization problem. The objective is to find a set of visual transformations that maximizes the satisfaction of all visual context management constraints (e.g., ensuring semantic continuity and minimizing visual clutter). To the best of our knowledge, our work is the first to address visual context management using an optimization-based approach. As a result, it offers two unique contributions:</p><p>1. It can derive a set of near optimal visual transformations by simultaneously balancing a broad set of constraints for diverse interaction situations. 2. It is easily extensible, since it uses feature-based metrics to uniformly model all visual context management constraints. In the rest of the paper, we first discuss the related work. We then describe our optimization-based approach to visual context management, highlighting the aspects mentioned above. Finally, we present our evaluation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Our work is built upon the studies of understanding and analyzing the characteristics and cognitive functions of visual context (e.g., <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14]</ref>). For example, studies show that a semantically coherent visual context can facilitate the detection and identification of objects in complex scenes <ref type="bibr" target="#b2">[3]</ref>. Moreover, a coherent visual context provides strong cues to guide effective distribution of user's visual attention, which is critical in processing visual outputs <ref type="bibr" target="#b13">[14]</ref>. Moreover, the concept of visual momentum is introduced to measure how visual context transitions impact the user's ability in extracting and integrating information across multiple displays <ref type="bibr" target="#b17">[19]</ref>. While these works provide us with a theoretical foundation to draw upon, we offer a computational model that realizes these theories.</p><p>Researchers in the area of human-computer interaction and information visualization have exploited visual context in creating better visual interfaces. To provide users with desired visual support for their working memory, focus+context interfaces present important information within context <ref type="bibr" target="#b3">[4]</ref>. However, existing approaches focus on manipulating the views of a fixed data set (for example, a hyperbolic browser <ref type="bibr" target="#b12">[13]</ref> and a radial space-filling display <ref type="bibr" target="#b16">[18]</ref> load the entire data set at once). In contrast, we must dynamically update visual context to incorporate new, unanticipated information introduced during a user-RIA conversation.</p><p>To handle more dynamic situations where a visual context may be continuously updated through a head-tracked display, Bell et al. have used a greedy algorithm to maintain a set of visual constraints, such as preventing object occlusion and ensuring visual continuity <ref type="bibr" target="#b1">[2]</ref>. Compared to their approach, our work considers a more comprehensive set of constraints (including their visual constraints) to ensure both semantic and visual coherence of a display. Moreover, our optimization-based approach can simultaneously balance all constraints, including conflicting ones (e.g., maintaining semantic continuity vs. minimizing visual clutter).</p><p>Another piece of related work is using an AI planner to automatically derive a set of visual transformations for creating a coherent visual presentation <ref type="bibr" target="#b18">[20]</ref>. However such planning-based approaches are not scalable, since they are designed to handle small data sets (e.g., one patient at a time) in predictable situations. In contrast, our optimization-based method is designed to handle large data sets in a highly interactive environment. <ref type="figure" target="#fig_0">Figure 2</ref> provides an overview of RIA's visual designer, which automatically creates a visualization for a given user query in three steps. First, the sketch generator creates a visual sketch, describing the visual encodings of the data to be visualized <ref type="bibr" target="#b20">[22]</ref>. Second, the visual layout manager determines the geometry (e.g., size and location) of the visual encodings. Specifically, it dynamically derives a set of spatial layout constraints, such as avoiding object occlusion and ensuring visual balance. Third, the context manager updates an existing visual context to incorporate the new sketch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">VISUAL OUTPUT GENERATION</head><p>Although we have implemented the three-step pipeline, these three steps may be inter-twined. For example, the layout manager may need to relocate the objects after the context manager decide what to keep/delete. To avoid going back and forth between the two steps, our layout manager now computes a range of candidate geometric parameters for each visual object in the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXAMPLES</head><p>We use a set of examples to illustrate how visual context management is subject to a number of factors, such as user intention and various visualization constraints. First, a user's data navigation intention, which is often implied by a user query expression, impacts visual context management. For example, query U2 in <ref type="figure">Figure</ref> 1 may imply that the user is browsing the data, while U2' may suggest that the user is filtering the data 2 . Accordingly, for U2 RIA preserves most of the visual context to facilitate further data 2 Currently RIA infers the user navigation intention using keywords such as "just" or "which one". </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLS-1234567</head><p>Price: $485000 Bedrm: 3 . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cortland</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLS-7845167</head><p>Price: $550000 Bedrm: 4 . . . . . . . . .  browsing (R2), while zooming in on the cheapest house for U2' to satisfy data filtering (R2'). Moreover, conversation flow affects visual context management. By default, RIA assumes that a user conducts a continuous conversation and interprets a user query in the context of previous queries. Suppose that U3 in <ref type="figure" target="#fig_2">Figure 3</ref> is issued after U1 in <ref type="figure">Figure 1</ref>. As a continuation, RIA presents the requested city data while preserving the relevant houses. During a conversation, however a user may want to start a new context <ref type="bibr" target="#b2">3</ref> . Assume that the user issues query U4 <ref type="figure">(Figure 4</ref>) after U1 to switch to a new context. In this case, RIA creates a visual output that only shows the requested school data in the new context. In addition to user intentions, various visualization constraints, such as maintaining continuity across displays <ref type="bibr" target="#b17">[19]</ref> and minimizing visual clutter <ref type="bibr" target="#b11">[12]</ref>, influence visual context management. To maintain semantic continuity, for example, in <ref type="figure" target="#fig_2">Figure 3</ref> RIA incorporates the requested city data in the context of relevant houses. Moreover, to maintain visual continuity, in <ref type="figure">Figure 5</ref>(b) RIA displays the requested restaurants along with the hotels retrieved earlier, although the restaurants and the hotels are only remotely related according to our data ontology.</p><p>Over the course of a user-RIA conversation, visual objects may be accumulated in a visual context. Since complex visual displays may overload a user's working memory and impair information comprehension <ref type="bibr" target="#b11">[12]</ref>, RIA tries to minimize visual clutter while maintaining continuity. To reduce clutter, RIA simplifies the display of less important information like the houses in <ref type="figure" target="#fig_2">Figure 3</ref> and the hotels in <ref type="figure">Figure 5</ref>(b), or simply removes irrelevant data.</p><p>To help users to integrate information across multiple scenes, RIA must also maintain important perceptual landmarks <ref type="bibr" target="#b17">[19]</ref> and provide smooth transitions <ref type="bibr" target="#b13">[14]</ref>. In <ref type="figure">Figure 1</ref>, RIA preserves various geographical landmarks, such as the Hudson river and major highways, which help to anchor a visual transition. Moreover, RIA ensures smooth visual transitions to allow users to keep track of changes. For example, RIA animates camera changes and visual object updates (see accompanying video).</p><p>In summary, RIA must consider a wide variety of visual context management constraints, including accommodating user intentions, ensuring visual continuity, and minimizing visual clutter. These constraints often exhibit inter-dependencies and may even conflict with one another. For example, preserving semantic continuity may violate a visual clutter reduction constraint. It thus would be very difficult to maintain a coherent visual context using simple heuristics, which may not be able to balance all constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">OPTIMIZATION-BASED VISUAL CONTEXT MANAGEMENT</head><p>To balance all relevant constraints simultaneously, we develop an optimization-based approach to visual context management. We explain our approach in three steps. First, we present our featurebased representation that characterizes a visual context and visual operators. Here a visual operator defines a visual transformation that updates the properties of one or more visual objects. For example, a Highlight operator updates the color of a selected visual object. Second, we use feature-based metrics to uniformly model various visual context management constraints. In particular, each metric assesses the desirability of applying one or more visual operators to update a visual context. Third, we present a simulatedannealing algorithm that dynamically derives a set of visual operators by maximizing the satisfaction of all relevant constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Feature-based Representation</head><p>Since a visual context is continuously updated during a user-RIA conversation, we describe the state of the context at the beginning or end of each user turn. Formally, we use the following notations. Given user turn t+1, S t and S t+1 denote the visual context at the beginning and end of the turn, respectively. We use a set of features to describe semantic and syntactic properties of S t and S t+1 . Similarly, we use a set of features to characterize each visual operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Bi-Level Visual Context Representation</head><p>Visual context S t consists of a set of visual objects. For example, at the beginning of user turn U2 <ref type="figure">(Figure 1</ref>), the visual context contains objects such as retrieved houses and cities (R1). To characterize the overall scene at S t and each visual object involved, we develop a bi-level descriptor, which describes the overall properties of a scene and the details of each visual object.</p><p>Specifically we describe a scene using aggregated features, such as the total number of visual objects (volume) and the colors used (colorVariety) in the scene. As our goal is to incorporate a new scene into an existing visual context, we use the same set of features to describe the new scene (e.g., the details of the cheapest house in U2 in <ref type="figure">Figure 1</ref>). To facilitate the integration, we add a foci feature. Feature foci describes the current visual focus. For U8 in <ref type="figure" target="#fig_5">Figure 6</ref>, foci is the requested train stations. Given a user query, RIA's content selector dynamically decides visual foci <ref type="bibr" target="#b19">[21]</ref>. Now foci is used to set camera parameters and compute the semantic relevance between a new scene and an existing visual context.</p><p>Besides describing an overall scene, we characterize the properties of each visual object. A visual object is an encoding of a data object and has basic visual properties, such as color, size, and location <ref type="bibr" target="#b3">[4]</ref>. To facilitate visual context management, here we focus on the data and visual semantic features. We use data features like category to describe the semantic category of the encoded data, and visual features such as prominence to specify how a visual encoding may be perceived. <ref type="table" target="#tab_1">Table 1</ref> lists all semantic features that RIA uses. Here we focus on explaining three complex features: data importance (dImportance), visual proximity (vProximity), and visual prominence (prominence).  Data importance. Feature data importance indicates how important a data object is to a given user turn. All data objects start with the same importance value (0.0). RIA dynamically updates data importance in two ways. First, RIA uses its content selector to decide data importance <ref type="bibr" target="#b19">[21]</ref>. If the content selector chooses a data object to present at a given user turn, it updates the importance of this object accordingly. For query U8 in <ref type="figure" target="#fig_5">Figure 6</ref>, the content selector assigns the requested train station data with the highest importance and the house constraining the train stations with a lower importance. If a data object is not selected, its importance is reduced using a time decay function. For example, to respond to U2 in <ref type="figure">Figure 1</ref>, RIA does not select the houses shown earlier except the cheapest one. Accordingly, their importance values are reduced at turn U2. Whenever a user starts a new context, RIA resets the importance to 0.0 for non-selected data. Second, RIA uses data relationships to decide data importance. This is very useful when the content selector assigns the same importance to multiple data objects. In R1 <ref type="figure">(Figure 1</ref>), the content selector assigns the same importance to all retrieved houses. Since these houses are not mentioned in U3 <ref type="figure" target="#fig_2">(Figure 3</ref>), their importance is reduced. However houses located in Hartsdale are more semantically relevant to U3 than others are. As a result, RIA better preserves these houses to provide the user a useful context to comprehend the retrieved city data <ref type="figure" target="#fig_2">(Figure 3</ref>). Thus, we define a semantic relevance metric: In summary, we define overall data importance for data object d at a given user turn t+1 as follows: <ref type="bibr" target="#b0">(1)</ref> .</p><formula xml:id="formula_0">R s (d, D') = Max [R s (d, d' j ), ∀ j],</formula><p>Here val is the importance computed by the content selector and α is the decay factor. Current we use α = 1.5 for a rapid decay.</p><p>Visual proximity. While data importance assesses how a visual object is related to a user query semantically, visual proximity measures how a visual object is related to the current visual foci spatially. When presenting user-requested information, it is desirable to show items that are located nearby, since such items may help to establish a useful context for users to comprehend the intended information <ref type="bibr" target="#b0">[1]</ref>. For example, in <ref type="figure">Figure 5</ref>(b) RIA shows the requested restaurants and the nearby hotels to give users an overall sense of where everything is. By this notion, feature visual proximity computes the spatial distance of visual object v to visual foci V':</p><formula xml:id="formula_1">(2) I v (v) = 1 -Min[dist((v, v' j )</formula><p>, ∀j], where v' j ∈V', dist() computes the Euclidean distance of two visual objects in a normalized screen coordinate. Visual prominence. Visual prominence measures how easily a visual object can be perceived by a user. It is now modeled using three basic visual variables: color, size, and location. Given a visual object v, we define its color prominence P 1 (v), size prominence P 2 (v), and location prominence P 3 (v).</p><p>Color prominence states that the more contrast a visual object produces against its background, the more prominent it can be perceived. For example, a red object is more prominent than a yellow object against a white background. Color contrast can be computed based on color theory <ref type="bibr" target="#b7">[8]</ref>. We use function contrast() to compute the contrast value of object v against its background:</p><formula xml:id="formula_2">P 1 (v) = contrast(v).</formula><p>Size prominence asserts that the bigger a visual object is, the more prominent it appears:</p><formula xml:id="formula_3">P 2 (v) = v.bbx.width × v.bbx.height,</formula><p>where the bounding box is computed in a normalized screen coordinate.</p><p>Location prominence states that objects placed near the center of a display are more prominent than those located elsewhere: <ref type="figure">, c)</ref>, where c denotes the center of a display, function dist() computes the normalized screen Euclidian distance between the center of v and the center of the display.</p><formula xml:id="formula_4">P 3 (v) = 1 -dist(v</formula><p>Combining three formulas above, we model the overall visual prominence of visual object v:  </p><formula xml:id="formula_5">I d d t , ( ) α - ( ) exp × R s d D' , ( ) , [ ] ⎩ ⎪ ⎨ ⎪ ⎧ = (3)</formula><p>, where i=1..3, and weight u i =0.33.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Visual Operator Representation</head><p>We use visual operators to model visual transformations that update a visual context and incorporate new information <ref type="bibr" target="#b18">[20]</ref>. Visual operators can be categorized based on their effects. So far we have identified four groups of operators: camera operators that modify the parameters of a camera, appearance operators that update visual appearance (e.g., Highlight), geometry operators that change geometric properties (e.g., Move and Scale), and structural operators that modify a scene structure (e.g., Add). <ref type="table" target="#tab_2">Table 2</ref> lists all operators that RIA uses. Depending on the actual implementation, an operator may exhibit different visual effects. For example, we may implement Delete by making objects transparent gradually or simply hiding them. Moreover, we can easily define new visual operators based on application requirements.</p><p>To represent all visual operators uniformly, we associate each operator with seven features. Feature operand denotes visual objects that an operator manipulates, and feature parameter holds the specific information that is required to perform the intended transformation. As shown below, operator Scale has one parameter scaleFactor. Feature effect is a function measuring what properties of the operands will be modified after the operation. For example, Scale changes the size of an object. On the other hand, feature cost estimates the cost of performing the intended visual transformation. Currently, it measures the perceptual cost needed for a user to perceive the transformation. For example, it is more costly for a user to perceive object movements than highlighting effects <ref type="bibr" target="#b13">[14]</ref>. Finally, features temporal-priority, startTime and endTime control the timing of applying an operator. The fragment below outlines the definition of operator Scale: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Feature-based Desirability Metrics</head><p>As described in Section 3, a number of constraints influence visual context management, including user intentions and visualization constraints. To uniformly model all constraints, we define a set of metrics based on our representation of a visual context and visual operators. These metrics assess the desirability of applying one or more visual operators to an existing visual context to incorporate new information. By their purpose, we divide metrics into two groups: visual momentum metrics and visual structuring metrics.</p><p>Visual momentum metrics assess the coherence of a visual context across displays <ref type="bibr" target="#b17">[19]</ref>. Visual structuring metrics evaluate the structural coherence of a visual context after the new information is integrated <ref type="bibr" target="#b11">[12]</ref>. Our purpose here is not to enumerate a complete set of visual context management constraints, instead we show how to formulate key constraints quantitatively. For simplicity, all feature/metric values are normalized to lie between [0, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Visual Momentum Metrics</head><p>Visual momentum measures a user's ability to extract and integrate information across multiple displays. Since the amount of visual momentum is proportional to a user's ability to comprehend information across displays <ref type="bibr" target="#b17">[19]</ref>, RIA tries to maximize the visual momentum when updating a visual context. Specifically, we adopt three key techniques from <ref type="bibr" target="#b17">[19]</ref> that are most applicable to our visual context management task: 1). maximizing both semantic and visual overlaps of consecutive displays, 2). preserving perceptual landmarks, and 3). ensuring smooth visual transitions.</p><p>Maximizing display overlap. Proper display overlap helps users to piece together information across successive displays <ref type="bibr" target="#b17">[19]</ref>. Using two key display overlap techniques suggested by Woods <ref type="bibr" target="#b17">[19]</ref>, we define two metrics: visual overlap and semantic overlap metrics. A visual overlap metric computes the invariance between two displays, specifically the average invariance of each visual object in S t and its new state in S t+1 : .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Here visual object</head><formula xml:id="formula_6">v i, t ∈S t , v i, t+1 ∈S t+1, and v i, t+1 = op i (v i, t )</formula><p>, op i is a visual operator; N is the total number of visual objects in S t ; inv() computes the invariance between two visual objects. If v i, t+1 is invisible, inv() =0.0; otherwise it is the average invariance of locations, sizes, and colors:</p><formula xml:id="formula_7">inv(v i, t , v i, t+1 ) = Avg[inv_loc(v i, t , v i, t+1 ), inv_size(v i, t , v i, t+1 ), inv_color(v i, t , v i, t+1 )].</formula><p>Similarly, we define a semantic overlap metric that assesses whether semantically related items remain together across displays. It computes the semantic relevance of S t and S t+1 : , where data objects d i and d j are encoded by v i, t and v j, t+1 , respectively, R s () computes their semantic relevance using Formula (a) on page 4.</p><p>Using the visual and semantic overlap metrics defined above, we model an overall display overlap metric regulated by the user navigation intention, which allows more display overlap for data browsing but less overlap for data filtering <ref type="bibr" target="#b8">[9]</ref>:</p><formula xml:id="formula_8">(4) O(S t , S t+1 )=ε[w 1 ×O v + w 2 ×O s ]. where weights w 1 =w 2 =0.5,</formula><p>and ε is a constant, ε = 1.0 for data browsing, otherwise ε = 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preserving perceptual landmarks.</head><p>Perceptual landmarks are distinguishable features that anchor a visual context transition, which in turn helps users to relate information in successive scenes <ref type="bibr" target="#b17">[19]</ref>. For example, the Westchester county map serves as a common background for all displays <ref type="figure">(Figures 1-4</ref>) and key geographical landmarks such as major rivers and highways persist in scenes whenever possible (e.g., Hudson river in <ref type="figure">Figure 1</ref>). To preserve the maximal number of perceptual landmarks in a visual context, we count the normalized number of landmarks in the context:</p><formula xml:id="formula_9">(5) L(S t+1 ) = L t+1 /N, where L t+1</formula><p>is the number of landmarks existing in visual context S t+1 , and N is the total number of landmarks existing in an entire application. </p><formula xml:id="formula_10">P v ( ) u i P i × v ( ) i ∑ =</formula><formula xml:id="formula_11">O v S t S t 1 + , ( ) 1 N ----inv v i t , v i t 1 + , , ( ) i ∑ = O s S t S t 1 + , ( ) 1 N 2 ------ R s d i d j , ( ) ( ) j ∑ i ∑ =</formula><p>Ensuring smooth transition. Sudden changes in a scene prevents users from visually tracking the changes. As a result, the causal connection between an existing scene and a new scene may be lost <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b5">6]</ref>. To ensure smooth transitions between successive displays, animation is often used to provide users with a powerful cue to interpret the changes <ref type="bibr" target="#b5">[6]</ref>. We define a metric to compute the average smoothness of applying a set of visual operators: </p><formula xml:id="formula_13">smoothness(op i ) = 1 -cost(op i ).</formula><p>The above metric states that the less mental cost that an operator incurs, the smoother the transition that the user perceives.</p><p>Combining formulas 4-6, we define an overall visual momentum metric to ensure the maximal across-display continuity:</p><formula xml:id="formula_14">(7) φ(Op, S t , S') = Avg[O, L, T]</formula><p>, where visual operators Op transform visual context S t to incorporate the new scene S'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Visual Structuring Metrics</head><p>In addition to maximizing visual momentum during a visual context transition, we ensure that the structure of the context be coherent after the transition. Since our sketch generation takes care of the structuring issues regarding visual encoding <ref type="figure" target="#fig_0">(Figure 2</ref>), here we focus on the structuring issues concerning visual integration from two aspects. One is to ensure a proper visual ordering so that users can easily attend to the new content <ref type="bibr" target="#b11">[12]</ref>. The other is to minimize the visual clutter after the integration of new information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Establishing a proper visual ordering.</head><p>To establish a proper visual ordering, we constrain that data items important to the current user query be expressed prominently. For example, in <ref type="figure" target="#fig_5">Figure  6</ref>(b) RIA highlights the newly requested train station information, while simplifying the house representations based on their relevance to the train stations. Here the house constraining the train stations is slightly simplified, while the others are reduced to their minimum <ref type="figure" target="#fig_5">(Figure 6a-b)</ref>. To capture such desired correlation between data importance and visual prominence, we define a visual ordering metric: <ref type="bibr" target="#b7">(8)</ref> .</p><p>Here d i and d j are data objects, and v i and v j are their corresponding encodings at turn t+1. Function I() is the overall importance of a data objects d i and its visual encoding v i using Formulas 1-2:</p><formula xml:id="formula_15">I(d i , v i ) = µ 1 ×I d (d i ) + µ 2 ×I v (d i ), where weights µ 1 =0.7, µ 2 =0.3</formula><p>to favor the semantic importance. Moreover, P t+1 () computes the visual prominence by Formula 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Minimizing Visual Clutter.</head><p>A visually cluttered presentation may create confusion and make the scene impossible to scan <ref type="bibr" target="#b11">[12]</ref>. To provide an informative but uncluttered visual context, we measure the overall complexity of a display. Oliva et al. have discussed two key sets of factors that affect visual complexity <ref type="bibr" target="#b14">[15]</ref>. One set of factors includes the quantity of objects and the variations of their properties, such as the number of different colors and shapes appearing in a scene. Another set of the factors is concerned with the spatial layout of a scene, such as symmetry and openness. Since our layout manager maintains spatial layout constraints including symmetry <ref type="figure" target="#fig_0">(Figure 2</ref>), here we measure visual complexity using the first set of factors:</p><formula xml:id="formula_16">χ(S t+1 ) = λ 1 × colorVariety(S t+1 )/N c + λ 2 × areaUsage(S t+1 ) + λ 3 × shapeComplexity(S t+1 ).</formula><p>Here weights λ 1 =λ 2 =λ 3 =0.33, N c is the total number of colors allowed in one display (now N c = 7), and colorVariety() obtains the total number of colors used in S t+1 .</p><p>Metric areaUsage() computes the normalized screen space occupied by S t+1 :</p><p>, where visual object v i ∈S t+1 , boundingArea() returns the screen space occupied by v i in a normalized screen coordinate.</p><p>Metric shapeComplexity() computes the total number of distinct shapes in S t+1 and the average complexity of all shapes <ref type="bibr" target="#b3">4</ref> :</p><formula xml:id="formula_17">shapeComplexity(S t+1 ) = shapeVariety(S t+1 )/N × Avg[shapeComplexity(v i )].</formula><p>Here N is the total of visual objects in S t+1 , shapeVariety() and shapeComplexity() are two features defined in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>To minimize the visual complexity of a scene, we maximize:</p><formula xml:id="formula_18">(9) ψ(S t+1 ) = γ [1 -χ(S t+1 )]</formula><p>, where γ is a constant, γ = 1.0 for data browsing to allow more visual objects; otherwise γ = 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Simulated-Annealing Algorithm</head><p>Combining Formulas 7-9, we define an overall objective function: <ref type="bibr" target="#b9">(10)</ref> . Here Op is a set of visual operators for transforming visual context S t to incorporate new scene S', and weights w 1 =w 2 =w 3 =0.33.</p><p>Our goal now is to find a set of visual operators that maximizes the objective function. This task is to solve a typical quadratic assignment problem, which is NP-hard <ref type="bibr" target="#b15">[17]</ref>. Since a simple greedy algorithm may suffer from being trapped at local maxima, we adopt simulated annealing, which has proven to be effective for solving this class of problems <ref type="bibr" target="#b10">[11]</ref>. <ref type="figure" target="#fig_8">Figure 7</ref> outlines our algorithm. The input to our algorithm is visual context S at the beginning of user turn t+1, and a new scene S' to be integrated. The algorithm uses a "temperature" parameter T to populate the desired result list iteratively (line 2-15). In our experiments, T is initialized to be T0=2.0, the minimal temperature Tmin=0.05 and reduction rate ∆t=0.1, which together control the number of iterations.</p><p>During each iteration, the algorithm samples a set of operators (now MAX_SAMPLE_COUNT=40) (lines 4-13). In each sampling,</p><formula xml:id="formula_19">ζ S t 1 + ( ) 1 I d i v i , ( ) P v j ( ) × I d j v j , ( ) P v i ( ) × - - j ∑ i ∑ = 4</formula><p>Different shapes are assigned different complexity values. For example a text is considered more complex than a simple geometric shape like circle.</p><formula xml:id="formula_20">areaUsage S t 1 + ( ) boundingArea v i ( ) i ∑ = reward Op S , t S' , ( ) w 1 φ × w 2 ζ × w 3 + ψ × + =</formula><p>List simulatedAnnealing(Scene S, Scene S').  routine find_operator() uses a greedy strategy to find a top candidate (line 5). Specifically, it computes a reward() for applying already selected operators and an operator op to a visual object that has not been updated by the same operator (Formula 10). It then ranks all candidates by their reward values and returns the top one. Using the top candidate and the existing result set, the algorithm tests whether the reward be greater than that of using the existing result set alone (lines 7-8). If it is better, the candidate is then added to the result set (line 9). Otherwise, it tests whether the current control probability is greater than a random number generated by rand() between [0, 1] (line 10). If it is true, the candidate is then added (line 11).</p><p>In each iteration, parameter T controls the probability of accepting sub-optimal operators. It is then gradually reduced so that the algorithm is less likely to accept sub-optimal operators (line 14). When the algorithm eventually converges (i.e., T reaches a target minimum temperature), it returns a set of visual operators that maximizes our objective function in Formula 10 (line 16). The complexity of find_operator() is O(n 2 ×m 2 ), where n is the total number of visual objects in S t and S', and m is the number of available operators. Since the number of steps in temperature decrease and the total number of samples evaluated at each temperature are constants, the total complexity of our algorithm is O(n 2 ×m 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Grouping and Ordering Visual Operators</head><p>After finding a set of desired visual operators, RIA groups the operators by their type and by their operands. For example, RIA groups together all Highlight operators that have the same type of operands. RIA then determines the order of applying these operators. Currently, operators within a group are applied at the same time. Such application guides users to recognize perceptual groupings during visual transition <ref type="bibr" target="#b13">[14]</ref>. For example, highlighting a group of houses simultaneously allows users to perceive them as a group. Moreover, operators in different groups are ordered by their temporal priority. For example, Delete normally occurs before Add to prevent the obsolete data from clobbering the new data. Now the temporal priority for each type of operator is pre-defined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EVALUATION</head><p>We have implemented RIA using Java and C++, with the graphics renderer running on Linux and all other components on Windows. RIA has been applied to two applications running on realistic, sizable data sets. For example, our real-estate data is subscribed from a multiple listing service, containing 2000+ houses and each with 128 attributes (e.g., price and style). In our experiments, we assign equal weights for metrics initially and tune them based on application needs (e.g. increasing the weight for visual momentum to better help a user in visual transitions).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experiments</head><p>We have tested RIA extensively as a whole by letting users run a wide variety of queries in a number of experiments. From these tests, we have collected a set of common queries for both applications. Using these queries, we designed a set of experiments to evaluate our work on visual context management. Our experiments consisted of two parts.</p><p>First, we randomly chose 50 query sequences from 26 users with each sequence containing about 10 queries on average. We ran RIA on these queries using three different visual context management methods: a). without visual context management, b). a greedy approach, and c). simulated annealing described in this paper. Similar to the algorithm used in <ref type="bibr" target="#b1">[2]</ref>, our greedy approach uses two features, data importance (Sec 5.1.1) and data relevance (Sec 5.2.1), to guide operator selection. For each query, we recorded the results produced by each of the three methods: the operator selected and the order of applying these operators.</p><p>Second, we recruited two graphic designers, whom we asked to perform visual context management for 10 query sub-sequences randomly selected from the 50 sequences used above. To minimize accumulation errors, we used shorter query sequences, each of which contains 2-4 queries. For each query, we provided the designers with the data content selected by RIA, the existing visual context, and a set of available visual operators <ref type="bibr" target="#b4">5</ref> . We then asked them to mark the visual operators that they would use and the order of applying these operators. The rationale of involving designers in our evaluation is to test whether our approach performs as adequately as professional designers would do.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Result Analysis</head><p>We compared the results produced by RIA with those made by the human designers. Specifically, we count the total number of queries where RIA and each designer produced the same results: the operator set 6 and the operator ordering. <ref type="figure" target="#fig_9">Figure 8</ref> shows the comparison results. When compared to designer 1, for example, the simulated annealing produced the same results for 92% of the queries. Unsurprisingly, our optimization-based approach did the best, the greedy approach came in second, and the results from no visual context management (No VCM) were the last. Note that even without visual context management, RIA produced results similar to those of designers for over 50% of the queries. This is because the same Add and Camera operators were always used for opening queries in a sequence or for context-switch queries (e.g., <ref type="figure">Figure 4</ref>).</p><p>We also carefully examined the differences, which mainly fell in two categories. One is that RIA chose to zoom in on the new content (R2' in <ref type="figure">Figure 1</ref>), while the designers added the new content without change the field of view. The designers did explain their rationale-they assumed that the user was browsing the data. For validation purpose, we adjusted the user navigation intention in Formula 9, RIA did produce the same results in these cases. The other case is that the designers did not seem to order the operators consistently. For most of the cases, the designers chose to apply operator Simplify before Add. In several cases, however the order was reversed without obvious reasons.</p><p>Due to the time and effort required, it is impractical to ask a human designer to perform visual context management for a large number of queries. For validation purpose, we compared the results produced by the greedy approach and by the simulated annealing for all 50 query sequences (a total of 500 queries). These two approaches produced different results in 25% of the queries. This is consistent with their results for the 10 query sequences, where there are discrepancies in 20% of the queries. Most of these discrepancies rise from the fact that the greedy approach favors the semantic continuity, while the simulated annealing tries to balance both semantic and visual continuity. As a result, the greedy approach tended to delete more content from the existing context. <ref type="bibr" target="#b4">5</ref> We also let the designers add other visual operators if needed. 6 Two operators are considered the same if the parameters of the computer-derived operator match the qualitative description of the designerderived operator. During our study, we also observed that our designers spent a considerable amount of time to finish the task for all 10 query sequences (over an hour). Especially in our experiments, each query sequence is relatively short so that the designer would not need to integrate information across many displays. When asked, they attributed their time cost to weighing all the possible ways of managing the visual context under various situations (e.g., user intention and visual continuity). Since our method automatically balances various constraints, it is very valuable especially for handling continuous and often unanticipated user interaction patterns.</p><p>Although the study demonstrated the promise of our work, it also exposed some of its limitations. First, we would need a finergrained model to determine the temporal order of the operators. For example, RIA now always applies the same type of operators simultaneously. As our designers have shown, the same type of visual operators (e.g., Add) may be applied at different times to ensure better visual effects. To create a display like <ref type="figure">Figure 4</ref>, our designer would add the school district boundary first, then the school icons and labels, and last the details shown in the green rectangle. Moreover, our current work takes little advantage of the overall user interaction pattern, although it exploits individual query expressions (e.g., U2 vs. U2' in <ref type="figure">Figure 1</ref>). In fact, both designers commented that they would make better decisions if they knew the user interaction pattern. For example, in one tested sequence, the same query was repeated a few times. Once our designer detected this pattern, he kept as much relevant information as possible on the screen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">SET UP AND APPLICATION</head><p>It takes three steps to set up to use our visual context management method. First, we define the static features such as assigning data semantic categories <ref type="table" target="#tab_1">(Table 1)</ref>. Building a simple data ontology helps to define these features. Second, we build a catalog of visual operators <ref type="table" target="#tab_2">(Table 2)</ref>. Third, we formulate feature-based metrics to model various constraints important to an application (Sec 5.2). For example, in a mobile application, we may model devicedependent visual context management constraints <ref type="bibr" target="#b9">[10]</ref>.</p><p>To bootstrap the process and avoid tuning fallacy, we recommend starting with simple settings. So far we have used a simple data ontology, a set of basic visual operators, and equally weighted metrics (Formula 10) to adequately handle diverse interaction situations in two different applications. When necessary, it is easy to extend what we have. First, we can introduce new visual operators easily (e.g., adding a fisheye view operator for visual morphing). Moreover, we can easily incorporate new features/metrics in our objective function for new factors of context management(e.g., a new metric for modeling user cognitive states).</p><p>In addition to supporting multimodal conversation systems like RIA, our approach to visual context management is applicable to the broader problem of creating better visualization. For example, it can be used in a GUI-driven interactive visualization system, where a more coherent visualization can be produced to integrate information obtained across multiple turns of user interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">CONCLUSIONS</head><p>When creating a visualization in an interactive environment, we must dynamically decide how to incrementally integrate new information into existing displays to ensure the coherence of the overall context. Here we present an optimization-based approach to visual context management. Given an existing visual context and the new information to be presented, our goal is to find a set of visual operators that can best update the existing visual context and incorporate the new information. To achieve this goal, we formulate a set of metrics to model various context management constraints, such as preserving visual ordering and maintaining visual momentum. Using these metrics, we define an objective function to assess the overall desirability of applying a set of visual operators. Finally, we use a simulated-annealing algorithm to maximize the objective function and find the desired operators.</p><p>Unlike existing approaches, which often consider a subset of our constraints in a more deterministic context, our optimizationbased approach dynamically balances a variety of constraints for diverse interaction situations. It is also easily extensible, since we can easily incorporate new features/constraints. We have applied our work to two different applications, and our study shows that RIA performs adequately against human designers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Visual output generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Tell me about Hartsdale.RIA: Hartsdale is located in central Westchester county, 22 milesfrom Manhattan.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>A follow-up query to U1 to show semantic continuity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3</head><label></label><figDesc>Currently in RIA, a user can click on the "new context" button to indicate the starting of new context.U4:Tell me about Scarsdale schools.RIA:Here is the information about Scarsdale school district.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>A subsequent query issued in a new context. U5 Show hotels near IBM Hawthorne R5 I found 4 hotels near IBM Hawthorne. &lt;Display (a)&gt; U6 What about Chinese restuarants? R6 There are two chinese restuarants near IBM. &lt;Display (b)&gt; Maintaining visual continuity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>U7:Figure 6 .</head><label>6</label><figDesc>Show colonials under $300k in Yonkers. RIA: There are 4 houses meeting your criteria. &lt;Display (a)&gt; U8: Show train stations within 1 mile of this one &lt;click on the house with mls 2421595&gt; RIA: I found two train stations. &lt;Display (b)&gt; Maintaining a proper visual ordering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Scale</head><label></label><figDesc>extends Operator { List operand Vector3f scaleFactor // parameter stating how much to scale float effect() // this function modifies the size of operands float cost = medium float temporal-priority = medium float startTime, endTime }</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>16</head><label></label><figDesc>List result ← empty float T ← T0 while (T &gt; Tmin) do for each sample count ∈ [1, MAX_SAMPLE_COUNT] Operator op ← find_operator(S, S', result) if (op == null) then return result endif List currOpList ← result + op float diff ← reward(currOpList) -reward(result) if diff &gt; 0 then add op to result else if probability exp(-diff/T) &gt; rand(0, 1) then add op to result endif endif endfor T ← T -∆t endwhile return result</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>The outline of our simulated annealing algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Experiment result summary for 10 query sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>where d is a data object, and D' is the current query foci, d' j ∈D', and R s (d, d' j ) computes the relevance between two data objects: (a) . Here β is a function indicating the relevance value of the relation r between d and d' j in a database. RIA uses a data ontology to look up the data relation r between d and d' j . It then uses the database to verify the relation. Let d be a house and d' j be a city in a database. By our ontology, a house is located-in a city. Using this relation, RIA verifies whether house d is in fact located in city d' j in the database. If the relation holds, then β=1, otherwise β=0. Now the value of r() for each type of data relation is defined in our data ontology.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Features for a visual context (*dynamically computed).</figDesc><table><row><cell>Feature</cell><cell>Definition</cell></row><row><cell cols="2">Visual object-level features</cell></row><row><cell>category</cell><cell>semantic category of data defined in a data ontology</cell></row><row><cell>landmark</cell><cell>whether an object is a landmark specified by ontology</cell></row><row><cell cols="2">shapeComplexity how complex a shape it is-a pre-assigned score</cell></row><row><cell>dImportance*</cell><cell>how semantically important it is to the current context</cell></row><row><cell>vProximity*</cell><cell>how visually close it is to the current visual foci</cell></row><row><cell>prominence*</cell><cell>how visually prominent it appears</cell></row><row><cell cols="2">Visual scene-level features</cell></row><row><cell>volume*</cell><cell>number of visual objects in a scene</cell></row><row><cell>colorVariety*</cell><cell>number of different colors in a scene</cell></row><row><cell>shapeVariety*</cell><cell>number of different geometric shapes in a scene</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>A catalog of visual operators in RIA</figDesc><table><row><cell>Operator</cell><cell>Definition</cell></row><row><cell cols="2">Camera Operators</cell></row><row><cell>Camera</cell><cell>Update camera parameters (e.g., zoom in)</cell></row><row><cell cols="2">Appearance Operators</cell></row><row><cell>Highlight</cell><cell>Highlight an existing visual object (e.g., change color)</cell></row><row><cell cols="2">Geometry Operators</cell></row><row><cell>Move</cell><cell>Modify the location of a visual object</cell></row><row><cell>Scale</cell><cell>Modify the size of a visual object</cell></row><row><cell cols="2">Structural Operators</cell></row><row><cell>Simplify</cell><cell>Simplify the representation of a visual object</cell></row><row><cell>Add</cell><cell>Add a visual object to a scene</cell></row><row><cell>Delete</cell><cell>Delete a visual object from a scene</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>6) T(Op) = Avg[smoothness(op i ), ∀ i], where visual operator op i ∈Op, smoothness() is defined by operator cost (Sec 5.1.2):</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Focus plus context screens: Combining display technology with visualization techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baudisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Good</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. UIST &apos;01</title>
		<meeting>UIST &apos;01</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="31" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">View management for virtual and augmented reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hoellerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. UIST &apos;01</title>
		<meeting>UIST &apos;01</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Perceiving real-world scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Biederman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">177</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="77" to="80" />
			<date type="published" when="1972-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Information Visualization: Using Vision to Think</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Card</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mackinlay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schneiderman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Context-based multimodal input understanding in conversation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Houck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICMI &apos;02</title>
		<meeting>ICMI &apos;02</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="87" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Animation: From cartoons to the user interface</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UIST 93</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="45" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Contextual cueing: Implicit learning and memory of visual context guides spatial attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="28" to="71" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Computer Vision: A Modern Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Prentice-Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Gadget/iv: A taxonomic approach to semi-automatic design of InfoVis applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fujishiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Furuhata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Info-Vis&apos;00</title>
		<meeting>IEEE Info-Vis&apos;00</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="77" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">AppLens and LaunchTile: Two designs for one-handed thumb use on small devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bederson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sangiovanni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI &apos;05</title>
		<meeting>CHI &apos;05</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="201" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Optimization by simulated annealing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gelatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vecchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">220</biblScope>
			<biblScope unit="issue">4598</biblScope>
			<biblScope unit="page" from="671" to="680" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Designing Visual Interfaces. SunSoft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mullet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sano</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">H3: Laying out large directed graphs in 3d hyperbolic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munzner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE InfoVis &apos;97</title>
		<meeting>IEEE InfoVis &apos;97</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="2" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cognitive layouts of windows and multiple screens for user interfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Weldon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Man-Machine Studies</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="229" to="248" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Identifying the perceptual dimensions of visual complexity of scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Mack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Peeper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the 26th Cognitive Science Society Annual Meeting</title>
		<meeting>the 26th Cognitive Science Society Annual Meeting</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pardalos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wolkowicz</surname></persName>
		</author>
		<title level="m">Quadratic Assignment and Related Problems</title>
		<imprint>
			<publisher>American Mathematical Society</publisher>
			<date type="published" when="1994" />
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Focus+context display and navigation techniques for enhancing radial, space-filling hierarchy visualizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE InfoVis &apos;00</title>
		<imprint>
			<biblScope unit="page" from="57" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visual momentum: A concept to improve the cognitive coupling of person and computer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Woods</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Man-Machine Studies</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="229" to="244" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visual planning: A practical approach to automated presentation design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI &apos;99</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="634" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An optimization-based approach to dynamic data content selection in intelligent multimedia interfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. UIST &apos;04</title>
		<meeting>UIST &apos;04</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="227" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automated generation of graphic sketches by examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI &apos;03</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="65" to="71" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
