<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Production Visualization for the ASCI One TeraFLOPS Machine</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">D</forename><surname>Heermann</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sandia National Laboratories</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Production Visualization for the ASCI One TeraFLOPS Machine</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Accelerated Strategic Computing Initiative</term>
					<term>Visualization</term>
					<term>Isosurfacing</term>
					<term>Systems Engineering</term>
					<term>Massive Parallel Processing</term>
				</keywords>
			</textClass>
			<abstract>
				<p>The delivery of the first one tera-operations/sec computer has significantly impacted production data visualization, affecting data transfer, post processing, and rendering. Terascale computing has motivated a need to consider the entire data visualization system; improving a single algorithm is not sufficient. This paper presents a systems approach to decrease by a factor of four the time required to prepare large data sets for visualization. For daily production use, all stages in the processing pipeline from physics simulation code to pixels on a screen, must be balanced to yield good overall performance. Performance of the initial visualization system is compared with recent improvements. &quot;Lessons learned&quot; from the coordinated deployment of improved algorithms also are discussed, including the need for 64 bit addressing and a fully parallel data visualization pipeline.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The delivery of the first one tera-ops/sec computer has significantly impacted data visualization. The one trillion operations per second machine, is the first high-speed machine from the U.S. Department of Energy's Accelerated Strategic Computing Initiative (ASCI) Program. The machine, also known as ASCI Red, is a massively-parallel distributed memory machine containing over 9000 Intel Pentium-Pro processors. The primary purpose of the ASCI Red machine is to greatly decrease the simulation time for large physics calculations The leap forward in compute technology has impacted all aspects of visualizing simulation results. The data sets produced by this machine can greatly overwhelm common networks and storage systems. Data file formats, networks, processing software, and rendering software and hardware must be improved. A Systems Engineering approach is necessary to achieve improved performance. The common approach of improving a single algorithm can actually decrease performance of the overall system.</p><p>The discussion here considers systems that are used daily by analysts. In this production environment, entire system throughput is as important as the efficiency of a single system component. Many visualization algorithms achieve improved interaction or rendering performance at the expense of preprocessing the data. The preprocessing, however, can cost more in processing time or I/O than the benefit of the improved algorithm.</p><p>Consider the analysis loop in <ref type="figure" target="#fig_0">Figure 1</ref>. An engineer or physicist usually operates on a cycle with three basic stages, analysis preparation, physics computation, and analysis of results. During analysis preparation, problem topology, boundary conditions, physics models and other input to the simulation code are determined. This input is used by the second stage, the physics simulation, which generates output for analysis. Some smaller simulations are performed interactively, but many of the simulations requiring many hours or days are performed in a batch-processing mode. From a Systems Engineering perspective, minimizing the time to complete this analysis loop is the objective. Optimizing one stage at the expense of another stage may slow the loop cycle and increase the time required for analysis, thus lowering the productivity of the analyst. For maximum productivity, the preparation, simulation and data analysis stages must function together. From this perspective, it may be beneficial to slightly increase simulation time by writing to data visualization friendly format instead of selecting a fast output format that requires extensive post processing for visualization. The acceleration of the ASCI program exacerbates the need for system balance because a simulation code leveraged by the ASCI Red machine can completely overwhelm all supporting networks and computers. To illustrate this, consider the Sandia structured-grid shock physics code, CTH.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">6L LP PX XO OD DW WL LRQ RQ</head><formula xml:id="formula_0">9 9L LV VXD XDO OL L]D ]DW WL LR RQ Q $ $QD QDO O\V \VL LV V</formula><p>For CTH, 20-50 million cell calculations are routine. Three hundred million cell calculations can be easily run and a few one billion cell runs have been completed as proof of principle calculations. A 300 million-cell calculation generates approximately 50 gigabytes per time dump. Three days of running on the ASCI Red machine can produce 100 compressed dump data sets on the order of 350 gigabytes.</p><p>Data sets of this size quickly overwhelm most visualization systems. Simply transferring 350 gigabytes on a 100 megabit/sec Ethernet requires about 10 hrs. Even if the data is on a relatively fast 50 megabytes/sec disk RAID system, it would require 2 hours to stream the data into a post processing system, assuming that sufficient processing power is available to analyze or prepare the data for visualization at that data rate. In practice, most graphics tools are scalar and designed for much smaller data sets. On ASCI data sets, common commercial tools can take tens of minutes to hours to load and produce the first image. Much of this time is a result of the tool starting from raw data and producing isosurfaces, streamlines, or other features at initial startup. For faster loading and interaction, it is desirable to preprocess raw data into data sets that can be directly rendered.</p><p>To explore the issues of very large data sets, this discussion will begin with an early production visualization system. This initial system required over 16 hours to preprocess data for visualization. The goal of work presented here was to reduce this time to 4 hours to allow single day analysis of large data sets. This system performance goal required a balanced combination of hardware and software. The following sections will present the visualization task, the initial system performance, and the solutions to improve process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Visualization Task</head><p>The visualization task considered here is isosurface visualization of shock physics data generated by Sandia's CTH code. The data is a structured grid where matter moves through a 3-D grid in time. Each cell in the grid can contain empty space or one, two or as many as 20 materials. This is recorded in each cell as a "volume fraction," where one indicates the cell is completely full of a material and zero indicates no material in the cell. For multiple materials the sum of the volume fractions plus the "void fraction" equals one. For visualization, 3-D isocontours are generated from the volume fractions for each material and the resulting polygonal surfaces are rendered.</p><p>To rapidly explore the database, the rendering and isosurface generations were performed as separate tasks. The processing consisted of the following stages:</p><p>1. Simulation run on MP machine, 2. Transfer data to visualization server, 3. Isosurface extraction <ref type="bibr" target="#b0">[1]</ref> and polygon decimation <ref type="bibr" target="#b1">[2]</ref>, 4. Concatenation of polygon files, 5. Real-time exploration of isosurfaces.</p><p>The simulation run on the MP machine produces one file for each processing node. Thus, a simulation on 2500 nodes produces 2500 files, each containing that node's portion of the overall problem. The 2500 files are transferred across a network to a large visualization machine for the remainder of the steps. On the visualization server, each file is processed independently to extract the isosurfaces contained in each file. The list of polygons is decimated in memory prior to writing the polygons to disk. The resulting 2500 polygon files for each time dump are concatenated into a single file for easy loading by the rendering software. This isosurface/decimation/concatenation process is repeated for each of 10-100 time dumps contained in the original node files. The resulting concatenated polygon files, one for each time step, are selectively loaded into memory for interactive rendering with Sandia's Eigen/VR[3]. This process provides interactive exploration of the data set limited only by the rendering performance of the graphics systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Initial System Performance</head><p>The initial visualization system was developed for visualizing results from an 1860 node Intel Paragon MP supercomputer. The network system was a 10-megabit Ethernet between the Paragon and a Silicon Graphics ONYX for visualization. The ONYX system was configured with four R4400 250 MHz processors, two REALITY II graphics pipes, 6 gigabytes of main memory, and 96 gigabytes of RAID 0 disk. This system was capable of the following processing times for a 100 million-cell calculation computed on the ASCI Red machine: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="16.5">hours</head><p>The rendering times shown in <ref type="table" target="#tab_0">Table 1</ref> are a single decimated frame containing 13.7 million triangles. All stages in the processing path were considered to be inadequate for TeraFLOPS problems. A goal was set to reduce the total processing time to less than 4 hours. This goal would permit 100 million cell calculation to prepared for exploration in a morning of work.</p><p>Also, the rendering system had several issues that needed to be resolved. First, Eigen/VR was a 32-bit code, which was limited to a 2-gigabyte memory image (i.e., only 1-2 time-steps in memory). The rendering speed was too slow at ~104 seconds/frame for easy interaction. Many other visualization tools, which integrate the isosurface generation with rendering, were tested, but all required minutes to tens of minutes to generate frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Improved Production System</head><p>To meet the 4-hour time and improved interaction goals, the entire post-processing path from the physics simulation code to the analyst's interaction with the visualization system was improved. The visualization server was improved to a Silicon Graphics ONYX II visualization system with sixteen 190 MHz R10000 processors, four INFINITE REALTY graphics pipes, 32 Gigabytes of memory, and 1.5 terabytes of Fiber Channel connected disk. The network was improved from a single Ethernet to four ATM OC/3 channels increasing network bandwidth to 155 Megabits/sec per channel.</p><p>To improve the software, several research and development paths were explored. The isosurface software, a marching-cubes algorithm, was parallelized <ref type="bibr" target="#b2">[4]</ref> and the concatenation function was integrated into the software. The rendering software was rewritten to accommodate a 64-bit memory image and improve the efficiency of using the graphics hardware. Although the software is undergoing continuous evolution, the current enhancements for the 100 million-cell CTH data set are presented in <ref type="table" target="#tab_1">Table 2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">hours</head><p>The parallelization of the isosurface/decimation algorithms shifted the bounding limit from processor limited to I/O limited. Thus, current work is focused on improving I/O performance <ref type="bibr" target="#b3">[5]</ref>. Also improved rendering rates are still desired. The graphics pipe efficiency increase was due to many small improvements; however, the major gain in performance was achieved by replacing polygon lists with triangle strips. The triangle stripe conversion adds a 10% overhead to the isosurfacing time The current 12 fold improvement in rendering speed is due to approximately a four fold increase in hardware speed and an approximately three fold increase in software efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Lessons Learned</head><p>The improved hardware and software have greatly decreased the pipeline latency for isosurface exploration of structured data sets. The speedup was accomplished by improving all stages of the pipeline. During the process, several lessons were uncovered that apply to all stages in the pipeline. The four primary lessons learned during this work are:</p><p>1. buffer and 32bit addressing limits, 2. stay parallel at all points in the path, 3. balance all components, and 4. the importances of parallel I/O.</p><p>Primary issues impacting the ASCI visualization are buffer and memory limits. These problem surfaces in two main problem areas, insufficient buffer sizes and 32bit limited addressing. Many tools, like the Unix csh, have internal buffers that overflow when presented with 2500 files in a directory. A prime example is "cp *.dat." This is a common way to transfer files on a disk system from one directory to another location. The csh, while generating the list of file names for the "cp" command, overflows a buffer and issues a copy with less than the full number of files. Recursive copy, tar, and other tricks provide a work-around to problems but the issue is that Unix on many machines may have 64 bit file systems and still have difficulty with large numbers of files.</p><p>The second issue, 32 bit addressing, is simply that files and memories larger than 2 gigabytes require 64 bit addressing. A classic example of this is the rendering code. Managing polygon lists for multiple variables across multiple time dumps easily exceed 2 gigabytes. Initially Sandia's Eigen/VR code was used for rendering, however, it was coupled to a variety of virtual reality input devices. Many of these had only 32 bit driver libraries. Therefore, it proved to be faster rewriting the rendering software rather than disentangle all the 32 bit dependencies.</p><p>It is also very important to stay parallel at all stages in the pipeline. Any time the processing drops to a single processor it proves to be a bottleneck. A classic example is that in the quest to speed rendering, it was decided to switch from polygon lists to triangle strips. This introduced another step into the pipeline. The initial polygon list to triangle strip conversion was a uniprocessor application (i.e., adds hours to the preprocessing path). After tuning and parallelization, however, the triangle strip conversion was reduced to approximately 4 minutes/per frame (~14 million triangles frame). Further improvements to the triangle strip code, however, will require integration into the output portion of the isosurface/decimation code. The integration is necessary to eliminate writing intermediate results to the file system. Any file I/O is time consuming when working with tens of gigabytes.</p><p>The need to couple the isosurface/decimation code to the triangle striping code is an example of the third issue. The requirement to couple the codes is generated by attention to balancing the overall system. The simplest way to deploy the triangle-striping code would be to simply read the output files of isosurface/decimation code and convert them to triangle-strips. This initial solution, however, adds two file system accesses to the processing pipeline, which greatly delays the processing path.</p><p>The final issue is parallel I/O. The need for good parallel I/O can not be understated. On the current visualization server with 16 processors the disks sustain read rates above 300 megabytes/sec. Dividing the disk bandwidth by the number of processors yield slightly less than 20 megabytes/sec per processor. This is a mediocre bandwidth considering the speed of existing processors. Sixteen processors all trying to access disk at the same time, however, delivers miserable performance as 16 requests fight for the disk heads. To mitigate these problems, file formats and libraries need to consider support for coordinated I/O. Both parallel data transfers and parallel processing tools are critical for rapid visualization of ASCI data sets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Figures</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Computational Physics Analysis Loop</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :Figure 2 :Figure 1 :Figure 2 :</head><label>1212</label><figDesc>Example of Shock Physics Calculation: Pre-impact Re-entry Body Example of Shock Physics Calculation: Re-entry Body Collision Example of Shock Physics Calculation: Pre-impact Re-entry Body Example of Shock Physics Calculation: Re-entry Body Collision Production Visualization for the ASCI One TeraFLOPS Machine Philip D. Heermann</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Initial Visualization System Performance</figDesc><table><row><cell>Task</cell><cell>Processing</cell><cell>Notes</cell></row><row><cell></cell><cell>Time</cell><cell></cell></row><row><cell cols="2">Simulation Time 17 hours</cell><cell>6 Gbyte database</cell></row><row><cell>Data Transfer</cell><cell>5.5 hrs</cell><cell>10 Mb Ethernet</cell></row><row><cell>Isosurfacing/</cell><cell>11 hours</cell><cell>20-30</cell></row><row><cell>Decimation</cell><cell></cell><cell>min./timestep</cell></row><row><cell cols="2">Concatenation~ 1 minute</cell><cell>Disk Bandwidth</cell></row><row><cell></cell><cell></cell><cell>limited</cell></row><row><cell>Frame</cell><cell cols="2">04 sec/frame~5% of peak</cell></row><row><cell cols="2">Rendering Time~1</cell><cell>graphics pipe</cell></row><row><cell></cell><cell></cell><cell>efficiency</cell></row><row><cell>Total</cell><cell></cell><cell></cell></row><row><cell>Postprocessing</cell><cell></cell><cell></cell></row><row><cell>Time</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Current Data Visualization System Performance</figDesc><table><row><cell>Task</cell><cell>Processing Time</cell><cell>Notes</cell></row><row><cell cols="2">Simulation Time 17 hours</cell><cell>6 Gbyte database</cell></row><row><cell>Data Transfer</cell><cell>5 minutes</cell><cell>24 Mbytes/sec</cell></row><row><cell>Isosurfacing/</cell><cell>3.7 X speedup on 4</cell><cell>Exclusive of I/O.</cell></row><row><cell>Decimation</cell><cell>processors</cell><cell>Parallel file I/O is</cell></row><row><cell></cell><cell></cell><cell>still under</cell></row><row><cell></cell><cell></cell><cell>development</cell></row><row><cell>Concatenation</cell><cell>Integrated with</cell><cell></cell></row><row><cell></cell><cell>Isosurfacing</cell><cell></cell></row><row><cell>Frame</cell><cell cols="2">.2 seconds/frame~15% peak</cell></row><row><cell cols="2">Rendering Time~8</cell><cell>graphics pipe</cell></row><row><cell></cell><cell></cell><cell>utilization</cell></row><row><cell>Approx. Total</cell><cell></cell><cell></cell></row><row><cell>Postprocessing</cell><cell></cell><cell></cell></row><row><cell>Time</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgements</head><p>The work to improve the production visualization system depends on many researchers. Pat Crossno and Dino Pavlakos have worked to parallelize the isosurface and decimation code. Marlin Kipp provided the CTH data sets. The Sandia networking staff and Marty Barnaby have greatly improved the network hardware and software systems. Pang Chen has worked to improve the parallel file systems. Jake Jones authored the 64 bit rendering code. Without the talents and time of these researchers and many others this work would not be possible. This work is supported by United States Department of Energy under contract DE-AC04-94AL85000. Additional funding provided by Sandia National Laboratories internal research program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Marching Cubes: A High Resolution 3D Surface Construction Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Lorensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Cline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (SIGGRAPH &apos;87 Proceedings</title>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="page" from="163" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Decimation of Triangle Meshes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Zarge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Lorensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (SIGGRAPH &apos;92 Proceedings)</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="65" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Heterogenous Graphics Procedure for Visualization of Massively Parallel Solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Crossno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Cline</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Jortner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CFD Algorithms and Applications for Parallel Processors, ASME Fluids Engineering Conference</title>
		<meeting><address><addrLine>Washington D.C.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="page" from="65" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Sturtevant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Christon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Heermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">PDS/PIO: Lightweight Libraries for Collective Parallel I/O, accepted for publication in the Proceedings of SC98</title>
		<meeting><address><addrLine>Orlando, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
