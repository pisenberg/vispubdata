<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">High Quality Rendering of Attributed Volume Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Tiede</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Mathematics and Computer Science</orgName>
								<orgName type="institution">Medicine University Hospital Eppendorf</orgName>
								<address>
									<settlement>Hamburg</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schiemann</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Mathematics and Computer Science</orgName>
								<orgName type="institution">Medicine University Hospital Eppendorf</orgName>
								<address>
									<settlement>Hamburg</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Heinz</forename><surname>Höhne</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Mathematics and Computer Science</orgName>
								<orgName type="institution">Medicine University Hospital Eppendorf</orgName>
								<address>
									<settlement>Hamburg</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">High Quality Rendering of Attributed Volume Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.3.3 [Computer Graphics]: Picture/Image Generation-Display algorithms; I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Color</term>
					<term>shading</term>
					<term>shadowing and texture partial-volume-effect</term>
					<term>ray-casting</term>
					<term>tomographic data</term>
					<term>Visible-Human-Project</term>
				</keywords>
			</textClass>
			<abstract>
				<p>For high quality rendering of objects segmented from tomographic volume data the precise location of the boundaries of adjacent objects in subvoxel resolution is required. We describe a new method that determines the membership of a given sample point to an object by reclassifying the sample point using interpolation of the original intensity values and searching for the best fitting object in the neighbourhood. Using a ray-casting approach we then compute the surface location between successive sample points along the viewingray by interpolation or bisection. The accurate calculation of the object boundary enables a much more precise computation of the gray-level-gradient yielding the surface normal. Our new approach significantly improves the quality of reconstructed and shaded surfaces and reduces aliasing artifacts for animations and magnified views. We illustrate the results on different cases including the Visible-Human-Data, where we achieve nearly photo-realistic images.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Three-dimensional (3D) visualization of tomographic volume data such as Computer-Tomography (CT) or Magnetic-Resonance-Imaging (MR) is an important aid for diagnosis, treatment planning, teaching and other applications. The common feature of these imaging modalities is that different objects are characterized as different levels of the measured intensity values. At the boundary of adjacent objects the intensity jumps from one level to the other. However, due to the limited spatial resolution of the scanning devices the intensity of the voxels containing the boundary is averaged. This effect is called the partial-volume-effect. A variety of 3D-visualization methods using this effect implicitly have been published, e.g. isosurface reconstruction and http://www.uke.uni-hamburg.de/idv volume-rendering One well-known algorithm for isosurface reconstruction is Marching-Cubes <ref type="bibr" target="#b5">[7]</ref> which produces polygonal meshes of the structures that are within a given intensity range. The polygonal object representation can be rendered very fast on special graphics hardware. The disadvantage is that only surfaces can be displayed, because the information about the interior of the object contained in the original intensity values is lost. However, these values might be of great concern for diagnostic purposes.</p><p>In the volume-rendering approach originally investigated by <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">6]</ref> an opacity value is computed for each voxel by assigning color or transparency values to the original gray values and weighting them with the magnitude of the gray-level-gradient at the voxel position. A voxel with a high opacity is less transparent than a voxel with a lower value. During ray-casting the voxels are shaded and weighted with their respective opacity yielding multiple transparent surfaces, which are difficult to interpret <ref type="bibr" target="#b11">[13]</ref>.</p><p>Although volume-rendering produces nice looking images most medical applications require an explicit object representation that allows measurements and removal of structures which obscure the view to the objects of interest.</p><p>In many cases objects can be differentiated with distinctive intensity ranges, a method known as threshold segmentation. Typically the average intensity value of adjacent objects is used for thresholding. An intensity range for an object is then specicified using a lower and an upper threshold value. However, if different objects share the same intensity range an explicit assignment which voxel belongs to which object is required. We use morphological operations and connected-component-analysis as described in <ref type="bibr" target="#b3">[4]</ref>. In difficult cases the segmentation has to be done manually. The result of the segmentation process is an additional volume containing discrete attribute values for each voxel about its membership to a certain organ. These attribute values trigger the individual visualization of the respective objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem</head><p>Ray-casting for visualizing sampled volume data has been elaborately described in the literature <ref type="bibr">[5,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b13">15]</ref>. The classical approach uses discrete scan-conversion algorithms to access the attribute values which control the visualization of the objects. The pitfall of this method is that the subvoxel resolution of the viewing ray is lost which causes staircase looking surfaces of the objects ( <ref type="figure">fig. 1 left)</ref>. For technical reasons or patient care the enhancement of the spatial resolution of the volume data to reduce this effect is not possible in most cases. Although the real boundary of an object cannot be reconstructed exactly, for realistic and high resolution renderings of attributed volume data a better approximation of the surface than the arbitrary voxel boundaries is required ( <ref type="figure">fig. 1 right)</ref>. None of the methods published so far accomplish this for segmented data. <ref type="figure">Figure 1</ref>: Classical ray-casting using discrete scan-conversion produces staircase looking surfaces (left). Aim is a better approximation of the object boundaries in subvoxel resolution (right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHOD</head><p>In the following section we describe a new technique for the reconstruction of object boundaries in subvoxel resolution based on the object classification resulting from the segmentation step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Key idea</head><p>We propose the following scheme: geometrically correct ray-casting using floating-point calculation instead of discrete scan-conversion; reclassification of the sample position by interpolating the gray values and searching for the best fitting attribute value in the 2 2 2 neighbourhood;</p><p>interpolation of the surface location between successive sample points; computation of the surface normal at the subvoxel surface location using the gray-level-gradient; detection and handling of special situations where no threshold boundary exists as is the case for manually segmented objects.</p><p>The most difficult thing is to determine the attribute value at an arbitrary subvoxel position, because it can be different from the value at the nearest neighbour position as it was originally assigned to the voxel by the segmentation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Computation of the attribute value</head><p>We compute the attribute value at a particular subvoxel sample position of the viewing ray as follows:</p><p>initialize a counter to zero; calculate the gray value at the sample position using tri-linear or higher order interpolation (e.g. B-splines); for each attribute voxel in the 2 2 2 neighbourhood of the sample position check if the gray value is within the intensity range of the respective object definition. If the value is inside increment the counter. Each distinct object is considered only once;</p><p>after all neighbours have been tested the counter is in one of three states:</p><p>1. the counter is "0", i.e. no fitting object was found, so the sample position is marked unclassified;</p><p>2. the counter is "1", i.e. the interpolated intensity value fits exactly to one object definition. Take the attribute value of this object. This is the normal situation;</p><p>3. the counter is "2" or greater. In this case the classification condition of more than one object is true. A situation that occurs e.g. for manually segmented objects. For instance the gray matter of the brain can be classified using thresholding, the individual gyri forming the gray matter cannot be distinguish this way. A simple solution for these critical areas is to use the attribute value of the nearest-neighbour voxel. However, this would result in staircase surfaces and introduces aliasing. So we propose an interpolation scheme based on the attribute values (see section 2.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Interpolation of the surface location</head><p>In the previous step the object membership of the sample position on the viewing ray was computed. For a realistic rendering we need a precise calculation of the surface orientation. For tomographic data the gray-level-gradient is the superior method to approximate the surface normal <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">9]</ref>. However, the sample position is in general not on the boundary, but somewhere inside the object, therefore calculating the gradient at the sample position would introduce aliasing. The boundary of the object must be located between the current and the previous sample position. (Otherwise the previous position would have triggered the visualization.) A first approximation is to linear interpolate the boundary between successive sample points Pi,1 and Pi in the following way, 1. calculate the gray-level intensities gi,1 and gi at the respective sample points. (These values have already been computed for the attribute determination, so we use them again.)</p><p>2. given the threshold t of an object the location of its boundary B is interpolated as</p><formula xml:id="formula_0">B = Pi , s Pi , Pi,1 with s = t , gi,1 gi , gi,1 ,</formula><p>where t is the lower bound of the intensity range if gi,1 t , otherwise t is the upper bound.</p><p>Thus the distance between Pi,1 and Pi is divided with the same ratio as s divides gi,1 and gi. However, tri-linear interpolation of the intensities produces curved boundaries between the regions seperated by a threshold t. This is demonstrated in <ref type="figure" target="#fig_0">fig. 2</ref>. Given two sample points along the viewing ray and their respective intensity values linear interpolation yields the object boundary at B1. Besides the incorrect location of the boundary it is important to realize that the value of B1 depends of the sample positions, which themselves depend on the viewing parameters and therefore introduces aliasing in animated sequences. We also notice a C 1 discontinuity of the gray values along the ray at the position where the neighbourhood for the interpolation changes. Although an analytical solution to compute the correct boundary B2 might exist (at least for linear interpolation) for simplicity reasons we have implemented a bisection algorithm that approximates B2 through bisecting the line between Pi,1 and Pi and recalculation of g and s until jg , tj " .</p><p>Choosing different values for " enables control of accuracy versus speed. This scheme also works correctly for higher order interpolation methods like cubic-splines as long as they are well-behaved and do not produce multiple extrema on the intensity profile between successive sample points. if the intensity ranges form a gap there are two surfaces. The one we take depends on the viewing direction (normally the backfacing surface is not interesting). The area between the surfaces is unclassified; in cases where the intensities of the objects overlap, e.g. manual segmentation, the boundary is not unique. This situation is detected as follows, 3. if neither threshold produce a boundary, as is the case for manually segmented objects, we calculate the g 0 s using interpolated attribute values (see section 2.5), which then produce a unique boundary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Detecting thin and small objects</head><p>The combination of discrete attribute voxels and thresholding interpolated intensities at the sample positions along the viewing can produce very small and thin objects which might get lost if the sampling rate is too large. This results in serious aliasing artifacts, which prominently occur in animated sequences. Our new method computes the correct attribute value at any arbitrary subvoxel position and therefore enables classical oversampling ( <ref type="figure">fig. 4</ref>). As the Nyquist frequency is not known a tradeoff between computation <ref type="figure">Figure 3</ref>: Depending on the intensity ranges assigned to the objects different surface boundaries exist. For manually segmented objects the boundary is not unique, because the intensity ranges overlap (right).</p><p>time and the accuracy of small structures has to be made. We found a sample point distance that is about the pixel size of the resulting image is a practical solution. <ref type="figure">Figure 4</ref>: The classification driven attribute computation enables oversampling so that even small and thin objects are rendered correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Interpolating attribute values</head><p>Although the described methods work well for normal situations there still remains a number of problematic situations, where the attribute value cannot be determined using additional threshold classification. These situations occur especially where objects have been segmentated manually, however, the affected regions are generally small, so only a few voxels are involved. Therefore our approach here is to generate interpolated values from the discrete attribute values themselves. To check if a sample position is inside of a specific object we 1. create a binary subvolume where each voxel in the 2 2 2 neighbourhood of the sample position is set to "1", if the attribute value in the corresponding attribute volume is the same we are testing. Otherwise the voxel is marked "0"; 2. compute a tri-linear interpolated value of the binary volume at the sample position;</p><p>3. if the interpolated value is less than 0:5 the position is outside of the object, otherwise it is inside. <ref type="figure" target="#fig_1">Figure 5</ref> shows some of the surfaces generated by the 256 possible configurations of 8 neighboured "binary" voxels. This scheme can be easily extended to a larger neighbourhood, e.g. <ref type="bibr" target="#b3">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">for</head><p>higher order interpolation and the calculation of the gradient for the surface normal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1">Extension for multi-modality volume data</head><p>The described method for determining the attribute value of a given sample position in subvoxel resolution can be easily extended for 2. compute an interpolated intensity value for each of these attribute values from their respective classification volume; (For each different volume this value is computed only once independant how many attributes refer to it.)</p><p>3. increase the counter if the interpolated value fits into the intensity range of the respective object;</p><p>The remainder of the algorithm is the same. This simple extension is not restricted to two volumes but allows a different volume for the classification of each distinct object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Calculating surface color</head><p>One interesting aspect of the Visible-Human-Dataset is that in contrast to tomographic data, which provide no color information, one can use the rgb-tuples to improve the realism of the surface appearance. Instead of assigning an artificial color to an object description, which is then attenuated by a light reflection model, each distinct surface location can be shaded individually using its respective color. Again for high quality rendering the rgb-values have to be interpolated. However, if one interpolates the color directly at the surface boundary a mixture of the colors of the adjacent objects results. To get a more realistic color we can shift the interpolation center along the surface normal underneath the surface ( <ref type="figure">fig. 6</ref>). <ref type="figure">Figure 6</ref>: A more realistic object color is achieved when interpolated underneath the surface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS</head><p>We integrated the methods into our VOXEL-MAN visualization system and applied them to different segmented volumes. The first example is the "standard" MR volume of a head used by many groups, which we segmented into over 200 constituents <ref type="bibr" target="#b1">[2]</ref> ( <ref type="figure" target="#fig_2">fig.7</ref>). The superior quality of our approach is shown in <ref type="figure" target="#fig_3">fig. 8</ref> where the classical discrete scan-conversion technique generates staircase artifacts on the surface especially on objects boundaries while the attribute reclassification scheme produces smooth renderings at any scale.</p><p>As long as the size of a voxel is about the same size as a pixel of the resulting projection the difference between the classical raycasting approach and our new method is not noticeable. However, if we compute a magnified view the discrete nature of the attribute voxels becomes prominent ( <ref type="figure" target="#fig_4">fig. 9)</ref>.</p><p>The difference in image quality is also demonstrated in <ref type="figure" target="#fig_5">fig. 10</ref> showing a magnified view of the bony structures taken from CT data of a cadaver.</p><p>The next example illustrates the new quality on 3D-views of blood vessels reconstructed from MR-and CT-angiography acqui-  <ref type="figure" target="#fig_2">fig. 7</ref>: classical ray-casting yields staircase artifacts especially at the object boundaries (left). The brain surface appears much smoother using the reclassification algorithm. sitions ( <ref type="figure" target="#fig_7">fig. 11</ref>). Our technique also enables endoscopic views even of very small structures like bifurcations of cerebral arteries as shown in <ref type="figure" target="#fig_0">fig. 12</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Visible-Human</head><p>The Visible-Human-Dataset provided by the NLM has gained much attention in the past years <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b10">12]</ref>. For the first time colored digital cross-sectional images of human anatomy in a resolution unatainable of today's tomographic scanner technology are avaiable. We reduced the slice resolution by a factor of three to get a managable volume size and segmented different parts of the body using ellipsoids in RGB-space for the classification of the objects <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b8">10]</ref>. For the visualization we adapted the reclassification scheme to deal with ellipsoids: to determine the interpolation factor s as described in section 2.3 the 2nd order surface of an ellipsoid requires the solution of a quadric equation. As described in section 2.5.1 the interpolation of the color value directly at the object boundary yields in the case of the skin surface a strange appearance caused by the blue background of the data ( <ref type="figure" target="#fig_6">fig. 13 left)</ref>. We therefore emperically shifted the interpolation center beneath the surface until skin ( <ref type="figure" target="#fig_6">fig.  13 right)</ref> and muscular structures ( <ref type="figure" target="#fig_10">fig. 14)</ref> unveiled a very natural nearly photo-realistic appearance.</p><p>The multi-modality capability of our approach is demonstrated in <ref type="figure" target="#fig_1">fig. 15</ref> where we took the skull from CT and muscles and vascular structures from the RGB-volume.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Artificial object</head><p>For the simulation of very small manually segmented objects we have created a volume containing a one voxel thin helical structure. The object voxels are 26-connected and no partial-volume-effect has been modeled to demonstrate the attribute interpolation scheme. Serious artifacts are visible using a straight forward ray-casting implementation ( <ref type="figure" target="#fig_11">fig. 16 top left)</ref>. Although the classical discrete scanconversion algorithm produces a mathematically correct result (top right) the voxel-like surface is probably not what one would expect from natural objects like blood vessels. The lower left image shows the reconstruction using the reclassification method in combination with the binary attribute value interpolation, and the lower right image is computed with B-spline interpolation and a slightly lower threshold to improve connectivity.</p><p>To demonstrate the quality aviechable for "natural" objects we rendered the structures in <ref type="figure" target="#fig_2">fig. 7</ref> with thresholding turned off for attribute determination and gradient calculation. It turns out that the appearance of areas with high curvature such as the surface of   <ref type="figure" target="#fig_2">fig. 17)</ref>. Hence the described reclassification scheme is still the superior method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Computational costs</head><p>What are the additional computational costs of our new method in contrast to classical ray-casting?</p><p>1. floating-point calculation for ray-traversal to maintain subvoxel resolution in contrast to digital-differential-analyzer techniques which can be implemented using integerarithmetic only;</p><p>2. tri-linear (or higher order) interpolation of the gray-values at each sample position;</p><p>3. examination of the 8 neighbours;</p><p>4. calculation of the surface location between sample points. (For opaque surface this is only done once per pixel).  If we take volume-rendering with non-transparent objects and early ray termination as the basis only 3. applies as additional effort. <ref type="table" target="#tab_1">Table 1</ref> lists typical computation times for some of the figures.</p><p>As our new approach generates superior image quality for segmented data computation times are not directly comparable, for instance the endoscopic view of the blood vessels in <ref type="figure" target="#fig_0">fig. 12</ref> cannot be created with classical ray-casting. For the images shown here we found that the determination of the attribute value requires approximately 40-60%, interpolation 25%, ray-casting 14%, gradient calculation 2.5% and surface location computation 2% of the total CPU time. Image computation time will therefore mostly benefit from reducing attribute calculation costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSION AND FUTURE WORK</head><p>We have developed a new method for the precise location of object boundaries segmented from tomographic volume data. The method is based on the partial-volume-effect and reclassifies the sample positions along the viewing rays by interpolating the original gray-values and searching for the best fitting attribute value in the neighbourhood. The boundary is then determined between successive sample points using linear interpolation or bisection. This in turn enables a more accurate calculation of the surface normal using the gray-level-gradient. Our method also detects regions which result from manual segmentation or other unusual conditions in which case we apply an interpolation scheme on the attribute values. We have demonstrated the new quality achievable with a number of different datasets. Small and thin objects like blood-vessels as well as endoscopic views can be rendered in high resolution using the oversampling ability of the technique. In the case of the colored Visible-Human cross-sections nearly photo-realistic images result. The described techniques form the kernel of the VOXEL-MAN visualization system. Our future work will include the improvement for manually segmented regions, where the proposed interpolation of the attribute values is only a first step; the extension of the scheme to other segmentation methods. So far we have only incorporated thresholding for scalar volume data and ellipsoids for the colored Visible-Human images;</p><p>quantitative measurements about the accuracy of the method.      <ref type="figure" target="#fig_2">fig. 7</ref>). Objects with high curvature look very similar while areas with low curvature such as the skin surface reveal some artifacts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Linear interpolation between sample points results in an object boundary at B1. Bisecting the line between Pi,1 and Pi yields a better approximation to the correct result at B2. Notice the C 1 discontinuity where the neighbourhood changes.Depending on the segmentation different situations for the location of the object boundary occur (fig.3), exactly one boundary exists if the intensity ranges of adjacent objects are in touch;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Interpolation of attribute values in a 2 2 2 neighbourhood. Most attribute configurations yield smooth surfaces. The surfaces are complementary (e.g. 1st row middle and 2nd row left), so overlapping cannot occur.multi-modality data such as the Visible-Human-Dataset (see section 3.1) which provides colored anatomical cross-sectional images together with registered CT scans of the frozen cadaver. This allows the segmentation of bony structures from the CT volume while all other structures are better segmentable from the anatomical data. The classification of different objects depends then on different volumes. We therefore changed the basic algorithm in section 2.2 in the following way:1. determine all distinct attribute values in the neighbourhood;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 :</head><label>7</label><figDesc>Reconstruction of the "standard" MR head.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 :</head><label>8</label><figDesc>Magnified view of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 :</head><label>9</label><figDesc>Comparison of classical ray-casting (left column) and subvoxel precise determination of the attribute values (right column). Notice the smooth boundaries in the magnified view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 :</head><label>10</label><figDesc>Reconstruction of the hip joint of a cadaver taken from CT. Notice the smooth shadow on the background wall generated by our reclassification scheme. the brain are very much alike, however, low curvature structures show certain artifacts (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 13 :</head><label>13</label><figDesc>Calculation of surface color: directly at the surface boundary (left), one and two steps underneath the surface (middle and right). Notice the natural appearance of the lips (see color plate).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 :</head><label>11</label><figDesc>Reconstruction of blood vessels segmented from MRangiography: The recognizability of small vessels is significantly improved (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 :</head><label>12</label><figDesc>Endoscopic view of the cerebral blood vessels (left): the viewer is located at the tip of the arrow (right) inside of the left middle cerebral artery in front of a bifurcation. Although the entire structure is only a view voxel in diameter the attribute reclassification yields smooth surfaces unatainable with classical methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig</head><label></label><figDesc>Fig.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 14 :</head><label>14</label><figDesc>Photo-realistic reconstruction of muscles and vascular structures of the Visible-Human.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 16 :</head><label>16</label><figDesc>Different renderings of a 26-connected one voxel thin artificial object simulating very small manually segmented structures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 17 :</head><label>17</label><figDesc>Object rendering using attribute interpolation only (compare with</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>1. if the gray value gi at position Pi is inside the intensity range of object Oi and gi,1 at position Pi,1 is outside take the boundary of object Oi;</figDesc><table /><note>2. if gi as well as gi,1 are inside Oi try if the threshold of object Oi,1 forms a boundary, i.e. gi,1 is inside Oi,1 and gi is outside Oi,1;</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Computation times given for a DEC-Personal-Workstation (433 Mhz, 1 GB main memory)</figDesc><table><row><cell>.</cell><cell cols="3">image size [Pixel] volume size [Voxel, MB] CPU [s]</cell><cell>remarks</cell></row><row><cell>7</cell><cell>512542</cell><cell>150200192, 11.0</cell><cell>93</cell><cell>shadow, 4 lights</cell></row><row><cell>17 11 left</cell><cell>" 512512</cell><cell>" 256256240, 30.0</cell><cell>137 52</cell><cell>attribute interpolation classical ray-casting</cell></row><row><cell>11 right 12 left</cell><cell>" 512542</cell><cell>" 25625690, 16.9</cell><cell>131 18</cell><cell>endoscopic view</cell></row><row><cell>12 right 15</cell><cell>" 10241024</cell><cell>" 183246277, 97.1</cell><cell>169 556</cell><cell>shadow</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">A</forename><surname>Drebin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loren</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="65" to="74" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">VOXEL-MAN, Part 1: Brain and Skull, Version 1.0</title>
		<idno>3-540-14517-6</idno>
	</analytic>
	<monogr>
		<title level="m">Springer-Verlag Electronic Media, Heidelberg</title>
		<editor>Karl Heinz Höhne</editor>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Shading 3D-images from CT using gray level gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Heinz Höhne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging, MI</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="47" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Interactive 3D-segmentation of MRI and CT volumes using morphological operations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Heinz Höhne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Hanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Assist. Tomogr</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="285" to="294" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Display of surfaces from volume data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Graphics Appl</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="29" to="37" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Marching Cubes: A high resolution 3D surface construction algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">E</forename><surname>Lorensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harvey</forename><forename type="middle">E</forename><surname>Cline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="163" to="169" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The Visible Human Project</title>
		<ptr target="http://www.nlm.nih.gov/research/visible/visiblehuman.html" />
		<imprint>
			<date type="published" when="1996" />
		</imprint>
		<respStmt>
			<orgName>National Library of Medicine</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Surface shading in tomographic volume visualization: A comparative study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Pommert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Tiede</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunnar</forename><surname>Wiebecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Heinz</forename><surname>Höhne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First Conference on Visualization in Biomedical Computing, Proc. VBC &apos;90</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
	<note>Multi-modality reconstruction of skull and muscles of the Visible-Human</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Segmentation of the Visible Human for high quality volume based visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schiemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Tiede</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Heinz</forename><surname>Höhne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="263" to="270" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Volumetric ray tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">M</forename><surname>Sobierajski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arie</forename><forename type="middle">E</forename><surname>Kaufman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE Volume Visualization Symposium Proceedings</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1994-10" />
			<biblScope unit="page" from="11" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Visible Human male: A technical report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Spitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Ackerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><forename type="middle">L</forename><surname>Scherzinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Whitlock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Med. Inf. Ass</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="118" to="130" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Investigation of medical 3D-rendering algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Tiede</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Heinz</forename><surname>Höhne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bomans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Pommert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riemer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunnar</forename><surname>Wiebecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Graphics Appl</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="41" to="53" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visualizing the Visible Human</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Tiede</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schiemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Heinz</forename><surname>Höhne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Graphics Appl</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="9" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Discrete ray tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roni</forename><surname>Yagel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arie</forename><surname>Kaufman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Graphics &amp; Appl</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="19" to="28" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
