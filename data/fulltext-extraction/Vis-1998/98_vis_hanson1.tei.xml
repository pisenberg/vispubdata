<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image-Based Rendering with Occlusions via Cubist Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Hanson</surname></persName>
							<email>fhanson@cs.indiana.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Indiana University Bloomington</orgName>
								<address>
									<postCode>47405</postCode>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">A</forename><surname>Wernert</surname></persName>
							<email>ewernertg@cs.indiana.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Indiana University Bloomington</orgName>
								<address>
									<postCode>47405</postCode>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Image-Based Rendering with Occlusions via Cubist Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I</term>
					<term>3</term>
					<term>6 [Computer Graphics]: Methodology and Techniques-Interaction Techniques</term>
					<term>I</term>
					<term>3</term>
					<term>7 [Computer Graphics]: Three-Dimensional Graphics and Realism</term>
					<term>I</term>
					<term>3</term>
					<term>8 [Computer Graphics]: Applications Image Based Rendering; Occlusions</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Figure 1: (a) &quot;Head of Woman Reading&quot; and (b) &quot;Sleeping Nude&quot; by Pablo Picasso.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We address the problem of generating image sequences of a static scene with occluded objects using a single source image. Our approach is to abandon the geometric relationship between sourceimage pixels and scene geometry used in conventional image-based rendering (IBR); instead, we substitute more abstract source images inspired by the multiperspective content of cubist art, as represented in the Picasso paintings of <ref type="figure">Figure 1</ref>. There are two basic steps in the process: first is the selection of an arrangement of source pixels that covers the scene areas to be viewed; the second is the determination of key-frame projections of camera rays into this source image. Since, as in the examples of <ref type="figure">Figure 1</ref>, the geometric relationships of the source pixels may be arbitrarily stretched, distorted, or even torn, the entire geometric content of image production is shifted to the locations of the camera-ray intersections warped into the source image. The rendering of image sequences from spatially sparse and temporally sparse keyframes can in principle be accomplished by interpolating camera rays directly in the source image.</p><p>The "cubist" source images that we attempt to exploit typically contain occluded data and geometric distortions that may not be attributable to a real camera; in addition, we normally impose the condition that no scene point appears more than once. This distinguishes our construction from those whose source images are obtained from a sequence of physically plausible cameras, and which may contain multiple copies of the same scene element (see, e.g., the multiperspective panorama approach <ref type="bibr" target="#b14">[16]</ref>). The most natural domains for our approach are thus terrain and terrain-like scenes that can be "shrink-wrapped" with a single continuous sheet, though other topologies can also be treated. The fact that geometry is removed from the image data is both a strength, in that the image can be warped to suit our needs, and a weakness, in that choosing this warping is a nontrivial task. Finding camera ray positions is also costly because in principle it requires having an accurate geometric model of the scene and the transformation to the source image.</p><p>There are two distinct mechanisms by which we can achieve the data compression characterizing successful IBR methods. No compression at all results if we raytrace each image pixel into the source image at each time step of the animation. We might just as well prestore the entire image sequence. By doing a spatially sparse ray sampling with dense time, we can achieve one level of data compression using texture mapping methods. The more challenging problem is to take sparse time samples as well; temporal interpolation of such key-frame time samples in source-image space produces a smooth time sequence without the associated cost per frame.</p><p>Hints about the relationship between desirable image warpings and camera ray interpolation can be determined by examining exact frame-by-frame projections of a moving camera's pixel rays into the 3D scene and their paths in the (warped) source image. Acceptable animations of highly occluded scenes can be constructed by choosing key frames whose interpolants closely follow the exact paths. This procedure requires only the single source image plus a chosen set of ray intersections.</p><p>A visual summary of the entire process is presented in <ref type="figure">Figure 2</ref>, which is explained in detail later.</p><p>A key feature of our approach is that it separately encodes all of a scene's photometry in a single source image in a form distinct from the geometry. This is significant for visualization applications since data in this source image can be manipulated using a variety of standard 2D image filters (e.g., color mapping, contrast tuning, edge-detection, etc.), thereby creating a complete re-rendering of the animation at the cost of a single image filtering. The option of storing per-pixel normal and position information with the source image provides additional flexibility for recomputing lighting effects.</p><p>In summary, our main result is the introduction of "cubist" multiperspective source images and the mapping of viewing rays from camera keyframes into this distorted scene to produce accurate occluded-scene animations. The benefits include potential data compression in both the spatial and temporal domains and the ability to easily modify a range of rendering attributes.</p><p>Background: Image Based Rendering. Image-based rendering (IBR) is based on the idea that prerendered pixel arrays can be associated with sufficient scene geometry to allow the accurate reconstruction of digital images appropriate to some constrained set of moving camera parameters. Thus, IBR avoids the expense of storing a full time-step array of images by enforcing camera constraints (e.g., restricting the camera motion to pivot about a fixed focal center) that enable the use of prerendered texture maps embodying very complex image models (including real-life photographs) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b13">15]</ref>. The challenge of the IBR approach is to produce realistic images while using less image data than a direct animation sequence and less geometric knowledge about the scene than one would need to directly render each changing viewpoint of a textured polygon model, e.g., with textures derived from a radiosity approach or actual photographs. Unfortunately, when one attempts to move beyond the now-traditional constraints of fixed-focal center with only rotation and zoom camera motions, straightforward texture warping as a substitute for geometry breaks down and it has been found necessary to bring in additional geometry. Major obstacles to straightforward image warping include the tearing and folding of the image when general camera motions are attempted in the presence of occlusions. One approach to handling these difficulties is to obtain images from neighboring viewpoints to fill in the holes <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">13]</ref>. The distortion maps of vanishing points in the image of a moving camera also need further image-plane geometry in the form of specially structured meshes, as investigated in <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b15">17]</ref>; foreground geometry can be extracted into a separate layer and the images recomposited using masks. The warping of images to account for changes in the camera model has been studied in <ref type="bibr" target="#b8">[10]</ref>. Other methods require the addition of depth or range data <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">11]</ref> for each source image used for rendering, or adapt simplified polygon models to the task <ref type="bibr">[3]</ref>. The multiperspective panorama approach <ref type="bibr" target="#b14">[16]</ref> achieves some extra flexibility by using a single image as a repository for composited information relevant to many viewpoints on a single constrained camera animation path. Alternative approaches based on light fields can require vast amounts of data storage <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" target="#b12">14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Concepts</head><p>In this paper, we study the idea of IBR based on a flattened, possibly torn, wrap-around texture image (the source image) and the paths in time of camera rays mapped from their 3D scene intersection points to this single warped image. We also investigate the feasibility of sparse spatial camera-ray sampling and sparse cameraray time samples whose interpolations can have spatial singularities: such singularities naturally occur wherever occlusions require jumps across unseen texture patches (see, e.g., <ref type="figure">Figure 12</ref>(c)). The ability to tolerate sparse space or time samples of the camera ray grids can greatly reduce data storage requirements. With these concepts, we are able to select camera motions and generate acceptable image sequences for many scenes containing significant geometric effects and occlusions.</p><p>The basic idea is to flatten 3D image data, e.g., of mountains, buildings, people, or even bridges, to a single 2D texture, and to create interpolations across this virtual terrain that accurately encode geometric distortion as well as the appearance and disappearance of occlusion edges. The use of such a flattened image is very similar to a method employed extensively by the cubist school of painting (see the Picasso paintings in <ref type="figure">Figure 1</ref>), which depicts objects as they are conceptually represented in the mind and not the way they are seen; all sides of a person or object may be presented to the viewer at once on a 2D canvas. While the viewer of a Picasso is expected to reconstruct the object being represented using the imagination, we approximate the same result using complex mappings of camera rays to the warped source image, and by analyzing the behavior that would be needed in the corresponding keyframe interpolation methods. Our approach is also closely related to methods of classical topology for representing 2D surfaces embedded in 3D; as shown in <ref type="figure">Figure 3</ref>, a complex surface may be represented by a flat polygon with certain edges identified; in our philosophy, such objects can in principle be handled if they are in contact with the ground plane by mapping them into a flattened torn or cut-out sheet, as shown in <ref type="figure" target="#fig_0">Figure 4</ref>.</p><p>We observe that the original IBR concept of simplifying continuous image generation by constraining the camera motion to a single center of rotation can, in our approach, be extended to much more general motions; the interesting feature of this is that if there are very complex areas whose interpolations we cannot handle smoothly, we can revert to forbidding certain camera parameter ranges, thus restricting the user's freedom to sets of views that have reduced rendering complexity criteria; this is a good example of an adaptation of the constrained navigation paradigm to the IBR arena, and is in some sense complementary to the multiperspective panorama concept <ref type="bibr" target="#b14">[16]</ref>. The result is to provide a natural context for the exploitation of constraint manifolds that reduce the scope and complexity of the rendering task as well as enabling simplified navigation through the scene (see, e.g., <ref type="bibr" target="#b4">[6]</ref>).</p><p>The procedure we propose is then summarized as follows: Instead of assigning texture to each facet of the geometric structures, <ref type="figure">Figure 2</ref>: Frames of an animation illustrating the projection of camera rays into the 3D scene model followed by a warp to the distorted 2D source image. In the first row, the 3232 array of dots represents the paths of rays from the camera focal point through sampled image pixel centers. In the second row, the ray intersections are seen obliquely, and the ray paths clearly make large jumps in the real spatial geometry as they pass from the occlusion edge of a nearby building to the face of a more distant building. In the third row, after the points where the rays intersect the geometry are warped to the source image, the geometry is embodied in the positioning of the ray intersections. The bottom row shows the scene as it appears reconstructed from the data using our method with a sample density of 6464. of which there may be millions in the scene, we use camera-ray casting followed by a warp to assign a geometric property to each pixel of an "unwrapped," possibly torn, flattened texture that represents (uniquely) every pixel viewable from every viewpoint in the camera navigation domain; this geometric assignment may lead to distortions and singularities, but it does not require multiple images or range data like other approaches. The fundamental issue is whether or not the per-pixel data structure associated with our source image data structure and the time taken to perform the singular warp transformation is more or less efficient than a direct textured polygon rendering. Some of the needed features might in principle be supportable via imaginative usage of present graphics hardware capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Fundamental Methods.</head><p>In a typical IBR method, the camera view plane is geometrically related to the prestored source image, and that image is globally warped as necessary to produce a precise representation of what the camera would see as points in the projected 3D scene. We propose the following paradigm to extend the applicability of the method to scenes with occlusion whose photometry is encapsulated in a "cubist" source image:</p><p>Warp Scene Texture to Single Image. The simplest example of the type of warped texture we require is a simple terrain map texture, with a pixel intensity I x; y assigned to each facet of the 2D scalar field z = f x; y defining the terrain (see <ref type="figure">Figure 5</ref>). Objects with vertical or overhanging faces like buildings must be distorted so that the pixels originally assigned to the roof are shrunk to accommodate wall textures as shown in <ref type="figure">Figure 6</ref>. Scenes with non-trivial topology can be handled using one of several variations on <ref type="figure">Figure  3</ref> or our method contains only RGB color data at each pixel of the source image; one may store additional geometric information to achieve specific effects, but this is not required.</p><p>Project Camera Rays to Scene. To construct a key-frame data structure, pass the camera rays through each image pixel (or a sampled subset) and record intersection points with the true 3D scene geometry. This is closely related to the construction of a shadow map, and is illustrated in the first and second rows of <ref type="figure">Figure 2</ref>: when we trace rays from the eyepoint through each sampled pixel to the scene geometry, an oblique view shows the shower of rays skipping past occluded polygons to strike more distant objects. For certain applications, samples more heavily weighted around the center of attention might be preferred to regularly-spaced pixel samples.</p><p>Warp Ray Intersections from Scene to Source Image. The source image is a rectangular array resulting from a rubbersheet warping of all the "shrink-wrapped" scene texture to a (possibly torn) 2D image. The ray intersections form a warped grid in this space, with large gaps wherever an occlusion boundary occurred in the original projection. If there are no occlusions, there is still a non-trivial warping affect that becomes singular at the instant an occlusion boundary appears. Thus each point of the camera ray grid stores only the texture coordinates of the corresponding source image point. The third row of <ref type="figure">Figure 2</ref> shows the source-image mapping of the rays. <ref type="figure">Figure 2</ref> as a whole shows typical changes in the ray locations after small camera motions.</p><p>Fill in Sparse Space/Time Samples. Sparse spatial samples can be used to generate much richer detail by using the source image as a texture map and interpolating appropriately in space (see the fourth row of <ref type="figure">Figure 2</ref>). Sparse time samples of camera ray intersections with the scene can also be filled in to produce dense-time animations using interpolation methods; because of discontinuities in the interpolation when occlusions are encountered, this can be a challenging task. The time needed to reconstruct an image is independent of the resolution of the source image and is dependent on the resolution of the keyframes and the complexity of the temporal interpolation method.</p><p>Image-Based Spatial Compression and Color Manipulation. If minor distortions are acceptable for the purpose of an inexpensive visualization, one can replace the image derived from sampling all pixels at the ray intersections in the source image by an equivalent image defined by the texture coordinates at the ray intersections; using only a sparse sample of camera rays can give a high quality result with only a fraction of the data storage for rays. <ref type="figure" target="#fig_4">Figures 11(a)</ref> and 11(b) illustrate the use of this method; the sparse pixel samples in (a) give a clearly inferior image to that in (b) computed using a texture map based on the identical ray samples. Another bonus for data that requires transformations such as color mapping for visualization applications is that we can easily reassign or modulate the colors in the image, add contour lines, and so on; this visualization application is illustrated in <ref type="figure" target="#fig_2">Figure 7</ref>, which shows a B2 bomber wing's unwrapped density field as a source image and several visual transformations that can be performed when the image is rendered. Just as various advantages are achieved in other IBR techniques by adding information such as depth or range, we can get new capabilities by adjoining surface normal information to the base image on a per-pixel basis as in <ref type="figure" target="#fig_2">Figures 7(d)</ref> and <ref type="bibr">11(c)</ref>; this allows us to independently add approximate shading and specular highlight enhancement interactively while varying either the camera motion or the light direction. In principle, one might even simulate a mirror by following a set of ray bounces and treating them in IBR mode as well.</p><p>Animating Time Sequences. We are guaranteed to get geometrically accurate samples of the source image if we create one set of camera pixels per minimal camera displacement; however, this is not very useful, since if there is one ray per pixel, the amount of data stored is at least as great as if we had stored the corresponding stack of completely rendered images and played them back as an (uncompressed) MPEG movie. This is why we demonstrated in <ref type="figure" target="#fig_4">Figure 11</ref>(b) that we could convert from a dense set of camera rays to a sampled set (say every 4th image pixel) and get better results by using the more sparsely sampled set as texture coordinates. The result is that with 1/16 of the data, we get a good approximation to a full animation.</p><p>However, to get accurate images, we had to store camera pixel arrays for every minimal time step. What happens when we also reduce the sampling rate in time and interpolate temporally to fill in the intermediate steps?</p><p>The answer is very interesting from the point of view of camera-constraint technology: In <ref type="figure">Figure 8</ref>, we show five frames of a simple geometric image with a cone on a plane. Images (a) and (e) are keyframes, while the three intermediate images (b), (c), (d) are interpolations. Rotating the camera around the cone gives quite reasonable intermediate images, even though, in principle, this should be a very hard case due to the infinite aspect graph of a conical object. Contrast this with <ref type="figure">Figure  9</ref>, which also has three interpolated frames in the middle of two keyframes; the path now is a rigid camera translation, and the intermediate frames are badly distorted. The lesson is that camera motion constraints can be chosen to optimize the effectiveness of simple interpolations.</p><p>There is also a converse approach to optimizing the appearance of an interpolated time sequence. Recall first that we have made no compelling argument for any unique way to warp an image so that, cubist-style, all viewed pixels appear in the source image; there are many ways. We may thus select a camera path constraint and a mapping to the source image pixels, and then attempt to find an optimal rewarping of the source image to provide better performance. In the next section, we give some of the details of how to begin such an analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis: Deducing Acceptable Approximations</head><p>Our guiding principle is to use low-storage, singular interpolation methods to transform as accurately as possible among constrained key-frames. This appears adequate to produce reasonable renderings of occluded scenes using single source images, and thus to achieve most of the desired features of IBR. In practice, the deduction of appropriate interpolations is difficult: to understand reliable results about the situation, we can study very closely spaced sequences of projected camera grids in a source image to see exactly how the interpolations among more widely-spaced camera key-frames should be carried out. We may then later attempt to adjust our heuristic interpolations to approach more closely the veridical image data.</p><p>To illustrate our application of this procedure, we begin by showing in <ref type="figure">Figure 12</ref>(a) four time frames of a typical scene with a coneshaped object, where a column of five camera rays has been selected for study. <ref type="figure">Figure 12(b)</ref> shows the locations of these pixels as a function of time in the flattened source image. <ref type="figure">Figure 12(c)</ref> shows the comparison in base-image-space between a cubic interpolation among pixels in image space (red) and the frame-accurate camera motion (blue). The blue paths simulate a "perfect" time interpolation among a set of sparse keyframes. Using a cubic interpolation produces a badly distorted animation unless the camera motion is constrained so that the interpolations match, or the scene is so featureless that the image-space interpolations cannot differ. In general, one has massive discrepancies such as those shown in <ref type="figure">Figure 12</ref>(c). Nevertheless, this fact itself can now be used to guide further development of this work; the next step is to find families of interpolations, possibly combined with rewarping the source image, that minimize the difference between the bare spline interpolation and the veridical path.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Adjustment of Heuristics among Scene Types</head><p>The complete determination of an appropriate interpolation for sparse time frames is in general an unsolvable problem. However, with the philosophy of applying domain-dependent constraints on the camera motion to scenes with various characteristics, we can list a variety of cases and speculate on how to handle them appropriately: These families fall into the following categories: Match image symmetry. If the image, for example, has vertical columns in the image foreground, one can move the camera along vertical paths and have negligible evidence of new data appearing behind the columns; there is one simple family of singularities at the column edges, and the interpolation can directly accommodate that using prestored edge data.</p><p>Pyramid: Finite Aspect Graph Occlusions. It is relatively straightforward to handle pyramid-like objects, whose full texture map can be captured in a single aerial or top-down image, and whose aspect graphs are finite.</p><p>Cones: Infinite Aspect Graph Objects. When one extends the allowed objects to cones, one has an infinite aspect graph, so that the occlusions must continually roll across to accommodate new views.</p><p>Terrain with Occlusions. This is similar to the cones problem, but contains uncontrollable numbers of extra occlusions.</p><p>Buildings. No more than three of five visible building sides can be viewed at once by a real camera; with appropriate warping into a frustum, the building texture can be made easily into a warped image that is visible to cameras on all sides.</p><p>Arches and Loops. These require topological tearing and unfolding of the image in nontrivial ways (see <ref type="figure" target="#fig_0">Figures 3 and  4</ref>). The images correspond very closely to the cubist Picasso images in the introduction, and the interpolation must be adjusted to jump over the required slits in the image to avoid unacceptable effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have studied an approach to image-based rendering in which we assign photometric properties to each pixel of an "unwrapped," possibly torn, flat texture (the source image) that represents every pixel viewable from every viewpoint in the camera navigation domain. The scene geometry is then embodied in the process of casting camera rays into the 3D scene followed by the warping of those intersection points into the source image. Depending upon our knowledge of the space interpolation and time interpolation characteristics of a particular domain, we can achieve animations with free camera motion but much lower storage requirements than a full animation sequence of images; the use of a single source image also confers many additional advantages since no rerendering is required to incorporate color transformations and lighting effects. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Representations of the torus, a surface of genus one. (a) This abstract representation with opposite edges identified as marked precisely embodies our proposal to represent photometry independently from geometry. (b) The corresponding geometric object that the camera rays would be traced to is this toroidal surface, which results from attaching the edges together as marked. Example of a possible deformation of an arch into the global source image. (a) Sketch of a tubular arch attached to background plane. (b) Cutting a slit all the way around the inner hole, separating it slightly to leave two exposed edges, each shaped like a half disc. (c) Flattening the now strip-like arch down into the image plane, leaving two discontinuous cuts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .Figure 5 :Figure 6 :</head><label>456</label><figDesc>Note that the basic data model required by Terrain models are perfectly adapted to our method: (a) Viewed obliquely, a non-pathological terrain image has both texture and topography described by some single-valued elevation function z = f x; y. (b) Viewed from an appropriate point (possibly of infinite altitude), we see a single 2D texture map that can be used to texture all the polygons of the terrain. Demonstration of scene warping for polyhedral objects such as conventional buildings. (a) Oblique view of normal scene geometry. (b) The first stage of warping for a building is to taper the top to form a frustum, thus exposing the vertical walls to the view of a distant aerial camera. (c) Flattening the deformed scene into a plane preserves all the viewable texture information. (d) The 2D source image contains all viewable pixels in the 3D scene, but has lost geometric accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 :</head><label>7</label><figDesc>(a) Pressure density on a B2 bomber wing unwrapped to give a flat source image. (b) A useful color map of this density data (this is not modern art). (c) Unlit rendering of the wing with color-map texels indexed by a 6464 camera keyframe. (d) Lit rendering of the wing, where the stored density data has been augmented by the local surface normals. (Data from IRIS Explorer distribution.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fixed</head><label></label><figDesc>focal point. Traditional IBR works perfectly if you do not move the camera's focal point, even if there are occlusions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 11 :</head><label>11</label><figDesc>Future work would include further studies of the nature of singular image-space ray interpolations, possibly accompanied by complementary rewarping of the source image.Figure 10: (a) Standard 3D rendering of building model with camera rays marked. (b) Oblique view of rays. (c) Warp of camera rays to an 640640 flattened source image. (Owen Hall model courtesy of Indiana University Architect's Office.) (a) Scene reconstruction from 6464 sampled image points. (b) Scene reconstruction using the same 6464 ray intersections as texture coordinates in the source image. (c) Tailored visualization achieved by altering color and lighting characteristics of source image. (a) (b) (c) Figure 12: Examining interpolation errors. (a) Four time keyframes of a five-pixel column of the camera ray array. (b) Locations of the keyframe pixels in the source image. (c) Trace of veridical pixel paths (blue) and approximate cubic interpolation (red) between keyframes.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was made possible in part by NSF infrastructure grant CDA 93-03189 and the support of the Indiana University Advanced Information Technology Laboratory.</p><p>Note added: since this paper was written, two papers have appeared that utilize related but not identical concepts that may also be of interest to the reader: the multiple-center-of-projection approach <ref type="bibr" target="#b10">[12]</ref>, and multiple viewpoint rendering <ref type="bibr" target="#b3">[5]</ref>.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quicktime VR -an image-based approach to virtual environment navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Shenchang Eric</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 95 Conference Proceedings, Annual Conference Series</title>
		<editor>Robert Cook</editor>
		<meeting><address><addrLine>Los Angeles, California</addrLine></address></meeting>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="1995-08" />
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">View interpolation for image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Shenchang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (SIGGRAPH &apos;93 Proceedings)</title>
		<editor>James T. Kajiya</editor>
		<imprint>
			<date type="published" when="1993-08" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="279" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The lumigraph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radek</forename><surname>Gortler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Grzeszczuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">F</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 96 Conference Proceedings, Annual Conference Series</title>
		<editor>Holly Rushmeier, editor</editor>
		<meeting><address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="1996-08" />
			<biblScope unit="page" from="4" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multiple viewpoint rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 98 Conference Proceedings, Annual Conference Series. ACM SIGGRAPH</title>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Constrained 3D navigation with 2D controllers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wernert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Visualization &apos;97</title>
		<meeting>Visualization &apos;97</meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="175" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generation of widerange virtual spaces using photographic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Endo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of VRAIS &apos;98</title>
		<meeting>VRAIS &apos;98</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tour into the picture: Using a spidery mesh interface to make animation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youichi</forename><surname>Horry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Ichi Anjyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoshi</forename><surname>Arai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 97 Conference Proceedings, Annual Conference Series</title>
		<editor>Turner Whitted</editor>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="1997-08" />
			<biblScope unit="page" from="225" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Light field rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Levoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 96 Conference Proceedings, Annual Conference Series</title>
		<editor>Holly Rushmeier</editor>
		<meeting><address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="1996-08" />
			<biblScope unit="page" from="4" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Post-rendering 3d warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Macmillan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 1997 Symposium on Interactive 3D Graphics</title>
		<meeting>1997 Symposium on Interactive 3D Graphics</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="7" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Plenoptic modeling: An image-based rendering system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><surname>Mcmillan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GRAPH 95 Conference Proceedings, Annual Conference Series</title>
		<editor>Robert Cook</editor>
		<meeting><address><addrLine>Los Angeles, California</addrLine></address></meeting>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="1995-08" />
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
	<note>August 1995</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multiple-center-of-projection images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rademacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 98 Conference Proceedings, Annual Conference Series. ACM SIGGRAPH</title>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">View morphing: Synthesizing 3D metamorphoses using image transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 96 Conference Proceedings, Annual Conference Series</title>
		<editor>Holly Rushmeier</editor>
		<meeting><address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="1996-08" />
			<biblScope unit="page" from="4" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Time critical lumigraph rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Sloan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gortler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 1997 Symposium on Interactive 3D Graphics</title>
		<meeting>1997 Symposium on Interactive 3D Graphics</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="17" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Creating full view panoramic mosaics and environment maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 97 Conference Proceedings, Annual Conference Series</title>
		<editor>Turner Whitted</editor>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="1997-08" />
			<biblScope unit="page" from="251" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multiperspective panoramas for cel animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">N</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">F</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><forename type="middle">E</forename><surname>Thayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 97 Conference Proceedings, Annual Conference Series</title>
		<editor>Turner Whitted</editor>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="1997-08" />
			<biblScope unit="page" from="243" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Constructing 3D natural scene from video sequences with vibrated motions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of VRAIS &apos;98</title>
		<meeting>VRAIS &apos;98</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
