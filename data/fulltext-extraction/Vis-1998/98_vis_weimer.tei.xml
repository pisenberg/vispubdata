<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Co-Triangulation of Large Data Sets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Weimer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rice University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Warren</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rice University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Troutner</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rice University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendell</forename><surname>Wiggins</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rice University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Shrout</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rice University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Western</forename><surname>Geophysical</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rice University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Co-Triangulation of Large Data Sets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Delaunay triangulation</term>
					<term>scattered data</term>
					<term>multidimensional approximation</term>
					<term>higher-dimensional approximation</term>
					<term>computational geometry</term>
					<term>data-structures</term>
				</keywords>
			</textClass>
			<abstract>
				<p>This paper presents an efficient algorithm for the reconstruction of a multivariate function from multiple sets of scattered data. Given N sets of scattered data representing N distinct dependent variables that have been sampled independently over a common domain and N error tolerance values the algorithm constructs a triangulation of the domain of the data and associates multivariate values with the vertices of the triangulation. The resulting linear interpolation of these multivariate values yields a multivariate function, called a co-triangulation, that represents all of the dependent data up to the given error tolerance. A simple iterative algorithm for the construction of a cotriangulation from any number of data sets is presented and analyzed. The main contribution of this paper lies in the description of a highly efficient framework for the realization of this approximation algorithm. While the asymptotic time complexity of the algorithm certainly remains within the theoretical bounds, we demonstrate that it is possible to achieve running times that depend only linearly on the number of data even for very large problems with above two million samples. This efficient realization of the algorithm uses adapted dynamic data-structures and careful caching in an integrated framework.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION AND MOTIVATION</head><p>Multidimensional approximation problems arise in many application areas. For example, in medical imaging one commonly wants to represent an iso-density surface of an MRI scan as a triangulated surface; the vertices of the triangulation have additional properties such as the density gradient associated with them. In climate modeling one has to represent quantities such as air temperature, air pressure, humidity and so on varying over the earth as the domain. Similarly, in 3-D depth imaging and geophysical exploration one has to model geological fault or horizon surfaces in the ground. Functions such as depth, velocity of sound, density and porosity vary over these surfaces.</p><p>This paper presents a method for constructing one triangulation of the domain that can be used to represent any number of dependent functions accurately up to a preset error tolerance. The input of the algorithm is a sequence of N sets of scattered data with as- The outcome of the algorithm is a Delaunay triangulation of the domain <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21]</ref>. Every vertex of the triangulation is associated with a N-dimensional multivariate value and the resulting linear interpolation of these multivariate values yields a multivariate linear function that contains all of the dependent data within the preset error tolerance. We call this methodology of constructing one triangulation of the domain to accurately represent many sets of dependent data over the same domain a co-triangulation of the data. a.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>b.</head><p>c. A common way of solving such a multivariate approximation problem is to designate one of the dependent variables as more important than the others. Then a triangulation that approximates this designated variable is constructed. In a post-processing step additional attributes are associated with the knots of this triangulation in order to reproduce all of the dependent variables. However, this approach has a fundamental problem: the resolution of the triangulation based on only one variable is rarely fine enough to represent all of the dependent functions well. In particular, it is possible that some of the dependent properties vary greatly in an area where the designated property is mostly flat. In such areas the triangulation only consists of large simplices. The approximation of the remaining dependent functions will therefore be poor.</p><p>A simple example of the problem is given in figure 1. Three sets of dependent data representing the red, green and blue color components of the image in (a) have been sampled randomly. On the one hand, part (b) depicts a triangulation of the domain that has been constructed to approximate all of the samples for the blue color component within 10% error tolerance. The resulting color figure would clearly show discoloration and blurring. On the other hand, part (c) depicts the domain of the co-triangulation of the three data sets. In this example the co-triangulation is a two-dimensional function in five-dimensional space that contains the three color components as simple projections along three of the coordinate axis. Note that the much denser triangulation in (c) is necessary for the satisfactory approximation of all three color channels.  The algorithm presented here is based on the idea of carrying out the approximation in higher-dimensions. Given N data sets representing N different D-dimensional dependent functions, the algorithm constructs one D-dimensional piecewise linear object in D + N-dimensional space such that N orthogonal projections of this object are good approximations of the N data sets. <ref type="figure" target="#fig_3">Figure   2</ref> gives an example of a co-triangulation for two one-dimensional functions in three-dimensional space.</p><p>Co-triangulations of several functions are very important for the modeling of complex natural phenomena. They can be used to represent any number of properties in one single domain mesh and are, for example, applicable in the following fields: medical modeling of tissue with associated pathological attributes, representation of geological structures with many different physical properties, representation of climate data involving parameters such as air pressure and air temperature, mesh generation for the visualization and analysis of higherdimensional phenomena.</p><p>Our presentation starts out with the description of a simple iterative algorithm for co-triangulation of multiple data sets. The presentation of the basic algorithm is followed by a description of algorithmic modifications and extensions that are used to make the algorithm time and space efficient. In particular, we will demonstrate that it is possible to achieve a running time of the algorithm that is only linear in the number of input data for data sets with up to two million samples. The algorithm certainly can be asymptotically no better than the theoretical minimum for this problem, On log n where n is the number of input samples, but due to small constants in the framework presented here, the algorithm exhibits linear time complexity even for very large sets of input. Towards the end of our exposition we will present some example applications of our algorithm to real world problems. The paper will finish up with a brief summary and a discussion of the possible directions for future research.</p><p>The main focus of this paper will be on devising a simple and fast algorithm for constructing approximating co-triangulations for several very large sets of scattered data. The basic algorithm is simple and we will demonstrate that this simple method can be made very efficient by the use of adapted dynamic data structures and careful caching of previously computed information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>A fair amount of work has been done on constructing approximations for sets of scattered data. A very good overview of research in the field can be found in <ref type="bibr" target="#b17">[18]</ref>. These algorithms can be categorized into approaches using smooth basis functions, such as B-splines, thin plate splines, or multi-quadrics, e.g. <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b10">11]</ref>, methods which construct triangulations to approximate the given samples, e.g. <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b15">16]</ref>, frameworks which first triangulate the data densely and then decimate the triangulation to get a more concise representation, e.g. <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>Turk <ref type="bibr" target="#b22">[23]</ref> introduced a method to derive several levels of detail from a polygonal model description by inserting new vertices into the original model and then removing the old vertices in a topology preserving manner.</p><p>Schroeder et al. <ref type="bibr" target="#b21">[22]</ref> developed an algorithm to decimate the number of vertices in a triangle mesh. After vertices have been deleted from the original mesh the resulting holes are patched in a local re-triangulation process.</p><p>An algorithm which produces a triangulation to approximate a given set of points in 3D is described by Hoppe, DeRose et al. <ref type="bibr" target="#b12">[13]</ref>.</p><p>They define an error-function f : D ! IR where D is an estimation of the shape of the objects and f is an approximation of the geometric distance of the approximation f from the real object. The approximation of the object can then be constructed form the iso-surface f = 0 . In a second step a discrete energy minimization which captures tight geometric fit and sparse representation can then be used to locally optimize the resulting triangulation <ref type="bibr" target="#b13">[14]</ref>.</p><p>Margaliot and Gotsman <ref type="bibr" target="#b15">[16]</ref> suggest a similar approach. They approximate a smooth surface from scattered samples by constructing the Delaunay triangulation and then optimizing it to achieve better precision.</p><p>Hamann suggests in <ref type="bibr" target="#b9">[10]</ref> a method which iteratively removes triangles in nearly planar surface regions of an object. The rationale behind his approach is that flat regions of an object can be represented well by large triangles. A curvature estimate of the triangulated object is suggested to identify such regions.</p><p>Klein and Liebich <ref type="bibr" target="#b14">[15]</ref> use the Hausdorff distance between the original mesh and the resulting simplification to pick vertices of a polygonal object for elimination and re-triangulate locally.</p><p>An algorithm to create a sequence of continuous resolution representations of a complex polygonal object by iteratively collapsing edges of the finer mesh into a single vertex has been devised by Hoppe <ref type="bibr" target="#b11">[12]</ref>.</p><p>A very interesting approach for computing the overall shape of an object has been introduced by Edelsbrunner and Mücke <ref type="bibr" target="#b5">[6]</ref>. They define the three-dimensional -shape for some given set of points and precision as roughly the shape remaining in space when a ball of radius is rolled over the cloud of points such that the ball only touches but never enters the data points.</p><p>Lastly, we believe that this problem is also related to texture mapping, where one can represent a high resolution function (i.e. the texture) on coarse geometry using bilinear interpolation <ref type="bibr" target="#b0">[1]</ref>. Unfortunately, this approach does not generalize easily to scattered samples because texture images are inherently gridded.</p><p>All of the algorithms mentioned above were designed to handle small or moderately sized inputs of scattered data. We are focusing mainly on handling data sets with millions of points and can not afford to run expensive optimization methods, such as least squares fitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ALGORITHM FOR APPROXIMATING CO-TRIANGULATION</head><p>This section first introduces the terminology used throughout our presentation. There follows an outline of the basic approximation algorithm at a fairly high level. Once the reader is familiar with the big picture of the approximation algorithm, we are going to discuss efficient solutions for the crucial sub-problems involved in the basic approximation algorithm. Section 4 will provide an integration of these efficient solutions for the parts of the algorithm into one efficient framework for scattered data approximation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Terminology</head><p>Throughout this paper we will make use of some helpful terminology. We will refer to the input data sets as S1, S2, , SN. These N data sets are assumed to represent N distinct dependent functions sampled over a common D-dimensional domain. The data is assumed to be functional, i.e. no two samples in the same set Si agree in the first D coordinates. We refer to the elements of the sets Si as samples and we say a sample is covered by a simplex in the triangulation if the sample lies inside the simplex in the projection into the parameter domain. Note that by definition every sample is covered by exactly one simplex. An example from IR 3 is given in figure 3.</p><p>The input to our co-triangulation algorithm is a sequence Si,  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Basic Algorithm</head><p>This section provides a high level overview of the algorithm for approximative co-triangulation of several scattered data sets which share a common domain. The remainder of the section will then present solutions for the details involved in the algorithm.</p><p>Our algorithm finds the desired approximating co-triangulation for the given N data sets Si, i = 1 ; ; N in an iterative fashion, considering one data set at a time. The algorithm starts out with a simple initial triangulation T0 with knots in the domain IR D of the data. The particular choices for T0 are discussed further in section 3.3.</p><p>In the first stage of the algorithm an approximation for S1 is constructed. Initially, one attribute has to be associated with each of the knots in the initial triangulation T0. These attributes are chosen to produce a reasonable first guess for the approximation of the first data set, S1. Algorithms for the determination of these initial attribute values are presented in more detail in section 3.4. Now the quality of the current approximation is assessed by determining the maximal deviation between the current approximation and the samples in S1. This deviation is measured by evaluating the absolute value of the difference in the last coordinate between the interpolation of the attributes associated with the knots of the triangulation and the samples. New knots are inserted into the triangulation using a greedy strategy until the desired tolerance criterion 1 for S1 is satisfied. During this refinement of T0 the new knots are chosen directly from samples in S1, i.e. the coordinates in the domain are taken to be those of a sample and the attribute associated with the new knot is the data-value of the sample itself. Call the resulting triangulation T1. T1 is an approximating triangulation for the data set S1 that guarantees all samples lie within 1 of the interpolant of the first set of attributes associated with the knots.</p><p>An example of this first stage of the approximation algorithm is shown in figure 4 in two dimensions. The information in S1 are samples from a curve in 2D, i.e. a one-dimensional object. At any step, new knots are inserted corresponding to the samples with maximal deviation from the current approximation until all data lies within the desired tolerance (the shaded region). Knots are chosen on the abscissa corresponding to the position of the samples with maximal deviation; the attributes associated with the knots are the values of these samples themselves.</p><p>At this point the approximation T1 of S1 is complete. The second data set S2 is considered next.</p><p>The triangulation T1 is used as the initial triangulation for S2 and the approximation starts over again: The knots of T1 are associated with new attributes to produce a first guess for the approximation of S2. Then the triangulation is refined by iteratively inserting samples from S2 into T1. Again the coordinates of the knot in the domain are picked from the sample with maximal deviation from the current approximation and the associated new attribute is taken from the sample itself. However, the attributes related to the approximation of the previous data set, S1, are determined from the linear interpolation of the attributes for S1 inside the simplex in T1 which contains the new knot.</p><p>This iterative refinement continues until the desired tolerance criterion 2 has been satisfied for S2. The resulting triangulation is called T2. The algorithm starts over with the samples in S3 and so on.</p><p>This iterative approximation commences until all N data sets have been incorporated. The result is a triangulation of the domain of the data where each knot has N associated attributes that can be used to approximate all of the data up to the preset error tolerances. The algorithm is fairly straightforward and intuitive. It has several properties that we shall point out:</p><p>The data sets are considered one at a time. This means that at any time memory for at most one of these sets is used. Therefore, the total memory consumption is bound by the size of the largest set of samples, not by the sum of the sizes of all samples.</p><p>The algorithm is guaranteed to converge. In the worst case all of the samples have to be inserted. In reality, this case occurs only if all of the tolerances were set to zero, a case that makes no sense for real world problems because sampled data is inherently imprecise.</p><p>Inserting one sample at a time into a Delaunay triangulation entails a highly local change of the approximation. This allows for very efficient hierarchical methods to determine a data-sample with maximal deviation from the approximation to be used in the next refinement step. The algorithms used will be discussed further in the following sections. In the end, they lead to a highly efficient approximation method which behaves mostly linearly for problems with two million samples or less.</p><p>The remainder of section 3 discusses in more detail the possible choices for picking the initial triangulation of the domain, T0, for determining the initial attributes associated with an existing triangulation, and for finding a sample with maximal deviation from the approximation. Section 4 presents an efficient framework for integration of these steps into a complete approximation algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Choosing an Initial Triangulation</head><p>Our approximation method starts out by finding a simple initial triangulation T0, which is then iteratively refined. Three different methods can be used to choose this initial triangulation:</p><p>The bounding box in the parameter domain which contains the projection of all samples can be split into two initial triangles.</p><p>The convex hull of the projection of all samples into the parameter domain can be triangulated.</p><p>The Delaunay triangulation of a user-defined set of initial knots in the parameter domain can be used.</p><p>The first approach, based on the bounding box of all samples, is appealing because it is extremely simple and can be determined in minimal time, involving as it does, only one scan over all the samples. The biggest disadvantage of this approach is that it introduces knots into the triangulation that are not related to any sample unless the corners of the bounding box are themselves samples. Furthermore, the resulting triangulation in the parameter space usually covers an area that is significantly larger than the real domain of the data.</p><p>The second method, using the convex hull of the samples, is appealing because the resulting triangulation exactly covers the area of the domain involved in the samples. The problem with this approach is that the convex hull of the projection of all samples into the parameter space is asymptotically as expensive to compute as the triangulation of the projection of all the samples themselves. Thus, this approach is unacceptable for large problems.</p><p>The method based on a user-defined set of knots constitutes a compromise of the two previous approaches. One often knows from the source of the data which part of the domain is really covered by the data. It is in most cases easy to delineate the real domain of the data closely with a few points in the parameter space. The resulting approximation will only include small areas of the parameter space that do not contain any samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Associating New Attributes with the Knots of an Existing Triangulation</head><p>Whenever the approximation algorithm has achieved the desired tolerance bound for one of the data sets, all of the samples from that set are discarded and the approximation starts over with the next set. At this point the knots of the existing triangulation are associated with new attributes for the next data set in order to construct a reasonable initial approximation for that set.</p><p>An initial approximation can be achieved by finding the k closest samples for any of the knots of the triangulation for some given k 1 and associating the average of the data values of these k samples with the respective knot of the triangulation. The exact data value has to be used if the set contains a sample lying exactly on the knot. Finding these attributes for the knots of an existing triangulation involves solving the k nearest neighbor problem, a process that can be carried out in time proportional to OlogjSjj + k, with Ok 2 jSjj logjSj j preprocessing time to construct the k-th order Voronoi diagram <ref type="bibr" target="#b20">[21]</ref>. For a simpler implementation, space partitioning methods other than the Voronoi diagram can be used. For example a generalized octree approach achieves very good performance in our implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Finding a Sample with Maximal Deviation</head><p>During the refinement of the approximation for one particular dataset, the algorithm proceeds until the desired tolerance has been achieved. In every step a sample with maximal deviation from the approximation has to be determined. A corresponding new knot with associated attributes is inserted into the triangulation. Finding one such sample with maximal deviation in a possibly large set of scattered data is crucial for the overall performance of the algorithm. The method presented here makes use of careful caching of previously computed information and of selective, minimal updates of dynamic data-structures.</p><p>The most naive approach for finding the sample with maximal deviation from the approximation is to simply evaluate the error for every sample in the current data set and then to find a maximal value among them. This procedure requires a time proportional to the number of samples in the set for every single knot in the triangulation.</p><p>This simple approach can be vastly outperformed by exploiting the observation that adding a new vertex to the approximation only entails a local change of the approximation. As pointed out by Bowyer <ref type="bibr" target="#b1">[2]</ref>, adding a knot into a Delaunay triangulation requires removing all simplices which contain the new knot in their circumcircle and re-triangulating the exposed "star-shaped" region (see example in <ref type="figure" target="#fig_7">figure 5</ref>). This local re-triangulation limits the extent of change in the quality of the approximation to samples covered by this re-triangulated area. A heap <ref type="bibr" target="#b6">[7]</ref> containing all samples in the current data set can be used to keep track of these changes from iteration to iteration. In the heap the samples are ordered by their deviation from the current approximation. Upon insertion of a new knot, the heap has to be updated for those samples that are covered by the deleted simplices.</p><p>On average, this limited update will lead to a considerable speedup as typically the number of points covered by a single triangle is small and the number of triangles affected by inserting one knot into the triangulation is small. Let ni denote the number of points which were covered by triangles erased in iteration i of the approximation of Sj. Then, this approach has time-complexity Oni log jSjj for the update of the heap compared to OjSjj in the naive approach.</p><p>Further improvement can be achieved. Observe that the samplê p 2 Sj with maximal deviation from the approximation is covered by some trianglet in the current triangulation Ti. Likewise, the samplep has maximal deviation compared to all other samples covered by the same trianglet -see <ref type="figure" target="#fig_8">figure 6</ref>. Therefore it is sufficient to consider only one sample per simplex in the heap. For every triangle t in the approximation Ti only one sample with maximal error covered by the triangle t has to be stored in the heap. When updating the triangulation Ti by inserting a new knot, those heap entries coming from deleted simplices have to be deleted. One single entry for every newly created simplex in the re-triangulation of the star-shape is added to the heap. This improvement reduces the size of the heap to the number of simplices in triangulation Ti. Let np del denote the set of triangles which are removed from Ti upon insertion of samplep, and let np ins denote the number of triangles which are created in step i. Deletion and insertion into a heap of size k can be achieved in time Olog k. Hence, the time-complexity for one update of the heap is Onp del log jTij + np ins log jTi+1j which is generally significantly smaller than both OjSj and Oni log jSj.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EFFICIENT ORGANIZATION OF THE COMPLETE ALGORITHM</head><p>The previous section provided an outline of the approximation algorithm and of the methods used for finding an initial triangulation for the determination of a sample with maximal deviation from the current approximation and for the re-triangulation of the approximation once such a sample has been found. This section discusses the integration of these parts of the algorithm into one complete, efficient framework for scattered data approximation. In particular the cooperation of the local re-triangulation and determination of the algorithm for finding a sample with maximal error is presented in detail. In the end this integrated algorithm will be suitable for the approximation of very large data sets in acceptable running time and space. Once a sample with maximal deviation from the current approximation has been determined in the algorithm introduced in section 3.2, the triangulation of the parameter space has to be updated to incorporate the corresponding knot.</p><p>Updating can be achieved by deleting all simplices in the triangulation that contain the new knot in their circumcircle. This removal of simplices exposes a region in the triangulation with knots on its boundary that are all visible from the new knot, i.e. a straight line from the new knot to any vertex on the boundary of the region does not intersect any of the remaining edges of the triangulation. In the literature this area is called a star-shaped region <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>The simplices that have to be erased from the current triangulation can be found in an outward search starting at the simplex covering the new knot. During this search only simplices that are vertex-adjacent to some other simplex which has the new knot in its circumcircle have to be considered <ref type="bibr" target="#b1">[2]</ref>.</p><p>To facilitate the start of the outward search, a reference to the covering simplex is stored with every sample in the current data set. Upon insertion of a new knot for the samplep into the triangulation, the triangle tp which coversp can be found in constant time using this reference. Trivially, tp has the projection ofp inside its circumcircle. Hence, triangle tp has to be erased from the triangulation.</p><p>Starting at tp a search outward in the triangulation for simplices with the projection ofp in their circumcircle is performed. A stack is used to keep track of those simplices that have to be considered in the search. After finding the set of triangles that have to be erased, the Delaunay triangulation can be modified locally. The triangles withp in the circumcircle are deleted and edges from the knotp to any knot on the boundary of the star-shape are inserted.</p><p>The heap that is used to determine the sample with maximal deviation has to be modified at this time. All entries corresponding to deleted simplices have to be erased from the heap. They will be replaced by new entries created to a re-tiling of the star-shape.</p><p>To update the triangulation as well as the references to covering simplices which are stored with every sample, the knots on the boundary of the star-shape are sorted radially with respect to the new knot. In a radial sweep new faces are added to the triangulation to re-triangulate the star-shape and at the same time references corresponding to the new covering are associated with the samples covered by the new simplex.</p><p>After the re-tiling of the star-shaped region and re-assignment of all exposed samples to a new covering simplex, one single pass over the samples assigned to every newly added simplex is necessary to find one sample with maximal deviation from the updated approximation for each of these new simplices. This sample together with its deviation is inserted into the heap used to find a sample with maximal error. After the update of the heap is completed, the sample with maximal deviation is stored at the root of the heap and can be found easily to continue the iterative refinement of the approximation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>This section presents two real world applications of our approximation algorithm. In the first example, drawn from the approximation of climate data, we demonstrate that it is possible to accurately and efficiently model a significant number of climate parameters over a common domain using one concise triangulation of the parameter space. The second application comes from exploration geophysics, where we show that co-triangulations of several scattered data sets can be used to precisely model multiple interesting properties of geological structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Climate Data Approximation</head><p>We applied our algorithm to a climate model for the tropical pacific ocean. The data is courtesy of NOAA <ref type="bibr" target="#b18">[19]</ref> and consisted of five different data sets: Each set contained a regular grid of 60 160 samples in one degree resolution ranging from 29.5N to 29.5S and 120.5E to 80.5W that represented average air temperature, air pressure, relative humidity, sea surface temperature and vapor pressure measurements for the month January of the years 1945-1989 in this region.</p><p>Color <ref type="figure" target="#fig_9">figure 8</ref> shows the result of a traditional approximation approach: We first triangulated the air temperature up to a tolerance of 0.25 degrees Centigrade and used the resulting triangulation to represent the four remaining properties. The results have been color mapped to display the values of the approximations. One can clearly see artifacts in the figures (b) thru (e) introduced by insufficient resolution of the triangulation of the domain based on information about air temperature.</p><p>On the other hand, color <ref type="figure" target="#fig_10">figure 9</ref> shows the result of cotriangulating the domain using our algorithm for all of the five input data sets. The air temperature is again approximated to 0.25 degrees tolerance but now the triangulation also honors the air pressure to a precision of 0.25 millibar, the relative humidity to 0.5 percent, sea surface temperature to 0.25 degree Centigrade and the vapor pressure to 0.5 millibar precision. The figures are again color mapped and obviously represent all of the properties to high precision. In particular artifacts from insufficient resolution of the triangulation are no longer visible. This approximation was constructed in 84 seconds on a workstation with one 195MHz R10000 processor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Approximation of Seismic Data</head><p>The need for co-triangulation in seismic data processing arises as different model representations are used. Geological horizons are usually auto-tracked during interpretation or gridded from another modeling system. The resulting sets of scattered data can be very large. Attribute data such as velocity and density may vary along the surfaces.</p><p>If the attribute data were triangulated by itself, a few, large triangles would be adequate to represent the data. However, this triangulation would not be adequate to represent the horizon. The co-triangulation procedure finds an equivalent triangular mesh efficiently. The mesh is guaranteed to represent all the input data within user-specified tolerances.</p><p>Color figure 10 depicts an example application of a cotriangulation in geophysical modeling. Shown are 23 horizon surfaces which have been co-triangulated with samples of the sediment velocity above and below the respective surface. Embedded is a salt mass and the variation of the velocity field is shown color mapped on a cutting plane.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Performance Analysis</head><p>This section presents running time data of the algorithm for very large sets of scattered data. In particular we produced artificial data sets by picking random samples of two functions f2x; y = e ,1+x 2 ,1:2+y <ref type="bibr" target="#b1">2</ref> and f6x; y = 3 5 f2x; y + e ,,1+x 2 ,,1+y 2 in the domain ,2; 2 2 . The problems contained between 10000 and 2 million samples and have been approximated to 1% and 5% tolerance. The running times for the different problems on a machine with one 195MHz R10000 processor are shown in <ref type="figure">figure 7</ref>. Note that the curves are linear in the number of samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND FUTURE WORK</head><p>This paper presented a simple iterative algorithm for the cotriangulation of several sets of scattered data that share a common domain. Co-triangulations can be used to represent any number of data sets with a high accuracy.</p><p>The simple algorithm was then augmented using adapted dynamic data-structures which improve the running time of this method dramatically. The efficiency gain of the final algorithm is due to three key improvements:  <ref type="figure">Figure 7</ref>: Running time to approximate random samples from two different functions with 1% and 5% tolerance on a workstation with one 195MHz R10000 processor.</p><p>a covering relationship between samples in the input and simplices in the triangulation of the domain, hierarchical tournaments to assess the quality of the approximation between the samples covered by the same simplex and between all the simplices, minimal updates of the triangulation and of all information pertinent to the assessment of the quality of the approximation during a re-triangulation.</p><p>Experimental results show that with the improved algorithm it is possible to approximate even very large problems in reasonable time. While the asymptotic time complexity of the algorithm remains within the theoretical bounds, the examples show that the algorithm's running time depends mostly linearly on the number of samples for problems with up to two million samples.</p><p>The resulting triangulations are guaranteed to contain all of the samples from all of the given dependent data sets within predefined error tolerances, i.e. the L1-norm of the pointwise distance between the data sets and the approximation is guaranteed to be below the preset tolerance value. No guarantees regarding the optimality of the approximation can be made. In further experiments we augmented the iterative refinement algorithm such that it always picks a sample that optimizes L1-norm of the approximation. While the running time of the approximation procedure grows drastically due to the linear programming that is necessary to determine one optimal choice for the next sample, the overall rate of convergence of the algorithm did not improve significantly. Therefore, we believe that the use of expensive optimization techniques during the approximation is not warranted, especially for the approximation of very large problems.</p><p>Finally, the resulting triangulations depend on the order in which vertices are picked for insertion during the iterative refinement. However, the presence of certain sample points in the final triangulation can always be enforced explicitly.</p><p>In the future we plan to investigate the use of non-Delaunay triangulations, such as data-dependent triangulations <ref type="bibr" target="#b3">[4]</ref>, to improve the convergence properties of the approximations. However, because we are focusing on the fast approximation of very large data sets we anticipate that expensive optimization techniques have to be considered very carefully.</p><p>Finally, the time optimal algorithm for Delaunay triangulations is based on a divide and conquer approach <ref type="bibr" target="#b20">[21]</ref>. We would like to investigate the applicability of a divide and conquer strategy for the construction of an approximating co-triangulation of multiple data sets over a common domain. a.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>b.</head><p>c.</p><p>d. e. f.   : Seismic data approximations of the subsurface. Shown here is a salt mass from the Gulf of Mexico embedded in a multilayer sediment velocity field. The sediment velocity horizons were co-triangulated using picks from an interpreted water bottom surface and velocities along the surfaces. Two sets of velocity samples were used per surface such that the velocities can vary both above and below the surface. The salt mass shown in red was approximated from picks for a top of salt and base of salt interpretation. The sediment velocity model has been color mapped to a cutting plane.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>sociated tolerances. These sets have been sampled independently over a common domain in IR D and every set contains samples from a different D-dimensional function. No inherent structure of the data is assumed. In particular, it is possible that for some domain point p 2 IR D some of the data sets Si contain a sample at p while others do not. Thus, the information provided at the domain point Department of Computer Science, Rice University, 6100 South Main Street, Houston, TX 77005-1892, contact email: henrik@rice.edu y Western Geophysical, 10001 Richmond Avenue, Houston, TX 77252-2469, contact email: jane.troutner@waii.com p is incomplete in the sense that we do not have information about all of the dependent variables at p.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Simple approximation vs. co-triangulation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Given scattered data from two dependent functions f and g in one independent variable x (a and b), a one-dimensional linear approximation can be constructed in (2+1)-dimensional space (c). The original functions f and g are represented as the front-toback (d) and top-to-bottom (e) projections of the space curve in (c) respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>i = 1 ;</head><label>1</label><figDesc>; N of N sets of scattered data together with N associated tolerances i . By convention all of the input sets contain scattered data which has been sampled from N different D-dimensional objects. The domain of the data can be interpreted as a subset of IR D . The output of the algorithm is a piecewise linear representation of a D-dimensional object in N +D-dimensional space such that N projections of this objects represent the N input data sets up to the specified tolerances. In this way, the co-triangulation can be used to represent all of the given scattered data up to the preset tolerance criteria. In this paper the tolerance is assessed using the maximum of the absolute value of the coordinate-wise distance between the x1 x2 x3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>A sample is covered by a simplex if it lies inside the simplex in the projection in the parameter space.interpolation and the samples. Any other reasonable error metric can be used in complete analogy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Iterative refinement continues until the tolerance criterion is satisfied for all the samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Inserting a vertex into a Delaunay triangulation entails a local change.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>It is sufficient to keep track of one sample with maximal error per simplex in the approximation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Climate data approximation based on the triangulation of only one of the modeled parameters, air temperature (a and f), introduces artifacts and leads to poor accuracy for the remaining properties air pressure (b), humidity (c), sea surface temperature (d), and vapor pressure (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Climate data approximation using a co-triangulation of the domain (f) represents air temperature (a), air pressure (b), humidity (c), sea surface temperature (d), and vapor pressure (e) very accurately. In particular no artifacts from the underlying triangulation are visible in the figures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10</head><label>10</label><figDesc>Figure 10: Seismic data approximations of the subsurface. Shown here is a salt mass from the Gulf of Mexico embedded in a multilayer sediment velocity field. The sediment velocity horizons were co-triangulated using picks from an interpreted water bottom surface and velocities along the surfaces. Two sets of velocity samples were used per surface such that the velocities can vary both above and below the surface. The salt mass shown in red was approximated from picks for a top of salt and base of salt interpretation. The sediment velocity model has been color mapped to a cutting plane.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The data used in the evaluation of our algorithms is courtesy of the National Oceanic and Atmospheric Administration, the Rice University Development Department, Western Geophysical and the Society of Exploration Geophysicists. This work has been supported in part under National Science Foundation grant CCR-9500572 and Texas Advanced Technology Program grant 003604-010.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Texture and reflection in computer generated images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Blinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Newell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="542" to="547" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Computing dirichlet tesselations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bowyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Computer Journal</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="162" to="166" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bulletin de l&apos;Académie des Sciences de l&apos;URSS,VII Série</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Delaunay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Class des Sciences Mathématiques et Naturelles</title>
		<imprint>
			<biblScope unit="page" from="793" to="800" />
			<date type="published" when="1934" />
		</imprint>
	</monogr>
	<note>Sur la sphère vide</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Data-dependent triangulations for scattered data interpolation and finite element approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rippa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Numerical Mathematics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page">89</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Algorithms in Combinatorial Geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Edelsbrunner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Three-dimensional alpha shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Edelsbrunner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernst</forename><forename type="middle">P</forename><surname>Mücke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="72" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Algorithm 245: Treesort3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">W</forename><surname>Floyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">701</biblScope>
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Least squares surface approximation to scattered data using multiquadric functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">M</forename><surname>Nielson</surname></persName>
		</author>
		<idno>NPS-MA-93-0008</idno>
	</analytic>
	<monogr>
		<title level="j">Naval Postgraduate School</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Smooth interpolation of large sets of scattered data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Nielson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal for Numerical Methods in Engineering</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1691" to="1704" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Data reduction scheme for triangulated surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Hamann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Aided Geometric Design</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="197" to="214" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Triangular nurbs surface modeling of scattered data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gérard</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Visualization</title>
		<meeting>Visualization</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="295" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Progressive meshes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 96 Conference Proceedings, Annual Conference Series</title>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="1996-08" />
			<biblScope unit="page" from="99" to="108" />
		</imprint>
	</monogr>
	<note>Holly Rushmeier</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Surface reconstruction from unorganized points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Derose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duchamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Werner</forename><surname>Stuetzle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="71" to="78" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mesh optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Derose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duchamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Werner</forename><surname>Stuetzle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics</title>
		<imprint>
			<biblScope unit="page" from="19" to="26" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mesh reduction with error control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunther</forename><surname>Liebich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Straßer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Visualization &apos;96</title>
		<meeting>Visualization &apos;96</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Piecewise-linear surface approximation from noisy scattered samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Margaliot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Gotsman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Visualization</title>
		<meeting>Visualization</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="61" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A method for interpolating scattered data based upon a minimum norm network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nielson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Comp</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="253" to="271" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scattered data modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nielson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">UWM/COADS: Atlas of surface marine data 1994</title>
		<ptr target="http://ferret.wrc.noaa.gov/fbin/climateserver" />
		<imprint>
			<date type="published" when="1994" />
		</imprint>
		<respStmt>
			<orgName>National Oceanic and Atmospheric Administration ; U.S. Department of Commerce, National Oceanic and Atmospheric Administration, Pacific Marine Environmental Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fitting triangular b-splines to functional scattered data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Pfeifle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="23" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Computational Geometry -An Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Preparata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shamos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Decimation of triangle meshes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">William</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">A</forename><surname>Zarge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">E</forename><surname>Lorensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="65" to="70" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Re-tiling polygonal surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Turk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="55" to="64" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
