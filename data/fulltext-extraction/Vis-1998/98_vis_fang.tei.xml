<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image-Based Transfer Function Design for Data Exploration in Volume Visualization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiaofen</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="institution">Indiana University Purdue University Indianapolis</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Biddlecome</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="institution">Indiana University Purdue University Indianapolis</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihran</forename><surname>Tuceryan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="institution">Indiana University Purdue University Indianapolis</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Image-Based Transfer Function Design for Data Exploration in Volume Visualization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>volume visualization</term>
					<term>3D image processing</term>
					<term>transfer function</term>
					<term>volume rendering</term>
					<term>data exploration</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Transfer function design is an integrated component in volume visualization and data exploration. The common trial-and-error approach for transfer function searching is a very difficult and timeconsuming process. A goal-oriented and parameterized transfer function model is, therefore, crucial in guiding the transfer function searching process for better and more meaningful visualization results. This paper presents an image-based transfer function model that integrates 3D image processing tools into the volume visualization pipeline to facilitate the search for an image-based transfer function in volume data visualization and exploration. The model defines a transfer function as a sequence of 3D image processing procedures, and allows the users to adjust a set of qualitative and descriptive parameters to achieve their subjective visualization goals. 3D image enhancement and boundary detection tools, and their integration methods with volume visualization algorithms are described in this paper. The application of this approach for 3D microscopy data exploration and analysis is also discussed.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Volume visualization is a field in scientific visualization that is concerned with the abstraction, interpretation, rendering and manipulation of large volume datasets. It has been widely used in many scientific, engineering and biomedical applications. Two typical volume visualization techniques are volume rendering and surface rendering. Volume rendering algorithms <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b15">16]</ref> can directly display the volume information contained in the dataset through semi-transparent images, and thus allow the users to explore the internal structures of the image volume. One key component in this process is the transfer function that maps the volume's intensity values to color and opacity values for display. In surface rendering <ref type="bibr" target="#b9">[10]</ref>, the transfer function defines the thresholds with which iso-surfaces are extracted and rendered as surface objects. In both cases, the transfer function is used as both a filter that selects a subset of the volume information to be presented, and an information interpreter that determines how the selected information is to be presented.</p><p>The need for transfer function design and modeling comes from the dynamic and often subjective visualization goals and requirements in different applications. Users often need to interactively search and manipulate the transfer function in the visualization process to view different sets of sub-structures, surfaces and frequencies in desired ways. For applications such as medical CT image viewing, this may not be a difficult problem, since the CT intensities of most medical objects (e.g. bones and tissues) are generally known. Even so, many advanced applications still need to visualize the finer anatomic structures that cannot be easily obtained with trivial transfer functions. A more challenging situation is when the volume dataset contains objects that are highly complex with various levels of signal-to-noise ratios. Moreover, the structures embedded within the dataset can be partially or entirely unknown. One example is the visualization of microscopy image volumes for the study of cell biology, where many of the cellular structures, their optical/material properties, and the associated image signals are highly complex, noisy and often unknown <ref type="bibr" target="#b0">[1]</ref>. In such cases, an efficient and goal-oriented transfer function model is crucial in exploring the unfamiliar data environment for scientific discovery, data analysis, or medical diagnosis.</p><p>The most common approach in transfer function design is by trial-and-error. It allows the user to arbitrarily and repeatedly manipulate the coefficients of some mathematical representation of the transfer function to adjust the visualization outcome. Common forms of such mathematical representations are piecewise linear functions, polynomials and splines. In practice, however, such a trial-anderror approach is very difficult and time-consuming without prior knowledge about the optical and material properties of the underlying image volumes. This is mainly because the coefficients of the transfer function representation has little or no pre-defined qualitative meanings or visual relationships to the rendered images. As a result, there is very little guidance that can be followed in the transfer function searching process, and the visualization outcome, thus, largely depends on the user's experience and "luck". Since such a trial-and-error process needs to be done all over again for each new image volume or visualization objective, it puts an enormous burden to the application users. Furthermore, such "arbitrary" transfer function manipulation without proper constraints can easily lead to confusing, misleading and dubious visualization results.</p><p>Previous effort in improving the transfer function searching is rather limited. One interesting work is by He et al <ref type="bibr" target="#b5">[6]</ref>. It uses a stochastic search technique to generate many image samples based on an initial population of pre-defined transfer functions. Users are then allowed to select the proper sample images, based on visual examinations, to assist the filtering and evolution of the transfer function population. A similar approach has also been taken in the Design Galleries work by Marks et al <ref type="bibr" target="#b10">[11]</ref>. Although this approach provides some level of heuristics for transfer function searching, it is still based on some mathematical representations with parameters that are not directly and intuitively related to user's visualization objectives. Moreover, simply selecting sample images appears to be a very inefficient and unnatural way of expressing the user's visualization goals. The limitations in the number of sample images, their viewing perspectives and the time that is needed to generate the sample images also severely constrain the applicability of this approach. Another related topic is volume shading <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12]</ref>. Often, the gradient of each point in the volume is used as the normal vector of an imaginary local surface which can be lighted with some surface shading model. It assigns colors that depend on the local image properties (gradients), and can effectively reveal structural details that are related to the gradient field of the volume. As described in this paper, image dependent transfer functions can also be used to achieve many other visualization goals such as information filtering and surface rendering.</p><p>The objective of this work is to develop an image-based, goaloriented, and parameterized transfer function model for volume visualization and data exploration. This is done by integrating 3D image processing tools into a volume visualization process, and defining the transfer function to be a sequence of 3D image processing procedures as part of the volume visualization pipeline. Two major types of 3D image processing tools <ref type="bibr" target="#b13">[14]</ref>, image enhancement and boundary detection, are applied. Users will be able to specify and manipulate a small number of parameters to define their visualization goals, and to view desired features in various levels of details. Since these parameters represent the parameter settings of the image processing procedures with clear meanings and objectives, it provides an efficient, image-guided and goal-oriented interaction for data exploration. Although image processing techniques have previously been applied to the segmentation of 3D CT and MRI image volumes <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>, and image gradient information has also been widely used for volume and surface renderings <ref type="bibr" target="#b7">[8]</ref>, there has not been a general and systematic solution for the integration of 3D image processing in transfer function design.</p><p>In the following, we will first present, in Section 2, the imagebased transfer function model, and the schemes that integrate this model into the volume rendering process. Section 3 describes the various image processing operations as the building blocks of the transfer function model, including point enhancement, spatial enhancement and boundary detection. Some implementation issues, and the application of this approach in a microscopy data exploration project are discussed in Section 4. Section 5 concludes the paper with some final remarks and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">AN IMAGE-BASED TRANSFER FUNCTION MODEL</head><p>We define a volume to be a 3D array of voxels in an intensity (scalar) field, and a color volume (or RGBA volume) to be a 3D array of voxels in an RGBA (color and opacity) field. A color volume can be directly displayed using a volume rendering algorithm such as raycasting <ref type="bibr" target="#b8">[9]</ref> or 3D texture mapping <ref type="bibr" target="#b2">[3]</ref>, without the need for a transfer function. Therefore, we define a transfer function to be a two step process. The first step consists of a sequence of intensity mappings in a volume's intensity field, and performs tasks such as information filtering, noise reduction, surface extraction, etc. The second step is essentially the "coloring" process, which generates colors and opacity values directly from intensity values using either an intensity-to-RGBA color look-up table or a shading procedure. In the color look-up table, a linear ramp should be used for the opacity component and the appropriate (depending on the desired colors) color components, since the necessary "intensity processing" is done in the first step. Shading can be done by computing the gradients of the resulting intensity field of the first step, and using the gradients as local "surface normals" in computing the lighting effects in the rendering process. Since the "coloring" step is a fairly straightforward process, we will only consider the transfer function design problem in the volume's intensity field. Based on the above definition, a volume is essentially a 3D gray level image, and its transfer function problem can be considered as a 3D gray level image processing problem. Naturally, various image processing tools can be applied, each having a specific image pro-cessing objective with user defined parameters. In addition to providing a goal-oriented model for transfer function design, the combination of 3D image processing and volume visualization also offers some other unique advantages. First, when integrated into a visualization pipeline, the 3D image processing operations only need to be applied to the visible regions of the volume. This may lead to significant savings of the image processing cost. Secondly, many traditionally difficult problems in image processing and computer vision, such as the topological consistency and connectivity problems in boundary detection, may become trivial in an interactive visualization environment, where certain "intelligent" decisions are left to the viewers. Thirdly, as described in <ref type="bibr" target="#b14">[15]</ref>, interactive visualization may also provide useful human intervention to assist some difficult image processing tasks such as 3D segmentation.</p><p>An image-based transfer function, F : I ! I , can be defined as a sequence of intensity mappings: T i; j; k V x + i; y + j; z + k Some higher order and global image processing operations, such as dilation/erosion and anisotropic diffusion <ref type="bibr" target="#b12">[13]</ref>, cannot be directly represented as a neighborhood function. But these operations are normally applied in some pre-computation processes for the definitions of other simpler intensity mappings, as shown in Section 3.3.</p><formula xml:id="formula_0">F = fn fn,1</formula><p>Each of the intensity mappings, fi, represents an image processing procedure with user defined parameters, which computes either an intensity table or a neighborhood function. The set of all parameters of a sequence of image processing procedures defines one transfer function in the transfer function space. Such goal-oriented parameterization of the transfer function model makes the searching and navigation of the transfer function space much more user friendly and intuitive than previous methods.</p><p>There are three different approaches, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, of applying this transfer function model in volume visualization. In the first approach, the transfer function is defined as a sequence of 3D image processing procedures through a sequence of intermediate volumes, and the final resulting volume is passed to the "coloring and rendering" step. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>(a), this approach requires the reconstruction of an intermediate volume for each image processing procedure. Although this approach is easy to implement, its obvious drawback is the high cost in memory use and computational time for volume reconstructions. Since data exploration requires experimenting with many different sets of parameters for The second approach integrates all the image processing procedures into one volume rendering algorithm, so that every access to an intensity value in the volume rendering algorithm will directly go through the computations of all the 3D image processing procedures <ref type="figure" target="#fig_0">(Figure 1(b)</ref>). In addition to avoiding the large memory requirement and the associated memory operation costs, this approach applies the image processing procedures only when a sample point in the volume is actually used by the visualization algorithm for rendering. Since the number of sample points used for rendering is normally much smaller (less than 10 in most cases) than the total number of voxels in the volume, such integrated approach is, in general, a more efficient solution for interactive data exploration applications where the change of transfer function parameters is very frequent, and the interactive renderings are often lower resolution and "region of interest" based.</p><p>The integration of the intensity tables into a visualization pipeline is very straightforward. The intensity table can be directly used as the opacity and/or color transfer functions in any volume rendering algorithm (e.g. raycasting). In volume rendering using 3D texture mapping, it can also be conveniently used as a color table, such as the one defined in OpenGL, that maps (often by hardware) the texture mapping results to color and opacity values. Very little computational overhead is involved in this process.</p><p>The integration of neighborhood functions are, however, more costly. A straightforward approach is to make recursive procedural calls to the neighborhood functions to dynamically compute the image processing results for individual sample points when they are accessed by the rendering algorithm. One problem with this approach, however, is the potentially repeated computation with multiple neighborhood functions. Since each voxel can fall into the neighborhoods of several other voxels, and may therefore be accessed (and computed) multiple times when more than one neighborhood functions exist in a transfer function. When the number of neigh-borhood functions in a sequence is large, such overhead can be significant. Fortunately, this has not been a major problem in most cases for two reasons. First, the number of image processing procedures applied at one time is normally small in most practical applications. Secondly, since the number of sample points that need to be computed by the transfer function for each rendering is not very large (normally less than 10 of the total number of voxels in a volume), it is possible to apply some buffering mechanism to store the result of each computed sample point for possible later accesses (e.g. a buffer can be used for each neighborhood function). This way, the overhead can be partially or entirely eliminated depending on the buffering capacity.</p><p>The third approach uses a hybrid strategy, and allows intensity mappings to be concatenated into groups to generate intermediate volumes <ref type="figure" target="#fig_0">(Figure 1(c)</ref>). The grouping of the intensity mappings can either be defined by users or done automatically based on their computational complexities. This is mainly useful for transfer functions with long sequences of neighborhood functions. One advantage of the hybrid approach is that the parameters of the image processing procedures are naturally defined in groups. For example, an intermediate volume can be generated when a satisfactory set of parameters for a group have been found, and the subsequent image processing operations may then be based on the intermediate volume instead of the original volume.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">IMAGE VOLUME OPERATIONS IN TRANSFER FUNCTIONS</head><p>In this section, we will describe various 3D image processing operations and their applications in transfer functions for volume visualization. We are mainly interested in two major types of operations: image enhancement (including point enhancement operations and spatial enhancement operations) and boundary detection <ref type="bibr" target="#b13">[14]</ref>. The goal of image enhancement is to improve the quality of the 3D image volume for better visual appearance, based on certain visualization goals. Boundary detection, on the other hand, automatically finds the voxels that belong to some surface boundary defined by the parameters of the edge detection procedures, and is mostly used for surface rendering purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Point Enhancement Operations</head><p>A point enhancement operation applies some function to each intensity value, individually, to generate a new value. Since the result of a point enhancement operation only depends on the intensity value of the point on which it is applied, the corresponding intensity mapping can be represented as an intensity table, obtained from a pre-computation of an image processing operation under given parameters. The two most common point enhancement operations are Intensity modification and Histogram modification.</p><p>Intensity Modification: In intensity modification, the intensity curve of the input volume can be altered by modifying one or more segments of the intensity curve at corresponding intensity intervals to increase or decrease the exposure of the given regions. In <ref type="figure" target="#fig_1">Figure 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Spatial Enhancement Operations</head><p>A spatial enhancement operation derives the new intensity value of a given point from its neighborhood points, i.e. the result is region dependent. Therefore, spatial enhancement operations, in general, can only be represented as neighborhood functions. As in the 2D case <ref type="bibr" target="#b13">[14]</ref>, spatial operations can be classified into smoothing and sharpening operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Smoothing Operations:</head><p>We use smoothing operations primarily to remove image noise. We sometimes also want to remove very small feature details in order to better present the larger features. These are often achieved by applying a spatial lowpass 3D mask to smooth out high frequency components from the image. The mask represents a weighted average of the intensity values in a mmmneighborhood of each point in the volume. For smoothing purpose, all coefficients in the masks are positive numbers. In addition to the mask size m, several other parameters may also be defined to adjust the level of smoothing and blurring by manipulating the weights for the averaging. A popular smoothing operation is the Gaussian smoothing defined by a Gaussian mask with parameter 2 0; +1 : T i; j; k = e , i 2 +j 2 +k 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">2</head><p>The parameter may also be different in X, Y and Z directions:  <ref type="figure">Figure 4</ref> and <ref type="figure">Figure 5</ref>.</p><formula xml:id="formula_1">T i; j; k = e ,<label>i 2 2</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Boundary Detection Operations For Surface Rendering</head><p>Boundary detection operation finds the surface boundary voxels to derive the appropriate transfer function or thresholds for surface rendering. Most 2D edge detection algorithms <ref type="bibr" target="#b13">[14]</ref> can be extended for 3D boundary detection. Many of these algorithms, such as the Sobel detector <ref type="bibr" target="#b13">[14]</ref>, employ some convolution masks to compute the discrete approximations of some differential operators to measure the rates of changes of the intensity field (gradients), and then classify surface boundary voxels based on a magnitude thresholding of the gradient values. More sophisticated edge detection algorithms have also been developed in Computer Vision. One example is the class of anisotropic diffusion algorithms that successively blur the image using a diffusion process with a spatially varying conductance parameter that is correlated to the image's gradient values <ref type="bibr" target="#b12">[13]</ref>. A general form of this operation is: It = divcx;y;z;trI = cx; y; z; tr 2 I + rc r I where the conductances cx;y;z;t are usually taken to be high in uniform regions and low at high gradient points in the image. This results in the image being smoothed in uniform regions but the discontinuities (edges) be kept at the high gradient points. If the conductance function cx;y;z;t is constant, then the result is essentially a Gaussian blurring process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Iso-surface Based Approach</head><p>Conventional surface rendering algorithms <ref type="bibr" target="#b9">[10]</ref> first extract the surface boundaries as polygonal objects by intensity thresholding, and then display the surface boundaries using standard surface graphics techniques. In many cases, however, the intensity thresholds defining the iso-surfaces are not known in advance. Finding these thresholds by trial-and-error is difficult and time-consuming, particularly when the volume contains many different types of object boundaries with different iso-values. Using the image-based approach, we can first apply an edge detection operation to the image volume, and then automatically derive, based on the edge detection results, the surface boundary iso-values for iso-surface extraction. These isovalues may also be used to defined a transfer function through intensity modification with a standard volume rendering algorithm that renders only a layer of surface voxels, defined by intensity intervals surrounding these iso-values. <ref type="figure" target="#fig_1">Figure 2(b)</ref> shows an example where the boundary intensity range, t1; t 2 and t3; t 4 , are stretched to highlight the surfaces, and the intensity values of the rest of the intensity field is reduced to zero.</p><p>To derive the surface intensity thresholds after boundary detection, we first use a simple thresholding on the boundary detection result to extract all the boundary voxels, and then generate a histogram based on the intensity values of all boundary voxels in the original volume. For volumes with well defined surfaces, this histogram should exhibit clear peaks and valleys. The intensity values at which the histogram reaches local maxima (peaks) can then be used as the surface thresholds. A smoothing operation (e.g., Gaussian smoothing) normally needs to be applied to the histogram first to remove the noise. Two examples are shown in <ref type="figure" target="#fig_2">Figure 3</ref>, generated from a CT volume of a human head and a microscopy image volume of a Golgi Complex <ref type="figure">(Figure 6</ref>). In <ref type="figure" target="#fig_2">Figure 3(a)</ref>, the histogram has two local maxima at intensity values 61 and 84 that represent the iso-values of the skin and skull surfaces, respectively. It should be mentioned that, with this approach, both the boundary detection and histogram analysis are pre-computations of the actual rendering process. By setting different scale parameters in the boundary detection process, a set of multiscale iso-values can be pre-computed, and then used to define a set of multiscale transfer functions (as simple intensity tables) for different levels of surface rendering in data exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Dynamic Boundary Detection Based Approach</head><p>A second approach in using boundary detection for surface rendering is to directly apply a boundary detection operation to each sample point when it is accessed by the rendering algorithm. This allows the rendering algorithm to dynamically determine whether a sample point belongs to a surface boundary or not for appropriate rendering actions. With this approach, only simple boundary detection methods (e.g. convolution mask based detectors) ought to be applied for speed reason. This approach is particularly useful for surfaces that cannot be simply defined as iso-surfaces, i.e. the boundary intensity values are not constants. Usually, such non-constant-intensity boundaries are caused by the data collection process. In 3D microscopy, for instance, photobleaching may cause the same material to have different intensity values in separate slices with different depths into the volume. In this case, working directly with gradients is more effective than working with iso-surfaces. For example, the magnitudes of gradients may be proportionally mapped to the opacity values in the opacity transfer function to emphasize high gradient regions for surface rendering effect. A gradient thresholding may also be used to render only the high gradient voxels. An example is given in <ref type="figure">Figure 6</ref>(b) which shows better defined surfaces than iso-surface based rendering ( <ref type="figure">Figure 6(c)</ref>). In general, this approach requires the intensity mappings for transfer function definition be represented as neighborhood functions, and is therefore more expensive than the iso-surface based approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">IMPLEMENTATION AND APPLICATIONS</head><p>The approach described in this paper is being implemented in a microscopy visualization and data exploration system -a joint project with the Department of Medicine of the Indiana University Medical School. Microscopy volumes offer some unique challenges to volume visualization techniques. First, fluorescently-labeled samples characteristically have low signal levels, sometimes consisting of a single photon, so that microscopy images are typically much noisier than CT or MRI images. Furthermore, since excitation of fluorescence also destroys fluorophores through photobleaching, signal-tonoise ratio decreases with the collection of each focal plane of an image volume. The resulting low contrast and small intensity gradients make these image volumes sensitive to small changes in rendering parameters. Consequently, ordinary volume visualization algorithms frequently fail to capture the delicate structures present in many cellular objects. Secondly, structures in the microscopic scale typically show higher complexity than those of the anatomic organs in CT or MRI images. This is particularly true in multi-parameter images, in which several different proteins will be imaged simultaneously, each in a specific color of fluorescence. A third problem is that the structure of the objects to be examined are often partially or entirely unknown. Without prior knowledge of the structures in an image, it is difficult to determine an appropriate transfer function to generate a useful and informative rendering.</p><p>As a result, we found that, without an interactive and imagebased environment for transfer function design, it is often impossible to generate useful images from a complex, noisy and unknown microscopy image volume. We also found that, for CT images of human anatomy, the enhancement operations do not have as significant improvements in visualization quality as they do with microscopy images. This is perhaps because the CT images we tested are already very clean and the transfer functions are fairly obvious for the common visualization goals.</p><p>In integrating image-based transfer functions into volume visualization algorithms, the first two approaches given in Section 2 have been implemented and tested using a standard raycasting volume rendering algorithm with shading. Volume rendering using hardware assisted 3D texture mapping has also been implemented in this context. But since the hardware rendering pipeline with 3D texture mapping cannot be easily modified by the algorithm, only the first approach was used with the texture mapping algorithm. The volume reconstruction based approach is, in general, fairly slow. For instance, reconstructing a 256 3 volume with a 555 mask takes about 4 minutes on an SGI O2 (R5000) workstation. If the convolution is integrated into the rendering pipeline, only about 10 seconds need to be spent on computing the convolution. In the case of multiple neighborhood functions in a transfer function, if a separate buffer for each neighborhood function is used to avoid repeated computation, the overhead from the integration is very little, i.e. the integrated approach is always faster. But if no buffering is applied, the overhead is significant -usually when there are more than three neighborhood functions, the integrated approach quickly becomes slower than the image reconstruction based approach. It should be mentioned that the intensity tables do not have any noticeable im-pact to the rendering speed in the integrated approach.</p><p>Several rendering examples of confocal microscopy volumes using the image-based transfer function model are given below. <ref type="figure">Figure 4</ref> shows the rendering of a microtubule dataset using image enhancement operations. Microtubules are the molecular tracks within cells. Intracellular organelles or compartments are spatially distributed inside the cell but this spatial distribution is dynamic. The organelles can move along these tracks in molecular motors. <ref type="figure">Figure 4(a)</ref>is rendered with a linear ramp transfer function. <ref type="figure">Figure 4(b)</ref> is rendered with a median filter applied first. In <ref type="figure">Figure 4(c)</ref>, an additional highpass filter, the Laplacian filter, is applied after the median filter. The result shows the tubule structures that are not clearly visible from other renderings. <ref type="figure">Figure 5</ref> shows the effect of image enhancement in the volume rendering of fluorescently labeled Actin filaments volume. An Laplacian mask is applied followed by an unsharp masking operation with = 3 <ref type="figure">(Figure 5(b)</ref>) and = 10 ( <ref type="figure">Figure 5(c)</ref>)to show the actin structures that are not at all visible from the rendering with a linear ramp ( <ref type="figure">Figure 5(a)</ref>). <ref type="figure">Figure 6</ref> shows two surface rendering examples of a Golgi Complex. <ref type="figure">Figure 6</ref>(a) is rendered with a linear ramp transfer function. The surface rendering by the dynamic boundary detection based approach is shown in <ref type="figure">Figure 6</ref>(b). A gradient magnitude threshold 60 was used to classify the boundary voxels. <ref type="figure">Figure 6</ref>(c) is rendered by the iso-surface based approach with an intensity interval 77; 97 centered at the surface iso-value 87, which is derived from the boundary histogram <ref type="figure" target="#fig_2">(Figure 3(b)</ref>) using the same boundary detection procedure. For this dataset, the dynamic boundary detection based approach generates better surfaces.</p><p>We have also applied the boundary detection approach for surface rendering on a CT volume of a human head. <ref type="figure">Figure 6(d)</ref> is rendered by the dynamic boundary detection based approach. A gradient magnitude threshold 40 was used. The surface iso-values, 61 and 84, derived from the boundary histogram <ref type="figure" target="#fig_2">(Figure 3</ref> 74; 94 , respectively. In cases like this where more than one types of surfaces are present, the isosurface based approach is more flexible since it can select individual surfaces to display, while the dynamic boundary detection based approach has to display all surfaces at once.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>We have presented a transfer function model based on 3D image processing operations. In this approach, transfer functions are represented as a sequence of intensity mappings that can be either intensity look-up tables or neighborhood functions. The computation of these intensity mappings can be integrated into a volume rendering algorithm for better memory and time efficiency. Both 3D image enhancement operations and boundary detection operations are described and integrated into the transfer function design problem. The parameterization of this transfer function model allows goaloriented interactions in transfer function searching and data exploration. This approach is particularly useful for volume datasets that are complex and noisy with unknown structures. It is our belief that static or pre-defined sequences of renderings cannot provide sufficient insight into a complicated image volume. It is the dynamic and interactive exploration process with guided user control that provides the most comprehensive perspective into the dataset.</p><p>In the future, we plan to embed a richer set of 3D image processing tools into volume visualization. One particularly important subject is 3D image segmentation applied for transfer function design. This will lead to object level transfer function definitions, and model-based volume representations, manipulations and analysis. Another important future work is the interactivity of this image-based approach. Currently, the rendering speed is not interactive due to mainly the time spend on image processing operations. More efficient 3D image processing algorithms, and a spatially and temporally optimal buffering mechanism need to be developed for truly interactive data exploration. In terms of applications, we are currently developing an interactive environment for microscopy data exploration based on the approach described in this paper. A multiscale approach will be employed to provide a rich and well organized parameter hierarchy for transfer function searching and volume data exploration. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) The volume reconstruction based approach; (b) The fully integrated approach; (c) The hybrid approach various visualization goals, constructing new intermediate volumes for every change of parameters may not be practical.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(a), for example, interval t1; t 2 is stretched to expose more details within this intensity range. Although t1, t2 and r can all be used as parameters of the transfer function, they more likely come from the output of some other image processing procedures, such as boundary detection for surface rendering, as shown in Figure 2(b) and explained in Section 3.3.1. Histogram Modification: Histogram modification changes the histogram curve of a volume to generate a re-distribution of the intensity values. One particularly important and useful operation is (a) A simple intensity modification; (b) Intensity modification for surface rendering the histogram equalization, which flattens the histogram to increase the contrast in the areas with large number of voxels concentrated around certain values. Histogram equalization does not require parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Histograms of surface voxels after boundary detection from: (a) a CT volume of a human head; (b) a microscopy volume of a Golgi Complex.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 3(b) shows one local maximum, at intensity value 87, for the Golgi surface boundaries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a)), are then used to render Figure 6(e) and Figure 6(f), using intensity thresholding with intervals 53; 69 and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :Figure 5 :Figure 6 :</head><label>456</label><figDesc>(a) Volume rendering by a linear ramp transfer function; (b) After a median filtering; (c) Median filtering followed by a highpass filter with a Laplacian mask (a) Volume rendering by a linear ramp transfer function; (b) Laplacian masking followed by unsharp masking with = 3; (c) Laplacian masking followed by unsharp masking with = 1 0 Surface rendering examples. (a) Golgi complex rendered by a linear ramp; (b) Golgi surface rendering with gradient threshold 60; (c) Golgi surface rendering with intensity interval 77; 97 ; (d) CT head surface rendering with gradient threshold 40; (e) Skin surface rendering with intensity interval 53; 69 ; (f) Skull surface rendering with intensity interval 74; 94</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>f2 f1where I is the volume's intensity domain, fi : I ! I are intensity mappings corresponding to the sequence of image processing procedures applied. In our transfer function model, each fi is one of the following two types of mappings:</figDesc><table><row><cell>fx; y; z =</cell><cell>m 2 X i;j;k=, m 2</cell><cell>:</cell></row></table><note>1. Intensity table. It is an intensity-to-intensity look-up table rep- resenting a piecewise linear function over the volume's inten- sity field. Linear interpolation will need to be applied for each input intensity value that is not one of the table entries.2. Neighborhood function. It is a function computed from the intensity values in a mmm neighborhood of a given voxel, where the neighborhood size, m, can be an adjustable param- eter of the transfer function. A median filter[14], for instance, can be considered as a neighborhood function. A more typi- cal example is the 3D spatial convolution, as a linear 3D filter, of a volume V with a mmm mask T</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Another useful smoothing operation is the Median filter that returns the median intensity value in a mmm neighborhood. An advantage of the Median filter is that it can avoid blurring edge and surface boundaries, while still being able to achieve effective noise reduction. V1 and high-frequency component V ,V1 of an imagevolume V : V 0 = V , V1 + V1 = V + 1 , V1 +1represents the amount of high-frequency component used in the enhanced image volume V 0 . 2 0; 1 results in a smoothing of V , and 1 emphasizes the high-frequency component, and therefore sharpens features and details. V1 can be obtained from V by applying a smoothing operation to V . For instance, if a simple 333 averaging mask is applied to compute V1, the overall convolution mask for this unsharp oper-</figDesc><table><row><cell></cell><cell></cell><cell>where coefficient 2 0; ation will be: T i; j; k = n 26 + 1 =27 if i = j = k = 0 1 , =27 otherwise</cell></row><row><cell></cell><cell></cell><cell>Since sharpening operations tend to enhance high frequency inten-</cell></row><row><cell></cell><cell></cell><cell>sity signals, including noise which usually exhibit strong high fre-</cell></row><row><cell></cell><cell></cell><cell>quency characteristics, a smoothing operation may need to be ap-</cell></row><row><cell></cell><cell></cell><cell>plied first to remove or reduce the noise before the sharpening op-</cell></row><row><cell></cell><cell></cell><cell>eration. Some examples are shown in</cell></row><row><cell>2 1</cell><cell>+ j 2 2 2 2</cell><cell>+ k 2 2 2 3</cell></row><row><cell cols="3">Sharpening Operations: Sharpening operations aim to enhance of</cell></row><row><cell cols="3">geometric features by emphasizing the high frequency components</cell></row><row><cell cols="3">of the images[14]. This can be achieved by applying a highpass</cell></row><row><cell cols="3">convolution mask to the image volume. An example is the Laplacian-</cell></row><row><cell>type filter:</cell><cell></cell><cell></cell></row></table><note>fx; y; z = gx; y; z , 5 2 gx; y; z which can also be represented as a mmm mask. Another use- ful operation is the unsharp masking that blends the low-frequencycomponent</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGMENTS</head><p>We would like to thank Dr. Ken Dunn and Dr. Bob Bacallao from the Indiana University School of Medicine for providing the microscopy image volumes and the relevant biological background. We would also like to thank Yi Dai for implementing many of the image processing tools.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Image guided interactive volume visualization for confocal microscopy data exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Biddlecome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiaofen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihran</forename><surname>Tuceryan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1998 SPIE International Symposium on Medical Imaging</title>
		<meeting>1998 SPIE International Symposium on Medical Imaging</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast algorithms for volume ray tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Danskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1992 Workshop on Volume Visualization</title>
		<meeting>1992 Workshop on Volume Visualization</meeting>
		<imprint>
			<date type="published" when="1992-10" />
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deformable volume rendering by 3D texture mapping and octree encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiaofen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajagopalan</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghu</forename><surname>Raghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Visualization&apos;96</title>
		<meeting>of IEEE Visualization&apos;96<address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-10" />
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Nonlinear anisotropic filtering of MRI data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kikinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Jolesz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="221" to="232" />
			<date type="published" when="1992-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised tissue type segmentation of 3D dual-echo MR head data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kikinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shenton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Jolesz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="349" to="360" />
			<date type="published" when="1992-07" />
		</imprint>
	</monogr>
	<note>IPMI 1991 special issue</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generation of transfer functions with stochastic search techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization 96</title>
		<imprint>
			<date type="published" when="1996-10" />
			<biblScope unit="page" from="227" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Interactive 3D segmentation of MRI and CT volumes using morphological operations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Höhne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Assisted Tomography</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="285" to="294" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Display of surfaces from volume data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Application</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="29" to="37" />
			<date type="published" when="1988-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient ray tracing of volume data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="245" to="261" />
			<date type="published" when="1990-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Marching cubes: A high resolution 3D surface construction algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Lorensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Cline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="163" to="169" />
			<date type="published" when="1987-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Design galleries: Ageneral approach to setting parameters for computer graphics and animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (SIGGRAPH &apos;97 Proceedings)</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="389" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Optical models for direct volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Max</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE trans. on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="108" />
			<date type="published" when="1995-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scale-space and edge detection using anisotropic diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="629" to="639" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Digital Picture Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azriel</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Kak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982" />
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Interactive 3D-segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schiemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bomans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Tiede</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hohne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Visualization in Biomedical Computing</title>
		<meeting>Visualization in Biomedical Computing<address><addrLine>Chapel Hill, NC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">V-buffer: Visible volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Upson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Keeler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics, SIGGRAPH&apos;</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="59" to="64" />
			<date type="published" when="1988-08" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
