<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generating Sub-Resolution Detail in Images and Volumes Using Constrained Texture Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lujin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Visual Computing</orgName>
								<orgName type="department" key="dep2">Computer Science</orgName>
								<orgName type="institution">Stony Brook University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Mueller</surname></persName>
							<email>mueller@cs.sunysb.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Visual Computing</orgName>
								<orgName type="department" key="dep2">Computer Science</orgName>
								<orgName type="institution">Stony Brook University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Generating Sub-Resolution Detail in Images and Volumes Using Constrained Texture Synthesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-20T19:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.3.7 [Computer Graphics]: Color</term>
					<term>shading</term>
					<term>shadowing and texture I.3.3 [Computer Graphics]: Picture/Image Generation texture synthesis</term>
					<term>semantic zoom</term>
				</keywords>
			</textClass>
			<abstract>
				<p>A common deficiency of discretized datasets is that detail beyond the resolution of the dataset has been irrecoverably lost. This lack of detail becomes immediately apparent once one attempts to zoom into the dataset and only recovers blur. Here, we describe a method that generates the missing detail from any available and plausible high-resolution data, using texture synthesis. Since the detail generation process is guided by the underlying image or volume data and is designed to fill in plausible detail in accordance with the coarse structure and properties of the zoomed-in neighborhood, we refer to our method as constrained texture synthesis. Regular zooms become &quot;semantic zooms&quot;, where each level of detail stems from a data source attuned to that resolution. We demonstrate our approach by a medical applicationthe visualization of a human liverbut its principles readily apply to any scenario, as long as data at all resolutions are available. We will first present a 2D viewing application, called the &quot;virtual microscope&quot;, and then extend our technique to 3D volumetric viewing.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>When viewing an image (note, a volume is considered a 3D image for the discussion here) the amount of detail that can be visually explored is fundamentally bounded by the image resolution. Magnification will not extend the amount of visible detail, it will only spread it out in space such that it can be better discerned by the observer. Magnification typically entails some blurring, depending on the quality of the magnification filter used <ref type="bibr" target="#b19">[20]</ref>. However, it should be obvious that even with the best filter, pure magnification can not add detail where it has not been sampled before. Therefore, zooming into an image or volume at high magnification factors tends to create a rather boring, non-informative, and non-satisfying viewing experience.</p><p>The amount of available detail may be constrained by: (i) economical limits bounding the size and therefore the detail of the image, and/or (ii) technical limits inherent in the image acquisition process. As an example for the latter, optical lenses generally are only able to provide focus within a certain range of scale, while imaging technologies, such as MRI and CT, impose physical limits on the amount of detail they can resolve. Should detail on other scales be desired, alternative lenses or imaging methods, such as optical, confocal, and electron microscopy are required.</p><p>In computer graphics, texture mapping has long been a method by which interesting detail can be added. However, texture placement is usually guided by geometry, and not by semantic constraints imposed by the image to be enriched. Texture mapping may also cause repetitive tiling artifacts. Texture synthesis has more promise in this respect. For example, Freeman et al. <ref type="bibr" target="#b7">[8]</ref> established a database of coarse-fine resolution mappings that they used to add fine detail to magnified images of natural scenes. This fine detail, however, was on the same order of scale as the base image, and only magnifications at the same semantic level of scale were possible.</p><p>In this paper, we propose to extend the notion of image-guided detail enhancement to multiple levels of scales. However, we would like to avoid traditional image pyramids where multi-scale detail stems from the repeated smoothing of a single high resolution image. This is because requiring such an image would violate one or both of the constraints mentioned above. Instead, we introduce the notion of semantically constrained multi-scale texture synthesis to facilitate zooms at a virtually infinite number of scales, as long as the corresponding texture data are available (see <ref type="figure" target="#fig_1">Fig. 1</ref>). Here, the term "semantic zooming" means that the multiscale detail is not derived from one image to the other via simple filtering, but via different sampling processes tuned to the respective level of scale. An everyday example of semantic zooming <ref type="bibr" target="#b6">[7]</ref> is electronic maps, where each level of zoom is an excerpt of a different map, such as country, state, city, neighborhood, etc., bearing a very different style and type of detail.</p><p>In contrast to the aforementioned maps, our application does not store complete images at every level. One of our main design goals is to generate the semantic detail at a minimum of memory cost, thus providing a solution that will scale well. Therefore, our system will not yield an accurate multi-scale "map", rather, it will generate something that looks like an accurate multi-scale map,  however, one in which large-scale features and its small-scale detail smoothly blend into one another.</p><p>For example, one of the possible domain applications of our system is the "virtual microscope", where users start at a low-resolution MRI or CT image of some biological tissue and then slowly zoom in anywhere they desire to reveal the underlying cell structure, and finally the interior of the individual cells themselves. This process is illustrated in <ref type="figure">Fig. 2</ref>, for a human liver. Other possible applications include multi-resolution viewers for terrains, the universe, a sheet of metal, or any other domain that offers multiple levels of semantically constrained data, under the assumption that these data can be obtained. The fact that the different levels are obtained via synthesis and not via filtering of a common source imposes certain restrictions on the use of our technique. For exam-ple, our medical viewer would not be suitable for diagnosis of a diseased liver. However, it could be employed in a surgical simulation trainer, an electronic atlas for medical students, or a scientific illustration tool. Note that in these application scenarios the data at the different levels of scale do not have to be acquired from the same specific object, or in this case, person. This is especially advantageous since some of the higher resolution acquisition methods may be destructive. Similar restrictions and applications can also be envisioned for other application domains.</p><p>Our paper is structured as follows. First, in Section 2, we will elaborate on related work and then present our contribution. Section 3 will illustrate our 2D system by a specific example. The 3D extension is discussed in Section 4, and Section 5 will end with conclusions. <ref type="figure">Figure 2</ref>: Illustration of the semantic zooming capabilities facilitated by the virtual microscope, using a human liver as an example: (a) MRI image of a liver, where the white square is the user-specified region of interest, (b)-(s) a typical image sequence during a semantic zoom, in which (k) is the synthesized histology level image, and (s) is the synthesized cell level image, (c)-(e) magnified MRI level images, (l)-(p) magnified histology level images, (f)-(k) images obtained by blending magnified MRI and minified histology level images, (o)-(s) images obtained by blending magnified histology and minified cell level images.</p><formula xml:id="formula_0">(a) (b) (c) (d) (e) (f) (g) (h) (i) (j) (k) (l) (m) (n) (o) (p) (q) (r) (s) 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RELATED WORK</head><p>There has been much research focusing on texture synthesis approaches and applications in recent years. Texture synthesis algorithms take sample images as input and synthesize new images with similar textures. These algorithms can be roughly classified into three categories: statistical, pixel-based and patch-based texture synthesis. They have been found very helpful in image processing and also have some exciting extensions, such as surface texture synthesis <ref type="bibr" target="#b18">[19]</ref> <ref type="bibr" target="#b21">[22]</ref>, temporal textures synthesis <ref type="bibr" target="#b16">[17]</ref>[21] <ref type="bibr" target="#b11">[12]</ref>, reflectance texture synthesis <ref type="bibr" target="#b17">[18]</ref>, and others. In the following, we only discuss the synthesis approaches and applications most related to our work. Pixel-based synthesis algorithms synthesize textures pixel by pixel, which makes them rather flexible and easy to extend and apply to different areas. The representative algorithms include: Efros/Leung's non-parametric sampling algorithm <ref type="bibr" target="#b5">[6]</ref>, Wei/ Levoy's multiresolution synthesis algorithm <ref type="bibr" target="#b20">[21]</ref>, which performs exhaustive search and accelerates based on tree structured vector quantization (TSVQ). Ashikhman's coherent synthesis <ref type="bibr" target="#b0">[1]</ref> and Tong's k-coherent synthesis <ref type="bibr" target="#b17">[18]</ref> algorithms reduce the search space significantly. Their synthesis process is faster, but only suits particular types of textures well. Hertzmann's Image Analogies algorithm <ref type="bibr" target="#b9">[10]</ref> combines <ref type="bibr" target="#b20">[21]</ref> and <ref type="bibr" target="#b0">[1]</ref>, uses PCA, and approximates nearest neighbor search (ANN) to accelerate the search process, offering better results. Zelinka and Garland <ref type="bibr" target="#b26">[27]</ref> synthesize textures in real-time using a Jump Map, after a relatively slow analysis process. However, many pixel-based approaches suffer from image blurring and garbage growing.</p><p>Compared to pixel-based algorithms, patch-based synthesis algorithms tend to be faster and more stable, and do not suffer from blurring and garbage growing. They are, however, less flexible since they generate textures by copying whole patches from the input. Xu's chaos mosaic <ref type="bibr" target="#b22">[23]</ref>, Efros/Freeman's image quilting <ref type="bibr" target="#b4">[5]</ref>, Liang's <ref type="bibr" target="#b12">[13]</ref>, and Kwatra's Graphcut <ref type="bibr" target="#b11">[12]</ref> algorithms all belong to this category. We use image quilting in our system, since it is efficient and also easy to implement. For 3D applications, Graphcuts <ref type="bibr" target="#b11">[12]</ref> also seem to be a good choice.</p><p>For our application, we combine pixel-based synthesis <ref type="bibr" target="#b20">[21]</ref>, image quilting <ref type="bibr" target="#b4">[5]</ref>, and our pattern-based synthesis. Our approach is fundamentally different from that of Nealen and Alexa <ref type="bibr" target="#b14">[15]</ref> who use pixel-based re-synthesis to eliminate remaining errors in the overlap regions of patch-based synthesis. In contrast, we apply different types of synthesis methods to synthesize different regions and features in an image. Further, our pattern-based synthesis is location constrained and differs from the algorithms based on pattern placement in the surface texture synthesis domain, such as pattern-based texturing revisited <ref type="bibr" target="#b15">[16]</ref>, and texture particles <ref type="bibr" target="#b2">[3]</ref>.</p><p>In our application, we make frequent use of constrained texture synthesis, where the patch selection and texture generation is made dependent on some underlying constraints. This technique has been utilized in image processing, such as image restoration <ref type="bibr" target="#b23">[24]</ref> and texture transfer <ref type="bibr" target="#b0">[1]</ref>[2][5] <ref type="bibr" target="#b9">[10]</ref>. Another example is the textureby-numbers technique <ref type="bibr" target="#b9">[10]</ref>, which is able to perform synthesis from images in which the texture distribution is not stationary but is based on the labeling of the component textures of images. These label images, representing the segmentation information of images, are created beforehand, possibly by the user. Some automatic color or texture segmentation methods are used for guiding the texture synthesis process in <ref type="bibr" target="#b10">[11]</ref> <ref type="bibr" target="#b3">[4]</ref>. Our constrained texture synthesis follows a similar idea, but here only the segmentation of the sample images can be performed in advance. The features or patterns in the synthesized images have to be detected and labeled automatically when they are needed during zooms (see Section 3.2 for further detail). To enable proper semantic relationships across zoom levels, component textures should be placed carefully, following certain constraints including color, intensity, distance fields, location, and features/patterns of the image.</p><p>In contrast to Freeman's super-resolution algorithm <ref type="bibr" target="#b7">[8]</ref> which generates enlarged images on the same semantic level than the base image, our application performs enlargement/zooming <ref type="bibr" target="#b13">[14]</ref> that spans several semantic levels. Our main contributions are:</p><p>• Semantic zooming uses texture synthesis to extend imageguided detail enhancement to multiple levels of scales.</p><p>• Constrained texture synthesis facilitates smooth semantic evolution and detailing of features across zoom levels.</p><p>• Feature-guided texture synthesis considers the properties of features or patterns in the image at a certain semantic level and chooses image quilting, pixel-based, or pattern-based texture synthesis methods in accordance with the region's synthesis requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE VIRTUAL MICROSCOPE −− A 2D VIEWER</head><p>We first discuss the 2D application, which acts like a microscope with a wide range of magnification. Then, in the next Section, we will discuss its extension to 3D. A system overview is shown in <ref type="figure">Fig. 3</ref>. First the underlying multi-resolution image data are collected and preprocessed to build a set of sample images. Then the sample images are analyzed to choose the appropriate texture synthesis approaches and constrained rules for each pair of adjacent levels. All these are stored in a small database, which will be used during the semantic zoom operation.</p><p>At the beginning, the user views the image at the coarsest resolution ( <ref type="figure">Fig. 2a</ref>). Once the user specifies a region of interest in this image and zooms in, this part of the image is gradually magnified. When the image magnification reaches a certain scale, the image detail of the next level is generated through semantically constrained texture synthesis based on the currently magnified image region. For instance, when the user zooms into the image from the MRI level to the histology level, the system needs to synthesize the corresponding histology level image. The same is the case for the cell level. Blending of two consecutive levels enable the system to go smoothly from small-scale features to high-scale features. Thus, there are three main tasks in our system: data preprocessing, Semantic Zoom Preprocessing <ref type="figure">Figure 3</ref>: System Overview constrained texture synthesis, and level blending. We will now describe each of them in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Collection of Images</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preprocessing</head><p>We first need to collect data corresponding to the various levels and perform some amount of preprocessing on them. <ref type="figure">Fig. 4</ref> shows the sample images used in the liver example: an MRI image, a lowscale histology image, and a large-scale histology image. These three levels will be referred to as MRI level, histology level, and cell level, respectively. However, it is easy to increase the number of levels as long as the corresponding texture data are available. Once the images have been collected the following pre-processing steps have to be performed. Colorization: Typically, the images that are collected have different colors. In order to reduce the distinct discontinuities arising from mismatched colors during zooms, we need to match the colors across levels. The color correction can be easily done by image processing methods or tools, such as Adobe Photoshop. The colorized images shown in <ref type="figure">Fig. 4</ref> are the sample images that will be used to guide the synthesis later on. Since we use the color of the low-scale histology image for transfer, this image requires no change.</p><p>Segmentation: The sample images need to be segmented into prominent features or patterns, based on color, shape, or preknowledge. In our particular example, for the MRI image, we segment out the liver region as well as the portal vein and the artery elements. The segmentation can mostly be done via image processing methods <ref type="bibr" target="#b8">[9]</ref> or tools. The segmentation results, which will later help us to match texture synthesis methods with different features or patterns, are stored in tag images (see <ref type="figure">Fig. 5</ref>).</p><p>The data preprocessing is the only part in our system which may require some manual work to refine the image processing results, but it needs only to be done once. After that, no manual work is required. The colorized sample images and the corresponding tag images are then stored in a database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Constrained Texture Synthesis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Synthesis approaches</head><p>As mentioned before, a variety of texture synthesis approaches could be applied to generate the image detail for semantically different levels. For each pair of adjacent levels, which texture synthesis approaches should be used depends on the texture features, and the region in which the texture will grow.</p><p>• If the texture is isotropic, semi-structured, or structured, and grows in a large region, image quilting or other patch-based algorithms produce better quality results than pixel-based methods. The primary parameters in image quilting include patch size and overlapping region size. Both mainly depend on the prominent structures of the texture and should be decided before synthesis.</p><p>• If the texture has layers and/or grows within a small irregularly shaped region, then a modified pixel-based approach forms a convenient way to add fine detail in the magnified images. We give the details of our algorithm in Section 3.2.3. The parameters in a pixel-based synthesis algorithm <ref type="bibr" target="#b20">[21]</ref> include the shape and size of the pixel neighborhood, as well as the number of levels if a multi-resolution algorithm is applied.</p><p>• If the texture is composed of atomic patterns which should be preserved during synthesis, our pattern-based synthesis is employed to synthesize the patterns, while other pixel-based or patch-based approaches can be applied to synthesize the background color (see Section 3.2.4 for further details). Why do we need constrained texture synthesis? We need it to ensure that the generated textures on one level are semantically consistent with the level before. Since we use level blending to facilitate intermediate zooms, this is obviously very important. Standard texture synthesis algorithms only use the present layer information in the generation process, and <ref type="figure" target="#fig_3">Fig. 6</ref> demonstrates the poor blending that will occur if we perform texture synthesis on the histology level without constraining it to the lower-scale MRI level. Similar problems arise for the cell level and the lower-scale histology level. Thus, textures of the high-scale image should always be synthesized to match the features of the low-scale image under specific constraints. For this reason, the system always computes a tag image of the current result image to facilitate the matching process. This is somewhat similar to the label-constraints used in <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b0">[1]</ref>, but in our application the constraint tags are not specified by the user but generated automatically, using image processing techniques. portal vein artery</p><p>In our system, three texture synthesis methods are combined to synthesize the image. We mainly discuss the algorithm modifications which need to perform constrained synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Constrained image quilting</head><p>Image quilting is used to generate the background texture for the histology level and the cell level image, but other patch-based synthesis methods, such as Graphcuts, may also work. In the histology level, background is defined as everything except the vessels and their surrounding layer. In the cell level, background is defined as everything except the cells, the vessels and their surrounding layer. We also tried pixel-based synthesis methods to generate the background as well, but neither the single resolution nor the multiresolution (with TSVQ acceleration or PCA and ANN acceleration) algorithm seemed to work well for the textures used here, mainly because the features in the texture tended to come out blurred.</p><p>Our constrained quilting algorithm differs from typical quilting in the following two ways. First, not all patches in the segmented sample image can be used for synthesis. For example, at the histology level, the textures around the portal vein and the artery are different from the background texture (see <ref type="figure">Fig. 4c</ref>). Hence, the patches falling into those regions should not be used to generate the background texture. Second, both patch placement and selection are constrained to satisfy the match requirement. Especially at the cell level, in order to match the histology level features, the quilting process is constrained by the color/intensity of the magnified histology level image. An example for this are the white areas, called sinusoids, which appear on both synthesized levels and should be matched. Thus, when selecting a candidate patch for the third level, the location, shape, and distribution of its sinusoids must match that of the corresponding second-level area. This is not a limitation since our sample database is diverse enough, and we have never encountered a case where no fit could be found. Considering the texture structure size, the quilt patch size is chosen to be pixels, and the overlapping width is 6-8 pixels. A further constraint for background texture synthesis are object boundaries, both interior and exterior. The tag images play an important role in complying to these boundary constraints, and this will be discussed at the end of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Constrained pixel-based synthesis</head><p>Smaller structures constrained to tight and curved boundaries are better generated using pixel-based synthesis methods, since patchbased methods work on a scale too large to adhere well to the object's geometry. In our application, we use this type of approach to generate the small textures in the surrounding layer around the portal veins. However, at the same time it is desirable to transfer the global characteristics of the sample texture to the output image as well. For example, texture features, such as smooth muscle cells in our application, which are closer to the object boundary in the sample should also be placed closer to the boundary in the output image. We can achieve this by constraining the texture generation process by a measure imposed by the object geometrydistance fields, which we use here to (i) constrain the texture generation and (ii) help to find the outside boundaries for magnified veins to guide the synthesis process. We will first illustrate our pixel-based algorithm for the general case (see <ref type="figure" target="#fig_4">Fig. 7</ref>) and then discuss how it is applied within a specific example.</p><p>We calculate the distance field using a distance transform and normalize it to a range of [0,1]. The distance field is shown in <ref type="figure" target="#fig_4">Fig. 7a</ref> as a grey image, in which pixel value maps to distance. If the given sample texture has a layered appearance <ref type="figure" target="#fig_4">(Fig. 7a)</ref>, then the synthesis process must depend on these distance values. After calculating the distance fields for both sample and result image, we use the standard scan-line order to synthesize the pixels. There, for   each pixel in the result image, the matched pixel must be chosen from the set of pixels that (i) observe the usual texture synthesis metrics <ref type="bibr" target="#b20">[21]</ref> and (ii) have a similar distance field value.</p><p>If the input image is part of a layered texture, or if we want to reduce the sample image size to speed-up the synthesis, our pixelbased synthesis method will not only depend on the distance value, but also on the texture direction, which is calculated from the distance field and represented by a gradient field (see <ref type="figure" target="#fig_4">Fig. 7c</ref>). The pixel synthesis order depends on the distance values, and, based on the gradient, rotated L-neighborhoods are compared to find the best match.</p><p>In our bio-tissue example, we pre-compute the normalized distance field around the portal vein based on the tag image of the sample histology level image (see <ref type="figure" target="#fig_6">Fig. 8a</ref>). When synthesizing the histology level image, we compute a similar distance field around the vein of the magnified MRI image to find the boundary of the vein structures <ref type="figure" target="#fig_6">(Fig. 8b)</ref>. The detail in the vein periphery is then synthesized based on the distance and gradient values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Pattern-based synthesis</head><p>Our pattern-based algorithm is designed to preserve potential atomic structures, i.e. structures that cannot be cut, such as cells. Pixel-based or patch-based synthesis methods cannot generally guarantee that features remain uncut or undistorted, since they have no knowledge about which part of the texture constitutes a whole atomic pattern. We require an algorithm that will ensure that atomic structures remain intact and, at the same time, satisfy the match requirements.</p><p>We can achieve this by identifying the location of the atomic structures on the low-resolution level and replace them by highresolution versions in the magnified level. If these structures have fuzzy boundaries that blend with the background, it is useful to keep these as well. They can then later help to integrate the features into the background in a coherent way.</p><p>The first step involves identifying the atomic features. In our liver tissue example, these atomic features are represented by the cells in the cell level sample image <ref type="figure">(Fig. 4e</ref>) and are segmented as patterns <ref type="figure">(Fig. 5c</ref>). When synthesizing the cell level image, the algorithm first detects all possible cells (dark points) of the magnified histology level image based on the image intensity, and records this location information. We detect the dark points using two thresholds. Then location-constrained pattern placement proceeds, and the cell patterns are chosen randomly to increase the variation of the result. A similar method can also be used for magnifying the cells in the layer around the portal veins.</p><p>As we have mentioned above, the tag image, which corresponds to the current zoomed image, is important to comply with the match requirement. For example, the vessels (portal veins and arteries) represent interior objects which should be preserved as they are and properly scaled under zoom. However, scaling the tag image presents a problem. When the image is magnified, the corresponding tag image should also be enlarged at the same rate. Without any specific process, the boundary of the enlarged tag image will have a binarized effect <ref type="figure" target="#fig_7">(Fig. 9a</ref>). To prevent this, we use a smooth interpolator for the tag values, and then choose an intermediate value as the threshold to decide the boundary. Using this procedure, the magnified image will still have a smooth boundary <ref type="figure" target="#fig_7">(Fig. 9b)</ref>. Another possible solution is to represent the boundary as a spline curve. If the segmentation information is stored using a spline curve, the enlarged spline curve can be calculated based on several control points while the image is magnified. In this way, the boundary can be very accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Smooth Semantic Zooms</head><p>When zooming into a specific region of the image, our system combines two processes: (i) magnification of the current level image, and (ii) minification of the synthesized next high-scale level image. This achieves any level of magnification from only a few images with different semantic detail.</p><p>The system has a number of parameters, some are set by the user and some are decided by the available data. The first such parameter is the size of the output image, , which specifies the screen size of the microscope. A second parameter is the maximum zoom scale Z max for each level, which is determined by the resolution of the subsequent, more fine-scale level. This factor determines the amount of standard magnification that needs to be performed using the current level data before new semantic detail can be filled in by synthesizing from next-level data. Obviously, the more levels are available, the less blur will be encountered when zooming in. Since for real optical, confocal, or electron microscopes the maximal zoom scale can be from thousands to millions, our application accelerates the zooming activity by dramatically reducing Z max . When the present level data is magnified at Z max , the resolution has been reached at which the next higher level data can be synthesized to provide the missing detail.</p><p>Also, at the beginning, the user specifies a zoom focal point F, which determines the center of the region of interest R. This region R has a size and is calculated by the system, such that . R marks the image region that will be replaced by the next higher level detail when the zoom scale Z reaches Z max (in our example, this region is shown as the white square in <ref type="figure">Fig. 2a)</ref>.</p><p>The last parameter that our system maintains is the view port VP which is centered at F and has a size . It varies with Z, such that . At any given Z, the system will capture the image inside the VP, and then magnify and fit it into the output image. At startup, the image is not magnified, i.e. and Z=1, and is shown as the output image directly <ref type="figure">(Fig. 2b)</ref>. When the image is gradually magnified by the user, Z increases, while the VP  </p><formula xml:id="formula_1">M M × N N × N M Z ⁄ max = V V × V M Z ⁄ = V M =</formula><p>decreases. Once the VP has reached R, synthesized image data due to the next higher-level detail should be made available.</p><p>It is desirable to avoid a sudden change of the display, where the image generated from the next higher level of resolution suddenly pops in. We accomplish a graceful transition by blending the images of two consecutive levels over some range of zooms, properly weighted by a zoom-related weighting function. In addition, we prefer to do this without having to view blurred features of the present level. We can achieve both of these requirements by specifying a transition point t with a zoom scale Z t , where</p><p>, at which we compute the image for the next level, minify it, and blend it with the magnified present level. This early computation of the high-resolution image, however, requires the computation of extra data at boundaries, later culled with further zooming until the . More specifically, suppose that the synthesized image has size , then . The advantage of having a larger image available is that it allows more panning activity within the next semantic level.</p><p>The smooth image transition process over a range of consecutive zooms is illustrated in <ref type="figure" target="#fig_1">Fig. 10</ref> below. After the transition point, the magnified present image and minified synthesized image are smoothly blended by gradually changing their weights inversely, i.e. the magnified image will fade out while the synthesized image will fade in.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results</head><p>In this section we report on our specific applicationthe virtual microscope viewing a liver datasets at three levels of semantic scales. The sample images and corresponding tag images stored in our database have been shown in <ref type="figure">Fig. 4</ref> (b,c,e) and <ref type="figure">Fig. 5</ref>. A few frames of the resulting image sequence during a semantic zoom are shown in <ref type="figure">Fig. 2</ref>. When the user specifies a region-of-interest in the MRI image of a liver and zooms in, then this part of the MRI level image is gradually magnified and blended with the synthesized histology level image. If the user further zooms in from the histology level, the histology level image is magnified and eventually blends with the synthesized cell level image. This resembles the functionality obtained with a real microscope, when slowly examining an interesting part of a liver. Besides zooming, the user can also pan to inspect nearby regions.</p><p>In our algorithm, once the sample images are chosen, the time to synthesize a certain level image mainly depends on the output image size M and the magnification scale Z t of the transition point. When M is fixed, the time spent on synthesis and the blending process can be adjusted by Z t . For example, suppose the output image size is fixed on and the maximal scale Z max = 4. If the specified scale Z t of the transition point is 2, then the synthesized image has a size of . With the current implementation, it will take several minutes to generate the result image. If Z t is increased to 3, the corresponding synthesized image becomes , which reduces the time spent on synthesis. However, the blending effect is also reduced, which means the synthesized next-level image will pop in more abruptly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXTENSION TO 3D</head><p>The idea extends well to volumetric data. In order to generate subresolution detail for volume data, we extend image quilting to volume quilting, and also apply a 3D pixel-based synthesis algorithm. In volume quilting, we apply the graph cuts algorithm <ref type="bibr" target="#b24">[25]</ref>[26] to find the best seam surface between two neighboring blocks, instead of using the shortest path algorithm, which is applied in image quilting but not easy to be extended to 3D <ref type="bibr" target="#b11">[12]</ref>. From the Visible Man's cryosection data, we reconstructed the volume and segmented out the liver. Similar to the 2D case, the volume data is also colorized to match the histology data. The sample histology volume is built based on the features in the 2D image and certain 3D growth rules. We could also apply Wei's solid texture synthesis method <ref type="bibr" target="#b21">[22]</ref> to generate a sample volume, however, it is difficult to get a high quality solid texture. <ref type="figure" target="#fig_1">Fig. 11</ref> shows the volume data required by the synthesis procedure.</p><p>In the 3D extension of our viewer, the user specifies a volumeregion-of-interest <ref type="figure" target="#fig_1">(Fig. 12a)</ref>, and this volume region is cut out from the original volume and rendered. During 3D zooms, the volume region is magnified and smoothly blended with the minified synthesized higher level volume. The observed volume size changes during zooms, in contrast to the fixed-size output images in the 2D system. Some volumetric semantic zooms are shown in <ref type="figure" target="#fig_1">Fig. 12</ref>. For the histology level, as in 2D, the textures around the vein are synthesized by a pixel-based algorithm, while other textures are created by volume quilting. <ref type="figure" target="#fig_1">Fig. 13</ref> shows volume with cut and translucent rendering results. The translucent volumes are rendered using the OpenQVis software (http://openqvis.sourceforge.net/). An advantage of volume synthesis over traditional surface synthesis is that only the former can illustrate the translucent effect of internal structures. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>We have described a new constrained multi-scale texture synthesis method to facilitate semantic zooms. Pixel-based, image quilting, and pattern-based synthesis methods were unified to generate highdetail images under certain constraints. Our demo application, a virtual microscope, demonstrated that quite interesting and useful image sequences can be generated using our framework.</p><p>In future work, we would like to improve our algorithm in terms of accuracy and speed. For the former, more sophisticated segmentation and constraints may yield more refined small detail. We would also like to explore better interpolation methods for the oriented texture synthesis to overcome some of the remaining visual artifacts. Finally, optimization and GPU acceleration of our algorithm will provide more interactive capabilities, i.e. for generating the detail on demand when zooming into an image or volume. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Semantic zooming based on texture synthesis Synthesize new detail Synthesize new detail</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Image data and pieces of colorized sample images. (a) MRI liver image, (b) colorized, (c) low-scale histology image, (d) high-scale histology image, (e) colorized (Images (c) and (d) courtesy of http://www.bu.edu/histology) Some tag images for the liver example. (a) MRI image, (b) histology-level image, (c) celllevel image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Mismatched levels. The histology level image (b) does not match the specified region of the MRI level image (a), and the cell level image (c) does not match the specified region of (b) either.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Our pixel-based synthesis methods. Pixel synthesis based on distance field: (a) sample image and its distance field, (b) reference distance fields and corresponding synthesis results. Pixel synthesis based on distance field and gradient field: (c) sample image and its distance and gradient fields, (d) synthesis process and result, (e) sample image and the result of synthesizing a thick skin histology image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Vein periphery synthesis based on distance fields. (a) generated from the segmented sample image, (b) generated from magnified MRI image, (c) texture detail.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Smooth boundary problem caused by tag image magnification: (a) dentate boundary, (b) smooth boundary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Image transition process. Volume data and colorized volume. (a) visible man's volume, (b) colorized volume, (c) segmented liver, (d) an example of the sample histology level volume and its translucent result (e).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :Figure 13 :</head><label>1213</label><figDesc>Illustration of semantic zooming into volume data. (a) first level for part of the liver, (e) histology level of (a), (b)-(d) volumes obtained by blending the magnified first level volume and minified histology level volume. Synthesized volume with sub-details.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>Support was provided by NSF Career grant ACI-0093157. We would like to thank Dimitris Samaras and Neophytos Neophytou for their help in the preparation of an early version of this paper and the anonymous reviewers for their thoughtful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ashikhmin</surname></persName>
		</author>
		<title level="m">Synthesizing natural textures. ACM Symposium on Interactive 3D Graphics</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast texture transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ashikhmin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="38" to="43" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Texture particles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Dischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maritaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ievy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghazanfarpour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="401" to="410" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fragment-based image completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Drori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeshurun</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGGRAPH 2003</title>
		<meeting>of SIGGRAPH 2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="303" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image quilting for texture synthesis and transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGGRAPH</title>
		<meeting>of SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="341" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Texture synthesis by non-parametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1033" to="1038" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Space-scale diagrams: understanding multiscale interfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Bederson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CHI&apos;95 Human Factors in Computing Systems</title>
		<meeting>of CHI&apos;95 Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Example-based super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pasztor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="page" from="56" to="65" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Woods</surname></persName>
		</author>
		<title level="m">Digital Image Processing</title>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Addison-Wesley Pub Co</publisher>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Salesin</surname></persName>
		</author>
		<title level="m">Image analogies. Proc. of SIGGRAPH</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="327" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inference of segmented color and texture description by tensor voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="771" to="786" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Graphcut textures: Image and video synthesis using graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schödl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bobick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGGRAPH 2003</title>
		<meeting>of SIGGRAPH 2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="277" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Realtime texture synthesis by patch-based sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="127" to="150" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pad: an alternative approach to the computer interface</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Perlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGGRAPH 1993</title>
		<meeting>of SIGGRAPH 1993</meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="57" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nealen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexa</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<title level="m">Hybrid texture synthesis. Proc. Eurographics workshop on Rendering</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Neyret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Cani</surname></persName>
		</author>
		<title level="m">Pattern-based texturing revisited. Proc. of SIGGRAPH 1999</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="235" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Video textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schödl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Essa</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGGRAPH</title>
		<meeting>of SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="489" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Synthesis of bidirectional texture functions on arbitrary surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">02</biblScope>
			<biblScope unit="page" from="665" to="672" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Texture synthesis on surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Turk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGGRAPH</title>
		<meeting>of SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="347" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Digital Image Warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wolberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>IEEE Computer Society Press</publisher>
			<pubPlace>Los Alamitos, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast texture synthesis using treestructured vector quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGGRAPH</title>
		<meeting>of SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="479" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Texture synthesis by fixed neighborhood searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Chaos mosaic: fast and memory efficient texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Shum</surname></persName>
		</author>
		<idno>MSR-TR-2000- 32</idno>
		<imprint>
			<date type="published" when="2000" />
			<pubPlace>Microsoft Research</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image restoration using mutli-resolution image synthesis and image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yamauchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Computer Graphics International</title>
		<imprint>
			<biblScope unit="page" from="120" to="125" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third International Workshop on Energy Minimization Methods in Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="359" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fast approximate energy minimization via graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="377" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Towards real-time texture synthesis with the jump map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zelinka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2002 Eurographics Workshop on Rendering Techniques</title>
		<meeting>2002 Eurographics Workshop on Rendering Techniques</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="99" to="104" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
