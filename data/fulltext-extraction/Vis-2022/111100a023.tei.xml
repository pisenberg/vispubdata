<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Uncertainty-Aware Multidimensional Scaling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">David</forename><forename type="middle">H</forename><surname>Ägele</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tim</forename><surname>Krake</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Daniel</forename><surname>Weiskopf</surname></persName>
						</author>
						<title level="a" type="main">Uncertainty-Aware Multidimensional Scaling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-03-27T09:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Uncertainty visualization</term>
					<term>dimensionality reduction</term>
					<term>multidimensional scaling</term>
					<term>non-linear projection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. 1. Uncertainty-Aware Multidimensional Scaling (UAMDS) for increasing levels of uncertainty. The top row shows 4-dimensional data sets consisting of 6 entries (color-coded). The left data set has no uncertainty, the middle data set models one entry as a normally distributed random vector, and the right data set entirely consists of multivariate normal distributions. The bottom row shows the respective UAMDS projection, where the isolines indicate the 25 th to 75 th percentile range of the projected (normal) distributions. The projected points of the left projection are shown in faded color in the other two plots to indicate the change due to uncertainty.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Dimensionality reduction is used in numerous fields of application as a tool to reduce data while keeping the relevant data characteristics. It conceptually projects high-dimensional data to, or embeds it into, a target space of lower dimensionality. It is a popular approach for visualizing high-dimensional data: the mapping to 1D, 2D, or 3D spaces enables visual representation, processing, and interaction. However, dealing with uncertain data makes these tasks significantly more challenging and needs additional consideration. Unfortunately, non-linear dimensionality reduction techniques such as multidimensional scaling (MDS) <ref type="bibr" target="#b18">[19]</ref>, t-distributed stochastic neighbor embedding (t-SNE) <ref type="bibr" target="#b34">[35]</ref>, or uniform manifold approximation and projection (UMAP) <ref type="bibr" target="#b22">[23]</ref> do not provide a direct mechanism to deal with uncertain data and propagate such uncertainty through the process of dimensionality reduction.</p><p>In this paper, we pick MDS because it is one of the most common dimensionality reduction techniques. We address the problem of uncertainty visualization by introducing uncertainty-aware MDS (UAMDS), an extension of MDS to uncertain data. The contributions of this paper can be summarized as follows: first, the extension of MDS to uncertain data by providing a generic and rigorous mathematical formulation that supports any kind of distributions and a variety of stress types; second, a reformulation of the generic concept for the important case of normally distributed input data and squared stress, leading to an efficient numerical algorithm and intuitive interpretation of the mathematical terms that go into the MDS computation; third, several visualization techniques tailored to our approach that address the trustworthiness and sensitivity of dimensionality reduction under uncertainty. In addition to these technical contributions, we show the usefulness of UAMDS with several examples and illustrate the application of our visualization techniques.</p><p>Fig. <ref type="figure">1</ref> shows a simple example, where the left column demonstrates the application of UAMDS to a small data set without uncertainty (in this case, UAMDS boils down to traditional MDS). The middle col-umn presents the result of UAMDS for the same data set, yet with uncertainty attached to one point (modeled as a random vector). In the right column, we introduce uncertainty to the remaining data points. UAMDS embeds the multivariate probability density functions from the input data into the 2D space. This leads to 2D probability densities, which are illustrated via isolines. We can see that uncertainty not only manifests itself in spread-out distributions, but also in the location of the centers of these distributions. The UAMDS projection in the right column shows that there is an even greater impact on the means. In addition, this projection also changes the orientation of the orange distribution in 2D space. In short, MDS needs to be extended to UAMDS to correctly capture the impact of data uncertainty on dimensionality reduction, leading to a more reliable representation of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Our paper addresses uncertainty visualization, which has been identified as one of the challenging problems of visualization research <ref type="bibr" target="#b16">[17]</ref>.</p><p>General approaches to uncertainty visualization. In their seminal paper, Pang et al. <ref type="bibr" target="#b25">[26]</ref> provide a classification of uncertainty visualization techniques and describe several strategies for representing uncertainty. Since then, much additional progress has been made in uncertainty visualization; a summary of techniques can be found in survey papers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18]</ref> and in an interactive visual collection <ref type="bibr" target="#b15">[16]</ref>.</p><p>The above surveys also discuss models of representing uncertainty in data. A common approach assumes some kind of probabilistic modeling. We follow this approach in our paper as well: we describe uncertainty of input data in the form of probability density functions, which allows us to describe typical sources of uncertainty, such as measurement imprecision or variability from statistical aggregation over input samples. A broader discussion of uncertainty levels is provided by Skeels at al. <ref type="bibr" target="#b32">[33]</ref>; however, probabilistic modeling can play a relevant role in many of their levels of describing uncertainty.</p><p>According to Brodlie et al. <ref type="bibr" target="#b2">[3]</ref>, there is a difference between visualization of uncertainty and uncertainty of visualization. Both aspects play a role for UAMDS. Our main goal is to make MDS aware of uncertainty in the data, addressing the problem of visualizing uncertainty. However, dimensionality reduction introduces additional error because it cannot guarantee that distances are preserved, i.e., we are also facing the problem of uncertainty of visualization. We address this problem by devising visualization techniques that explicitly show such error. In this way, we contribute to improving trust in the visualization <ref type="bibr" target="#b29">[30]</ref>. Indeed, our approach looks at how uncertainty is propagated through the visualization pipeline <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11]</ref>. We focus on the propagation of uncertainty through dimensionality reduction via (UA)MDS, but also consider the visual representation at the stage of visual mapping and rendering.</p><p>Dimensionality reduction, MDS, and uncertainty. There are many publications on linear or non-linear dimensionality reduction. An overview of non-linear techniques is provided by Lee and Verleysen <ref type="bibr" target="#b19">[20]</ref>. As this paper addresses the extension of MDS to uncertain data, our discussion focuses on methods that consider uncertainty.</p><p>One of the most frequently used techniques of (linear) dimensionality reduction is principal component analysis (PCA) <ref type="bibr" target="#b26">[27]</ref>. In the context of fuzzy data, Masson and Denoeux <ref type="bibr" target="#b6">[7]</ref> presented an extension of PCA by using autoassociative neural networks. Görtler et al. <ref type="bibr" target="#b8">[9]</ref> introduced an uncertainty-aware version of PCA with a formulation that incorporates moments of a distribution. Their method projects the first and second order moments via a linear mapping from high-dimensional to lowdimensional space. Correa et al. <ref type="bibr" target="#b5">[6]</ref> demonstrated uncertainty-aware visual analytics and also showed propagation of uncertainty through PCA. While these approaches are based on linear dimensionality reduction, our approach extends the non-linear technique MDS, which requires different mathematical and numerical modeling.</p><p>Schulz et al. <ref type="bibr" target="#b31">[32]</ref> presented a method for a probabilistic graph layout of uncertain networks. Their method uses Monte Carlo sampling of graphs and combines the resulting graph layouts to uncertainty visualization. In contrast, UAMDS does not require a (computationally costly and potentially inaccurate) Monte Carlo approximation but utilizes an analytic mathematical model that considers distributions.</p><p>In the context of MDS, there are probabilistic extensions. For example, the model developed by Zinnes and MacKay <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b36">37]</ref> assumes random vectors with fixed variance in all dimensions as input and outputs corresponding distributions in screen space. Therefore, it differs from our approach. Masson and Denoeux <ref type="bibr" target="#b21">[22]</ref> adapted MDS to fuzzy systems where dissimilarities are given as intervals or fuzzy numbers. Their approach results in fuzzy regions. Besides the modeling aspects, our work also addresses a much more generic concept of uncertainty.</p><p>Bayesian models of MDS were presented by Bakker and Poole <ref type="bibr" target="#b0">[1]</ref> and Soh <ref type="bibr" target="#b33">[34]</ref> for probabilistic embeddings of noisy dissimilarities. Their approaches estimate distributions in the embedding space, whereas our mathematical model results in a projection operator that maps high-dimensional distributions to low-dimensional space. Nguyen and Holmes <ref type="bibr" target="#b23">[24]</ref> consider uncertain dissimilarities for MDS projections allowing them to propagate uncertainty into visualization spaces. Our method takes more uncertainty information into account, e.g., covariance, and is able to propagate orientation and shape of distributions.</p><p>Another related approach was developed by Chen et al. <ref type="bibr" target="#b3">[4]</ref>. Their uncertainty-aware projection technique for multivariate ensemble data estimates distributions from data and uses MDS on the means of a subset of the distributions for an initial layout of landmark points. The remaining points are then projected to screen space using a least squares approach that takes mean distances but also a divergence measure between distributions into account. Although the approach also deals with distributions and projections for dimensionality reduction, our mathematical model is a consistent extension of MDS that takes all aspects of a distribution simultaneously into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MATHEMATICAL MODEL AND NUMERICAL ALGORITHM</head><p>In this section, we motivate and derive our mathematical model of UAMDS. We start with basic conceptual considerations and the derivation of the generic model that allows processing of arbitrary kinds of distributions. Then, we consider the special, yet common and relevant case of uncertainty described by normal distributions. Accordingly, we reformulate the generic model, leading to a concise description of a corresponding optimization problem. Based on this, we discuss the numerical algorithm to compute UAMDS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Concept</head><p>We start with MDS, i.e., data is yet not associated with uncertainty. MDS formulations are typically classified into metric and non-metric types. Our work addresses the class of metric MDS using a generalized form of the stress consisting of a sum of squares of dissimilarity differences. Given points p 1 ,..., p K ∈ R N in the high-dimensional space R N , the goal is to find points x 1 ,...,x K ∈ R n in a low-dimensional target space R n (i.e., n &lt; N) such that the following stress is minimized:</p><formula xml:id="formula_0">S(x 1 ,...,x K ) = ∑ i, j p i − p j α p − x i − x j α p 2 .</formula><p>(</p><p>The relevant information about objects in high-dimensional space are expressed by pairwise dissimilarities d i, j = p i − p j α p , often with p = 2 (representing Euclidean distance) and α = 1.</p><p>How can we now arrive at the uncertainty-aware version of MDS? Our derivation is inspired by limit process considerations in physics that take one from the mechanics of individual mass points to continuum mechanics <ref type="bibr" target="#b7">[8]</ref>. Fig. <ref type="figure" target="#fig_0">2</ref> applies this concept to our problem of uncertainty awareness in MDS. The left part of Fig. <ref type="figure" target="#fig_0">2</ref> shows the "world" of probability density functions, both in the high-dimensional space (top-left; illustrated as 2D space for the sake of visual presentation) and the lowdimensional space (bottom-left; illustrated as 1D space). Unfortunately, we do not yet know how to handle this left part. Therefore, we reduce the problem to something that we already know: MDS for data points. We achieve this by conceptually sampling the distributions, which leads to clouds of data points in high-dimensional space (top-right). For these, we can then apply MDS as usual, leading to the dimensionality reduction of the point cloud (bottom-right). Finally, we can construct the corresponding probability density functions in the low-dimensional space (bottom-left). This is a conceptual model assuming that we have a limit process of infinitely dense sampling-similar to the limit process taken in calculus for defining Riemann integration. The resulting integral representation of UAMDS can be seen as the result of a limit process of the sampled model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sampled Model Our Generic Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Generic Mathematical Model</head><p>To work with this conceptual model, we need a formulation that provides points in high-dimensional space. For the time being, we assume that these are given in a high-dimensional Cartesian space. Later, we will see that we do not need Cartesian coordinates for the final computation, but that distances are sufficient, as for traditional MDS.</p><p>With this, we can express the desired low-dimensional points x 1 ,...,x K in terms of the high-dimensional points p 1 ,..., p K by using local projections Φ 1 ,...,Φ K : R N → R n , i.e., we obtain the relation</p><formula xml:id="formula_2">x k = Φ k (p k ) for k = 1,...,K.</formula><p>In other words, we describe the overall dimensionality reduction as a combination of individual local projections, one for each of the uncertain input distributions. We want to point out that the local projections are not yet known; in fact, it is our goal to eventually compute them in order to determine the dimensionality reduction. In the next subsection, we will see that the formulation of the stress truly restricts the dimensionality reduction process and, thus, reasonable local projections can be determined.</p><p>Due to the use of local projections, the minimization problem in Equation 1 is equivalent to minimizing the following stress (with projections Φ k that map p k to x k , e.g., constant functions Φ k (p) = x k ):</p><formula xml:id="formula_3">S(Φ 1 ,...,Φ K ) = ∑ i, j p i − p j α p − Φ i (p i ) − Φ j (p j ) α p 2 .</formula><p>(</p><formula xml:id="formula_4">)<label>2</label></formula><p>With this representation, it is possible to formulate UAMDS because the dimension of the point objects is the same for all i and j. Now, we need to transition from the world of data points (righthand side of Fig. <ref type="figure" target="#fig_0">2</ref>) to the world of distributions (left-hand side of Fig. <ref type="figure" target="#fig_0">2</ref>). Given random vectors P 1 ,...,P K with realizations in the high-dimensional space R N and associated probability density functions f P1 ,..., f PK : R N → R, the goal is to find low-dimensional random vectors X 1 ,...,X K via X k = Φ k • P k with realizations in the lowdimensional target space R n and associated probability density functions f X1 ,..., f XK : R n → R such that stress S is minimized.</p><p>For the formulation of the stress, we again resort to the conceptual model of sampled random vectors, as illustrated in Fig. <ref type="figure" target="#fig_0">2 (top-right</ref>). In traditional point-based MDS, we would have to compute pairwise distances (i.e., distances between all pairs of sampled points), which go into the summation in the stress term (Equation <ref type="formula" target="#formula_1">1</ref>). Now, we have to transform this by the corresponding limit process. Here, we replace the summation for the traditional stress term by integration, leading to the following stress (compare Fig. <ref type="figure" target="#fig_0">2</ref>):</p><formula xml:id="formula_5">S(Φ 1 ,...,Φ K ) (3) = ∑ i, j R N R N f Pi,Pj (v, w) • v − w α p − Φ i (v) − Φ j (w) α p 2 dv dw,</formula><p>where f Pi,Pj is the joint probability density function of f Pi and f Pj .</p><p>We can see that the integrand directly corresponds to the summands in Equation <ref type="formula" target="#formula_1">1</ref>. The joint probability density function f Pi,Pj describes the probability density for the respective distance pair. The integration has to go over the space of these pairs, now represented as the space of pairs of points from R N × R N . The integration variables v and w represent the locations of the two respective points.</p><p>In the case of independent random vectors, it holds:</p><formula xml:id="formula_6">f Pi,Pj (v, w) = f Pi (v, w) • f Pj (v, w).</formula><p>Hence, the formulation is a direct extension of Equation 2 to an integral representation that accounts for the uncertainty in the data. This aspect is clearly observed if the random vectors are modeled by Dirac delta distributions (thus, there is no uncertainty in the data). In this case, Equation 3 directly leads to Equation <ref type="formula" target="#formula_1">1</ref>. This shows that our new model is compatible with traditional MDS.</p><p>Besides the stress function, another modeling parameter of Equation 3 are the local projections Φ 1 ,...,Φ K that determine the nature of the low-dimensional random vectors X 1 ,...,X K via X k = Φ k • P k . We observe that this factor highly depends on the input distributions P 1 ,...,P K . In the next subsection, this aspect is examined in the framework of normal distributions. For a completely generic approach, however, it is useful to approximate the local projections via a finite Taylor series that is tailored to the distribution, e.g., the Taylor series should be evaluated at the expected values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Normal Distributions</head><p>While the generic UAMDS model above provides the framework for arbitrary distributions, in this subsection, we want to derive a model based on normal distributions. We have two goals: (1) find an adequate choice of local projections Φ 1 ,...,Φ K that are suitable for multivariate normal distributions; (2) evaluate the stress (Equation <ref type="formula">3</ref>) to obtain an adequate mathematical formulation and solve it numerically.</p><p>We choose p = 2 and α = 2 for the dissimilarity, i.e., dissimilarity based on the squared Euclidean distance. The reason for this choice is the tractability of the resulting integrals. Eliminating the implied square root of the regular Euclidean distance allows us to handle the integrals analytically. For the sake of transparency and easy reproducibility, the full details of the technical parts of the calculations are placed in the supplemental material <ref type="bibr" target="#b12">[13]</ref> (Sections 1-3) while important results are summarized in this documents as well.</p><p>According to the previous subsection, random vectors P 1 ,...,P K with realizations in high-dimensional space R N and associated probability density functions f P1 ,..., f PK are given.</p><p>Assumption 1: The random vectors P 1 ,...,P K are independent and normally distributed with</p><formula xml:id="formula_7">P k ∼ N (μ Pk , Σ Pk ), μ Pk ∈ R N , Σ Pk ∈ R N×N ,</formula><p>and the singular value decompositions</p><formula xml:id="formula_8">Σ Pk = U Pk S Pk U Pk T with S Pk = diag(s Pk 1 ,...,s Pk N ) and U Pk = u Pk 1 ... u Pk N .</formula><p>The first consequence is that f Pi,Pj = f Pi • f Pj due to the independence of the random vectors. Secondly, we can perform a change of variables with the transformations</p><formula xml:id="formula_9">Ψ k : R N → R N , Ψ k (v) = U Pk S Pk 1/2 v + μ Pk (</formula><p>for the degenerate case, a limit process is needed that leads to the same result, though):</p><formula xml:id="formula_10">S(Φ 1 ,...,Φ K ) = ∑ i, j R N R N 1 (2π) N 1 (2π) N e − 1 2 v T v e − 1 2 w T w (4) U Pi S Pi 1/2 v + μ Pi −U Pj S Pj 1/2 w − μ Pj 2 2 − Φ i (U Pi S Pi 1/2 v + μ Pi ) − Φ j (U Pj S Pj 1/2 w + μ Pj ) 2 2 2 dv dw.</formula><p>Now, the local projections Φ 1 ,...,Φ K are the only remaining model parameters in Equation <ref type="formula">4</ref>. In general, the use of an affine transformation (first-order Taylor) Φ : R N → R n , Φ(v) = Bv + c is sufficient for multivariate normal distributions. In fact, for a normally distributed random vector P ∼ N (μ, Σ), the transformed distribution is given by X = Φ • P ∼ N (Bμ + c, BΣB T ). Therefore, we model the local projections by the following affine transformations.</p><p>Assumption 2: The local projections Φ 1 ,...,Φ K : R N → R n are modeled by adjusted affine transformations</p><formula xml:id="formula_11">Φ k (v) = B k U PkT (v − μ Pk ) + c k with variables B k = b k 1 ... b k N ∈ R n×N and c k ∈ R n .</formula><p>The effect of the adjusted affine transformations onto the highdimensional normally distributed random vectors P k ∼ N (μ Pk , Σ Pk ) is that the low-dimensional random vectors are normally distributed as well with</p><formula xml:id="formula_12">X k = Φ k • P k ∼ N (μ Xk , Σ Xk ) = N (c k , B k S Pk B k T ).</formula><p>Due to the adjustments, the transformation has a clear interpretation for the distributions. After the extensive evaluation of the integrals in Equation 4 via affine modeled projections (leading to a representation that only depends on the projection components c k and B k ), the stress S(Φ 1 ,...,Φ K ) can now be equivalently expressed as a function of the projection components c = (c 1 ,...,c K ) and B = (B 1 ,...,B K ). We denote this equivalent function as S. For the sake of transparency, this stress S :</p><formula xml:id="formula_13">R N × ••• × R N × R N×N × ••• × R N×N → R ≥0</formula><p>is split into three parts (see Subsection 3.1 in the supplemental material <ref type="bibr" target="#b12">[13]</ref>):</p><formula xml:id="formula_14">S(c, B) = ∑ i, j S i, j 1 (c, B) + S i, j 2 (c, B) + S i, j 3 (c, B), (<label>5</label></formula><formula xml:id="formula_15">)</formula><p>where S 1 , S 2 , and S 3 are given by (δ kl is the Kronecker delta)</p><formula xml:id="formula_16">S i, j 1 (c, B) = 2 N ∑ k=1 N ∑ l=1 s Pi k s Pi l δ kl − b i k , b i l 2 2 (6) + 2 N ∑ k=1 N ∑ l=1 s Pj k s Pj l δ kl − b j k , b j l 2 2 + 4 N ∑ k=1 N ∑ l=1 s Pi k s Pj l u Pi k , u Pj l 2 − b i k , b j l 2 2 S i, j 2 (c, B) = N ∑ k=1 s Pi k u Pi k , μ Pi − μ Pj 2 − b i k , c i − c j 2 2<label>(7)</label></formula><formula xml:id="formula_17">+ N ∑ k=1 s Pj k u Pj k , μ Pi − μ Pj 2 − b j k , c i − c j 2 2 S i, j 3 (c, B) = μ Pi − μ Pj 2 2 − c i − c j 2 2 (<label>8</label></formula><formula xml:id="formula_18">)</formula><formula xml:id="formula_19">+ N ∑ k=1 s Pi k 1 − b i k , b i k 2 + N ∑ l=1 s Pj l 1 − b j l , b j l 2 2 .</formula><p>For a compact matrix notation of the three stress components, we refer to Subsection 3.2 in the supplemental material <ref type="bibr" target="#b12">[13]</ref>.</p><p>With the interpretation that a normally distributed random vector can be represented as an ellipse in 2D, it can be observed that the stress components characterize certain aspects of a normal distribution. In this representation, the ellipse is spanned by the principal axes, which are given by the eigenvectors of the covariance matrix. Using this interpretation, the first stress component S i, j 1 solely addresses the assimilation of the covariance matrices in terms of size and shape.</p><p>In contrast, the third component S i, j</p><p>3 characterizes two aspects: fitting of the expected values and the comparison of the principal axes of the covariance matrices. Since the first stress component already covers this covariance matrix aspect (in fact, this term is also included in the sums), S i, j 3 mainly fits the expected values. The only thing missing is the orientation of the ellipses. This point is reflected in the second stress term S i, j 2 , where the alignment of principal axes to the difference of the expected values (used like an orientation line) is used.</p><p>Another important aspect of the stress formulation is that the actual random vectors do not need to be defined explicitly. In fact, analogously to traditional MDS, where only distances of points (d i j = p i − p j 2 ) are necessary, our normal distribution UAMDS only requires certain scalar values as well. These are given by:</p><p>• distance of the expected values: • scalar products of the principal axes:</p><formula xml:id="formula_20">d E i j = μ Pi − μ Pj •</formula><formula xml:id="formula_21">u Pi k T u Pj l for k, l = 1,...,N • scalar products u Pi k , μ i − μ j 2 , u Pj k , μ i − μ j 2 for k = 1,...,N</formula><p>This list shows the input for normal distribution UAMDS, indicating the information requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Numerical Algorithm</head><p>After the evaluation of the stress, we have to deal with a minimization problem. In case of a normal distribution model, our goal is to find min c,B S(c, B). For solving the minimization problem, an off-the-shelf optimizer can be used. Our implementation is based on a generalpurpose gradient descent method, where we vectorize the parameters c, B of the affine transformations and jointly optimize for minimal stress.</p><p>The reference implementation is available as a Java library <ref type="bibr" target="#b11">[12]</ref>.</p><p>According to the previous subsection, the stress consists of three different components. While the presented formulation (Equation <ref type="formula" target="#formula_14">5</ref>) highlights the connections between single objects, the corresponding matrix notation, which can be found in the supplemental material <ref type="bibr" target="#b12">[13]</ref> (Subsection 3.2), has numerical and algorithmic advantages. For the implementation, in particular, for the evaluation of the stress, we recommend using the matrix notation. For an implementation of gradient descent, we also provide an analytic gradient in the supplemental material <ref type="bibr" target="#b12">[13]</ref> (Section 4) for the normal distribution model. However, for more complex distributions or choices of local projections, there may not be an easy analytical solution of the gradient. In this case, we recommend resorting to a numerical gradient computation.</p><p>By default, our implementation uses a random initialization of the affine transformations, but we also support two other initialization strategies for speeding up the starting stage of the optimization: The No-Variance initialization performs normal distribution UAMDS on a version of the data set with zero covariances, which allows for omitting major parts of the stress calculation and provides reasonable c i , and the UA-PCA initialization performs UA-PCA <ref type="bibr" target="#b8">[9]</ref> on the data set and uses the resulting linear projection as initialization for affine transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">VISUALIZATION</head><p>In this section, we discuss how to visualize the results of UAMDS and we provide visualization techniques that support the analysis with UAMDS. Finally, we present a visualization analysis for varying uncertainty. We are omitting the normal distribution indication when refering to our evaluated UAMDS model from here on for conciseness. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Representation of Distributions</head><p>Here, we describe how the resulting low-dimensional random vectors can be visualized. According to our generic approach, the lowdimensional random vectors are characterized by X k = Φ k • P k , where P k is the input high-dimensional random vector and Φ k : R N → R n the projection operator computed via UAMDS. The effect of the projection operator onto the probability density functions is given by</p><formula xml:id="formula_22">f Xk (z) = Φ −1 ({z}) f Pk (w) dw . (<label>9</label></formula><formula xml:id="formula_23">)</formula><p>In the case of arbitrary distributions, there might be no compact formula for the probability density function. Thus, a discretization is needed to visualize the shape of the distribution in the low-dimensional space. However, in the case of normal distributions, our model shows that the projection of the continuous distributions P k ∼ N (μ Pk , Σ Pk ) boils down to a projection of the characteristic moments, i.e., Φ k</p><formula xml:id="formula_24">• P k ∼ N (μ Xk , Σ Xk ) = N (c k , B k S Pk B k T ).</formula><p>For the visualization of normally distributed random vectors in 2D, there are several options <ref type="bibr" target="#b35">[36]</ref>. A simple but also effective option is the use of scatter plots, where a large number of samples (drawn from a distribution) are visualized (see Fig. <ref type="figure" target="#fig_1">3a</ref>). Another sampling-based approach are hypothetical outcome plots <ref type="bibr" target="#b14">[15]</ref>, which use animation to show possible realizations of random vector. The latter technique allows an understanding of distributions and correlations over time, whereas the former provides a sense of sample likeliness based the density of certain regions.</p><p>Visualizing the probability density function as a 2D scalar field provides information about likeliness of certain regions as well. While it is possible to encode continuous probability densities with a color gradient, the effect of binning certain value ranges has its advantages <ref type="bibr" target="#b24">[25]</ref>. The use of isobands at specific quantities of the distribution is shown in Fig. <ref type="figure" target="#fig_1">3b</ref>. In this context, the 50 th , 80 th , and 95 th percentiles are used. Using stippling of the scalar field, the issue of overdraw present with isobands can be mitigated <ref type="bibr" target="#b30">[31]</ref>. In our visualizations of projected normal distributions, we choose isolines (see Fig. <ref type="figure" target="#fig_1">3c</ref>), which have less overdraw issues than probability samples or isobands, and give a clear impression of shape and size. These aspects are beneficial for our analysis and comparisons of projections. In general, we advise users of UAMDS to choose a representation that fits the individual tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Trustworthiness of Projections</head><p>When projecting data with UAMDS, or any other dimensionality reduction method, some information cannot be preserved. While a projection is found that is optimal with respect to the stress, in our case, some random vectors may have a worse representation than others in favor of the global projection. Thus, by the nature of dimensionality reduction, a projection cannot be fully trusted. This is amplified when dealing with uncertain inputs and the more important it is to support the assessment of the projection quality and faithfulness to help the analysis.</p><p>A straightforward way to inform about projection faithfulness of the individual distributions is through color coding of the stress. We compute how much stress results from projection of each distribution P i , which is given by S</p><formula xml:id="formula_25">i (c, B) = ∑ j S i, j 1 (c, B) + S i, j 2 (c, B) + S i, j</formula><p>3 (c, B), and color the projected elements in the visualization accordingly. This makes it easy to spot projections that may not faithfully represent the high-dimensional distribution and its relationship to others.</p><p>Leveraging interaction, we allow for selection of an individual distribution P i and give more details on the stress by color coding elements corresponding to P j according to pairwise stress components S i, j (c, B). This way, it can be assessed which distances between the projected distributions can be trusted and which cannot. Fig. <ref type="figure">4</ref> shows an example, where distributions 1 and 4 have most overall stress. The right plot in Fig. <ref type="figure">4</ref> shows the local stress of distribution 1, i.e., distributions are colored according to the stress they share with distribution 1. It can be seen that especially the adjacent distribution 4 causes high stress, while the others exhibit only little stress, hinting at a faithful representation of their distances. Fig. <ref type="figure">10</ref> shows the individual pairwise stress components in a scatter plot with x = S i, j 1 , y = S i, j 2 , which allows for even more detailed assessment of projection quality and detection of poorly represented relationships. Looking at the stress is a good first indicator for projection quality, but we can do more to assess how well distances are preserved in the projection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distribution Shepard Diagram</head><p>Since UAMDS is a distancepreserving technique, the use of a Shepard diagram to evaluate projection quality is an obvious choice. In regular point-based projections, such as MDS, the Shepard diagram is a scatter plot of pairwise distances of the original and the projection space. A single point in a Shepard diagram has coordinates (D i, j , d i, j ) and its position shows how the distance in the projection d i, j between i and j compares to the distance D i, j . This notion of distances is also present in our general UAMDS model where D i, j = v − w p and d i, j = Φ i (v) − Φ j (w) p (see Equation <ref type="formula" target="#formula_4">2</ref>). However, the high-dimensional points v, w are samples from their respective distributions P i , P j so that there can be infinitely many of them. Since we are interested in the projection quality for the whole distributions, we may analyze the distribution of high-and low-dimensional distances between random vectors P i and P j . For this, we define the bivariate random vector D i, j of pairwise distances between P i and P j as follows:</p><formula xml:id="formula_26">D i, j = P i − P j 2 Φ(P i ) − Φ(P j ) 2 . (<label>10</label></formula><formula xml:id="formula_27">)</formula><p>In a Monte Carlo fashion, we draw a number of samples from P i and P j , project them according to UAMDS and compute pairwise highand low-dimensional distances. Plotting the samples from D i, j in a scatter plot shows an approximation of its distribution. Observing its location and spread with respect to the plot's diagonal, we can see that distances between P i and P j are overestimated (above diagonal) or underestimated (below diagonal) in the projection. Using brushing and linking, we support highlighting of samples in the Shepard diagram corresponding to projected distributions in the visualization to mitigate the problem of massive overdraw stemming from sampling. Fig. <ref type="figure">5</ref> shows a Shepard diagram for pairwise distances D 1, j , i.e., the distances between distribution 1 (which has high overall stress) and the others. It can be seen that there is high point density along the diagonal and some spread above and considerable spread below it, which means that distances from 1 have a tendency to be underestimated, specifically distances to 4 (orange) are underestimated. In the right Shepard diagram, the pairwise distance distribution D 1,0 is highlighted (red dots), which was occluded in the other diagram. It can be seen that the points are close to the diagonal, which means that distances between distribution 0 and 1 are faithfully represented.</p><p>We also provide two other Shepard diagram versions tailored to projections with UAMDS. For these, we consider the pairwise differences of the normally distributed random vectors Y i j = P i − P j ∼ N (μ Pi − μ Pj , Σ Pi + Σ Pj ). To analyze the degree to which variability is preserved we use two different metrics on the respective covariances. Fractional anisotropy</p><formula xml:id="formula_28">√ N √ N−1 (λ1, .. ,λN ) (λ1− λ , .. ,λN − λ )</formula><p>, where λ is the average eigenvalue of an N × N covariance matrix with eigenvalues λ 1 , .. ,λ N , is a metric for the anisotropy of the distribution. We use the trace tr(</p><formula xml:id="formula_29">Σ Pi + Σ Pj ) = E[ Y i j 2 2 ] − E[Y i j ] 2</formula><p>2 as a metric for the total variability of a distribution (i.e., the expected squared norm of the random vector Y i, j without taking into account the expected location). With these metrics, we can analyze how well variability information is preserved in the projection when plotting them for pairs of a high-and low-dimensional random difference vectors. Together with brushing and linking, we facilitate quality assessment of a UAMDS projection as shown in Fig. <ref type="figure">10</ref> and also in the supplemental material <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Sensitivity Analysis of Dimensionality Reduction</head><p>We argue that the analysis of high-dimensional data becomes even more challenging when data is uncertain. While using off-the-shelf dimensionality reduction is possible, for instance, when only expected values are regarded, the faithfulness of such projections has to be questioned since the uncertainty is not accounted for. In Fig. <ref type="figure">1</ref>, we illustrate that taking different levels of uncertainty into account leads to different results compared to projection of just the expected values. To show that UAMDS is indeed aware of uncertainty, we provide an analysis on a synthetic data set for the sensitivity to data variance.</p><p>Our synthetic data set consists of 5 uncertain inputs described by 4D multivariate normal distributions. For the analysis, we introduce a scaling parameter to vary the uncertainty by scaling the distributions' covariance matrices. Initially, we scale down the covariances to zero, and project the data with UAMDS, which yields an equivalent projection to regular MDS in this case. We then iteratively increment the scaling of the covariances of the input and apply UAMDS while using the affine transformations that resulted in the previous iteration, for initialization. This is important since it provides consistent global orientation over all scaling steps and avoids convergence to very different local optima in each iteration. While gradually increasing the scaling back to 1, we trace the the projected means and covariances.</p><p>In Fig. <ref type="figure" target="#fig_3">6a</ref>, the projection for the covariance scalings 0.0 and 1.0 are shown. It can be seen that distributions 0 and 2 as well as 1 and 4 are projected close to each other when no uncertainty is considered, but they are drifting apart when introducing variance. According to the isolines, the mean of distribution 1 is within the 80 th percentile of projected distribution 2, which means that possible realizations of the random vectors can be closer than what is illustrated by the upper plot.</p><p>In Fig. <ref type="figure" target="#fig_3">6b</ref>, we plotted the trace of the means in the projection as they travel with increasing uncertainty. We also show the principal axes of the projected covariances for scalings 0.0125 (in light color), 0.5, and 1.0 (saturated color) on the mean traces. It can be seen that means travel in different directions, blue (1) and orange (4) for instance, diverge from each other, but blue also takes a sharp turn after the first half of the upscaling process. While red and purple stay rather stationary, green moves toward blue during the second half of the process. We can also see that the principal axes of covariances turn, which means that not only the positions change, but also the way in which distributions are oriented to each other in the projection to account for a better preservation of distances of hypothetical samples.</p><p>Taking a closer look at the rate of change of projected means and covariances to further analyze the sensitivity, we plotted the magnitudes of the velocities of means and covariances in Fig. <ref type="figure" target="#fig_3">6c</ref>. The upper plot shows the velocity of the means for increasing scalings. High velocity for all of the means can be seen in the beginning, hinting at a high sensitivity to changes in variance when variance is small. The positions of means seem to be converging around a scaling of 0.35, but further increments to the scaling cause some of the means to move again. Blue and green even gain more and more momentum. The velocities of the covariances of projected distributions show a behavior similar to the means in the beginning and around the 0.35 scaling mark.</p><p>We also looked into the effect of only varying the variance scaling for a single input. In Fig. <ref type="figure">1</ref>, the effect of introducing uncertainty to only a single data vector (colored orange) is shown in the middle column. The former positions of the projected data points are indicated by faded color so that it can be seen that some are mostly unaffected (red and green), and others have moved considerably. In general, we could see from our analysis that UAMDS is sensitive to changes in uncertainty on a global and local level, and that changes in the projection can be rather unpredictable and surprising as could be seen from the velocities in Fig. <ref type="figure" target="#fig_3">6c</ref>, for example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>While only synthetic data has been used in the previous sections, we will showcase our technique on non-synthetic example data sets in the following. We also take the opportunity to demonstrate the versatility of UAMDS and what benefits come with our mathematical model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Iris Data Set</head><p>Since dimensionality reduction is often used to assess similarity of subsets of data, e.g., when dealing with labeled data, the locations of single points in the projection may not be of interest. Instead, the shape of clusters and, for example, their overlap is interesting to analyze. Furthermore, many data sets consist of observations that can be interpreted as samples of an underlying distribution. Such distributions are usually not explicitly given and are a source of uncertainty for the analysis. When estimating the underlying distributions from data, the uncertainty is quantified and we can use UAMDS to visualize it. With our current model, we need to assume that the distributions are normally distributed, so we can estimate their densities from mean and sample covariance, and then map them to two dimensions. As an example of this procedure, we use the well known Iris data set, which consists of observations from three different species of Iris flowers. Fig. <ref type="figure" target="#fig_4">7</ref> shows a comparison of a regular MDS projection and our UAMDS projection. For Fig. <ref type="figure" target="#fig_4">7b</ref>, the high-dimensional distributions of the three different species have been estimated from the data as normal distributions. It can be seen that the kernel densities of the UAMDS projection are very similar in orientation and spread compared to the clusters in Fig. <ref type="figure" target="#fig_4">7a</ref>. Both plots show an overlap of the Versicolor (orange) and Virginica (blue) species, and that Setosa (green) samples are less similar to the aforementioned ones.</p><p>Outliers cannot be spotted in the UAMDS density plot, since it projects the estimated distribution instead of the whole data set. However, the densities are well suited for analyzing relationships between whole populations. In fact, we are able to project the data points using UAMDS as well, since our model is based on projection operators for distribution samples, which we approximate by affine transformations. Applying the transformation UAMDS yields for each distribution to the original data, provides results very similar to regular MDS (see Fig. <ref type="figure" target="#fig_4">7b</ref>). This way, UAMDS can also be used for processing massive data sets of many instances, which is often infeasible with regular MDS. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Student Grades Data Set</head><p>In this section, we will demonstrate the application of our method to data sets with uncertain observations. Since uncertainty can stem from different sources, let us first characterize what kind of uncertainty in a data set we are considering here. What we mean by data consisting of uncertain observations is that every instance of the set is a random vector instead of a regular vector. The random vector could be fully described by a multivariate distribution or each feature as a distribution on its own. An example of this is the student grades data set presented by Denoeux and Masson <ref type="bibr" target="#b6">[7]</ref> and also used by Görtler et al. <ref type="bibr" target="#b8">[9]</ref>. It consists of four features describing students' performance in different subjects (M1, M2, P1, P2). The performance in a subject is based on grading, ranging from 0 to 20 points. However, the grading is expressed with different levels of uncertainty, for instance, it can be given as a real number, a range of values, or even as a textual description such as 'very good' or 'fairly bad. <ref type="bibr">'</ref>  For UAMDS, we need normally distributed uncertainty. Value ranges In the following, we will compare UAMDS and uncertainty-aware principal component analysis (UA-PCA) <ref type="bibr" target="#b8">[9]</ref> on the basis of the student grades data set. For this, we compute a projection with UA-PCA, use the UA-PCA projection as initialization for UAMDS, and compare both projections. We choose to initialize UAMDS with UA-PCA to obtain similar global orientation. From the density plots on the left half of Fig. <ref type="figure" target="#fig_6">8</ref>, it can be seen that the projections look similar, but there are some differences, e.g., the distribution for Tom is more spread out for UAMDS. There is also more overlap between Jane and Tom in the UAMDS projection, and less for Tom and David.</p><p>Using a Shepard diagram, we can assess the difference in projection quality in terms of distance preservation. By nature of PCA, with its global linear projection, distances cannot be overestimated, which is why all pairwise distances are on or below the diagonal. We take a closer look at distances to Tom's distribution (red) in the Shepard diagram, since it is the distribution causing most projection stress. For UAMDS, we can see that there is slight over-and underestimation for distances between Tom and the green, orange, and brown distributions (Bob, Joe, Jack). These distances seem to be more faithfully represented with UAMDS than with UA-PCA. Also, the pairwise distances between Tom and Jane (purple in the Shepard diagram) are more closely located to the diagonal in the Shepard diagram. When looking at the projections with the alternative mapping of textual grades to distributions (right half of Fig. <ref type="figure" target="#fig_6">8</ref>), we see that this difference of modeling uncertainty has a noticeable effect on the projections (UA-PCA and UAMDS), underlining their sensitivity to uncertainty. From the Shepard diagram with highlighted red distribution, we can see that distances of samples within Tom's distribution are more faithfully represented by UAMDS since it has less spread and deviation from the diagonal in the Shepard diagram. Also interesting to see are the prominent horse-shoe shapes in the Shepard diagram for distances to David (blue) and Jane (purple). This means that for sample pairs with large distance in the original space, the projection either represents those distances well, or underestimates them considerably. This can be the case when overlapping and non-overlapping parts of the distributions are projected to close-by locations, similar to looking at a photo where foreground and background elements can be close. We encourage the reader to have a look at our supplemental material <ref type="bibr" target="#b12">[13]</ref>, where we provide additional visualizations for this data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Pokemon Data Set</head><p>Uncertain and statistical data can be found in many domains, especially video games contain carefully crafted sources of uncertainty to make gameplay more diversified. From this domain, we use a data set stemming from the Pokemon video games that consists of monsters (the Pokemon) and their different battle qualities Health Points (HP), Attack Strength (Att), Defense (Def), Special Attack Strength (SpA), Special Defense (SpD), and Speed. Each Pokemon comes with a type (e.g., fire-, water-, grass-, bug-type), but we omit the explanation of this mechanic for conciseness. The source of the uncertainty is the value ranges for the different stats that are possible. We collected the possible minimum and maximum values of the six stats for every kind of Pokemon from a web source <ref type="bibr" target="#b27">[28]</ref>, resulting in a set of 6D random vectors. In the following analysis, we select the first 15 Pokemon, containing the three starter Pokemon and their evolutions, as well as two early obtainable bug type Pokemon and their evolutions.</p><p>From the left plot in Fig. <ref type="figure" target="#fig_7">9</ref>, we can see a pattern of grass, water, and fire types following the same direction. The strongest Pokemon (Charizard, Venusaur, and Blastoise) are found on the right side of the plot while their weaker evolution predecessors are located left of them. The weakest monsters are found on the left side of the plot. In traditional MDS, such patterns would also be possible to find, however, when using UAMDS we have means to analyze why elements have a certain arrangement in the visualization. Examining the projection matrices of the different Pokemon, we see how the different dimensions of their distribution are mapped to 2D space. Each distribution could potentially use a very different projection matrix, but due to the coupling of projections in the stress term, the matrices need to be similar to some extent. Visualized at the bottom are the projection matrices for a selection of Pokemon, where it can be seen how the different stats are projected to 2D. For instance, the Charizard distribution is mapped in such a way that samples with high speed stats are mapped to the top right. It can also be seen that all samples with higher than average stats are found right from the mean, which means that weaker Charizard samples are mapped to the left. When looking at the visualizations of the other projection matrices, we see that they all agree on "weak samples left," which implies that monsters are ordered from left to right according to total strength in the whole visualization. Comparing the projection matrices of Charizard and Blastoise, we can see that they agree on projection directions to a large degree. Looking at the most dominant projection directions, speed and defense, we can explain the spatial arrangement of the distributions in this local area. Quicker monsters are located to the top right, defensive monsters are found toward the bottom, which implies that their distance in the visualization mainly encodes these stats.</p><p>The right plot in Fig. <ref type="figure" target="#fig_7">9</ref> color codes the stress of the projection. It can be seen that the projection of Beedrill's distribution causes the most stress. When examining the projection matrix, we can see that it has Att as dominantly mapped stat in place of Speed. The main part of Beedrill's high stress is caused by the nearby Butterfree distribution, where the projection almost omits the Att stat, which explains the stress and also tells us that the spatially encoded similarity of the two can only be explained when ignoring this stat. Fig. <ref type="figure">10</ref> shows the individual stress components S i, j 1 and S i, j 2 , as well as the Shepard diagrams for covariance quality preservation. It can be seen that pairs including Beedrill tend to have greater stress in the part responsible for size Fig. <ref type="figure">10</ref>. Plots for assessing projection quality of the UAMDS projection of Fig. <ref type="figure" target="#fig_7">9</ref>. The stress parts S 1 , S 2 for all pairs i, j are plotted (left). The Shepard diagrams compare the trace of high and low-dimensional covariance combinations (middle) and high and low-dimensional fractional anisotropy of difference distribution pairs (right). Dots with light colors are pairs of different Pokemon (i = j), where their original color mapping is blended. Highlighted dots with outline are pairs with Beedrill. and shape of the distribution. Beedrill's distribution is also projected considerably more isotropically than the original distribution, as can be seen in the Shepard diagram (middle low dark green dot). The total variability of all distributions is mostly underestimated as indicated by the dots under the diagonal in the right Shepard diagram.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We showed that our conceptual idea of considering an infinite number of samples from input distributions in the limit and plugging these into an already existing optimization problem, provides a good starting point for developing uncertainty-aware variants of already existing methods, such as MDS in our case. We think that this concept can also be fruitful for other dimensionality reduction methods, but more considerations will be required to arrive at useful models, such as the introduction of a projection operator in UAMDS. Even though we have a distributionagnostic general model, a concrete stress has to be derived from it for specific distributions to be able to perform UAMDS. We already provide a model for the common case of normal distributions, and show a detailed derivation in the supplemental material <ref type="bibr" target="#b12">[13]</ref>. This derived model assumes data consisting of normally distributed random vectors, independence between random vectors, uses squared distances in the stress, and estimates a separate linear projection operator for each of them. The process of introducing new types of distributions involves more derivations to be done and is thus not straightforward. Fortunately, normally distributed data is quite common, so our presented model should be widely applicable nonetheless. Assuming independence between random vectors of our normal distribution model is not always valid. For example, student grades could be correlated due to students being taught by the same teacher. However, such information is usually not provided, in which case independence is assumed anyways. Our choice of squared distances for dissimilarity made the stress term tractable for analytical stress derivation. However, this choice implies that pairs of distributions with large distances are emphasized in the stress, whereas pairs of close-by distributions are less crucial for stress minimization. Realizing the projection through local affine transformations follows the principle of first-order Taylor approximation, but may not perform well in cases where the operator cannot be assumed to be locally linear. This can be the case when there is major overlap between distributions in high-dimensional space. To support identification of such cases we provided visualization techniques for checking the trustworthiness of the projection. However, we also benefit from assuming local linearity as it is easy to interpret and provides the opportunity to apply analysis methods from linear projections as we showed in the results.</p><p>Our examples and the comparison to UA-PCA showed that UAMDS is a useful addition to uncertainty-quantifying methods. Due to its approximation of a projection operator by local linear projections, it provides greater flexibility than UA-PCA, which is especially useful when dealing with anisotropic distributions. Generally, UAMDS makes it possible to analyze relationships in high-dimensional data sets while considering uncertainty and propagating it to the visualization space, therefore allowing for a more informed and faithful analysis.</p><p>In the future, we want to derive models for a broader range and mix of distributions, e.g., uniform, bimodal, and heavy-tailed distributions. We would also like to support a wider variety of stress types, such as nonmetric MDS, and test more sophisticated projection operators. Investigating the viability of using different dissimilarity measures as well as their effect on the projection will be an important part of improving the flexibility of UAMDS. Investigating ways to make other dimensionality reduction methods, like t-SNE or UMAP, uncertaintyaware and examining their stability <ref type="bibr" target="#b28">[29]</ref> under uncertainty is also on our roadmap. Finally, we plan to perform a user study based evaluation <ref type="bibr" target="#b13">[14]</ref> of the uncertainty visualization via MDS.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. A visualization of our our generic model compared to the sampled model. The illustration shows that UAMDS extends the concept of a finite number of pairwise distances (processed with MDS) to an infinite number of distances weighted by the corresponding probability density function. The resulting integral representation of UAMDS can be seen as the result of a limit process of the sampled model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Typical techniques to depict 2D probability density functions.</figDesc><graphic url="image-1.png" coords="5,65.14,73.00,76.66,76.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. Using color to show which distributions cause most stress: (a) Regular color coding for discerning distributions. (b) Global stress, i.e., the stress for each distribution to all others. (c) Local stress, i.e., stress between a selected distribution (thick isolines) and the others.</figDesc><graphic url="image-7.png" coords="5,313.71,214.89,113.82,113.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. Effect of scaling variance of the input distributions on the UAMDS projection. In (a), the projection is shown for zero (top) and full variance (bottom). In (b), we trace the means as they travel to different locations while increasing scaling of the covariance matrices from 0 to 1. We show the orientations of the projected distributions' principal axes for scalings of 1/80 (brighter color), 1/2 and 1 (saturated color). Plotted in (c) is the rate of change of mean locations Δμ (top) and covariances ΔΣ F (bottom) of the projected distributions, for scalings between 1/80 and 1.</figDesc><graphic url="image-9.png" coords="6,75.76,73.00,90.27,180.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. (a) Scatter plot of MDS-projected Iris data set. (b) Density plot of UAMDS-projected distribution estimates of the Iris data set (isolines of 50 th , 80 th , and 95 th percentiles). (c) Scatter plot of UAMDS-projected data points. The UAMDS plots have been rotated to roughly match the orientation of the clusters in the MDS plot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>[a, b] are translated to normal distributions with mean μ = (b − a)/2 and variance σ 2 = (b − a) 2 /12. For textual descriptions, we provide two mappings to normal distributions as shown in Fig. 8 (top).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Comparison of UA-PCA and UAMDS on the student grades data set. The left and right side use different distributions to model uncertainty in textual grade descriptions (top row): VB="very bad", B="bad", FG="fairly good." Projections of the students' grade distributions with UA-PCA and UAMDS are shown as density plots (middle row). Shepard diagrams (bottom row) show high-and low-dimensional pairwise distances between the red distribution (Tom) and the others. The right side additionally shows highlighted pairwise distances for selected distributions for better visibility.</figDesc><graphic url="image-18.png" coords="8,64.91,264.21,118.47,118.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. UAMDS projection of the first 15 Pokemon. The left side uses coloring by type of Pokemon (bug, fire, grass, water). The right side color codes the stress of projected distributions. The bottom shows the projection matrices of the affine transformations for the individual Pokemon (each column is depicted as a direction into which the related stat is projected).</figDesc><graphic url="image-23.png" coords="9,228.94,72.77,66.38,173.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>The following table shows the data set.</figDesc><table><row><cell></cell><cell>M1</cell><cell>M2</cell><cell>P1</cell><cell>P2</cell></row><row><cell>Tom</cell><cell>15</cell><cell>fairly good</cell><cell>N (14, 5.7 2 )</cell><cell>[14, 16]</cell></row><row><cell>David</cell><cell>9</cell><cell>good</cell><cell>fairly good</cell><cell>10</cell></row><row><cell>Bob</cell><cell>6</cell><cell>[10, 11]</cell><cell>[13, 20]</cell><cell>good</cell></row><row><cell>Jane</cell><cell>fairly good</cell><cell>very good</cell><cell>19</cell><cell>[10, 12]</cell></row><row><cell>Joe</cell><cell>very bad</cell><cell>fairly bad</cell><cell>[10, 14]</cell><cell>14</cell></row><row><cell>Jack</cell><cell>1</cell><cell>[4, 6]</cell><cell>9</cell><cell>[6, 9]</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)-Project ID 251654672-TRR 161 (Project A01). In addition, the authors would like to thank Daniel Klötzl for practical suggestions on the paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bayesian metric multidimensional scaling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="125" to="140" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Overview and state-of-the-art of uncertainty visualization</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bonneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Potter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rheingans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4471-6497-51</idno>
	</analytic>
	<monogr>
		<title level="m">Scientific Visualization</title>
				<editor>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Hansen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Kaufman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Hagen</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A review of uncertainty in data visualization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Brodlie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S A</forename><surname>Osorio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lopes</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4471-2804-56</idno>
	</analytic>
	<monogr>
		<title level="m">Expanding the Frontiers of Visual Analytics and Visualization</title>
				<editor>
			<persName><forename type="first">J</forename><surname>Dill</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Earnshaw</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Kasik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Vince</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Wong</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="81" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Uncertainty-aware multidimensional ensemble data visualization and exploration</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2015.2410278</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1072" to="1086" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An operator interaction framework for visualization systems</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Riedl</surname></persName>
		</author>
		<idno type="DOI">10.1109/INFVIS.1998.729560</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Information Visualization (InfoVis &apos;98)</title>
				<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="63" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A framework for uncertaintyaware visual analytics</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Correa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1109/VAST.2009.5332611</idno>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Symposium on Visual Analytics Science and Technology</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="51" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Principal component analysis of fuzzy data using autoassociative neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Denoeux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Masson</surname></persName>
		</author>
		<idno type="DOI">10.1109/TFUZZ.2004.825990</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="336" to="349" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Classical Mechanics</title>
		<author>
			<persName><forename type="first">H</forename><surname>Goldstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980">1980</date>
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Uncertainty-aware principal component analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Görtler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Spinner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Streeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Deussen</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2019.2934812</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="822" to="831" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The visualization of uncertain data: Methods and problems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Griethe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Simulation und Visualisierung</title>
				<imprint>
			<date type="published" when="2006">SimVis 2006. 2006</date>
			<biblScope unit="page" from="143" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Visualization idioms: A conceptual model for visualization systems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Mcnabb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visualization in Scientific Computing</title>
				<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Nielson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Shriver</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Rosenblum</surname></persName>
		</editor>
		<meeting><address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="74" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Source Code for Uncertainty-Aware Multidimensional Scaling</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hägele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krake</surname></persName>
		</author>
		<idno type="DOI">10.18419/darus-2995</idno>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Supplemental Material for Uncertainty-Aware Multidimensional Scaling</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hägele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<idno type="DOI">10.18419/darus-3104</idno>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">In pursuit of error: A survey of uncertainty visualization evaluation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Correll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kay</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2018.2864889</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="903" to="913" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hypothetical outcome plots outperform error bars and violin plots for inferences about reliability of variable ordering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Resnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Uncertainty visualisation: An interactive visual survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Engelke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dwyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Raiamanickam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Paris</surname></persName>
		</author>
		<idno type="DOI">10.1109/PacificVis48177.2020.1014</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Pacific Visualization Symposium</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">PacificVis 2020. 2020</date>
			<biblScope unit="page" from="201" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Top scientific visualization research problems</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="DOI">10.1109/MCG.2004.20</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="13" to="17" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recent advances and challenges in uncertainty visualization: a survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kamal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhakal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Javaid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Devabhaktuni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Zaientz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Marinier</surname></persName>
		</author>
		<idno type="DOI">10.1007/s12650-021-00755-1</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Visualization</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="861" to="890" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">J. B. Kruskal. Multidimensional Scaling. Sage</title>
		<imprint>
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Nonlinear Dimensionality Reduction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Verleysen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A probabilistic model for the multidimensional scaling of proximity and preference data</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Mackay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Zinnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Science</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="325" to="344" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multidimensional scaling of fuzzy dissimilarity data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Masson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Denoeux</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0165-0114(01)00162-2</idno>
	</analytic>
	<monogr>
		<title level="j">Fuzzy Sets and Systems</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="339" to="352" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Umap: Uniform manifold approximation and projection for dimension reduction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Melville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03426</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bayesian unidimensional scaling for visualizing uncertainty in high dimensional datasets with latent ordering of observations</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Holmes</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12859-017-1790-x</idno>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">394</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Evaluating the impact of binning 2D scalar fields</title>
		<author>
			<persName><forename type="first">L</forename><surname>Padilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Quinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Creem-Regehr</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2016.2599106</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="431" to="440" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Approaches to uncertainty visualization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Wittenbrink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Lodha</surname></persName>
		</author>
		<idno type="DOI">10.1007/s003710050111</idno>
	</analytic>
	<monogr>
		<title level="j">The Visual Computer</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="370" to="390" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">on lines and planes of closest fit to systems of points in space</title>
		<author>
			<persName><forename type="first">K</forename><surname>Pearson</surname></persName>
		</author>
		<author>
			<persName><surname>Liii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="559" to="572" />
			<date type="published" when="1901">1901</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<ptr target="https://pokemondb.net/" />
		<title level="m">Pokemon database</title>
				<imprint>
			<biblScope unit="page" from="2022" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Visualizing the stability of 2d point sets from dimensionality reduction techniques</title>
		<author>
			<persName><forename type="first">C</forename><surname>Reinbold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumpf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Westermann</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.13806</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="333" to="346" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The role of uncertainty, awareness, and trust in visual analytics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sacha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Senaratne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2015.2467591</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="240" to="249" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-class inverted stippling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Kwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Becher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Baumgartner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Reina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Deussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<idno type="DOI">10.1145/3478513.3480534</idno>
	</analytic>
	<monogr>
		<title level="m">Article No.: 245</title>
				<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Probabilistic graph layout for uncertain network visualization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nocaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Goertler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Deussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Brandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2016.2598919</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="531" to="540" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Revealing uncertainty for information visualization</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Skeels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Robertson</surname></persName>
		</author>
		<idno type="DOI">10.1057/ivs.2009.1</idno>
	</analytic>
	<monogr>
		<title level="j">Information Visualization</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="70" to="81" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Distance-preserving probabilistic embeddings with side information: Variational Bayesian multidimensional scaling Gaussian process</title>
		<author>
			<persName><forename type="first">H</forename><surname>Soh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI&apos;16</title>
				<meeting>the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI&apos;16</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2011" to="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Uncertainty visualization: Concepts, methods, and applications in biological data visualization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<idno type="DOI">10.3389/fbinf.2022.793819</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Probabilistic multidimensional scaling: Complete and incomplete data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Zinnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Mackay</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF02314675</idno>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="48" />
			<date type="published" when="1983-03">Mar 1983</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
