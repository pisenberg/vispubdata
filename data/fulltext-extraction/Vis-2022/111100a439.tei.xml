<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PuzzleFixer: A Visual Reassembly System for Immersive Fragments Restoration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shuainan</forename><surname>Ye</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhutian</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiangtong</forename><surname>Chu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Juntong</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Guohua</forename><surname>Geng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yingcai</forename><surname>Wu</surname></persName>
						</author>
						<title level="a" type="main">PuzzleFixer: A Visual Reassembly System for Immersive Fragments Restoration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-03-27T09:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Immersive visualization</term>
					<term>interactive exploration</term>
					<term>fragment reassembly</term>
					<term>cultural heritage</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. <ref type="figure">1</ref>. Reassemble a defect object via PuzzleFixer. Given a set of fragments with matching errors, PuzzleFixer successively provides three stages to solve the puzzle: 1) inspection stage that allows users to find and pick out the mismatches, 2) exploration stage that supports searching for possible solutions from a large number of matching candidates, and 3) confirmation stage that ensures the accuracy and efficiency of manual alignment of fragments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Reassembly, which reconstructs the fragments of a fractured object to restore its initial state, is an imperative task to recover the function and value of the object. It has proven benefits across various application domains, such as artifact reassembly in archaeology <ref type="bibr" target="#b45">[47]</ref>, fracture restoration in surgery <ref type="bibr" target="#b60">[62]</ref>, and physical evidence reconstruction in forensics <ref type="bibr" target="#b28">[30]</ref>. Prior research on reassembly is built upon the advances in 3D scanning technology and geometry processing, mainly developing automatic approaches that match the adjacent fragment pairs <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b41">43]</ref> and gradually merge the pairs until the fractured object is restored <ref type="bibr" target="#b55">[57,</ref><ref type="bibr" target="#b70">72]</ref>. While these methods significantly improve the reassembly efficiency, their success rates are still far from satisfying as the broken pieces and missing fragments cause large noises and ambiguities <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b26">28]</ref>. Therefore, experts need to manually fix the errors after object is automatic reassembled. For example, analysts usually observe the semantic features (e.g. textures and global appearance) to identify the mismatches and adjust the position and rotation of the fragments when manually reassembling an object <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b22">24]</ref>.</p><p>Various techniques have been proposed to facilitate manual reassembly of 3D fragments, including direct interaction <ref type="bibr" target="#b35">[37,</ref><ref type="bibr" target="#b50">52]</ref>, automatic alignment assistance <ref type="bibr" target="#b38">[40]</ref> and immersive reassembly tools <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b52">54]</ref>. However, most of these techniques assemble the fragment from the scratch, requiring for many efforts from users to find and align the fragments. Recently, Lima-Hernandez et al. <ref type="bibr" target="#b11">[13]</ref> has largely simplified such laborious work by introducing automatic techniques into manual reassembly. They pre-assembled the fragment pairs to derive coarse results before manual correction. However, these previous methods do not provide visualization of the internal match relation between multiple reassembled fragments, which is essential for experts to identify the mismatches during reassembly correction.</p><p>We collaborate with six experts from cultural relics protection to develop our system that facilitates object correction. Considering that the fragments and the reassembling interactions in our situation are inherently three-dimensional, we adopt virtual reality (VR) techniques which enable realistic perception of the 3D objects and direct control over the movement and rotation of the fragments. We identify two challenges in developing the system. First, finding the optimal solution to correct the mismatches is laborious since it requires fre-quent observation and comparison of the shape and match relations among multiple reassembly solutions. The massive reassembly possibilities are overwhelming for experts as they need to determine the reassemblies correctness based on excessive fragment relations and detailed information on the surface. Second, it is difficult to rectify the alignment between multiple fragments. Direct mid-air interaction provided by the immersive system allows users to intuitively transform the fragments <ref type="bibr" target="#b24">[26]</ref> and specify which part of the fragment surface should be matched and aligned. However, when matching multiple fragments, users need to simultaneously control the transformation of each fragment and justify the alignment accuracy of fragments with several factors (e.g. the alignment of detailed surface and the overall appearance). This reduces the direct interaction efficiency as the scale and shape complexity of the fragments increase.</p><p>To overcome these challenges, we propose PuzzleFixer, an semiautomatic interactive system for experts to better visualize and correct the defective reassembled object in the immersive environment. Based on automatic matching techniques, PuzzleFixer provides multi-level exploration and instant feedback, which enables detection of reassembly errors, identification of possible matches, and refinement of detailed alignments. To address the first challenge, PuzzleFixer automatically generates reassembly categories according to matching similarity. Each category provides coarse matching results (e.g. which fragments are matched and their coarse transformation), allowing experts to exclude unreasonable solutions and explore the remaining ones in depth. To ease matches finding, we place the solutions in front of users and order the solutions by their matching similarities. For the second challenge, we augment reassembled objects with 3D node-link network where a node represents a fragment and an edge connects a pair of matched fragments. Based on the network, we design visual and force feedback to help users align the fragments. Specifically, when users are interacting with fragments, PuzzleFixer instantly generates edges between the matching faces on the fragments and guides the fragment transformation by introducing resistance along the edges of the matched faces to prevent these faces from being separated. We demonstrate the usability and effectiveness of our system by conducting two case studies based on a real-world cultural relic restoration. We further collected expert feedback from a semi-structured interview and reported insights and suggestions about our system.</p><p>To summarize, our contributions are listed as follows:</p><p>• We developed PuzzleFixer, an interactive system for defective reassembled object correction, featuring reassembly cluster visualization for solution exploration and instant feedback guidance for efficient alignment. • We conducted case studies on the reassembly of a set of real-world culture relics fragments and collected feedback from six domain experts, which provided insights into reassembly correction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Automatic Reassembly Process for Virtual Fractured Object. Automatic reassembly of fractured objects is a vibrant and complex topic in graphics and cultural heritage research. It aims to calculate the relative position of each fragment model (e.g., 3D meshes converted from physical fragments) and construct the virtual entirety shape in a fully automatic way. Due to the great advantages in terms of efficiency and avoidance of secondary damage, the automatic reassembly process has been extensively studied for various applications <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b60">62,</ref><ref type="bibr" target="#b67">69]</ref>. Papaioannou et al. <ref type="bibr" target="#b41">[43]</ref> systematically presented the pipeline for automatic fragment reassembly, which classified the process into two phases: pairwise matching and multi-part optimization.</p><p>The pairwise matching phase generates a set of potentially correct matches that align pairs of fragments through fracture surfaces. It extracts various geometry features from the fragment models, searches all possible fragment pairs, and provides each pair a score indicating its match quality <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b23">25]</ref>. For example, Huang et al. <ref type="bibr" target="#b23">[25]</ref> used a forward search algorithm to find and measure the fragment pairs based on the sharpness and roughness similarity of faces. Later, Zhang et al. <ref type="bibr" target="#b70">[72]</ref> improved the matching robustness for thin-shell objects by matching fragments against predefined templates before matching search. Recently, a coarse-precise matching strategy <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b67">69]</ref> was proposed, which rapidly excludes a large number of impossible matches based on salient features before searching and aligning fragment pairs precisely. In this work, we leverage various state-of-the-art methods <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b65">67]</ref> depending on the fragment type used in our case studies to generate potential fragment pairs with scores and alignments for manual reassembly correction.</p><p>The multi-part optimization is responsible for determining correct matches and constructing the entire object. Current methods for multipart optimization are generally based on graph optimization. In particular, a reassembly graph is generated where a node represents a fragment and an edge with weight represents a pairwise match and its matching score. Reassembly can be built by finding the maximum cumulative score on edges using the shortest path algorithm <ref type="bibr" target="#b42">[44,</ref><ref type="bibr" target="#b43">45]</ref>. To increase the quality, some of the common constraints, such as loop closure <ref type="bibr" target="#b26">[28]</ref> and non-intersection constraint <ref type="bibr" target="#b23">[25]</ref>, are introduced to ensure stable transformation of fragments without penetration. Building upon the constraints, Zhang et al. <ref type="bibr" target="#b70">[72]</ref> iteratively selected the edge to maximize the score using the greedy-based method. Sizikova et al. <ref type="bibr" target="#b55">[57]</ref> proposed a genetic algorithm to increase the robustness of noisy and incorrect matches. Although these methods are promising, the results are not always reliable due to low quality of fragments caused by inevitable field damage <ref type="bibr" target="#b41">[43]</ref> as well as local optimum problem induced from optimization algorithms. We leverage multi-part optimization approaches to pre-reassemble the object before manual refinement and obtain match relations between fragments. Based on the result, we design efficient visualizations and interactions to help experts identify the problem region and refine the reassembly in the immersive environment. Immersive Interface for Manual Puzzle Solving. Solving the virtual fragment puzzle manually requires users to understand the process of reassembly and accurately control the fragment model in 6 degrees of freedom (DOF). Given the benefits of visualizing and interacting 3D data <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b68">70,</ref><ref type="bibr" target="#b71">73]</ref>, many immersive systems have been developed for 3D reassembly tasks <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b22">24]</ref>. One of the most common approaches for enhancing interaction accuracy is to augment the object with visual cues. For example, Sreng et al. <ref type="bibr" target="#b56">[58]</ref> designed sphere and arrow glyphs with visual effects to indicate real-time distances and angles between objects. Weiß et al. <ref type="bibr" target="#b62">[64]</ref> overlaid the target region with a crosshair glyph and pie chart to show the splicing position and transforming angle while operating objects. Besides visual guidance, some approaches propose the precise mode to improve the accuracy of manual operation <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b48">50,</ref><ref type="bibr" target="#b52">54]</ref>. In contrast to direct interaction with free-hand movement, the precise mode only allows objects to be transformed in one DOF at a time. Apart from the fully manual method, some studies designed the snap effect <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b35">37]</ref> that automatically aligned a fragment pair using geometry processing such as iterative closest point (ICP) <ref type="bibr" target="#b51">[53]</ref>.</p><p>However, these methods do not consider aligning multiple fragments with complex shapes and match relations. As the number of regions on the fragments increases, visualizing and specifying the correct regions to align becomes a challenge. We design instant visual and force feedback according to the relative position of the current fragments to improve the efficiency of manual alignment. Integrating Human Knowledge with Automatic Reassembly techniques. Due to the limitations in the automatic reassemble process and manual matching in practice, semi-automatic tools that integrate human knowledge with matching algorithms have been developed to empower the reassembly performance. One of the integrating strategies is to precompute pairwise matching candidates for users to identify correct matches. For example, Brown et al. <ref type="bibr" target="#b3">[5]</ref> allowed users to iteratively compare and identify matches through small multiples after all candidates were automatically found. Similarly, Deever and Gallagher <ref type="bibr" target="#b12">[14]</ref> visualized the automatically assembled document pieces in order of similarity to facilitate evaluation. Besides, to improve the candidate quality, Palmas et al. <ref type="bibr" target="#b39">[41]</ref> enabled users to define alignment constraint points directly on fragments to instruct the automatic matching process. Yet these approaches are worked in the traditional desktop environment. Recently, 3D Puzzling Engine <ref type="bibr" target="#b11">[13]</ref> visualized precomputed rough alignments of fragment pairs in immersive space. These pairs were augmented with matching lines that served as cues for users to estimate and manually refine the matches.</p><p>While most systems enable users to assemble the unaligned fragments, only a few studies have investigated refining the reassembled object. Refining the reassembly introduces extra tasks, including identifying mismatches in objects and finding the correct reassembly from numerous candidates. We propose PuzzleFixer, which visualizes match relations with an embedded node-link diagram. Our system further enables level-of-detail explorations that visualize and filter the potential reassemblies from coarse match relations to fine shape details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BACKGROUND</head><p>Automatic reassembly approaches generate reassembled objects for expert review and correction. This section introduces the concept of automatic reassembly and describes the data produced by automatic reassembly techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Automatic Reassembly face segmentation match between faces aligned fragments</head><p>Fig. <ref type="figure">2</ref>. A demonstration of the automatic reassembly procedure (models from huang et al. <ref type="bibr" target="#b23">[25]</ref>). Left: The surface of a fragment is segmented into fractured faces (colored) and intact faces (white). Middle: A pairwise match between two fragments. Right: The aligned fragments.</p><p>Automatic reassembly identifies the fractured face of each fragment, and determines fragment relations (which fragments are matched) and their transformation (how the matched fragments align with each other).</p><p>The final result of automatic reassembly is a reassembled object comprising all fragments with their respective transformations determined. Specifically, there are three steps in automatic reassembly, as shown in the following:</p><p>• Step 1: Face Segmentation. The fragment surface is categorized into 1) fractured faces caused by object damage, which are to be matched with other fractured faces; and 2) intact faces, which are used to construct the profile of the original object (Fig. <ref type="figure">2</ref> Left). • Step 2: Pairwise Matching. With the fractured faces, the reassembly procedure finds the potential matches between all pairwise fragments and generates alignment for each match. Here we define a match is an adjacent relation, which refers to a pair of fractured faces of two fragments that can be adjoined against each other (Fig. <ref type="figure">2 Middle</ref>). An alignment is the transform of the matched fragments (Fig. <ref type="figure">2 Right</ref>). It comprises the transformation of the matched fragments to make the fractured faces merge with each other. For each match, a score is recorded to represents the alignment accuracy. • Step 3: Multi-part Optimization. Finally, a reassembly is generated from a set of pairwise alignments by the multi-part optimization approach. Specifically, the procedure gradually selects a pairwise match from the set that maximizes the accumulated matching score, merges the match to the current reassembly while optimizing all the reassembled alignments under certain constraints (e.g, prevent interpenetration). All possible selections construct a reassembly space, which includes all combinations of the fragments with different alignments. It has been proven that search for the optimal reassembly from a reassembling space is a NP-hard problem <ref type="bibr" target="#b13">[15]</ref>. In summary, automatic reassembly provides pairwise matches and alignments with accuracy scores and constructs an entire object from the reassembly space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data Description</head><p>The data produced by automatic reassembly consists of shape and relation data. To visualize and interact with the reassembly from pairwise matches to global alignments, we further differentiate the two kinds of data from different levels.</p><p>Data Type from reassembled objects includes the 3D shape and its corresponding match relations.</p><p>• Shape Data contains information of 3D fragment models that illustrates what the reassembly look like. The shape of a fragment is defined by: 1) a triangle mesh that constructs the surface of the fragment, and 2) the transformation (i.e., position and rotation) of the fragment. Shape data describes fragment faces and alignments, and provides virtual models for experts to visualize and interact with. • Relation Data includes the fragment matches and their corresponding scores, explaining how fragments are reassembled. Previous studies have introduced the concept of graph structures to represent fragment relations, including weighted graphs <ref type="bibr" target="#b23">[25]</ref>, directed multi-graphs <ref type="bibr" target="#b26">[28]</ref> and reassembly graphs <ref type="bibr" target="#b70">[72]</ref>. In our study, we describe fragment relations with a matching graph G = (F, E). Specifically, each node</p><formula xml:id="formula_0">f i ∈ F in the graph denotes a fragment that contains M segmented faces s i m ∈ f i , m = 1, . . . , M.</formula><p>An edge e i m , j n ∈ E represents a match between the m-th face on f i to the n-th face on f j . Each edge e i m , j n includes a score p i m , j n that measures the alignment accuracy between s i m and s j n . Particularly, if a s i m have not yet found its matched fractured face, we denote its edge as e i m ,−1 and assign it a zero score. Therefore, relations in a reassembly can be illustrated by an undirected graph weighted by scores. Data Level decides the level of details that visualizations and interactions can access. The data produced by automatic reassembly ranges from the transformation between fragment pairs in one reassembly to a set of reassemblies. Therefore, we further categorize the data into three levels (Fig. <ref type="figure" target="#fig_0">3</ref>):</p><p>• Candidate Level data consists of a set of possible reassemblies from the reassembly space. Each reassembly is a reassembled object with different matches and alignments. Hence, candidate level data mainly provides an overview of all possible solutions to reassemble the given set of fragments. • Fragment Level data is obtained from a set of reassembled fragments.</p><p>The relation between a fragment pair f i , f j is represented by a match together with the average score on all aligned fractured faces, which is denoted as e i, j and p i, j . Data at this level provides an important basis for correctness judgment of reassembly, such as texture continuity across adjacent surfaces and the score from the inside aligned fractured faces. Therefore, the fragment level data is mainly used for correctness check of a reassembly. • Face Level data comprises both intact and fractured faces on each fragment. Each fractured face is characterized by one score. Unlike automatic matching techniques generally based on geometrical features of fractured faces, human experts further use semantic features (e.g. textures and inscriptions) on intact faces to visualize the reassembly. Reassembling at this level focuses on refining the matches between faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">REQUIREMENT ANALYSIS</head><p>To guide the system design, we first abstract the reassembly tasks into inspection, exploration, and confirmation stages. We then derive experts' requirements in each stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Workflow for Reassembly Optimization</head><p>To clarify the tasks experts need to perform in the correcting process, we conducted a semi-structured interview with two experts in the digital restoration domain. One expert is well experienced in automatic reassembly and digital protection of cultural heritage for over ten years; the other is a professor who specializes in geometry processing and visualization for 3D cultural heritage artifacts.</p><p>In the interview, the experts revealed that optimizing the reassembly relies on the selection of correct matches and interactions for refining alignments. To ensure the efficiency in such process, they conduct analytics tasks on both reassembled object observation and possible solutions searching. Observation task aims to judge the correctness of the reassembly and identify mismatches. Based on the observation, exploration task is conducted to learn the reassembly space and determine potential matches that can be aligned correctly. Subsequently, they conduct alignment task to carefully align the matched fragments by adjusting their transformation through direct manipulation. Completion task is finally performed by automatic reassembly that restores the entire object based on the refined alignments. In addition, they perform all the tasks iteratively since matches changed in one region may lead to changes and even errors in other regions, thus causing a knock-on effect.</p><p>Based on interview results, we propose a three-stage iterative workflow that integrates the tasks with automatic reassembly techniques for reassembly refinement (Fig. <ref type="figure">4</ref>): Inspection Stage: At the beginning of optimization, experts conduct the observation task to identify mismatches from the reassembled object, which is generated by automatic reassembly approaches. The mismatches are unveiled by several features, such as the overall discordant shape appearance, discontinuous surface texture between adjacent fragments, and low matching score measured by feature similarity on matched fractured faces. When mismatches are identified, the expert needs to disconnect the corresponding edges and divides the object into multiple reassembly groups. Exploration Stage: In this stage, the expert performs exploration task to determine possible solutions for mismatches. These solutions can be obtained by traversing all possible alignments between matches. However, the reassembly space might be too large and presenting all matches would be overwhelming. Therefore, this stage allows the expert to efficiently exclude impossible matches and identify a subset of interests from the large reassembly space. Confirmation Stage: With a set of potential matches determined, the expert then needs to confirm the correct matches, refine their alignments, and enter the next reassembly iteration through the alignment and completion tasks. Precisely aligning multiple fragments by direct interaction is a challenge task, given that merging multiple irregular faces simultaneously without destroying other aligned faces. Thus, this stage provides interaction aids to reduce the difficulty and improve alignment accuracy. When experts are satisfied with the current solution, automatic reassembly approaches will precisely align the mismatches that do not covered in the selected candidate and generate the entire object based on the interaction results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Design Requirement Analysis</head><p>Based on the workflow, we further communicated with experts, demonstrated several prototypes of our reassembly system, and collected feedback regarding detailed concerns of each stage. After several rounds of refinement, we distilled six design requirements centered around the three stages to guide the design of our system. Inspection Stage: • I1: Visualize fragment shape with relation data. The shape and Fig. <ref type="figure">4</ref>. The workflow of PuzzleFixer consists of inspection, exploration, and confirmation stages. In the first two stages, automatic approaches provide the data for users to inspect mismatches and explore the reassembly space. When the correct matches are aligned and confirmed in the last stage, the automatic approach restores the entire object.</p><p>relation data are crucial for mismatch observation as they provide match evidence at the fragment level (e.g. how the reassembled object looks like) to justify the reassembly correctness and the face level (e.g. similarity between aligned faces) to find mismatched fractured faces. Thus, the system should integrate fragment shapes with relation data to facilitate the identification of mismatches.</p><p>• I2: Provide intuitive interaction for modifying the matching graph. Interacting with the matching graph is essential to specify and remove the mismatches between fractured faces. However, prior direct interactions on fragment shapes <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b50">52,</ref><ref type="bibr" target="#b52">54]</ref> are not enough to select and remove relation data at face level. Thus, the system should provide intuitive interactions for match relation modification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exploration Stage:</head><p>• E1: Present potential reassemblies based on match evidence. To help experts quickly identify candidates with high matching potential, reassemblies should be organized in a certain order. Since candidates are selected by experts based on match evidence, reassemblies should be ordered by evidence similarity. • E2: Reveal matching patterns of possible reassemblies. To exclude a large number of unreasonable candidates from the reassembling space, experts need to understand overall patterns of the space, such as what types of matches are available and the approximate appearance of each type. Hence, the system should cluster candidates based on similar match evidence, provide a visual summary of each cluster to assist experts with correctness justification and focus on the interested subset of candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Confirmation Stage:</head><p>• C1: Enhance awareness of face relations based on instant feedback. Aligning fragments manually is challenging since face correspondence will changes in real-time with variations in the relative positions of fragments. Experts need to constantly check the face relations to perform appropriate fragment transformation, such as "Which faces are currently facing?" and "Which direction should not be moved to retain aligned faces?" Instant feedback on the face relations can guide experts in fragment alignments and interaction efficiency enhancement <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b56">58,</ref><ref type="bibr" target="#b58">60</ref>]. • C2: Generate precise reassembly according to alignment interactions. Considering the imprecision nature of the direct interactions <ref type="bibr" target="#b36">[38]</ref>, the system should instantly align manually transformed fragments precisely to improve the alignment quality. Moreover, after the expert finishes the confirmation, the system needs to match the mismatches not included in the chosen candidate and update the relation data accordingly to form a complete object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">PUZZLEFIXER: IMPLEMENTATION AND DESIGN</head><p>Based on the requirements, we design and implement PuzzleFixer, a visual reassembly system that enables experts to refine the reassembled object in VR. In this section, we first present a usage scenario of the system to demonstrate the workflow of PuzzleFixer and then introduce the details in each stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Usage Scenario</head><p>We demonstrate how PuzzleFixer can support an expert in fragment puzzle reassembly on the gargoyle dataset from prior work <ref type="bibr" target="#b23">[25]</ref>. The dataset contains 30 3D segments with 98 fracture surfaces and is described by a point cloud with over 3.5 million data points.</p><p>Inspecting erroneous matches. When the expert is wearing the VR headset and controllers, PuzzleFixer immerses the expert in a museum environment and presents each fragment according to the position generated by automatic reassembly approaches (Fig. <ref type="figure" target="#fig_1">5a</ref>). The expert quickly notices that the body (highlighted in yellow dotted box) is not well connected to the head and bottom of the gargoyle statue. To help experts find the specific mismatches, PuzzleFixer presents matches with a node-link diagram that shows matches between faces (I1). The match quality is encoded with a blue-to-green colormap, where a color closer to green indicates better quality. The expert then navigates to the matches between the body and other parts (Fig. <ref type="figure" target="#fig_1">5</ref> b1, b2) and identifies false matches by touching the corresponding edges (I2).</p><p>Exploring potential reassemblies. All fragment combinations that can be reassembled to the body are generated and clustered into several groups (Fig. <ref type="figure" target="#fig_1">5c</ref>) (E2). Each group is represented by an averaged diagram that indicates the coarse positions and matches of fragments within the group. Averaged diagrams are scattered in front of users' view, where adjacent graphs exhibit similarities in positions and matches (E1). Shapes of the body are presented to visualize match positions of the closest diagram. The expert gradually shifts the averaged diagrams to the enlarged body shapes and rotates the shapes to observe the match position and angles from different perspectives. When averaged diagram is not ideal as expected, the expert further shifts the clusters in different directions to search for the others. Through scanning the diagrams, the expert focuses on the upper-right diagram since it exhibits a high match quality and matches all faces on the upper side of the body with regular positions. Thus, he selects the group by clicking the "trackpad" on the controller. Accordingly, PuzzleFixer presents all reassemblies in this group (Fig. <ref type="figure" target="#fig_1">5d</ref>). Each reassembly contains fragments that directly match the body.</p><p>Confirming correct matches. After observing the reassemblies shown in Fig. <ref type="figure" target="#fig_1">5d</ref>, the expert chooses one reassembly which contains headrecognizable fragments. Next, he grabs the fragments and attempts to align their matches (Fig. <ref type="figure" target="#fig_1">5e</ref>). When the expert navigates to the faces, PuzzleFixer displays the overall shapes through a reassemblyin-miniature. The facing faces are instantly connected with hint edges to guide angle and position adjustment for fragment alignment (C1).</p><p>Once the expert complete an alignment attempt, PuzzleFixer precisely aligns the interacted faces. Satisfied with the results, the expert ends the alignment operation. PuzzleFixer automatically reassembles the left fragments and matches and shows the final result (Fig. <ref type="figure" target="#fig_1">5f</ref>) (C2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Inspecting the reassembled object</head><p>At the beginning of the workflow, PuzzleFixer visualizes the reassembled object with relation data to help users identify mismatches.</p><p>Reassembly generation. Prior research on automatic reassembly has proposed different techniques for resembling objects in various shapes and materials. According to the characteristics of the fragments we used in our case study, we adopt multiple computer graphics approaches to generate potential reassemble solutions.</p><p>• We obtain meshes from the 3D point cloud of fragments by screened poison reconstruction <ref type="bibr" target="#b25">[27]</ref>. Based on the meshes, we adopt a robust face and boundary extraction approach <ref type="bibr" target="#b65">[67]</ref> to get the intact and fractured faces of each fragment. • We apply pairwise matching algorithm <ref type="bibr" target="#b30">[32]</ref> to calculate matching scores between all pairwise fragment matches. • With the scores, we adopt multi-part reassembly <ref type="bibr" target="#b41">[43]</ref> to automatically select the matches and use openGR <ref type="bibr" target="#b33">[35]</ref> for alignment and construct a reassembly result. Visualizing and identifying the mismatches. We develop Embed Component to visualize the reassembly with two types of data (I1). As shown in Fig. <ref type="figure" target="#fig_1">5a</ref>, the match between a face pair is presented by a 3D tube that links between the centers of corresponding fragments. In particular, we represent an unmatched face by a tube that links between the face center to its fragment center. The score of each match is encoded by the color of its face, where the color closer to green indicates a higher score. Consequently, Embed Component integrates match relations with fragment shapes with a skeleton metaphor, where a node represents a fragment, and an edge denotes a match between a fragment pair. To reduce the visual clutter of edges, we support users to filter the edges by their scores or direct manipulations. The edge scores are normalized within 0 and 1. The users can define a threshold to filter the edges and focus on unqualified matches. In addition, users can filter edges by direct manipulations through the virtual hands, which allows the system to only show the interesting matches to reduce edge clutter and ease mismatch searching.</p><p>Based on the skeleton metaphor, PuzzleFixer provides three kinds of interactions to support mismatch identification and modification: • Object navigation. PuzzleFixer provides grab and transform gestures that allow users to transform the entire object. The object and its edges will be rotated and shifted synchronously with hand transformation. Users can also freely move in the virtual space to observe the object from different perspectives. • Match modification. Face Component leverages touch gesture to enable users to identify mismatches (I2). The touched edge will be replaced by two disconnected edges that represent the corresponding mismatched faces (Fig. <ref type="figure" target="#fig_1">5b1, b2</ref>). Moreover, the user can touch an intact face to switch the material between translucent or opaque to focus on the edges or surface texture. • Target group selection. When mismatches are specified, the object is divided into several interconnected fragment groups. PuzzleFixer allows users to select a target group via touching gesture. Next, the user can explore how the remaining groups match with it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Exploring reassembly candidates</head><p>Based on the mismatches, PuzzleFixer first generates all possible candidates, then features Layout Component and Cluster Component to support overall reassembly pattern understanding and help users progressively find the correct reassembly, respectively. Candidate generation. Given the groups G = {G 1 , G 2 , ..., G n }, Puz-zleFixer generates matching candidates by permutating the mismatches between the target and other groups:</p><formula xml:id="formula_1">n G i k n G ′ k k!</formula><p>, where G i is the target group and G ′ is the set of remaining groups other than G i . n G i and n G ′ are the number of mismatches in G i and G ′ respectively, k = min(n G i , n G ′ ). We use the mean of pairwise matching scores to indicate the match quality of a candidate. Since the number of candidates grows combinatorially with the increase in the number of mismatches, we set up two rules to exclude impossible matches to ensure generation efficiency: 1) alignments of the permutation matches cannot be achieved by breaking down the other alignments within that group, and 2) fragments should not penetrate into another after alignment. These rules help to eliminate meaningless candidates before user exploration. Layout of candidates. PuzzleFixer applies Layout Component to distribute candidates based on the similarity of their skeleton to facilitate match identification (E1). Specifically, Layout Component 1) calculates Chamfer distance <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b49">51]</ref> between the nodes on the skeletons to represent similarities between candidates, and 2) performs dimensionality reduction using t-SNE <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b59">61]</ref> to obtain candidate distribution on a 2D canvas. Previous studies <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b32">34]</ref> have confirmed the efficiency of exploring 3D small multiples using a flat layout in an immersive setting. Following this method, we place canvas vertically in front of users. We leverage t-SNE to ensure the similarity between neighboring candidates and reveals patterns of the matched shapes. Supported by Layout Component, PuzzleFixer uses the grab and transform gestures to achieve the following interactions at candidate level: • Candidate navigation. After users grab the controller in the midair, all candidates are panned along with the canvas and rotated synchronously according to hand transform. • Candidate selection. Layout Component zooms in on candidate to be selected to perform a focus by context layout. Users can select the enlarged candidate for further in-depth analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Candidate Clustering Candidate Average</head><p>Cluster Layout Visual summary of candidates. After dimensionality reduction, candidate are represented by a scatterplot on a 2D plane. Cluster Component groups candidates and provide a level-of-detail searching approach to prevent the exploration from getting overwhelming (E2). Specifically, Cluster Component uses DBSCAN <ref type="bibr" target="#b15">[17]</ref> to group candidates into clusters. We choose DBSCAN since it has been widely used to show the patterns of points after dimensionality reduction <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b47">49]</ref>. We perform two steps to generate visual summary of candidates (Fig. <ref type="figure" target="#fig_2">6</ref>): 1) generate an average skeleton by averaging each fragment node in each candidate skeleton; 2) place the average skeleton at the mean position of the nodes it refers to. The score of each cluster is averaged and encoded by the corresponding skeleton color. Based on the cluster, Cluster Component supports candidate navigation by 1) categorizing all candidates based on the skeleton similarity to avoid visual overwhelming and 2) providing level-of-detail searching that enables users gradually find candidates according to coarse match relations and specific shape alignments. When users complete cluster selection, both the shape and skeleton of candidates in the clusters are visualized (Fig. <ref type="figure" target="#fig_1">5d</ref>).</p><p>The visual summary of a cluster mainly incorporates three considerations: 1) expressive so that it shows the essential match evidence for experts to determine the potentially correct ones, 2) succinct so that the presented evidence does not require excessive time for judgment, and 3) representative so that it presents the thumbnail of all the candidates in the cluster. To fulfill the expressive and representative criteria, we leverage the average skeleton to present the relation data and show the general form of the clustered candidates. The relative positions between the matching edges (i.e., edges which match the fragments to the target group) provide feedback on which faces are matched. The edge color indicates the quality of the alignment. For the succinctness, we avoid showing the surface of each cluster because it contains many detailed matching evidence (e.g., specific shape alignments and texture continuity) that requires much time for in-depth analysis of each candidate individually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Confirming the matches</head><p>We design Matching Component and Snapping Component to assist with manual alignment (C1) and obtain precise reassemble results (C2). Aligning fragments with instant feedback. PuzzleFixer allows users to align face pairs by grabbing inside the fragment shape and transforming the fragment with the controller. To guide the alignment, Matching Component provides two kinds of feedback (Figure <ref type="figure" target="#fig_3">7</ref>): 1) visual feedback that instantly shows the aligning matches with a 3D dotted tube, which links faces that are currently close and facing each other and matches confirmed by previous alignment attempts; 2) the force feedback that exerts a resistance along with the edges of confirmed matches when these aligned faces are dragged away or the directions in which they are facing are changed. The visual feedback presents instant relations between the aligning faces, help users explore whether the matches are in agreement with their expectation. Instead of providing force feedback from the controller <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b27">29]</ref>, we use the change in the fragment transformation relative to the controller to create a force feedback cue. Specifically, as the user holds a fragment by grabbing the controller, the controller acts as a realistic reference to map its transformation directly onto the fragment without scaling. However, when the fragment is partially matched, those fragment transformations that lengthen the confirmed edges will be applied with a constant decreasing factor. Consequently, the user needs a larger transformation magnitude to move away from the prior matched faces until the distance between the matched faces exceeds a threshold. This decreasing transformation mapping reduces the impact of misoperations (e.g., matches new faces but breaks previous ones), while creating a perception that the fragments are dragged by the edges.</p><p>Reassemble fragments with precise alignment. After the match is confirmed, Snapping Component uses a magnetic metaphor to automatically align the corresponding faces. We use the global registration technique <ref type="bibr" target="#b33">[35]</ref> to perform a precise and fast alignment according to the meshes of matched face pairs. After all matches are confirmed, PuzzleFixer calls the automatic method to reassemble left mismatches and starts next iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Implementation</head><p>The PuzzleFixer is implemented in a server-client architecture. The client provides an immersive environment for users to perform the reassembly workflow. We use Unity and HTC VIVE VR system with two hand controllers to set up the immersive environment and express the hand gestures. The server is built with Python for dimensionality reduction, clustering, and automatic matching methods. Specifically, we use scikit-learn <ref type="bibr" target="#b44">[46]</ref> to generate clusters. These task are performed asynchronously to ensure interaction fluency. ZeroMQ [1] is applied to send and receive messages between the server and client.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERT EVALUATION</head><p>We invited six domain experts to evaluate PuzzleFixer on a real-world cultural relics dataset. We report the study settings, two cases of reassembling process conducted by the experts, and the qualitative feedback and insights from the post-study expert interviews. We didn't perform quantitative analysis due to the limited sample size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Study Settings</head><p>Dataset and preprocessing. The dataset is digitized from Ma Lin Broken Stele, engraved with calligraphy on its surface, and has high artistic and historical value. The dataset consists of ten broken stones of various sizes and in irregular shapes, with a total of 33 fracture surfaces.</p><p>All the stone fragments are scanned and converted to Wavefront format files for 3D model presentation. We apply surface segmentation <ref type="bibr" target="#b65">[67]</ref> and pairwise matching <ref type="bibr" target="#b30">[32]</ref> to obtain an initial reassemble result with a matching structure and matching score.</p><p>Participants. We invited six experts (P1-P6; age: 23-52) from the university and museum to participate in our interview. One of the experts is a professor engaging in the field of restoration of digital artifacts. Another expert is a researcher who has experience in archaeology and cultural heritage protection for over 20 years. Other experts major in digital artifact modeling and restoration. All the experts have experience in manual reassembly in virtual reality. Only the professor participated in our previous workflow discussion.</p><p>Procedure. We conducted one-on-one interviews with the six experts. The interview included four parts: 1) We introduced the concepts of three-stage workflow and six components of PuzzleFixer, following a system demonstration to present the process of reassembly refinement (20 minutes). 2) After the introduction, we asked them to set up the VR headset and repeat the process in the demonstration to get familiar with the system (15 minutes). 3) When experts were confident in operating the system, we changed the dataset to the Ma Lin Broken Stele and encouraged the experts to reassemble the fragments (20 minutes). 4)</p><p>The procedure concluded with a semi-structured interview, including the subjective perception, evaluation of workflow and each system component, and suggestions for improvement (30 minutes).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Case Studies</head><p>Case I: Finding Correct Matches from Series of Errors. This case produced by P1 focuses on identifying mismatches and searching for correct solutions.</p><p>Inspecting the reassembly fragments. After importing the dataset, the expert glanced at the reassembled object and quickly noticed the discordant fragment in the upper right area through the mismatched edges and the blue fractured faces (Fig. <ref type="figure" target="#fig_4">8a</ref>). The skeleton showed that there were several mismatched faces on f 1 . Thus, he dragged the object closer to disconnect e 1,4 and navigated to the bottom half of the stele (highlighted in the blue dotted box) for detailed observation. Through the inscription letters and the decorations on the surfaces (Fig. <ref type="figure" target="#fig_4">8 b1</ref>), the expert found that their faces (s 2,1 and s 3,1 ) were discontinuous with their adjacencies (s 4,1 and s 5,1 ). Thus, he considered edges e 2,4 and e 3,7 were incorrect and disconnected them accordingly (Fig. <ref type="figure" target="#fig_4">8 b2, b3</ref>).</p><p>After identifying these error matches, he chose the largest group (Fig. <ref type="figure" target="#fig_4">8c</ref>) as the target group and entered the exploration phase to see the possible solutions.</p><p>Exploring the candidates. As shown in Fig. <ref type="figure" target="#fig_5">9</ref> a1, the system presented the clusters of reassembly results in a plane canvas. The expert noticed that the overall results showed two forms: all the fragments were concentrated below the target (upper area of the canvas) or matched at the lower and the upper right area, respectively. Since the initial result was presented in the latter form, the expert was curious about what all the other fragments matched below would look like. Through scanning those clusters, he noticed that all the mismatched edges at the bottom of the skeleton sk 1 had corresponding edges with reasonable matching angles. Hence he selected it for further exploration. As shown in Fig. <ref type="figure" target="#fig_5">9</ref> a2, the candidates in this category matched f 2 and f 3 to the same position while enumerating a variety of matching possibilities for f 1 . After panning and rotating the candidates, the expert quickly ruled out candidates whose appearances were obviously incorrect and selected candidates c 1 and c 2 to explore their details. Confirming the matches. The filtered candidates were placed side-byside (Fig. <ref type="figure" target="#fig_5">9 b1</ref>). To identify the correct one, the expert hid the skeleton and focused on the surface around the matching regions. He recognized that the decoration on faces s 5,1 and s 1,1 was continuous in c 1 (Fig. <ref type="figure" target="#fig_5">9</ref> b2) but not in c 2 (Fig. <ref type="figure" target="#fig_5">9 b3</ref>). Hence, he confirmed that f 1 and f 5 in c 1 were matched correctly and refined their alignment following the visual feedback (Fig. <ref type="figure" target="#fig_5">9 b4</ref>). After the expert finished the alignment refinement, PuzzleFixer automatically aligned the remaining fragments and entered the inspection stage in the next iteration. Case II: Assembling Fragments with Manual Matching. In this case, P3 completed the refinement mainly based on the exploration and confirmation stage. Finding the potential matching shapes. At the beginning, the object was composed by two separate groups, g 1 and g 2 (Fig. <ref type="figure" target="#fig_6">10a</ref>). The expert noted that the right part of g 1 seemed neat, while the fragment f 4 had several mismatched edges. Thus, he disconnected f 4 from g 1 and selected the remaining right part as the target to explore the matching opportunities. As shown in Fig. <ref type="figure" target="#fig_6">10b</ref>, the Layout Component showed two general matching styles: the clusters inside the gray circle tended to concentrate the matching position on the left surface of the target group, while the rest matched more fragments at the bottom of the target group. The expert was interested in how to match the target from the left. Thus he focused on the candidates in the circle and chose cluster sk 5 , where all the left-mismatched edges in the target had corresponding matches. The selected candidates showed that their shapes were similar, and they only differed in the correspondence of the faces (Fig. <ref type="figure" target="#fig_6">10c</ref>). Therefore, by default, he selected c 1 with the highest matching score to manually align the matches. Correcting the matches with transform level interactions. As shown in Fig. <ref type="figure" target="#fig_6">10d</ref>, the candidate contained three groups. The expert started with the match between g 1 and g 2 . By grabbing the group, the matching cues were visualized to inform the instant matching face and indicate how to transform the fragments (Fig. <ref type="figure" target="#fig_6">10e</ref>). Thus, he aligned the matches by dragging g 1 according to the cues to refine the alignment. Next, to refine the alignment of g 3 , the expert observed the details on the surface and realized that g 3 was matched incorrectly. Hence, he decided to turn g 3 around for alignment. To this end, he first confirmed the match e 1,2 by dragging f 2 closer to f 1 along the matching cue line (Fig. <ref type="figure" target="#fig_6">10f</ref>). To finish the alignment for f 3 , he rotated g 3 around the aligned edge (e 1,2 ). The force feedback was imposed along the matched edge to ease the rotation by preventing tilting or stretching the edge. When he finished fragment transforming and released the grab, Snapping Component immediately aligned f 1 and f 2 together. After alignment, the expert was satisfied with the match (Fig. <ref type="figure" target="#fig_6">10g</ref>) and reviewed the result: the stele was relatively complete, while some fragments were missing between f 1 and f 2 in this stele dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Observations and Expert Feedback</head><p>Time performance and usability. All participants could successfully correct the reassembly with a maximum of four iterations. The average completion time of the participants was 14.3 minutes (1.1 min, 1.2 min, and 1.4 min in each stage, respectively). Note that we see these times as qualitative evidences due to the limited sample size. We found that the participants did not take much time during the exploration stage as they tended to roughly select several candidates rather than one for subsequent in-depth analysis. The confirmation stage is timeconsuming as most of the participants needed to work back and forth to confirm the alignment correctness when transforming the fragments. Nevertheless, the participants were able to refine several mismatches within an object in one iteration within a few minutes. As for the usability, the participants thought that the system was easy to use and the interaction was intuitive, suggesting that "the graph modification of VR operation is not difficult" (P1) and "these operations are just a matter of habit" (P5). Some experts criticized that the system should be improved by adding, for example, "more remarks to explain the operations" (P2) and "fallback function to reselect the candidates" (P3). We take these suggestions for future improvements.</p><p>Refine the reassembly based on suggestions. We were interested in the comments about integrating automatic reassemble results into manual reassembly refinement. Overall, the experts confirmed the usefulness of showing potential reassemblies instead of trying to find these solutions manually by trial-and-error. This is because that automatically generated solutions "can show coarse results for me to evaluate the matching correctness" (P1, P2) and "provide guidance for manually aligning fragments" (P4-P6). We observed in our case study that although participants can freely align fragments in the confirmation stage, they prefer to make a minor modification to the generated reassembly in most cases. A main reason is that fragment alignment requires users to navigate and observe details while controlling fragment transformation with mid-air interactions. A low-quality candidate may cause users to make multiple alignment attempts, which is prone to arm fatigue <ref type="bibr" target="#b53">[55,</ref><ref type="bibr" target="#b57">59]</ref>. If satisfying candidates were not filtered out, the participants would tend to find and reselect good candidates in next iteration, instead of aligning a worse candidate. This finding suggests that experts' interactions tend to rely on automatically generated results. Assist relation understanding through network embedding. All experts spoke highly of the network design and showed enthusiasm for observing the matching results at the inspection stage. P4 thought the network was intuitive and helpful in enhancing understanding of the current matching structure. For example, "It intuitively shows match relations" (P3) and "I can intuitively see whether there is a match relationship between key faces" (P2). All experts confirmed that they could find erroneous matches through visualization of reassembled objects. Nonetheless, we observed that users still spent much time on navigation. For example, our participants frequently walked around to observe an object from different perspectives and switched view between the global appearance and detailed faces. Despite that the experts believe this real-world-like navigation is intuitive and useful, its efficiency might be affected by structural complexity of objects. However, how to improve the navigation of 3D structures with complex fragment shapes and relations remains unclear. Improve search efficiency with the level of detail selection. The experts highlighted the effectiveness of exploring solutions with twolevel filtering. They thought the clusters showed the matching cues could eliminate a large number of mismatches and guide subsequent selections. P5 mentioned, "when I observed this cluster, I thought it would be correct if edges were rotated at the bottom a little bit. So I will focus on these intended matches in subsequent searches." Although P6 thought that selecting the optimal reassembly takes time due to complexity of the shapes and the large number of reassembling possibilities, he thought the layout of each reassembly would improve the search efficiency since it "provides the trend of matches." While adding more visual hints can further ease the searching process, we leave this for future improvements to avoid overwhelming users in the current implementation. Reduce interaction laborious with instant feedback. When asked how instant feedback could assist manual alignment in the confirmation stage, experts reported that instant feedback immensely helped increase interaction efficiency because "I instantly know which faces are currently being aligned." (P5) P1 praised the force feedback as he said "It limits incorrect fragment transformation" and "helps alleviate mis-operations." P6 mentioned that the force feedback allowed him to align multiple faces sequentially instead of all of them at once, since the force added on aligned matches "guides me to align remaining faces." Suggestions. The experts in our case study pointed out several limitations of our system. First, some fractured surfaces were not detected in preprocessing. To this end, area selection tools should be provided, such as the lasso of mesh surfaces, to help users manually add or modify areas of fractured surfaces. Second, automatic alignment from Snapping Component was sometimes inaccurate, which affects subsequent matching of surrounding fragments. However, many state-of-the-art point cloud registration algorithms <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b40">42]</ref> cannot perform the mesh alignment within response time or suffer from limitations in alignment of arbitrary and damaged surface pairs. To solve these problems, we further exploit a precise mode of direct interaction <ref type="bibr" target="#b52">[54]</ref> for manual alignment improvement. This interaction allows users to gradually adjust the fragment transformation from one DOF in turn. Third, the system currently does not support fragment annotation, which easily leads to confusion between fragments with a similar appearance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION</head><p>Significance-incorporating human insights into solving the NPhard searching problem. Restoration of important objects is a prerequisite to manifest their value. It's challenging to find a unique solution from numerous reassembly possibilities due to the huge reassembly space, which causes a NP-hard problem. While various automatic approaches have been proposed to solve the problem, the result fails when objects are diverse, incomplete, and low-quality <ref type="bibr" target="#b22">[24]</ref>. We explore this direction by incorporating human insights in facilitating the reassembly searching. We further developed PuzzleFixer that visualizes 3D fragments with relational networks and provides instant feedback to facilitate manual reassembly. Our system is evaluated through case studies on real cultural relics, which shows the usefulness and efficiency in reassembly refinement in real-life scenarios. Generalizability-from cultural relic reassembly to educational demonstration. Although PuzzleFixer is evaluated on thick stele blocks, the reassembled objects can be extended to types of data such as thin-shell pottery and shredded paper in manuscripts or images. Since our reassembly workflow is independent of automatic matching method, various types of data can be imported to our system once corresponding method is well prepared. Our reassembly environment allows users to immerse themselves in a virtual setting and experience object reassembly, which can be used for cultural relic restoration and educational purposes. These adaptations allow our system to be used in other scenarios, such as experiencing cultural heritage protection <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b29">31]</ref> and understanding the complex anatomical structure <ref type="bibr" target="#b46">[48,</ref><ref type="bibr" target="#b64">66,</ref><ref type="bibr" target="#b69">71]</ref>. Potentiality-towards more efficient physical reassembly. Reassembling fragments in virtual reality generates reassembly steps and offers guidance for restorers when conducting the physical reconstructions <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b37">39]</ref>. However, in practice, restorers need real-time guidance that provides in-situ aligning guidance while operating fragment reassembly. Guiding physical reassembly with a traditional 2D diagram cannot instantly show reassembling cues regarding the current perspective of restorers and fragment situation. While AR has prospects in crafting and objects assembly by embedding instructions into the physical context <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" target="#b61">63,</ref><ref type="bibr" target="#b62">64]</ref>, AR-assisted physical reassembly in-situ raises new challenges such as reassembly simulation, planning, and designing of collaborative interaction <ref type="bibr" target="#b54">[56,</ref><ref type="bibr" target="#b63">65]</ref>. We hope our exploration will inspire future work in this direction. Study limitations. Although PuzzleFixer leverages levels of selection to help users find solutions efficiently with tens of fragments, identifying the optimal reassembly from tens of thousands of candidates is not supported due to the heavy burden of candidate generation and exploration. To avoid such a situation, we limit the number of mismatches and fragment groups specified at the inspection stage to no more than 10 and 5, respectively. The other wrong matches inside each group can be further corrected in next iteration. We also select the top-k clusters or candidates with the highest average matching score for presentation. A key observation is that the candidates with a low average score are usually incorrect. We set k to 15 in our case study, considering the complexity of the shape and structure. Besides, we haven't considered the situation where the automatic approaches completely fail to reassemble the fragments into a single object. In this case, it is difficult for automatic approaches to provide potentially correct candidates. This makes it difficult for users to take advantage of automatically generated solutions, and fragment refinements can only be performed manually. Finally, our system has considered only two types of data, i.e., the shape data and the relation data. More rich contexts, such as the physical location of each fragment and classification information <ref type="bibr" target="#b66">[68]</ref>, can also promote experts' judgment and objects reassembly. However, these new types of data introduce challenges for new visualization designs, which naturally require further studies. We plan to open source the manual interaction part of our system to the community<ref type="foot" target="#foot_0">1</ref> and involve more information to facilitate reassembly refinement in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>In this work, we present a virtual reassembly system, PuzzleFixer, to help experts refine the reassembled object in an immersive environment. Given a pre-assembled object, our system provides a three-stage workflow for users to inspect the mismatches, explore potential solutions for reassembly, and confirm the alignments. To facilitate mismatch inspection, PuzzleFixer visualizes 3D fragments with an embedded network to augment the fragment shapes with match relations. Once users identify the mismatches, PuzzleFixer generates and clusters potential solutions to provide level-of-detail explorations to facilitate reassembly searching. Moreover, we design visual and force feedback to improve manual alignment. We evaluate our system by conducting case studies on real cultural relics fragments. Insights and expert feedback illustrate the usefulness and effectiveness of the system and enlighten future research opportunities.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>𝒆Fig. 3 .</head><label>3</label><figDesc>Fig.3. Three levels for characterizing the reassembling data (models from huang et al.<ref type="bibr" target="#b23">[25]</ref>). The candidate level includes all possible combinations of fragments. The fragment level data serves a fragment as a unit and reveals the quality through fragment relation scores (numbers on the edges). The face level data serves one face relation as a unit, shows which faces are used to align the fragments (e.g. f 5 aligns with f 3 with 2 faces, and f 6 has one mismatched face).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Reassembling the gargoyle statue. (a) The fragments are automatically reassembled and visualized with the embedded node-link network. (b1, b2) The user modifies the error matches with hand gestures. Based on the modification, the possible matches are clustered and presented through summarized skeleton diagrams (c). Shapes of the reassemblies from the selected cluster are visualized in (d). In (e), the user manually aligns the faces. The other fragments are reassembled automatically, as shown in (f).</figDesc><graphic url="image-27.png" coords="6,372.34,115.47,89.06,78.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. The visual summary of candidates. Clusters are first identified by the layout position of each candidate and then visualized through the average skeleton. The cluster layout follows the rules from Layout Component and presents the target group shape of the cluster at the canvas center for focused observation.</figDesc><graphic url="image-29.png" coords="6,309.62,420.24,124.34,80.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The demonstration of two instant feedback. (a) visual feedback instantly shows a facing match. (b) force feedback adds the resistance (red arrow) along with the edge of a confirmed match.</figDesc><graphic url="image-31.png" coords="6,430.92,420.24,124.34,80.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Identify the mismatches. (a) The skeleton reveals the mismatches from f 1 . (b1) The faces show s 4,1 and s 5,1 are inscribed faces, which are not matched with s 2,1 and s 3,1 . (b2-b3) the mismatch e 2,4 is disconnected through touch gestures. (c) a target group is selected.</figDesc><graphic url="image-33.png" coords="7,316.62,49.50,250.37,85.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Explore and confirm the matches. (a1) Clusters show two patterns of the matching location. (a2) Candidates present the result with shapes. (b1) Two candidates selected by the expert. (b2-b3) Decorations on the surface show the continuity of the neighbor fragments. (b4) Align the matches by the grab and transform gesture.</figDesc><graphic url="image-34.png" coords="7,321.76,404.67,237.86,137.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Finishing the stele reassembly. (a) The skeleton and shapes shows two separate groups. (b) The clusters reveal that other fragments can be matched at bottom or left side of the target group. (c) The candidate in the selected cluster has the same appearance but different match relations. (d) The selected candidate has three groups. (e) Visual cues are shown in real-time when transforming the fragment groups. (f) Force cue is added on the e 1,2 to help rotate the g 3 to align f 3 . (g) The reassembled stele after finishing the workflow.</figDesc><graphic url="image-35.png" coords="8,45.00,49.50,513.02,218.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>• S. Ye, X. Chu, and Y. Wu are with the State Key Lab of CAD&amp;CG, Zhejiang University. They are also with the Laboratory of Art and Archaeology Image (Zhejiang University), Ministry of Education, China. E-mail: {sn ye, xiangtongchuu, ycwu}@zju.edu.cn. • Z. Chen is with John A. Paulson School of Engineering and Applied Sciences, Harvard University, Cambridge, MA. E-mail: ztchen@seas.harvard.edu. • K. Li and G. Geng are with Northwest University. E-mail: {likang, ghgeng}@nwu.edu.cn. • J. Luo and Y. Li are with Xi'an Beilin Museum. E-mail: ljuntong@foxmail.com, liyi beilin@163.com. • Y. Wu and K. Li are the co-corresponding authors. Manuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication xx xxx. 201x; date of current version xx xxx. 201x. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org.</figDesc><table /><note>Digital Object Identifier: xx.xxxx/TVCG.201x.xxxxxxx</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/PuzzleFixer/PuzzleFixer</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The work was supported by NSFC (62072400), the National Key Research and Development Program of China (No.2019YFC1521102), and the Collaborative Innovation Center of Artificial Intelligence by MOE and Zhejiang Provincial Government (ZJU).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Touching data with PropellerHand</title>
		<author>
			<persName><forename type="first">A</forename><surname>Achberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Heyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vidackovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sedlmair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Visualization</title>
				<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Parametric Correspondence and Chamfer Matching: Two New Techniques for Image Matching</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Barrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Joint Conference on Artificial Intelligence</title>
				<meeting>the 5th International Joint Conference on Artificial Intelligence<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1977">1977</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="659" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Solving Jigsaw Puzzles With Eroded Boundaries</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bridger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Danon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2020-06">jun 2020</date>
			<biblScope unit="page" from="3523" to="3532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tools for virtual reassembly of fresco fragments</title>
		<author>
			<persName><forename type="first">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Laken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dutré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rusinkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weyrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of heritage in the digital era</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="329" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Archaeo Puzzle: An Educational Game Using Natural User Interface for Historical Artifacts</title>
		<author>
			<persName><forename type="first">N</forename><surname>Capece</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Erra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gruosso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Anastasio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Workshop on Graphics and Cultural Heritage. The Eurographics Association</title>
				<editor>
			<persName><forename type="first">M</forename><surname>Spagnuolo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Melero</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Virtual Training: Learning Transfer of Assembly Tasks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Vance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Luse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="770" to="782" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">MARVisT: Authoring Glyph-Based Visualization in Mobile Augmented Reality</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2645" to="2658" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Augmenting Static Visualizations with PapARVis Designer</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI &apos;20: CHI Conference on Human Factors in Computing Systems</title>
				<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">April 25-30, 2020. 2020</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exploring the design space of immersive urban analytics</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Informatics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="132" to="142" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">LassoNet: Deep Lasso-Selection of 3D Point Clouds</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="195" to="204" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">TIVEE: Visual Exploration and Explanation of Badminton Tactics in Immersive Visualizations</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="118" to="128" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Hybrid Approach to Reassemble Ancient Decorated Block Fragments through a 3D Puzzling Engine</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lima-Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vergauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">16</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-automatic assembly of real cross-cut shredded documents</title>
		<author>
			<persName><forename type="first">A</forename><surname>Deever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gallagher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 19th IEEE International Conference on Image Processing</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="233" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Demaine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Demaine</surname></persName>
		</author>
		<title level="m">Jigsaw Puzzles, Edge Matching, and Polyomino Packing: Connections and Complexity. Graphs and Combinatorics</title>
				<imprint>
			<date type="published" when="2007-02">feb 2007</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="195" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Solving archaeological puzzles</title>
		<author>
			<persName><forename type="first">N</forename><surname>Derech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shimshoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page">108065</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, KDD&apos;96</title>
				<meeting>the Second International Conference on Knowledge Discovery and Data Mining, KDD&apos;96</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="226" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">RestoreVR: Generating Embodied Knowledge and Situated Experience of Dunhuang Mural Conservation via Interactive Virtual Reality</title>
		<author>
			<persName><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Supporting Analysis of Dimensionality Reduction Results with Contrastive Learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fujiwara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="45" to="55" />
			<date type="published" when="2020-01">jan 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning How to Match Fresco Fragments</title>
		<author>
			<persName><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Toler-Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Castañeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dobkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rusinkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weyrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Computing and Cultural Heritage</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011-11">nov 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The Perfect Match: 3D Point Cloud Matching with Smoothed Densities</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Gojcic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Andreas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on computer vision and pattern recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5545" to="5554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">New guidance for using t-SNE: Alternative defaults, hyperparameter selection automation, and comparative evaluation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cadalzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Leiby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zaitzeff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>Visual Informatics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">FlowNet: A Deep Learning Framework for Clustering and Selection of Streamlines and Stream Surfaces</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1732" to="1744" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Puzzling Engine: a Digital Platform to Aid the Reassembling of Fractured Fragments. ISPRS -International Archives of the Photogrammetry</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D L</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vincke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bassier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mattheuwsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Derdeale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vergauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing and Spatial Information Sciences</title>
		<imprint>
			<biblScope unit="volume">4215</biblScope>
			<biblScope unit="page" from="563" to="570" />
			<date type="published" when="2019-08">Aug. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reassembling Fractured Objects by Geometric Matching</title>
		<author>
			<persName><forename type="first">Q.-X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Flöry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gelfand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pottmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="569" to="578" />
			<date type="published" when="2006-07">jul 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Digital restoration of fragmentary human skeletal remains: Testing the feasibility of virtual reality</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jurda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Urbanová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chmelík</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of forensic and legal medicine</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="50" to="57" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Screened Poisson Surface Reconstruction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2013-07">jul 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">JigsawNet: Shredded Image Reassembly Using Convolutional Neural Network and Loop-Based Composition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4000" to="4015" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pseudohaptic feedback: can isometric input devices simulate force feedback?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lécuyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Coquillart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kheddar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Coiffet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Virtual Reality</title>
				<meeting>IEEE Virtual Reality</meeting>
		<imprint>
			<date type="published" when="2000">2000. 2000</date>
			<biblScope unit="page" from="83" to="90" />
		</imprint>
	</monogr>
	<note type="report_type">Cat. No.00CB37048</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Henry Lee&apos;s crime scene handbook</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Palmbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Computing for Chinese Cultural Heritage</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Informatics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pairwise Matching for 3D Fragment Reassembly Based on Boundary Curves and Concave-Convex Patches</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="6153" to="6161" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Towards an Understanding of Situated AR Visualization for Basketball Free-Throw Training</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nobre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, CHI &apos;21</title>
				<meeting>the 2021 CHI Conference on Human Factors in Computing Systems, CHI &apos;21</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Design and Evaluation of Interactive Small Multiples Data Visualisation in Immersive Spaces</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Prouzeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dwyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="588" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">OpenGR: a 3D Global Registration Library</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mellado</surname></persName>
		</author>
		<ptr target="https://github.com/STORM-IRIT/OpenGR" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Super 4PCS Fast Global Pointcloud Registration via Smart Indexing</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mellado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Aiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="205" to="215" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Semi-Automatic Geometry-Driven Reassembly of Fractured Archeological Objects</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mellado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Reuter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schlick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Virtual Reality, Archaeology and Cultural Heritage, VAST&apos;10</title>
				<meeting>the 11th International Conference on Virtual Reality, Archaeology and Cultural Heritage, VAST&apos;10<address><addrLine>Goslar, DEU</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="33" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The Benefits of DOF Separation in Mid-Air 3D Object Manipulation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Relvas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jorge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology, VRST &apos;16</title>
				<meeting>the 22nd ACM Conference on Virtual Reality Software and Technology, VRST &apos;16</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="261" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A Comparison of Virtual and Physical Training Transfer of Bimanual Assembly Tasks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Murcia-López</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Steed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1574" to="1583" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Snap-to-fit, a haptic 6 DOF alignment tool for virtual assembly</title>
		<author>
			<persName><forename type="first">P</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nysjö</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Carlbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 World Haptics Conference (WHC)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="205" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A computer-assisted constraint-based system for assembling fragmented objects</title>
		<author>
			<persName><forename type="first">G</forename><surname>Palmas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pietroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cignoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scopigno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 Digital Heritage International Congress (DigitalHeritage)</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Iterative Global Similarity Points : A robust coarse-to-fine integration solution for pairwise 3D point cloud registration</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">From Reassembly to Object Completion: A Complete Systems Pipeline</title>
		<author>
			<persName><forename type="first">G</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schreck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Andreadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mavridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sipiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vardis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Computing and Cultural Heritage</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017-03">mar 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Image reassembly combining deep learning and shortest path problem</title>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Paumard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018</title>
				<editor>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="155" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deepzzle: Solving Visual Jigsaw Puzzles With Deep Learning and Shortest Path Optimization</title>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Paumard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3569" to="3581" />
			<date type="published" when="2020-01">jan 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Scikitlearn: Machine learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A Survey of Geometric Analysis in Cultural Heritage</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pintus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weyrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gobbetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rushmeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
				<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="4" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Supporting Anatomy Education with a 3D Puzzle in a VR Environment -Results from a Pilot Study</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pohlandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Preim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Saalfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Mensch Und Computer</title>
				<meeting>Mensch Und Computer</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="91" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">matExplorer: Visual Exploration on Predicting Ionic Conductivity for Solid-state Electrolytes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="75" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Interactive Hand Gesture-based Assembly for Augmented Reality Applications</title>
		<author>
			<persName><forename type="first">R</forename><surname>Radkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stritzke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 International Conference on Advances in Computer-Human Interactions</title>
				<meeting>the 2012 International Conference on Advances in Computer-Human Interactions</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="303" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Reizenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08501</idno>
		<title level="m">Accelerating 3D Deep Learning with PyTorch3D</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Archeo-TUI-Driving Virtual Reassemblies with Tangible 3D Interaction</title>
		<author>
			<persName><forename type="first">P</forename><surname>Reuter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Riviere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Couture</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mahut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Espinasse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Computing and Cultural Heritage</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010-10">oct 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Efficient Variants of the ICP Algorithm</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rusinkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Third International Conference on 3-D Digital Imaging and Modeling</title>
				<meeting>Third International Conference on 3-D Digital Imaging and Modeling</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="145" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">VR System for the Restoration of Broken Cultural Artifacts on the Example of a Funerary Monument</title>
		<author>
			<persName><forename type="first">P</forename><surname>Saalfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Böttcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Klink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Preim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE Virtual Reality and 3D User Interfaces (VR)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="739" to="748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Augmented Reality Map Navigation with Freehand Gestures</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Satriadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cordeil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Czauderna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Willett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="593" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Sereno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Besancon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Mcguffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Isenberg</surname></persName>
		</author>
		<title level="m">Collaborative Work in Augmented Reality: A Survey. IEEE Transactions on Visualization and Computer Graphics</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Wall Painting Reconstruction Using a Genetic Algorithm</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sizikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Computing and Cultural Heritage</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017-12">dec 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Using Visual Cues of Contact to Improve Interactive Manipulation of Virtual Objects in Industrial Assembly/Maintenance Simulations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sreng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lecuyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Megard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Andriot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1013" to="1020" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Natural multimodal interaction in immersive flow visualization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Informatics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="56" to="66" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dragicevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schmalstieg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tabard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A M</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Willett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="185" to="220" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
	<note>Situated Analytics</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Visualizing Data using t-SNE</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Virtual Reassembly of Fractured Bones for Orthopedic Surgery</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on Virtual Reality and Visualization (ICVRV)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="20" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A comprehensive survey of augmented reality assembly research</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Nee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Manufacturing</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Revisited: Comparison of Empirical Methods to Evaluate Visualizations Supporting Crafting and Assembly Purposes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Weiß</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Angerbauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Voit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schwarzl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sedlmair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1204" to="1213" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Analytic Review of Using Augmented Reality for Situational Awareness</title>
		<author>
			<persName><forename type="first">J</forename><surname>Woodward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ruiz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A survey of volume visualization techniques for feature enhancement</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Informatics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="70" to="81" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Robust surface segmentation and edge feature lines extraction from fractured fragments of relics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Design and Engineering</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="79" to="87" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Classification of 3D terracotta warriors fragments based on geospatial and texture information</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visualization</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="259" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Matching Method of Cultural Relic Fragments Constrained by Thickness and Contour Feature</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Mingquan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Pengfei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Guohua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="25892" to="25904" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">ShuttleSpace: Exploring and Analyzing Movement Trajectory in Immersive Visualization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="860" to="869" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">VeLight: A 3D virtual reality tool for CT-based anatomy teaching and training</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ouwerling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Svetachov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Van Ooijen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kosinka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visualization</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="293" to="306" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">3D Fragment Reassembly Using Integrated Template Guidance and Fracture-Region Matching</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Manhein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Waggenspack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2138" to="2146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Metaverse: Perspectives from graphics, interactions and visualization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>Visual Informatics</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
