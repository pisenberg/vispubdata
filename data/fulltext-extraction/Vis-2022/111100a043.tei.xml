<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ErgoExplorer: Interactive Ergonomic Risk Assessment from Video Collections</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Manlio</forename><surname>Massiris</surname></persName>
							<email>mmassiris@uninorte.edu.co</email>
						</author>
						<author>
							<persName><forename type="first">Fern</forename><surname>Ández</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sanjin</forename><surname>Radoš</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Krešimir</forename><surname>Matković</surname></persName>
						</author>
						<author>
							<persName><forename type="first">M</forename><forename type="middle">Eduard</forename><surname>Gr</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Claudio</forename><surname>Delrieux</surname></persName>
						</author>
						<author>
							<persName><forename type="first">•</forename><surname>Manlio</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Massiris</forename><surname>Fernández</surname></persName>
						</author>
						<author>
							<persName><forename type="first">•</forename><forename type="middle">M Eduard</forename><surname>Gröller</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Departamento de Ing. Electrica y Computadoras</orgName>
								<orgName type="institution" key="instit1">Universidad Nacional del Sur</orgName>
								<orgName type="institution" key="instit2">Escuela de Ingenierías Industriales</orgName>
								<orgName type="institution" key="instit3">Universidad de Extremadura</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Departamento de Ing. Electrica y Computadoras</orgName>
								<orgName type="institution">Universidad Nacional del Sur</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ErgoExplorer: Interactive Ergonomic Risk Assessment from Video Collections</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-03-27T09:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Ergonomic assessment</term>
					<term>workplace safety</term>
					<term>visual analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. 1. A dashboard to support the analysis of complex relationships between risk assessments of individual body parts over long sessions that span single or multiple operations. Ergonomists can understand how to mitigate ergonomic risk by using coordinated multiple views (CMV) (a) ErgoView, (b) ErgoTimeline, (c) scatter plot matrix, and (d) parallel coordinates.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recent advancements in smart factories, via Industry 4.0 (I4.0), raised generalized requirements concerning a more thorough analysis of workers' activities in several settings. These include manufacturing, process industries, and construction, to mention just a few <ref type="bibr" target="#b18">[19]</ref>. Also, workplace accidents or other health-related incidents that might cause injuries to workers, raise legal disputes in which carefully collected evidence may be required to set out the actual responsibilities and eventual compensations <ref type="bibr" target="#b5">[6]</ref>. However, traditional workplace activity monitoring and ergonomic assessments rely on self-reporting or specialists' direct observations. In risk evaluation, for instance, observations regarding the human body focus on measuring angles of the trunk and limb joints. This makes the procedure costly and severely prone to intra-and interobserver variances <ref type="bibr" target="#b26">[27]</ref>. Several alternatives to automate procedures for tracking and collecting ergonomic data have been proposed, including accelerometers, RFID devices, motion sensors, LiDAR scanners, GPS, physiological monitoring, and many others. We refere to Subedi and Pradhananga <ref type="bibr" target="#b29">[30]</ref> for a thorough literature review on the topic.</p><p>The new data-driven I4.0 framework requires a more systematic and unbiased collection and analysis of workers' activities. This helps optimize processes in manufacturing, construction, and various other industrial settings, in which human-assisted observation is inadequate. Computer Vision (CV) is becoming the main alternative to human-assisted monitoring. Recently several CV-based approaches have been proposed aimed to provide adequate unsupervised solutions to ergonomic assessment. In particular, deep-learning based body pose estimation like STAF <ref type="bibr" target="#b24">[25]</ref> or VIBE <ref type="bibr" target="#b11">[12]</ref> are enabling novel and significant breakthroughs in several contexts, including workspace ergonomic assessment <ref type="bibr" target="#b19">[20]</ref>. This trend enabled a complete digitization of the different workplace aspects, including personnel activities, providing a wealth of potentially valuable data. In ergonomic assessment, for instance, routine monitoring may require evaluating about 25 joint angles and their combined relative values at least once per second. Thus, a single worker's hourly activity generates a large amount of data that require adequate tools to perform exploratory analyses. This helps ergonomists pinpoint situations or contexts that require intervention or find relevant situations that may inform litigation. However, to the best of our knowledge, there are no attempts to provide means to extract and handle significant ergonomic information in workplaces in a sensible way.</p><p>In this work we introduce ErgoExplorer, a visualization tool able to explore and analyze time dependent ergonomic scores from observations encompassing very long periods. By means of CV-based data extraction from regular cameras, the system can analyze and synthesize large amounts of ergonomic data. The goal is to facilitate the ergonomists' tasks to detect unwanted or unexpected workplace conditions. This in turn enables insights from complex interplays of situations, to evaluate possible workplace scenarios on a sound basis, and to address matters in which a careful yet massive ergonomic data analysis must be carried out. ErgoExplorer is based on coordinated multiple views, incorporating the traditional score tables used by ergonomists in an interactive manner. They are linked with other depictions, including two novel views, ErgoView and ErgoTimeline. In addition we customize other views to study temporal aspects of the scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">ERGONOMIC ASSESSMENTS</head><p>Workplaces enforcing a safety culture are known to be more productive, with high employee morale and low burnout. A careful ergonomics integration is among the most relevant factors in this enforcement. Work-related musculoskeletal disorders (WMSDs) are typical workplace health issues that may result in inflammation or degeneration of functional body structures <ref type="bibr" target="#b7">[8]</ref>. WMSDs are the leading cause of productivity losses due to deaths and work-related permanent disabilities, litigation, sick leaves, and the related indirect costs <ref type="bibr" target="#b3">[4]</ref>. Ergonomic assessment programs aim to detect and assess actual or potentially harmful workplace situations, suggesting interventions to prevent the occurrence of WMSDs <ref type="bibr" target="#b16">[17]</ref>.</p><p>Traditional workplace activity monitoring and ergonomic assessments rely on self-reporting or on specialists' direct observations regarding the angles of main body joints. Rapid Upper Limb Assessment (RULA) <ref type="bibr" target="#b20">[21]</ref> and Rapid Entire Body Assessment (REBA) <ref type="bibr" target="#b21">[22]</ref> appear to be the most widespread approaches in this respect <ref type="bibr" target="#b14">[15]</ref>. Humanassisted evaluation has to be performed by specialized ergonomists. The complexity of such a manual task typically requires the evaluation to focus only on critical moments in which the workers are executing potentially or actually risky movements. With pencil-and-paper-based methodologies, ergonomists observe workers performing tasks and then fill a table. They are slow and cumbersome when it comes to monitoring a worker over an extended period of time. This requires large intervention times of human experts, whose sole presence may also alter the actual worker's performance. As a consequence, the resulting assessments are vulnerable to observer fatigue, have scalability difficulties, and depend heavily on the experts' subjective criteria. This leads to non-uniform evaluations, intra-and inter-observer variance, and other detrimental issues <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. However, the scoring tables are essential as experts rely on them to consolidate the measurements taken throughout an activity and they are the primary tool for decision making up to now.</p><p>Rapid Entire Body Assessment (REBA) generates a postural examination framework sensitive to musculoskeletal risks in various work tasks. It is specifically applicable to unpredictable working postures found in construction, health care, and other service industries <ref type="bibr" target="#b21">[22]</ref>. With REBA, body joints (e.g., shoulders or elbows) are evaluated based on their angular deviations from a predefined safe and comfortable posture. Ergonomists assign individual joint and posture scores to each body region for a visually monitored working task. Depending on the specific case, they fill additional worksheets for each significant change of a body posture. The ergonomic assessment considers bio-mechanical and postural load requirements on the neck, trunk, and upper limbs during the work cycle. For this, a systematic process has to be performed to evaluate the required body postures, exerted forces, and repetitions for the tasks being assessed. Moreover, limbs are analyzed separately (e.g., right versus left upper arm) if performing different actions. So instead of a single-page worksheet containing the three tables, ergonomists need additional tables to assign respective scores. Deviations from the predefined safe postures receive individual scores that are integer numbers related to the actual angular deviations in all joints. Joint scores are then combined into limbs and trunk scores using specific tables typically presented in ad-hoc worksheets. The limbs and trunk scores finally guide the computation of the overall grand score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PREVIOUS WORK</head><p>Visual analysis of joint movements was proposed in several contexts <ref type="bibr" target="#b2">[3]</ref>, including the study of locomotion <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b31">32]</ref>, deviations in movement patterns <ref type="bibr" target="#b17">[18]</ref>, ortopedics-oriented biokinematic data <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13]</ref>, sports <ref type="bibr">[11]</ref>, and human-computer interaction <ref type="bibr" target="#b32">[33]</ref>. However, there are only a few proposals in the literature targeting the visual analysis of workers' performance in actual workplace settings. The work of Han et al. <ref type="bibr" target="#b8">[9]</ref> introduces simulated 3D working environments to perform ergonomic analyses. The simulated environment is used as an observation tool. Specialists can assess the different tasks and activities that the workers will perform, and if they will result in unhealthy postures, inappropriate repetitions, excessive force or static loading, or stress in some body parts. Teizer et al. <ref type="bibr" target="#b30">[31]</ref> present an educational and training environment for construction workers that is based on the integration of real-time location tracking and immersive 3D data visualization. Kanan et al. <ref type="bibr" target="#b9">[10]</ref> propose an autonomous system to monitor the position of workers and equipment. Its effectiveness has been tested on real construction sites. As a result, they display a video summary to supervise the proximity of the worker and the risk area around the equipment.</p><p>More automatized proposals apply CV-based ergonomic analyses. Bauters et al. <ref type="bibr" target="#b1">[2]</ref> discuss a multicamera-based model to automate the analysis of assembly workstations. The proposed system generates realtime information to support improvements in workers' performances. The collected data is analyzed in terms of specific key performance indicators (KPIs). The resulting information is presented in a specifically designed dashboard that visualizes the workers' efficiency, pace, value-adding activities, anomalous work cycle situations, and other relevant parameters. Video processing is based on traditional engineered feature extraction. Thus, the approach is less flexible compared to state-of-the-art practices, and highly dependent on the fine-tuning of several processing aspects, camera settings, and data fusion.</p><p>A similar goal was investigated by Li et al. <ref type="bibr" target="#b13">[14]</ref>, where the authors apply 3D skeletal modeling to emulate the workers' movements in real construction and manufacturing sites. According to the authors the method is able to discern ergonomic risks by detecting inadequate body postures and also to evaluate force and load handling that may potentially generate injuries. The actual information involved in tasks maneuvering have to be delivered from the workplace design and environment, and the task schedule, which, according to the authors, can be obtained from direct observation or video recordings. The resulting data is then used together with the mentioned 3D model to infer Compare ergonomic information such as the joints' locations and angles. This enables a successive risk assessment analysis using traditional methods like RULA <ref type="bibr" target="#b20">[21]</ref> or REBA <ref type="bibr" target="#b21">[22]</ref>. The latest breakthroughs in deep learning applied to CV enable a more thorough and rigorous monitoring, in particular by means of body-pose estimation-modules like STAF <ref type="bibr" target="#b24">[25]</ref> or VIBE <ref type="bibr" target="#b11">[12]</ref>. In Massiris Fernandez et al. <ref type="bibr" target="#b19">[20]</ref>, for instance, the effectiveness and accuracy of a CV-based approach was tested in a variety of scenarios. Difficult workplace settings with several workers are included, involving occlusions and self-occlusions, varying illumination conditions, moving and egocentric cameras, etc. The success of this approach triggers new challenges, in particular how to convey and help make sense of the large amount of activity data that is collected second by second.</p><p>Another significant aspect of video-based massive data collection is related to adequately filter out irrelevant parts of takes that may hamper the significance of the overall data analysis and visualization. Workers often perform movements that are unrelated to their actual activities, and in a different time granularity. Examples include a short arm movement to scratch the forehead, a more prolonged arm and head movement to check the time on a smartphone, an even longer and more complex sequence of movements to grab a water bottle, and sip, etc. Indiscriminate movement collection and analysis, oblivious to the aimlessness of these or other kind of events, will certainly generate noisy parameters that in the long term may compromise the overall performance of the system. For this reason, event-based semantic video summarization appears to be a feasible alternative. In Song et al. <ref type="bibr" target="#b28">[29]</ref> the authors propose an event-centric video summarization method (i.e., an approach not based on takes or key-frames). The underlying method considers event detection based on trajectory analysis, and a random forest classifier to recognize abnormal deviations from the previously detected trajectories. These detected abnormal events then enable a coverage algorithm that summarizes the complete relevant set of frames.</p><p>In most of these approaches, the main interest is to collect and represent relationships between different movements (e.g., joints, poses) instead of collecting, analyzing, and visualizing large sets of sequential data. This limits their applicability in large-scale projects like the ones required for I4.0. An example would be the combined analysis of several workers' activities over extended periods of time and in varying working conditions. Some approaches take advantage of the advancements in virtual 3D worlds, CV, and data visualization. So far none aims to produce an integrated solution that is able to provide all aspects required for ergonomic assessment of several workers, along large periods of time, and to summarize the analysis in sensible ways. In this work, we propose an encompassing methodology that is able to:</p><p>• collect CV-based information related to workers' performance in real working sites,</p><p>• evaluate this information regarding the most widespread ergonomic assessment methods,</p><p>• split the ergonomic analysis into domain tasks, determined by specific topic questions, in order to generate visualizations that fulfill the requirements, and</p><p>• present the results in a flexible and actionable dashboard that facilitates the most useful data manipulation operations to easily extract the relevant conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TASKS ABSTRACTION AND REQUIREMENT ANALYSIS</head><p>We first perform a design study comprising a characterization of the problem domain, which in our case is an ergonomic hierarchical task analysis <ref type="bibr" target="#b0">[1]</ref>. As Meyer et al. <ref type="bibr" target="#b22">[23]</ref> suggested for real-world problemdriven studies, we initiated participatory design sessions with four domain experts. In the sessions with the domain experts, we identified several analysis tasks for the exploratory analysis of ergonomics movement data. The subdivision into tasks is similar to other commonly applied time-and-motion studies. Splitting into tasks and the underlying question scheme are adequate to pinpoint and prioritize the circumstances that may be riskier. Our aim is to design a suitable interface to support ergonomic decision-making and thus improve workers' well-being. The objects of analysis are ergonomic and angular distribution data and their temporal, similarity, quantity, and dependency relations as quantified through an automated video analysis. The analysis task provides a dataset composed of tables describing the angular joint distribution per worker and per video frame. These angles are calculated based on 3D CV body-joint inferences <ref type="bibr" target="#b19">[20]</ref>. The underlying CV-based algorithms to estimate the ergonomic data have many parameters, for instance detection thresholds for filtering outliers, or acquisition-confidence factors if the video takes are not of good quality. Extra filtering procedures may be needed due to other circumstances, like worker's occlusions or self-occlusions. The REBA method uses the resulting joint angle information as the basis to calculate ergonomic risk. It determines risk attributes with ordinal categories, where diverging low values represent a low ergonomic risk and viceversa.</p><p>The main focus of our approach is to support a-posteriori analysis tasks of time dependent scores obtained by one of the scoring schemes (REBA or RULA). In an ergonomic evaluation case, the analysis aim might vary depending on what kind of information is most relevant to the stakeholders, and on the peculiarities of each evaluation task. The ergonomic analysis begins with an overview of the time-and-motion study, which is afterwards decomposed into tasks at desired levels of detail, in a similar vein to Schneiderman's visualization mantra <ref type="bibr" target="#b27">[28]</ref>. Then, tasks are sorted hierarchically, depending on various factors in light of the analysis purposes. In particular, and after the interviews, we have distinguished six different evaluation tasks, which are decomposed into 17 basic questions that ergonomists have to address at different analysis stages (see Table <ref type="table" target="#tab_0">1</ref>). These questions were compiled from two sources: interviews with ergonomists about their data and analysis methods, and surveys of problems addressed in the literature <ref type="bibr" target="#b0">[1]</ref>. The questions are the basis for defining the design goals of our visual analysis tool, and for conveying the relevant information and data relationships in a clear and distinguishable way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Task abstraction</head><p>Defining the type of ergonomic analysis (T1) and the goals and evaluation criteria (T2), it is essential to highlight that these are contextdependent aspects. An ergonomic analysis is generally part of the continuous improvement practices in enterprises and companies. However, an on-premise ergonomic analysis may also be required due to regulation changes, direct expert or authority recommendations, the detection and determination of undesired situations, or as part of other corrective actions after contingencies or accidents. In this context, questions Q1 and Q2 clarify the focus of the ergonomic analysis, and questions Q3 and Q4 state the expected performance criteria. In this initial phase, it is essential that our tool supports the user to understand what happened during the worker's activity, what should have happened in case something was undesirable, what might have happened in hypothetical contexts, and to estimate the rate and cost of failures. Task T3 is concerned with adequate factorization of the analysis, decomposing the overall activity into smaller analysis units. Question Q5 focuses on the work-to-task division. Questions Q6, Q7, and Q8 point to the task triage. The word triage is borrowed from medical parlance, in which a sorting by urgency determines the allocation of patients according to system priorities. In the task triage, we sort each task in terms of input, output, protocols, records, value enrichment, and risk criteria. During task T4, the requirement is to confirm the task decomposition, proposed goals, and performance benchmarks. For this purpose, questions Q9 and Q10 intend to filter outliers or useless data by selecting an ideal task candidate, which represents the movements performed by a worker during a routine task.</p><p>Task T5 is concerned with understanding the actual ergonomic problems, in particular risky or potentially harmful movements or prolonged positions. For this, we have to express if risky movements are associated with a specific task (Q11 and Q12), a body joint (Q13), or a body side (Q14). Finally, in T6 it is essential to arrive at conclusions, especially if a specific ergonomic improvement is required with some urgency. In REBA, for instance, if a task is found to be of very high risk, an ergonomic intervention is demanded immediately. Our tool also provides specific support for this kind of actions (Q15). Perhaps the most important question during the ergonomic analysis is Q16, which is how to mitigate ergonomic risk. An expert should refer to the prevailing state-of-the-art or best practices to implement and validate (Q17) the potential alternatives. The underlying purpose of Q16 and Q17 is to establish an impact measure. This may have a translational effect in reducing accidents, diminish the incidence of WMSDs, retrain personnel, and overall save undesired ergonomy related-costs. Whenever possible, such an impact measure confirms the validity of the diagnosis and the proposed solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Requirements</head><p>Once the analysis tasks were defined, we then elicited the following requirements from the ergonomy experts that participated in this study:</p><p>• R1: Enable a fast path for an initial observation (T1).</p><p>• R2: Provide a way for observing the workers' movements and postures during several work cycles (T2).</p><p>• R3: Quickly specify the most compromised postures, critical angle ranges, and highest force-load tasks (T3).</p><p>• R4: Locate postures that are held during the longest period of time (T3).</p><p>• R5: Provide a way to find (or discard) atypical actions or jointrisk estimations (T4).</p><p>• R6: Provide a way for visual and descriptive identification of the task and the movements performed (T1, T5).</p><p>• R7: Provide means to easily locate frames or video portions with risky movements for workers' retraining (T1, T6).</p><p>• R8: Incorporate the REBA score tables which provide an action level with an indication of urgency (T3, T6)</p><p>• R9: Provide comparisons of time-dependent scores for single and multiple joints (T5).</p><p>• R10: Quickly compare the risk distribution for each joint between the two sides of the body (T5, T6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">THE ErgoExplorer DESIGN</head><p>Based on the identified analysis tasks and the given requirements, we designed the ErgoExplorer tool. It provides means to quickly identify the focus required in a given analysis task, and to detect and understand relevant information within large amounts of information. It offers an initial overview of the data with an adequate level of detail. The tool then allows to delve into specific subsets of the data (e.g., when risky movements arise) and gather all the details relevant to the specific situation. To make the ErgoExplorer dashboard easier to interpret, we place a human body image in the center of the coordinated multiple views, which are then arranged around it, as shown in Fig. <ref type="figure">1</ref>. All the views are linked. If the user brushes in one view, the corresponding data items are highlighted in all the other views. In the following subsections we present and motivate the rationale behind the design and use of the different views according to the tasks and requirements. We start with the description of visual encodings in ErgoView, i.e., the central view which is composed of several smaller views. We continue with the description of the ErgoTimeline, and conclude the section with the chosen interaction design. In order to illustrate the new techniques, we visualize ergonomic data from two video collections. The first dataset contains 15861 video frames and 30 corresponding ergonomic data attributes per captured video frame for a worker painting a wall (approx. 4.5 hours of video material). The second dataset has only 300 frames and describes a person who does gymnastics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Visual encodings in ErgoView</head><p>The main component, ErgoView, is the central view for the exploration, which includes other views arranged around a human body image (see Fig. <ref type="figure" target="#fig_0">2</ref>). This arrangement addresses different design requirements, presents known information appropriately and, according to the experts' opinion, engages, and facilitates the specific analysis tasks. Besides the human silhouette which serves as an orientation landmark, the ErgoView contains REBA Tables, ErgoGauges, and ErgoMovements. The well-known REBA Tables represent a link to the conventional way in the domain of risk assessment, and they are required by the domain experts (R8). REBA Tables are arranged around the picture of a human body, where the body silhouette is seen from behind. This orientation makes it easier for ergonomists to quickly associate the scores in the tables positioned on the right side with the corresponding joints on the right side of the human body, and analogously for the left side. The same is true for the ErgoGauges, which are placed in a row above the silhouette. They support a detailed analysis of ergonomic risk for all body joints (R9). In addition to the proper left and right placement, a line also links each gauge to the corresponding body joint, making the visual connection even more explicit. Finally, at the bottom we have the ErgoMovements view, which makes it possible to include further sources of information into the analysis, such as images and video (R7). Each of the three components of the ErgoView is explained in detail in subsections below. The ErgoView layout proved to be very practical according to the experts, and it is used as a starting point in the analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">REBA Tables</head><p>At the beginning of an analysis experts need to quickly reveal the spread of potentially risky movements in the data (R1). Our view of REBA Tables is designed to support this basic requirement. In particular, it Because REBA Tables are still widespread in the application domain, we display them (R8), but in an interactive and cumulative way (R3). In our approach, posture scores are computed automatically using CV methods as explained before. A vast number of scores for an extended period of time can be calculated and deployed for analysis automatically. We display all scoring tables at once, three for the left and three for the right body side (R10). Moreover, we propose to augment all tables to allow ergonomists to compare time-dependent scores for single and multiple joints (R9). Our design relies on histograms and a heatmap to show time-dependent data while retaining the primary purpose of REBA Tables.</p><p>We briefly explain the original table design. Since all tables share similar designs, we will use Table <ref type="table">A</ref> as an example. The three data attributes used in this table are TrunkScore, NeckScore, and LegScore. The original design of Table A is shown in Fig. <ref type="figure" target="#fig_1">3(a)</ref>. Note that TrunkScore goes from 1 to 5, NeckScore from 1 to 3, and LegScore from 1 to 4. The posture scores for each attribute are pre-entered in the corresponding cells. Ergonomists can quickly select one value for each joint based on their observation of the worker's posture. In this figure, a diamond icon indicates the assigned score (2 for the neck, 3 for the legs, and 5 for the trunk). An established hierarchical scoring scheme is used, which means that in order to calculate the final posture score, ergonomists must go from the top data attribute (Neck). Depending on the score given there, they mark the score in the level below (Legs) and then find the intersection with the third attribute (Trunk) to read the derived score, which is 8 in this case. The same procedure is repeated for Table B and for Table <ref type="table">C</ref>. Because each cell in a table can hold only a single scalar value, the original REBA Tables are not appropriate for analyzing a collection of observations. For this reason, we adopt the basic table design, but allow each cell to hold more than one value. We encode the resulting value in color, and the table becomes a heatmap. We also add marginal histograms for rows and columns. To explain our design, we use terminology that differentiates between horizontal and vertical data attributes-this relates to rows and columns in the tables, respectively, but also to how the histograms in the tables are oriented. For example, Fig. <ref type="figure" target="#fig_1">3(b)</ref> shows four horizontally oriented histograms (one for horizontal-data attribute NeckScore, and three for horizontaldata attribute LeftLegScore) and one vertically oriented histogram (for vertical-data attribute TrunkScore). The numbers of bins in the histograms in the first level, i.e., for the horizontal data attribute 1, and vertical data attribute 1, correspond to the number of posture scores that can be assigned to the respective joint. In the second level, i.e., for the horizontal data attribute 2, the number of histograms corresponds to the number of bins in the histogram that is one level above. Also at this level, the number of bins in each histogram corresponds to the number of scores for the respective joint. Since at the first level there is only one histogram the total number of data items is the sum over all bins. However, at the second level, the total number of data items is the sum of all bins of all histograms. Here, each histogram has as many items as contained in the bin of the first-level histogram located at the top of a second-level histogram. REBA Tables do not use more than two levels. However, our approach allows for more horizontal or vertical levels in a table (e.g., the RULA tables are commonly designed with two vertical levels). The height of a bin in a histogram indicates the frequency of the corresponding joint score in the data, while a heatmap uses color intensity to indicate the frequency distribution of the computed posture scores. We opted for a heatmap because we wanted to keep the numerical values of the overall REBA score for each table cell as well. If we use a single posture measure, instead of showing many measures, then on each level there is only one populated bin in the histogram, and in the heatmap only one cell is used. This would be the same as looking at the original table depicting only single scalar values. The dataset shown in the example case has more than 15000 rows, i.e., unique posture measurements. As histograms and heatmaps represent aggregated visualizations, depicting such an amount of data is not a problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">ErgoGauge</head><p>The REBA method defines a safe posture for each joint in terms of the most desirable joint angles, and our REBA Tables support a wholebody ergonomic risk evaluation. The requirement R3 describes the need to support a detailed ergonomic-risk distribution-analysis for each joint. A safe posture, e.g., for the elbow, means the forearm position is within an angle between 60 and 100 degrees concerning the vertical axis of movement. As we have angles to visualize, we decided to use a radial layout. We show a single measurement angle as a radial line. Since data are angle measurements of human body joints, we display only a part of the circle corresponding to the valid range of a particular joint movement. In this way, a quick connection between the actual and valid joint movements is established. Fig. <ref type="figure" target="#fig_2">4</ref> shows the basic design of the view.</p><p>Linking an angle to its assigned score is also an important aspect that we want to convey in this visualization. Since angles are only indirectly related to posture scores, we use color coding to communicate the movement risk for each joint angle. The ErgoGauge classifies and locates each joint's estimated risk according to the ergonomic angle using the traffic light palette (red, yellow, and green), with red indicating the worst score. We choose red, yellow, and green after discussions with domain experts although these colors are not distinguishable for persons with impaired color vision. In such a case, the color palette can be modified accordingly. Varying the length of segments (without coloring them) is another option to reduce cognitive demand and help ergonomists to concentrate on analyzing and comparing different angle scores and related angle ranges (see Fig. <ref type="figure" target="#fig_2">4(b)</ref>). While this choice was appreciated by ergonomists, it was found that, in rare cases, different posture scores may be assigned very similar or the same joint-angle value. Other parameters such as load may affect the given score. In this particular case, the combination of the color and length channel to encode the joint score proved to be better in emphasizing such an unusual posture (R5). A corresponding ErgoGauge example is shown in Fig. <ref type="figure" target="#fig_2">4(d)</ref>. Without color coding, the ErgoGauge shows only the possible range of joint movements. All three design choices are shown in Fig. <ref type="figure" target="#fig_2">4</ref>. The top row shows Dataset1 with fifteen thousand entries compared to the bottom row with Dataset2 and a few hundred entries. Using only the ErgoGauge by itself, it is hard to know how many items are within a certain range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">ErgoMovements</head><p>There is a general trend towards an automatic monitoring in workplaces. Ergonomy experts will be provided with detailed ergonomic data to conduct a thorough a-posteriori analysis. However, visual data will still be the most important information in their work. One of the requirements has been that ErgoExplorer should include means to analyze videos and posture images during the assessment task. This can be accomplished with the ErgoMovements view, which provides standard video playback options (R7) and also supports further requirements, including R1, R2, R3, R5, R7, and R6, as explained in the following. Compared to the pencil-and-paper-based REBA method, ErgoMovements helps users to create a better mental representation of the analyzed data by linking numerical values in the views together with actual workers' movements. For example, by examining the quantitative and qualitative data presented in the REBA Tables and displayed in the ErgoView, the ergonomist can quickly conclude the seriousness of the situation and whether to react immediately. In order to help analysts perceive the depicted information more efficiently (especially regarding R5), we provide options to display different sets of pre-selected images (see <ref type="bibr">Fig 5)</ref>. Images are one of the attributes in our datasets, and each image is associated with a specific time point. The user can select any of the related data tables shown in the ErgoView, to display the corresponding set of images. Each of the images relates to its corresponding REBA score. There is an option to quickly switch between the tables to gain insight into the worker's actions in relation to the tables' scores.</p><p>We have considered different ways of selecting representative images since quite different postures can result in the same overall score in the table. In working conditions where actions are repeated cyclically (as is our case), the experts mentioned that any image of a group with the same score is a good representative of the whole group. In this case, they identified the relevant task to establish a relationship between the results presented in the REBA Tables, the complexity of the work, and the related risk factors to which the worker is exposed. ErgoMovements helps to clarify the observed workers' ergonomics data in the context of their original work environment (R2, R6, R7). Moreover, ErgoMovements can show examples of unsafe actions as well as good practices previously executed during the workday (R3, R5, R7), which in turn supports the reduction of ergonomic risks. For instance, workers who are at a high ergonomic risk undergo retraining sessions. During this retraining, ErgoMovements depicts representations of the currently performed movements and postures (R7), highlighting the aspects that need to improve, and also the progress to achieve safer working practices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">ErgoTimeline</head><p>The ErgoTimeline shows the distribution of joint angles and their risks over time (see Fig. <ref type="figure" target="#fig_4">6</ref>). ErgoTimeline analysis (T3) depicts an action's repetitions, duration, and other time-related aspects. This is practical for analyzing routine a-posteriori instead of on-premise working contexts. Usually, the selection of routine levels and a work-to-task partitioning (work sampling) are made by firsthand inspection, which is time consuming and lacks inter-rater reliability <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27]</ref>. Based on the tasks that make up the work at hand, five levels of decreasing routinization can be distinguished <ref type="bibr" target="#b6">[7]</ref>: i) a constant task with a predictable work cycle; ii) various cyclical tasks; iii) a mixture of cyclical and non-cyclical tasks; iv) a single non-cyclical task, and v) multiple noncyclical tasks. The ErgoTimeline assists experts in assessing as to whether the analyzed work has some level of routinization and how an action can be split into smaller parts. Moreover, it directly supports tasks T3 and T4, the decomposition of actions, and its validity. It also supports the evaluation of repeatability and facilitates the elimination of outliers. The main concept of this view is to depict the joint angles  <ref type="table">C</ref>, an image is automatically selected that gives visual feedback to the expert about the worker's action performed. In the shown case, the worst scores are nine and ten, and the related action is dipping a roller into a container with paint placed on the floor (R7). (b) Table A for Dataset2 is sparse, however, risk assessments for legs, neck, and trunk are most of the time within an acceptable range.</p><p>by a line chart. In this way, the analysts see the values as a function of time, and can easily spot cycles, repetitions, and irregularities (R2, R5). We show data for several joints in a single chart to support a visual correlation analysis (R9). Color coding and labels are used to distinguish between joints in this case. Finally, there are limits which indicate a risky or unhealthy value for each joint (R7). We depict the current limit values as vertical lines colored in green for non-risky and in red for risky postures. ErgoTimeline also superimposes the curves on colored background (R3). An example is given in Fig. <ref type="figure" target="#fig_4">6(c</ref>) to examine joint angles in detail. As limits are given per joint only, in case of comparing several joints a common vertical axis becomes impossible. In this case the vertical axis is split across parts of the view. An advantage of ErgoTimeline is that we do not rely on regular work cycles. We provide a scoring scheme for joint angle-movements created to handle static, dynamic, or unstable postures based on REBA.</p><p>The measured joint angles are not the only attributes for calculating the posture scores, but they are still crucial for a more detailed ergonomy analysis, e.g., to determine how an injury occurred. In Fig. <ref type="figure" target="#fig_4">6</ref>(a) we observe the effect of fatigue and lateral unbalance. At the beginning both shoulders move evenly and as the worker's right shoulder gets tired, the left shoulder begins to execute more abrupt movements. In Fig. <ref type="figure" target="#fig_4">6(b)</ref>, we see at the end of the sequence a posture correction made by the gymnast that stands upright. Finally, it is possible to inspect how a variation in the joint angle affects the estimated risk score with the overlap display, as depicted in Fig. <ref type="figure" target="#fig_4">6(c</ref>). The ErgoTimeline indicates a high risk exposure for elbows and cyclic movements in the shoulders.</p><p>In addition to the specifically designed views described above, we also allow for other standard views if needed. During the analysis tasks, parallel coordinates and scatter plots are often used to explore correlations between values. The layout is fully configurable, and Fig. <ref type="figure">1</ref> shows the setup preferred by the domain experts, though other settings are possible as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Interaction Design</head><p>Actionable views are essential for some requirements (e.g., R3, R6). For instance, brushing facilitates selecting the critical task to be ergonomically evaluated (Q9). In all views, we support very simple types of data selection, i.e., through mouse clicks. Where needed, we added more advanced brushing operations, including composite brushing and an angle-brush that selects a user-defined angle on a circle in the ErgoGauge view. Concerning the REBA Tables, the user can select a single bin in a histogram or make a composite brush by selecting several histograms with the mouse. If a user selects one or more cells in the heatmap, we regard this action as adding new data items to the same brush and not as creating a new brush and adding it into a composite brush (see Fig. <ref type="figure" target="#fig_5">7</ref>). Ergonomists appreciated this realization because, in most cases, they select a couple of cells (posture score values). Then they observe the relations in other views to generate hypotheses or valuable clues about the possible causes of actual (R6) or potential ergonomic risks (R8) and suitable feedback recommendations (R7). With ErgoGauges we support the analysis of joint angles that deviate from a safe posture. We implemented a brush that follows the radial layout of its parent view, as shown in Fig. <ref type="figure" target="#fig_6">8</ref>. We support a quick selection of all data items that share the same joint score by enabling the user to click on the outer ring of the ErgoGauge. Also, the user can adapt the brush handles or enter the values in numeric fields for exact positioning. More than one brush can be specified, allowing the user to combine different angle ranges. This may be needed to create a reference stage for a risk-distribution analysis.</p><p>Analyzing the time-dependent data is of high priority for ergonomists. To support this requirement, we implemented a range brush in the ErgoTimeline. The brush can be placed at an arbitrary position on the horizontal axis, and the user can change the brush's extent and position at any time.</p><p>The path of curves or the color-coded results in the ErgoTimeline are clearly visible for visualizing data with a small number of time steps. However, in some scenarios, posture measurements must be taken over a prolonged time, and as a consequence, fine details in the ErgoTimeline may be lost. As shown in Fig. <ref type="figure" target="#fig_7">9</ref>(a) it is impossible to perceive all the curves' fine-detail changes. Multiple brushes can be created to support time-interval comparisons, as shown in Fig. <ref type="figure" target="#fig_7">9(b)</ref>. We have implemented a magnifier (zoom slider) as a details-on-demand option that can be adjusted in two ways. First, the users specify the range on the temporal, i.e., horizontal, axis that the magnifier should enlarge. Then, they decide how much of the view space is dedicated to display the magnified data (see Fig. <ref type="figure" target="#fig_7">9(c)</ref>). In this way it is possible to create fine-detail brushes on the temporal axis. Second, it is possible to set up the magnifier to work in the other direction, i.e., to compress a large portion of the data onto a tiny part of the display. Our method allows the user zooming in and out to filter outliers in the work sequences (Q10) and to compare values obtained in the other views.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>As ErgoExplorer was modeled in a multidisciplinary effort, the complete design cycle received significant feedback from expert ergonomists. In the design, a fast access to situations and focus on information where REBA risk scores are high, has been a vital goal. Based on the analysis tasks and questions (see Table <ref type="table" target="#tab_0">1</ref>), ErgoExplorer has been able to simplify the experts' activities and to quickly pinpoint all interventions if they are immediately required (R1). The REBA Tables are especially useful in analyzing tasks with real or potential hazards. They are able to summarize the accumulated risk distribution along complete work cycles with different time granularities (i.e., simple task repetitions, complete tasks, complete working days, etc.).</p><p>Also, as mentioned in Sect. 4.2, determining and segmenting work cycles are lengthy and (sometimes cumbersome) procedures. In this regard, the ErgoTimeline presents an easy understandable representation of the ergonomic risks along time. It allows the users to discern the work cycles and all of the movements that characterize them (R2). This is especially relevant to pinpoint a single working cycle that can be taken as a representative of all the other cycles. The goals are to localize inadequate working postures (R3) or movements that repeat or extend for an inappropriately long duration (R4), and to pinpoint outliers or anomalies (R5). The traditional determination of work cycles is prone to subjective visual-interpretation biases. Therefore, it has been appreciated by the users to easily link the numerical ergonomic risk estimations in the ErgoTimeline (e.g., peaks) to viewing frames (R6) in the actual video capture where the associated events occurred.</p><p>Comparing the same movement performed across different tasks, has been another analysis task facilitated by the combination of ErgoTimeline with viewing frames. This possibility was quite well received by the users. It allows the experts to retrain workers concerning movements where they are exposing themselves to undue risks (R7), or otherwise to show them exemplars on how to correctly perform a task. It is common knowledge in the ergonomy literature that good practices are adopted faster if workers are able to see for themselves in the very moment when they are making a wrong movement sequence <ref type="bibr" target="#b23">[24]</ref> (R7).</p><p>As a noteworthy characteristic of REBA Tables, the final users needed almost no explanation since they are familiar with understanding the meaning of a plain traditional REBA Table <ref type="table">.</ref> For instance, if most values in the REBA Table C are concentrated in values above 8 <ref type="bibr" target="#b21">[22]</ref>, this means that the ergonomic risk is high and the current working activity requires an intervention by a specialist (R8).</p><p>Experts considered valuable to present angular estimations directly in the ErgoGauge view with a 180°rotation This generates a mirrorlike opposing visual between both sides of the body, instead of showing them in a separate user-interface widget. The users can remain focused on the main purpose of the visualization (R10). In addition to the presentation of the most frequent conditions, ergonomists are also interested in detecting irregular or undesirable body movements, and in identifying where and when these movements arise. This type of analysis is not easily feasible without a visualization tool like the one presented here. A manual inspection of the data at the required level of detail would be a daunting task. Our interactive visual analysis tool supports this and other complex tasks by utilizing the linking&amp;brushing mechanism of the coordinated multiple views (CMV). For instance, the user brushes in one of the views a region where angular deviations can be considered risky or inadequate. The selected movements are highlighted in all the other views, facilitating the remaining analysis tasks (R10). A zoom slider was added to simplify searching fine details and filtering with immediate response. After initial demos and presentations of our approach to the domain experts, several modifications were incorporated following their comments and suggestions (R9).</p><p>The accompanying video shows a representative example of a frequent use case. Initially, we represent responses to the ergonomic questions Q1, Q2, and later to questions Q8, Q9, and Q10. With the main view of ErgoExplorer (Fig. <ref type="figure" target="#fig_8">10</ref>), a user can define the focus of the ergonomic analysis. Typically it will be a routine inspection, but may also be a more pressing scenario where improvements are needed immediately. Following question Q8, the ergonomists can prepare a task triage, where they prioritize the inspection of high risk (Q11), more repetitive (Q6), or abnormal movements (Q9 and Q10). ErgoMovements provides access to the brushing results. Pictures containing the riskiest body positions are shown in Fig. <ref type="figure">11</ref> and the maximal REBA risk score is given in Fig. <ref type="figure">1</ref>. Additionally, the linkage to other views allows the users to focus on particular body segments that generate the riskiest body positions. The histograms in the tables highlight repetitive postures (Fig. <ref type="figure" target="#fig_0">12</ref>). Fig. <ref type="figure" target="#fig_1">13</ref> shows the results of brushing high-risk angles of the right shoulder through outlier detection on the ErgoTimelines. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We present ErgoExplorer, an approach for the interactive analysis, visualization, and interpretation of ergonomic risks in video sequences. It is based on the REBA scores of joint positions and angles as derived with computer vision techniques. Our aim is to develop useful visualization techniques for managing time series data of ergonomic information in conjunction with the video sequences. The purpose is to facilitate assessment tasks to pinpoint risky situations, repetitive tasks, and other undesirable conditions that may arise in workplaces. We propose a taxonomy of ergonomic evaluations based on a task analysis by generating the key questions that -according to the knowledge elicitation of the domain experts' activities -best describe the ergonomic analysis process. The complete ergonomic assessment cycle comprises the finding of adequate answers to the key questions following the proposed methodology. Based on the collaboration with the domain experts, several future research directions are being considered. The most salient is to incorporate the RULA ergonomic risk evaluation (and perhaps other, newer assessments) into our analysis. This would lead to a more comprehensive and complete pipeline for ergonomic analysis. Also, a freely available ErgoExplorer installation is expected to facilitate large-scale cooperative research activities in ergonomics, through the collection and assembly of much larger datasets with which more elaborate statistical analyses will be feasible. We initiated interchanges with the Colombian Academic Network of Ergonomics, and also with several ergonomists in Argentina, to provide them with free access to ErgoExplorer. This will allow us to access richer (properly anonymized) datasets, and enhance the usefulness of the proposed visualization and interaction approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. ErgoView. (a) The REBA Table B is maximized, which shows the data attribute Score for the wrist, lower arm, and upper arm of the left body side (see Fig. 3 for details on the design of the augmented score tables). (b) The ErgoGauge is used to visualize the entire range of measured values for the corresponding joint angle; here, the rightshoulder view is maximized. (c) The ErgoMovements view addresses the requirements (such as R5, and R7) to enable image(s) or video preview, for example, to inspect a time window around a specific pose (R1).</figDesc><graphic url="image-6.png" coords="5,64.03,169.09,132.29,64.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Design details of the REBA Table A. (a) The original table design as published by McAtamney and Hignett [22]. (b) The main parts of the augmented table. (c) The proposed Table A which is interactive and includes vertical and horizontal histograms, and a heatmap (R8).</figDesc><graphic url="image-13.png" coords="5,445.83,330.46,99.33,129.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Four possibilities to depict the actual range of the recorded angles using the ErgoGauge are shown. (a) and (b) use only one color (information) while (c) and (d) use three different colors (the usual heat map palette). (d) uses color and different line lengths. Since angles are only indirectly related to posture scores, this can be very useful for discovering deviating or inappropriate working postures that can affect the workers' performance or their health.</figDesc><graphic url="image-16.png" coords="6,478.78,72.77,66.38,118.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. (a) For each of the scores computed in TableC, an image is automatically selected that gives visual feedback to the expert about the worker's action performed. In the shown case, the worst scores are nine and ten, and the related action is dipping a roller into a container with paint placed on the floor (R7). (b) Table A for Dataset2 is sparse, however, risk assessments for legs, neck, and trunk are most of the time within an acceptable range.</figDesc><graphic url="image-19.png" coords="7,228.82,72.77,66.38,175.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. ErgoTimeline to depict angular values over time for gymnastics movements (Dataset2). (a) and (b) compare angles for the left and right elbow and for the left and right shoulder, respectively. (c) simultaneously compares angles &amp; risks of the elbows and shoulders. Different body joints operate in different angle ranges Each vertical axis is scaled according to the range of the corresponding data attribute (R9).</figDesc><graphic url="image-25.png" coords="7,314.00,257.58,132.18,92.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Brushing in REBA Tables. Two cells (highlighted by red rectangles) are selected in a heatmap (R8).</figDesc><graphic url="image-36.png" coords="8,87.14,370.26,92.73,81.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Brushing in ErgoGauge. (a) A brush was created to select the right-shoulder angles in the range from 62 to 66.8. (b) Corresponding radial lines are highlighted for the right elbow in the linked ErgoGauge (R10).</figDesc><graphic url="image-37.png" coords="8,179.49,370.26,92.62,81.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Interaction in ErgoTimeline. (a) The angular values over time for worker's movements (Dataset1) (b) Two brushes were created to select distinct ranges on the angles time-line for long-term observation of the wall painting task. The range values for the active brush (pink) are also shown. (c) Part of the timeline was zoomed in to analyze the disparity in movement between the right and left shoulder (R9).</figDesc><graphic url="image-41.png" coords="8,445.87,204.93,99.33,99.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Main view of ErgoExplorer (R1)</figDesc><graphic url="image-43.png" coords="9,64.26,284.44,230.83,119.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 12 .Fig. 13 .</head><label>1213</label><figDesc>Fig. 12. Repetitive postures are highligthed through brushing (R4)</figDesc><graphic url="image-45.png" coords="9,314.00,72.77,231.27,119.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Task types.</figDesc><table><row><cell>Ergonomic Task Analysis</cell><cell>Typical Topic Question</cell><cell>Abstract Task</cell></row><row><cell>T1: Determine the type of ergonomic analysis</cell><cell>Q1: What is an adequate overview? Q2: What is the purpose of the analysis?</cell><cell>Categorize (Fig 1) Distinguish</cell></row><row><cell>T2: Define goals and evaluation criteria</cell><cell>Q3: What are the expected outcomes of this analysis? Q4: What are the performance assessment criteria?</cell><cell>Identify Compare (Fig 1)</cell></row><row><cell></cell><cell>Q5: How can this work be split into tasks?</cell><cell>Distinguish</cell></row><row><cell>T3: Task decomposition table or diagram</cell><cell>Q6: How many times is each task performed? Q7: How much time is spent on each task?</cell><cell>Compare Compare</cell></row><row><cell></cell><cell>Q8: Which task should be analyzed first?</cell><cell>Identify</cell></row><row><cell>T4: Check decomposition validity</cell><cell>Q9: Which data represents the task best? Q10: Are outliers and wrong data filtered out?</cell><cell>Locate Distinguish</cell></row><row><cell></cell><cell cols="2">Q11: What risky movements are related to a particular task? Identify</cell></row><row><cell>T5: Identify risky movements</cell><cell>Q12: How are scores distributed in the risk tables?</cell><cell>Categorize</cell></row><row><cell></cell><cell>Q13: Which body joints present high risk?</cell><cell>Distinguish</cell></row><row><cell></cell><cell>Q14: Is the risk balanced on both sides of the body?</cell><cell>Compare</cell></row><row><cell></cell><cell>Q15: When is intervention required?</cell><cell>Distinguish</cell></row><row><cell>T6: Test hypotheses concerning performance factors</cell><cell>Q16: How to mitigate ergonomic risk?</cell><cell>Identify</cell></row><row><cell></cell><cell>Q17: How to validate improvements?</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors thank all the workers and experts who helped in the development of this research, with a special mention to the Occupational Health and Safety Service at the Universidad Nacional del Sur (https://uns.edu.ar), and Prof. Beatriz Tsukamoto, Prof. Sandra Liliana Joaqui, Dr. Martha Torres, and Prof. Ayda Cáceres from the Colombian Academic Network of Ergonomics (https://scergonomia.com.co/race). VRVis is funded by BMK, BMDW, Styria, SFG, Tyrol and Vienna Business Agency in the scope of COMET -Competence Centers for Excellent Technologies (879730) which is managed by FFG. This research was partially founded by the National Council for Scientific and Technical Research of Argentina (CONICET); and the SGCyT-UNS (grant 24/K083).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hierarchical task analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Annett</surname></persName>
		</author>
		<idno type="DOI">10.1201/9781410607775.ch2</idno>
	</analytic>
	<monogr>
		<title level="m">Handbook of Cognitive Task Design</title>
				<editor>
			<persName><forename type="first">E</forename><surname>Hollnagel</surname></persName>
		</editor>
		<editor>
			<persName><surname>Nj</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="17" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automated work cycle classification and performance measurement for manual work stations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bauters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cottyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Claeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Slembrouck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Veelaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Van Landeghem</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.rcim.2017.12.001</idno>
	</analytic>
	<monogr>
		<title level="j">Robotics and Computer-Integrated Manufacturing</title>
		<imprint>
			<biblScope unit="page" from="139" to="157" />
			<date type="published" when="2017-10">October 2017. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Approaches and challenges in the visual-interactive comparison of human motion data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vögele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fellner</surname></persName>
		</author>
		<idno type="DOI">10.5220/0006127502170224</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Joint Conference on Computer Vision</title>
				<meeting>the 12th International Joint Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="217" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Economic impact of musculoskeletal disorders (MSDs) on work in Europe. Best Practice and Research</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bevan</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.berh.2015.08.002</idno>
	</analytic>
	<monogr>
		<title level="j">Clinical Rheumatology</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="356" to="373" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Motion Browser: visualizing and understanding complex upper limb movement under obstetrical brachial plexus injuries</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Y</forename><surname>-Y. Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Nonato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Aluru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Silva</surname></persName>
		</author>
		<idno type="DOI">10.1109/tvcg.2019.2934280</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="981" to="990" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Retrieving similar cases for alternative dispute resolution in construction accidents using text mining techniques</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.autcon.2012.10.014</idno>
	</analytic>
	<monogr>
		<title level="m">Information Technologies in Safety Management</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="85" to="91" />
		</imprint>
	</monogr>
	<note>Automation in Construction</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Work routinization and implications for ergonomic exposure assessment</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Gold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Punnett</surname></persName>
		</author>
		<idno type="DOI">10.1080/00140130500356643</idno>
	</analytic>
	<monogr>
		<title level="j">Ergonomics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="12" to="27" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The French musculoskeletal disorders surveillance program: Pays de la Loire network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Roquelaure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leclerc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Touranchet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Imbernon</surname></persName>
		</author>
		<idno type="DOI">10.1136/oem.2008.042812</idno>
	</analytic>
	<monogr>
		<title level="j">Occupational and Environmental Medicine</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="471" to="479" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Computer 3D model-based physical demand and ergonomic assesment of buildings mechanical system construction</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Al-Hussein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>El-Rich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Telyas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CIB W78-W102 2011 International Conference</title>
				<meeting>the CIB W78-W102 2011 International Conference<address><addrLine>Sophia Antipolis, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">An IoT-based autonomous system for workers&apos; safety in construction sites with real-time alarming, monitoring, and positioning strategies</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Elhassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bensalem</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.autcon.2017</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning 3D human dynamics from video</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting><address><addrLine>Long Beach, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5614" to="5623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">VIBE: video inference for human body pose and shape estimation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5253" to="5263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visual analysis of multi-joint kinematic data</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Krekel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>De Groot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Nelissen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Botha</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-8659.2009.01681.x</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1123" to="1132" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3D visualizationbased ergonomic risk assessment and work modification framework and its validation for a lifting task</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gül</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Al-Hussein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>El-Rich</surname></persName>
		</author>
		<idno type="DOI">10.1061/(ASCE)CO.1943-7862.0001412</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Construction Engineering and Management</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ergonomics assessment methods used by ergonomics professionals</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Dempsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.apergo.2019.102882</idno>
	</analytic>
	<monogr>
		<title level="j">Applied Ergonomics</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page">102882</biblScope>
			<date type="published" when="2019-06">June. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Vision-based estimation of MDS-UPDRS gait scores for assessing Parkinson&apos;s disease motor severity</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Poston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pfefferbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">V</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Pohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="637" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Prevención de trastornos musculoesqueléticos en el lugar de trabajo</title>
		<author>
			<persName><forename type="first">A</forename><surname>Luttmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Griefahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Organización Mundial de la Salud</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A threedimensional data visualization technique for reporting movement pattern deviations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Manal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hamill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Stanhope</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jbiomech.2004.10.008</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Biomechanics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2151" to="2156" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic ergonomic postural risk monitoring on the factory shopfloor -The Ergosentinel tool</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Manghisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Uva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fiorentino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gattullo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boccaccio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Evangelista</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.promfg.2020.02.091</idno>
	</analytic>
	<monogr>
		<title level="j">Procedia Manufacturing</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="97" to="103" />
			<date type="published" when="2019">2019. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ergonomic risk assessment based on computer vision and machine learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Massiris</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Á</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Bajo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Delrieux</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cie.2020.106816</idno>
	</analytic>
	<monogr>
		<title level="j">Computers and Industrial Engineering</title>
		<imprint>
			<biblScope unit="volume">149</biblScope>
			<biblScope unit="page">106816</biblScope>
			<date type="published" when="2020-11">November. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">RULA: a survey method for the investigation of work-related upper limb disorders</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mcatamney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">N</forename><surname>Corlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied ergonomics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rapid entire body assessment</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mcatamney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hignett</surname></persName>
		</author>
		<idno type="DOI">10.1201/9780203489925.ch8</idno>
	</analytic>
	<monogr>
		<title level="j">Applied Ergonomics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="201" to="205" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">MizBee: A multiscale synteny browsers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Munzner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2009.167</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="897" to="904" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Human-centered computing and visual analytics for future of work in construction</title>
		<author>
			<persName><forename type="first">Deb</forename><surname>Nipun</surname></persName>
		</author>
		<author>
			<persName><surname>Nath</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Phd</publisher>
		</imprint>
		<respStmt>
			<orgName>Texas A&amp;M University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient online multiperson 2D pose tracking with recurrent spatio-temporal affinity fields</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Raaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00475</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019-06">June. 2019</date>
			<biblScope unit="page" from="4615" to="4623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Visionbased construction worker activity analysis informed by body posture</title>
		<author>
			<persName><forename type="first">D</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Torres Calderon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Golparvar-Fard</surname></persName>
		</author>
		<idno type="DOI">10.1061/(ASCE)CP.1943-5487.0000898</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Computing in Civil Engineering</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Intra-rater and inter-rater reliability of the rapid entire body assessment (REBA) tool</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Albin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Gerberich</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ergon.2019.02.010</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Industrial Ergonomics</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="111" to="116" />
			<date type="published" when="2018-03">March 2018. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The eyes have it: A task by data type taxonomy for information visualizations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Craft of Information Visualization</title>
				<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="364" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Event-based large scale surveillance video summarization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2015.07.131</idno>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">187</biblScope>
			<biblScope unit="page" from="66" to="74" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mapping datafication in constructionworker safety research to minimize injury-related disputes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Subedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pradhananga</surname></persName>
		</author>
		<idno type="DOI">10.1061/(ASCE)LA.1943-4170.0000464</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Legal Affairs and Dispute Resolution in Engineering and Construction</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Location tracking and data visualization technology to advance construction ironworkers&apos; education and training in safety and productivity</title>
		<author>
			<persName><forename type="first">J</forename><surname>Teizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.autcon.2013.03.004</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="53" to="68" />
		</imprint>
	</monogr>
	<note>Automation in Construction</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">An introduction to gait analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Whittle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Elsevier Ltd</publisher>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note>4 ed.</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Hand tracking and gesture recognition system for human-computer interaction using low-cost hardware. Multimedia Tools and Applications</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lim</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11042-013-1501-1</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="29" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
