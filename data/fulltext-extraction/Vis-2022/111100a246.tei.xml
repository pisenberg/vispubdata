<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Supporting Expressive and Faithful Pictorial Visualization Design with Visual Style Transfer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yang</forename><surname>Shi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pei</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Siji</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mengdi</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nan</forename><surname>Cao</surname></persName>
						</author>
						<title level="a" type="main">Supporting Expressive and Faithful Pictorial Visualization Design with Visual Style Transfer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-03-27T09:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Pictorial visualization, data-driven design</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pictorial visualizations portray data with figurative messages and approximate the audience to the visualization. Previous research on pictorial visualizations has developed authoring tools or generation systems, but their methods are restricted to specific visualization types and templates. Instead, we propose to augment pictorial visualization authoring with visual style transfer, enabling a more extensible approach to visualization design. To explore this, our work presents Vistylist, a design support tool that disentangles the visual style of a source pictorial visualization from its content and transfers the visual style to one or more intended pictorial visualizations. We evaluated Vistylist through a survey of example pictorial visualizations, a controlled user study, and a series of expert interviews. The results of our evaluation indicated that Vistylist is useful for creating expressive and faithful pictorial visualizations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Pictorial visualization, as a visual representation of data, use icon-based language to visualize otherwise abstract data points or data facts <ref type="bibr" target="#b76">[78]</ref>. Due to their positive effects on engagement, enjoyment, and memorability <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b60">62]</ref>, pictorial visualizations have been increasingly incorporated into narrative visualization (e.g., data videos, infographics, data comics) to support data-driven storytelling for the general public <ref type="bibr" target="#b62">[64]</ref>. However, creating pictorial visualizations is a challenging task, even for experienced information designers, as a well-designed pictorial visualization often requires faithful data binding as well as expressive visual design.</p><p>With recent advances in artificial intelligence (AI), researchers have introduced data-driven design support tools to facilitate pictorial visualization design. For example, Text-to-Viz <ref type="bibr" target="#b16">[17]</ref> takes a natural language statement on a proportion fact and generates pictorial visualizations. While this approach automates faithful data binding for proportional data, the expressiveness of information presentation is largely ignored, that is, it is based on limited pre-defined visual styles. In practice, designers often collect inspirational visual materials from online sources in early design stages to form creative ideas and promote expressive design <ref type="bibr" target="#b75">[77]</ref>. Based on these observations, design support tools that utilize examples to enable automatic generation have been proposed. For example, Retrieve-Then-Adapt <ref type="bibr" target="#b56">[58]</ref> generates proportional-related pictorial visualizations by retrieving an appropriate example from their example library for imitation. Chen et al. <ref type="bibr" target="#b14">[15]</ref> extracted extensible timeline templates from timeline images to generate new timeline infographics. However, the aforementioned methods are restricted to certain data types such as proportional or temporal facts, which accommodate specific real-world scenarios. Moreover, their methods constrain the visualization type being used in examples to be identical with that in results. That is, if a user attempts to design a timeline infographic, he or she has to search for a timeline example first. Such an approach may reduce the expressiveness and flexibility of pictorial visualization design.</p><p>To bridge such a gap, we introduce Vistylist, a design support tool that leverages online examples to facilitate expressive and faithful pictorial visualization design. Vistylist is built on a concept derived from the field of AI, style transfer, to separate and recombine visual style and data content, enabling a more extensible approach to crafting pictorial visualizations. To explore this, we used two complementary methods. First, we conducted formative interviews with domain experts to understand key elements that characterize the visual style of a pictorial visualization, including color, font, and icon. Second, we undertook a survey of high-quality pictorial visualizations to identify common design patterns that depict the data content of a pictorial visualization. These patterns are described along two dimensions, namely, visualization types and data binding types, based on which a pictorial visualization library was built and later used by Vistylist. Specifically, as a user uploads a source pictorial visualization collected from online sources, Vistylist can automatically extract its visual style and transfer the style to an intended pictorial visualization, where the user can select its visualization type from bar, circle, line, block, area, and number charts. Following style transfer, Vistylist suggests alternative visualization designs based on different criteria and allows the user to adjust the resulting pictorial visualization provided in a vector format.</p><p>To demonstrate the capability of Vistylist, we first created a gallery of diverse pictorial visualizations and compared them to the pictorial visualizations generated by Retrieve-Then-Adapt <ref type="bibr" target="#b56">[58]</ref>. We also conducted a user study and a series of expert interviews to understand how Vistylist is used when creating pictorial visualizations as well as infographics. Based on the results of the evaluation, we discuss the design implications on developing design support tools geared towards example-based design and co-creation with AI.</p><p>The main contributions of this work are as follows: • We developed a design support tool, Vistylist, to facilitate pictorial visualization design by disentangling the visual style of a source pictorial visualization from its content and then transferring the style to intended pictorial visualizations. • We collected a dataset of 1371 high-quality pictorial visualizations and identified a set of design patterns, which to our knowledge, had not yet been systematically analyzed. The dataset is available at https://idvxlab.com/vistylist/. • We conducted a survey, a user study, and a series of interviews. The results of our evaluation indicated that Vistylist can increase the likelihood of creating faithful and expressive pictorial visualizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Our work builds on prior research on design of pictorial visualizations, example-based design, and computational understanding of design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Design of Pictorial Visualizations</head><p>Pictographic language is used by ISOTYPE, defined by Otto and Marie Neurath in the 1920s <ref type="bibr" target="#b53">[54]</ref>, to organize statistics and convey information <ref type="bibr" target="#b24">[25]</ref>. As an essential member of the Neurath group, Gerd Arntz created many pictographs that are still used in traffic icons and warning labels <ref type="bibr" target="#b4">[5]</ref>. In the past decade, pictorial visualizations have been studied to understand how they are designed <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b60">62]</ref> and what benefits they can bring <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b24">25]</ref>. In terms of pictorial visualization design, Boy et al. <ref type="bibr" target="#b9">[10]</ref> proposed a design space of anthropographics, whose dimensions include the classes of visualization, human shape, unit labeling, and unit grouping. Following <ref type="bibr" target="#b9">[10]</ref>, Morais et al. <ref type="bibr" target="#b51">[52]</ref> further extended the design space by introducing the dimensions of granularity, specificity, coverage, verisimilitude, realism, physicality, and situatedness. In terms of the effectiveness of pictorial visualizations, Haroz et al. <ref type="bibr" target="#b24">[25]</ref> found that pictorial visualizations show positive impact on working memory, performance, and engagement. Amini et al. <ref type="bibr" target="#b2">[3]</ref> proposed that animation and pictographs can increase the understandability of data insights and elicit viewer engagement of data-driven clips. On the contrary, Burns et al. <ref type="bibr" target="#b11">[12]</ref> observed that using pictographs in part-to-whole visualization has little influence on understandability. Our work complements previous research by exploring the design patterns of pictorial visualizations that can be used to guide their generation. Specifically, we are interested in what visualization types reoccurred across pictorial visualizations and how data attributes are associated with the properties of icons in pictorial visualizations. Visualization authoring tools have been developed to support manually creating pictorial visualizations, including Data-Driven Guides <ref type="bibr" target="#b30">[31]</ref>, DataInk <ref type="bibr" target="#b72">[74]</ref>, Data Illustrator <ref type="bibr" target="#b42">[43]</ref>, InfoNice <ref type="bibr" target="#b69">[71]</ref>, DataQuilt <ref type="bibr" target="#b76">[78]</ref>. Recent research has also been devoted to enabling machines to generate pictorial visualizations <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b56">58,</ref><ref type="bibr" target="#b63">65]</ref>. For example, Retrieve-Then-Adapt <ref type="bibr" target="#b56">[58]</ref> generates proportion-related infographics by first finding appropriate examples in their example library and generating an initial draft, then adjusting spatial relationships between visual elements in the draft. Chen et al. <ref type="bibr" target="#b14">[15]</ref> proposed to use a deep neural network to extract extensible timeline templates from bitmap images and generate new timeline infographics. The aforementioned work initiated a first step in automating pictorial visualization authoring. However, their methods are restricted to certain templates or visualization types, thus reducing the expressiveness of design. Our work supports expressive and faithful pictorial visualization design with visual style transfer. First, Vistylist allows users to use pictorial visualizations of any type they collected as sources. In this way, users can specify the visual style they prefer instead of relying on the system to control the visual style by retrieving its predefined design templates or parameters.</p><p>Second, when compared to the aforementioned work that focused on the use of pictorial elements for distinguishing unique categorical values or relationships. Vistylist supports more diverse visualization types, including bar, circle, line, block, area, and number charts. More importantly, it allows users to identify the visualization type to be used in intended pictorial visualizations, which is not necessarily identical to the visualization type in a source.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Example-based Design</head><p>Examples can serve as design stimuli for design ideation, where designers can borrow low-level features or derive new inspirations from examples. For example, d.tour <ref type="bibr" target="#b59">[61]</ref> helps collecting relevant and inspiring examples through searching by stylistic similarity to a given example and stylistic keyword. Adaptive Ideas <ref type="bibr" target="#b39">[40]</ref> leverages a faceted metadata interface for modifying and combining elements from multiple example pages to create a new design. Such approaches is able to facilitate exploring existing ideas and generating new ideas. However, examples can instigate design fixation, where subjects constrain their thinking around specific functions and features <ref type="bibr" target="#b26">[27]</ref>.</p><p>To address the issue of fixation, prior research investigated methods to explore more diverse and distinct ideas <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b66">68]</ref>. For example, Koch et al. <ref type="bibr" target="#b33">[34]</ref> proposed an intelligent mood board that helps designers collect and curate inspirational and situationally relevant examples such as "more like this" or "surprise me". As a subsequent effort, MetaMap <ref type="bibr" target="#b27">[28]</ref> supports visual metaphor ideation by recommending examples based on semantics, color, and shape dimensions. IdeateRelate <ref type="bibr" target="#b73">[75]</ref> visualizes design examples by highlighting the conceptual distance between a new idea and a proposed solution. Based on the distance, it ranks these design examples from least similar to most similar. Compared to the aforementioned work, our work further supports design by leveraging examples instead of collecting and curating examples. Specifically, Vistylist introduced the concept of style transfer to visualization design by first disentangling the graphical and textual features of an example visualization from its content and then migrating them to intended pictorial visualizations. Also, the results are provided in a vector format instead of a bitmap image to allow further adjustment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Computational Understanding of Design</head><p>A design is a result of thousands of decisions on various factors such as color, layout, and font. Given such a large design space, prior research has leveraged computational methods to understand what constitutes an effective design. Regarding interaction design, Kumar et al. <ref type="bibr" target="#b35">[36]</ref> proposed Webzeitgeist, a platform for Web design mining that provides information about the visual appearance and structure of pages to understand design demographics and facilitate data-driven design tools. Wu et al. <ref type="bibr" target="#b71">[73]</ref> proposed a data-driven framework to model the perceptual personality of mobile app UIs using computable visual descriptors including color, organization, and texture. Liu et al. <ref type="bibr" target="#b41">[42]</ref> introduced a code-and vision-based approach to generating semantic annotations (UI components, text buttons, icons) for the elements that comprise mobile app UIs.</p><p>Researchers have also used computational approaches to learning the characteristics and styles of graphic design. For example, Zhao et al. <ref type="bibr" target="#b77">[79]</ref> used a learning-based approach to explore the effects of various design factors (e.g., color, font) on the perceived personalities of posters (e.g., cute, romantic). Some research interests have focused on understanding information design such as infographics <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b61">63]</ref> and iconography <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b50">51]</ref>. For example, Lu et al. <ref type="bibr" target="#b45">[46]</ref> applied automated image understanding to explore visual information flows of infographics (e.g., landscape, portrait, up-ladder). Madan et al. <ref type="bibr" target="#b46">[47]</ref> developed a synthetic data generation approach to detect icons for parsing and summarizing infographics. Our work is influenced by computational design approaches described above. Specifically, we focus on understanding and extracting features that characterize the visual style of a pictorial visualization, including color, font, and icon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARY STUDY</head><p>To inform the design of Vistylist, we first conducted interviews with domain experts. Based on their feedback, we then collected a dataset of 1371 pictorial visualizations to analyze design patterns reoccurred across pictorial visualizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Formative Interviews with Designers</head><p>To understand designers' current practices and challenges when creating pictorial visualizations, we conducted formative in-person interviews with four domain experts, including two information designers (D1, D2), one data analyst (A1), and one data journalist (J1). All the participants have more than four years of professional experience and rated their expertise in information design as 'proficient'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Methodology</head><p>We first asked the participants to present the pictorial visualizations they had worked on and then conducted a series of semi-structured interviews with them. In each interview, we asked them the following questions: (1) What workflow did you adopt for creating pictorial visualizations? (2) What parts of the workflow were easy or difficult to manage with existing tools? and (3) What potential functions will you suggest to existing tools to facilitate pictorial visualization authoring? The presentation and interview sessions lasted for 0.5 to one hour for each participant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Findings and Design Requirements</head><p>We found that two strategies are frequently adopted by the participants when creating pictorial visualizations. The first is collecting visual materials as inspiration and then designing from scratch using graphic editing tools such as Adobe Illustrator <ref type="bibr" target="#b0">[1]</ref>. Such a strategy is effective for users who are experienced in design and prefer flexible creation processes (D1, D2). The other strategy we observed for pictorial visualization design is leveraging online authoring tools such as Visme <ref type="bibr" target="#b68">[70]</ref> and starting by searching for a well-designed template (A1, J1). Although the above two strategies are commonly applied, all the participants noted that the challenge lies in deciding "which design is correct to use" (J1). Here, "correct" refers to the effectiveness of data binding. Specifically, D1 noted, "there are different ways to design data mapping, such as using the height or size of the icon, I have no idea which mapping is appropriate for my dataset, and if there are other mappings to use." A1 reflected, "I selected templates based solely on my experience; I've seen many charts that use icons to represent categorical information, so I visualize my data referring to such design."</p><p>The participants were further asked about the features or elements that they would use to characterize the visual style of a pictorial visualization. All the participants agreed that color receives priority, which is in line with the findings suggested by previous research <ref type="bibr" target="#b77">[79]</ref>. D1 commented, "the infographic I'm working on how to celebrate Christmas. I selected a color scheme featuring red, green, and white for all the pictograms to make sure they look compatible." Font and icon also play important roles as stated by D1, D2, and J1, "selecting a typeface for text, Serif or Sans, matters" (J1), "its style can be defined by the characteristics of its icons, such as strokes, fills, or corner smoothness" (D2). Interestingly, layout is not perceived as important as the aforementioned features, as "it depends on what visualizations are used. The layout in a bar chart can be different from that in a pie chart" (A1).</p><p>When asked about the tasks concerning pictorial visualization design that can be automated, three participants mentioned icon retrieval and arrangement. J1 noted, "the process of collecting visual materials such as searching for icons of specific semantics and visual styles, is timeconsuming and tedious." They implied it can be helpful if the system can automatically suggest icons based on data attributes. Regarding icon arrangement, the two information designers addressed the tasks of rescaling and organizing icons based on data values. D1 noted, "to design a pictorial fraction chart, for example, up to 60 percent of the adult human body is water, the easiest way is manually calculating the height of an anthropographic icon to map the ratio." Meanwhile, D2 acknowledged, "automating the process of using repetitions of the same icons and arranging them in a cohesive layout would be useful."</p><p>All the participants agreed that generating alternative design suggestions is one of the most desired functions in light of AI. The reason is that these suggestions can serve as a source for "design inspiration" or "design variants", and are especially helpful for example-based design. The participants also mentioned three important criteria based on which pictorial visualization designs can be suggested, including similarity, aesthetics, and inspiration. J1 said, "suggesting designs according to their ranking of similarity is helpful. I prefer selecting pictorial visualizations whose visual appearances are similar to the one that I've created, so I can use them to design an infographic." D1 and D2 addressed the importance of aesthetics, "I'd check the overall visual effect of a pictograph. If it uses harmonious colors or if it organizes elements in a balanced layout." Also, D1 noted, "I wish to get results of diversity. I want all of the visualizations to have their own characteristics".</p><p>Based on the findings regarding pictorial visualization design derived from the interviews and prior research, we established a set of design requirements to inform the development of Vistylist. DR1 Identifying a set of common design patterns that capture the content presentation of pictorial visualizations to guide automatic generation. The design patterns can be described as a function of visualization types and data binding types. DR2 Extracting elements that help characterize the visual style of a source pictorial visualization, including color, font, and icon, to support visual style transfer. DR3 Generating pictorial visualizations by retrieving icons semantically associated with data attributes and binding data with icons, minimizing the workload of laborious tasks in design. DR4 Suggesting alternative designs of pictorial visualizations based on various criteria, including similarity, aesthetics, and inspiration, to help design ideation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Analysis of Pictorial Visualizations</head><p>To identify design patterns of data content that reoccurred across pictorial visualizations (DR1), we collected and analyzed a corpus of pictorial visualizations. We introduced a high-level structure to systematically categorize these patterns, including a dimension for visualization types and one for data binding types. The detailed analysis of the dataset can be accessed at https://idvxlab.com/vistylist/. Following this, we built a pictorial visualization library based on these design patterns, which is later used by Vistylist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Methodology</head><p>To collect a corpus of pictorial visualizations, we started by searching for related datasets released by prior research, including anthropographics <ref type="bibr" target="#b51">[52]</ref>, infographics <ref type="bibr" target="#b38">[39]</ref>, and data videos <ref type="bibr" target="#b65">[67]</ref>. These anthropographics (117), infographics (976), and data videos (82) were collected from reputable sources (e.g., The Economist, The Guardian) and were of high quality (e.g., winners of Kantar Information is Beautiful Awards, top 500 most-viewed infographics on Visual.ly). We manually extracted 632 pictorial visualizations from the three datasets and these visualizations constitute our initial corpus. Then, we extended the corpus to include more pictorial visualizations by following the methodology proposed by Cui et al. <ref type="bibr" target="#b16">[17]</ref>. Specifically, we collected pictorial visualizations from well-known visualization communities (e.g., flowingdata <ref type="bibr" target="#b74">[76]</ref>, eagereyes <ref type="bibr" target="#b34">[35]</ref>), templates provided by infographic authoring tools (e.g., Canva <ref type="bibr" target="#b13">[14]</ref>, Visme <ref type="bibr" target="#b68">[70]</ref>), and opinionated lists of best visualizations. We also used search keywords such as "pictorial visualizations", "pictographs" and "pictograms" on Google and processed the top returned results. During the process, we complemented our corpus with additional 1142 pictorial visualizations. Note that we excluded diagrams <ref type="bibr" target="#b8">[9]</ref> such as timeline and flow chart from our data collection as they focus more on logical or conceptual relationships between items.</p><p>To ensure that these visualizations are data-driven <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b65">67]</ref>, we mandated that a pictorial visualization is qualified if it is in accordance with the following three criteria: (1) it communicates messages and arguments supported by data, (2) it uses at least one visualization, and</p><p>(3) it combines visualization with pictographic icons. As a result, we identified 1371 out of 1774 pictorial visualizations to be included in our final dataset.</p><p>We then coded the 1371 visualizations from two aspects by asking ourselves about (1) What visualization types are applied <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b1">(2)</ref> What data binding types are used <ref type="bibr" target="#b69">[71]</ref>. Regarding data binding types, our reading of the literature provides an initial understanding <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b69">71]</ref>. In terms of data binding types, we looked into how the graphical and symbolic properties of icons (e.g., color, size) are mapped to data attributes (e.g., numerical, categorical). First, two researchers with visualization-related backgrounds were in charge of independently coding 342 out of 1371 visualizations (25%). During the coding process, we met for three sessions to compare our codes, merge similar codes, and discuss disagreements. Then, we coded all the 1371 visualizations using the final coding scheme and reached a Cohen's Kappa of 0.93. We then discussed the mismatches and reached a 100% consensus. In the following two subsections, we will respectively introduce the visualization types and data binding types observed in our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Visualization Types</head><p>Visualization serves as a necessary device for communicating insights with data in pictorial visualizations, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. By analyzing our dataset, we first classified visualization types by extending the visualization taxonomy proposed by Borkin et al. <ref type="bibr" target="#b8">[9]</ref>. We observed that block charts (616, 44.9%) are dominant over other visualization types. Note that we added a category for block charts, which is functionally identical to circle charts that visualize percentages or fractions. We use the term block as this visualization type looks like it is designed with building blocks. Bar (358, 26.1%), number (180, 13.1%), and circle (86, 6.3%) charts follow. Area (83, 6.0%) and line charts (15, 1.1%) also constitute important components in pictorial visualizations. Map (11, &lt; 1.0%), point (10, &lt; 1.0%), tree and graph (9, &lt; 1.0%), and grid and matrix (3, &lt; 1.0%) were found to be less commonly used in pictorial visualizations. Such design decisions may be that pictorial visualizations are made to appeal to a broad audience and should thus reduce cognitive load.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Data Binding Types</head><p>Data binding is used to enable data-driven design for pictorial visualizations by associating graphical and symbolic properties with data attributes. Specifically, graphical properties of icons include common visual channels such as color and size while data attributes can be categorical values or numerical values. We analyzed how a mapping between icons and data is constructed and distinguished between seven different data binding types as follows.</p><p>Semantics (combined with color) to category (436, 31.8%). Pictorial visualizations frequently use icons as legend markers, allowing a visual association between the symbolic meaning of the icon with the meaning of categorical data. Moreover, the color of icons is often used in combination with their semantics to emphasize the difference between categories. Common examples include using icons of different shapes or colors representing female and male to encode the categorical information in a chart that depicts the gender ratio (Fig. <ref type="figure" target="#fig_0">1 (c)</ref>). Another interesting observation is that the imagery of icons can be used to augment the expressiveness of pictorial visualizations. For example, Fig. <ref type="figure" target="#fig_0">1 (j)</ref> showing running speed can use the line as the trajectory of a running man and stick the icon at the end of the trajectory, framing the chart as a scene. As this data binding type is supported in almost all pictorial visualizations in our dataset, we counted the frequency of pictorial visualizations that use only this data binding type without other data binding types added.</p><p>Area to quantity (112, 8.2%). Pictorial visualizations in this portion associate the area of icons with numerical data such as quantity, providing an overview of the relative size of data. This data binding type is commonly found in proportional area charts, where each bubble is replaced by a scaled icon whose area is identical to that bubble. Fig. <ref type="figure" target="#fig_0">1</ref> (n) scales the area of individual icons to map the size of data from different categories, supporting a comparison in terms of quantity.</p><p>Height to quantity (103, 7.5%). Another portion of pictorial visualizations uses the height of icons to represent quantity. We observed that this data binding type is frequently applied by bar charts, where each bar is replaced by a stretched icon whose height is equal to that bar. For example, Fig. <ref type="figure" target="#fig_0">1</ref> (q) vertically stretches the icons to map the numerical values on the y-axis and carefully scales specific segments of each icon to prevent distortion.</p><p>Height filled with color to proportion (136, 9.9%). To visualize proportion using pictorial visualizations, a common practice is filling an icon with proportional color. For example, Fig. <ref type="figure" target="#fig_0">1</ref> (o) presents multiple icons, where the colored segment of each icon is proportional to the numerical value that it represents. Similarly, Fig. <ref type="figure" target="#fig_0">1</ref> (p) stacks multiple colored segments in one icon to compare the corresponding proportions between different categories.</p><p>Unit to quantity (226, 16.5%). An icon in a pictorial visualization can serve as a unit, which represents a certain number of data points. For example, the Fallen of World War II uses each unit (an anthropomorphized icon) to represent 1,000 people who died in the war <ref type="bibr" target="#b23">[24]</ref>, attempting to make the design more unique and elicit empathy from the audience. Thus, mapping unit to quantity constitutes another portion of data binding types. Fig. <ref type="figure" target="#fig_0">1</ref> (r) and (s) respectively transforms a bar chart into a pictorial visualization by stacking multiple units vertically or horizontally based on different category.</p><p>Unit to proportion (207, 15.1%). Another portion of pictorial visualizations apply unit-to-proportion mappings and use each unit to visualize a certain percentage of data points. For example, Fig. <ref type="figure" target="#fig_0">1 (t</ref>) transforms a waffle chart into a pictorial visualization by replacing each cell with a colored unit whilst units visualizing the same category are stacked closely.</p><p>Unit filled with color to proportion (151, 11.0%). Mapping unit filled with color to proportion is an illustrative manner to communicate data and has become increasingly prevalent in modern infographics visualizing simple data facts. For example, to show that two out of ten people worldwide are at risk of severe COVID-19, Fig. <ref type="figure" target="#fig_0">1</ref> (u) can fill two units with color and greys out the remaining units. To further compare the pandemic over the recent three years, Fig. <ref type="figure" target="#fig_0">1</ref> (v) can visualize data from multiple categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">VISTYLIST SYSTEM DESCRIPTION</head><p>In this section, we introduce Vistylist, a design support tool that facilitates pictorial visualization design with visual style transfer. Specifically, we present how Vistylist addresses the design requirements DR1-DR4 by describing its user interface, user interaction, and computational pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">User Interface</head><p>The user interface of Vistylist consists of four main components: (1) a design component supports extracting the visual style from a source pictorial visualization (Fig. <ref type="figure" target="#fig_2">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Vistylist in Action</head><p>We describe how Vistylist works in action via a usage scenario. The scenario addresses the important interactions and functions of Vistylist by illustrating how a user creates pictorial visualizations.</p><p>Extraction (DR2) Imagine that the information designer, Alice wishes to design a pictorial visualization about global trends in obesity using Vistylist. Initially, Alice uses an example she collected from an online gallery as a source pictorial visualization and uploads it to the Design Panel (A). Vistylist automatically segments its visual elements including background, text, icon, and visualization. When selecting a specific visual element, its associated visual style and displayed. For example, Alice can select the text element to explore its color and font.</p><p>Generation (DR1, DR3) Next, Alice uploads an obesity dataset to the Data Panel (B), where a set of data fields extracted from the dataset is displayed. She selects a block chart as the visualization type and uses size and color, respectively, to encode rate and country. Alice then clicks the 'Generate' button. She finds that the resulting pictorial visualization on the Canvas (C) automatically applies the visual style extracted from the source and uses icons that are semantically associated with the data field, 'obesity'. She also observes that Vistylist extends the three-color palette extracted from the source to a five-color palette to match the number of countries. She can adjust the background color, font style, and icon of the result, or clear the canvas if necessary. First, Alice finds the icon less satisfying and decides to replace it with a new one. To do this, Alice selects the icon and then clicks the button in the toolbar. Vistylist then suggests alternative icons, which are displayed in the Icon Panel (Fig. <ref type="figure" target="#fig_2">2: D</ref>). Alice can scroll down to explore another set of suggestions. If Alice observes a preferable icon, she can select it to replace the existing one. She can also upload her own icon and use it in the result. Second, Alice would like to add a feeling of elegance and modernity to the overall appearance of the result by adjusting its color. She selects the legend item marker of Hungary to adjust the color of its associated elements. The Color Picker pops up and suggests a set of harmonious colors based on the current scheme and also allows Alice to set the parameters of R, G, B channels.</p><p>Suggestion (DR4) During the process, the Suggestion Panel (Fig. <ref type="figure" target="#fig_2">2</ref>: E) provides alternative designs of pictorial visualizations for Alice to explore. The suggestions are categorized by similarity, aesthetics, and inspiration. Specifically, similarity measures the results generated by Vistylist by comparing them with the source pictorial visualization, and the most similar one amongst the results is ranked highest. Aesthetics evaluates the results from the perspectives of color and layout while inspiration provides design alternatives by suggesting different visualization and data binding types. Alice clicks the 'inspiration' icon button and finds that Vistylist recommends two visualization types different from the one specified by Alice, including bar and circle charts, in the Suggestion Panel. The two design alternatives also visualize the same data fact of proportion as the resulting block chart. Such "non-obvious" choices inspire Alice to think of sparking design ideas she might not have initially considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Computational Pipeline</head><p>Given a source pictorial visualization as visual input and a dataset as data input, the objective of our computational pipeline is to separate the visual style of the source from its content and recombine the style with the data content of an intended pictorial visualization showing the input dataset. Specifically, the computational pipeline consists of three major components: Extraction, Generation, and Suggestion modules.</p><p>Extraction For a source pictorial visualization, we first attempted to decompose it into visual elements that characterize its style. Based on the preliminary study and literature review <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b61">63,</ref><ref type="bibr" target="#b77">79]</ref>, we identified that color, font, and icon are the key elements to extract. We excluded layout as it varies depending on the visualization type being used.</p><p>To perform element extraction, we applied one of the state-of-the-art object detection methods proposed by Lu et al. <ref type="bibr" target="#b45">[46]</ref>. Their method extended YOLO <ref type="bibr" target="#b57">[59]</ref> and can distinguish between four categories of data elements of infographics, including text, icon, index, and arrow. After successfully detaching the identified elements (text and icon) from the source, we extracted their features as follows.</p><p>Color. In terms of color, we extracted background color, font color, and color scheme from a source pictorial visualization using K-means clustering method. We also extend the extracted color scheme to match the data cardinality using the color palette rating model proposed by Kita et al. <ref type="bibr" target="#b31">[32]</ref>. The model learns human aesthetic preferences from a larger-scale palette dataset. Imagine the case that an M-color palette is extracted while a K-category data attribute is provided (M &lt; K), Vistylist will suggest K-M more harmonious colors.</p><p>Font. To identify the font from an image, we used DeepFont <ref type="bibr" target="#b70">[72]</ref>, which leverages a domain adapted Convolutional Neural Network (CNN) that recognizes the font of English text lines. DeepFont is capable of identifying 150 font families such as AmericanaStd and HelveticaLTStd-Roma.</p><p>Icon style. To learn the style of icons, we first collected icons from a range of online <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b52">53]</ref> as icon assets. Our icon assets is a collection of 3,233 icons whose example categories include people, food, and city. Then, we used a Siamese Neural Network (SNN) <ref type="bibr" target="#b36">[37]</ref> to measure style consistency. SNN maps input icons into a 256-dimensional feature space and compares the Euclidean distance between an icon pair. A closer distance indicates a more consistent style between the two icons. Thus, icons that are grouped in the same region are of a consistent style, such as the 12 colored regions in shown Fig. <ref type="figure" target="#fig_3">3</ref>.</p><p>Generation To generate an intended pictorial visualization, we performed a three-step procedure: (1) drafting a pictorial visualization based on the dataset and design pattern. To do this, we first constructed a pictorial visualization library based on the identified design patterns (Fig. <ref type="figure" target="#fig_0">1</ref>). As a result, our library includes 22 charts categorized by six visualization types and seven data binding types. As a user selects a specific visualization type and field-channel mapping, Vistylist can suggest appropriate data binding types and use the corresponding charts as drafts; (2) automatically retrieving icons that are semantically associated with the selected data attributes. In doing so, Vistylist is able to generate a pictorial visualization whose content is aligned with data input; and (3) transferring the extracted visual style (color, font, icon) to the intended pictorial visualization. For example, the retrieved icons should be stylistically consistent with the icons extracted from the source. Next, we will introduce semantic association in (2) and style consistency in (3) in detail.</p><p>Semantic Association. Given a specific categorical value of a dimension such as 'water', Vistylist performs a look-up in the WordNet <ref type="bibr" target="#b49">[50]</ref>. Specifically, Vistylist uses the Word2Vec technique <ref type="bibr" target="#b55">[56]</ref> to find the nearest neighbor concept of 'water' such as 'drop' whose label exists in our icon assets. Then, it retrieves the corresponding icon of the concept in the icon assets. Thus, the retrieved icon 'drop' is semantically related to the selected categorical value 'water'.</p><p>Style Consistency. To achieve style consistency, our goal is to retrieve icons of a consistent style to the icons extracted from a source pictorial visualization. We determined the icon whose style is the most consistent with that of the extracted icons by finding their nearest neighbor and corresponding group in the feature space in Fig 3 <ref type="figure">.</ref> Then, we can retrieve icons from that group given a certain categorical value.</p><p>Suggestion To suggest alternative designs of pictorial visualizations, we considered three different criteria, including similarity, aesthetics, and inspiration, which are calculated as follows. Similarity. To evaluate the similarity between a source pictorial visualization and an intended pictorial visualization, we used the simhash <ref type="bibr" target="#b48">[49]</ref> by converting the source image into a high-dimensional feature vector and then transforming the feature vector into a 64-bit fingerprint. By comparing the fingerprints of the source and intended pictorial visualizations using cosine distance, we can measure the similarity between them.</p><p>Aesthetics. The aesthetics of the pictorial visualizations generated by Vistylist is measured from the perspectives of both color and layout. In terms of color, we considered color harmony <ref type="bibr" target="#b29">[30]</ref> and color discriminability <ref type="bibr" target="#b44">[45]</ref>, which measure if neighboring colors cause a pleasing effect and produce a distinct difference in lightness, respectively. Regarding layout, we followed two design principles <ref type="bibr" target="#b54">[55]</ref>, including white space and balance. Specifically, white space evaluates if elements are organized in a stable and conventional layout while balance requires the distribution of elements evenly throughout a layout.</p><p>Inspiration. To support design ideation by reducing design fixation and promoting divergent thinking <ref type="bibr" target="#b6">[7]</ref>, Vistylist also suggests pictorial visualizations of different visualization and data binding types. The suggestions may include those that reflect a different design pattern according to the 22 design patterns shown in Fig. <ref type="figure" target="#fig_0">1</ref>, relative to the one initially specified by users. For example, to compare proportion between different categories, our system can suggest bar (Fig. <ref type="figure" target="#fig_0">1 (o)</ref>) and circle (Fig. <ref type="figure" target="#fig_0">1 (g)</ref>) charts in addition to block chart (Fig. <ref type="figure" target="#fig_0">1 (v)</ref>). Also, these charts apply different data binding types such as semantics to category and height to proportion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION</head><p>In our evaluation, we estimated the usability and usefulness of Vistylist through a survey of example pictorial visualizations, a controlled user study, and a series of expert interviews.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Example Pictorial Visualizations and Survey</head><p>To demonstrate the expressiveness of Vistylist, we created a gallery of diverse pictorial visualizations by using data samples from our dataset as source pictorial visualizations and real-world datasets as data input.  <ref type="bibr" target="#b67">[69]</ref>, (c) <ref type="bibr" target="#b21">[22]</ref>, (d) <ref type="bibr" target="#b17">[18]</ref>, (e) <ref type="bibr" target="#b18">[19]</ref>, (f) <ref type="bibr" target="#b22">[23]</ref>, (g) <ref type="bibr" target="#b15">[16]</ref>, and (h) <ref type="bibr" target="#b32">[33]</ref>. Fig. <ref type="figure" target="#fig_4">4</ref> shows example pictorial visualizations that cover some of the visualization and data binding types illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>To further evaluate the example pictorial visualizations, we conducted a survey to compare our results with the examples created by Retrieve-Then-Adapt (RTA) <ref type="bibr" target="#b56">[58]</ref>. First, we selected eight sourceresult pairs from our gallery and RTA (Fig. <ref type="figure" target="#fig_0">10</ref>  <ref type="bibr" target="#b56">[58]</ref>), respectively, resulting in 16 pairs. Then, we selected the remaining pictorial visualization pairs with single facts from RTA (Fig. <ref type="figure" target="#fig_0">10</ref> (a)(d)(e)(j)(n) in <ref type="bibr" target="#b56">[58]</ref>) and used their sources to generate five more pictorial visualizations using Vistylist, resulting in five triples, as shown in Fig. <ref type="figure" target="#fig_5">5</ref>. We also restricted the source and the results to the same visualization type to ensure a fair comparison between RTA and Vistylist.</p><formula xml:id="formula_0">(b)(c)(g)(h)(i)(k)(l)(o) in</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Methodology</head><p>We recruited 80 participants aged between 21 and 39 (M = 32.5, SD = 6.5) from Prolific [57] for our survey. The survey consisted of three sessions. Session 1 introduced our research intent and the structure of the survey. In Session 2, we randomized the order of the 21 groups of pictorial visualizations, including 16 pairs and 5 triples, and presented them to the participant, one group at a time. In each group, the participants were asked to compare the resulting pictorial visualization(s) with the source. Then, the participants were instructed to rate the quality of the resulting pictorial visualization(s) using a 5-point Likert scale and the measurements include faithfulness, consistency, expressiveness, flexibility, individuality, and inspiration. The measurements were selected from the high-level perceptual quality metrics regarding information design <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b28">29]</ref> and considering the design guidelines that support usability <ref type="bibr" target="#b19">[20]</ref>. After rating each group, the participants were also encouraged to explain their options. Each participant was paid $8.00 for completing the task. Session 3 asked the participants to fill out a demographic questionnaire. The three sessions lasted about 15-25 minutes for each participant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Analysis and Results</head><p>We conducted a paired t-test to examine if there is a significant difference among each measurement. Fig. <ref type="figure" target="#fig_7">6</ref> shows the results.</p><p>Vistylist produced significantly better results than RTA in terms of faithfulness (Fig. <ref type="figure" target="#fig_7">6</ref> (a)) (Vistylist: M = 3.67, SD = .62, RTA: M = 3.44, SD = .68) and expressiveness (Fig. <ref type="figure" target="#fig_7">6 (c</ref>)) (Vistylist: M = 3.73, SD = .62, RTA: M = 3.38, SD = .71). The reason may be that welldesigned pictographic icons not only enhance the artistry of information design, but also augment the semantic features of the data. However, Vistylist shows a significantly lower consistency compared to RTA (Fig. <ref type="figure" target="#fig_7">6 (b</ref>)) (Vistylist: M = 3.48, SD = .63, RTA: M = 3.80, SD = .72). One participant commented, "it's because of the layout I think, this pictograph is more identical to the example in layout (Fig. <ref type="figure" target="#fig_5">5 (b)</ref>, bottom)." Another participant noted, "using different visualization types causes inconsistency (Fig. <ref type="figure" target="#fig_4">4 (e)</ref>)." However, some participants had different opinions, explained that "'perfect' imitation cannot be considered as original work. I think Vistylist's style transfer uses a good degree of imitation by catching the personality of a design." Vistylist was rated significantly higher in flexibility (Fig. <ref type="figure" target="#fig_7">6</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">User Study</head><p>To evaluate the effectiveness of Vistylist, we conducted a user study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Methodology</head><p>In the study, we recruited participants by posting advertisements on the social media platforms of our lab. Our recruitment material indicated that we were looking for visualization researchers or practitioners who are experienced in designing pictorial visualizations. 14 participants (7 females) aged between 21 and 39 (M = 25.64, SD = 4.43) were recruited,including college students, researchers, and professionals from diverse backgrounds such as art and design, computer science, data science, and electronic engineering. All of the participants reported that they have experience in visualization: less than 1 year: 2, 1-2 years: 4, 2-5 years: 5, more than 5 years: 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data and Material</head><p>We provided the participants with three datasets, whose topics include how people allocate their time across daily activities, fruit sales in different cities, and popular cities in the world. According to previous studies <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref>, we also provided the participants with a set of extracted data facts (e.g., rank, proportion) and available visualization types <ref type="bibr" target="#b64">[66]</ref> for each dataset rather than raw data, keeping the focus of the user study on pictorial visualization design. In terms of materials, we first selected a group of six pictorial visualizations with similar visual complexity from our dataset. The six pictorial visualizations were provided to the participants as sources for their design tasks.</p><p>Tasks and Procedure The study task asked the participants to select a group of three source pictorial visualizations from the materials and create corresponding pictorial visualizations by referring to the sources. The study began with a 10-minute introduction explaining the goal of our study and the details of the three datasets. In each task, we first demonstrated the tutorial of our system. At the end of the study, we collected 52 pictorial visualizations in total from the participants and conducted a 15-minute semi-structured interview. After that, each participant filled out a brief demographic survey and completed a questionnaire using a 7-point Likert scale. The study lasted approximately 0.5-1.0 hours for each participant. The interaction process, completion time, and semi-structured interview were recorded for later analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Analysis and Results</head><p>We conducted both quantitative and qualitative analysis on the results of our user study. The quantitative results of the participants' responses to the questionnaire are included in Supplemental Material. In terms of qualitative analysis, our goal is to understand the participants' thoughts and suggestions in more depth. To do this, we first transcribed the audio recordings and then coded the data based on the questions about the effectiveness of AI-supported design and pictorial visualization design, following the thematic analysis process <ref type="bibr" target="#b10">[11]</ref>. Specifically, two researchers first independently read through all the transcriptions and generated codes related to the questions. At last, we met for two sessions to group similar codes and discuss mismatches until we reached a 100% agreement on the final three themes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inclusion of Suggestions</head><p>We observed that the average frequency of using suggestions of inspiration is higher than similarity and aesthetics (#inspiration: 2.79, #similarity: 1.71, #aesthetics: 0.21). The reason may be that inspiration can trigger one to think alternatives of representing data. Such different representations sometimes inspire new ways of analyzing data. P12 noted, "when exploring these suggestions, I found the recommended pie chart showing the percentage of fruit sales in different seasons, then I found that people buy more fruits in summer, which made me want to explore the reason behind it."</p><p>Perceived AI Helpfulness A majority of the participants agreed that AI can help automate their work in creating pictorial visualizations. Specifically, the participants provided positive feedback on the visual styles of color, icon, and font. "It helps save time by automatically extracting the color scheme instead of requiring users to manually use a color picker" (P9), "I noted that it applies icons of a similar design style such as flat design" (P4), "it tells me which typeface was used, that's helpful" (P11). In addition to style extraction, the participants also commented on icon retrieval, "the two charts I've created exhibit different icon styles, one uses stroke (Fig. <ref type="figure" target="#fig_8">7 (a)</ref>) while the other uses fill (Fig. <ref type="figure" target="#fig_8">7 (b)</ref>)" (P4). More than half of the participants acknowledged that Vistylist is able to generate high-quality pictorial visualizations. P12 commented, "by comparing the pictorial visualizations I designed with the two systems, I can see that the set generated by Vistylist looks more elegant. I don't have to adjust many details myself." P12 said, "I'm pretty satisfied with the results generated by Vistylist, such a variety of fancy pictorial visualizations. I only have to fine-tune the results such as changing the color or size of the text." In addition, some participants commented on the user experience of Vistylist, indicating it "provides a more user-friendly user interface that shortens the learning curve and supports a better workflow" (P13).</p><p>Design Ideation and Realization 12 out of 14 participants felt that the design alternatives provided by Vistylist are useful, as "they effectively portray the dataset and enable variations in representation" (P7). P12 suggested, "I initially decided to use a pie chart, but then I noted that a ring chart was recommended and thought this one looks better." P3 commented, "It helps ideation by suggesting different chart types." P2 said, "it supports exploring different types of pictographs... When finishing one design, I can quickly move to the next one." Besides, the participants appreciated that our system can support design realization in terms of accuracy. For example, P10 noted, "these visualizations serve as a faithful interpretation of the data and emphasize certain aspects in communication." P2 commented on the semantics of icons, "what impressed me most is the function of data-icon mapping. In this chart illustrating spring, summer, autumn, and fall, icons are semantically related to data categories and are of a consistent style." while P7 said, "I was surprised by the quality of the icon itself and the matching between the icon and data. These iconic representations accurately communicate the underlying messages of data." Regarding aesthetics and expressiveness, 10 out of 14 participants implied that Vistylist supports creating better pictorial visualizations. "They're aesthetically appealing and can be used in infographics" (P2).</p><p>Customized Design for Different Applications Vistylist can be used to quickly generate information design and make it accessible and understandable to a broader audience. For example, to educate the public about scientific knowledge, it needs to be easy to understand and disseminate. To this end, transforming underlying data into pictorial and symbolic design is desired. Vistylist can effectively generate information design accommodating different scenarios. P2 said,"I can imagine using it to visualize and educate marine science, teenagers will enjoy reading it. Also, Vistylist can also be helpful in the domain of data journalism, which requires real-time and faithful presentation of data facts due to its timeliness and authority, "I can easily generate a series of data news reports with a consistent visual style, reducing a large portion of the workload" (P2). Similarly, short videos, as an emerging medium, always seek to incorporate featured design elements to catch the viewer's eye. Vistylist can be useful for such short video creators by allowing users to upload icons or clipart and use them in information design. P3 said, "using Vistylist can efficiently generate a unique design that I can apply to my TikTok videos."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Expert Interview</head><p>To further how Vistylist is used when creating infographics, we performed a series of interviews with two domain experts who are experienced in information design. The first (E1) is an assistant professor in art and design at a university, whose research applies visualization techniques. The second (E2) is an information designer at a news agency with over eight years of experience in designing infographics. Before the interviews, the experts were instructed to use Vistylist to craft a group of pictorial visualizations using the datasets and source pictorial visualizations they prepared themselves. While exploring the system, the experts used the think-aloud protocol to add any comments and suggestions they might have. After finishing the task, each of the experts was encouraged to design an infographic by assembling the resulting pictorial visualizations and adding more details such as headlines and embellishments using graphics editors such as Adobe Illustrator. Fig. <ref type="figure" target="#fig_8">7 (c</ref>) and (d) show the two infographics created by E1 and E2, respectively.</p><p>Vistylist Facilitates the Design of Infogrpahics Both experts agreed that our system can support the design of infographics by facilitating pictorial visualization authoring. E2 said, "it's super effective to craft a group of isotypes, especially for infographics. The system can speed up the design tasks and ensure their quality at the same time." E1 noted, "I love the idea of applying style transfer to visualization design. It helps creating multiple pictorial visualizations with a consistent style and reducing the workload of manually adjusting colors for each of them." When asked about additional editing behaviors they performed when designing infographics, E1 noted, "Vistylist generates a group of high-quality pictographs, and I can directly use these pictographs as visual materials in my infographic" while E2 reflected, "I mainly refined the font size and font color of the visualizations when designing the infographic. For example, I uniformly specified the font size such as label and caption. I also tweaked its color to increase the readability of the numbers." Both experts noted being surprised by the fact that AI can augment human creative capabilities. E1 reflected, "it lowers the high barrier for users without design skills or talents." E2 noted, "once I click the Generate button, I can get a pictorial visualization that is at least 80% similar to the given example. It makes pictograph design much easier and faster. I'll recommend this tool to my colleagues."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>Designing pictorial visualizations with the intent of supporting datadriven storytelling has become a common practice among visualization designers. When asked about the main benefit that pictorial visualizations provide, most participants explained that such iconic representations are "novel and expressive", and "intuitive for simple facts", enticing the audience to inspect a visualization more closely. Despite these advantages, pictorial visualizations should be carefully designed to prevent misuse or overuse. For example, four participants noted that when visualizing proportion using a block chart (Fig. <ref type="figure" target="#fig_0">1</ref> (r)), misunderstanding can easily occur if the icon is of an "unfixed-width" shape. Accordingly, designing pictorial visualizations should strike a balance between the faithful portrayal of data and its expressive presentation. To better explore the design patterns of pictorial visualizations, future work should explore potential graphical and data properties and more. Previous research has studied various dimensions of the design space for pictorial visualizations, such as human shape <ref type="bibr" target="#b9">[10]</ref> and verisimilitude <ref type="bibr" target="#b51">[52]</ref>. More graphical properties such as texture and space are worth investigating. Animation is another research direction to explore, which can encode dynamic data and promote storytelling <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b65">67]</ref>. Also, we are interested in examining embodiment-related properties that can elicit various affective responses from the audience <ref type="bibr" target="#b38">[39]</ref>.</p><p>Supporting open-ended creative tasks such as information design with AI is of growing interest to both academia and industry. For example, style transfer is able to assist practitioners as organizations develop their own style guides for information design. A specific style guide can be transferred to newly-produced content and vice versa, a specific style guide can be updated by extracting features from newlydesigned work. However, style transfer for information design can introduce ethical issues. For example, allowing directly imitating the style of existing visualizations can be harmful if style transfer is used by bad actors to generate misleading information graphics that have the visual appearance of a reputable source. Also, if the source design and icon assets are uniquely associated with an individual designer's style, one possible concern is the abuse of style transfer to plagiarize or impersonate the work of other designers. Thus, artificial moral agents (AMAs) who can participate alongside humans in real-world ethical decision-making processes are desired <ref type="bibr" target="#b78">[80]</ref>. It is important for AMAs to detect plagiarism in the context of visualization design to protect and preserve the moral and material rights of all parties, especially if such plagiarism is not intentional. For example, AMAs should warn users of the material involved in the plagiarism to avoid potential risks.</p><p>There are several limitations in this work. First, the component of feature extraction in our computational pipeline can produce unsatisfactory results depending on the quality and content of source pictorial visualizations. For example, if a given source contains different text styles such as titles and subtitles, each with different colors, it is currently not possible for Vistylist to differentiate between text styles and extract colors accordingly. Second, we observed problematic mismatched or stereotype icon retrieval. For example, two participants noted that occasionally the semantics of the retrieved icons are not perfectly aligned with the data attribute while another participant raised the concern that bad-designed icons may suggest gender inequality such as social inequality and glass ceiling. These unsatisfying recommendations may impact the faithful portrayal of data or result in visualization design that perpetuates harmful stereotypes. Third, some participants noted that the component of color assignment employs a simple strategy and thus overlooks selecting semantically-resonant colors for visualization <ref type="bibr" target="#b40">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this work, we introduced Vistylist, a design support tool that facilitates pictorial visualization authoring with visual style transfer. Our work also shed light on a deeper understanding of the design patterns of content presentation in pictorial visualizations. Quantitative and qualitative feedback from the evaluation showed that Vistylist can effectively help design faithful and expressive pictorial visualizations. Future work includes constructing the design space for pictorial visualizations, exploring design guidelines for co-creation with AI, deploying Vistylist in designing infographics and data videos. We hope this work can contribute to the exploration and development of data-driven approaches to information designas well as example-based design.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The design patterns that capture content presentation of pictorial visualizations.</figDesc><graphic url="image-4.png" coords="4,64.04,256.47,441.84,61.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>: A); (2) a visualization component for creating intended pictorial visualizations based on the input dataset (Fig. 2: B); (3) a canvas for exploring and adjusting the resulting visualizations (Fig. 2: C, D); and (4) a suggestion capability for recommending alternative designs of pictorial visualizations based on various criteria (Fig. 2: E). Specifically, the Design Panel allows a user to upload a source pictorial visualization, whose visual elements and styles are automatically extracted and displayed. The user can upload a dataset in the Data Panel and select one of the six visualization types (bar, circle, line, block, area, and number chart) to create field-channel mappings. The toolbar at the top of the Canvas contains icon buttons of fill color, font, font size, font color, bold, italic, icon, save, and delete, which facilitates the adjustment of the color, text, and icons of pictorial visualizations. Clicking the button enables the Icon Panel to explore more icons. The Suggestion Panel lists alternative designs of pictorial visualizations categorized by similarity, aesthetics, and inspiration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The user interface of Vistylist consists of four main components: (1) users upload their source pictorial visualizations by dragging png/jpg files into the Design Panel (A) to extract visual elements, (2) users then upload their datasets by dragging CSV files into the Data Panel (B) to create field-channel mappings, (3) users can view the resulting pictorial visualizations on the Canvas (C), where they can adjust icons using the Icon Panel (D), and (4) the Suggestion Panel (E) suggested alternative pictorial visualizations. The source image uploaded by the user is shown on the left.</figDesc><graphic url="image-8.png" coords="6,63.90,73.00,221.10,206.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. t-SNE visualization of representative icons in our icon assets. We manually marked 12 clusters based on the proximity of these clusters.</figDesc><graphic url="image-13.png" coords="6,228.79,503.09,66.27,180.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Example pictorial visualizations created with Vistylist. Each pictorial visualization combines the content visualizing an input dataset with the style of a source pictorial visualization (bottom left). In (a)-(d), each source-result pair uses the same visualization type while in (e)-(h), each pair uses different visualization types. The sources are collected from online sources: (a) [60], (b)<ref type="bibr" target="#b67">[69]</ref>, (c)<ref type="bibr" target="#b21">[22]</ref>, (d)<ref type="bibr" target="#b17">[18]</ref>, (e)<ref type="bibr" target="#b18">[19]</ref>, (f)<ref type="bibr" target="#b22">[23]</ref>, (g)<ref type="bibr" target="#b15">[16]</ref>, and (h)<ref type="bibr" target="#b32">[33]</ref>.</figDesc><graphic url="image-15.png" coords="7,87.46,72.77,229.72,197.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The example pictorial visualizations from Retrieve-Then-Adapt [58], (a) and (d), were used as sources for Vistylist. Here, (b) and (e) show the results generated by Retrieve-Then-Adapt (RTA) while (c) and (f) show the results generated by Vistylist.</figDesc><graphic url="image-17.png" coords="7,63.91,563.02,132.29,103.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(d)) (Vistylist: M = 3.67, SD = .72, RTA: M = 3.32, SD = .78), individuality (Fig. 6 (e)) (Vistylist: M = 3.48, SD = .62, RTA: M = 3.32, SD = .78) and inspiration than RTA (Fig. 6 (f)) (Vistylist: M = 3.67, SD = .65, RTA: M = 3.39, SD = .75). The reason may be that Vistylist provides various visualization types, which can better express the user's design intent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Means and standard errors of each measurement in Retrieve-Then-Adapt (RTA) and Vistylist conditions ( * : p &lt; .05, * * : p &lt; .01).</figDesc><graphic url="image-19.png" coords="8,64.05,579.50,132.29,100.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Pictorial visualizations (a)(b) created with Vistylist by participant from the user study. For both (a) and (b), the left image collected from online sources ((a) [48], (b) [21]) shows the sources while the right image shows the intended pictorial visualization. Infographics created by Expert 1 (c) and Expert 2 (d), respectively, from the expert interview. Specifically, (c) includes five pictorial visualizations generated by Vistylist while (d) contains four pictorial visualizations generated by Vistylist.</figDesc><graphic url="image-22.png" coords="8,479.94,505.85,66.27,130.63" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>Nan Cao is the corresponding author. This work was supported by NSFC 62072338 and NSF Shanghai 20ZR1461500. We would like to thank all the reviewers for their constructive feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<ptr target="https://www.adobe.com/products/illustrator.html" />
	</analytic>
	<monogr>
		<title level="j">Adobe. Adobe illustrator</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Understanding data videos: Looking at narrative visualization through the cinematography lens</title>
		<author>
			<persName><forename type="first">F</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Henry Riche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hurter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2015 CHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1459" to="1468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hooked on data videos: assessing the effect of animation and pictographs on viewer engagement</title>
		<author>
			<persName><forename type="first">F</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Riche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leboe-Mcgowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 International Conference on Advanced Visual Interfaces</title>
				<meeting>the 2018 International Conference on Advanced Visual Interfaces<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Authoring data-driven videos with dataclips</title>
		<author>
			<persName><forename type="first">F</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Riche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Monroy-Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="501" to="510" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Arntz</surname></persName>
		</author>
		<ptr target="http://www.gerdarntz.org" />
		<title level="m">Gerd arntz web archive</title>
				<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Design patterns for data comics</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Murray-Rust</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Henry Riche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2018 CHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spinneret: Aiding creative ideation through non-obvious concept associations</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O.-H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chandrasegaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2020 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Quality metrics for information visualization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Behrisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blumenschein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>El-Assady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Seebacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Diehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Brandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="625" to="662" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">What makes a visualization memorable?</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Borkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2306" to="2315" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Showing people behind data: Does anthropomorphizing visualizations elicit more empathy for human rights data?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Boy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Satterthwaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Nov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bertini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2017 CHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5462" to="5474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Using thematic analysis in psychology</title>
		<author>
			<persName><forename type="first">V</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="77" to="101" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Designing with pictographs: Envision topics without sacrificing understanding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Franconeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cairo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mahyar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><surname>Bytedance</surname></persName>
		</author>
		<author>
			<persName><surname>Iconpark</surname></persName>
		</author>
		<ptr target="https://iconpark.oceanengine.com/home" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><surname>Canva</surname></persName>
		</author>
		<author>
			<persName><surname>Canva</surname></persName>
		</author>
		<ptr target="https://www.canva.com/" />
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards automated infographic design: Deep learning-based auto-extraction of extensible timeline</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="917" to="926" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Average energy prices for manufacturers</title>
		<author>
			<persName><surname>Conceptdraw</surname></persName>
		</author>
		<ptr target="https://www.conceptdraw.com/solution-park/charts-picture-graphs" />
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Text-to-viz: Automatic generation of infographics from proportion-related natural language statements</title>
		<author>
			<persName><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-G</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="906" to="916" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">People bar graph</title>
		<author>
			<persName><forename type="first">J</forename><surname>Decker</surname></persName>
		</author>
		<ptr target="https://www.pinterest.com/pin/225320787581742920/,2022" />
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Decsesznak</surname></persName>
		</author>
		<ptr target="https://news.bitcoin.com/millennial-analysis-bitcoin-competitors/" />
		<title level="m">Millennial analysis: Bitcoin vs competitors</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Human-Computer Interaction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Finlay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Abowd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Beale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Pearson Education Limited</publisher>
			<pubPlace>England</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Electricity set of industry power infographic in flat style</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Drozdova</surname></persName>
		</author>
		<ptr target="https://www.alamy.com/stock-photo-electricity-set-of-industry-power-infographic-in-flat-style-176458585.html" />
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">public-opinion-trumps-the-border-wall-400x.png?</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gillespie</surname></persName>
		</author>
		<ptr target="https://vizzlo.com/img/vizzards/examples/pictograph/american-" />
		<imprint>
			<date type="published" when="2018">2018. 2022</date>
		</imprint>
	</monogr>
	<note>American public opinion trumps the border wall</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Guardian</surname></persName>
		</author>
		<ptr target="https://www.theguardian.com/world/datablog/interactive/2013/apr/09/north-korea-south-korea-interactive" />
		<title level="m">North korea south korea in figures</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The fallen of world war ii</title>
		<author>
			<persName><forename type="first">N</forename><surname>Halloran</surname></persName>
		</author>
		<ptr target="http://www.fallen.io/ww2/" />
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Isotype visualization: Working memory, performance, and engagement with pictographs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Haroz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kosara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Franconeri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2015 CHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1191" to="1200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">K</forename><surname>Inc</surname></persName>
		</author>
		<author>
			<persName><surname>Kaggle</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Design fixation. Design studies</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Jansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Metamap: Supporting visual metaphor ideation through multi-dimensional examplebased exploration</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2021 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Designing information: Human factors and common sense in information design</title>
		<author>
			<persName><forename type="first">J</forename><surname>Katz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<pubPlace>Hoboken, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Perceptually-based color assignment</title>
		<author>
			<persName><forename type="first">H.-R</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I.-K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="309" to="318" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Data-driven guides: Supporting expressive design for information graphics</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schweickart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dontcheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Popovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="491" to="500" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Aesthetic rating and color suggestion for color palettes</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Miyata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific Conference on Computer Graphics and Applications</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="127" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Knight</surname></persName>
		</author>
		<ptr target="https://nexacu.com.au/insights-blog/creating-infographics-in-power-bi/" />
		<title level="m">Creating infographics in power bi</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">May ai? design ideation with cooperative contextual bandits</title>
		<author>
			<persName><forename type="first">J</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lucero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hegemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oulasvirta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2019 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Kosara</surname></persName>
		</author>
		<author>
			<persName><surname>Eagereyes</surname></persName>
		</author>
		<ptr target="https://eagereyes.org" />
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Webzeitgeist: design mining the web</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Satyanarayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Klemmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Talton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
				<meeting>the SIGCHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3083" to="3092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning icons appearance similarity</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lagunas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Garces</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gutierrez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="10733" to="10751" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Kineticharts: Augmenting affective expressiveness of charts in data stories with animation design</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="933" to="943" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Smile or scowl? looking at infographic design through the affective lens</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2796" to="2807" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Designing with interactive example galleries</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Brafman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Klemmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI conference on human factors in computing systems</title>
				<meeting>the 2019 CHI conference on human factors in computing systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2257" to="2266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Selecting semantically-resonant colors for data visualization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fortuna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="401" to="410" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning design semantics for mobile apps</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Craft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Situ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology</title>
				<meeting>the 31st Annual ACM Symposium on User Interface Software and Technology<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="569" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Data illustrator: Augmenting vector design tools with lazy data binding for expressive visualization authoring</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dontcheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Delorey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grigg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kerr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2018 CHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">Llc</forename><surname>Icons8</surname></persName>
		</author>
		<ptr target="https://icons8.com/" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Palettailor: Discriminable colorization for categorical data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sedlmair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Deussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="475" to="484" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Exploring visual information flows in infographics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lanir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2020 CHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Synthetically trained icon proposals for parsing and summarizing infographics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Madan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nobre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1807.10441" />
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Malhas</surname></persName>
		</author>
		<ptr target="https://malhasfakini.wordpress.com/2013/07/18/os-livros-mais-lidos-do-mundo/" />
		<title level="m">Os livros mais lidosi do mundo</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Detecting near-duplicates for web crawling</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Manku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Das</forename><surname>Sarma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on World Wide Web</title>
				<meeting>the 16th International Conference on World Wide Web<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="141" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Human Language Technology</title>
				<meeting>the Workshop on Human Language Technology<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="39" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pick me! getting noticed on google play</title>
		<author>
			<persName><forename type="first">A</forename><surname>Miniukovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">De</forename><surname>Angeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2016 CHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4622" to="4633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Showing data about people: A design space of anthropographics</title>
		<author>
			<persName><forename type="first">L</forename><surname>Morais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Andrade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dragicevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Mux</forename><surname>Iconfont</surname></persName>
		</author>
		<ptr target="https://www.iconfont.cn/" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Otto and marie neurath isotype collection</title>
		<author>
			<persName><forename type="first">O</forename><surname>Neurath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neurath</surname></persName>
		</author>
		<ptr target="https://www.reading.ac.uk/typography/collections-and-archives" />
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning layouts for single-pagegraphic designs</title>
		<author>
			<persName><forename type="first">P</forename><surname>O'donovan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1200" to="1213" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Retrievethen-adapt: Example-based automatic generation for proportion-related infographics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-G</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="443" to="452" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Research</surname></persName>
		</author>
		<ptr target="https://inkwoodresearch.com/reports/global-cloud-storage-market/" />
		<title level="m">Global cloud storage market forecast</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Klemmer. d.tour: Style-based exploration of design example galleries</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Kejriwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th annual ACM symposium on User interface software and technology</title>
				<meeting>the 24th annual ACM symposium on User interface software and technology</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Dear pictograph: Investigating the role of personalization and immersion for consuming and enjoying visualizations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Romat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Henry Riche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hurter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hinckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2020 CHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Learning style similarity for searching infographics</title>
		<author>
			<persName><forename type="first">B</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dontcheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1505.01214" />
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Narrative visualization: Telling stories with data</title>
		<author>
			<persName><forename type="first">E</forename><surname>Segel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1139" to="1148" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Automatic generation of semantic icon encodings for visualizations</title>
		<author>
			<persName><forename type="first">V</forename><surname>Setlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mackinlay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2014 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Calliope: Automatic visual data story generation from a spreadsheet</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="453" to="463" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Communicating with motion: A design space for animated visual narratives in data videos</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2021 CHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Ideawall: Improving creative collaboration through combinatorial visual stimuli</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing</title>
				<meeting>the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="594" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">business-statistics-graph-demographics-populationchart-600188342</title>
		<author>
			<persName><forename type="first">N</forename><surname>Vector</surname></persName>
		</author>
		<ptr target="https://www.shutterstock.com/zh/image-vector/" />
	</analytic>
	<monogr>
		<title level="m">Business statistics graph demographics population chart</title>
				<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title/>
		<author>
			<persName><surname>Visme</surname></persName>
		</author>
		<author>
			<persName><surname>Visme</surname></persName>
		</author>
		<ptr target="https://www.visme.co" />
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Infonice: Easy creation of information graphics</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2018 CHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Deepfont: Identify your font from an image</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Multimedia</title>
				<meeting>the 23rd ACM International Conference on Multimedia<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="451" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Understanding and modeling userperceived brand personality from mobile application uis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2019 CHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Dataink: Direct and creative data-oriented drawing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Henry Riche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chevalier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">De</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wigdor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2018 CHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Ideaterelate: An examples gallery that helps creators explore ideas in relation to their own</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Dow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">N</forename><surname>Yau</surname></persName>
		</author>
		<author>
			<persName><surname>Flowingdata</surname></persName>
		</author>
		<ptr target="https://flowingdata.com" />
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Cooks or cobblers? crowd creativity through combination</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Nickerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2011 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1393" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Dataquilt: Extracting visual elements from images to craft pictorial visualizations</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sultanum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bezerianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chevalier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2020 CHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM, ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">What characterizes personalities of graphic designs?</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Beyond kant and bentham: How ethical theories are being used in artificial moral agents</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zoshak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2021 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
