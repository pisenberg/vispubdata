<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DendroMap: Visual Exploration of Large-Scale Image Datasets for Machine Learning with Treemaps</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Md</roleName><forename type="first">Donald</forename><surname>Bertucci</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Montaser</forename><surname>Hamid</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yashwanthi</forename><surname>Anand</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Anita</forename><surname>Ruangrotsakun</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Delyar</forename><surname>Tabatabai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Melissa</forename><surname>Perez</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Minsuk</forename><surname>Kahng</surname></persName>
						</author>
						<title level="a" type="main">DendroMap: Visual Exploration of Large-Scale Image Datasets for Machine Learning with Treemaps</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-03-27T09:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Visualization for machine learning</term>
					<term>image data</term>
					<term>treemaps</term>
					<term>visual analytics</term>
					<term>data-centric AI</term>
					<term>error analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. 1. With DendroMap, users can explore large-scale image datasets by overviewing the overall distributions and zooming down into hierarchies of image groups at multiple levels of abstraction. In this example, we visualize images of the CIFAR-100 dataset by hierarchically clustering the image representations obtained from a ResNet50 image classification model. (B) Treemap View displays these clusters of images organized as a hierarchical structure by adapting Treemaps. By clicking on a cluster, a user can interactively (C) Zoom into that image group, revealing subgroups that replace and fill the available space with animation. The user clicked on a cluster for organism images, which creates distinct subgroups of fish, insects, fruits, and flowers. With (A) Sidebar View, the user can dynamically adjust the number of clusters to be displayed and inspect the class-level statistics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The machine learning (ML) community is increasingly aware of the importance of understanding datasets. There is a growing interest in Data-Centric AI, as opposed to the model-centric approach <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b62">62]</ref>. A deep understanding of the datasets can help inform design decisions for building ML models efficiently and appropriately. It motivates important decisions such as collecting more data, changing data labeling policy, debiasing models, and more. While images are used extensively in deep learning, fundamental challenges exist in exploring image datasets because they lack attributes like those found in tabular data. A commonly-used approach is to use dimensionality reduction (DR) techniques like t-SNE <ref type="bibr" target="#b57">[57]</ref> over multivariate features extracted from the datasets <ref type="bibr" target="#b46">[47]</ref>. To enable users to easily see the contents of the images, each data point in the projected space is often replaced with its corresponding image (like in Fig. <ref type="figure">2B</ref>) <ref type="bibr" target="#b52">[52]</ref>. However, for large datasets, there could be overlaps between images, and inefficient use of space (i.e., a lot of white space) makes the size of each image too small for users to inspect.</p><p>To scale up DR methods for large image datasets, data points can be re-positioned into a grid such that no images overlap and the grid fills the entire screen (Fig. <ref type="figure">2C</ref>) <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b45">46]</ref>. It is inspired by a common Fig. <ref type="figure">2</ref>. Commonly used approaches to visualizing image datasets for ML include generating a grid of images (A); projecting the images onto 2-D displays using techniques like t-SNE (B); and using a combination of these two approaches (C). However, they do not scale well to large datasets because images are ineffectively organized and interactions are insufficiently supported. We present DendroMap (D) which effectively organizes image datasets using a modified interactive treemap algorithm.</p><p>approach to visualizing image collections-displaying images as a grid <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b69">69]</ref>. This is also the case for ML image datasets. Many tutorials for image classification begin by displaying a sample of images as a grid <ref type="bibr" target="#b54">[54]</ref>. While the combination of the grid and t-SNE methods effectively use 2-D space, it is still severely limited by the size of the image datasets. Adding interactions may help, but the use of semantic zooming is not straightforward for the gridified version of t-SNE. This is because optimization algorithms were applied that distort the original space <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b45">46]</ref>, which means the before-zoom and after-zoom versions may present very different sets of images.</p><p>In this paper, we present DendroMap, a novel interactive visualization for exploring large-scale image datasets by adapting treemaps, a well-known visualization technique. DendroMap effectively organizes images using hierarchical clustering algorithms and displays the hierarchical structure with an interactive treemap. A set of image clusters provides an overview of a dataset, and users can interactively zoom into the clusters to investigate sub-clusters in the hierarchy. Fig. <ref type="figure">1</ref> illustrates an example. It initially displays eight clusters, each showing a sample of images whose size is proportional to the total number of images in that cluster. Unlike traditional treemaps, the number of image clusters showing can be dynamically changed by the user to customize the level of abstraction. Furthermore, clicking on a cluster will zoom (Fig. <ref type="figure">1C</ref>) into that cluster to reveal and fill the space with sub-clusters.</p><p>DendroMap aims to support a wide range of analytics tasks for ML practitioners. This includes bias and error analysis at the instance and subgroup levels, which have been identified as important in the literature <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b24">25]</ref>. To build highly accurate and less biased models, it is crucial to have datasets containing a diverse set of images. DendroMap enable users to categorize the types of images present in the datasets and estimate their distributions. Furthermore, DendroMap users can identify underperforming subgroups for error analysis <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b63">63,</ref><ref type="bibr" target="#b70">70]</ref>.</p><p>Contributions. The contributions of this paper are as follows:</p><p>• DendroMap, a novel interactive visualization system for exploring large-scale image datasets used in ML. DendroMap adapts an interactive zoomable treemap and supports the information seeking mantra "overview first, zoom and filter, then details-ondemand" <ref type="bibr" target="#b51">[51]</ref>. Combined with the sidebar as a part of multiple coordinated views, ML pracitioners can perform a wide range of tasks for data-centric analysis (e.g., error analysis, bias discovery). • An adapted treemap algorithm for hierarchical dendrogram structures of images, which allows users to dynamically specify the number of clusters to visualize, enabling exploration at multiple levels of granularity. Images are systematically sampled to fill the space for each cluster, providing an overview of the datasets. • Live demo on the web<ref type="foot" target="#foot_0">1</ref> with available code<ref type="foot" target="#foot_1">2</ref> and use cases for DendroMap demonstrating users' dataset exploration, bias discovery, and error analyses.</p><p>• A quantitative user study designed to compare DendroMap with a gridified version of t-SNE, a space-filling technique used by ML practitioners. Participants performed a wide range of grouping tasks and preferred DendroMap over the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Visualization for Machine Learning</head><p>Visualization has helped ML practitioners perform a variety of analytics tasks such as: exploring datasets, analyzing performance results, interpreting and explaining model internals, building models, monitoring training progress, and debugging models <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b68">68]</ref>. Many existing visualization tools for ML support the tasks of analyzing performance results and exploring datasets at multiple levels of abstraction, ranging from individual instances to entire classes. While ML practitioners often only use summary metrics (e.g., accuracy) or class-level statistics, visualization researchers have argued the importance of instance-level analysis. Early works include ModelTracker <ref type="bibr" target="#b1">[2]</ref>, Squares <ref type="bibr" target="#b47">[48]</ref>, and Facets-Dive <ref type="bibr" target="#b60">[60,</ref><ref type="bibr" target="#b61">61]</ref>. These tools represent each instance as a small square using the unit visualization technique <ref type="bibr" target="#b42">[43]</ref>, enabling users to see individual instances in the context of aggregated information. This can work particularly well for image datasets as each square can be replaced with a thumbnail of the actual image content.</p><p>While instance-level analysis has benefits, the scale of datasets urges researchers to develop ways to slice and filter datasets, resulting in subgroup-level analysis <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29]</ref>. This allows users to specify data subsets based on attributes and perform more fine-grained analysis than at the class-level. However, image data creates a fundamental challenge in supporting such analysis because there are no attributes beyond class labels. Therefore, group structures are often created with algorithmic approaches. A common approach is to use a DR technique like t-SNE <ref type="bibr" target="#b57">[57]</ref> or UMAP <ref type="bibr" target="#b35">[36]</ref>, which are often applied to high-dimensional representations obtained from neuron activations <ref type="bibr" target="#b46">[47]</ref>. We propose an alternative approach to capturing group structures.</p><p>ML researchers have stressed the importance of datasets by coining terms like Data-Centric AI and MLOps <ref type="bibr" target="#b38">[39]</ref>. Our work aligns with this trend to ensure that ML datasets are less biased, more fair and inclusive, and contain fewer errors. A recently developed tool named Know Your Data <ref type="bibr" target="#b53">[53]</ref> aligns with this goal, providing statistics based on attributes obtained from external APIs (e.g., face recognition, object detection). Our work instead focuses on making sense of raw image datasets by relying on human perception.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Image Browsing</head><p>Zahálka and Worring <ref type="bibr" target="#b69">[69]</ref> presented a comprehensive overview of multimedia visualization methods (primarily of images) in their survey. They categorized existing techniques into five types: basic grid, similarity space, similarity-based, spreadsheet, and thread-based. The three methods commonly used by ML practitioners described in Sect. 1 and Fig. <ref type="figure">2</ref> (i.e., random grid, t-SNE, and a grid version of t-SNE) belong to the "basic grid," "similarity space," and "similarity-based" categories, respectively. Our proposed treemap-based method can also be placed in the "similarity-based" category.</p><p>The idea of using treemaps for image browsing was proposed in PhotoMesa <ref type="bibr" target="#b3">[4]</ref>. It consists of two variations of the treemap algorithms: the ordered treemap algorithm ensures the order of images in each treemap block will match the order in file structures (e.g., by timestamp); and the quantum treemap ensures that the widths and heights of the generated rectangles are integer multiples of a given elemental size. Unlike the data commonly used in treemaps, ML datasets have different properties: each dataset has a set of classes, and the images within each class have no order. Because there is no existing hierarchical structure, we extract one using agglomerative clustering algorithms.</p><p>An important task in analyzing images or multimedia data is categorizing or exploratory searching. The key difference from tabular datasets is that image datasets are not annotated with structured attributes-images are unstructured. Many common data operations like filtering, grouping, and sorting cannot be easily applied. If we consider low-level tasks by Amar et al. <ref type="bibr" target="#b0">[1]</ref>, only a few of the 10 tasks can be applied to images <ref type="bibr" target="#b69">[69]</ref>. Thus, an important challenge in interactive visualization of image data is automatic extraction of semantic information, interactive exploration of categories, or both <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b65">65,</ref><ref type="bibr" target="#b70">70]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Similarity-based Visualization Methods</head><p>As we discussed in the previous subsection, our proposed work can be considered as a similarity-based approach. We briefly describe both the similarity-space and similarity-based approaches in the ML context.</p><p>The t-SNE algorithm is probably the most popular among ML researchers. It is often used to visualize cluster structures learned by deep learning models <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b58">58]</ref>. While t-SNE often plots each data point as a small circle in a 2-D space, the nature of images provides us with the opportunity to directly plot a small thumbnail instead of a dot. This enables users to see the image contents without interacting with each circle mark (e.g., clicking, hovering). For example, Embedding Projector <ref type="bibr" target="#b52">[52]</ref> displays MNIST images in t-SNE plots. However, as the number of images grows, images overlap, making it almost impossible to see them in high-density areas (see Fig. <ref type="figure">2B</ref>).</p><p>Researchers and practitioners have devised methods to address the issue of overlapping images. The images can be rearranged in a grid either by selecting a sample of images among many in each grid or redistributing all images into all the grid spaces in screen using optimization algorithms <ref type="bibr" target="#b27">[28]</ref>. Although we have not found research papers to gridify t-SNE or UMAP, there exist several implementations <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b45">46]</ref>, including one by <ref type="bibr">Karpathy [30]</ref>. This type of gridifying algorithm has been used in several visual analytics tools for ML for image data <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b71">71]</ref>.</p><p>Redistributing data points or images into a rectangular grid has also been studied in non-ML context, such as IsoMatch <ref type="bibr" target="#b16">[17]</ref> and rectangular packing <ref type="bibr" target="#b18">[19]</ref>. Removing overlaps can be more intelligent by balancing the full use of screen space and intentionally leaving some white-space to reveal cluster structures <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Hierarchical Exploration of Data</head><p>To begin our review of hierarchical exploration, we provide a brief background about clustering algorithms <ref type="bibr" target="#b37">[38]</ref>. Unlike the k-means algorithm which partitions data points into a fixed number of groups, the hierarchical clustering algorithms iteratively divide data space into smaller space (i.e., divisive) or merge from smaller groups into larger groups (i.e., agglomerative). We use the latter to form a hierarchy (called a dendrogram), since divisive does not produce high-quality results for high-dimensional data and is computationally expensive for large data. The agglomerative ones align more closely with useful characteristics of t-SNE: focusing on similar pairs to find cluster structures.</p><p>Existing work on visualizing dendrograms include Hierarchical Clustering Explorer (HCE) <ref type="bibr" target="#b48">[49]</ref>, Stacked Trees which interactively merge parts of the dendrogram <ref type="bibr" target="#b5">[6]</ref>, and Yang et al. for steering and revising the dendrograms <ref type="bibr" target="#b66">[66]</ref>. All these used node-link diagrams to display dendrograms; however, they are less desirable for image datasets, because the dendrograms require all instances to be positioned along a single line, which means the size of images would become very small if we want to display images in place of the leaves of the dendrogram tree. A space-filling technique like treemaps can resolve this issue.</p><p>Hierarchical data exploration has been studied extensively in text domains. Text data is unstructured, so automatic extraction of clusters is important too like images. HierarchicalTopics <ref type="bibr" target="#b14">[15]</ref> extracts hierarchical structures of latent topics and enables users to explore and revise them. TopicLens <ref type="bibr" target="#b31">[32]</ref> allows users to zoom into certain areas of projected twodimensional spaces. Marcilio et al. extracts hierarchical structures from high-dimensional representations of deep learning data <ref type="bibr" target="#b34">[35]</ref>. Nmap represents data as treemap-style representations, similar to ours <ref type="bibr" target="#b15">[16]</ref>. It adjusts initial positions of data items obtained from 2-D projection algorithms by iteratively creating treemap nodes using their modified slicing algorithm. We instead create tree structures using well-known clustering algorithms. Another difference is that our work targeting image data displays image thumbnails within treemap nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DESIGN GOALS</head><p>To help ML practitioners explore large-scale image datasets, we adapt treemaps with the following design goals:</p><p>1. Overview of Data Distributions. We aim to assist users in getting an overview of datasets as a beginning step for their analysis of datasets. This includes helping them answer questions like what kinds of images mostly exist in their datasets, whether they are diverse enough <ref type="bibr" target="#b26">[27]</ref> or biased towards any properties <ref type="bibr" target="#b8">[9]</ref>. 2. Exploring at Multiple Levels of Abstraction. We aim to design our visualization to provide users with abilities to interactively adjust the level of abstraction. While treemaps are effective at supporting abstract and elaborate interactions <ref type="bibr" target="#b67">[67]</ref>, we adapt the original treemap techniques by considering unique properties of the dendrogram structure and the domain of ML for images. 3. Instance-level Exploration. As images do not contain attributes, it is important for users to see the individual image contents while exploring datasets. We aim to effectively organize image thumbnails to help users find and inspect individual data points while they navigate over the tree structure. 4. Subgroup-level Analysis for ML. Both the literature in multimedia analytics and visual analytics for ML point out the importance of identifying subgroups from datasets <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b69">69]</ref>. This can be useful for performing a wide range of analytic tasks in ML, such as error analysis and bias discovery <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b63">63]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DendroMap CONSTRUCTION AND INTERACTIONS</head><p>This section describes how a dendrogram can be constructed from an image dataset, how DendroMap visualizes the dendrogram, and how supported interactions help achieve our design goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dendrogram Tree Construction</head><p>To create groups of images for hierarchical exploration, we use the well-known hierarchical agglomerative clustering algorithm <ref type="bibr" target="#b37">[38]</ref>.</p><p>The clustering algorithm takes as input high-dimensional representations of images. There are several ways to obtain such representations, such as by extracting high-dimensional embeddings from pre-trained or fine-tuned models, low-dimensional encodings using Autoencoders, or raw image pixels <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b46">47]</ref>. In our user study in Sect. 6, for the CIFAR-10 dataset, we extracted 1024 dimensional embedding vector representations from the second-to-last hidden (fully-connected) layer in pretrained ResNet50 models that we fine-tuned on CIFAR-10.</p><p>Given this input, each image vector is initialized as its own cluster to start, then the most similar image clusters are merged together using Ward linkage with the Euclidean distance metric to form more balanced trees <ref type="bibr" target="#b37">[38]</ref>. The agglomerative merging process repeats until the final two clusters merge into one cluster containing all the images in the dataset. The output of the algorithm forms a special tree structure, called dendrogram, with leaf nodes corresponding to data instances. Increasing the number of clusters to be shown will result in creating more partitions across the treemap with smooth animations. Fig. <ref type="figure">4</ref>. The slice-dice layout takes the available space given by the parent node v P and partitions the space into for its two children v LC and v RC . To reveal the v P 's hierarchy, padding is added to the children boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">DendroMap Visualization</head><p>DendroMap visualizes dendrogram structures using a modified treemap algorithm. It traverses the dendrogram and renders each cluster node as a grid of images using the available rectangular space.</p><p>Treemap Layout. The dendrogram resembles a binary tree, and all non-leaf nodes have only two child nodes. This allows DendroMap to adapt the traditional slice-dice treemap layout <ref type="bibr" target="#b50">[50]</ref>. Normally, slicedice creates undesirable aspect ratios when laying out many rectangles per level <ref type="bibr" target="#b4">[5]</ref>; however, this issue does not occur in ours because the dendrogram will not have more than two children per node, always resulting in just one partition of space.</p><p>We modify the slice-dice layout to display a grid of fixed sized images on top and to include padding (to highlight hierarchical structures). To demonstrate one iteration of the modified layout, consider a node v P that has two children v LC and v RC with 6 and 4 images, respectively. The goal is to fill a 100 by 90 pixel available space depicted in Fig. <ref type="figure">4</ref>. The algorithm works as follows:</p><p>1. Dice if the available space from the parent v P is a horizontal rectangle and slice if it is vertical. In Fig. <ref type="figure">4</ref>, v P 's width w P is 100 pixels and height h P is 90 pixels, so dicing is chosen. 2. Compute the ratio to partition the space. When dicing, the partition ratio is calculated by ratio := N LC /N P , where N i represents the number of images in v i . The left and right areas of the partition correspond to each child, v LC and v RC . In Fig. <ref type="figure">4</ref>, the dice partition ratio is computed as (6/10) = 0.6. Meaning 60% of the space is for the v LC and 40% is for v RC . 3. Adjust the partition to fit images. Based on the image size, compute the maximum amount of the images that can fit across entire parent's width (or height if slicing) by f it := w P /w image , where w P is the width of the available space for v P and w image is the width of each image. Then the actual partition dimensions can be calculated as f it × ratio pixels, resulting in a partition that fits images without cutting them off. 4. Add padding to show hierarchies. After laying out the v LC and v RC and assigning them their new dimensions, a fixed padding is added to reveal the parent cluster v P behind it (like in Fig. <ref type="figure">4</ref>). We set a fixed padding of 10 pixels in our implementation. Color can encode the remaining height of tree under that node <ref type="bibr" target="#b6">[7]</ref>. Adjusting the number of clusters. Traversing the entire dendrogram quickly fills the available screen space, making it hard to display many images. Thanks to the dendrogram's binary tree structure, each iteration of the DendroMap algorithm only lays out two children (one partition), which allows us to render specific number of clusters (i.e., k set by users). By traversing the tree breadth-first and counting the k clusters created so far, the algorithm can stop and show those k clusters. For example in Fig. <ref type="figure" target="#fig_0">3</ref> the dendrogram traversal stops to only render three clusters showing in the treemap.</p><p>Organizing images within the clusters. A useful property of dendrograms is that the leaf nodes (i.e., images) are positioned along a line based on the structure of the constructed tree in a way that there is no edge crossing. We use this positional information to organize the list of images for each cluster node. As seen in the Fig. <ref type="figure" target="#fig_0">3</ref> dendrogram, the similar images merge together starting from the bottom, and at each successive merge, they still maintain the position of the leaves in the dendrogram from left to right. The end result is the root node cluster's images are in the same position as the leaf nodes in the dendrogram, which lets similar looking images clump up together and nearby images in a cluster be likely more similar than images located far within the cluster. For example, in Fig. <ref type="figure">1</ref> on the right, insect images taken over white background are clustered together with a large node. When there exist a larger number of images to display than the amount of available space, we systematically sample images from the cluster. Specifically, we compute the period by calculating the total number of images in the cluster over the maximum number of images we can possibly show and round down to the nearest integer. We then sample images in the cluster with that period of frequency. For example, if 30 images can be shown in a given space and the cluster has 150 images, we compute the period to sample by 150/30 = 5 and iterate over the cluster {x 1 ,...,x 150 }. The end result is an image every 5 iteration to determine 30 images {x 1 , x 6 , x 11 ,...,x 146 }. This enables us to show representative samples of a cluster and avoid hiding images that occur later on. We display the total number of images at the top of each cluster node, as well as their classification accuracy if available.</p><p>Zooming interactions. For further navigation of the clusters, DendroMap supports zooming. When a cluster node is clicked, DendroMap animates to zoom into the new cluster, which enlarges the selected cluster to fit into the entire space, and creates a set of subclusters within the selected cluster. Our implementation basically follows Bostock's zoomable treemap implementation <ref type="bibr" target="#b7">[8]</ref>. In addition, by taking up the entire space with the zoom-in, more images can be shown with more specific hierarchies, leading to more in-depth exploration. This process corresponds to rendering a downstream portion of the dendrogram. At any point, by clicking back on the parent cluster, the reverse process of zooming-out goes back up the tree to reveal the top-level view again. These zooming interactions allow users to quickly explore large image collections at multiple levels of granularity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Coordinated Views with the Sidebar</head><p>We developed a system for DendroMap by designing coordinated views consisting of the main treemap view and the sidebar. The sidebar contains rendering settings for the treemap display, a class table for class-level error analysis, and a panel for details for a selected image.</p><p>DendroMap Settings. The sidebar contains two sliders to change the overview level: one controls the number of clusters visible and the other controls the image size. By default, DendroMap shows eight clusters of medium-sized images to balance the level of detail and overview such that many images can be shown while still separated into distinguishable groups. These sliders allow users to easily change the overview level based on their exploration needs.</p><p>When a dataset comes with predictions from a trained model, the sidebar provides two options to highlight misclassified images. One toggle highlights these images using a red border and the other toggle puts the images into focus by making the others translucent. Visually emphasizing misclassified images makes it easier for users to find groups of images that the model consistently misclassifies.</p><p>Class Table <ref type="table">.</ref> The class table is visible if model predictions are present. The table contains information for additional error analysis at the class level. The table updates based on the parent cluster's images (i.e., the root or previously selected cluster; by default, all images). Each row of the table corresponds to a specific class in the dataset (e.g., cat). The next two columns of the table displays the counts of images with a true or predicted class label matching the class specified.</p><p>The last three columns of the table provide useful metrics for classlevel error analysis: the prediction accuracy (i.e., how often the true and predicted classes matched that row's class), the false negative rate (i.e, how often the true class matched that row's class but the predicted class was different), and the false positive rate (i.e., how often the predicted class matched that row's class but the true class was different). As shown in Fig. <ref type="figure" target="#fig_1">5</ref>, each rate is encoded with the opacity of a colored dot.</p><p>By hovering over one of these entries in the table, the treemap view highlights the images used to determine that metric by making the other images translucent. This way users can use the class table in tandem with the treemap to isolate and find areas of high error or high accuracy.</p><p>Image Details. A user can click on an image in DendroMap to see detailed information: larger view of the image, true class label, predicted class label if it has one, and similar images. The similar images are determined based on distances in the high-dimensional space, which can be used for counterfactual analysis <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Implementation Details</head><p>The DendroMap system was built using Svelte<ref type="foot" target="#foot_2">3</ref> , a reactive JavaScript framework that has been increasingly used in the visualization community. The main component, the treemap view, is implemented primarily with D3.js <ref type="foot" target="#foot_3">4</ref> to create SVG elements and to transition the elements for natural animation. The complimentary component, the sidebar, is entirely implemented in Svelte, and uses Svelte store functionality to communicate between the treemap. The dendrogram structure is created from the SciPy<ref type="foot" target="#foot_4">5</ref> hierarchical clustering implementation with Ward linkage (recommended as default). The output dendrogram is exported as a nested JSON object to be rendered as a treemap on the client side.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">USE CASES</head><p>In this section, we describe how DendroMap can be used in practice to explore and analyze image datasets through three usage scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Examining Bias in Datasets</head><p>Consider Priya, a data scientist who lives in the Southeast region of Asia and is evaluating whether ImageNet can be used to train an image classification model that she can deploy in her country. After she loads the DendroMap interface, Priya begins to click around to "zoom" into different portions of the dataset. She first clicks on the rectangle containing the approximately half of the dataset and discovers a cluster containing everyday objects. She notices a cluster of taxi cabs and hovers over the class name "taxicab" in the sidebar's class table to put just the taxicab photos in focus while the rest become faded. She notices that most are black or yellow, but she knows from personal experience that many taxis are multicolored in her country, so she makes a note to supplement the "taxicab" class with some of those images. Priya "zooms out" by clicking on the outermost rectangle and decides to visit another cluster, this one featuring many images of people interacting with a variety of everyday objects, such as "violin" and "sunscreen". However, as she clicks on several images to get a better look at each one, she notices that the images tend to include people with lighter skin tones. She makes another note to supplement the dataset with images of people with darker skin tones interacting with the objects corresponding to each of the classes listed in the class table. Given these notes, Priya now would like to train models using this dataset and evaluate them by a set of slices which she made notes (e.g., skin tone), to make sure the models perform consistently over these slices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Identifying Underperforming Subgroups</head><p>Consider Dave, a ML engineer who is using the CIFAR-100 dataset to evaluate a trained image classification model. He opens DendroMap and sees the default view of eight rectangles or clusters. As Dave inspects the interface, he notices that the group of images with the lowest accuracy score (57 percent) consists mostly of human faces. He sees no obvious pattern at this level of overview in the hierarchical structure, so he clicks on another rectangle to get a closer look. From the class table in the sidebar, he observes that a majority of the images in this group were predicted to be "woman" or "girl", but most were incorrect. Dave thinks perhaps his classification model has trouble determining which of those two labels is correct. He navigates back up one level by clicking on the outermost rectangle. He selects a different cluster and this time he observes that a majority of the images are predicted as "man" or "boy", but with similar proportions of incorrect guesses (as shown in Fig. <ref type="figure" target="#fig_2">6</ref>). From these two insights, Dave hypothesizes that his model can distinguish male and female faces, but has difficulty determining whether the person is a child or adult. Then he decides to collect additional training data of human faces for four different categories: adult female, adult male, boys, and girls.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analyzing Classification Errors</head><p>Consider Anna, a ML practitioner who works in a team developing computer vision applications. While she trained a model, she noticed her model consistently had a harder time correctly predicting images from the artifact-related classes so she decided to analyze her model for these classes from the ImageNet dataset, such as "umbrella" and "frying pan". She opens DendroMap and toggles the "outline misclassified" and "focus misclassified" switches to spotlight the misclassified images, outlined in red, while the others fade. She notices that the red outlined images appear to be scattered without much of a pattern, so she gradually increases the number of clusters until DendroMap splits the images into subgroups of higher or lower accuracy. She stops when it reaches 18 clusters because she notices distinct subgroups of images with high accuracy (over 90 percent). Most of these subgroups focus on particular classes, such as "racket" or "potter's wheel". Anna wants to investigate the cause of clusters with much lower prediction accuracy, so she continually clicks on the next visible cluster with the lowest accuracy. She notices a pattern as she keeps drilling down towards the leaf nodes: accuracy rate decreases as the images become more cluttered. She clicks on several misclassified images to inspect their true and predicted class labels, and she discovers that the predicted labels are not necessarily inaccurate-it is that the true label and predicted labels are classifying the entire image based on only a portion of it. For example, she clicks on an image of a couple of people sitting on a bench on a sunny day. The true class label for this image is "sunglasses" because one person is wearing sunglasses, whereas the predicted label for the image is "park bench" because the two people are sitting on a bench. These errors can be critical for her team's applications, so Anna decides to consider object detection models which can locate multiple objects within a single image, instead of image classification models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">USER STUDY</head><p>To evaluate the effectiveness of DendroMap for exploring largescale machine learning datasets, we conducted a user study comparing DendroMap and a baseline visualization technique for images, t-SNE-Grid, a gridified version of t-SNE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Baseline: t-SNE-Grid</head><p>We compare DendroMap with a gridified version of t-SNE, which we call t-SNE-Grid. It re-adjusts the positions obtained from the t-SNE algorithm <ref type="bibr" target="#b57">[57]</ref>, by filling the available rectangular grid space with the images for effectively using screen space <ref type="bibr" target="#b29">[30]</ref>.</p><p>This process works by first taking the image representations from the dataset and reducing them down to their two-dimensional embeddings using t-SNE (like Fig. <ref type="figure" target="#fig_3">7A</ref>). Then, to fill the space, two dimensional grid points are evenly laid out over the space of image embeddings (like Fig. <ref type="figure" target="#fig_3">7B</ref>). Finally, each grid point is assigned the closest image embedding and the corresponding image is displayed on top (like Fig. <ref type="figure" target="#fig_3">7C</ref>). The result is a grid of images with the structure from t-SNE.</p><p>There may be overlap with what is considered the closest image embedding to each grid point, so to achieve a result where the sum of grid assignment distances is minimized, the Jonker-Volgenant algorithm is used to get the optimal assignments <ref type="bibr" target="#b27">[28]</ref>. The optimal grid assignments work by phrasing the problem as a linear assignment problem. For this user study, to enhance the t-SNE-Grid exploration further, we implemented a one-level zoom that recomputes the grid with a smaller number of images based on where the user clicks in t-SNE-Grid. In particular, the top-k closest to the click are recomputed with the Jonker-Volgenant algorithm to display a smaller and more focused grid of images to the user, where k is chosen based on the number of grids to show in the zoomed-in view. For example, to show a 5 × 5 grid, k is set as 25 to take the 25 closest points and gridify them. We open-sourced the grid assignment implementation and published it as a library<ref type="foot" target="#foot_5">6</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Study Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Participants.</head><p>We recruited 20 participants by using the departmental student mailing lists. Their average age was 26. Five were female and 15 were male. Six were undergraduate and 14 were graduate students. Their degree programs included computer science, robotics, and AI. We recruited only those who have taken at least one AI or ML course. Every participant attended the study in-person and we had one participant per session. Each participant was compensated with a $20 gift card.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Protocol.</head><p>We used a within-subject design such that each participant evaluated both DendroMap and t-SNE-Grid. To let participants work with different images for the two visualizations, we created two variations of the CIFAR-100 dataset (Artifact and Organism subsets which we describe in detail in Sect. 6.2.3). From the two visualizations and two datasets, we created four conditions. Each participant was assigned to one of these four conditions to ensure there was no bias in the order in which a participant used (shown in Table <ref type="table" target="#tab_1">1</ref>).</p><p>Every participant completed two sets of tasks, one for each visualization-dataset combination of their respective condition. For each phase, a participant was given a brief tutorial of the visualization, then they were asked to complete seven tasks while thinking aloud. After each phase, the participant filled out a post-questionnaire form. All participants used the same computer setup with a 32-inch monitor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Dataset and Models.</head><p>We used the CIFAR-10 and CIFAR-100 datasets <ref type="bibr" target="#b33">[34]</ref> for the study. The CIFAR-10 dataset has 10 classes, each containing 6,000 images (5,000 from training set and 1,000 from test set), while the CIFAR-100 dataset has 100 classes, each containing 600 images.</p><p>We fine-tuned the ResNet50 <ref type="bibr" target="#b19">[20]</ref> architecture that was pretrained on the ImageNet dataset provided by TensorFlow <ref type="foot" target="#foot_6">7</ref> . The CIFAR-10 and CIFAR-100 images were upsampled to fit the input shape of the ResNet50 model (i.e., 224 × 224 × 3). After extracting the image features from the models, we used Average Pooling, followed by three Dense layers (i.e., their sizes are 1024, 512, and the number of classes, respectively). The model was fine-tuned for 20 epochs, achieving a test set accuracy of 92.8% on CIFAR-10 and 76.3% on CIFAR-100. For use in the DendroMap and t-SNE-Grid algorithms, we represented the images in each dataset as high-dimensional vectors from embeddings of one of the last hidden layers in each respective model (i.e., for CIFAR-10, the second-to-last hidden layer, which is 1024-dimensional; for CIFAR-100, the last hidden layer, which is 512-dimensional <ref type="foot" target="#foot_7">8</ref> ). The DendroMap and t-SNE-Grid use the same representations as input to their respective algorithms.</p><p>We divided the classes of CIFAR-100 into two sets-"Artifacts" and "Organisms"-in order to have two distinct sets of classes for the withinsubject design. This helps ensure that results from the first interface only minimally affect those from the second interface. Each set consists of 40 classes (i.e., 4 superclasses, each consisting of 10 classes) <ref type="bibr" target="#b33">[34]</ref>. The Artifact set contains classes like chair, television, and bottles, while the Organisms set contains classes like tiger, crocodile, and trout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.4">Tasks</head><p>The participants completed seven tasks which can be divided into two broad categories: grouping and searching. The grouping tasks involved identifying groups of images based on semantically similar properties; the searching tasks involved searching for images based on specific properties. Table <ref type="table">2</ref> provides a summarized description of the tasks.</p><p>• In Tasks 1 and 2, participants were asked to categorize images into 3-4 groups based on semantically similar properties. Task 1 was designed to evaluate how users make sense of and categorize images across many (i.e., 40) classes whereas Task 2 focuses on how users make sense of images within a single class. The common objectives of these two tasks include analyzing diversity or any potential bias present in the distribution of the data as well as getting an overview of the data. • In Task 3, we asked participants to find two large groups, using images from a single class, that have very high classification accuracy and have specific properties. This task was designed to evaluate the scope of subgroup-level error analysis. • Task 4 is about examining the distribution of images for a single class.</p><p>This task was designed based on the "characterize distribution" task discussed by Amar et al. <ref type="bibr" target="#b0">[1]</ref>. The participants were asked to estimate the approximate proportions of four groups determined based on an attribute (e.g., of objects). Searching for an image with a given text description 6. Searching for an image with a given visual description 7. Searching for an anomalous image with an incorrect class label Table <ref type="table">2</ref>. Seven tasks designed to evaluate several grouping and searching tasks used in ML analysis</p><p>• The following two tasks are conventional searching tasks. In <ref type="bibr">Task 5,</ref><ref type="bibr"></ref> participants were asked to find an image that matches a provided text description. In Task 6, participants were asked to find the image that matches the one on the task sheet. • Lastly, Task 7 was designed to find probable anomalies. Participants were asked to find potential labeling errors among the misclassified images for a single class <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b64">64]</ref>. Note that every participant worked with the same task list for both DendroMap and t-SNE-Grid, but used a different dataset for each of the visualizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.5">Interface Setup</head><p>For fairer comparison, the sidebar component from DendroMap was added to the t-SNE-Grid visualization. Additionally, to confirm that certain sidebar components are not overused over the main visualization, the class table, class filtering, and similar images components were removed from the sidebar for both DendroMap and t-SNE-Grid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Results</head><p>The setup of our user study gives us the scope to analyze data from a multitude of perspectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Evaluation of task completion time</head><p>Our first set of analyses focused on task completion time. During the study, we recorded the time a participant took to complete each task. We conducted Wilcoxon signed-rank tests, and there is no significant difference between the average time taken by our participants with t-SNE-Grid and that with DendroMap for each of all the tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Evaluation of task responses</head><p>We evaluated the responses to the seven tasks using statistical methods.</p><p>Task 1. We instructed our participants to identify four groups such that an image can be assigned to only one group (mutually exclusive) and most images present in the interface can be assigned to one of the groups (collectively exhaustive). To evaluate the quality of groups made by the participants, we conducted three analyses. First, to measure the collectively exhaustive property of the groups, we counted the number of classes covered by at least one of the four groups and divided that number by the total number of classes present in the dataset (i.e., <ref type="bibr" target="#b39">40)</ref>. The reason why we counted the number of "classes" instead of "images" is the number of classes can approximate the number of images because each class has an equal number of images. In an ideal scenario, the value would be 1.0. If only a portion of images in a class belongs to a group, we count it as half. With DendroMap, the average value over all participants are higher with a value of 0.82, compared to 0.73 with t-SNE-Grid. A one-sided Wilcoxon signed-rank test indicates that its p-value is 0.089. This suggests that on average, participants were able to maintain the "collectively exhaustive" property more with DendroMap than t-SNE-Grid, but we note that the level of significance is not high. Next, to assess the mutual exclusiveness of the groups made by a participant, we counted the number of classes that belong to two or more groups. In an ideal scenario, the value is zero because there is no overlap between the groups. We calculated the average value to be 0.07 for t-SNE-Grid and 0.13 for DendroMap. The results of the same test indicate that on average participants were able to create more "mutually exclusive" groups with t-SNE-Grid than DendroMap (p-value = 0.062). Lastly, we calculated the entropy score of the probability distribution of the four groups to check how much the groups are equally distributed. We found the average entropy score of DendroMap to be similar to that of t-SNE-Grid (i.e., 1.37 vs. 1.34).</p><p>Task 2. Like Task 1, the participants were asked to identify mutually exclusive and collectively exhaustive groups. The main difference for Task 2 is that they worked with images for only one class. To evaluate the quality of groups identified by the participants, we conducted the same three analyses as for Task 1. However, for Task 2, instead of counting the number of classes, we labeled a 10% sample of individual images. In our first analysis of the collectively exhaustive property, the average values for t-SNE-Grid and DendroMap are almost the same with the values of 0.67 and 0.66 respectively. This also happened with the mutual exclusiveness analysis (i.e., 0.10 and 0.13). Our final analysis of the entropy scores is also no exception (i.e., 1.41 and 1.36).</p><p>Task 3. This task is also about grouping, but the participants were asked to find two large groups of images with high classification accuracy. We conducted two analyses. First, we assessed the average accuracy of the two groups. To find the accuracy of each group, we counted the correctly classified images from the total number of images covered by each group. The average accuracy values of the two groups are 92.2% and 93.2% for t-SNE-Grid and DendroMap, respectively. DendroMap is slightly higher, but there is no significant difference. Second, we measured the size of these groups. The average for t-SNE-Grid is 0.38 and for DendroMap is 0.34, with no significant difference.</p><p>Task 4. In this task, the participants estimated the approximate percentage of different cars and birds based on car color (yellow, red, white or silver, or other) or background of birds (e.g., sky), respectively. To evaluate their responses, we counted the number of car and bird images that correspond with the aforementioned criteria and calculated the Kullback-Leibler (KL) divergence score to quantify how much the probability distributions reported by our participants differ from the actual distributions. A score of 0 means the two distributions are the same. Our results show that DendroMap has more counts in between 0.0 and 0.1 than t-SNE-Grid (i.e., 10 vs. 7). This indicates that more participants were closer to the actual distribution when using DendroMap. This is also supported by the medians of the KL divergence scores where the median is 0.10 for DendroMap and 0.17 for t-SNE-Grid.</p><p>Tasks 5 &amp; 6. These tasks were about finding specific images. All the participants of our study were successful in finding the correct images using both the t-SNE-Grid and DendroMap.</p><p>Task 7. The participants were asked to find labeling errors from misclassified images. Unlike Tasks 5 and 6, multiple correct answers exist. We assessed the images selected by our participants and divided them into three categories: reasonable, somewhat reasonable, not reasonable. Based on our assessment of 20 images found by 20 participants, with t-SNE-Grid, 12 are reasonable and 3 are somewhat reasonable; with DendroMap, 15 are reasonable and 3 are somewhat reasonable. This indicates that DendroMap is likely more helpful in finding potential anomalies in image datasets. The images in DendroMap are divided into clusters with distinguishable boundaries, which makes it more convenient to systematically inspect a large number of images than with t-SNE-Grid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.3">Evaluation of post-questionnaires</head><p>Each participant answered 10 questions in two separate postquestionnaire forms: one for DendroMap and one for t-SNE-Grid. They provided ratings on a 7-point Likert scale (7 being strongly agree). The questions and their average ratings are shown in Table <ref type="table" target="#tab_2">3</ref>.</p><p>The results indicate that DendroMap received higher ratings than t-SNE-Grid in 8 out of 10 questions. The t-SNE-Grid received a better rating for only the first question regarding the learnability of the visualization. This is reasonable as t-SNE-Grid supports fewer interactions than DendroMap. From the ratings of several important aspects of image visualizations, DendroMap is found to be statistically significantly more preferable than t-SNE-Grid, such as getting an overview, performing detailed analysis, identifying image categories, and discovering new insights. Moreover, participants on average inclined more towards DendroMap than t-SNE-Grid in mentioning their eagerness to use the tool again.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Discussion</head><p>We observed participants' usage while they performed the tasks. Based on their usage patterns, we have made a few important findings. DendroMap provides a more structured workflow. Compared to t-SNE-Grid, it is easier to assess or follow how a user makes certain decisions with DendroMap. In DendroMap, the presence of clusters and the hierarchical relationships within them provide significant semantic information to the participants when they create groups or search images based on certain properties. One participant said: "The clustering of DendroMap was very intuitive, more so than the grid one where the boundaries between groups were not clearly defined. The ability to click into different levels of clusters was very useful as well."</p><p>DendroMap helps with extracting more specific properties. Using the semantic information provided by DendroMap, the participants could find more detailed information about different image groups. This is more evident with Task 3 where the participants worked with the images of ships and dogs to find two large groups that have high classification accuracy and specific properties. With DendroMap, the participants mentioned more specific properties compared to t-SNE-Grid. For example, regarding dogs, DendroMap users described their eyes, hair length, and facial structure in addition to generic properties such as size, color, and background. With the t-SNE-Grid, participants mostly described groups using only generic properties.</p><p>Image search can be narrowed down more with DendroMap. The hierarchical relationships within the clusters helped the participants narrow their search for a particular image. With DendroMap, they easily found specific clusters with more images similar to the one they were looking for. The sub-clusters present within a cluster then helped them further narrow the search space. On the other hand, with t-SNE-Grid, they had to check a large group of images as there is no structured way of narrowing the search. One participant said: "With the treemap, the ability to narrow down the search without having to recompute the grid size every time, having some predetermined way of organizing the images, and having the images broken up into clusters made it very easy to scan through the images without getting lost. I was able to quickly filter the exact things I was looking for."</p><p>Cluster summary provided with DendroMap is helpful. DendroMap provides information about each cluster and sub-cluster, such as the number of images and classification accuracy. The participants found this information useful, especially for Tasks 3 and 4. One participant expressed their liking by saying: "I like the clusters having details like how many images and the accuracy. Also, the outline of the different clusters having different sizes helped."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Limitations</head><p>No empirical study is perfect. We discuss threats to validity.</p><p>Different mechanisms for exploration. While DendroMap users can navigate tree-structured data at multiple levels, t-SNE-Grid does not create a hierarchy by default. These differences make DendroMap not-surprisingly do better with deeper levels of hierarchical analysis. Our intent was to compare our method against a popular baseline for ML practitioners, our target population. No matter our intent, the threat that any hierarchical method might show similar improvement over the baseline t-SNE-Grid should still be considered.</p><p>Types of images shown. An important potential threat to validity comes down to the image data we used. Depending on the background of the participants, other factors may explain differences in results, such as familiarity of images. In addition, the types of images are potentially an ecological threat to validity. In the real world, datasets may contain more diverse, complicated, and noisier images than what is contained in the CIFAR datasets used in our study. For the purposes of the study, it was necessary to limit the scope for reasonable comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">EXPERIMENTS: DISTANCE PRESERVATION</head><p>Lastly, we evaluate the quality of the cluster structures generated from DendroMap computationally. We quantitatively measure k-nearest neighbor accuracy-how well DendroMap preserves the top-k nearest neighbors in the original high-dimensional space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Setup</head><p>We measure the number of common images in the top-k nearest images between one of the techniques and the original high-dimensional representation of data, while varying k (i.e., the size of nearest neighbor list). This is a common way to evaluate the quality of DR methods <ref type="bibr" target="#b58">[58]</ref>. The techniques we compare are: (1) t-SNE, (2) t-SNE-Grid (described in Sect. 6.1), and (3) DendroMap. We performed this experiment over 12 different datasets: CIFAR-10, CIFAR-100, and 10 subsets of CIFAR-10, each from one of the 10 classes. All are trained with ResNet50 (same setup described in Sect. 6.2.3), but for the first two, the high-dimensional representations were taken from the last hidden layer, while those for the 10 subsets were taken from the second-to-last hidden layer.</p><p>While we compute Euclidean distances between 2-D points for ranking similar images in t-SNE and t-SNE-Grid which assigns a (x, y) value to each data point, DendroMap needed a different methodology because it additionally encodes hierarchical structures using treemaps. We define a distance between two images x i and x j in DendroMap by measuring the distance from the node for x i in the dendrogram tree to the nearest common ancestor node between x i and x j . This can be thought of as how many times a user needs to zoom-out from the leaf node for x i to reach to the cluster where both x i and x j belong to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Results</head><p>Figure <ref type="figure" target="#fig_4">8</ref> shows the results. For each of the 12 plots, the x-axis represents k (in k-nearest neighbor) and the y-axis represents the average number of common images in two top-k image lists. We display up to 300 for 10,000 image datasets and 50 for the class-level CIFAR-10 datasets As shown in the figure, in all cases, t-SNE outperforms the other two, as we can expect, because t-SNE is designed to optimize this metric. When comparing DendroMap and t-SNE-Grid, DendroMap shares more top-k nearest neighbors with the high-dimensional representations than t-SNE-Grid for all 12 datasets. This indicates that DendroMap preserves the local similarity structures better than t-SNE-Grid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">LIMITATIONS AND FUTURE WORK</head><p>Computational scalability of clustering. The agglomerative clustering algorithms can be a bottleneck when scaling DendroMap to larger datasets. The naïve algorithms grow by O(n 3 ) in time but can be brought down to O(n 2 ) with optimizations <ref type="bibr" target="#b36">[37]</ref>. The t-SNE method runs with a time complexity O(n 2 ) and can use approximation to get to O(n log n) <ref type="bibr" target="#b56">[56]</ref>. Although clustering is less efficient, it only needs to be computed once for interactive use in DendroMap. For the CIFAR-10 test set with 10, 000 images, the clustering algorithm took 36.8 seconds compared to 32.0 seconds for t-SNE<ref type="foot" target="#foot_8">9</ref> (ran on macOS 12.4, 2.6 GHz 6-Core Intel Core i7 cpu). Future work can investigate more efficient strategies to create hierarchical structures of data.</p><p>Comparison with other tree construction methods. In the user study, we compared DendroMap with t-SNE, the most well-known technique (specifically t-SNE-Grid); however, as noted in the limitations in Sect. 6.5, t-SNE does not create an explicitly hierarchical structure. In the future, DendroMap can be compared against a variety of other techniques (e.g., H-SNE <ref type="bibr" target="#b43">[44]</ref>) to evaluate the effectiveness of algorithms that produce hierarchical structures.</p><p>Interactive refinement of tree structures. While the agglomerative clustering algorithms generate hierarchical structures that allow users to flexibly specify the number of clusters to be displayed, the formed structures may not be ideal for some cases. Visualization researchers have extensively studied interaction methods for steering and refining clustering results <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b66">66]</ref>. Future research challenges include designing user interactions for refining clustering results in DendroMap.</p><p>Using interpretable attributes for tree construction. We used embedding vectors extracted from deep learning models as input to clustering algorithms, but alternative methods may help people better interpret substructures of each cluster in DendroMap. For example, representing each image with human-understandable concepts <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b71">71]</ref> or additional resources <ref type="bibr" target="#b65">[65]</ref> may make each dimension more interpretable. Alternatively, integrating information about each dimension of the embedding vectors into the interface using explainable AI methods can also be helpful <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>Formalizing interaction operations. Several data manipulation operations can also be provided in DendroMap. For example, sorting images within each node by user-specified criteria (e.g., prediction scores) or splitting and zooming into only a subset of nodes <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b66">66]</ref>. Formalizing these types of operations would allow for more flexible user exploration. Integrating some ideas presented in the unit visualization literature <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b61">61]</ref>, such as horizontally or vertically separating space based on categorical attributes in Facets-Dive <ref type="bibr" target="#b60">[60,</ref><ref type="bibr" target="#b61">61]</ref>, into the treemap context would also be an interesting future direction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. To scalably visualize the dendrogram tree structure created from agglomerative clustering methods, users can dynamically specify the number of clusters to be rendered in DendroMap. In this example, a portion of the dendrogram is rendered in the treemap view to show three image clusters. Increasing the number of clusters to be shown will result in creating more partitions across the treemap with smooth animations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The class table summarizes class-level statistics of images present in the selected cluster in the treemap view. The user can sort and search for classes, and hover over each entry to quickly locate accurate or error filled clusters highlighted directly on the DendroMap.</figDesc><graphic url="image-116.png" coords="5,64.04,73.00,231.27,93.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. In our case study, ML practitioner, Dave, investigates the specific classes that his model struggles with DendroMap.</figDesc><graphic url="image-118.png" coords="6,243.29,88.03,193.34,189.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Steps to generate t-SNE-Grid: From t-SNE embeddings (in A), we first overlay grid points on top of the embeddings (in B; 10 × 10 in this case). Then in C, we assign each grid with an image that has the smallest distance.</figDesc><graphic url="image-144.png" coords="6,223.38,597.97,59.08,59.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Average for the number of common k-nearest neighbours between t-SNE, t-SNE-Grid, or DendroMap high-dimensional representations of images. For all 12 datasets we tested, DendroMap preserves the top-k images better than t-SNE-Grid.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Four conditions for counterbalancing the orders of two interfaces in our within-subject design # Task Description 1. Categorizing images into groups across 40 classes 2. Categorizing images into groups for a single class 3. Identifying groups of images with high classification accuracy within a single class 4. Estimating the image count distribution over multiple groups within a single class 5.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Participants' average ratings for the two visualizations. DendroMap outscored t-SNE-Grid in 8 out of 10 questions. Bold indicates higher average ratings. * and • indicate 95% and 90% statistical significance in one-sided Wilcoxon signed-rank tests, respectively.</figDesc><table><row><cell>Question</cell><cell cols="2">t-SNE-Grid DendroMap</cell></row><row><cell>Easy to learn how to use</cell><cell>6.45</cell><cell>6.30</cell></row><row><cell>Easy to use Helpful for overview Helpful for detailed analysis Helpful for finding specific images Helpful to identify image categories Helpful to discover new insights</cell><cell>6.00 5.95 5.15 5.10 5.70 5.25</cell><cell>6.00 6.45 • 6.05  *  5.75 • 6.20 • 6.00 •</cell></row><row><cell>Confident when using the tool</cell><cell>5.85</cell><cell>6.05</cell></row><row><cell>Enjoyed using the tool Would like to use again</cell><cell>6.10 5.80</cell><cell>6.40 6.65  *</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">The live demo of DendroMap is available at https://div-lab.github. io/dendromap/.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">The code is available at https://github.com/div-lab/dendromap.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">Svelte JavaScript Framework: https://svelte.dev/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">D3 JavaScript Library: https://d3js.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">SciPy Python Library: https://scipy.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">https://www.npmjs.com/package/grid-assign-js</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">https://www.tensorflow.org/api_docs/python/tf/keras/ applications/resnet50/ResNet50</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7">For CIFAR-10, we chose the layer farther from the output layer, because we wanted to extract lower-level concepts that are less specific to classes for people to explore different types of images within each class<ref type="bibr" target="#b2">[3]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8">Agglomerative clustering was computed with Ward linkage using SciPy, and t-SNE was computed with the default parameters using scikit-learn.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Eric Slyman for their feedback. This work was supported in part by Google Cloud (GCP19980904), NAVER AI Lab, NSF and USDA NIFA (2021-67021-35344), and DARPA (N66001-17-2-4030).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Low-level components of analytic activity in information visualization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Amar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
		<idno type="DOI">10.1109/INFVIS.2005.1532136</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Information Visualization (InfoVis)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="111" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ModelTracker: Redesigning performance analysis tools for machine learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chickering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Suh</surname></persName>
		</author>
		<idno type="DOI">10.1145/2702123.2702509</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems</title>
				<meeting>the 33rd Annual ACM Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="337" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Network dissection: Quantifying interpretability of deep visual representations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6541" to="6549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">PhotoMesa: a zoomable image browser using quantum treemaps and bubblemaps</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Bederson</surname></persName>
		</author>
		<idno type="DOI">10.1145/502348.502359</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Annual ACM Symposium on User Interface Software and Technology</title>
				<meeting>the 14th Annual ACM Symposium on User Interface Software and Technology</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="71" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ordered and quantum treemaps: Making effective use of 2D space to display hierarchies</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Bederson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<idno type="DOI">10.1145/571647.571649</idno>
	</analytic>
	<monogr>
		<title level="j">AcM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="833" to="854" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving visualization of large hierarchical clustering</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Blanch</surname></persName>
		</author>
		<idno type="DOI">10.1109/IV.2012.45</idno>
	</analytic>
	<monogr>
		<title level="m">2012 16th International Conference on Information Visualisation</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="220" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Bostock</surname></persName>
		</author>
		<ptr target="https://observablehq.com/@d3/nested-treemap" />
		<imprint>
			<date type="published" when="2019">March 31, 2022., 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Bostock</surname></persName>
		</author>
		<ptr target="https://observablehq.com/@d3/zoomable-treemap" />
		<title level="m">Zoomable treemap</title>
				<imprint>
			<date type="published" when="2019">March 31, 2022., 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Gender Shades: Intersectional accuracy disparities in commercial gender classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Buolamwini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gebru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Conference on Fairness, Accountability and Transparency</title>
				<meeting>the 1st Conference on Fairness, Accountability and Transparency</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="77" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">FairVis: Visual analytics for discovering intersectional bias in machine learning</title>
		<author>
			<persName><forename type="first">Á</forename><forename type="middle">A</forename><surname>Cabrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Epperson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Morgenstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
		<idno type="DOI">10.1109/VAST47406.2019.8986948</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Conference on Visual Analytics Science and Technology (VAST)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="46" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">t-viSNE: Interactive assessment and interpretation of t-SNE projections</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chatzimparmpas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kerren</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2020.2986996</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2696" to="2714" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">OoDAnalyzer: Interactive analysis of out-of-distribution samples</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2020.2973258</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3335" to="3349" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">DECE: Decision explorer with counterfactual explanations for machine learning models</title>
		<author>
			<persName><forename type="first">F</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2020.3030342</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1438" to="1447" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Utopian: User-driven topic modeling based on interactive nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2013.212</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="1992">1992-2001, 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">HierarchicalTopics: Visually exploring large text collections using topic hierarchies</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ribarsky</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2013.162</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2002" to="2011" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nmap: A novel neighborhood preservation space-filling algorithm</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sikansi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Fatore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Fadel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">V</forename><surname>Paulovich</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2014.2346276</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2063" to="2071" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Isomatch: Creating informative grid layouts</title>
		<author>
			<persName><forename type="first">O</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Diverdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sizikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.12549</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
				<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="155" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ViCE: Visual counterfactual explanations for machine learning models</title>
		<author>
			<persName><forename type="first">O</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Holter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bertini</surname></persName>
		</author>
		<idno type="DOI">10.1145/3377325.3377536</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Intelligent User Interfaces</title>
				<meeting>the 25th International Conference on Intelligent User Interfaces</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="531" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">CAT: A hierarchical image browser using a rectangle packing technique</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miyazaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Itoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/IV.2008.8</idno>
	</analytic>
	<monogr>
		<title level="m">2008 12th International Conference Information Visualisation</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="82" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Where can we help? a visual analytics approach to diagnosing and improving semantic segmentation of movable objects</title>
		<author>
			<persName><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Shekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2021.3114855</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1040" to="1050" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Hilasaca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Marcílio-Jr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Eler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">V</forename><surname>Paulovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.06262</idno>
		<title level="m">Overlap removal of dimensionality reduction scatterplot layouts</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Hilasaca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">V</forename><surname>Paulovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.06262v1</idno>
		<title level="m">Distance preserving grid layouts</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Visual analytics in deep learning: An interrogative survey for the next frontiers</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pienta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2018.2843369</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2674" to="2693" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Summit: Scaling deep learning interpretability by visualizing activation and attribution summarizations</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2019.2934659</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1096" to="1106" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Crowdsourcing the perception of machine teaching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kacorri</surname></persName>
		</author>
		<idno type="DOI">10.1145/3313831.3376428</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2020 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A shortest augmenting path algorithm for dense and sparse linear assignment problems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jonker</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF02278710</idno>
	</analytic>
	<monogr>
		<title level="j">Computing</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="325" to="340" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">ActiVis: Visual exploration of industry-scale deep neural network models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kalro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2017.2744718</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">t-sne visualization of cnn codes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<ptr target="https://cs.stanford.edu/people/karpathy/cnnembed/" />
		<imprint>
			<date type="published" when="2014">March 31. 2022. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (TCAV)</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viegas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning (ICML)</title>
				<meeting>the 35th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="2668" to="2677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Topiclens: Efficient multi-level visual topic exploration of large-scale document collections</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2016.2598445</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="151" to="160" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Kogan</surname></persName>
		</author>
		<ptr target="https://ml4a.github.io/guides/ImageTSNELive/" />
		<title level="m">Image t-SNE live</title>
				<imprint>
			<date type="published" when="2022-03-31">March 31, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">ExplorerTree: a focus+context exploration approach for 2D embeddings</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Marcílio-Jr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Eler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">V</forename><surname>Paulovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Rodrigues-Jr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O</forename><surname>Artero</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.bdr.2021.100239</idno>
	</analytic>
	<monogr>
		<title level="j">Big Data Research</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Umap: Uniform manifold approximation and projection for dimension reduction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Melville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03426</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Müllner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1109.2378</idno>
		<title level="m">Modern hierarchical, agglomerative clustering algorithms</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A survey of recent advances in hierarchical clustering algorithms</title>
		<author>
			<persName><forename type="first">F</forename><surname>Murtagh</surname></persName>
		</author>
		<idno type="DOI">10.1093/comjnl/26.4.354</idno>
	</analytic>
	<monogr>
		<title level="j">The Computer Journal</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="354" to="359" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">A chat with andrew on mlops: From model-centric to data-centric ai</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<ptr target="https://www.youtube.com/watch?v=06-AZXmwHjo" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pervasive label errors in test sets destabilize machine learning benchmarks</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Northcutt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks (NeurIPS Datasets and Benchmarks)</title>
				<meeting>the Neural Information Processing Systems Track on Datasets and Benchmarks (NeurIPS Datasets and Benchmarks)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The building blocks of interpretability</title>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Satyanarayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mordvintsev</surname></persName>
		</author>
		<idno type="DOI">10.23915/distill.00010</idno>
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Contrastive identification of covariate shift in image data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dixit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ratzlaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kahng</surname></persName>
		</author>
		<idno type="DOI">10.1109/VIS49827.2021.9623289</idno>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE Visualization Conference (VIS)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Atom: A grammar for unit visualizations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2017.2785807</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3032" to="3043" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Hierarchical stochastic neighbor embedding</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pezzotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Höllt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lelieveldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Eisemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vilanova</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.12878</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
				<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="21" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">What can data-centric ai learn from data and ml engineering?</title>
		<author>
			<persName><forename type="first">N</forename><surname>Polyzotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.06439</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Ratajczak</surname></persName>
		</author>
		<ptr target="https://github.com/Pandinosaurus/tsne-grid" />
		<title level="m">tsne-grid</title>
				<imprint>
			<date type="published" when="2022-03-31">March 31, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Visualizing the hidden activity of artificial neural networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Fadel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Falcao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Telea</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2016.2598838</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="101" to="110" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Squares: Supporting interactive performance analysis for multiclass classifiers</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2016.2598828</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="70" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Interactively exploring hierarchical clustering results</title>
		<author>
			<persName><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>gene identification</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title/>
		<idno type="DOI">10.1109/MC.2002.1016905</idno>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="80" to="86" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Tree visualization with tree-maps: 2-d space-filling approach</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
		<idno type="DOI">10.1145/102377.115768</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="92" to="99" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The eyes have it: A task by data type taxonomy for information visualizations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
		<idno type="DOI">10.1109/VL.1996.545307</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings 1996 IEEE Symposium on Visual Languages. IEEE, 1996</title>
				<meeting>1996 IEEE Symposium on Visual Languages. IEEE, 1996</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Smilkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nicholson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">B</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05469</idno>
		<title level="m">Embedding projector: Interactive visualization and interpretation of embeddings</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Know Your Data: a new tool to explore datasets</title>
		<author>
			<persName><forename type="first">D</forename><surname>Smilkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pellat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Peran</surname></persName>
		</author>
		<ptr target="https://medium.com/people-ai-research/know-your-data-a-new-tool-to-explore-datasets-ba45b7665695" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Basic classification: Classify images of clothing</title>
		<author>
			<persName><surname>Tensorflow</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/tutorials/keras/classification" />
		<imprint>
			<date type="published" when="2022-03-31">March 31, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">ICLIC: Interactive categorization of large image collections</title>
		<author>
			<persName><forename type="first">P</forename><surname>Van Der Corput</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Van Wijk</surname></persName>
		</author>
		<idno type="DOI">10.1109/PACIFICVIS.2016.7465263</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Pacific Visualization Symposium (PacificVis)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Accelerating t-sne using tree-based algorithms</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">93</biblScope>
			<biblScope unit="page" from="3221" to="3245" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">86</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Understanding how dimension reduction tools work: an empirical approach to deciphering tsne, umap, trimap, and pacmap for data visualization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shaposhnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">201</biblScope>
			<biblScope unit="page" from="1" to="73" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">How to use t-SNE effectively</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="DOI">10.23915/distill.00002</idno>
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Facets: An open source visualization tool for machine learning training data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wexler</surname></persName>
		</author>
		<ptr target="https://ai.googleblog.com/2017/07/facets-open-source-visualization-tool.html" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">The what-if tool: Interactive probing of machine learning models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pushkarna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bolukbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2019.2934619</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="56" to="65" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Data collection and quality challenges in deep learning: A data-centric ai perspective</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Roh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-G</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.06409</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Errudite: Scalable, reproducible, and testable error analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weld</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1073</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Interactive correction of mislabeled training data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/VAST47406.2019.8986943</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Conference on Visual Analytics Science and Technology (VAST)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="57" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A semantic-based method for visualizing large image collections</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2018.2835485</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2362" to="2377" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Interactive steering of hierarchical clustering</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2020.2995100</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3953" to="3967" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Toward a deeper understanding of the role of interaction in information visualization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Jacko</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2007.70515</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1224" to="1231" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A survey of visual analytics techniques for machine learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/s41095-020-0191-7</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="36" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Towards interactive, intelligent, and integrated multimedia analytics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zahálka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<idno type="DOI">10.1109/VAST.2014.7042476</idno>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Visual Analytics Science and Technology (VAST)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">II-20: Intelligent and pragmatic analytic categorization of image collections</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zahálka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Van Wijk</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2020.3030383</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="422" to="431" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Human-in-the-loop extraction of interpretable concepts in deep learning models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2021.3114837</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="780" to="790" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
