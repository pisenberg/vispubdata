<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xumeng</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jiazhi</forename><surname>Xia</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhen</forename><surname>Wen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rongchen</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tobias</forename><surname>Schreck</surname></persName>
						</author>
						<title level="a" type="main">HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-03-27T09:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Federated learning</term>
					<term>data heterogeneity</term>
					<term>cluster analysis, visual analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. 1. The interface of HetVis for analyzing the heterogeneity issues among Federated Learning cooperation from the perspective of a participating client. (a) The module of federated learning process observation consists of an information panel introducing the federated model and the local client, the parameter projection view depicting the evolution of the disagreements between the local client and the integrated model, and a performance monitor view recording model performance and users' annotations. (b) The module of model output comparison identifies and clusters inconsistent records between the output of the federated learning model and the stand-alone training model. (c) The module of heterogeneity examination allows users to analyze an inconsistent cluster by observing the distribution and inspecting instances. Findings can be annotated to the cluster in the control panel.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>• Xumeng Wang is with TMCC, CS, Nankai University. E-mail: wangxumeng@nankai.edu.cn. In the big data era, data is often distributed among many isolated data owners. It is an urgent and challenging need for the data owners to benefit from data integration, e.g., to train high-quality models, while also respecting privacy concerns. To satisfy this requirement,</p><p>• Wei Chen and Jiazhi Xia are the corresponding authors. federated learning <ref type="bibr" target="#b39">[40]</ref> (FL) keeps data locally in clients when training a shared model. Only encrypted parameters are shared with other clients. Especially, horizontal federated learning (HFL), which integrates data from the same feature space but distributed at different clients, has been widely used in privacy-aware applications, like healthcare and mobile service <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>A key and common challenge of HFL is the heterogeneity of data distribution. The efficiency of current HFL techniques depends on the assumption that data distribution in different clients is independent and identically distributed (IID). However, the IID assumption usually does not hold. Non-IID data is widely used in model training to increase sample size and leads to difficulties in convergence <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b38">39]</ref>. Figuring out their existence is necessary for applications of HFL. Specifically, data heterogeneity can exert both negative impacts and positive impacts on the accuracy of trained models. Statistical heterogeneity, meaning that distributed data cover the same classes but different proportions, can be positive. For example, one client has insufficient samples in one class while the other clients have adequate samples in the same class. In this case, multiple clients are facilitated in the collaboration. Label heterogeneity means that similar instances are labeled differently in different clients. The divergence in label settings usually affects model training negatively. Therefore, it is critical for participating clients to diagnose the data heterogeneity in the training process and fine-tune the model and data <ref type="bibr" target="#b40">[41]</ref>. We identify two analysis goals in heterogeneity: <ref type="bibr" target="#b0">(1)</ref> detecting the existence of data heterogeneity and (2) knowing the impacts of data heterogeneity. For example, hospitals collect patient data for disease analysis, and search engines record user logs for advertising recommendations.</p><p>However, analyzing the data heterogeneity in HFL is highly challenging. Traditional approaches identify heterogeneity through comparing data <ref type="bibr" target="#b27">[28]</ref>. Due to the privacy issue, a client is prevented from accessing data owned by other clients directly. Direct comparison between local data and global data is infeasible. The HFL model is the only intelligence fed back to the client by the cooperation. As it is trained with the global model with a HFL framework, the global data is learned by the HFL model. There are two challenges in analyzing data heterogeneity based on the HFL model. First, although a lot of works have been dedicated to understanding deep learning models by input and output data, the inverse workflow of understanding the unavailable training data by the model is still an open problem. Second, the behavior of the HFL model depends on multiple factors, including the data and model architectures. How to distinguish the effect of training data from other factors is also challenging. An encompassing analysis workflow is still missing for HFL analysis.</p><p>To fill the above research gap, we present a visual analytics tool for client analysts to detect and understand data heterogeneity in HFL under the privacy limitation. We leverage a contrastive analysis approach to locate heterogeneous records and examine them. To overcome the privacy limitation, we propose to train a stand-alone model based on local data and compare it with the HFL model. Because data is learned by a model in the training process, the comparison between the two models discloses the heterogeneity between the local data and global data. We further propose a cluster analysis method based on rank-based distance measurement to distinguish the impacts from different group of heterogeneous records. We developed an interactive interface, called HetVis, in conjunction with our analysis approach to monitor, explore, and understand the models and instances. A comparative study is implemented to show that the proposed cluster analysis method is more helpful for analyzing heterogeneity than hierarchical clustering. Three case studies were designed to demonstrate the effectiveness of our interface when analyzing different kinds of data heterogeneity. We also collected informative reviews from three experts in FL, confirming the effectiveness of HetVis.</p><p>In summary, the contributions of this work are:</p><p>• A novel visual analysis workflow that assists to analyze the heterogeneity in HFL models.</p><p>• A context-aware clustering approach that hierarchically generates clusters of data which are inconsistent with others.</p><p>• A visual system that integrates a suite of novel designs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, related works are summarized from three aspects: heterogeneity in distributed learning, visualization for model diagnosis, and visualization for model comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Heterogeneity in Distributed Learning</head><p>The problem of data heterogeneity has put forward challenges for integrating distributed data into joint or public knowledge. Related scenarios mainly include distributed learning and statistical analysis.</p><p>In the first scenario, heterogeneity issues, including state heterogeneity <ref type="bibr" target="#b37">[38]</ref>, hardware heterogeneity <ref type="bibr" target="#b36">[37]</ref>, statistical heterogeneity, and label heterogeneity, lead to convergence problems of the global model <ref type="bibr" target="#b19">[20]</ref>. In this paper, we focus on identifying data heterogeneity issues (i.e., statistical heterogeneity and label heterogeneity). Related studies leverage automatic solutions to improve model robustness. FedProx <ref type="bibr" target="#b20">[21]</ref> modifies the weight updating algorithm to limit the impact of local updates on the global model. This approach, however, can hardly address the label heterogeneity problem, because the trained model can be meaningless if the definition of labels varies for different clients. Another solution is to train separated models instead of a global one. Separated models can learn from each other based on state-of-the-art frameworks, such as meta-learning <ref type="bibr" target="#b10">[11]</ref> and multi-task learning <ref type="bibr" target="#b29">[30]</ref>. However, these automatic solutions lack the ability to analyze heterogeneity and therefore miss the chance to optimize models by means, like managing distributed data.</p><p>In the second scenario, meta-analysis <ref type="bibr" target="#b27">[28]</ref> is proposed as a methodology to discriminate, combine and summarize multiple statistical analysis results. In the discrimination stage, statistical tests are leveraged to assess the significance of heterogeneity <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16]</ref>. If the heterogeneity is significant, the statistical analysis results cannot be integrated directly <ref type="bibr" target="#b4">[5]</ref>. A feasible solution is to locate and exclude the variables or moderators that result in the heterogeneity <ref type="bibr" target="#b27">[28]</ref>. However, these approaches are not practical for FL due to the privacy limitation.</p><p>In this paper, we leverage limited available information (the parameters and the output of HFL models) to facilitate heterogeneity detection and examination, so that users can better understand the significance of heterogeneity issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Visualization for Model Diagnosis</head><p>Approaches for model diagnosis mainly fall into three categories: monitoring the model performance fluctuation, inspecting model configuration, and leveraging instance-level analysis.</p><p>Visualization techniques aim to provide an overview of the model evolution. In existing applications, the performance of the model (e.g., loss) is frequently recorded and represented as time series <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10]</ref>. We also show the performance dynamics in our system to assist users in judging the convergence and performance of the federated model.</p><p>To reason why the performance fluctuates, users need to further inspect the model configuration. DeepEyes <ref type="bibr" target="#b24">[25]</ref> allows users to check details of detected stable layers in deep neural networks through three linked views, which depict activations, instances, and filters to judge if a layer is oversized or unnecessary. GANViz <ref type="bibr" target="#b31">[32]</ref> compares image features from the dimension of time. The comparison results reflect the impact of a feature during the model training process. DGMTracker <ref type="bibr" target="#b21">[22]</ref> applies a credit assignment algorithm to locate the neurons that contribute to the failure <ref type="bibr" target="#b21">[22]</ref>. However, various models with distinctive structures can be trained based on the federated architecture. We focus on the HFL process, namely the exchange of parameters between the local client and the server, from which the disagreements between the local client and the others can be studied.</p><p>Testing output requires a lower learning cost than model interpretation, which is more acceptable for domain experts. To diagnose and improve the model, failed cases should be emphasized. The What-If-Tool <ref type="bibr" target="#b34">[35]</ref> allows users to customize the input of models and learn the mechanics of models by comparing related outputs. RetainVis <ref type="bibr" target="#b14">[15]</ref> allows users to modify the input of an RNN model and figure out why a record is classified incorrectly. Krause et al. <ref type="bibr" target="#b12">[13]</ref> provide instance-level explanations to verify the effectiveness of features based on a single instance. However, none of the above studies consider data heterogeneity due to the difference between application scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Visualization for Model Comparison</head><p>Existing studies on model comparison mostly aim at selecting the best model, which requires to compare model performance. Instance-level analysis can be leveraged to facilitate the understanding of model behaviors. DeepCompare <ref type="bibr" target="#b23">[24]</ref> groups instances by the combination of classification results of a Convolutional Neural Network (CNN) and a Long Short-Term Memory (LSTM) model. The group information indicates where the two models disagree with each other. Users can start exploration from the instances that are misclassified by one or two models. The neuron activation pattern of the user-selected instance can be compared through a heatmap. However, models can achieve the same label with a different confidence <ref type="bibr" target="#b26">[27]</ref>. Manifold <ref type="bibr" target="#b42">[43]</ref> employs scatterplots with color encodings to show the overview of instance-level details, which are the confidence of the model pair and the ground-truth label.</p><p>Model comparison can also offer an in-depth understanding of the data. On the one hand, the output of the model reflects the underlying characteristics of the data. Employing multiple models allows users to learn data from multiple perspectives. Alexander and Gleicher <ref type="bibr" target="#b1">[2]</ref> compare the results of two topic models to illustrate the consistency of documents. They introduce two visual encodings to represent the outputs of the two models, respectively. PK-clustering <ref type="bibr" target="#b25">[26]</ref> compares the results of multiple clustering algorithms to reduce uncertainty in prior knowledge. The trend patterns can highlight inconsistencies. ConceptExplorer <ref type="bibr" target="#b32">[33]</ref> compares the performance fluctuation of the online learning models trained with different time-series datasets. Users can judge if those data have experienced consistent evolution based on the proposed drift-level index. Similarly, we attempt to comprehend the heterogeneity between the local data and the data distributed on the other clients by comparing the stand-alone training model with the HFL model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">APPROACH DESIGNS</head><p>In this section, we first introduce the background of horizontal federated learning and describe the design requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Horizontal Federated Learning Architecture</head><p>The typical architecture of horizontal federated learning (HFL) consists of a set of clients who own local data and a server which hosts the exchange of parameters among clients. At the beginning of the training, the model settings are unified for initialization. After that, cooperated training performs iteratively. Each communication round includes four steps. First, each client receives centralized parameters from the server. Second, each client updates the parameter by training and testing with local data. Then, each local update is submitted to the server. For privacy concerns, parameters can be encrypted <ref type="bibr" target="#b41">[42]</ref> or processed by differential privacy <ref type="bibr" target="#b33">[34]</ref>. Third, the server integrates the parameters from the clients by aggregation algorithms. (We employ FedAvg <ref type="bibr" target="#b22">[23]</ref> as default aggregation algorithms in this work.) Finally, the server sends the integrated results back, based on which each client updates the parameter of their own model and gets ready for the next round. In conclusion, each client can only access the parameters of the HFL model in the entire learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Requirements Analysis</head><p>Because clients cannot exchange the raw data, the primary issue of designing a visual diagnosis tool for data heterogeneity is that a direct comparison among data from different clients is infeasible. We interviewed two FL experts (E1 and E2) to define requirements. Both of them are senior machine learning researchers who have studied FL for more than 3 years and published related papers. They said that they have no effective solutions to deal with heterogeneity issues caused by heterogeneous data. Although a visualization system <ref type="bibr" target="#b18">[19]</ref> is proposed to inspect the training process of the HFL, a tool for heterogeneity exploration is still missing. Without the understanding of data heterogeneity, it is challenging to remove heterogeneity issues thoroughly. Therefore, the experts agree that an interactive visualization tool to examine the data heterogeneity is critical for real applications.</p><p>To identify the major requirements, questions are asked to two FL experts: Without the prior knowledge of data from other clients, how can you be aware of the existence of data heterogeneity? What kind of information is helpful in identifying and understanding the data heterogeneity?</p><p>Based on the interviews and literature review, we identify three requirements.</p><p>R1. Knowing the existence of data heterogeneity. Both experts pointed out that it is critical to know the occurrence of data heterogeneity during the model building, so that the investigation of data heterogeneity can be performed at appropriate rounds.</p><p>• R1.1 Tracking the dynamic performance of the federated model.</p><p>Data heterogeneity will hinder the convergence of the federated model and harm its performance. The fluctuation of performance metrics, such as loss, is a good indicator of the training process <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10]</ref>. Tracking them helps to know the dynamics of the training process and further locate the rounds when heterogeneity issues occur and affect the federated model. Comparing model output is a common practice to learn the differences in prediction behaviors of two models <ref type="bibr" target="#b42">[43]</ref>. For a comprehensive comparison, we should provide inputs that can cover the corresponding data space.</p><p>R3. Supporting visual examination of heterogeneity issues. Visual analysis approaches can facilitate users in understanding heterogeneity issues and reasoning their impacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• R3.1 Identify heterogeneity issues based on inconsistent outputs.</head><p>E2 said that clustering records with inconsistent model outputs can facilitate in identification of heterogeneity issues. According to the theories in meta-analysis <ref type="bibr" target="#b27">[28]</ref>, researchers need to deal with heterogeneity issues based on their significance. After clustering records, the significance of heterogeneity can be observed from the size of each cluster. If the ground-truth label is available, we can further calculate the prediction accuracy of the HFL model for the records in each cluster and assist client analysts to judge whether the impact of the corresponding heterogeneity issue is positive or negative.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monitoring the Learning Process</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">WORKFLOW</head><p>To support users, i.e., client analysts, analyzing heterogeneity issues, we propose a three-stage workflow (see Figure <ref type="figure" target="#fig_0">2</ref>): 1) monitoring the HFL process, 2) comparing model output, and 3) examining heterogeneity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Monitoring the Learning Process</head><p>In </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparing Model Output</head><p>In </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Examining Heterogeneity</head><p>In the third stage, users examine an inconsistency cluster to study the corresponding heterogeneity issue and understand the impacts of the heterogeneity issue. As shown in Figure <ref type="figure" target="#fig_0">2</ref>(g), users can check data space characteristics of the records in the selected inconsistency cluster. Related findings inspire users to come up with hypotheses on suspicious heterogeneity. To make tentative verification, users can select certain records in the inconsistency cluster and browse record details (Figure <ref type="figure" target="#fig_0">2</ref>(h)). Records with ground-truth labels can facilitate users to judge whether the heterogeneity issue leads to a higher accuracy than the stand-alone training model (R3.2). As shown in Figure <ref type="figure" target="#fig_0">2</ref>(i), users can annotate the records with suspicious heterogeneity issues and track these records in the following training process (R3.3). If the negative impacts of these records is weakened or even disappears, users can consider that the HFL models can overcome such heterogeneity. If not, this indicates users should pay attention to possible heterogeneity issues, and reassess the cooperation of FL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">MODELS</head><p>In this section, we introduce the two models employed in our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Input Generation from the Data Space</head><p>We generate a representative input that distributes all over the data space for a comprehensive test of output comparison. Because the original data space is often of too high dimensionality to perform an efficient sampling, we assume that the data is distributed in a low-dimensional subspace. Therefore, we perform PCA on the data to get the low-dimensional space. After that, we employ stratified sampling, which is more efficient and effective than random sampling, to sample inputs in the low-dimensional space.</p><p>The low-dimensional samples are then projected back to high-dimensional space to satisfy the input format. To avoid illegal input, range interception is implemented according to the definition of each dimension. For instance, in a handwritten digit dataset, the grayscale of a pixel is regarded as a dimension whose threshold is from 0 to 255. If a dimension is restored as 260, we correct it as 255 for legality and validity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Context-aware Clustering Approach</head><p>It is time-consuming to browse each inconsistent record from the high-dimensional space to compile a summary on heterogeneity issues. We, therefore, leverage a cluster analysis to organize inconsistent records and analyze heterogeneity. Observing clustering results with different settings of the cluster number can facilitate users to generalize heterogeneity issues from different levels. To support flexible adjustment of the cluster number, we employ hierarchical clustering to group inconsistent records.</p><p>However, inconsistent clusters may be mixed up with consistent records, which hardly contribute to extraction of heterogeneity issues. To exclude consistent records from inconsistency clusters, we have to consider the context of consistent records surrounding inconsistent records when constructing hierarchical clustering. Therefore, we change the distance measurement in the hierarchical clustering from Euclidean measurement to a rank-based measurement. We denote the set of all records as {s i |i = 1,...,n}. Among them, there exist m inconsistent records, which are {s i |i = 1,...,m} (m ≤ n). The Euclidean distance from s j to s k is represented by d E ( j, k). The rank-based measure calculates the distance between a pair of inconsistent records s j and s k (1 ≤ j ≤ m, 1 ≤ k ≤ m, and j = k) as:</p><formula xml:id="formula_0">d R ( j, k) = r j (k) × r k ( j),<label>(1)</label></formula><p>where r j (k) is the ranking of d E ( j, k) in {d E ( j, i)|i = 1,...,n}. Namely, s k is the r j (k)-th closest record to s j among all records, including consistent records. Based on the rank-based measurement, inconsistent records with less surrounding consistent records will be preferentially aggregated together, even if the inconsistent records are far apart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">VISUAL ANALYSIS INTERFACE</head><p>Corresponding to the workflow, the interface of HetVis consists of three modules, which are learning process monitor (Figure <ref type="figure">1</ref>(a)), output comparison (Figure <ref type="figure">1</ref>(b)), and heterogeneity examination (Figure <ref type="figure">1</ref> (c)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Learning Process Monitor</head><p>The module for monitoring the HFL process (Figure <ref type="figure">1</ref> Parameter projection view: As shown in Figure <ref type="figure">1</ref>(a1), the parameter projection view summarizes exchanges of model parameters between the local client and the server. Because model parameters updated in a single communication round could be affected by accident outliers, it is necessary to provide an overview of parameter updates during the entire training process. In each communication round, model parameters can be considered as a high-dimensional vector. To observe parameters from a low-dimensional perspective, we generate a 2D projection for the parameter vectors of the HFL model in all communication rounds. To reflect parameter exchanges, parameters submitted by the local client are transformed into vectors of the same size and projected onto the same plane. The employed projection approach is accelerated by a probabilistic algorithm <ref type="bibr" target="#b6">[7]</ref>. Considering that different parameters contribute in different ways, we allow users to specify a group of parameters and check them individually. For example, users can specify a layer in the neural network and check the projection of the model parameters on this layer.</p><p>In the parameter projection view, the points that project model parameters of the HFL model in each communication round are connected by a gray polyline. The grayscale encodes the time sequence.</p><p>A brown arrow is drawn from the point projecting federated parameters in round i − 1 to the local client parameters in round i. The federated parameters in round i are different from the local parameters because the federated parameters have integrated parameters from other clients. The size of the angle between the arrow and the polyline implies the disagreement of parameters at round i. The cosine of each angle in the high-dimensional space is calculated and encoded by the darkness of the corresponding arrow to avoid misunderstanding caused by projection distortion. Disagreements and compromises between the local client and the server can be observed by comparing historical parameter updates with the actual parameter evolution of the HFL model.</p><p>Performance view: At the bottom of the interface, the performance view (see Figure <ref type="figure">1</ref>(a2)) monitors the dynamics of the performance testified by the local data. Performance indicators consist of the training loss, the accuracy for the test set, and the total accuracy for local data. Users are allowed to switch indicators to apply a comprehensive evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Output Comparison</head><p>The second module (Figure <ref type="figure">1(b)</ref>) consists of an output comparison view and a summary of inconsistency clusters.</p><p>The result of output comparison is displayed in the output comparison view (Figure <ref type="figure">1</ref>(b1)). To contrast inconsistent records (in brown) with others (in gray), all records are projected through ccPCA. As mentioned in Section 4.2, the projection could hardly split all inconsistent records from the rest simultaneously. Users are allowed to check the overlaps by switching the top layer between inconsistency and consistency.</p><p>We list inconsistency clusters in the order of size (Figure <ref type="figure">1</ref>(b2)) to motivate heterogeneity examination. Each inconsistency cluster is represented by a glyph. In each glyph, the convex hull of the inconsistent records is superimposed on a density heatmap of all records in the output comparison view. The cluster size and the accuracy of the HFL model on the cluster are listed below the corresponding glyph. A cluster with an extremely low accuracy may suggest a significant impact from the corresponding heterogeneity issue. Clusters with insufficient records can be regarded as outliers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Heterogeneity Examination</head><p>Users examine an inconsistency cluster in the third module of the interface (Figure <ref type="figure">1(c</ref>)), which includes two views for distribution exploration, an instance verification panel (Figure <ref type="figure">1</ref>(c4)), and an annotations panel (Figure <ref type="figure">1</ref>(c5)). The distribution exploration views are introduced as follows.</p><p>Dimension exploration view: Users need to extract heterogeneity issues by identifying commons shared by inconsistent records but not shared by others. However, it is exhausting to inspect each dimension, considering the records are high dimensional. To improve analysis efficiency, we provide users with two entrances for dimension selections. The first entrance navigates users from the perspective of data space. This entrance shows how important each dimension is to distinguish the selected cluster from others based on the first two cPCs of ccPCA <ref type="bibr" target="#b3">[4]</ref>. The second entrance navigates users from the perspective of model behaviors, which only works for HFL based on CNN models. Gradient class activation maps (Grad-CAM) <ref type="bibr" target="#b28">[29]</ref> can measure how important each dimension is for a CNN model to classify a record. The average Grad-CAM of all records in the cluster can identify significant dimensions to a model. To analyze inconsistent model behaviors, we generate a pair of average Grad-CAMs for the stand-alone training model and the HFL model, respectively. Comparing the two average Grad-CAMs can help users identify the differences in model judgments. However, such differences in model judgment can hardly be demonstrated by Grad-CAMs in rare cases (e.g., models with cascading randomization) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12]</ref>. It is necessary to leverage both of the entrances.</p><p>We employ a pair of pixel maps to represent the user-specified dimension entrance. Note that both entrances generate a pair of importance values. When the entrance based on ccPCA is activated, two pixel maps represent the quantified importance of each dimension measured by the first two cPCs, respectively. When the entrance based on Grad-CAM is used, the two pixel maps represent the quantified importance evaluated by the two models, respectively. In a pixel map, each pixel corresponds to a dimension. If records are in the form of pictures, the relative position of a pixel is consistent with its position in pictures. If the pictures have more than one channel (e.g., RGB channels), users can select a channel and check a channel at a time. The color of a pixel encodes the dimension importance.</p><p>Users can hover on a pixel to find the pixel corresponding to the same dimension in the other pixel map. Users can also click a pixel and check the corresponding dimension distribution of all records or the records in the cluster (see Figure <ref type="figure" target="#fig_0">1(c2)</ref>). Percentage distributions of overall records, inconsistent records, and consistent records are displayed respectively. To adapt to different distribution patterns, the scale on the y-axis (percentage) can be switched from linear to logarithmic.</p><p>Label exploration view: A matrix design is leveraged to compare the ground-truth labels with the output of the HFL model (see Figure <ref type="figure" target="#fig_4">1(c3)</ref>). Each cell of the matrix corresponds to a pair of labels (see Figure <ref type="figure" target="#fig_4">3(a)</ref>). The horizontal position is specified by the ground-truth label, and the vertical position is specified by the output from the HFL model. The records in the non-diagonal cells indicate that the HFL model generates incorrect output. If extra labels (i.e., the labels not included in the ground-truth labels) exist in the output of the HFL model, there will be extra rows listed after the labels included in the ground-truth label. Users can scroll down to check the cells corresponding to extra labels. The records, which meet the pair of the ground-truth label and the output of the HFL model, are projected in the corresponding cell as a scatter plot by ccPCA <ref type="bibr" target="#b3">[4]</ref> (see Figure <ref type="figure" target="#fig_4">3(b)</ref>). For each cell, we count the inconsistent records in the selected cluster and the local data, respectively. The numbers are displayed in the upper-left corner (see Figure <ref type="figure" target="#fig_4">1(c3)</ref>). The inconsistent records and the consistent records are with the same color encoding as those in the output comparison view. Also, the convex hull of the selected cluster is drawn in each cell.</p><p>A grid-based heatmap is shown as a background in each cell (see Figure <ref type="figure" target="#fig_4">3(b)</ref>). The background color encodes the density of the corresponding ground-truth label. Through comparing the background with scatters or convex hulls, users can come up with conjectures, like "there exist label heterogeneity." Therefore, the cells in the same column are with the same background. The grid size can be adjusted to observe from different levels of granularity. Users are also allowed to hide the scatters and focus on the label distribution in cells. The contents in all cells can be zoomed in together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">User Interactions</head><p>Our system supports the following interactions.</p><p>Check an intermediate result of the HFL model. In the performance view, users can drag a handle to select a communication round and analyze the HFL model updated at this round. If the round is included in the user-specified range for the updates projection view, a circle appears in the projection view to highlight the corresponding round.</p><p>Request for recommended parameters. In consideration of efficiency, the initial contrastive parameters for ccPCA and the number of inconsistency clusters are default as 10 and 8. To seek better effectiveness, users can request recommended parameters by clicking the button on the right of the input box. The recommended contrastive parameter for ccPCA is provided by the original approach <ref type="bibr" target="#b3">[4]</ref>. The cluster number is recommended through the maximum difference <ref type="bibr" target="#b30">[31]</ref>.</p><p>Examine an inconsistency cluster. After browsing the list of inconsistency clusters, users can click on a glyph in the column of the model output comparison to examine it in the third module. The selected glyph will be highlighted by a thick stroke.</p><p>Apply instance verification. Users can select a record in the label exploration view by clicking. The record description, including dimension details, the ground-truth label, and the output of the two models, can be found in the instance verification view (see Figure <ref type="figure" target="#fig_5">1(c4)</ref>).</p><p>Manage records with suspicious heterogeneity issues. Users are allowed to select all records in the currently analyzed cluster as the object in the control panel (see Figure <ref type="figure">1(c5)</ref>). Note that local data records are selected by the convex hull if the data records in the cluster are sampled data. If necessary, the intersection set or the joint set of the current cluster and annotated records can be selected.</p><p>Leverage the analysis provenance. In the column of heterogeneity examination, users can annotate their findings and record the analyzed inconsistency cluster to the analysis provenance in the control panel (see Figure <ref type="figure">1</ref>(c5)). Each annotation generates a message icon in the performance monitor view. Users can click a message icon to review the details of previous annotations. Also, the records in the annotated set will be highlighted in both the model output overview and the label exploration view. If the annotated cluster has an overlap with an inconsistency cluster, the cluster glyph will be highlighted, and the size of the overlap will be shown below the glyph. Each annotation can be deleted by right-clicking on the icon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CASE STUDIES 7.1 Handwritten Digits Recognition</head><p>The MNIST dataset <ref type="bibr" target="#b16">[17]</ref> is employed in the first case to train a Multilayer Perceptron (MLP) for classification. The original dataset was distributed to 10 clients. Each client has 6,000 records of pictures, with two consecutive digital labels, e.g., digit-1 and digit-2. 10% records are employed in the test set. The analyzed client owns records with labels consisting of digit-0 and digit-1.</p><p>As shown in Figure <ref type="figure">1</ref>(a2), the loss became stable at the 40th round (R1.1). We check the convergence process by selecting this round. We employed local data as input to compare outputs from the stand-alone model and the HFL model (R2.2). 300 inconsistent records were identified from the comparison. As shown in Figure <ref type="figure" target="#fig_5">4</ref>, the numbers of inconsistent records in the label exploration matrix reflect that most inconsistent records locate in non-diagonal cells, where ground-truth labels are different from the output of the HFL model. The HFL model has a lower accuracy than the stand-alone training model. The HFL model classified partial records as other labels.</p><p>We generated a ccPCA projection based on the recommended contrastive parameter to have an overview of all records. The projection result indicated that inconsistent records are not gathered closely and heavily overlapped with consistent ones. To distinguish them, we captured the features of inconsistent records by inspecting their clusters (R3.1). We paid attention to the four clusters with the most records. The second, the third and the fourth clusters correspond to a rare handwritten style, respectively (see Figure <ref type="figure">5(a)</ref>).</p><p>We noticed that records with various handwritten styles are mixed up in the first cluster. To categorize styles and refine heterogeneity issues, we further split the first cluster up by setting a larger cluster number (see Figure <ref type="figure">5(b)</ref>). One of them, consisting of 47 records, can not be further split up with a minor increase in the cluster number. The label exploration view indicates that all of the records are with a ground-truth label of digit-1. To explore this cluster, we checked the cluster distributions of several dimensions with a high weight assigned by ccPCA. As shown in Figure <ref type="figure" target="#fig_7">6</ref>, certain dimensions are distributed in corners of the pictures. Verified by several records, we found that they were digit-1s in typography font (R3.2). The HFL model is inclined to misidentify this rare handwritten style as other digit labels, like digit-7.</p><p>We annotated the 47 records in the cluster and inspected these records in the final round (i.e., the 200th round) (R3.3). Partial records of digit-1 with an extra dot or giant digit-0 are corrected, while the rest are not (see Figure <ref type="figure">1</ref>(b2) and (c4) to inspect the cluster of digit-1s in typography font). Although the number of inconsistent records decreases to 208, the accuracy of the HFL model is still not satisfying as the stand-lone training model. Therefore, we should suggest other clients collect more pictures of digit-0 and digit-1 to optimize the HFL model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Face Mask Recognition</head><p>In the second case, we trained a federated CNN model to recognize if the person in a color picture wears a mask. Two clients participated in the FL cooperation. Our client used the face mask image dataset provided by Jangra <ref type="bibr" target="#b8">[9]</ref>. The other client employed the face mask detection dataset provided by Gurav <ref type="bibr" target="#b5">[6]</ref>. All records are unified to RGB images in the size of 28*28 pixels. The local training set has 6,000 records and the local test set has 1,792 records.</p><p>We first observed the learning process from the parameter projection view (R1.2). To compare high-level features extracted by CNN models, we selected the last two convolutional layers, respectively. Both the projected polylines of the parameters become relatively stable at the 90th round. We grouped the 547 inconsistent records identified at this round into 80 clusters (R3.1). Three clusters with extra-low federated accuracy and a relatively large size caught our attention. All of the records in these clusters are misclassified as with no mask by the HFL model. We then checked these records by clicking the highlighted circles in the label exploration view. As shown in Figure <ref type="figure">7</ref>(a1), the first cluster with 7 pictures corresponds to a person who puts a banana on the mouth to disguise a mask. These records are labeled incorrectly. In the other two clusters, the masks with special patterns are neglected by the HFL model (see Figure <ref type="figure">7</ref>(a2-a3)). We found more masks with special patterns after we set the cluster number as the recommendation (i.e., 125). As shown in Figure <ref type="figure">7</ref>(a4), a new cluster with 15 records is identified. Similarly, all records are misclassified by the HFL model.</p><p>To investigate the reasons, we compare discriminative regions of the HFL model with the stand-alone training model. Both Grad-CAMs highlight the same region (see Figure <ref type="figure">7(b)</ref>), which implies that the two models consider the same area as the basis of discrimination. However, the HFL model makes wrong judgments. We had a concern that the performance of the HFL model was affected because the data distributed in the other client is lacking our patterns.</p><p>To investigate the banana issue and the pattern issue, we annotated corresponding clusters and track them in the HFL process (R3.3). In the 200th round, the total of inconsistent records has decreased to 347. We reviewed the annotated records. Most pictures of masks with patterns are classified correctly by the HFL model (see Figure <ref type="figure">8</ref>). It turned out that the HFL model is capable of handling such statistical heterogeneity issues. However, a record with a "banana mask" was classified as with a mask. To optimize performance, we have to correct the labels of "banana masks" and notify the other client of this accident. We further compared model output by sample input (R2.2). The convex hulls of the most inconsistency clusters have no overlap with local data records. To judge whether corresponding data heterogeneity exerts positive impact on our classification task, we need to collect corresponding records from real world and verify the output with ground-truth labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Vehicle Recognition</head><p>In the third case, four clients seek accurate vehicle recognition by federated cooperation. Each client collected an equal number of pictures in three of four categories (i.e., plane, car, ship, and truck) from CIFAR-10 dataset <ref type="bibr" target="#b13">[14]</ref>. No picture is shared by two clients. The analyzed client owns 5,400 pictures: 1,800 pictures of plane, car, and truck, respectively. 900 pictures are included in the test set.</p><p>According to the pattern of parameter fluctuation (R1.1), we split the carried-out training process into three stages at the 25th round and the 110th round. As shown in the parameter projection view (Figure <ref type="figure">9</ref>(a)), parameters of the HFL model change significantly in the first 25 rounds, which indicates a fast learning process (R1.2). Then, the accuracy has approached its maximum around the 25th round (see Figure <ref type="figure">9(b)</ref>). At the second stage, divergence could be observed between local updates and the HFL model from the directions of the arrows. At the end of the second stage (i.e., the 110th round), the loss has reached its minimum (see Figure <ref type="figure">9(b)</ref>). In the last stage, the polyline in the parameter projection view fluctuates dramatically within a small range.</p><p>We checked the end of each stage to investigate the evolution of heterogeneity issues. Among 955 inconsistent records at the 25th round, 90% of them are classified incorrectly by the HFL model. The statistical results in the label exploration view demonstrate that the HFL model misclassified 425 records as ships (R3.2). Besides, the HFL model encountered difficulties in distinguishing between cars and trucks, which leads to other 225 incorrect records.</p><p>In the 200th round, 162 records were still classified as a ship (R3.3). To focus on the label distribution, we hid the scatters. As shown in Figure <ref type="figure">10</ref>(a), most planes were projected inside or beside the convex hull of inconsistent records, because planes have similar features to ships (e.g., with blue background). Meanwhile, records with labels of car or truck are distributed far from the convex hulls (see the green grids in Figures <ref type="figure">10(b</ref>) and (c)). The HFL model is inclined to classify a plane as a ship.</p><p>Nevertheless, the car-and-track issue was drastically alleviated since the 110th round. To inspect the confusion among ground-truth labels at the final round, we checked the first three rows of the label exploration matrix, which corresponds to the records classified as plane, car, and truck by the HFL model. There are 36 inconsistent records in the non-diagonal cells (misclassified by the HFL model) and 157 ones in the diagonal cells (misclassified by the stand-alone training model). Therefore, the HFL model is superior to the stand-alone training model in the task of distinguishing local labels. In summary, we could benefit from the HFL cooperation. To fix the plane-ship issue, we should invite more partners to train the HFL model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">DISCUSSION</head><p>In this section, we discuss 1) expert reviews on HetVis, and 2) the result of a comparative study for the proposed context-aware clustering approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Expert Reviews</head><p>We interviewed three researchers (E3, E4, and E5) who had worked on FL for two years. For each interview, we first introduced our approach and then presented a demo of our system. After that, experts were allowed to freely explore data heterogeneity of the above three cases in our system. At the end of the interview, we collected their feedback on the following four aspects. Each interview lasted about an hour.</p><p>Effectiveness. Due to data isolation, existing approaches to heterogeneity analysis are mainly by observing local data and identifying skewed distributions. All experts agreed that our system could help them better to formulate reasonable hypotheses of heterogeneity than their original approaches. E3 said that the heterogeneity issues could be solved by adjusting records in the local client (e.g., expanding records) and findings in our system indicated which kind of adjustments could improve the performance of the HFL. E5 also noted that cluster analysis could efficiently locate heterogeneous issues and guide batch corrections. Usage experience. The usability of our system received positive feedback from the experts. Both E3 and E4 appreciated the annotation functionality and commented that annotation allowed them to track certain sets of records. For visual designs, E3 and E5 were impressed by the intuitive representations in the parameter projection view. E3 commented that "It can clearly reflect the conflicts between local updates and the HFL parameters." Particularly, a disagreement occurred with the dimension exploration. E4 was inclined to navigate by ccPCA because it could summarize data characteristics. However, E5 preferred</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The three-stage workflow for analyzing heterogeneity issues in a federated cooperation from the perspective of a client.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>the first stage, users need to learn the model configurations of the global HFL model and observe the training process. Model configurations include the choice of aggregation algorithms, the number of clients, and the description of the local data. The training process of HFL can be described by performance fluctuation (R1.1) and the exchanges of model parameters between the local client and the server (R1.2, Figure 2(b)). Dramatic performance fluctuations or continuous conflicts of model parameters may indicate unsatisfying global data, which can motivate in-depth analysis for heterogeneity issues. The following analysis needs to be implemented based on a static model. To specify the intermediate HFL model, users need to select a communication round according to the observation results of the training process (Figure 2(c)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>the second stage, users need to indirectly compare global data and the local data by comparing prediction behaviors of the two models trained respectively. We employ local data to train a stand-along training model in the same model architecture with the global HFL model (R2.1). The stand-along training model is used to compare with the HFL model, which is trained with global data. To collect various outputs, we provide three sets of data records as input (R2.2, Figure 2(d)), which are local data and two automatically generated datasets (please refer to Section 5.1 for details). The records with inconsistent outputs (records classified as different labels by the two models), or we say in the following, inconsistent records, are then identified (Figure 2(e)). Users can leverage a cluster analysis method (R3.1, Figure 2(f), Section 5.2) to generate inconsistency clusters from inconsistent records.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a)) consists of an information panel, a parameter projection view (Figure 1(a1)), and a performance view (Figure 1(a2)). The information panel, which introduces the configuration information of the HFL model and the description of the analyzed client, are listed at the top of the module. The descriptions will be updated with the progress of the model training (i.e., a new communication round is finished).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The visual design of the label exploration view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Partial matrices in the label exploration view. A majority of inconsistent records distributed in the cells whose output from the HFL model includes other labels, like digit-5 and digit-6.</figDesc><graphic url="image-17.png" coords="7,84.92,86.57,99.55,99.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Fig. 5. Inconsistency clusters of inconsistent records identified from the 40th round. (a) The top four clusters with the recommended cluster number. (b) Two clusters split from the 1st cluster in (a) after cluster number reaches 50.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The distributions of three dimensions selected from the dimension weight chart. The high dimension values demonstrate that the corresponding pixel is passed by strokes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Fig. 7. Cluster analysis of the inconsistency identified at the 90th round. (a) Inconsistency clusters with an accuracy of 0%. (b) The Grad-CAMs of the 4th cluster in (a). The ground-truth label: With a mask</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="1,88.49,156.23,429.30,241.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>• Wei Chen, Zhen Wen, and Rongchen Zhu are with State Key Lab of CAD&amp;CG, Zhejiang University.</figDesc><table /><note>E-mail: {chenvis, whenzhen, zrcccrz}@zju.edu.cn. • Wei Chen is also with Laboratory of Art and Archaeology Image (Zhejiang University), Ministry of Education, China. • Jiazhi Xia is with School of Computer Science and Engineering, Central South University. E-mail: xiajiazhi@csu.edu.cn. • Tobias Schreck is with Graz University of Technology, Austria. E-mail: tobias.schreck@cgv.tugraz.at.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported by the National Natural Science Foundation of China (No. 62132017, 61972389). The work of Tobias Schreck has been supported by the Austrian FFG-COMET-K1 Center Pro 2 Future (Products and Production Systems of the Future), Contract No. 881844.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The 110th round   the model-driven Grad-CAM to understand model behaviors.</p><p>Findings. When the accuracy of either model was satisfying, E4 found that the inconsistent records could depict the boundary of two classes in the ccPCA projection. Based on this observation, E4 drew the conclusion that the HFL model and the stand-alone training model would fail to reach an agreement especially when the target records were hard to classify, i.e., the records were distributed along the classification boundary. E4 also found that ccPCA separated inconsistent records from the consistent context, which in the meantime split different classes (see Figures <ref type="figure">1(b1) and 8</ref>). This indicated that the projection results could also help users to assess model performance.</p><p>Advice. Despite the effectiveness and usefulness of our system, the experts offered two suggestions. First, we should take into account the architecture of the neural network when analyzing parameter exchanges (E3, E5). For example, users might focus on parameters in certain layers in the neural network when evaluating a deep learning model. Therefore, we improved the parameter projection view by grouping parameters based on layers. Users are allowed to select a layer and check parameters in the corresponding layer. Second, considering that dozens of labels might be yielded from the HFL model, E3 pointed out that the label exploration view needed an overview to facilitate object location. We plan to add it in the future version of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Comparative Study</head><p>As mentioned in Section 5.2, we proposed a context-aware clustering approach to extract heterogeneity issues. To prove the effectiveness of our clustering approach, we compared our approach with the distance-based clustering approach based on the local data in the first case to seek instance-level verification.</p><p>Two clustering approaches were applied to cluster the 208 inconsistent records identified in the 200th round. The maximum difference <ref type="bibr" target="#b30">[31]</ref> was employed to recommend appropriate cluster numbers for both clustering approaches. Our clustering approach generated 59 clusters, among which the largest cluster consisted of 45 records of typographic digit-1s while the distance-based approach yielded 118 clusters, none of which contained more than 6 records. Although the records in the same cluster were similar to each other, similar records (e.g., typographic digit-1s) were split into different clusters.</p><p>To eliminate the effects of the recommendation algorithm, we set the cluster number to 100 for both approaches in the second experiment. The distance-based approach showed little change-the largest cluster only consisted of 7 records. While our proposed approach still clustered 40 records of typographic digit-1s, which is much better than the other. The clustering results can be found in the supplemental material files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Limitations and Future Work</head><p>We discuss two limitations of HetVis and summarize our future work.</p><p>Scalability. The PCA-based algorithms employed in HetVis can hardly support the analysis of data with thousands of dimensions. We need to integrate high-performance dimensionality reduction approaches and dimension recommendation approaches. As mentioned by E3, the design of the label exploration view also has difficulty in adapting to a large number of labels. We plan to optimize this view by providing an overview and recommending significant labels in the future.</p><p>Extensibility. HetVis supports vector data, such as image data and tabular data. But text data and other data modalities are not supported by the current system due to different requirements of federated learning settings. Extending to new data modalities and machine learning tasks is an interesting future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSION</head><p>In this paper, we propose HetVis, a visual analysis approach to assist identification and examination of data heterogeneity under the privacy limit of HFL. Instead of directly comparing local data and global data, we compare the output of the HFL model with a stand-alone training model. A contrastive clustering analysis approach is leveraged to extract heterogeneity issues from the inconsistent records identified from the output comparison. In the future, we would like to extend our system to support online tuning of HFL model. The code of our system is available at the following link: https://github.com/EmmaammE/ HetVis.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sanity checks for saliency maps</title>
		<author>
			<persName><forename type="first">J</forename><surname>Adebayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Muelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Task-driven comparison of topic models</title>
		<author>
			<persName><forename type="first">E</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="320" to="329" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">RNNbow: Visualizing learning via backpropagation gradients in RNNs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cashman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mosca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="39" to="50" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Supporting analysis of dimensionality reduction results with contrastive learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fujiwara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O.-H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="55" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Heterogeneity in unclassifiable interstitial lung disease. a systematic review and meta-analysis</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ellison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Algamdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Collard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Ryerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of the American Thoracic Society</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="854" to="863" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Face mask detection dataset</title>
		<author>
			<persName><forename type="first">O</forename><surname>Gurav</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/omkargurav/face-mask-dataset" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions</title>
		<author>
			<persName><forename type="first">N</forename><surname>Halko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-G</forename><surname>Martinsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Tropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="217" to="288" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Quantifying heterogeneity in a meta-analysis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics in Medicine</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1539" to="1558" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Face mask ∼12k images dataset</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jangra</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/ashishjangra27/face-mask-12k-images-dataset" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">DRLViz: Understanding decisions and memory in deep reinforcement learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Jaunet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vuillemot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="49" to="61" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adaptive gradient-based meta-learning methods</title>
		<author>
			<persName><forename type="first">M</forename><surname>Khodak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-F</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5917" to="5928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The (un) reliability of saliency methods</title>
		<author>
			<persName><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hooker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Adebayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dähne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Explainable AI: Interpreting, Explaining and Visualizing Deep Learning</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="267" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A workflow for visual diagnostics of binary classifiers using instance-level explanations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Swartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Aphinyanaphongs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bertini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Visual Analytics Science and Technology</title>
				<imprint>
			<biblScope unit="page" from="162" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">RetainVis: Visual analytics with interpretable and interactive recurrent neural networks on electronic medical records</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="299" to="309" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A comparison of heterogeneity variance estimators in simulated random-effects meta-analyses</title>
		<author>
			<persName><forename type="first">D</forename><surname>Langan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bowden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Veroniki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kontopantelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Viechtbauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Simmonds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Research synthesis methods</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<title level="m">The MNIST database of handwritten digits</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A review of applications in federated learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Industrial Engineering</title>
		<imprint>
			<biblScope unit="page">106854</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Inspecting the running process of horizontal federated learning via visual analytics</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Federated learning: Challenges, methods, and future directions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="50" to="60" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sanjabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.06127</idno>
		<title level="m">Federated optimization in heterogeneous networks</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Analyzing the training processes of deep generative models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="87" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Communication-efficient learning of deep networks from decentralized data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hampson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Arcas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1273" to="1282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deepcompare: Visual and interactive comparison of deep learning model performance</title>
		<author>
			<persName><forename type="first">S</forename><surname>Murugesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="47" to="59" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">DeepEyes: Progressive visual analytics for designing deep neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pezzotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Höllt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Lelieveldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Eisemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vilanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="108" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Pister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Buono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Fekete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Plaisant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valdivia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.02972</idno>
		<title level="m">Integrating prior knowledge in mixed initiative social network clustering</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Squares: Supporting interactive performance analysis for multiclass classifiers</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="70" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Meta-analysis. Stevens&apos; handbook of experimental psychology</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Dimatteo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Grad-CAM: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Federated multi-task learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sanjabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4424" to="4434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Who belongs in the family?</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Thorndike</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="267" to="276" />
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">GANViz: A visual analytics approach to understand the adversarial game</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1905" to="1917" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">ConceptExplorer: Visual analysis of concept drifts in multi-source time-series data</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schreck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Conference on Visual Analytics Science and Technology</title>
				<imprint>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Federated learning with differential privacy: Algorithms and performance analysis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Farokhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Q</forename><surname>Quek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Poor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="3454" to="3469" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The What-If Tool: Interactive probing of machine learning models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pushkarna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bolukbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="56" to="65" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Federated learning for healthcare informatics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Glicksberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Healthcare Informatics Research</title>
		<imprint>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Helios: heterogeneity-aware federated learning with dynamically balanced collaboration</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2021 58th ACM/IEEE Design Automation Conference</title>
				<meeting>2021 58th ACM/IEEE Design Automation Conference</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="997" to="1002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Characterizing impacts of heterogeneity in federated learning upon large-scale smartphone data</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
				<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="935" to="946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06983</idno>
		<title level="m">Heterogeneity-aware federated learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Federated machine learning: Concept and applications</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A survey of visual analytics techniques for machine learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="36" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">BatchCrypt: Efficient homomorphic encryption for cross-silo federated learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">{USENIX} Annual Technical Conference</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="493" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Manifold: A model-agnostic framework for interpretation and diagnosis of machine learning models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Ebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="364" to="373" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
