<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Studying Early Decision Making with Progressive Bar Charts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ameya</forename><surname>Patil</surname></persName>
							<email>ameyap2@cs.washington.edu</email>
						</author>
						<author>
							<persName><forename type="first">Gaelle</forename><surname>Richer</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Jermaine</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ameya Patil is with University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dominik</forename><surname>Moritz</surname></persName>
							<email>domoritz@cmu.edu</email>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Jean-Daniel</forename><surname>Fekete</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Gaelle</forename><surname>Ric</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ameya Patil is with University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">¨her and Jean-Daniel Fekete are with Inria &amp; Universite Ṕaris-Saclay</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Studying Early Decision Making with Progressive Bar Charts</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-03-27T09:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Progressive visualization</term>
					<term>Uncertainty</term>
					<term>Bar charts</term>
					<term>Confidence intervals</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>a) baseline (b) ci (c) history (d) history ci Fig. 1: Four progressive bar chart designs showing the means for four columns of a data table loaded progressively, updated every second: (a) baseline bar chart-baseline, (b) bar chart with confidence intervals (95%)-ci, (c) bar chart with near-history-history, and (d) bar chart with near-history and confidence intervals-history ci. All the bar charts represent the same data at one step of the progression. For our proposed new designs (c) and (d), opacity and position along the X-axis encode the recency of the updates; opaque bars on the right represent more recent updates, and transparent bars on the left represent older updates.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Interactive visual data analysis on big data can suffer from poor user experience due to long wait times, which affect the user's analysis process <ref type="bibr" target="#b32">[33]</ref>. Progressive visualization systems address this interactivity issue by incrementally loading, processing, and visualizing data. As more data is processed over time, the visualization is refined from a crude to an increasingly accurate estimate. Although the intermediate visualizations enable users to decide early, their limited accuracy can make them unreliable, possibly leading to incorrect decisions. In this article, we study how fast and accurate people are when making decisions using progressive bar charts, and more specifically, the benefits of confidence intervals, and that of near-history representation which visualize the recent history of each bar, as shown in Fig. <ref type="figure">1</ref>.</p><p>To understand the benefits and challenges of progressive analysis, consider the example of tracking the vote count for a US presidential election. Typically, the results are tallied and sometimes even visualized as votes are received, to determine as early as possible which party will win. To compare estimates accurately, one needs to know if the intermediate visualization can be relied on to make inferences or if it is still insufficient. In the case of tracking vote counts, the processing order of states and the various types of ballots-paper, digital, mail-in, etc., create known biases in the early results. In a progressive system, the process is often simpler and is not biased since the data is loaded or processed in reasonably random order. Yet temporal artifacts can still happen, resulting in possibly misleading false patterns. Thus, the main challenge of progressive visualization is to accurately convey the uncertainty of intermediate results to help people avoid mistakes while supporting them in making decisions as soon as possible. Uncertainty also has a dynamic aspect related to the convergence of results: the instability of an estimate over time signals that it may not reflect the final result. Intuitively, users will monitor its stability for some time before trusting it. To help gauge stability, some systems display the evolution of the quality of intermediate results over time (e.g., <ref type="bibr" target="#b6">[7]</ref>).</p><p>In this article, we focus on progressive bar charts and investigate both uncertainty and stability cues for early decision-making. Progressive bar charts are standard bar charts where the heights of the bars change at every update (e.g., every second) as estimates are refined until they settle to their final value. Many progressive visualization systems use bar charts <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b59">60]</ref>, usually with confidence intervals shown as error bars to indicate estimate uncertainty. Prior work on uncertainty visualization has shown that uncertainty in data is difficult to understand in static visualizations <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27]</ref>, and in particular confidence intervals visualized as error bars are inaccurately interpreted for static bar charts <ref type="bibr" target="#b10">[11]</ref>. We thus study the usability of confidence intervals for bar charts in the progressive setting. Further, to better show the uncertainty due to the instability of the progression, we introduce a near-history representation that displays the history of estimates as traces (Fig. <ref type="figure">1c</ref>). The goal of this representation is to ease assessing the stability of the estimate by explicitly visualizing its trend over the progression. We also pair near-history with confidence intervals (Fig. <ref type="figure">1d</ref>), thus also visualizing the trend of the uncertainty of the estimates.</p><p>We present an empirical study on human decision-making performance using progressive bar charts and investigate whether nearhistory representations and confidence intervals improve early decisionmaking, i.e., lead to earlier decisions or fewer mistakes. We compared four designs for progressive bar charts: standard, with confidence intervals as error bars, with near-history, and with both confidence intervals and near-history. In a crowdsourced study, we ask participants to compare pairs of bars in a progressive setting. We used three realistic datasets: data generated from commonly found distributions (Normal and Power law) <ref type="bibr" target="#b23">[24]</ref>, and data generated progressively using the Wander Join algorithm for join operations in databases <ref type="bibr" target="#b31">[32]</ref> on a real-world dataset. Note that we generated randomly ordered data while not all real-world datasets come in random order. We evaluated the four designs quantitatively based on participants' response time and their error rate computed against the ground truth (the comparison of the final bar values) and qualitatively based on participants' confidence ratings. To estimate how early participants answer compared to the optimal time for a given accuracy, we also compared their performance to those of automated decision procedures. The results of our study suggest that people can make early decisions reliably using progressive bar charts, with decisions taken in 22 s on average, on our test having a maximum duration of 120 s (18% of the data), with 8% error. We measured a 14 s difference on average between the response times of participants and automated decision procedures on the same tasks for the same error rate. We report that confidence intervals and the near-history display significantly improve user performance over the standard bar charts, and bar charts combining both confidence intervals and near-history, but effect sizes remain small. User confidence ratings align with this trend and show the potential of near-history display for informing users on the uncertainty and stability of intermediate results. In summary, we contribute the following:</p><p>• an empirical study on the usability of progressive bar charts for a value comparison task; • new visualization designs for progressive bar charts showing near-history information; • a comparative evaluation of standard bar charts with/without confidence intervals, and with/without near-history. We also introduce the use of sequential tests in the context of progressive visualization to assess human performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>Our research questions and the motivation behind our proposed bar chart designs are informed by the existing progressive visualization systems and practices in decision-making. We briefly present these points in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Progressive Visualization</head><p>An important aspect of visual data analysis is the interactive latency <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b49">50]</ref>. Progressive visualization techniques reduce interactive latency, thus improving user experience, by breaking down different stages of the visualization pipeline into iterative steps (e.g., <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b51">52]</ref>). These techniques have also been shown empirically to be effective at mitigating the latency in visual explorations <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b57">58]</ref>. However, this iterative procedure means that the initial versions of the visualization show approximations which are refined over time. This gives rise to certain challenges with using progressive visualizations.</p><p>Analysis in progressive visualizations can be characterized temporally in three phases: early partial results that are not yet reliable, mature partial results that are trustworthy but susceptible to small changes, and definitive results that are stable <ref type="bibr" target="#b2">[3]</ref>. The length of these three phases is affected by the speed of access to the data, the data distribution, the order in which it is processed, and the underlying computation processing it. The user essentially tries to balance between speed and accuracy along these three phases <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b43">44]</ref>. The challenging aspect is the uncertainty in the data or computations and the resulting visualizations. Uncertainty in the visualizations tends to make users less confident about the conclusions drawn from them <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b37">38]</ref>. Thus, in our study, we evaluate the efficacy of confidence intervals in the progressive setting and propose new designs for progressive bar charts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Decision-making in the Progressive Setting</head><p>Two factors can affect decision-making in the progressive setting: errors due to repeated testing, and biases due to uncertainty.</p><p>Inferential methods inform on a property about a population, by studying a sample from the population and generalizing the results to the population. When significance tests are applied to multiple independent-drawn samples, it increases the probability of false positives. This undesirable effect of repeated testing is known as inflation of the family-wise error rate <ref type="bibr" target="#b0">[1]</ref>. Significance tests performed on progressively increasing samples are not independent since data points from previous samples are also accounted for in subsequent significance tests. Repeated testing causes problems of inflated error rate even in this case, although not as severe as in the case of multiple testing on independent samples <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b54">55]</ref>. In both cases, the issue occurs due to repeated application of the test until the p-value drops below the chosen significance level, biasing the outcome of the experiment. This is a well-studied problem in the field of sequential analysis that develops methods for performing repeated tests on progressive samples without the assumption of a fixed sample size <ref type="bibr" target="#b54">[55]</ref>.</p><p>The problem of inflated error rates has been addressed before by correcting the significance level for each test <ref type="bibr">[4, p. 27]</ref> depending on the number of times the test is planned to be applied and using confidence sequences <ref type="bibr" target="#b22">[23]</ref>. However, these corrections may be too strong in the context of progressive visualizations where the user does not necessarily decide at every time step, potentially switching focus between different tasks. More relevant to our work is the work of Zgraggen et al. <ref type="bibr" target="#b58">[59]</ref>, which highlights the problem of repeated testing specifically during visual analysis, but for static visualizations. We investigate whether humans are susceptible to this issue in the progressive setting, where there may not be enough time to decide at every step.</p><p>Decision-making may also be affected by how progressive estimates change over time and the biases that this may introduce. Users predict the ground truth based on the intermediate results they have seen so far. This reliance on trends in past data to predict the future could lead to inference uncertainty <ref type="bibr" target="#b50">[51]</ref>. Yet another possible source of error is misjudgments of the uncertainty due to the dynamic nature of visualizations in a progressive setting, also known as uncertainty bias <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b43">44]</ref>. This bias includes biases due to human tendencies to distrust fluctuating values-ambiguity bias <ref type="bibr" target="#b5">[6]</ref>, and neglect the visual glyphs for uncertainty visualization-neglect of probability bias <ref type="bibr" target="#b52">[53]</ref>. Finally, false patterns that occur in intermediate results have also been shown to affect decision-making, known as illusion bias <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b43">44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Visualizing Uncertainty for Progressive Analytics</head><p>Skeels et al. <ref type="bibr" target="#b50">[51]</ref> categorize uncertainty as: measurement uncertaintydue to imprecise data collection, completeness uncertainty-due to missing data, and inference uncertainty-due to making inferences based on models created from past data. Progressive visualizations systems potentially suffer from 2 levels of uncertainty-completeness uncertainty, and inference uncertainty. Progressive visualizations should ideally depict any uncertainty for users to make informed decisions. However, the very nature of uncertainty and the challenge of defining the correct behavior when using uncertainty visualizations, makes it difficult to account for uncertainty when analyzing data <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>Many techniques have been proposed to visualize the measurement and completeness uncertainty <ref type="bibr" target="#b42">[43]</ref>. Traditionally, uncertainty is represented with error bars and box plots <ref type="bibr" target="#b7">[8]</ref>; more recently, violin plots <ref type="bibr" target="#b21">[22]</ref> and gradient plots <ref type="bibr" target="#b27">[28]</ref> have been used as alternatives that allow for a continuous encoding of uncertainty <ref type="bibr" target="#b10">[11]</ref>. Gradient plots and violin plots were found to be better than error bars for understanding uncertainty <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11]</ref>. Animation has also been shown to be effective for judging uncertainty in static data, even for non-trained users <ref type="bibr" target="#b29">[30]</ref>. In progressive visualizations, using animation to convey the uncertainty of intermediate results may conflict with the animation due to progressive updates. Despite the new designs developed for uncertainty visualizations, bar charts with error bars for confidence intervals remain the standard in many progressive visualization systems today <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b59">60]</ref>. We thus narrow the scope of our study to bar charts with error bars and their variations since they have not been evaluated yet in the progressive context and the proposed near-history representation also integrates more easily with the standard encoding than with e.g., violin plots.</p><p>On the other hand, there is very little prior work on visualizing the stability or convergence, which is one of the sources of inference uncertainty in progressive visualizations. Fisher <ref type="bibr" target="#b17">[18]</ref> called for research on the visualization of convergence trends in progressive visualization systems. Visualizing stability in progressive visualization is connected to visualizing changes in data. Robertson et al. <ref type="bibr" target="#b47">[48]</ref> studied the effectiveness of visualizing changes in data using traces, animation, and small multiples, and found traces and small multiples to be better for visual analytics. This motivates our proposed near-history representation that follows the principle of traces, and we briefly discuss an alternative based on small multiples in Sect. 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">STUDY RATIONALE</head><p>Comparing values is one of the fundamental low-level visual analysis tasks, used in many of the high-level tasks like filtering, sorting, finding trends, finding extrema, etc. <ref type="bibr" target="#b1">[2]</ref>. These tasks are the basis for user intents like understanding who won the elections or understanding crop yield trends over a year. Since value comparison tasks are performed efficiently on bar charts compared to other visualization types <ref type="bibr">[39,49, p. 152]</ref>, we focused the study on value comparison using bar charts.</p><p>Comparing values with a progressive bar chart differs from a traditional static bar chart. Typically, the decision is that the value depicted by the height of bar A is higher than, lower than, or is not significantly different from the other bar B. With a static bar chart, the user can answer with any of the three options. However, with a progressive visualization, at any time of the progression, the user has also the option to wait before deciding. The progressive setting has an additional uncertainty due to the way data is sampled until it has been entirely processed. Even after all the data is processed, it can have an intrinsic uncertainty, which is not considered in our experiment. We want to determine if humans can make accurate decisions with progressive bar charts and if they can make these decisions early i.e., before the end of the progression, ideally as soon as the level of uncertainty allows for it.</p><p>Specifically, we focus on the task of comparing the mean estimates of two distributions A and B, represented as two progressive bars. We generate data distributions to have a non-zero significant difference between their true means so that either A or B is truly larger. We do so to avoid making the user wait unnecessarily until the end of the progression due to the possibility of there being no significant difference. We investigate whether users make mistakes, deciding that A is larger when actually it is B, or the opposite, by deciding too early.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Research Questions</head><p>The goal of our study is to understand how useful progressive bar charts are for decision-making and find ways to improve them. In particular, we identify the following research questions:</p><p>• (R1) How fast and accurate are humans when making decisions using progressive bar charts? • (R2) How do different designs for progressive bar charts affect human performance when making decisions? • (R3) How do different designs for progressive bar charts affect human confidence? To answer these research questions, we performed a crowdsourced study, described in Sect. 4. We evaluated four progressive bar chart designs, shown in Fig. <ref type="figure">1</ref>. Two are commonly used in progressive systems today: the standard bar chart-baseline (Fig. <ref type="figure">1a</ref>), and the bar chart with confidence intervals-ci (Fig. <ref type="figure">1b</ref>). In the following subsections, we motivate our proposed near-history designs for progressive bar charts, describe design considerations for near-history visualizations, and present our hypotheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Why Near-History Visualizations</head><p>In progressive analytics, one goal is to help users to decide when to decide. A decision can be made when (a) the estimates have low uncertainty or variance, and (b) the estimates have stabilized over time. x% confidence intervals (CI) for an estimate give a range of values that is x% likely to contain the ground-truth value <ref type="bibr" target="#b13">[14]</ref>, taking into account the estimate's variance and number of contributing data points. While CIs convey the uncertainty or variance of the estimates, users still have to rely on their memory of the past estimates to assess the stability of estimates. To avoid this additional cognitive load, we propose to extend progressive bar charts with near-history (traces). Instead of visualizing only the latest progressive estimate (and CI), we visualize the last n progressive estimates (and CIs), refer Fig. <ref type="figure">1c</ref> and Fig. <ref type="figure">1d</ref>.</p><p>In a progressive setting, since the convergence of estimates to the true values cannot be assessed in practice, patterns of stability in the progressive estimates are a useful indicator of their convergence. More precisely, an instability pattern acts as a proxy for the non-convergence of the estimate. Eventually, after a substantial amount of data is processed, it is more likely that both the CI and near-history information will suggest that a decision can be made i.e., narrow CI with a stability pattern. Even in this scenario, if the progression happens to process an outlier next, a decision made before that point may be incorrect. Both CIs and near-history only provide precursory information about how the progression might evolve in different ways. They provide sufficient conditions to not decide at any point, but not to decide at any point. The value of near-history is in providing the user a glimpse of how the estimate and the CIs have evolved so far so that any inaccuracy in decision-making can be better anticipated. The benefit of the nearhistory representation is to offload the task of remembering the trend of the estimates and variance. We thus hypothesize, based on the findings from Hullman et al. <ref type="bibr" target="#b25">[26]</ref>, that more visual information should lead to more correct decisions with more confidence, which we formally present in Sect. 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Design of Near-History Visualizations</head><p>When iterating on near-history designs, we compared different encodings, refer Fig. <ref type="figure" target="#fig_1">2</ref>. Our main design requirements were: (D1) visually scale to many bars, (D2) compare the latest estimates, (D3) compare the error bars of the latest estimates, and (D4) assess the trend of estimates.</p><p>We first considered encoding sequences of estimates with overlaid colored polylines with error bands for CIs, Fig. <ref type="figure" target="#fig_1">2a</ref>. Although this encoding is a natural choice, it can become cluttered by many categories close in range of values (D1). Closer to the traditional bar charts, we considered a mixed encoding where the polyline is nested in a bar representing the last estimate value, Fig. <ref type="figure" target="#fig_1">2b</ref>. However, to keep the encoding of near-history estimates and CIs as close as possible to their encoding in their counterpart standard bar charts, we discarded these designs. Focusing on encodings with both the history of estimates and CIs as bars, we considered two arrangements. The first arrangement spatially groups historical mini-bars by recency similar to juxtaposing multiple bar charts (small multiples), Fig. <ref type="figure" target="#fig_1">2c</ref>. The second arrangement is the design used in our experiment which we refer to as history (Fig. <ref type="figure">1c</ref>). It spatially groups historical mini-bars by category where the recency of updates is encoded by the position along X-axis, and opacity. Thus, mini-bars for old estimates are on the left and are more transparent than those for the newer estimates. These two designs have different trade-offs regarding our requirements. The design chosen in our study  can make requirements (D2) and (D3) difficult due to an increased gap between the visual marks to compare <ref type="bibr" target="#b53">[54]</ref>. On the other hand, the alternative design can make requirement (D4) difficult. To mitigate the issue due to increased separation in our chosen design, we frame each group of mini-bars with a black outline whose height encodes the latest estimate, thus bringing the marks to be compared, closer. We discarded the alternative design considering requirements (D1) and (D4)-with more bars, trend assessment would be almost impossible. We touch upon the use of interactions to achieve requirement (D1) in Sect. 6.4, but do not consider it in our study.</p><p>We also visualize the history of the CIs in history ci (Fig. <ref type="figure">1d</ref>). The idea is to combine the benefit of CIs with that of near-history information. Effectively, this design offloads the task of remembering the trend of both the estimate and its uncertainty or variance over time, by explicitly visualizing it. Although the increased number of visual elements in history , and even more in history ci , could increase the time required to process or engage with it, it can also improve understanding <ref type="bibr" target="#b25">[26]</ref>.</p><p>Other techniques like gradient plots have been found to be better than error bars <ref type="bibr" target="#b10">[11]</ref>. However, prior work by Procopio et al. <ref type="bibr" target="#b43">[44]</ref> did not find significant difference between the two in the progressive setting. Also, since bar charts are ubiquitous in progressive visualization systems today, we first evaluate near-history visualization with bar charts, and leave their evaluation with gradient or violin plots as future work.</p><p>The number of mini-bars in the near-history encodings can affect user performance. Fewer mini-bars would give lesser cues about the stability pattern, thus requiring users to memorize the trend more, especially if the progressive updates include frequent outliers and are thus chaotic. On the other hand, more mini-bars may overwhelm the user with increased movement in the visualization and take more screen real estate. Although it would be important to evaluate the effect of the number of mini-bars, we fixed it in our study, to evaluate the feasibility of near-history visualizations as the first step, and reduce the parameter space of the experiment. We fixed the number of mini-bars to 10 as a balance between providing sufficient stability cues and keeping the chart area small and leaving the determination of an optimum number of mini-bars for future work. The chart area is kept the same across all four designs (with/without near-history) in our study to avoid any confounding bias due to the chart area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Measures and Hypotheses</head><p>For the value comparison task in the progressive setting in our user study, we use the following measures to express our hypotheses: Accuracy: Accuracy is the ratio of correct decisions to the total number of decisions. A decision is a choice between "A is larger than B" and "B is larger than A" at an intermediate stage of the progression. The correctness of a decision is determined by the ground-truth i.e., the relation between A and B once all the data is processed. Response Time: Response time is the time taken to decide from the beginning of the progression in seconds. Equivalently, it can be expressed in the number of updates or the amount of data processed since the same amount of data is added each second in the experiment. User Confidence: User confidence is a subjective measure of usability collected per visualization on a 7-point Likert scale and done retrospectively so that participants can reflect on all visualizations. Note that we asked participants to rate the visualizations and not their decisions on individual tasks, since we want to evaluate the visualization designs themselves and not the decisions.</p><p>Our hypotheses for each of our research questions are:</p><p>• (H1) Humans can achieve high accuracy using progressive bar charts. We hypothesize that participants can achieve an accuracy of at least 80% for the value comparison task using progressive bar charts. We chose the threshold for accuracy partly based on results from prior work in decision making for value comparison tasks on static bar charts <ref type="bibr" target="#b48">[49]</ref> (92%), progressive bar charts <ref type="bibr" target="#b43">[44]</ref> (86%), and partly based on our subjective view of how accurate we think is good enough in the progressive setup, similar to the choice of .05 as the standard level of statistical significance <ref type="bibr" target="#b11">[12]</ref>. • (H2) In terms of user performance (accuracy and response time), bar charts with history and confidence intervals are better (higher accuracy and earlier response) than only with history or only with confidence intervals, both of which are better than plain bar charts i.e., history ci &gt; (history and ci ) &gt; baseline .</p><p>• (H3) In terms of user confidence history ci &gt; (history and ci ) &gt; baseline . We base the last two hypotheses on the motivation behind near-history visualizations discussed in Sect. 3.2. To assess whether the response times are early or rather late, we would ideally compare them with a reference response time. To compute these references, we investigated optimal sequential tests adapted to our comparison task, which would also be free of parameters and distribution assumptions to be fair to the participant's knowledge of the data. Here, optimal means minimizing the expected response time (also called sample size) for a fixed upper bound on the probability of error δ (usually δ = .05). However, having an optimal decision test for unknown parameters even under the normality assumption is challenging: most available solutions are only approximate or asymptotically optimal <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b41">42]</ref>. Without an ideal test, we compared human response times to approximations of optimal times computed with automated decision procedures, which we describe in Sect. 4.4. Note that in this work we only consider automated decision procedures as a means to obtain reference response times, to quantify how efficient humans are. We do not study these automated procedures to integrate them into progressive analytics systems or to completely replace humans in the decision-making process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">STUDY DESIGN</head><p>To address our research questions, we conducted a crowdsourced study on Prolific. We asked participants to compare the mean values of pairs of distributions using the four progressive bar chart designs, as accurately and quickly as possible. Existing literature on crowdsourced studies shows that participants tend to complete the tasks as quickly as possible <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b45">46]</ref>. Therefore, we awarded a £0.31 bonus (10% of the base rate of £3.13) if their overall accuracy was more than 50%, to discourage participants from botching tasks and encourage them to balance their accuracy and speed. Participants could also track their overall accuracy and thus, expected bonus through the experiment.</p><p>The study was performed in two rounds-a pilot study and the final study. Based on the findings and participant feedback from the pilot study, we made changes to the study design for the final study on two broad aspects: participant screening, and task complexity. The results did not change significantly between the two rounds, so the subsequent discussion will refer to the final study unless specified otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Participants</head><p>We restricted our participant pool to people with at least a collegelevel degree and formal education in science, mathematics, economics, and finance. Although we provided the required background about the use of progressive visualization, confidence intervals, and nearhistory information, our pilot study revealed possible difficulties for participants without some basic statistics background in understanding the experiment. Thus, to ensure a low rejection rate of responses, we added the aforementioned restrictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Tasks and Procedure</head><p>We performed a full-factorial within-subjects study for two factors: bar chart design (4) and dataset (3), and with four serial repetitions per pair of factors. We used a Latin square design for the design and dataset combinations. Each participant was randomly allocated a set of 48 comparison tasks (owing to 16 per dataset and 12 per bar chart design). Each comparison task corresponds to a pair of precomputed data sequences represented by two progressive bars, A and B, that are to be compared by participants. Each dataset is made of 100 pairs of precomputed data sequences, with their associated ground truth, i.e., which of A and B is truly larger. The pilot study had only one comparison task per trial page, i.e., asked participants to compare only two bars at a time, with which we saw a ceiling effect for accuracy. To mitigate this ceiling effect and also to make our setting more realistic, we switched to 4-bar bar charts in the final study and asked participants to compare two separate pairs of bars simultaneously on each. Fig. <ref type="figure" target="#fig_2">3</ref> shows a sample trial page for our revised study. The bars to be compared were randomly interleaved (compare A with B, C with D or compare A with C, B with D), with an equal number of each configuration for each combination of design and dataset. Different colors were used for each of the four bars to ensure a clear separation .</p><p>Each user session started with a consent page, followed by a tutorial on progressive visualization, the usage of confidence intervals, and near-history, followed by an explanation of the task-to compare the mean values of pairs of distributions using the progressive bar chart, as accurately and quickly as possible. We asked participants to selfreport their level of skill in information visualization and statistics and provided one training task per design before starting the recorded tasks. Each trial page included two questions, four answer buttons, and a blank visualization. When ready, participants could click the 'Start' button to start the progressive visualization updates; this also started a timer. To answer the questions, participants could click on one of the two answer buttons for each question (Fig. <ref type="figure" target="#fig_2">3</ref>). The progression stopped when both questions were answered or when all data were processed. Above the chart, a progress bar showed the progression as a percentage of processed data. Updates were made each second, for a total time of 120 s; however, participants were not explicitly informed that all progressions would last for 120 s, so as to nudge them towards thinking in terms of data processed and not time spent. As illustrated in Fig. <ref type="figure" target="#fig_2">3</ref>, clicking an answer button shows textual feedback about the correctness of the answer and updates the bottom bar with the new percentage of correctly answered questions. This was done to nudge participants to focus on accuracy rather than time. Finally, after answering all tasks, participants rated each design based on how confident they were using it to answer the tasks and explained their ratings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Datasets</head><p>The bar charts shown to the study participants visualized the mean of progressive samples of data drawn from a distribution. We used three different realistic data distributions to generate the datasets for our experiment. Two datasets are synthetically generated using two common distributions in real-world datasets: Normal and Power law <ref type="bibr" target="#b23">[24]</ref>. The third is made from the output of a progressive algorithm that computes the mean of the result of a join operation over real-world data. The four bars on each trial page corresponded to four different data sequences from the same dataset, i.e., generated from the same process but with different parameters. Each data sequence is an ordered set of 10k data points drawn from a distribution with a known true mean. Each dataset is a set of 100 pairs of data sequences to be compared, generated such that the difference between their means is always the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Synthetic Data: Normal and Power law Distributions</head><p>For the first dataset, normal , we used Normal distribution with mean and standard deviation chosen to represent real-life distributions of heights of dogs (μ ∈ <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30]</ref>, σ 2 ∈ <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b34">35]</ref>). For the second dataset, power law, we used Power law (or Pareto) distributions to represent salaries in a population <ref type="bibr" target="#b46">[47]</ref>. A Power law distribution has a denα−1 sity function of the form p(x) = (α − 1)x −α and a finite mean min x for α &gt; 2. Most naturally occurring Power law distributions, including salary distributions, have α ∈ <ref type="bibr">[2, 3] [40]</ref>. Thus, to keep the dataset realistic while ensuring meaningful mean estimates, we generate parameter α ∈ [2.75, 3.5] for our experiment (and parameter x min ∈ <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10]</ref>). To generate a data sequence for the normal or power law datasets, we first pick its distribution parameters at random within the ranges specified above. Next, we draw a sample of 10k data points from the distribution thus defined. The sequence of progressive samples is formed by accumulating chunks of 84 data points from this sample. For each progressive sample, the empirical mean and associated 95% confidence intervals are computed using bootstrapping <ref type="bibr" target="#b15">[16]</ref>.</p><p>Real-world Data: Wander Join Algorithm Our third dataset was generated using the Wander Join algorithm <ref type="bibr" target="#b31">[32]</ref> on the flights dataset <ref type="bibr" target="#b56">[57]</ref> to get the flight delay data. Wander Join algorithm computes approximate results for join queries and has been the foundation of subsequent work on progressive aggregation over joins <ref type="bibr" target="#b44">[45]</ref> owing to its suitability for OLAP use cases. The algorithm works by performing random walks. In each random walk, the algorithm randomly selects a tuple across the tables that satisfies the join clause and computes the required aggregation over multiple such walks, along with the associated uncertainty. In our study, we asked participants to compare the average flight delay for two categories of flights.</p><p>To sample approximately 10k data points for the two categories, we performed 2 × 10k random walks with appropriate filter clauses in the query. The mean estimates and confidence intervals were computed as part of the algorithm i.e., no bootstrapping was performed. We refer to the dataset created using this algorithm as wander join.</p><p>The stationarity of a process generating time series data means that the distribution of the generated data is constant over time, and consequently the statistical properties, such as the mean and variance. In a strong sense, this means that the distribution of different chunks of the generated data is the same. This kind of statistical consistency is desirable in the progressive setting because it allows for reliable predictions about the true estimate based on the intermediate progressive estimates. The generation process for normal and power law aims for stationarity: data sequences are random samples from a single distribution. Wander Join is an example of an algorithm that tries to generate a stationary sequence of progressive data but cannot avoid abrupt changes due to possible outlier values.</p><p>We generated distributions in pairs, such that the true mean of either one is necessarily greater than the other and that the empirical means computed at the end of the progression reflect the order of the true means, i.e., shows the correct answers. We also ensured that the mean estimate at the end of the progression was within 5% of the true mean of the distribution. Further, to avoid any confounding bias due to the difference between the true means of the two distributions, we ensured the same absolute difference of 2 units between the true means. This was done by adding an offset value to one of the two distributions in a pair. In some cases, this offset caused some data points to have impossible values, e.g., negative salary. However, the true means remained possible values so the data distribution was not changed in any way. We also used the same Y-scale for the bar charts in all trials, which ensures that the absolute difference of 2 units between true means is translated to an absolute visual difference as well.</p><p>To summarize, the participants were shown a progression of a total of 10k data points per bar, with updates once per second corresponding to 84 new data points. Similar to Procopio et al. <ref type="bibr" target="#b43">[44]</ref>, the data generation, computation of the means, and confidence intervals of each progressive sample were done offline. This allowed for assigning the same datasets to multiple participants and ensured that all saw the same progressions. During the experiment, the visualization was updated every second with the corresponding pre-computed mean estimates and confidence intervals for all four bars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Calculation of Measures</head><p>We used accuracy and response time as evaluation measures for human performance and compared them with the accuracy and response time of decision procedures to estimate how far people are from the optimal response time. Since finding a sequential and distributionagnostic decision procedure is an open problem, we use three decision procedures, chosen for their specific benefits: the confidence interval procedure, the t-test, and the General Likelihood Ratio Test (GLRT). The confidence interval procedure is a valuable baseline since it uses the same information as the one visually shown to participants with the ci and history ci designs. The t-test procedure is based on the well-known t-test, and, as such is easy to implement. Finally, the GLRT is a sequential test tailored to our comparison task. We run the procedures at the same data rate as in the user study i.e., progressive samples grow by the same amount of data points as between visualization updates and until a decision is reached (that either A or B is larger) or until the progression completes. For a fair comparison with humans, the procedures are entirely non-parametric; they decide solely based on empirical statistics (mean and variance) of the current progressive samples and the specified error threshold that we denote by δ . Confidence Intervals Procedure This procedure uses (1 − δ )% confidence intervals (CI) for the mean for every pair of progressive samples, as computed by the Wander Join algorithm or by the bootstrapping method without any correction for repeated testing. To compare the mean estimates of A and B, it compares the overlap between the CIs for A and B. If at any instant there is no overlap, the procedure decides that the highest mean is the largest. Otherwise, it follows Rule 4 described by Cumming [14, p. 7] which computes the proportion overlap of the CIs and allows to decide with 95% confidence when proportion overlap is 50% or less. With a larger proportion overlap, the procedure does not decide yet. Essentially, this inference rule performs a t-test using CIs, which we apply to the bootstrap and Wander Join CIs rather than regular CIs. This inference rule was included in the user study tutorial. t-test Procedure This procedure runs a Welch's two-sample t-test <ref type="bibr" target="#b55">[56]</ref> for every pair of progressive samples. If the p-value is larger than δ , the procedure continues. Otherwise, the procedure decides which of A or B is larger by comparing their empirical means. Said graphically, the procedure computes the function f (t) giving the p-value at every step t of the progression and stops once it crosses the boundary y = δ . Generalized Likelihood Ratio Test Wald's Sequential Probability Ratio Test (SPRT) <ref type="bibr" target="#b54">[55]</ref>), also called sequential likelihood ratio test, is a sequential test to decide between two simple hypotheses (of the form H 0 : μ = μ 0 against H 1 : μ = μ 1 ) for fixed error probabilities. The test is formulated as a boundary-crossing procedure: at each step, it compares the likelihood ratio, which measures how likely the observed values support one hypothesis against the other, with a boundary function defined to guarantee certain levels of error. While SPRT's boundary functions are proven to be optimal, the test does not support the decision task of our study. Generalized likelihood ratio tests (GLRT) <ref type="bibr" target="#b40">[41]</ref> extends the likelihood ratio test approach to support composite testing problems such as the one modeling the task of our study:</p><formula xml:id="formula_0">H 0 : μ A &gt; μ B against H 1 : μ B &gt; μ A .</formula><p>The GLRT statistic is the ratio between the likelihoods of the best-fitting parameter value from each hypothesis. The decision rule of the GLRT compares this statistic with a threshold function. If the statistic is smaller, the procedure continues. Otherwise, the procedure decides which of A or B is larger by comparing their empirical means. To achieve a reliable testing procedure, one has to devise a threshold function matching the given levels of error, in our case δ for both hypotheses. Our chosen threshold function follows the results developed for A/B testing by Kaufmann et al. <ref type="bibr" target="#b30">[31]</ref>.</p><p>The three procedures remain heuristics since their underlying decision rules assume normality, which is broken for two out of the three datasets. Therefore, while all procedures take a δ parameter, they might be too optimistic, i.e., fail more often than specified over many examples, or pessimistic i.e., decide late and fail less often than the specified error rate δ , by a large margin. This is even more likely for the CI and the t-test procedures that also ignore the sequential context and could suffer from repeated testing bias. A decision stripe is green if the decision is correct, red otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS &amp; ANALYSIS</head><p>Based on the results of our study, we present an analysis of human performance in the progressive setting, evaluate the progressive bar chart designs, and compare human performance with that of automated procedures. For the final study, we rejected 12 participants out of 90 owing to quality issues, to have a total of 78 participants. 80% of participants were at least competent in information visualization and around 60% were at least competent in statistics. We used a t-test to compute the significance of differences between techniques for task accuracy, response time, and confidence rating (shown with a dashed line on figures), and Cohen's d coefficient to compute their effect sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Task Complexity Level</head><p>Due to the nature of the distributions-Normal and Power law, and the Wander Join algorithm, the respective data sequences have different convergence behaviors. Thus, each dataset presents a different level of task complexity. Fig. <ref type="figure" target="#fig_3">4</ref> presents an example of pairs of data sequences (colored blue and orange) for each dataset, showing the mean estimates and confidence intervals at each step of the progression. We also show the decisions taken by the statistical procedures (described in Sect. 4.4) and for the sake of comparison, by all our study participants who were allotted that example pair of data sequences in their comparison tasks.</p><p>In general, the normal dataset is more well-behaved than the others, as can be seen in Fig. <ref type="figure" target="#fig_3">4</ref>. Very rarely during the progression, an incorrect decision is made by both the procedures and humans. With the power law dataset, the two mean estimates tend to fluctuate early in the progression, which produces misleading intermediate results.</p><p>Both the procedures and humans tend to decide later than for normal . The wander join dataset shows the most chaotic progression of the three, with more fluctuations between the two mean estimates and their confidence intervals. Both the procedures and humans tend to decide later or more incorrectly compared to the former two datasets. Thus, the general trend is that tasks from the wander join dataset are the most complex, followed by power law, and then normal , as illustrated by Fig. <ref type="figure" target="#fig_3">4</ref>. However, the random choice of distribution/algorithm parameters for generating the individual examples may result in certain examples being easier to compare in wander join than in power law or normal . We do not explicitly control the task complexity but instead rely on the random process to generate a variety of examples per dataset. We present results at the dataset level during our analysis.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Human Performance Evaluation</head><p>In this section, we report on the results of our study for task accuracy, response time, and user confidence ratings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Task Accuracy and Response Time</head><p>Fig. <ref type="figure" target="#fig_4">5</ref> shows the percentage accuracy (left) and response time (right) averaged across all participants, for each design, for each dataset, and across all datasets. Both the normal and power law datasets show an accuracy of around 95% across all designs. Since the accuracy is high, these two datasets do not provide much insight into the efficacy of different designs; there is no significant difference in accuracy and the effect sizes are small (d &lt; .2). For the more complex wander join dataset, the average accuracy is around 85% which highlights more differences. Specifically, history ci design was found to be less accurate compared to both ci and history with a small effect size (d ≈ .2), and the same is observed across all datasets (d ≈ .1). For all datasets combined, participants were 92% accurate on average, with no clear winner/loser in terms of task accuracy. However, the comparatively lower accuracy for history ci design in the complex wander join dataset and across all data sets suggests possible usability issues for history ci in real-life scenarios. The task response time is roughly the same at around 22 s, corresponding to less than 20% processed data (less than 2k data points out of 10k per bar) for each dataset and each design. There are significant differences between the response times for baseline and the other designs, in both the normal and power law datasets. However, the effect sizes are not large enough to be considered important with d ≈ .3 (absolute difference of 2-3 s). The wander join dataset has higher average response time compared to the other two owing to its higher complexity. This could indicate that users wait longer before deciding on chaotic progressions regardless of the visual representation. However, it gives a significant advantage to history compared to history ci . Overall, participants decided significantly earlier with the history and ci designs than with the baseline and history ci designs. However, none of the differences have a considerable effect size (d ≈ .1), with absolute differences of about 3 s.</p><p>Fig. <ref type="figure" target="#fig_5">6</ref> summarizes the results of human performance in terms of both task accuracy and response time. The bar charts which have high accuracy and low response time-the top-left corner of the plot, are better. The ci and history designs are comparable and marginally better compared to the baseline and history ci designs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Confidence Rating</head><p>To evaluate how confident users felt while using each design, we compared the confidence rating assigned to each, at the end of the study (1 for least, 7 for most confident). Although there are no decisive results about the efficacy of different designs in terms of accuracy and response time, Fig. <ref type="figure" target="#fig_6">7</ref> clearly shows that participants were the least confident with the baseline design, followed by the ci design with the history and history ci designs enjoying the highest confidence. Only the differences between the confidence rating for baseline and history</p><p>, and for baseline and history ci are significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison to Decision Procedures</head><p>To put the performance of the participants in perspective, in particular how early their responses were, we compared them with the decision procedures described in Sect. 4.4 on the same data. First, we checked how reliable the decision procedures are by comparing their accuracy with their pre-specified error threshold δ , here 5%. Fig. <ref type="figure">8</ref> shows the resulting accuracy and response time for the decision procedures run on the experiment data, for each dataset, and across all datasets, with a dotted line at 95% accuracy (the expected minimum accuracy). The t-test and GLRT were reliable as they exhibit more accuracy (97% and 99% respectively) than the pre-specified 95% threshold, and are even relatively robust on the non-normal datasets; only the t-test fails to match the threshold on wander join. The CI procedure exhibits an average accuracy close to random overall, making it unusable. The fact that it also decides almost immediately shows that it is overly optimistic and probably not calibrated for the sequential setting. Thus, we exclude it from the subsequent comparison.</p><p>Next, we look at the average response time of the t-test and GLRT. They respond relatively early, with decisions at 14 and 30 time steps on average i.e., with less than 25% of the data or 2500 data points. Since the procedures are not theoretically optimal, we can only assume that optimal decision times are earlier than their actual response times.</p><p>Finally, to compare the human response time with decision procedure response times at equal accuracy, we run the t-test and GLRT while varying the accuracy threshold from 95% to 80% (i.e., δ from 5% to 20%). We adopt this sampling approach because both procedures are conservative and achieve higher accuracy than their pre-specified threshold. Fig. <ref type="figure" target="#fig_5">6</ref> shows the corresponding variation of accuracy relative to response time for t-test and GLRT as a black curve. This highlights how conservative GLRT is, with an achieved accuracy above 97% even at an 80% threshold. The reticle in Fig. <ref type="figure" target="#fig_5">6</ref>'s inset is centered on the human average and highlights the results at the same accuracy/response time. Compared to the average human performance, GLRT achieves 97% accuracy in the same response time (21 time steps), and the t-test Fig. <ref type="figure">8</ref>: Task accuracy and response time of automated decision procedures with 95% accuracy threshold (dotted line), for each dataset and overall, with 95% confidence intervals for mean values. In gray, the average human performance for reference. Fig. <ref type="figure">9</ref>: Confidence ratings are broken down as per the participants' average compared to the median, for accuracy (rows) and response time (columns), with 95% confidence intervals for the median of confidence rating, and mean of accuracy and response time.</p><p>achieves the same accuracy (92%) in only 6 time steps. In summary, we find that human performance-both accuracy and response time, is close to the reference performance of decision procedures (GLRT and t-test). Although we find a difference of 15 time steps on average between the response of humans and the t-test for the same accuracy, the difference cannot solely be explained by a difference in the sample size required to decide, since measured response time for humans also includes a reaction time. We discuss this aspect in Sect. 6.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Hypotheses Evaluation</head><p>We now evaluate our hypotheses based on the results and analysis, thereby answering the corresponding research questions: • (H1) Humans can achieve high accuracy using progressive bar charts. Our study participants were around 92% accurate across all conditions, with an average response time of around 22 s (18% data processed). We thus accept it. • (H2) In terms of user performance (accuracy and response time), bar charts with history and confidence intervals are better (higher accuracy and earlier response) than only with history or only with confidence intervals, both of which are better than plain bar charts i.e., history ci &gt; (history and ci ) &gt; baseline . The quantitative evaluation failed to bring out a clear winner in both accuracy and response time. Both ci and history were found to be comparable and slightly better in accuracy or response time compared to baseline and history ci . We thus only partially accept it. • (H3) In terms of user confidence history ci &gt; (history and ci ) &gt; baseline . Participants only rated designs with near-history (history and history ci ) significantly higher than the baseline (by a small margin). We thus only partially accept it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>The results of this experiment demonstrate that participants could perform comparisons accurately and decide quickly, even with the baseline design for progressive bar charts, surprisingly. Progressive bar charts with confidence intervals (ci ) and with near-history (history ) facilitated earlier responses and participants felt more confident using the near-history designs (history and history ci ), although the differences between the four designs were not always significant and rather small. We now discuss some interpretations of our findings, generalizability of our work, and possible future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Understanding Errors</head><p>This user study of progressive bar charts was aimed at assessing how well users performed on realistic data but not at explaining the causes of their decision errors. On average, participants responded after 20 s. Would their accuracy have been different if they only saw the snapshot of the progressive bar chart at this point i.e., without seeing the progression? Without comparison to a static or a one-shot approximate bar chart, we cannot evaluate how much the error rates are explained by factors related to the progression such as the illusion bias or uncertainty bias, or if they are due to other factors that are not related to the progression, such as misreading confidence intervals.</p><p>Inspecting the verbal explanations for confidence ratings, it seems some participants were not used to confidence intervals e.g., reporting "I am not 100% comfortable with confidence intervals, [ci ] required more time to analyze and being confident with my answer" or faced difficulty in decision making-"When is the confidence interval small enough for me to trust the size of the bars?" Overall, 37% of participants declared limited statistical literacy and about a third scored baseline higher than ci in the confidence rating questionnaire. Since we did not collect a confidence rating per answer, we cannot analyze the relationship between perceived and displayed uncertainty (e.g., in the case of ci and history ci ). In the comments on their understanding of confidence intervals, many participants reported difficulty using it, stating that the error bars "look complicated just by looking at it", "[are] kind of misleading", "got [them] very distracted, [...] especially because they changed sizes way too often" and that "it is like one more data to take into account and it deconcentrates [them]". These results could relate to known inference issues in error bars (binary interpretation <ref type="bibr" target="#b10">[11]</ref>) or loss of trust due to fluctuations (ambiguity bias <ref type="bibr" target="#b5">[6]</ref>). It would thus be worth investigating the differences between progressive bar charts and one-shot approximate bar charts, and also how user confidence is affected by the displayed uncertainty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Breakdown by Human Performance</head><p>Participants rated ci , history , and history ci higher than the baseline on the confidence by the end of the study, although they did not always perform largely better with them during the study. This difference hints toward the higher learning curve of near-history designs or unfamiliarity with confidence intervals. Still, some participants reported the number of visual elements in the near-history designs, and even in ci design in a few cases, to be overwhelming, even by the end of the study. To explore the relationship between confidence and performance, we look at a breakdown of confidence ratings according to performance, shown in Fig. <ref type="figure">9</ref>. We split participants into four groups according to their average accuracy and response time compared to the median accuracy and response time of the experiment. We present this analysis as exploratory due to the small number of participants (≈ 20) and possible variations in the trial data of each group.</p><p>We can assume that the participants with the earliest responses (rightmost column) are relying on the cues provided in the visualization more than the others, who generally wait more to confirm their decision. Accordingly, the performance of these participants is more likely to highlight the effectiveness of each design. Further, participants with the earliest responses and the highest accuracy can be said to have a relatively better understanding of the study and the features of the different designs. Thus, participants in this category (bottom-right cell in Fig. <ref type="figure">9</ref>) might give a measure of how good users can get with more practice and exposure. From the low to high accuracy categories (middle to bottom row in Fig. <ref type="figure">9</ref>), there is a clear drop in confidence in the baseline design, and a slight increase in confidence in the history ci and history designs relative to the other designs. This hints toward the potential of near-history for more experienced users.</p><p>Finally, we look at the discrepancy between the confidence and performance results for history ci : it is rated among the highest in confidence rating while leading to the lowest accuracy overall. Participants with the lowest accuracy (middle row in Fig. <ref type="figure">9</ref>) make the most decision errors, i.e., have the lowest accuracy, using history ci while the rest of the participants achieve comparable accuracy with all designs. It may imply that history ci is a comparatively worse option than history or ci for users with less experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Improvements with Interactions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Generalizability</head><p>Our× study× represents× a× simplified× version× of× a× progressive× visual× anal-ysis× scenario,× with× two× tasks× to× perform× on× a× 4-bar× bar× chart× without× distracting× elements.×Accuracy× and× response× time× could× suffer× in× the× real-life× scenario× having× a× busier× interface× with× multiple× components× that× could× distract× users× from× focusing× on× only× one× or× two× progressions× and×more×bars×on×the×screen;×the×near-history×representation×could× become×more×effective×because×it×requires×less×recall.×We×designed× the× experiment× with× two× simultaneous× questions× concerning× two× pairs× of×bars,×interleaved×for×half×the×trials,×so×the×task×is×less×likely×to×be× done× by× memorizing× previous× values× of× one× bar× and× is× more× realistic.× As× the× number× of× bars× increases,× progressive× bar× charts× would× require× interaction× to× ease× comparison× between× bars.×New× experiments× could× be× done× to× verify× if× our× results× still× hold× when× progressive× systems× are× more× widespread.× Progressive×visualization×might×take×hours×to×complete×while×our× experiment× only× runs× for× 2× min.×Our× experiment× setting× provides× only× a×small×time×benefit×to×participants×as×they×could×afford×to×wait×till× the×end×for×securing×their×decision.×Making×decisions×early×is×even× more×important×for×longer×progressions×to×save×more×time;×shorter× progressions× would× not× need× progressiveness× at× all.×Consequently,× when× users× necessarily× have× to× decide× early,× the× scope× for× error,× and× thus× the× efficacy× of× near-history× could× increase.×Our× experiment× uses× relatively× small× sequences× (10k× data× points)× over× a× short× but× realistic× time× (120× s).× In× a× real× setting,× a× progressive× system× could× easily× download× 100k× table× rows× per× second× from× a× remote× server,× leading× to× faster× convergence.× Only× data× produced× through× complex× computations× or× very× slow× data× channels× could× lead× to× the× scenario× in× our× experiment.× The×three×datasets×in×our×experiment×represent×commonly×found× and× realistic× data× distributions× and× three× levels× of× complexity× for× the× decision× task.×We× expect× these× datasets× to× cover× both× the× easiest× and× most× difficult,× yet× realistic,× convergence× behaviors.×The× Pareto× nature× of× Power law distributions× (80/20× rule)× makes× it× more× likely× than× for× Normal distributions× to× observe× fluctuations× in× the× early× estimates.×The× wander join dataset×uses×real-world×data×which×we×found×contains× more× outliers× on× average× than× the× synthetic× datasets,× with× outliers× de-fined×as×values×falling×1.5×IQR×below×the×first×quartile×or×1.5×IQR× above×the×third×quartile,×where×IQR×is×the×interquartile×range.×Our× experiment× relies× on× randomly× ordered× data× whereas× real-life× datasets× can×come×in×a×particular×order×e.g.,×chronological.×Progressive×com-putations× are× sensitive× to× data× order× and× progressive× systems× do× their× best× to× operate× on× data× as× shuffled× as× possible× <ref type="bibr" target="#b9">[10]</ref>.×The× Wander Join algorithm× <ref type="bibr" target="#b31">[32]</ref>× simulates× random× order× and× stationarity.× Although× value× comparison× is× a× low-level× task,× it× is× very× common× and×the×basis×for×more×complex×compound×tasks× <ref type="bibr" target="#b16">[17]</ref>.×Future×work× is× needed× to× validate× other× visualization× tasks× in× a× progressive× setting.× With× respect× to× generalizability× to× other× chart× types× like× scatter× plots,× we× first× need× to× study× how× and× when,× near-history× traces× can× be× employed× meaningfully,×which×is×another×avenue×of×future×work.×Overall,×we× believe×our×experiment×is×ecologically×valid×for×dataset×complexity,× visual× task,× and× chart× type.×For× realistic× progressive× system× interfaces× and× the× progression× duration,× the× differences× between× the× efficacy× of× different× bar× charts× studied× in× our× work× will× become× clearer.× Part×of×the×difficulty×of×decision-making×with×progressive×visualiza-tion× is× in× making× precise× judgments× under× (dynamic)× uncertainty.×On× bar×charts,×confidence×intervals×facilitate×bar-to-value×and×bar-to-bar× comparison.×Pairwise× comparison× could× be× eased× by× highlighting× sig-nificant× differences× upon× hovering× on× a× bar× (or× mini-bar).×Interactions× could×be×used×to×improve×scalability×to×more×bars×and×support×other× tasks.×Showing× the× mini-bars× of× the× near-history× design× upon× interac-tion× only× can× improve× its× scalability× while× retaining× the× ability× to× assess× the×convergence×of×an×estimate.×The×four×visual×annotations×of×Fer-reira× et× al.× <ref type="bibr" target="#b16">[17]</ref>× designed× for× bar× charts× with× confidence× intervals× could× be×integrated×into×the×four×proposed×progressive×bar×chart×designs×to× support× more× comparison× tasks× (bar-to-bars,× bar-to-value,× bar-to-range),× and× extrema× identification.×Visually,× these× techniques× could× integrate× easily× as× they× use× color× scales× to× encode× the× probability× relevant× to× the× task,× e.g.,× the× probability× of× being× the× maximum× estimate,× or× of× being× over/under× another× estimate.×We× see× their× computational× adaption× to× the× progressive× setting× as× a× great× avenue× for× future× work.×</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Assessing Performance with Decision Procedures</head><p>In× this× work× we× introduce× the× use× of× decision× procedures,× inspired× by× sequential× tests,× to× assess× how× efficient× participants× are× for× our× chosen× comparison× task.×Decision× procedures× are× merely× used× as× a× means× to× estimate× the× optimal× decision× times× for× our× tasks× at× a× given× accuracy× threshold,×i.e.,×to×obtain×a×reference×for×participants'×response×times.× We× see× two× main× methodology× limitations× to× this× comparison× that× we× believe×are×also×promising×directions×for×future×work.× The×first×is× to×identify×a×valuable×comparison×procedure.×While×searching×for×a× baseline× for× our× experiment× results,× we× could× not× find× an× optimal× non-parametric× statistical× test× designed× for× sequential× testing× and× suited× to× our× comparison× task.×All× existing× tests× make× assumptions× about× the× data× distribution× and× their× parameters× whereas× our× humans× were× shown× data× with× no× prior× information× about× their× distribution.×The× initial× version× of× the× GLRT× we× devised× with× an× expert× turned× out× to× be× too× conservative× in× deciding× on× our× power law examples.×We× eventually× opted× for× two× well-performing×decision×procedures×and×make×all×experiment×data× available×to×facilitate×future×analyses×with×more×optimal×procedures.× The×second×limitation×is×the×measure×of×response×time.×Without×any× existing× model× of× human× decision-making× in× the× progressive× context,× we× cannot× determine× how× much× of× the× measured× response× time× is× taken× by× cognition× efforts× or× the× action× to× click× on× the× answer× buttons.×Indeed,× in× our× experiment× design,× the× view× is× updated× every× second× with× new× data,× even× without× an× explicit× decision× from× the× participants× to× continue.× As×a×result,×the×measured×response×time×of×our×participants×includes× response× delays× that× we× cannot× measure× with× our× experiment.×For× this× reason,× we× refrain× from× presenting× the× found× time× differences× as× ratios.×</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In× summary,× we× conducted× an× empirical× study× to× evaluate× how× efficient× progressive× bar× charts× are× for× early-decision× making× and× compare× four× alternative× designs:×standard× bar× chart× with/without× error× bars× for× confi-dence× intervals,× and× near-history× with/without× error× bars.×Our× results× provide× evidence× that× humans× can× do× accurate× comparisons× early× in× the× progression×using×progressive×bar×charts,×even×for×data×with×difficult× convergence× behavior.×More× experiments× are× needed× to× understand× how× this× finding× generalizes× to× other× tasks× and× chart× types.× In× terms× of× performance× and× confidence,× our× results× suggest× that× con-fidence× intervals× and× near-history× individually× support× users× in× deciding× earlier×and×with×higher×confidence,×more×than×the×standard×bar×chart× design× and× even× more× than× the× design× with× both× near-history× and× con-fidence× intervals.×Although× not× all× differences× were× significant× in× our× controlled× experiment,× we× believe× that× this× trend× would× be× accentuated× for× real-life× scenarios× and× for× trained× users.×</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Overlaid bands. (b) Bar-nested bands. (c) Juxtaposed bars.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Alternative near-history designs, with confidence intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Example trial page with two simultaneous tasks (Q1, Q2). A progress bar (blue) shows the percentage of data processed. Besides immediate textual feedback upon answering, participants can track their overall accuracy throughout the study on the yellow progress bar.</figDesc><graphic url="image-15.png" coords="5,81.52,366.50,196.32,124.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig.4: Examples of task data from the three datasets. For each dataset, the top plot shows the mean estimate and confidence interval at each time step for the two values to be compared (in blue and orange), and the bottom plots show the corresponding response times and correctness of the decision procedures and humans (12 to 21 participants) with stripes. A decision stripe is green if the decision is correct, red otherwise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Participant accuracy and response time for each bar chart design (colored marks), dataset (black marks), and overall (last row), with 95% confidence intervals for mean values and dashed lines for significant differences (p &lt; .05).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Task accuracy vs. response time for each bar chart design across datasets (colored ellipses) with 95% confidence intervals for the mean visualized by ellipse height and width. Black lines show the reference performance of decision procedures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Confidence rating for each bar chart design, with 95% confidence intervals for the median and dashed lines for significant differences (p &lt; .05, Wilcoxon t-test).</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We×thank×the×participants×of×our×pilot×study×for×their×time,×Emilie× Kaufmann×for×her×guidance×with×statistical×tests,×Pierre×Dragicevic,× Catherine×Plaisant,×and×Emanuel×Zgraggen×for×their×feedback.×This× article× is× a× result× of× the× Dagstuhl× seminar× 18411.×</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Problem with Unadjusted Multiple and Sequential Statistical Testing</title>
		<author>
			<persName><surname>×c</surname></persName>
		</author>
		<author>
			<persName><surname>Albers</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41467-019-09941-0</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1921</biblScope>
			<date type="published" when="2019-12">Dec. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Low-level components of analytic activity in information visualization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Amar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
		<idno type="DOI">10.1109/INFVIS.2005.1532136</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Information Visualization, 2005. INFOVIS 2005</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="111" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Review and Characterization of Progressive Visual Analytics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Angelini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Santucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Schulz</surname></persName>
		</author>
		<idno type="DOI">10.3390/informatics5030031</idno>
	</analytic>
	<monogr>
		<title level="j">Informatics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Sequential medical trials</title>
		<author>
			<persName><forename type="first">P</forename><surname>Armitage</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>2d ed. ed.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Repeated Significance Tests on Accumulating Data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Armitage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Mcpherson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Rowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series A (General)</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="235" to="244" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Omission Bias and Pertussis Vaccination</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Asch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kunreuther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meszaros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ritov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Spranca</surname></persName>
		</author>
		<idno type="DOI">10.1177/0272989X9401400204</idno>
	</analytic>
	<monogr>
		<title level="j">Medical Decision Making</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="118" to="123" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Steering the Craft: UI Elements and Visualizations for Supporting Progressive Visual Analytics</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Badam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Fekete</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.13205</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="491" to="502" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Overview and State-of-the-Art of Uncertainty Visualization</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bonneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Potter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rheingans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4471-6497-51</idno>
	</analytic>
	<monogr>
		<title level="m">Scientific Visualization, Mathematics and Visualization</title>
				<editor>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Hansen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Kaufman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Hagen</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Examining effort in 1d uncertainty communication using individual differences in working memory and nasa-tlx</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Quinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hosseinpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Padilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="411" to="421" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bi-Level Online Aggregation on Raw Data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rusu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3085504.3085514</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Scientific and Statistical Database Management, SSDBM &apos;17</title>
				<meeting>the 29th International Conference on Scientific and Statistical Database Management, SSDBM &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Error Bars Considered Harmful: Exploring Alternate Encodings for Mean and Error</title>
		<author>
			<persName><forename type="first">M</forename><surname>Correll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2014.2346298</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2142" to="2151" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On the Origins of the .05 Level of Statistical Significance</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cowles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Psychologist</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="553" to="558" />
			<date type="published" when="1982-05">May 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Vizdom: interactive analytics through pen and touch</title>
		<author>
			<persName><forename type="first">A</forename><surname>Crotty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Galakatos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zgraggen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Binnig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kraska</surname></persName>
		</author>
		<idno type="DOI">10.14778/2824032.2824127</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
				<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="2024" to="2027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inference by Eye: Confidence Intervals and How to Read Pictures of Data</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cumming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Finch</surname></persName>
		</author>
		<idno type="DOI">10.1037/0003-066X.60.2.170</idno>
	</analytic>
	<monogr>
		<title level="j">American Psychologist</title>
		<imprint>
			<biblScope unit="page" from="170" to="180" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sequential testing of a wiener process with costly observations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dyrssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ekström</surname></persName>
		</author>
		<idno type="DOI">10.1080/07474946.2018.1427973</idno>
	</analytic>
	<monogr>
		<title level="j">Sequential Analysis</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="58" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Better bootstrap confidence intervals</title>
		<author>
			<persName><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical Association</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">397</biblScope>
			<biblScope unit="page" from="171" to="185" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sample-oriented task-driven visualizations: allowing users to make better, more confident decisions</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>König</surname></persName>
		</author>
		<idno type="DOI">10.1145/2556288.2557131</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGCHI Conf. Human Factors Comp. Sys</title>
				<editor>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Palanque</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Schmidt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Grossman</surname></persName>
		</editor>
		<meeting>SIGCHI Conf. Human Factors Comp. Sys</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="571" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Incremental, approximate database queries and uncertainty for exploratory visualization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fisher</surname></persName>
		</author>
		<idno type="DOI">10.1109/LDAV.2011.6092320</idno>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE Symposium on Large Data Analysis and Visualization</title>
				<meeting><address><addrLine>Providence, RI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011-10">Oct. 2011</date>
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Trust me, I&apos;m partially right: Incremental visualization lets analysts explore large datasets faster</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schraefel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGCHI Conf. Human Factors Comp. Sys</title>
				<meeting>SIGCHI Conf. Human Factors Comp. Sys<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1673" to="1682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dive in! Enabling Progressive Loading for Real-Time Navigation of Data Visualizations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Glueck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Wigdor</surname></persName>
		</author>
		<idno type="DOI">10.1145/2556288.2557195</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGCHI Conf. Human Factors Comp. Sys., CHI &apos;14</title>
				<meeting>SIGCHI Conf. Human Factors Comp. Sys., CHI &apos;14<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="561" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Online Aggregation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1145/253260.253291</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGMOD, SIGMOD &apos;97</title>
				<meeting>SIGMOD, SIGMOD &apos;97<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="171" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Violin Plots: A Box Plot-Density Trace Synergism</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Hintze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Nelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="184" />
			<date type="published" when="1998-05">May 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Time-uniform, nonparametric, nonasymptotic confidence sequences. The Annals of Statistics</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramdas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauliffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sekhon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1055" to="1080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">VizNet: Towards A Large-Scale Visualization Learning and Benchmarking Repository</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gaikwad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hulsebos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zgraggen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kraska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Satyanarayan</surname></persName>
		</author>
		<author>
			<persName><surname>Demiralp</surname></persName>
		</author>
		<idno type="DOI">10.1145/3290605.3300892</idno>
	</analytic>
	<monogr>
		<title level="m">ACM Human Factors in Computing Systems (CHI)</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Why Authors Don&apos;t Visualize Uncertainty</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hullman</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2019.2934287</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="130" to="139" />
			<date type="published" when="2020-01">Jan. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Benefitting InfoVis with Visual Difficulties</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shah</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2011.175</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2213" to="2222" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<author>
			<persName><forename type="first">J</forename><surname>Hullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Correll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kay</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2018.2864889</idno>
	</analytic>
	<monogr>
		<title level="m">Pursuit of Error: A Survey of Uncertainty Visualization Evaluation</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="903" to="913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Displaying Uncertainty With Shading. The American Statistician</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jackson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="340" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Participants at your fingertips: Using amazon&apos;s mechanical turk to increase student-faculty collaborative research</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Borden</surname></persName>
		</author>
		<idno type="DOI">10.1177/0098628312456615</idno>
	</analytic>
	<monogr>
		<title level="j">Teaching of Psychology</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="245" to="251" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hypothetical outcome plots help untrained observers judge trends in ambiguous data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="892" to="902" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On the complexity of A/B testing</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Cappe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garivier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="461" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Wander Join: Online Aggregation via Random Walks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1145/2882903.2915235</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGMOD, SIGMOD &apos;16</title>
				<meeting>SIGMOD, SIGMOD &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="615" to="629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The Effects of Interactive Latency on Exploratory Visual Analysis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2014.2346452</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2122" to="2131" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Financial incentives and the &quot;performance of crowds</title>
		<author>
			<persName><forename type="first">W</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Watts</surname></persName>
		</author>
		<idno type="DOI">10.1145/1600150.1600175</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGKDD Workshop on Human Computation, HCOMP &apos;09</title>
				<meeting>SIGKDD Workshop on Human Computation, HCOMP &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="77" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Taking a HIT: Designing around Rejection, Mistrust, Risk, and Workers&apos; Experiences in Amazon Mechanical Turk</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mcinnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cosley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Leshed</surname></persName>
		</author>
		<idno type="DOI">10.1145/2858036.2858539</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="2271" to="2282" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The Human User in Progressive Visual Analytics</title>
		<author>
			<persName><forename type="first">L</forename><surname>Micallef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Angelini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aupetit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kohlhammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Santucci</surname></persName>
		</author>
		<idno type="DOI">10.2312/evs.20191164</idno>
	</analytic>
	<monogr>
		<title level="m">EuroVis 2019 -Short Papers. The Eurographics Association</title>
				<editor>
			<persName><forename type="first">J</forename><surname>Johansson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Sadlo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Marai</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Continuous Prefetch for Interactive Data Applications</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Netravali</surname></persName>
		</author>
		<idno type="DOI">10.14778/3407790.3407826</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
				<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2020-07">July 2020</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="2297" to="2311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Trust, but Verify: Optimistic Visualizations of Approximate Queries for Exploring Big Data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3025453.3025456</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="2904" to="2915" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Visualization Analysis and Design. A K Peters Visualization Series</title>
		<author>
			<persName><forename type="first">T</forename><surname>Munzner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Power laws, Pareto distributions and Zipf&apos;s law</title>
		<author>
			<persName><forename type="first">M</forename><surname>Newman</surname></persName>
		</author>
		<idno type="DOI">10.1080/00107510500052444</idno>
	</analytic>
	<monogr>
		<title level="m">Contemporary Physics</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="323" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On the Use and Interpretation of Certain Test Criteria for Purposes of Statistical Inference: Part I</title>
		<author>
			<persName><forename type="first">J</forename><surname>Neyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Pearson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1/2</biblScope>
			<biblScope unit="page" from="175" to="240" />
			<date type="published" when="1928">1928</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sequential probability ratio test for skew normal distribution</title>
		<author>
			<persName><forename type="first">L</forename><surname>Opperman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ning</surname></persName>
		</author>
		<idno type="DOI">10.1080/03610918.2019.1614623</idno>
	</analytic>
	<monogr>
		<title level="j">Communications in Statistics -Simulation and Computation</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2823" to="2836" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Uncertainty Visualization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Padilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hullman</surname></persName>
		</author>
		<idno type="DOI">10.1002/9781118445112.stat08296</idno>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="page">2021</biblScope>
			<pubPlace>Ltd</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Impact of Cognitive Biases on Progressive Visualization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Procopio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mosca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2021.3051013</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graphics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><surname>Procopio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Selective Wander Join: Fast Progressive Visualizations for Data Joins</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Choice behavior in an optional stopping task</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rapoport</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tversky</surname></persName>
		</author>
		<idno type="DOI">10.1016/0030-5073(70)90008-5</idno>
	</analytic>
	<monogr>
		<title level="j">Organizational Behavior and Human Performance</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="105" to="120" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">From gene families and genera to incomes and internet file sizes: Why power laws are so common in nature</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Hughes</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevE.66.067103</idno>
	</analytic>
	<monogr>
		<title level="m">Physical review. E, Statistical, nonlinear, and soft matter physics</title>
				<imprint>
			<date type="published" when="2002-12">Dec. 2002</date>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page">67103</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Effectiveness of Animation in Trend Visualization</title>
		<author>
			<persName><forename type="first">G</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2008.125</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1325" to="1332" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Task-Based Effectiveness of Basic Visualizations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Saket</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Endert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">¸</forename><surname>Demiralp</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2018.2829750</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2505" to="2512" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Response Time and Display Rate in Human Performance with Computers</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
		<idno type="DOI">10.1145/2514.2517</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="265" to="285" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Revealing uncertainty for information visualization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Skeels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Robertson</surname></persName>
		</author>
		<idno type="DOI">10.1145/1385569.1385637</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. AVI &apos;08</title>
				<meeting>AVI &apos;08<address><addrLine>Napoli, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page">376</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Progressive Visual Analytics: User-Driven Visual Exploration of</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Stolper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gotz</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2014.2346574</idno>
	</analytic>
	<monogr>
		<title level="j">Progress Analytics. IEEE Trans. Vis. Comput. Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1653" to="1662" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Probability Neglect: Emotions, Worst Cases, and Law</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sunstein</surname></persName>
		</author>
		<idno type="DOI">10.2139/ssrn.292149</idno>
	</analytic>
	<monogr>
		<title level="j">The Yale Law Journal</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<date type="published" when="2001-11">Nov. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Four Experiments on the Perception of Bar Charts</title>
		<author>
			<persName><forename type="first">J</forename><surname>Talbot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Setlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2014.2346320</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2152" to="2160" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Sequential tests of statistical hypotheses. The Annals of Mathematical</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wald</surname></persName>
		</author>
		<idno type="DOI">10.1214/aoms/1177731118</idno>
	</analytic>
	<monogr>
		<title level="j">Statistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="117" to="186" />
			<date type="published" when="1945">1945</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The generalization of &apos;Student&apos;s&apos; problem when several different population variances are involved</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Welch</surname></persName>
		</author>
		<idno type="DOI">10.1093/biomet/34.1-2.28</idno>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="28" to="35" />
			<date type="published" when="1947-01">Jan. 1947</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<author>
			<persName><forename type="first">H</forename><surname>Wickham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASA 2009 Data Expo</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="281" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">How Progressive Visualizations Affect Exploratory Analysis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Zgraggen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Galakatos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Crotty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Fekete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kraska</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2016.2607714</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1977" to="1987" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Investigating the Effect of the Multiple Comparisons Problem in Visual Analysis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Zgraggen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zeleznik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kraska</surname></persName>
		</author>
		<idno type="DOI">10.1145/3173574.3174053</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGCHI Conf. Human Factors Comp. Sys</title>
				<meeting>SIGCHI Conf. Human Factors Comp. Sys</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018-04">Apr. 2018</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Pattern discovery: A progressive visual analytic design to support categorical data analysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jvlc.2017.05.004</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Languages &amp; Computing</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="42" to="49" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
