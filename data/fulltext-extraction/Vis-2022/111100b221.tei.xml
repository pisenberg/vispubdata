<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Comparison Conundrum and the Chamber of Visualizations: An Exploration of How Language Influences Visual Design</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Aimen</forename><surname>Gaba</surname></persName>
						</author>
						<author>
							<persName><roleName>Member </roleName><forename type="first">Vidya</forename><surname>Setlur</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Arjun</forename><surname>Srinivasan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jane</forename><surname>Hoffswell</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Cindy</forename><surname>Xiong</surname></persName>
						</author>
						<title level="a" type="main">Comparison Conundrum and the Chamber of Visualizations: An Exploration of How Language Influences Visual Design</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-03-27T09:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Comparative constructions</term>
					<term>cardinality</term>
					<term>explicit and implicit comparisons</term>
					<term>natural language</term>
					<term>intent</term>
					<term>visual analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. 1. Four comparison utterances from our design space with varying cardinalities for the comparison entities (1-1, 1-n, n-m, n) and different levels of concreteness (explicit and implicit). Each of these comparison utterances was included in our online survey in which participants ranked their preference for the different visualization types; the most preferred visualization(s) have a colored border.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Visual comparisons are a common and critical task in analytic work-design choices in a visualization can nudge viewers to see different flows. When people read a visualization, each comparison they make patterns and produce different sentences <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b68">69]</ref>, further highlighting can be thought of as an analytical sentence that describes common the complexities and nuances of comparison tasks. For example, Figpatterns, differences, and trends in the data <ref type="bibr" target="#b62">[63]</ref>, such as "the recovery ure 2 shows how the juxtaposition of small multiples, either vertically rate is overall the same for Hospitals A and B." Even seemingly-small or horizontally, can elicit different comparisons across the two groups.</p><p>Recommendation systems attempt to provide smart defaults to help users gain insights into their data but focus only on common operations</p><p>• Aimen Gaba and Cindy Xiong are with UMass Amherst. E-mail: {agaba, such as filtering and sorting. They do not provide recommendations for yaxiong}@umass.edu. facilitating visual comparisons <ref type="bibr" target="#b58">[59]</ref>. Recently, visual analysis interfaces have introduced natural language (NL) queries into recommendation {vsetlur, arjunsrinivasan}@tableau.com.</p><p>systems <ref type="bibr" target="#b2">[3]</ref> that enable users to type NL utterances and receive appropri-</p><p>• Jane Hoffswell is with Adobe Research. E-mail: jhoffs@adobe.com. ate visualization responses. Such systems make extracting insights from data more intuitive, as the process of generating utterances exposes the  <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b53">54]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>xx xxx. 201x; date of current version xx xxx. 201x. For information on</head><p>However, the complexity of comparison expressions makes supporting obtaining reprints of this article, please send e-mail to: reprints@ieee.org.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NL-based visual comparisons a non-trivial task.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Digital Object Identifier: xx.xxxx/TVCG.201x.xxxxxxx</head><p>We identify two main challenges for building an effective recom-1221 Fig. <ref type="figure">2</ref>. Examples of two sets of spatial arrangements for visual comparisons based on findings from Xiong et al. <ref type="bibr" target="#b68">[69]</ref>. Results found that stacking small multiples vertically tends to elicit 1-to-1 comparisons of individual values in a set, whereas aligning them horizontally tends to elicit a comparison of the two sets as two groups.</p><p>mendation system that can help users more easily make comparisons in visualized data. First, designing a visualization involves a series of decisions, each of which can afford different viewer takeaways <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b71">72]</ref>. For example, past work has shown that people tend to make magnitude comparisons when viewing bar charts and trend comparisons when viewing line charts <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b72">73]</ref>. However, to the best of our knowledge, prior work does not propose a comprehensive mapping between comparison intents and the best visualization for the intended comparisons.</p><p>Second, there exists semantic ambiguity and complexity in NL that makes the intent difficult to parse. These characteristics hinder the effectiveness of incorporating NL approaches to support visual comparison tasks <ref type="bibr" target="#b61">[62]</ref>. While explicit comparisons such as "compare the number of trees planted in location A to the number in location B" are easy to decode, implicit comparisons are difficult for visualization systems to interpret precisely. For example, consider the example, "Are the number of trees planted in location A tall?". For comparison utterances that use vague modifiers such as "tall" <ref type="bibr" target="#b32">[33]</ref>, it is difficult to determine how the user intended for "tall" to be interpreted, i.e., what constitutes something being tall? For comparisons that contain underspecified references to the dataset, such as referring to "performance" when "performance" is not a variable explicitly contained in the dataset, it is similarly difficult to determine how a system should behave. Figuring out how to decode comparison intents from implicit utterances and underspecified references in NL for visualization use cases still remains an open question.</p><p>Contributions: We contribute a preliminary design space of NL comparison utterances covering four cardinalities of comparisons and addressing ambiguities by differentiating implicit and explicit ways of expressing these comparisons. Through a series of interviews with 16 visualization novices and experts, we create a potential mapping between comparison utterances in our design space and different visualization techniques. We then empirically validate this mapping through an online survey with 77 participants to highlight three design implications for interpreting comparison utterances for visualization use cases. These design implications can inform the design of future NL-based recommendation systems to better support visual comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Designing effective visualizations for comparison utterances is an ongoing area of research and can be categorized into three main themes: <ref type="bibr" target="#b0">(1)</ref> comparisons in visualization, <ref type="bibr" target="#b1">(2)</ref> comparisons in computational linguistics, and (3) natural language interfaces (NLIs) for visual analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Comparisons in Visualization</head><p>Representing comparisons in data visualizations is an important aspect of a user's analytical workflow and prior work has surveyed a variety of visualization solutions to better support comparisons. For example, Tufte discussed small multiples as a way to use the same graphic to display different slices of a data set for comparison <ref type="bibr" target="#b67">[68]</ref>. Graham and Kennedy <ref type="bibr" target="#b29">[30]</ref> surveyed a range of visual mechanisms to compare trees, while other surveys consider methods for comparing flow fields <ref type="bibr" target="#b47">[48]</ref>. Gleicher et al. <ref type="bibr" target="#b26">[27]</ref> presented a broad survey with over 100 different comparative visualization tools from information visualization domains, organized by their comparative visual designs into a general taxonomy of visual designs for comparisons. Designs were grouped into three categories: juxtaposition, superposition, and explicit encodings.</p><p>The perceptual and cognitive science communities have also considered the problem of visual comparison for decades, including the issues around change blindness <ref type="bibr" target="#b51">[52]</ref>. Other studies focused on comparing methods for specific tasks, such as the evaluation of scalar field comparisons <ref type="bibr" target="#b45">[46]</ref> or brain connectivity graphs <ref type="bibr" target="#b6">[7]</ref>. Franconeri <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> discussed several limitations in the mechanisms of perception that may have a direct impact on the design of comparison methods. For example, translations of an object are easy to compare, but texture, orientation, scale, space, and time may complicate comparison tasks <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>Existing work has largely focused on how visualization type can influence viewer perception and decisions <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b72">73]</ref>. For example, bar charts plot the data as discrete objects, motivating people to compare them as two distinct units (e.g., A is larger than B), while line charts plot data as one single object, eliciting the interpretation of trends, changes over time or relations (e.g., X fluctuates up and down as time passes) <ref type="bibr" target="#b72">[73]</ref>. Showing difference benchmarks on bar charts can not only facilitate a wider range of comparison tasks <ref type="bibr" target="#b64">[65]</ref> but also increase the speed and accuracy of the comparisons <ref type="bibr" target="#b46">[47]</ref>. Charts that show probabilistic outcomes as discrete objects, such as a beeswarm chart, can promote a better understanding of uncertainties <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b66">67]</ref>.</p><p>Comprehension of visual comparisons is an important aspect of determining their efficacy. Researchers have referenced knowledge from basic human perception research to generate design guidelines regarding graphical elements, such as the visualization color and shape. For example, in multi-class scatterplots, viewers can compare classes of scatterplots more effectively when they are colored differently compared to when they are plotted with different shapes <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28]</ref>. Shah and Freedman <ref type="bibr" target="#b62">[63]</ref> investigated the effect of format (line vs. bar), viewers' familiarity, and their graphical literacy skills on the comprehension of multivariate data presented in graphs. The differences between people's perceptions of bar graphs and line graphs can be explained by differences in the visual chunks formed by the graphs based on Gestalt principles of proximity, similarity, and good continuity. Jardine et al. <ref type="bibr" target="#b33">[34]</ref> conducted an empirical evaluation on two comparison tasks: identify the "biggest mean" and "biggest range" between two sets of values. Their work showed that visual comparisons of these tasks are most supported by vertically stacked chart arrangements, and this pattern is substantially different across different types of tasks.</p><p>Much of this prior work has focused on exploring effective visual representation of comparisons in data. However, with the prevalence of NLIs that support visual analysis, there is a need for such systems to be able to interpret comparison utterances and provide useful visualization responses to the user. Our work specifically focuses on better understanding the design space of mapping data comparisons expressed through language with useful visual representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Comparisons in Computational Linguistics</head><p>The syntax and semantics of comparatives have been the topic of research in computational linguistics for quite some time. Comparative expressions which establish orderings among objects according to the amount or degree to which they possess some property is a basic component of human cognition <ref type="bibr" target="#b37">[38]</ref>. Language constructs often provide concepts to express gradable concepts such as the explicit orderings between two objects (e.g., "the price of housing in the Bay Area is higher than in Texas") <ref type="bibr" target="#b55">[56]</ref>. Extensive prior work in computational linguistics has explored the semantics of comparison based on gradable concepts such as "more", "less", "-er" <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b57">58]</ref>.</p><p>The notion of vagueness in comparative language has previously been studied in the computational linguistics community <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21]</ref>. The semantics of comparatives can be vague as their interpretation depends on the context and the boundaries defining the comparative. For example, "when is it the safest time to fly?" implicitly compares flying safety across different time periods, with "safe" being a fuzzy, subjective concept. Prior work has focused on the conceptualization and representation of vague knowledge. For example, Kessler and Kuhn presented a corpus of annotated comparison sentences from English camera product reviews. Each sentence contains the comparative predicate that expresses the comparison, the comparison type, the two entities that are being compared, and the aspect in which they are compared <ref type="bibr" target="#b38">[39]</ref>.</p><p>While linguistic vagueness has been explored for comparative expressions along with their semantic variability, little work has been done in determining how best to visually represent comparatives based on these variations, especially in the context of visual analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Natural Language Interfaces (NLIs) for Visual Analysis</head><p>Much of the effort for supporting NLIs for visual analysis <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b58">59]</ref> focuses on interpreting a user's analytical intent by providing useful visualization responses. The methods of interpreting intent typically rely on explicitly named data attributes, values, and chart types in the user's input queries. Ask Data <ref type="bibr" target="#b61">[62]</ref> supports analytical expressions in NL such as grouping of attributes, aggregations, filters, and sorts. The system also handles impreciseness around vague numerical concepts such as "cheap" and "high" by inferring a range based on the underlying statistical properties of the data. Hearst et al. <ref type="bibr" target="#b32">[33]</ref> explore appropriate visualization responses to vagueness by interpreting singular and plural superlatives (e.g., "highest price" and "highest prices") and numerical graded adjectives (e.g., "higher") based on the shape of the data distributions. Law et al. <ref type="bibr" target="#b43">[44]</ref> investigated how the visual design of answers to why questions might influence user perceptions of a question-answering system. They found that users have a strong tendency to associate correlation with causation when systems do not provide clear explanations for the answers.</p><p>The space of analytical expression in NL is rich and much more nuanced than what these interfaces currently support. A study was conducted to assess NL input to visualization systems <ref type="bibr" target="#b61">[62]</ref> where 75 participants were asked to write NL queries based on five underlying datasets (i.e., bird strikes, world indicators, superstore, mutual funds, and Olympic medals). Of the 578 NL queries, common ones included "Are there more strikes on takeoff or landing?", "Are certain seasons more dangerous?", and "Which country has more female medalists?" The data suggests that when participants were not restricted in the format of expression, they often chose to specify utterances with an underlying intent to either explicitly or implicitly compare values. Current NL systems, however, do not explore how utterances about comparisons ought to be interpreted even though such forms of intent are prevalent.</p><p>Our paper identifies a gap in mapping how users express comparative utterances in NLIs to appropriate visualizations. To address this problem, we explore a design space connecting language and visual representation for a range of comparison utterances, varying in what is being compared and how these comparisons are specified. Finding from our work provide implications for NLIs and recommendation tools to better interpret and support comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DESIGN SPACE OF COMPARISON UTTERANCES</head><p>In visual analytics, Gleicher et al. <ref type="bibr" target="#b25">[26]</ref> define comparisons as an analytical task involving two components: a set of targets, i.e., the set of items being compared, and an action performed on the relationships among these targets, e.g., similarities and differences. We extend this definition of comparison as the task of identifying similarities and differences between two or more categories of data values based on one or more data attributes shared among those categories. For the remainder of this paper, we refer to this extended definition of comparisons.</p><p>In this work, we explore the intricacies of language constructs used to express comparisons and particularly focus on understanding the semantic variations in comparison expressions and how those variations influence the corresponding visual representations. To this end, we define a comparison utterance as a textual sentence, inputted by a user to an NLI, for example, that expresses the intent of performing a comparison with a given dataset. An example of a comparison utterance is "compare the sales for Washington and California," where "Washington" and "California" are the values, and sales is the attribute. Inspired by the language constructs from the computational linguistics literature <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b63">64]</ref>, we describe our design space for comparison expressions in terms of the cardinality and concreteness of the comparisons.</p><p>Cardinality: Comparison utterances can express relationships between individual entities, e.g., "compare the effectiveness of treatment A to treatment B", as well as relationships between individuals and larger sets containing multiple entities, e.g., "compare the effectiveness across all treatments". Similar to the process described by Xiong et al. <ref type="bibr" target="#b68">[69]</ref>, we list four cardinalities for the entities in comparison utterances in the context of visual analysis:</p><p>1-1: Compare one entity to another entity e.g., "compare the IMDB ratings of Squid Game and Midnight Mass" 1-n: Compare one entity to another set of multiple entities e.g., "compare the performance of Starling to other PG-13 movies" n: Compare multiple entities within the same set e.g., "compare the budgets across all US movies" n-m: Compare one set of entities to another set e.g., "compare crime shows to thriller shows in terms of box office" Concreteness: Comparison utterances can explicitly mention data attributes and values to be compared; for example, the comparison expression "compare the number of silver medals won by Rebecca Adlington to all other participants in the Women's Swimming Event" explicitly refers to the values to consider ("Rebecca Adlington" and "all other participants in the Women's Swimming Event") as well as the exact data attribute to use in the comparison (silver medals). Due to language variations, we account for plurality (e.g., "book" and "books"), spelling variation (e.g., "color" and "colour"), and stemming (e.g., "costly" and "cost") for explicit references.</p><p>Comparison utterances can also be underspecified when one or more data attributes or values are implicitly referenced in the comparison <ref type="bibr" target="#b59">[60]</ref>. For example, the utterance "compare the popularity of all movies in 2021" could be considered implicit if the dataset does not contain an attribute or value that explicitly matches the token "popularity." Rather, other attributes such as number of reviews or user rating may be more appropriate to consider when comparing the popularity of the movies. In addition, implicit comparison utterances can employ language constructs such as gradable vague modifiers like "low", "high", or "cheap" when comparing data, e.g., "compare athletes who won a high number of gold medals". These gradable adjectives are often mapped to an upper or lower range of data values in a data column <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b61">62]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Identifying the design scope of comparisons</head><p>Both the cardinality and concreteness of the comparison utterance can impact the desired visual characteristics to perform the corresponding comparison task. Within each cardinality, there are theoretically 16 combinations of comparisons that we can identify via a truth table by permutating whether each of the two data values and data attributes are implicit or explicit (2 × 2 × 2 × 2). However, not every combination produces a valid or natural comparison scenario. For example, the order of elements for degrees of concreteness does not fundamentally change the resulting comparison task; in other words, the comparison "compare the performance of Starling to other high rated movies" (an explicit value followed by an implicit value) is essentially the same as the comparison "compare the performance of high rated movies to Starling" (an implicit value followed by an explicit value). We can therefore reduce the space to focus on utterances that are order agnostic. Furthermore, while we initially consider a design space of two values with two corresponding attributes, comparisons that involve different attributes for each value are rather nonsensical like comparing "apples" and "oranges", e.g., "compare the gold medals obtained by Rebecca Adlington to the bronze medals obtained by Nathan Ghar-Jun Adrian". Finally, we chose to exclude comparisons with different degrees of concreteness for the values as many of these comparisons felt artificial compared to real-world comparison utterances, e.g., "compare a low budget TV show to Squid Game with respect to their popularity".</p><p>The final four combinations of concreteness in our design space are as follows: explicit data values paired with explicit data attributes (EV-EA); explicit values and implicit attributes (EV-IA); implicit values and explicit attributes (IV-EA); and implicit values paired with implicit attributes (IV-IA). Our final design space, therefore, includes a total of 16 utterances across four cardinalities (1-1, 1-n, n, n-m) and four combinations of concreteness (EV-EA, EV-IA, IV-EA, IV-IA). The final design space along with example queries for both datasets can be found in our supplemental material. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">STUDY 1: DESIGNING VISUALIZATIONS FOR COMPARISONS</head><p>We conducted user studies with visualization experts and non-experts to better understand how they visualize data for comparison utterances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Participants</head><p>We recruited 16 participants (six female, ten male). All participants were fluent in English and self-reported being comfortable designing visualizations. Eight participants (four female, four male) self-reported as data visualization experts with an average of ∼11 years of visual analytics experience and were recruited from a visual analytics company. The data visualization experts had a variety of job backgrounds including three data visualization consultants, two user experience designers, one business strategist, one visualization lead expert, and one solution engineer. The other eight (two female, six male) self-reported as data visualization non-experts and were recruited from an academic institution. All of the non-expert participants had a computer science or engineering background, with six being graduate students and two being undergraduates. Participants were recruited via a company mailing list, Slack, and using snowball sampling <ref type="bibr" target="#b24">[25]</ref>. The students were offered to enter a $30 gift card raffle as an incentive to participate, while the participants recruited from the visual analytics company were not offered any financial incentive due to company policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Procedure</head><p>Each study session lasted 60 minutes and was conducted remotely via Zoom. Two researchers supported each session: one facilitator and one note-taker. All sessions were video recorded. Field notes were expanded to a video log after the study through partial transcription of the videos. The video log (and raw video for reference) was then qualitatively coded for high-level themes.</p><p>Participants recruited from the academic institution were asked to fill out a consent form to indicate their interest in receiving a gift card. All participants were asked for their consent for the session to be recorded, and participants were then asked to share their screens in order to make the session more interactive between the participant and the facilitator.</p><p>During each session, participants were first introduced to the study and asked about their visualization background and current role. Participants were then given instructions for the study and were asked to draw one or more visualizations for four comparison queries. The main study prompt was as follows: "Imagine that you are a visualization recommendation tool that generates visualization(s) when prompted by a user. Assume that the user imported a [Netflix or Olympics] dataset, and inputted some utterances describing what they want to see from the data. Your task is to generate a visualization or multiple visualizations (if you think that it can be visualized in multiple ways) as a response to the utterance and present it back to the user. Please sketch your visualization response that you think will best respond to their request."</p><p>Participants were first provided a weblink to a Google Sheet containing the dataset along with a metadata summary sheet. They then read four comparison utterances one at a time and used Google Jamboard <ref type="bibr" target="#b4">[5]</ref> to sketch out the visualizations. Participants were encouraged to think aloud during the study, and we employed a question-asking protocol to elicit qualitative feedback. Responses to common questions were documented ahead of time so that the facilitator could provide consistent responses to every participant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Design</head><p>This study employed a 4 × 4 Graeco-Latin square design that covered four different cardinalities (1-1, 1-n, n, and n-m) and two levels of concreteness (I: implicit and E: explicit) for the data values (V) and data attributes (A). Each participant came across all four cardinalities and 1 Additional details regarding the experimental protocol, datasets, and all the comparison utterances can be found in supplemental material. all four combinations of concreteness (EV-EA, EV-IA, IV-EA, IV-IA); the 4 × 4 Graeco-Latin square design balances the order in which the participant comes across the utterances. The conditions are shown in Figure <ref type="figure" target="#fig_0">3</ref> and described as follows:</p><formula xml:id="formula_0">• Condition 1: 1-1 EV-EA, 1-n EV-IA, n IV-EA, n-m IV-IA • Condition 2: 1-n IV-IA, 1-1 IV-EA, n-m EV-IA, n EV-EA • Condition 3: n EV-IA, n-m EV-EA, 1-1 IV-IA, 1-n IV-EA • Condition 4: n-m IV-EA, n IV-IA, 1-n EV-EA, 1-1 EV-IA</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experimental Dataset</head><p>In each condition, participants interacted with one of two datasets: a Netflix dataset <ref type="bibr" target="#b10">[11]</ref> and an Olympics medals dataset <ref type="bibr" target="#b52">[53]</ref>. The Netflix dataset contains 13 attributes including seven categorical (e.g., Title, Genre), six numeric (e.g., IMDB Rating, Box office), and one temporal (Release year). The Olympics dataset also contains 13 attributes composed of six categorical (e.g., City, Event), six numeric (e.g., Height, Gold), and one temporal attribute (Year). We chose these datasets as they contain a combination of continuous and categorical variables that can support a variety of comparison utterances. The datasets were also familiar to a wide set of participants and hence could be easily interpreted. We used a subset of 60-70 rows for each of the two datasets to prevent participants from becoming overwhelmed with the amount of data when performing the tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Pilot Studies</head><p>We conducted nine pilot studies to test the study protocol and stimuli. Given the remote experimental setup using a combination of online tools, the goal of the pilots was to streamline the user study process and resolve any points of confusion, particularly around the instructions and utterances. We conducted, transcribed, and analyzed ∼540 minutes of pilot interviews before conducting the final user study.</p><p>Based on feedback from the pilot participants, we updated the instructions for the study and refined the utterances and datasets. To better orient the participant about the dataset and its attributes and values, we included a metadata summary. We also incorporated a short tutorial in the study protocol to familiarize participants with the various drawing options in Jamboard and data manipulation features in Google Sheets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Study 1: Qualitative Analysis</head><p>In total, we gathered ∼960 minutes of video and audio recordings, transcribed them using Amazon AWS Transcribe <ref type="bibr" target="#b40">[41]</ref>, and then placed each unique statement into our analysis software, a shared spreadsheet. We extracted the following details from the recordings:</p><p>Visualization design: We extracted details about the visualization type (e.g., bar chart, scatter plot) and encoding (e.g., color, size), which included details about the mapping between data values and visual properties. We also considered high-level properties of the visualization layout such as the orientation (e.g., vertical or horizontal), arrangement <ref type="bibr" target="#b68">[69]</ref> (e.g., adjacent, overlaid), and whether or not the visualization used a small multiples design <ref type="bibr" target="#b7">[8]</ref>.</p><p>Visualization details: We also extracted additional narrative characteristics of the visualization such as the use of highlighting to emphasize certain features and the use of annotations (e.g., labels, reference lines, legends). We also recorded whether or not the visualization was sorted.</p><p>Visualization or data transformations: An important characteristic of this study was how participants interpreted implicit data values and attributes; if participants identified multiple interpretations, we also recorded which of them the participant preferred. As part of the design process, we recorded how the data was filtered before drawing; for instance, for the utterance "compare the performance of Starling with other PG-13 movies," some participants chose only the "top 3" movies to compare with Starling, whereas others chose 5, and so on. Finally, we extracted the desired interactive features of the visualization described by participants, such as the inclusion of a tooltip or data label on hover, or other ways in which the visualization could change on demand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participant details &amp; preferences:</head><p>In addition to demographic characteristics like the participant's occupation, and expertise, we extracted the number of visualizations created during the study and in cases where multiple visualizations were drawn for an utterance, we also extracted which visualizations the participant preferred and why.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.1">Interpreting ambiguity of implicit values and attributes</head><p>One of the main goals of this study was to understand how participants interpret the concreteness of the data values and data attributes. Participants came across multiple implicit data values such as "high rated movie", "high budget movie", "tall athlete", "successful wrestler", and "high achieving", as well as implicit data attributes such as "performance," "popularity," "physique," "achievements," and "long runtime". Additional details regarding the datasets and their metadata summaries can be found in the supplementary material. While a few of the participants interpreted the implicit data values or attributes in only a single context from the list given above, most of them interpreted them in multiple contexts. In the case of underspecified values, we notice a general trend of participants interpreting values as being above or below average. In the case of vague modifiers, participants usually picked all the attributes from the dataset related to the implicit value or attribute and mapped all of them to it. For example, P9 explained their approach to resolving the ambiguity as follows: "performance is kind of a vague thing. So let's just assume number of medals. That would be the best thing to assume. So performance is how many accolades they got.". Participants also tended to use some form of a weighted average, algorithmic, or parametric approach to interpret the implicit terms; for example, P4 interpreted "high rated" as "I would statistically identify the high ratings more of like in a statistical or algorithmic way."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.2">Developing clear charts for the intended comparison task</head><p>Some participants found the decision process for developing the visualization designs particularly difficult when trying to handle aggregation or other forms of data transformation. Reflecting on their role as "a visualization recommendation tool," one participant explained that "I would prefer for a recommendation engine to say don't look at the mean, look at the median. <ref type="bibr">But</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>if it's a normal distribution, I'd prefer it to say, let's look at the mean... you can trust us and that we are automatically providing the best results." [P4].</head><p>A few participants suggested leveraging appropriate design decisions, such as highlighting certain parts of the visualizations or adding data transformations, to make the comparison easier for a viewer to comprehend. P9 shared insights on how design decisions can support comparison tasks: "So even something as simple as like a dot on the left to draw your eye to Rebecca because she is central to the question... or I would do the same chart, but put her at the very top, so she's floated to the top and then beneath her we're still in descending order.".</p><p>Another participant argued that the initial visualizations should be rather simple when describing their 2-bar bar chart: "I don't want to start with more than they asked. The two bars next to each other, I feel like they're good if there are not too many things that you're comparing. So if it's just two movies, I feel like this works really well." [P4].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Study 1: Quantitative Analysis</head><p>We analyzed the time spent designing visualizations based on the four cardinalities and level of concreteness, as well as the number of visualizations that the participants sketched. Additional details can be found in the supplementary materials.</p><p>Time Spent: Two-way ANOVAs comparing the time people spent for each of the 16 queries from both datasets showed no difference between cardinalities, concreteness, nor datasets (p &gt; 0.05). On average, participants spent 9.75 min (SD = 0.52) per query. sketched for each query in both datasets suggests that there is no significant effect of concreteness, cardinality, or their interaction. On average, participants drew 2.25 visualizations (SD = 0.15) per query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Visualizations: Two-way ANOVA comparing the number of visualizations (including the scratched out ones) that participants</head><p>We also recorded the number of visualizations with interactive features and annotations for each dimension (Table <ref type="table" target="#tab_2">1</ref>). Based on the four cardinalities, we see that cardinality n had the highest percentage of visualizations with interactive features, followed by cardinality 1-n. In terms of concreteness, IV-EA had the highest percentage of visualizations with interactive features, closely followed by IV-IA. When P11 was asked why they added interactivity to their sketch, they commented, "I don't know anyone who would really want to look at gold and silver, but I would look at gold medals versus all medals seems like a reasonable question... being able to do things like add events at years, filter by medals, they seem to be the adjacent questions to me in this."</p><p>Regarding the percentage of annotated visualizations, n had the highest, followed by n-m and 1-n. In terms of concreteness, IV-EA had the highest percentage of annotated visualizations, followed by EV-IA, and then IV-IA. Many participants emphasized the importance of annotations and highlighting to make the comparisons clearer. P2 explained that "you could highlight a specific show... so it stands out and then all the other ones would be a different color." This strategy was common across participant sketches and informed the design of the representative visualizations in Study 2 (Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Identifying Common Visualizations per Cardinality</head><p>Based on the participants' sketches, we created illustrations of the visualizations using Notability software <ref type="bibr" target="#b1">[2]</ref>; a sample set shown in Figure <ref type="figure" target="#fig_2">4</ref>. We clustered the visualizations based on the four cardinalities. Two authors went through the visual clusters to find commonalities between the charts. We initially thought that the top three visualizations from each cardinality would be sufficient to represent the common visual charts. However, after some extensive examination of the visual clusters, the top four visualizations for each cardinality were chosen.</p><p>The top four visualizations were chosen based on the occurrence of the chart type, arrangement, and orientation of the chart. We then looked at whether and how these charts were annotated in order to include those annotations in our generalized drawings. We also looked in detail into how the implicit values and attributes were interpreted by the participants. We commonly noticed the introduction of interaction, grouped/small multiples/stacked, just mapping of one attribute or value from the dataset in visualizations that were sketched for levels of concreteness involving implicitness. As the focus of this paper is not to look into interactions in visualizations, we decided to map multiple or single attribute(s)/value(s) in the levels of concreteness that involve ambiguity. In the following paragraphs, we describe in detail the commonalities in the sketches by the participants and the process by which 16 visualizations were extracted, i.e., the top four for each cardinality.</p><p>Cardinality 1-1: From a total of 30 visualizations, there were 15 bar charts, 8 unit charts, 2 scatter plots, 2 pie charts, 1 line chart, and 1 box plot. 10 of the charts had a vertical orientation and 7 had a horizontal orientation. For the bar charts, seven were grouped bar charts and the rest were simple two-bar bar charts. We noticed a trend of both the values being in a different color. The simple bar charts were annotated with the value of the attribute on top of the bar. There were three adjacent labeled unit charts where the data values were mentioned in a bigger font below the circles, and there were four other unit charts. There was a trend of values included in the charts as legends or labels. For 1-1, we created (A) a simple bar chart, (B) an adjacent unit chart, (C) a horizontal unit chart, and (D) a grouped bar chart.</p><p>Cardinality 1-n: From a total of 35 visualizations, there were 21 bar charts, 5 scatterplots, 4 dot plots, 2 unit charts, 2 line charts, and 1 pictograph. 12 of the charts had a horizontal orientation. For the bar charts, 2 were grouped bar charts and 2 stacked. Generally, there were multi-bar bar charts and two-bar bars charts where one bar depicts a value and the other depicts the average value of others. We also noticed a general trend of coloring the value mentioned in the query in a different color as compared to others. The simple bar charts were annotated with the value of the attribute on top of the bar. There was another trend of data values included in the charts as legends or labels. For 1-n, we created (E) a horizontal multi-bar chart, (F) a horizontal simple bar chart, (G) a dot plot, and (H) a scatterplot.</p><p>Cardinality n: From a total of 31 visualizations, there were 17 bar charts, 5 scatterplots, 3 box plots, 1 line chart, 1 parallel coordinates chart, and 1 sentiment chart. 10 of the charts had a horizontal orientation and 9 had a horizontal orientation. For bar charts, there were four small multiples, five stacked, and two grouped. We also noticed a general trend of all the values being in the same color representing a single attribute in the case of bar charts. All the values were shown in different colors or depicted in a legend in the case of scatter plots. For n, we created (I) a horizontal multi-bar chart, (J) a small multiple bar chart, (K) a scatterplot, and (L) a box-plot.</p><p>Cardinality n-m: From a total of 41 visualizations, there were 21 bar charts, 5 scatterplots, 6 dot plots, 2 box plots, 2 unit charts, 4 line charts, and 1 pie chart. 15 of the charts had a vertical orientation and 10 had a horizontal orientation. For bar charts, six were grouped bar charts, five small multiples, and two stacked. We also noticed a general trend of all the values of n and all the values of m in a different color. All the values were in a different color in the case of scatterplots and grouped bar charts. For n-m, we created (M) a grouped bar chart, (N) a small multiples bar chart, (O) a simple vertical bar chart, and (P) a scatterplot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">STUDY 2: IDENTIFYING VISUALIZATION PREFERENCES</head><p>We conducted an online crowdsourced experiment to cross-validate user preferences for the 16 representative visualizations identified in Section 4.8 for our design space. We focus on non-expert user preferences because the designs from visualization experts are often meant for communicating key patterns to naive viewers to help them make comparisons. The results of this study can inform the design choices for visualization recommendation systems focused on comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Participants</head><p>Based on a pilot study with 10 participants using only 1-1 queries, we conducted a power analysis to determine the number of participants required for the experiment to find an overall difference in preference rankings. With a medium effect size of ∼0.49, our analysis suggests that a target sample of 76 would yield 95% power to detect an overall difference between preference rankings for the four visualizations at an alpha level of 0.05. We recruited 79 participants via Prolific.com <ref type="bibr" target="#b48">[49]</ref> to complete an online survey through Qualtrics <ref type="bibr" target="#b50">[51]</ref>. They were compensated at 10.55 USD per hour. In order to participate in our study, the workers had to be based in the United States and fluent in English. After excluding participants who failed attention checks (e.g., failing to select a specific answer in a multiple-choice question) or entered illegible/nonsensical response, we ended up with 77 participants, with 58 that identified as women (M age = 39.34, SD age = 16.18), 18 as men (M age = 40.94, SD age = 13.90), and one chose to not disclose.</p><p>The participants completed a subjective graph literacy report <ref type="bibr" target="#b22">[23]</ref> and reported an average value of 3.87 out of 6 (SD = 0.82, 1 = low self-reported literacy, 6 = high self-reported literacy), suggesting that most participants were comfortable with visualizations but did not identify as visualization experts. Only 5 people reported that they create visualizations often for work or as a hobby, and 17 people reported that they rarely interact with visualizations in their day-to-day life.</p><p>We asked our participants how much effort they put in completing our study; they were told that the answer to this question would not affect their compensation and were encouraged to answer honestly. 56 participants reported having put in a lot of effort into our study, carefully thinking through their answers before responding. 21 participants reported having put in some effort, having answered the questions without thinking too deeply about anything.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Stimulus and Design</head><p>Similar to Study 1, we considered four cardinalities and four levels of concreteness, resulting in 16 comparison queries. We used the Amazon Books dataset <ref type="bibr" target="#b54">[55]</ref> and generated queries similar to those from Study 1. The dataset had seven attributes including three categorical (e.g., Book Title, Author), three numeric (e.g., User rating, Reviews), and one temporal attribute (Year). We created visualizations following the representative designs extracted from Study 1 (Section 4.8). We varied the data attributes and values used to generate the visualization to match the content of the queries for each cardinality. For the 16 queries, we made 64 visualizations using Vega-Lite <ref type="bibr" target="#b56">[57]</ref>; 16 visualizations for each cardinality that included four visualizations for each level of concreteness (query). We briefly describe how the visualizations looked for each level of concreteness in Study 2. The full set of comparison queries and example visualizations can be found in Figure <ref type="figure" target="#fig_3">5</ref>. <ref type="figure">and n-m (M,  N, O</ref>) the visualizations were straightforward with explicit data values compared to an explicit data attribute (User rating) in case of 1-1 and the data attribute (Price) in case of 1-n, n, and n-m.</p><formula xml:id="formula_1">EV-EA: For 1-1 (charts A, B, C), 1-n (E, F, G), n (I, L),</formula><p>EV-IA: For 1-1 (charts A, B, and C), <ref type="figure" target="#fig_4">1-n (E, F, and G), n (I, and L), and  n-m (M, N, and O</ref>) the visualizations were straightforward with explicit data values compared to an implicit data attribute ("Popularity"), which is a vague modifier and is mapped to the attribute (Reviews).</p><p>IV-EA: For 1-1 (charts A, B, and C), 1-n (E, F, and G), n (I, and L), and n-m (M, N, and O) the visualizations were straightforward with implicit data values compared to an explicit data attribute (User rating) in case of 1-1 and the attribute (Price) in case of 1-n, n, and n-m. The titles of implicit values such as "a bestseller book in 2012" in case of 1-1, rated books" in case of 1-n and n, and "high rated fiction books" in case of n-m are shown in the charts as labels or legends.</p><p>IV-IA: For 1-1 (charts A, B, and C), 1-n (E, F, and G), n (I, and L), and n-m (M, N, and O) the visualizations show implicit data values compared to an implicit data attribute ("Popularity") which is mapped to the attribute (Reviews). The titles of implicit values such as "a cheap book" in case of 1-1 and 1-n, "expensive books" in case of n, and "high rated non fiction books" in case of n-m are shown in the charts as labels or legends.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Procedure</head><p>Participants were given a link to our survey. After consenting to participate, participants were given a metadata sheet describing the variables in the Amazon Books dataset. Participants had access to this metadata sheet throughout the experiment. After a brief introduction to the survey, participants viewed all sixteen comparison queries in random order on separate pages. For each comparison query, participants were given the four corresponding visualizations based on the cardinality of the query and were instructed to rank them in terms of how well they enable a viewer to make the intended comparison. Participants were told to assume that the viewer was not familiar with the dataset and there were no correct answers. At the end of the survey, participants reported demographic information, completed the self-report visual literacy test, and reported the amount of effort they put in completing the study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results for Each Comparison Utterance</head><p>We conducted a Friedman Rank Sum test for each comparison query using the PMCMRplus R package <ref type="bibr" target="#b49">[50]</ref> to compare preference rankings for the four visualizations, with post-hoc pair-wise comparisons via Conover's test with Bonferroni's correction to determine the specific ranking differences (Table with complete analysis can be found in the supplementary material). Figure <ref type="figure" target="#fig_3">5</ref> shows the summary results.</p><p>Cardinality 1-1: Accounting for all combinations of concreteness, participants generally preferred (A) simple bar charts or (B) adjacent unit charts to help them make 1-1 comparisons. They preferred (C) horizontal unit charts and (D) grouped bar charts significantly less. There is a significant difference in user preferences for the EV-EA query (Friedman χ 2 = 57.28, p &lt; 0.001), the EV-IA query (Friedman χ 2 = 21.85, p &lt; 0.001), the IV-EA query (Friedman χ 2 = 80.97, p &lt; 0.001), as well as the IV-IA query (Friedman χ 2 = 18.29, p &lt; 0.001).</p><p>Cardinality 1-n: Accounting for all combinations of implicit and explicit data variables and data attributes, participants generally preferred (E) multi bar charts to help them make 1-n comparisons. Participants preferred (G) dot plots and (H) scatterplots significantly less. There is a significant difference in user preferences for the EV-EA query (Friedman χ 2 = 71.42, p &lt; 0.001), the EV-IA query (Friedman χ 2 = 60.45, p &lt; 0.001), the IV-EA query (Friedman χ 2 = 91.51, p &lt; 0.001), as well as the IV-IA query (Friedman χ 2 = 88.51, p &lt; 0.001).</p><p>Cardinality n: Accounting for all combinations of implicit and explicit data variables and data attributes, participants generally preferred (I) multi bar charts to help them make n comparisons, similar to the 1-n cardinality. Participants preferred (L) box plots the least, with (J) small multiples bar charts and (K) scatterplots in the middle of the pack. There is a significant difference in user preferences for the EV-EA query (Friedman χ 2 = 123.34, p &lt; 0.001), the EV-IA query (Friedman χ 2 = 123.34, p &lt; 0.001), as well as both the IV-EA and IV-IA queries (Friedman χ 2 = 113.83, p &lt; 0.001 for both queries).</p><p>Cardinality n-m: Accounting for all combinations of concreteness, participants equally preferred (M) grouped bar charts, (N) small multiples bar charts, and (O) simple bar charts to help them make n-m comparisons. Participants generally preferred (P) scatterplots the least. There is a significant difference in user preferences for the EV-EA query (Friedman χ 2 = 58.24, p &lt; 0.001), the EV-IA query (Friedman χ 2 = 69.36, p &lt; 0.001), the IV-EA query (Friedman χ 2 = 76.78, p &lt; 0.001), as well as the IV-IA query (Friedman χ 2 = 92.38, p &lt; 0.001).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Summary Findings</head><p>Results from Study 2 provide insights into which visualization arrangement may be preferred for a given type of comparison utterance from our design space. Overall, we found that the preference ranking of charts within the four cardinalities (i.e., 1-1, 1-n, n, and n-m) was consistent, even taking into account the combinations of concreteness (Figure <ref type="figure" target="#fig_3">5</ref>). We use the notation [W #] when referring to participants in these studies. We now discuss specific preferences and participant feedback for the various cardinalities of comparison utterances. Several participants expressed difficulty interpreting the scatterplot for a comparison task; for example, W 63 noted that "Scatterplots are difficult to read. The first option is cleanest and easiest to read to compare the cost of books by the author." and W 25 explained in detail that "My first choice showed at a quick glance the exact comparison between data sets -and number two was relatively easy as well as three. But number four, again... those dots!"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>To summarize, the findings from both studies are consistent with that from prior work, where bar charts are generally conducive to visual comparisons <ref type="bibr" target="#b68">[69]</ref>. However, we found interesting insights regarding the type of bar charts that were preferred. Feedback from participants showed that bars were easily comparable when visually aligned and spatially proximate. In terms of language, we observed a trend towards requiring interactivity and annotations where implicit values/attributes were involved. Insightful trends were found regarding the general interpretation of vague modifiers and underspecified implicit values by the participants. Observations from these studies provide four key design implications for recommendation tools and NLIs to support comparisons during visual analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Design Implications</head><p>Basic charts are reasonable responses for a variety of comparisons. The preference ranking consistently showed that bar charts are preferred for their simplicity in representing comparisons. Our findings suggest that for 1-1, vertical bar charts and unit charts were preferred. For 1-n, preference was towards horizontal bar charts. Horizontal and small multiple bar charts were popular for n; whereas for n-m, grouped, small multiple or simple bar charts were preferred. Many tools are already capable of creating basic charts such as sorted horizontal and vertical bar charts with support for highlighting a subset of bars for easier comparison. By extending the language parser in these tools to recognize comparison expressions, they can support a repertoire of comparison types without the need to generate bespoke chart types.</p><p>Include necessary information useful for the comparison task. Viewers ranked charts that contained only information relevant to the comparison higher than those that had extra or unnecessary information, an observation synergistic with previous NLI research for visual analysis tasks <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b60">61]</ref>. When generating visualization responses, tools should ensure that they contain only information that is indicated in the comparison utterances. Participants explained their rationale for ranking charts with superfluous information lower than others: "The bottom chart [small multiples bar chart] contains extraneous information that is not needed for the comparison sentence (price)." [W 6]. Similarly, for comparisons involving specific values, viewers preferred charts showing unaggregated data: "Preferred graph gives comparative information. All of the others compare one book with an aggregate." <ref type="bibr">[W 31</ref>].</p><p>Indicate how implicit entities are interpreted. Emulating previous NL systems <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60]</ref>, we map vague modifiers to more concrete representations. Viewers appreciate the importance of exposing the provenance of how implicit entities such as "cheap" and "best-selling" are mapped to specific data attributes in the visualization. W 72 remarked that "The first option [small multiples bar chart] allowed readers to gauge both price and review at the same time, while the other graphs did not show them at all. 'Expensive' books are subjective". In general, for vague modifiers, participants usually picked all the attributes from the dataset that somewhat matched the implicit value or attribute and mapped all of them to it (e.g., 'performance' as a combination of boxoffice revenue and IMDB ratings). Underspecified values were often interpreted as being above or below average. Support user interaction and text with the visualizations. We found that users preferred interactivity (e.g., filter controls, changing the attribute, or modifying the data values), particularly for comparison utterances concerning implicit concepts. In addition, users preferred the inclusion of text describing how these implicit concepts were interpreted in the visualization responses. As shown in Table <ref type="table" target="#tab_2">1</ref>, IV-EA had the highest percentage of visualizations with interactive features, closely followed by IV-IA. IV-EA also had the highest percentage of visualizations with annotations, closely followed by EV-IA and IV-IA. In terms of cardinality, n had the highest percentage of visualizations with interactive features, followed by 1-n. n also had the highest number of visualizations with annotations, followed by n-m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Limitations and Future Directions</head><p>Our work specifically explores the interplay between language and visual representations for comparisons. However, we discuss some limitations of our work and identify promising future directions in this important area of research.</p><p>Explore additional design considerations for comparisons. The visualizations used in Study 2 varied across several design dimensions (e.g., arrangement, encodings, chart type, annotation), unlike prior studies <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b70">71]</ref>), where one design element (such as visual arrangement) was isolated and tested comprehensively. We considered the current approach because past work has shed light on how vast and complex the experimental space can be. To avoid a combinatorial explosion of experimental conditions, we hence adopted a design-and-then-validate approach to cover a wide range of visualization designs for multiple types of comparison utterances. The approach enabled us to produce immediately actionable guidelines for visualization recommender and NL systems. Future work should examine how interactivity, such as brushing, linking, dynamic filtering and scaling, and annotation techniques might facilitate different types of comparisons.</p><p>Explore the role of ethics for interpreting comparison utterances. With the impreciseness of language, we acknowledge that people may interpret implicit attributes and vague concepts in unique, subjective ways. There is a need to further explore the ethical issues surrounding how tools implementing comparison utterances can be transparent about their underlying assumptions and decision-making processes. Future work should specifically evaluate the ethical considerations for NLIs and recommendation systems <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b73">74]</ref> for responsibly supporting analytical inquiry for comparisons.</p><p>Leverage additional metrics beyond user preferences. The visualization responses for comparison utterances were informed by a series of interviews that were subsequently validated by a user preference study. Future research should evaluate the effectiveness of these preferred visualizations by examining the speed and accuracy with which users can make the intended comparisons, or test the types of comparisons they afford via qualitative responses. Other research directions should explore how NLIs can take into account implicit feedback by users through refinement and repair of system choices for visual comparisons as well as query reformulations <ref type="bibr" target="#b61">[62]</ref>. In addition, telemetry data collected from production systems and deployed visual analysis tools can be used as a training set for machine learning models to learn and improve user expectations over time. Other metrics involve explicit feedback through surveys and a like/dislike functionality built into NLIs could provide additional insight into the utility of the visualization response in supporting a comparison intent.</p><p>Extend comparisons to support exploratory data analysis. Our paper focuses on a question-answering form of interaction for supporting comparison intents. However, future research should explore how users express comparisons to support exploratory data analysis, focusing specifically on language pragmatics for follow-up inquiry during an analytical conversation. Such insights can also be useful to help inform the design of recommender tools for suggesting comparison utterances and views as part of a larger analytical workflow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>Visual comparisons are an important form of analytical workflows as people reason about and make sense of data. With the growing popularity of NLIs and recommendation systems, interpreting and visualizing the semantic nuances of comparison utterances can be challenging. In this paper, we explore a preliminary design space of NL comparison utterances covering four cardinalities of comparisons and addressing varying degrees of concreteness in these comparison intents. Through a series of interviews with 16 visualization novices and experts, we create a potential mapping between comparison utterances in our design space and different visualization techniques. By empirically validating this mapping in a preference ranking study with 77 participants, we provide design implications for interpreting the comparison language for a range of scenarios. We hope that our findings are a step towards addressing the complex interplay between language and visualization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. A Graeco-Latin square with example utterances from the Netflix dataset and four participants came across each condition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Netflix Dataset:</head><label></label><figDesc>The following list describes the implicit data values and data attributes, their implicit type (i.e., vague modifier or underspecification), and examples of how they were commonly interpreted by participants for the Netflix dataset: • performance (vague modifier): [Watched] OR [IMDB rating] OR [Rotten tomatoes rating] OR ROI/Profit [Box office − Budget] OR [Box office] • popularity (vague modifier): [Box office − Budget] OR [Watched] • high rated (underspecified): [Rotten tomatoes rating] &gt; 80/100 OR [Rotten tomatoes rating] &gt; AV G(Rotten tomatoes rating) OR [IMDB rating] &gt; AV G(IMDB rating) OR [IMDB rating] &gt; 8/10 • high budget (underspecified): [Budget] &gt; AV G(Budget) OR [Budget] &gt; 95 th percentile • low budget (underspecified): [Budget] &lt; AV G(Budget) OR [Budget] &lt;5 th percentile OR lowest budget value • long runtime (underspecified): [Duration] &gt; 100 minutes OR [Duration] &gt; 80 th percentile Olympics Dataset: The following list describes some of the implicit values and attributes, their implicit type, and examples of how they were interpreted for the Olympics dataset: • achievements (vague modifier): SUM(Gold, Silver, Bronze) • performance (vague modifier): SUM(Gold, Silver, Bronze) OR weighted metric of [Gold, Silver, Bronze] • physique (vague modifier): [Height] OR [Weight] OR metric of [Weight, Age, Height] • tall athlete (underspecified): [Height] &gt; AV G(Height) OR [Height] &gt; MEDIAN(Height) OR [Height] ≥ 180cm • short athlete (underspecified): [Height] &lt; AV G(Height) OR [Height] &lt; MEDIAN(Height) OR [Height] &lt; 180cm • strong physique (underspecified): [Weight] &gt; AV G(Weight) OR [Weight] &gt; weighted sum of [Weight, Height, Age] • successful player (underspecified): SUM( Gold, Silver, Bronze ) &gt; 0 • high achieving (underspecified): [Gold] &gt; 0 • young athlete (underspecified): [Age] &lt; MEDIAN(Age) OR [Age] &lt; 20 • top-winning (underspecified): [Gold] &gt; 0 OR weighted sum of [Gold, Silver, Bronze] &gt; 0 • senior athlete (underspecified): [Age] &gt; AV G(Age)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Sketches elicited from Study 1. Charts are recreated for enhanced clarity based on the original sketches and interview script.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Study 2 preference rankings for visualizations extracted based on Study 1 (Section 4.8). Each row represents a query with varying cardinalities and levels of concreteness (explicit or implicit). The boxed number represents the average preference ranks assigned to each visualization, based on a Friedman Rank Sum test. Connecting lines represent significant differences based on post-hoc comparisons. Three asterisks (***) indicate that the comparison p-value &lt; 0.001. Two asterisks (**) represent p &lt; 0.01. One asterisk (*) represents p &lt; 0.05. One dot (.) represents p &lt; 0.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1- 1</head><label>1</label><figDesc>Comparisons: Viewers preferred the (A) simple bar charts without the need for unit charts or grouped bar charts: "The bar graphs are easier to compare than the bubbles." [W 18] and "The simple bar plot was easiest to understand and provided a perfect conveyance of the data." [W 33]. Other viewers had trouble interpreting unit charts; for example, "I have no idea what [the unit chart] is trying to say." [W 12]. 1-n Comparisons: Viewers preferred (E) multi-bar charts sorted in descending order with the singleton value in the comparison utterance highlighted for easier relative judgment tasks. As a second option, people indicated a preference for (F) a simple horizontal bar chart comparing the singleton value to an aggregated bar of other entities: "The first one [multi-bar chart] is the best visual as it gives the most accurate ranking system, while the other one [simple horizontal bar chart] only shows averages, which could cause misrepresentation as it lumps all the books." [W 68]. (G) Dot plots and (H) scatterplots were both equally ranked lower in the rankings. W 65 summarized that "The bar chart is easy to compare due to The Alchemist having a brighter color that stands out on the chart. The scatterplot is somewhat confusing due to the cluster of circles bunched together." n Comparisons: The top two user preferences were (I) a multi-bar chart and (J) small multiples of simple bar charts showing the entities sorted by a common property such as the price. Participants ranked the (K) scatterplot lower than the bar charts with the (L) box plot ranked last. W 5 commented that "The first two graphs [bar graphs] are just as easy to understand and the third graph [scatterplot] is a little less easy to understand and the last one [box plot] is objectively pretty terrible at illustrating the popularity across all of JK Rowling's books." n-m Comparisons: There was a tie in preferences among the three types of bar charts: (M) grouped, (N) small multiples, and (O) a simple bar chart. (P) Scatterplots were consistently ranked last. Viewers liked to see the breakdown of the entities being compared rather than viewing aggregated data: "The first two choices breakdown the price by book, while the last two choices don't have that breakdown." [W 2].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>• Vidya Setlur and Arjun Srinivasan are with Tableau Research. E-mail:</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>The percentage of visualizations that participants described as interactive or annotated for each cardinality and concreteness.</figDesc><table><row><cell></cell><cell></cell><cell cols="5">Percent of visualizations with interaction</cell></row><row><cell></cell><cell cols="2">Cardinality</cell><cell></cell><cell></cell><cell cols="2">Concreteness</cell></row><row><cell>1-1</cell><cell>1-n</cell><cell>n</cell><cell cols="2">n-m EV-EA</cell><cell>EV-IA</cell><cell>IV-EA</cell><cell>IV-IA</cell></row><row><cell cols="4">23.3 25.7 45.2 24.4</cell><cell>13.2</cell><cell>28.6</cell><cell>37.0</cell><cell>36.1</cell></row><row><cell></cell><cell></cell><cell cols="5">Percent of visualizations with annotation</cell></row><row><cell></cell><cell cols="2">Cardinality</cell><cell></cell><cell></cell><cell cols="2">Concreteness</cell></row><row><cell>1-1</cell><cell>1-n</cell><cell>n</cell><cell cols="2">n-m EV-EA</cell><cell>EV-IA</cell><cell>IV-EA</cell><cell>IV-IA</cell></row><row><cell cols="4">36.7 42.9 45.2 43.9</cell><cell>34.2</cell><cell>40.5</cell><cell>51.9</cell><cell>38.9</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors wish to thank Shubham Mishra and Nisarga Patil from UMass Amherst for helping with the coding process, the study participants for their feedback, and the reviewers for their thoughtful comments and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Q</forename><surname>Microsoft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<ptr target="https://powerbi.microsoft.com/en-us/documentation/powerbi-service-q-and-a/" />
		<imprint>
			<biblScope unit="page" from="2022" to="2028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><surname>Notability</surname></persName>
		</author>
		<ptr target="https://notability.com/" />
		<imprint>
			<biblScope unit="page" from="2022" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Tableau&apos;s Ask Data</title>
		<ptr target="https://www.tableau.com/products/new-features/ask-data" />
		<imprint>
			<biblScope unit="page" from="2022" to="2028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<ptr target="http://www.thoughtspot.com/" />
		<title level="m">ThoughtSpot</title>
				<imprint>
			<biblScope unit="page" from="2022" to="2028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Google</forename><surname>Jamboard</surname></persName>
		</author>
		<ptr target="https://support.google.com/jamboard" />
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2022" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Analytics</forename><surname>Ibm Watson</surname></persName>
		</author>
		<ptr target="http://www.ibm.com/analytics/watson-analytics/" />
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2022" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weighted graph comparison techniques for brain connectivity analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Alper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Henry Riche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Isenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Fekete</surname></persName>
		</author>
		<idno type="DOI">10.1145/2470654.2470724</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI &apos;13</title>
				<meeting>the SIGCHI Conference on Human Factors in Computing Systems, CHI &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="483" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Animation, small multiples, and the effect of mental map preservation in dynamic graphs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Archambault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Purchase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pinaud</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2010.78</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="539" to="552" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The syntax of comparative constructions : operators, ellipsis phenomena and functional left peripheries</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bacskai-Atkari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semantic framework for comparison structures in natural language</title>
		<author>
			<persName><forename type="first">O</forename><surname>Bakhshandeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Allen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1115</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-09">Sept. 2015</date>
			<biblScope unit="page" from="993" to="1002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Bansal</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/shivamb/netflix-shows" />
		<title level="m">Netflix movies and tv shows</title>
				<imprint>
			<date type="published" when="2021-09">Sep 2021</date>
			<biblScope unit="page" from="2022" to="2028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The semantics of gradation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bierwisch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">How to evaluate data visualizations across different levels of understanding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Franconeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cairo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mahyar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Workshop on Evaluation and Beyond-Methodological Approaches to Visualization (BELIV)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="19" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On the process of comparing sentences against pictures</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">G</forename><surname>Chase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="472" to="517" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ethical dimensions of visualization research</title>
		<author>
			<persName><forename type="first">M</forename><surname>Correll</surname></persName>
		</author>
		<idno type="DOI">10.1145/3290605.3300418</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, CHI &apos;19</title>
				<meeting>the 2019 CHI Conference on Human Factors in Computing Systems, CHI &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The semantics of degree</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cresswell</surname></persName>
		</author>
		<idno type="DOI">10.1016/B978-0-12-545850-4.50015-7</idno>
		<editor>B. H. PARTEE, ed., Montague Grammar</editor>
		<imprint>
			<date type="published" when="1976">1976</date>
			<publisher>Academic Press</publisher>
			<biblScope unit="page" from="261" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Analyza: Exploring data with conversation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dhamdhere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Mccurley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nahmias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Intelligent User Interfaces</title>
				<meeting>the 22nd International Conference on Intelligent User Interfaces</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="493" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visual search and stimulus similarity</title>
		<author>
			<persName><forename type="first">J</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Humphreys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">433</biblScope>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The Nature and Status of Visual Resources. Oxford Library of Psychology</title>
		<author>
			<persName><forename type="first">S</forename><surname>Franconeri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Three perceptual tools for seeing and understanding visualized data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Franconeri</surname></persName>
		</author>
		<idno type="DOI">10.1177/09637214211009512</idno>
	</analytic>
	<monogr>
		<title level="j">Current Directions in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A general computational treatment of the comparative</title>
		<author>
			<persName><forename type="first">C</forename><surname>Friedman</surname></persName>
		</author>
		<idno type="DOI">10.3115/981623.981643</idno>
	</analytic>
	<monogr>
		<title level="m">27th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting><address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1989-06">June 1989</date>
			<biblScope unit="page" from="161" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Datatone: Managing ambiguity in natural language interfaces for data visualization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dontcheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Karahalios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual ACM Symposium on User Interface Software Technology, UIST 2015</title>
				<meeting>the 28th Annual ACM Symposium on User Interface Software Technology, UIST 2015<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="489" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Communicating health risks with visual aids</title>
		<author>
			<persName><forename type="first">R</forename><surname>Garcia-Retamero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Cokely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Directions in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="392" to="399" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Communicating treatment risk reduction to people with low numeracy skills: a cross-cultural comparison</title>
		<author>
			<persName><forename type="first">R</forename><surname>Garcia-Retamero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Galesic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American journal of public health</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2196" to="2202" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Quantitative and qualitative research: Beyond the debate</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">C G</forename><surname>Gelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Braakmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Benetka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Integrative Psychological and Behavioral Science</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="266" to="290" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Considerations for visualizing comparison</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="413" to="423" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Visual comparison for information visualization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Albers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Jusufi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Roberts</surname></persName>
		</author>
		<idno type="DOI">10.1177/1473871611416549</idno>
		<imprint>
			<date type="published" when="2011-10">Oct. 2011</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="289" to="309" />
		</imprint>
	</monogr>
	<note>Information Visualization</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Perception of average value in multiclass scatterplots</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Correll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nothelfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Franconeri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2316" to="2325" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On the give and take between event apprehension and utterance formulation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Gleitman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>January</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Trueswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of memory and language</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="544" to="569" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exploring multiple trees through dag representations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2007.70556</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">2007</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Comparing semantic theories of comparison arnim von stechow</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hamann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Heim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Seuren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sternefeld</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The impact of the format of graphical presentation on healthrelated knowledge and treatment choices</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Hawley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zikmund-Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ubel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jancovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fagerlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Patient education and counseling</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="448" to="455" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Toward interface defaults for vague modifiers in natural language interfaces for visual analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hearst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Setlur</surname></persName>
		</author>
		<idno type="DOI">10.1109/VISUAL.2019.8933569</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Visualization Conference (VIS)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="21" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The perceptual proxies of visual comparison</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jardine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Ondov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Franconeri</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2019.2934786</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1012" to="1021" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">When (ish) is my bus? user-centered visualizations of uncertainty in everyday, mobile predictive systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Munson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 CHI</title>
				<meeting>the 2016 CHI</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5092" to="5103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Keefe</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1747-9991.2008.00124.x</idno>
	</analytic>
	<monogr>
		<title level="j">Vagueness: Supervaluationism. Philosophy Compass</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="315" to="324" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Projecting the adjective: The syntax and semantics of gradability and comparison</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kennedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">01</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Kennedy</surname></persName>
		</author>
		<idno type="DOI">10.1016/B0-08-044854-2/01028-2</idno>
		<title level="m">Comparatives, semantics of. Encyclopedia of Language &amp; Linguistics</title>
				<imprint>
			<biblScope unit="volume">08</biblScope>
			<biblScope unit="page">2004</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">A corpus of comparisons in product reviews</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kuhn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">05</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A semantics for positive and comparative adjectives</title>
		<author>
			<persName><forename type="first">E</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF00351812</idno>
	</analytic>
	<monogr>
		<title level="j">Linguistics and Philosophy</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="45" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Kranz</surname></persName>
		</author>
		<author>
			<persName><surname>Transcribe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Size scaling in visual pattern recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bundesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of experimental psychology. Human perception and performance</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Effects of spatial separation in visual pattern matching: Evidence on the role of mental translation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bundesen</surname></persName>
		</author>
		<idno type="DOI">10.1037/0096-1523.24.3.719</idno>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="719" to="731" />
		</imprint>
	</monogr>
	<note>Journal of experimental psychology. Human perception and performance</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Causal perception in question-answering systems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Y</forename></persName>
		</author>
		<author>
			<persName><forename type="first">.-H</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Endert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<idno>ArXiv, abs/2012.14477</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Kwon</surname></persName>
		</author>
		<title level="m">Vlat: Development of a visualization literacy assessment test. IEEE transactions on visualization and computer graphics</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="551" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Evaluation of trend localization with multi-variate visualizations</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Livingston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Decker</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2011.194</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2053" to="2062" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Measures of the benefit of direct encoding of data deltas for data pair relation perception</title>
		<author>
			<persName><forename type="first">C</forename><surname>Nothelfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Franconeri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="311" to="320" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Comparative visualization -approaches and examples</title>
		<author>
			<persName><forename type="first">H.-G</forename><surname>Pagendarm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Post</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">01</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Prolific. ac-a subject pool for online experiments</title>
		<author>
			<persName><forename type="first">S</forename><surname>Palan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schitter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Behavioral and Experimental Finance</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="22" to="27" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Package &apos;pmcmrplus&apos;. R Foundation for Statistical Computing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pohlert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Pohlert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>Vienna, Austria</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename><surname>Qualtrics</surname></persName>
		</author>
		<author>
			<persName><surname>Qualtrics</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<pubPlace>Provo, UT, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Change detection</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Rensink</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev.psych.53.100901.135125</idno>
		<idno type="PMID">11752486</idno>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Psychology</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="245" to="277" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">120 years of olympic history: Athletes and results</title>
		<author>
			<persName><forename type="first">Rgriffin</forename></persName>
		</author>
		<ptr target="https://www.kaggle.com/datasets/heesoo37/120-years-of-olympic-history-athletes-and-results" />
		<imprint>
			<date type="published" when="2018-06">Jun 2018</date>
			<biblScope unit="page" from="2022" to="2028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Asymmetric coding of categorical spatial relations in both language and vision</title>
		<author>
			<persName><forename type="first">J</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Franconeri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in psychology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">464</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Amazon top 50 bestselling books</title>
		<author>
			<persName><forename type="first">S</forename><surname>Saalu</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/sootersaalu/amazon-top-50-bestselling-books-2009-2019" />
		<imprint>
			<date type="published" when="2009">2009-2019. Oct 2020</date>
			<biblScope unit="page" from="2022" to="2028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Grading, a study in semantics</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sapir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophy of Science</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="93" to="116" />
			<date type="published" when="1944">1944</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Vega-lite: A grammar of interactive graphics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Satyanarayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wongsuphasawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2016.2599030</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="341" to="350" />
			<date type="published" when="2017-01">jan 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Quantifiers in comparatives: A semantics of degree based on intervals</title>
		<author>
			<persName><forename type="first">R</forename><surname>Schwarzchild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wilkinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Semantics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="41" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Eviza: A natural language interface for visual analysis</title>
		<author>
			<persName><forename type="first">V</forename><surname>Setlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Battersby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gossweiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1145/2984511.2984588</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual Symposium on User Interface Software and Technology, UIST &apos;16</title>
				<meeting>the 29th Annual Symposium on User Interface Software and Technology, UIST &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="365" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Sentifiers: Interpreting vague intent modifiers in visual analysis using word co-occurrence and sentiment analysis</title>
		<author>
			<persName><forename type="first">V</forename><surname>Setlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="DOI">10.1109/VIS47514.2020.00050</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Visualization Conference (VIS)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="216" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">How do you converse with an analytical chatbot? revisiting gricean maxims for designing analytical conversational behavior</title>
		<author>
			<persName><forename type="first">V</forename><surname>Setlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tory</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI conference on Human Factors in computing systems</title>
				<meeting>the SIGCHI conference on Human Factors in computing systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Inferencing underspecified natural language utterances in visual analysis</title>
		<author>
			<persName><forename type="first">V</forename><surname>Setlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Djalali</surname></persName>
		</author>
		<idno type="DOI">10.1145/3301275.3302270</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Intelligent User Interfaces, IUI &apos;19</title>
				<meeting>the 24th International Conference on Intelligent User Interfaces, IUI &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="40" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Bar and line graph comprehension: An interaction of top-down and bottom-up processes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Freedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Topics in cognitive science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="560" to="578" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Shapiro</surname></persName>
		</author>
		<title level="m">Vagueness in Context</title>
				<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">What&apos;s the difference? evaluating variations of multi-series bar charts for visual comparison tasks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brehmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Drucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2018 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">The good, the bad, and the biased: five ways visualizations can mislead (and how to fix them). interactions</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Szafir</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="26" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Presenting research risks and benefits to parents: does format matter? Anesthesia and analgesia</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Tait</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Voepel-Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Zikmund-Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fagerlin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page">718</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Envisioning information</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Tufte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Graphics Press</publisher>
			<pubPlace>Cheshire, Conn.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Visual arrangements of bar charts influence comparisons in viewer takeaways</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Setlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Franconeri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="955" to="965" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Illusion of causality in visualized data</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Franconeri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="853" to="862" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Visual salience and grouping cues guide relation perception in visual data displays</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stokes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Franconeri</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Perceptual and Cognitive Affordances of Data Visualizations</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y Y</forename><surname>Xiong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
		<respStmt>
			<orgName>Northwestern University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Bars and lines: A study of graphic communication</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zacks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tversky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Memory &amp; Cognition</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1073" to="1079" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">An evaluation-focused framework for visualization recommendation algorithms</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Battle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="346" to="356" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
