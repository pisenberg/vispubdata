<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RoboHapalytics: A Robot Assisted Haptic Controller for Immersive Analytics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shaozhang</forename><surname>Dai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jim</forename><surname>Smiley</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tim</forename><surname>Dwyer</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Barrett</forename><surname>Ens</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lonni</forename><surname>Besancon</surname></persName>
						</author>
						<title level="a" type="main">RoboHapalytics: A Robot Assisted Haptic Controller for Immersive Analytics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-03-27T09:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Haptic Feedback</term>
					<term>Human Centred Interaction</term>
					<term>Robotic Arm</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. 1. Use Cases of the dynamic slider system: a time-series chart selector (left top and left bottom); a 3D medical image sectioning tool; and a time-varying scatter plot (middle top and middle bottom) (right top and right bottom</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In 1965, at the dawn of interactive computer graphics technology, Ivan Sutherland remarked that "If the task of the display is to serve as a looking-glass into the mathematical wonderland constructed in computer memory, it should serve as many senses as possible" <ref type="bibr" target="#b64">[65]</ref>. He went on to speculate that in "The Ultimate Display" the computer would be able to control the existence of matter. While that precise</p><p>• Shaozhang Dai is with Monash University. E-mail:</p><p>shaozhang.dai1@monash.edu • Jim Smiley is with Monash University. E-mail: jim.smiley@monash.edu • Tim Dwyer is with Monash University. E-mail: tim.dwyer@monash.edu • Barrett Ens is with Monash University. E-mail: barret.ens@monash.edu technology remains science fiction, virtual reality coupled with active tangible devices allows us to begin to simulate the look and feel of computer-controlled matter. There have been various systems proposed to provide immersive data visualisations with various levels of haptic feedback (see Sect. 2). However, in this paper we present a novel approach to immersive haptic data interaction which uses a robot arm to automate the position of tangible controls (an actuated slider and a rotary encoder), and offers interaction affordances congruent to the common navigation and selection tasks of many visualisation applications. To our knowledge we are the first to use a desktop robot arm for tangible interaction and "encountered haptics" (Sect. 2.3) in an immersive analytics context. The robot allows us to:</p><p>• position and orient physical controls for direct interaction with data with six Degrees of Freedom (6 DoF); • extend the range of a small physical slider control to the full reach of the robot, i.e. where past work exploring slider controls for immersive analytics had a 10cm range (Sect. 2), ours has an effective range of up to 76cm (Sect. 3); • change the orientation of the slider to control any of the principal orthogonal (x, y, z) dimensions, or any other orientation (e.g., to align with the axes of a rotated model rather than world coordinates); and</p><p>• simulate a non-linear slider control with a linear physical slider (Sect. 7.3). The other key technology that enables our system is precise handposition tracking. Hand-position tracking allows us to: • place the physical controller at the location of a VR affordance justin-time as the user reaches for it (e.g., to place the slider knob at the position of a data point or at the filter control of an axis); • provide physical feedback for a multiplicity of virtual affordances using the motor of the actuated slider for precise haptics; and • get the physical controller out of the way when not required for hands-free mid-air interaction. For immersive analytics, this means that we can: • provide tangible interaction affordances for data elements anywhere within the 3D volume of reach of the robot; • allow users to perform slider selections and filtering operations along arbitrary (even non-linear) axes.</p><p>There are questions about how effective our robot-assisted solution is in achieving the above criteria. In particular, does using the robot to extend the range of a small slider actually provide a selection device that is as effective as a large physical slider? Furthermore, how do these tangible slider solutions compare to a more standard mid-air selection gesture? To address these questions we report on a comparative study evaluating three slider conditions: virtual, physical, and dynamic (robotassisted). We find that the robot-assisted slider approaches the physical slider in terms of speed and provides a similar accuracy, while both tangible controls are significantly faster and more accurate than mid-air interaction, as reported in Sect. <ref type="bibr" target="#b3">4</ref>.</p><p>Thus, while the robot-assisted slider has similar capability to a large physical slider in terms of speed and accuracy, it enables the other possibilities, mentioned above, of being arbitrarily movable and reorientable while the physical slider is too large and unwieldy to allow this. We further demonstrate the capabilities offered by integration of the robot controlled slider device with hand-position tracking through exploration of three data visualisation use-cases: • A time-series chart selector, demonstrating one-to-one scale direct interaction with a time-axis slider to read off specific data points (Sect. 7.1); • A 3D medical image sectioning tool, demonstrating hand-tracking and one-to-one scale range filtering across three orthogonal axes and integrating a second tangible control mounted on the device to perform rotations of the model (Sect. 7.2); • A VR implementation of the DimpVis technique <ref type="bibr" target="#b39">[40]</ref> for direct interaction with time-varying scatter plots (Sect. 7.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>The presented work pertains to immersive analytics <ref type="bibr">[13,</ref><ref type="bibr" target="#b49">50]</ref>, humanrobot interactions in virtual reality, as well as haptic controllers and tangible interactions in the context of visualisation and data analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Tangible Interaction for visualisation</head><p>Tangible interaction aims to leverage natural human interaction with everyday 3D objects to facilitate interacting with digital content <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b33">34]</ref>. For 3D manipulations, tangible interaction has been shown in past research to be fast and precise <ref type="bibr" target="#b7">[8]</ref>, entertaining <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b74">75]</ref>, as well as fostering and assisting collaborations <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b52">53]</ref>. Researchers have thus tried to develop tangible devices to assist visual exploration of complex and usually spatial data <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12]</ref>. A pioneering example is from Hinckley et al. <ref type="bibr" target="#b32">[33]</ref> who used tracked props for neurosurgeons that allowed them to manipulate cutting planes and data in order to explore the internal structure of their datasets. While some research prototypes make use of generic tangible devices, to increase versatility, other research projects have investigated specific tangible props. Such devices range from a 3D gun to explore DNA data <ref type="bibr" target="#b57">[58]</ref>, a paper roll to select thin fiber structures <ref type="bibr" target="#b35">[36]</ref>, or balls to interact with geo-data <ref type="bibr" target="#b55">[56]</ref>; to pen-like props for 3D brushing <ref type="bibr" target="#b27">[28]</ref>, selection of regions of interest <ref type="bibr" target="#b18">[19]</ref> or manipulate cutting planes <ref type="bibr" target="#b34">[35]</ref>.</p><p>Other research projects have instead studied the use of existing, 'generic' devices (some with built-in tracking solutions) to increase their versatility. Tablet-like devices have been used for this purpose, whether self-tracked or relying on external tracking. Cassinelly and Matasoshi <ref type="bibr" target="#b14">[15]</ref> used a screen combined with external tracking to allow experts to explore and annotate medical data. Similarly, Spindler et al. <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b63">64]</ref> used tracked screen-like props to augment an existing visual representation with colours or to switch between multiple levels of abstraction. Pushing the concept further, some researchers proposed to use mobile devices which, combined with their tactile screens, provide more versatility for data exploration or selection through a combination of touch and tangible interaction <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b61">62]</ref>. Other approaches have also relied on commercial devices such as the 3D mouse <ref type="bibr" target="#b72">[73]</ref>.</p><p>Particularly related to our work and on the spectrum between more specialised and generic devices, is the work from Cordeil et al. <ref type="bibr" target="#b16">[17]</ref> investigating a novel approach to controller design with the concept of spatio-data coordination <ref type="bibr" target="#b17">[18]</ref>. In their work, they used an actuated slider which they mounted on three orthogonal axes to provide a volumetric range selector. This work was recently extended by Smiley et al. <ref type="bibr" target="#b60">[61]</ref> who decoupled the three axes from Cordeil et al. <ref type="bibr" target="#b16">[17]</ref> to provide universal and versatile axes controls that are handheld and can be combined. In their work, the slider constrains interaction to a single axis. Such constrained interactions with tangibles have been shown in the past to be beneficial to data manipulation <ref type="bibr" target="#b30">[31]</ref>. While Smiley et al. <ref type="bibr" target="#b60">[61]</ref> used their axes for collaborative immersive analytics and by proposing to explore multiple axes, we instead focus on using a single axis for a single user and explore how it can be used to provide immersive haptic data interaction facilitated by a robot arm to automatically position the axis under the user's hand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Haptic and actuated devices for visualisation</head><p>Our concept relies on an actuated slider to provide users with a versatile tangible device for visualisation tasks. Numerous research projects have previously investigated actuated devices in the context of visualisation. One category of such devices lies in actuated physicalisation of datasets. While most of the early physical visualisation prototypes were static, more recent prototypes have been developed to be dynamic and potentially interactive. The majority of these systems function with arrays of motorised bars <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b65">66]</ref>, but some also propose a more fluid control of the final physical shape <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b54">55]</ref>. In these actuated systems, the physical coordinates represent the digital dataset and their actuation allows users to visualise different datasets or to, for instance, experience how a dataset changes over time <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b65">66]</ref>. Such actuated systems usually rely on expensive and specific hardware and only rarely allow for interaction (see e. g., <ref type="bibr" target="#b65">[66]</ref>). To provide a more versatile control of the final geometry and facilitate richer interactions, Le Goc et al. developed Zooids <ref type="bibr" target="#b41">[42]</ref>: small robots that, thanks to external tracking, rearrange themselves on a table to provide several visual representations and adapt to different datasets. They also serve as versatile controllers to interact with the data they display. It should be noted that compared to the robot system we present here, Zooids only operate on a 2D surface and are far too small to provide force feedback.</p><p>Our work takes more inspiration from the possibilities offered by other haptic and actuated systems provided by some commercial devices. One such device is the PHANToM and other haptic pens that have been extensively used in visualisation applications. They have been used, for instance, to allow users to feel and explore through haptics scalar and vector fields <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b68">69]</ref> or to simulate medical examinations <ref type="bibr" target="#b67">[68]</ref>. However, those devices and associated research projects mostly focus on providing haptic forces to allow for exploration and help users feel with only a pen stylus as a tangible tool. In contrast, our approach is particularly inspired by the work of Lischke et al. <ref type="bibr" target="#b45">[46]</ref> who used a motorised slide potentiometer as an interactive device for virtual environments. Our prototype, however, investigates such a solution for visualisation tasks and proposes to dynamically adjust the position of the device to follow the user's hands to provide seamless data interaction mechanisms in immersive contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Encountered-type Haptics</head><p>In the field of virtual reality, researchers have tried to introduce realistic haptic feedback through different approaches. Particularly related to our work is the idea of "encountered-type haptics" <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b71">72]</ref>. The idea is simple: an "active" device is used to provide different contact points under various situations. In other words, the device will adjust its position and orientation to match the user's actions in order to provide them with realistic haptic feedback in a variety of tactile situations. The idea to use robotic arms as active interaction devices has received a lot of attention in the literature through the study of human-robot interactions <ref type="bibr" target="#b29">[30]</ref>. Many studies have involved human-robot interactions in the VR field as well. For example, Lee et al. <ref type="bibr" target="#b42">[43]</ref> developed a remote control approach to manipulate the robot end effector movement with hand gestures through virtual reality controllers. Robot arms are also widely used in encountered-type haptics to continuously follow and match the user's actions <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b71">72]</ref>. For example, Araujo et al. <ref type="bibr" target="#b2">[3]</ref> used a robot arm to provide feeling of textures through a tactile device on a robot arm and simple interactions through buttons. Mercado et al. <ref type="bibr" target="#b51">[52]</ref> used a robot arm to provide an on-demand tangible surface by placing a physical plane under the contact area when the user is putting an object on a surface in the virtual world. Such approaches have been evaluated and the results seem to indicate that they enhance immersive experiences as they provide real-time "being there" sensation when interacting with virtual objects <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b51">52]</ref>.</p><p>While other devices such as drones <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b75">76]</ref> or other custom robots <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b66">67]</ref> could be used for haptics, they tend to restrict the direction or angles that the device can eventually reach. Robot arms, however, are more versatile with a high load-bearing ability and have thus even been used to simulate large objects such as walls or tables in virtual experiences <ref type="bibr" target="#b51">[52]</ref>. The speed at which they operate allows for the representation of several objects (see, e. g., <ref type="bibr" target="#b51">[52]</ref>). In addition, the detachable head design of commercial robot arms increases the variety of haptic feedback available for a single encountered haptic feedback system <ref type="bibr" target="#b2">[3]</ref>.</p><p>However, most of the past encountered haptic systems focus on providing support to users and increase their immersive experience. In contrast, we aim at using the possibility to provide users with a system that allows them to interact seamlessly with visualisation independent of their hands' positions. We thus combine a tangible axis with a haptic slider on the robot arm to explore the possibilities that such a system can offer in immersive data analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DYNAMIC SLIDER SYSTEM</head><p>We designed a robot assisted dynamic slider system (Dynamic Slider) which can automatically align itself with its model in the virtual world (see Fig. <ref type="figure">2</ref>). It is able to align to different virtual sliders of various lengths and orientation in the same scene. The sliders are built from open-source CNC gantry kits. <ref type="foot" target="#foot_0">1</ref> These were were modified by mounting a 3D printed knob to the gantry plate, and replacing the stepper motor with a DC motor to allow constant torque. A 600 pulse per revolution industrial optical encoder was also installed, and we 3D printed 40-tooth, 2 mm pitch timing pulleys for a distance of 0.13 mm per encoder pulse. This was found to be too precise to easily select an exact value manually, and the code adjusted to give a discrete increment spacing at 0.4 mm. An additional rotary encoder can be mounted at a perpendicular angle at the end of the axis.</p><p>To help negate cogging torque of the DC motor, a small haptic pulse is integrated directly at the microcontroller encoder interrupt call, which delivers a small amount of torque per discrete value to the H bridge motor driver in the opposite direction of slider travel, assisting the user to not overshoot. An alternative to this could be investigating the use of low cogging torque slotless DC motors, however these are expensive and not easily available.</p><p>The slider assembly is mounted on the end of a UR3 (Universal Robot 3.0 series) robotic arm. For the version attached to the robot, we configured the sliders to have a 170 mm selectable range. Longer than this was considered to be impractical for robot mounting, as it would mean that extremities of the slider assembly would move very quickly (potentially dangerously so) when rotated. The UR3 has a 500 mm Fig. <ref type="figure">2</ref>. The dynamic slider system is composed of our self-built slider (from a DC motor, an optical encoder, a 3D printed slider knob, a rotary encoder, and a modified belt driven CNC gantry) and a robotic arm. A hand tracking glove is used to capture hand position and posture in VR, and a red button is attached on the work desk for the experiment. radius of reach, meaning that the total extended slider range available for user selection can be up to 1170 mm, at base, allowing a potential 2,925 discrete values per axis, at full extension.</p><p>For our study, we limited the range of of our dynamic slider to 760 mm, allowing a total of 1900 discrete points along the slider's axis, and the full length physical slider adjusted to match this exactly. (see Sect. 4). The robot is controlled through its API,<ref type="foot" target="#foot_1">2</ref> and all the scripts involved are uploaded to GitHub. <ref type="foot" target="#foot_2">3</ref>We used a Vicon motion capture system with five cameras for accurate position tracking and alignment of the slider, hand, and the VR environment. The encoder is read by an Arduino Micro, which also drives the motor via an L298N H-Bridge driver board. The Arduino is connected to the local PC via a USB serial connection. Information from the Vicon is collected and processed from another local device and transmitted through UDP wireless communication. We use a local server to exchange information between the robotic arm and the local PC with a constant communication delay of 5∼6 ms. Fig. <ref type="figure" target="#fig_0">3</ref> illustrates the interconnection between the main components of our system. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Position Synchronisation</head><p>Position synchronisation between the physical world and the virtual world is crucial as we are using a short slider that is supposed to mimic, in real time, the interaction that a long slider could provide. The position change of the virtual slider knob, d v , regarding a fixed coordinate system (the global coordinate in the virtual space) depends on the displacement of the robotic arm, D r , and the position of the slider knob referring to the physical slider itself, d s (see Fig. <ref type="figure" target="#fig_1">4</ref>). According to equation 1, by having a correct measure of the robotic arm displacement and the physical slider change, an accurate virtual slider displacement can be obtained, where α and β represent the coefficient of converting the physical displacement of the robotic arm and the physical slider knob into the virtual environment coordinates. Therefore, with the tracking of the slider knob position and the position of the robotic arm end effector, the position change of the slider knob is presented in the virtual environment within the offset of the Vicon tracking tolerance and the encoder accuracy.</p><formula xml:id="formula_0">d v = αD r + β d s (1)</formula><p>During development, we tested several camera-based hand tracking techniques including Leap Motion and built-in hand tracking from the VR headset. Although camera-based hand tracking is easy to implement, and can capture the hand pose with acceptable accuracy, the reliability is highly dependant on the distance from the camera and the tracking range is substantially limited by the view angle of a single device. Therefore, to have a reliable and robust tracking result, we chose the combination of a dedicated VR glove (Manus Prime II) with a Vicon motion tracking system for pose and position tracking respectively. Several tracking markers are mounted on the back of the glove for overall position tracking, and flex sensors within each finger of the glove provide real-time update of hand posture in VR. An initial calibration step automatically aligns the VR and the physical worlds by comparing the vector angle and position of two VR controllers, tracked both by the Vicon and the VR headset, and then applies a position and rotation offset to the VR rig root position in the Unity Engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Slider and Robotic Arm Behaviour</head><p>To extend the perceived slider range and simulate the experience of interacting with a longer slider, both the mobility and the force feedback of the slider should be made as smooth as possile. After some pilot testing, we decided to assign a buffer at either end of the slider, which activates motion of the robotic arm. Outside of these buffer regions, interaction with the slider knob is identical to a normal slider, however once the knob enters a buffer region, the robotic arm starts to move with the slider to extend its range. We considered three different methods of using buffers to achieve a realistic long slider: • Large-Buffer: a large buffer assignment which activates the robotic arm with constant speed and displacement; • Small-Buffer: a small buffer assignment to maximize the region not affected by the robotic arm movement; and Fig. <ref type="figure">5</ref>. Interaction scenario for the dynamic slider. The robot arm moves in the same direction as the slider knob movement, while the slider motor is pulling in the opposite direction. As the knob gets close to the end of the slider, the speed of the robotic arm and motor output increases.</p><p>• Dynamic-Buffer: a large buffer assignment which affects the robotic arm dynamically. The speed of the robotic arm varies based on the relative knob position inside the buffer region; We conducted a pilot controlled experiment with simple pointing task with six participants to evaluate these three methods. The results of this pilot experiment are detailed in our supplementary materials at https: //osf.io/puzch/. Overall, our data indicate that the Dynamic-Buffer technique had noticeably better results in both speed and accuracy. We therefore decided to focus on the Dynamic-Buffer design only in the remainder of this manuscript.</p><p>When the slider knob enters the buffer region (green area in Fig. <ref type="figure">5</ref>), the robotic arm is activated and starts moving in the same direction as the slider knob. To adapt to the resulting change in speed, the speed of the robotic arm is inversely proportional to the distance between the slider knob and the end of the physical slider. Therefore, the robot's speed matches the user's hand movement unless the user is moving the knob faster than the implemented speed limit, which will result in hitting the end of the physical slider. This will, however, not stop the robot's operation which would eventually catch up.</p><p>When the robot is moving in the same direction as the user's hand, the slider friction normally felt when pushing the slider will be reduced. This may cause an unnatural experience that the slider is 'pulling' the user's hand. To mitigate this, we activate the DC motor on the slider simultaneously. This generates a force acting on the slider knob in the opposite direction of the robotic arm movement to provide an illusory pushing sensation. The magnitude of the generated force is proportional to the acceleration of the robotic arm, which is tuned to provide the 'correct' amount of opposing force against the hand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Safety Consideration</head><p>The UR3 has a maximum output setting of 250 N Force; 1000 W Power; 5000 mm/s Speed; and 100 kgm/s Momentum. Although the robotic arm has a collision detection system to automatically stop the robot operation when obvious resistance is detected, for safety we have limited the maximum speed of the robot to minimise the potential for damage and injury. The robot speed was limited to 300 mm/s during the experiment, which is much less than the safety guidelines <ref type="bibr" target="#b70">[71]</ref>. We have also set limitations to each of the individual joints so that the robotic arm can only operate in the specifically permitted area. We have also eliminated unexpected movement by resetting the robot position near the base of the robot whenever there is no active interaction in the virtual world and designing the path of the robot to ensure it would always approach the user from far to near rather than laterally. Since we are using a self-built physical slider mounted on the robot, all visible edges on the physical slider are covered with a layer of soft foam to further limit risks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONTROLLED EXPERIMENT</head><p>We conducted a controlled experiment in which users manipulate a slider in an immersive context to evaluate the performance difference between three conditions: Virtual -without any tangible or haptic feedback during interactions; Physical -with haptic feedback from a 1:1 ratio physical slider; Dynamic -with haptic feedback from the combination of a short physical slider and a robotic arm. We measured task completion time and error rate during positioning of a virtual control at a specific location, as well as subjective feedback from the users. Such tasks are common to evaluate new interactive techniques or devices in the HCI and visualisation literacture (see, e. g., <ref type="bibr" target="#b36">[37]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Task and Study Design</head><p>We now describe the design of our user study and the motivation behind it. The aiming task is conducted by generating a random target and asking the participant to move the knob and point it to the centre of the target. For precise visual feedback, a red arrow is attached to the virtual slider knob and the random target is designed as a yellow arrow with a thin red line indicating the centre of the target (see Fig. <ref type="figure" target="#fig_3">7</ref>). There is a red button in both the virtual world and the physical world for the participants to claim their completion. Participants are asked to perform the task with their right hand only, which provides a real-world-like experience where small position adjustment is required to approach each object. The target position is generated randomly based on the current position of the virtual slider knob, and the new position is restricted in three ranges: small range where wrist movement only can cover the target range (19 cm to 21cm); middle range where elbow movement only is sufficient (34 cm to 36 cm); and large range where waist rotation may be required (51 cm to 53 cm). The task repeats 10 times for each range and each condition includes all three ranges. Therefore, the total number of responses per participant is 3 conditions × 3 ranges × 10 repeated tasks = 90 responses.</p><p>The maximum speed and reachable area of the robotic arm is restricted for safety as described above. These limitations reduce the flexibility of the dynamic slider and the influence of this restriction is expected in the data collection and the subjective feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Procedure</head><p>The order of conditions for each participant and the order of the random target for each condition are counter-balanced to reduce the impact of learning effects. Participants were instructed to put on the headset prior to starting each condition, to prevent them from seeing the physical slider. For each condition, there are 10 training trials of the pointing task. Before the start of the actual tasks, participants are asked to move the virtual knob to the centre of the slider.</p><p>At the end of each condition, a set of questionnaires involving 14 virtual reality presence questions (IPQ) <ref type="bibr" target="#b58">[59]</ref> and two confidence questions is completed by the participant based on their experience. After all three conditions are completed, participants need to finish a post-study survey giving their reflections about the three conditions.</p><p>In summary, our study consisted of four main steps: briefing, training, tasks, and post-study questionnaire. The briefing stage consisted of approximated five minutes of reading the task descriptions. We deliberately asked participants to perform the training trials slowly at first, to ensure they could get used to the interaction with the slider. Each participant performed a total of 100 trials (90 actual trials + 10 training trials) in a maximum of 20 minutes time. After each condition the participant was asked to fill the condition feedback survey and then, after the whole experiment, to complete a post-study questionnaire. Completing the feedback forms could take up to 15 minutes. In total, the duration of the study was around 45 minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Participants and Data Collection</head><p>We recruited a total of 30 participants from our university (23 males and 7 females with mean age of 24.7 and median age of 25). The study was approved by the local ethics committee. We collected three types of data: i) task-related data, ii) interaction log data, iii) subjective measures data. For the task-related data, we recorded the error rate measured as the difference (in mm) between the current position of the virtual slider knob and the centre of the target after each of the button presses, as well as the completion time. Meanwhile, every position change in the virtual world during the experiment is recorded for later analysis as the interaction log data. In the post-study questionnaire, we asked participants to rank the sliders in all three conditions with respect to ease of use, confidence to use, and realism of the experience compared to a physical task (presence). We also encouraged participants to briefly describe the reasons for their choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>Data from experiments has been historically analysed with null hypothesis significance testing (NHST). However, recent criticism of NHST-based analysis have highlighted its limitations. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27]</ref> and its tendency to lead to dichotomous inferences <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b31">32]</ref> which can eventually lead to less robust findings in scholarly communications <ref type="bibr" target="#b15">[16]</ref>. We thus decided to follow current APA recommendations <ref type="bibr" target="#b69">[70]</ref> and report our results using estimation techniques with simple effect size and confidence intervals (CIs). The term effect size here refers to mean differences and ratios <ref type="bibr" target="#b20">[21]</ref>. We interpret our CIs as providing different strengths of evidence about the mean <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref> and rather avoid the use of words such as "significant" which reflect more dichotomous interpretations. For completeness and although we only use estimation techniques, we provide p-values in each figure and in our additional materials, but a method to obtain a p-value reading of our results is detailed by Krzywinski and Altman <ref type="bibr" target="#b40">[41]</ref>. Following current best practices, we pre-registered the analysis of our empirical data <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16]</ref> and share our experimental materials, data, pre-registration, and code at https://osf.io/puzch/.</p><p>We detail below the results of our pre-registered analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Completion time</head><p>We report the absolute mean values of task completion time in seconds for each condition in Fig. <ref type="figure">8</ref> and the pair-wise ratios between each condition in Fig. <ref type="figure">9</ref>. To correct for the positive skewness of completiontime data, we analysed log-transformed measurements and present anti-logged results, which is, considering the log-normal distribution of time data, a standard analysis <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b56">57]</ref> that is commonly used in HCI and visualisation work (e. g., <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b72">73]</ref>). We consequently present geometric means which dampens the effect of potential extreme completion times that are likely to bias arithmetic means. We can find in Fig. <ref type="figure">8</ref>, from the complete lack of overlap between the confidence intervals of each conditions, very strong evidence that the virtual slider is slower than the dynamic slider which is in turn slower than the physical slider. This is confirmed by Fig. <ref type="figure">9</ref> which highlights that the virtual slider is almost 1.5 times (1.45, CIs[1.41;1.48]) slower than the physical slider but that the physical slider is 0.86 (CIs[0.844;0.874]) times faster than the dynamic slider. The size of the confidence intervals in Fig. <ref type="figure">9</ref> also highlight that the performances across participants are stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Error rate</head><p>We report the means of accuracy (in mm) for each condition in Fig. <ref type="figure">10</ref> and the pair-wise ratios between conditions in Fig. <ref type="figure">11</ref>. We observe in Fig. <ref type="figure">10</ref> very strong evidence that the virtual slider has a higher error rate than the other two techniques for which our experimental data does not seem to show evidence of a difference. This is confirmed in Fig. <ref type="figure">11</ref> which highlights that the difference is of slightly less than 1 mm between the virtual slider and both the physical (0.77, CIs[0.652;0.894]) and dynamic slider (0.75, [0.876;0.635]). The accuracy of the physical and dynamic slider is, according to our data, approximately down to the millimeter (respectively 0.99 [0.923;1.06] and 1.01 [0.950;1.07]).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ranking</head><p>We report on the ranking of each condition by the participants in Table <ref type="table" target="#tab_1">1</ref>.</p><p>We can see in the table that the fully virtual slider was ranked as the least preferred technique more often than the other two (19 times ranked last), while the physical slider was ranked as the preferred technique the most (14 times). Overall, the mean ranking for the physical slider (1.67) are slightly better than the mean ranking for the dynamic slider (1.9) and much better than the mean ranking for the virtual slider (2.43). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Qualitative Feedback</head><p>We report now the comments that the participants made about each of the three techniques that they tried in our controlled experiment.</p><p>Virtual Slider: The user experience of interacting with a slider with purely mid-air hand gestures is highly dependant on their background experience of VR. Indeed, the experimenter observed that most of the participants who did not have a lot of experience with VR systems were struggling with the virtual condition during the experiment. Four participants said they cannot grab the virtual slider knob properly even after the training, and one of them felt it is confusing to grab the virtual slider knob without actually touching something in hand. Only two participants who have a lot of experience in hand gesture interactions preferred this condition over the other two because of its flexibility in the interaction speed. Of six participants who prefered the virtual slider the most (Table <ref type="table" target="#tab_1">1</ref>), four had no prior VR experience. However, almost all participants (24 out of 30 including the four with no VR experience) agree that it is the most difficult condition to perform an accurate pointing task.</p><p>Physical Slider: The most common comment (21 out of 30) for the physical slider is that it is the easiest slider to use with minimum learning effort required as it resembles what participants do in real life. Additionally, the pointing task seems to be effortless in this condition. Participants also mentioned that they appreciated having the ability to "touch something". Another comment (7 out of 30) on the physical slider is that a visible gap between the virtual hand and the virtual slider knob during interaction makes the experience seem less real.</p><p>Dynamic Slider: Most of the participants (20 out of 30) commented that the user experience of the dynamic slider is somewhere in between the virtual slider and the physical slider. Although the generated haptic feedback and the presence of the tangible controller (the physical slider knob) provides a better interaction experience, there are still tiny but noticeable difference between the dynamic slider and the physical slider (comments from 15 participants). This kind of difference might be negligible if a user is focusing on something other than the slider, but it was logically more problematic in our controlled experiment which solely   focused on a pointing task. Another common comment is that when participants were moving the knob and approaching the destination, there was a small vibration-like experience and a small unexpected movement of the slider knob. Therefore, participants have to make an extra adjustment in order to point to the target properly. Despite the above comments, an interesting fact is that although participants noticed the difference between dynamic slider and the physical slider, many of them (around 18) did not realise they were actual interacting with a short slider on a moving robotic arm, as participants were intentionally asked to wear the VR headset at the beginning of each condition so that their feedback would not be affected by the physical environment change (the robotic arm with the short slider will be right in front of their hand).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Additional analyses</head><p>We detail below the additional analyses that we have conducted that were not specified in our pre-registration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Presence questionnaire</head><p>Although we initially planned to report on the IPQ questionnaire for each condition, we neglected to specify this analysis in our preregistration. The results of the questionnaire are visible in Fig. <ref type="figure" target="#fig_6">12</ref>.</p><p>Judging from the overlap of the confidence intervals for all conditions and subsets of the IPQ in Fig. <ref type="figure" target="#fig_6">12</ref>, our experiment did not allow us to find evidence of a difference between the three tested conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The impact of distance</head><p>In our controlled experiment we tested small, medium and large distance ranges to the target (Sect. 4.1). To confirm that the distance to the target does not influence the results, we conduct a supplementary, i. e., non pre-registered, analysis of the impact of range on the results. A breakdown per range is presented in Fig. <ref type="figure" target="#fig_7">13</ref> for completion time and in Fig. <ref type="figure" target="#fig_1">14</ref> for error rates. We see in Fig. <ref type="figure" target="#fig_7">13</ref> that the results found in the previous figures hold across all possible range of distance that we investigated. Similarly, Fig. <ref type="figure" target="#fig_1">14</ref> confirms that the tendencies observed above are consistent across all studied ranges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subjective assessments on usability and confidence</head><p>We report in Fig. <ref type="figure">15</ref> the Likert-scale evaluations of the participants. These quantitative results seem to confirm participants' comments and ranking that we previously detailed. The physical slider appears the easiest to use and the one providing the highest level of confidence, followed quite closely by the dynamic slider. The virtual-only slider is reported as the worst overall for both confidence and ease of use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>From our experiment and its data we can conclude that the dynamic slider that we developed seems to be as accurate as a real physical slider and much more precise than a virtual slider. This difference is likely to be partially explained by the exit-error <ref type="bibr" target="#b10">[11]</ref>: it is impossible for participants to stay completely still after correctly positioning the virtual knob and before validating the placement. The accuracy difference between the virtual slider and the other two conditions is, however, relatively small. While such a small difference in accuracy might not matter in many use cases, past work (e. g., <ref type="bibr" target="#b10">[11]</ref>) has argued that for visualisation cases it might, e. g., in the case of a surgeon performing a surgery planning task in an immersive context such as the one we used.</p><p>Our data also highlights that the physical slider is faster than the dynamic slider which is in turn faster than the virtual slider. From our observation of the participants, we argue that this difference in task completion time can be explained by the fact that participants tended to adjust the knob's position more times with the virtual slider than with the two physical sliders. It is likely that these extra adjustments were not necessary in the two physical conditions since users could physically hold a knob, thus echoing past research on the benefits of tangible devices in immersive contexts <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b73">74]</ref>. The difference between the dynamic slider and the physical one is most likely explained by the current speed limitation of the dynamic slider. Theoretically, interaction with the dynamic slider could be made to match the physical slider as the robotic arm can accelerate extremely quickly. But for safety reasons the current implementation limits the robot speed.</p><p>Overall, it seems that our dynamic slider outperforms a fully virtual slider in all aspects (speed, accuracy and preferences) while being slightly slower than a physical slider and not quite as highly preferred. Since the physical slider was designed to be the best possible baseline for this particular task, these slight differences are not surprising and the performances we obtained with the dynamic slider are promising.</p><p>Indeed, it is unlikely that physical 1:1 sliders will be made for all possible scenarios and task types and our dynamic slider provides more versatility and modularity even for the simple task that we decided to focus on in this controlled experiment. But, regarding the advantages of the dynamic slider, we have to consider use-cases beyond the very simple, yet important one, studied in this controlled experiment. Our ultimate vision for a robot-facilitated interactive device that would follow the user's hand is within the context of immersive visualisation tasks for which we argue in the following section that the device can be useful and that we now illustrate in three use cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">USE CASES</head><p>We explored three different use cases of the dynamic slider system. These use cases demonstrate the potential of the haptic feedback implementation in immersive data visualisation and are presented here in order of increasing sophistication and flexibility of the mapping of the physical control to the interaction affordances in the immersive visualisation. Please see the companion video to this paper for a detailed walk-through demonstration of the various interactions we implemented for each of these use cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Independent Variable Selection in a Time-Series Chart</head><p>Our first use case is a straight-forward application of the dynamic slider configuration of an extended-range but fixed-orientation axis slider selection task, similar to that tested in our study (Section Sect. 4). The 76 cm range of the dynamic slider can easily span a comfortable width for a detailed VR view of potentially complex data, hovering at waist-to-head height at about arms' length from the user.</p><p>To demonstrate this, we created a time-series data visualisation with a one-to-one mapping of slider knob position to position the x-axis, corresponding to the independent variable in a 2D line chart, e.g. time or (as shown in Figure Fig. <ref type="figure" target="#fig_9">16</ref>) income. Following their design space for slider interaction with visualisation dynamics, Smiley et al. <ref type="bibr" target="#b60">[61]</ref> we use haptic notching to snap the slider position to the nearest data point in the visualisation, and then display an infobox showing details for the data element at that point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">3D Medical Image Sectioning Tool</head><p>Cordeil et al. presented an "Embodied Axes" device featuring six small physical slider devices, paired along three orthogonal axes, to select a  box region within the 10 × 10 × 10 cm volume of the axes <ref type="bibr" target="#b16">[17]</ref>. The device was presented to domain experts in medical imaging in the context of slicing 3D CT imagery. The experts gave positive feedback about the utility of such an embodied interaction device, but with clear consensus that the biggest limitations were the small selection volume and the fixed orientation of the axes. The robot assisted haptic slider overcomes both of these limitations, extending the range of the selection to a reorientable 76 × 76 × 76 cm volume, see Figure    While only one physical slider is present, in VR the user sees six slider controls, paired along three orthogonal axes. As the user reaches towards a virtual slider control, the robot arm moves and reorients the physical slider such that the physical knob is in-situ with the virtual slider's knob by the time the user's fingers arrive at its location. Further, the user sees three rotation controls aligned at the top, front and side central axes of the image volume. When the user reaches for these controls the robot places the physical rotator encoder control at the end of the slider in-situ with the axis rotation affordance (see Fig. <ref type="figure" target="#fig_11">18</ref>). They can then freely rotate the image around that axis. That is, in the former, the sliders remain fixed in world x, y, z coordinates to allow further sectioning of the image along arbitrary cutting planes, or the sliders are reoriented with the image such that sectioning remains aligned with the axial, sagittal and coronal planes of the rotated image volume.</p><p>The average haptic delay of this use case, i.e. time between the user initiating interaction in the virtual world and the dynamic robot/slider system moving the knob to the target location from its base position, is measured by initiating five evenly distributed points on each of the slider in the x, y, z axis direction. The average haptic delay for x and y are 1004 ms and 1296 ms respectively. It is 2881 ms for vertical axis, z. This is due to the fact that direct movement from the base position to any any point along this axis requires large joint movement and has high risk of collision with the table below and the user. Therefore, a sequence of movement is designed to avoid the risk, which requires more time. This delay could be significantly reduced by increasing the maximum speed at which we allow the robot to move, but for now we conservatively limit it for safety (Sect. 3.3). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">DimpVis Multi-time-Series Curve Selector</head><p>The final use-case we consider is a virtual reality adaptation of the DimpVis scenario for direct interaction with time-series data curves, introduced by Kondo and Collins for large touch screen interaction <ref type="bibr" target="#b39">[40]</ref>. An interesting aspect of this application is that, while in the other use cases presented above the slider control is used for interaction with the axes corresponding to up to three spatial dimensions, in DimpVis the primary interaction affordances are the datapoint marks themselves.</p><p>As seen in Figure Fig. <ref type="figure" target="#fig_16">19</ref>, the data considered in this use case is from the GapMinder 4 dataset of life-expectancy versus income curves for selected countries. While the user is not interacting, the view shown is a scatter plot of average income versus average life expectancy with a point for each of the countries for a single year. When the user moves their hand within the axis of the scatter plot, the data point nearest to the user's fingers is highlighted and a time curve is displayed showing the trajectory of life-expectancy versus income for the country corresponding to that point over the full time range of the data. Simultaneously with the nearest data point being highlighted in this way, the robot rotates the slider control to the tangent of the curve at that point, and moves the slider-knob such that the knob will be ready at the position of the data point by the time the user's fingers reach it. Moving the slider forward along the tangent will then advance the highlight to the next data point in the series, and simultaneously animate all the other data points to show the data for the same year.</p><p>The average haptic delay for this use case is measured in two ways: the delay between the occurrence of interaction in VR until the physical slider has reached its destination from base position to one of the data points (base-to-point delay) or from a previous selected point to another point (point-to-point delay). The average base-to-point delay is 2515 ms, and the average point-to-point delay is 1282 ms. However, both delays are theoretically linearly proportional to the distance travelled. Consequently, the average haptic delay of this use case is likely to vary for different data sets with different scatter point distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">LIMITATIONS</head><p>While our controlled study required participants to remove their hand and then reacquire the control after restarting the task timer, the study did not evaluate cases where the haptic controls move independently of the user's hand. Further research could explore in what circumstances (if any) such independent movement has an effect on users' ability to reacquire the control.</p><p>Our system relies on a marker-based infrared motion capture system which is currently the fastest and most accurate system for real-time hand tracking. While some headsets such as Occulus Quest2 feature in-built markerless tracking their capability is still limited in latency and hand pose accuracy. However, we expect that the headset-based tracking will improve to the point where an external system is no longer necessary. Similarly, the robot device we use (UR3) is relatively expensive and requires an external control and powersupply box. While the cost and set up for such equipment may be acceptable for certain 4 gapminder.org industrial and medical applications, simpler, lower-cost commodity devices would be required for wide adoption of encountered haptic visualisation applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSION AND FUTURE WORK</head><p>While our controlled experiment and use-cases already demonstrate usability and applicability of the robot-assisted haptic controller we have developed, there are many avenues for future research in this vein.</p><p>A first obvious avenue for future work lies in the evaluation of the use cases we have presented in this manuscript. Indeed, we focused our evaluation on a 1D use case with a slider but the usability of the proposed interaction system should further be evaluated in other 3D use-cases. From our results, we can hypothesize that the very good accuracy we obtained would also be found in 3D use cases and that it is likely that participants would also feel like they are interacting the virtual slider with realistic haptic feedback. Concerning the observed delay measured in the use cases, it could be significantly reduced by increasing the robot speed or designing a unique and smoother path of movement from the base position to various destinations.</p><p>Second, we have so far tested only a slider and rotary encoder as controls attached to the robot arm. There are many other physical controls and actuators that could be attached, in order, for example, to simulate an entire cockpit of controls. Similarly, there may be application of a system like ours to provide access to data for blind or low vision (BLV) people. Past work has found benefit in static tactile graphics and 3D models for BLV people <ref type="bibr" target="#b13">[14]</ref>. In future we would like to explore whether a system like ours could be a general solution to simulate actions with realistic haptic feedback for BLV people in their training or education.</p><p>Another exciting avenue for future work lies in extending the range of user interactions through different hardware and combinations of devices. Our UR3 robot is the most affordable model in that suppliers' series of "collaborative robot arms" (safe for direct human-robot interaction), but similar robots with up to approximately double the range are available, and one may envision combining sets of robots to fully surround the users. Mounting the robots on wheeled bases would extend the range even further, allowing the robots to follow users as they walk around the virtual environment. Of course, with greater range and complexity, safety may become a concern, or an opportunity for intelligent robot uprising.</p><p>Further study would also test "robohapalytics" in different application domains and use user-centred and collaborative design techniques to seek critical and formative feedback from domain experts. Further, while we are satisfied that the speed, accuracy and presence feedback collected in our controlled study demonstrates the viability and usability of our dynamic slider system to simulate a slider with range that is physically larger than the range of the slider track, a logical next step would be to test the dynamic reorientation of the slider in interactive scenarios such as those presented in Sections Sect. 7.2 and Sect. 7.3. In particular, it would be interesting to see if there are limits to the degree of curvature of non-linear trajectories that can be usefully tracked by the robot in scenarios similar to the DimpVis use case.</p><p>Finding a markerless alternative for the hand tracking system would also have significant improvement on the overall immersive experience if bare-hand interaction is allowed. Another interesting possibility for future study would be to test how robot maneuvered data physicalisations can support remote collaboration. For example, an assumption would be that haptic representations of data that is manipulated by other users would give a stronger sense of physical collocation than purely visual representations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. System connection diagram. A TCP server is used for communication between robot and Unity, and an Arduino is used for communication between physical pops and Unity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Displacements during interaction: D r , Displacement of the robotic arm with regard to physical world; d s , Displacement of the physical slider knob with regard to the slider itself; and d s , Displacement of the virtual slider knob with regard to the virtual world.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Diagram of the coverage of the robotic arm (blue area), the base position for reset (yellow point), and the permitted area for user study (green area) from top-down view (Left), and the side view of the robotic arm with slider and button (Right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. VR scene of the study. Users the virtual knob and point to the yellow marker with the help of a red-arrow.</figDesc><graphic url="image-80.png" coords="5,315.01,73.32,230.39,113.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .Fig. 9 .</head><label>89</label><figDesc>Fig. 8. Mean completion time for each condition. Errors bars are 95% Confidence intervals (CIs).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 10 .Fig. 11 .</head><label>1011</label><figDesc>Fig. 10. Average absolute error (in mm). Error bars: 95% CIs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Subscales of the IPQ questionnaires. IPQ: General Presence (GP), spatial presence (SP), involvement (INV), and realism (REAL).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Mean completion time for each range. Error bars: 95% CIs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 14 .Fig. 15 .</head><label>1415</label><figDesc>Fig. 14. Mean error for each range. Error bars: 95% CIs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 16 .</head><label>16</label><figDesc>Fig.<ref type="bibr" target="#b15">16</ref>. Interacting with the time-series selector in both physical world (Left) and the virtual world (right).</figDesc><graphic url="image-81.png" coords="8,64.14,72.88,99.37,76.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 17 .</head><label>17</label><figDesc>Fig. 17. One robotic arm and one mounted slider represents multiple slider controls and reaches to the required position according to hand position (left). Select a slicing plane of a skeleton in the virtual world and move among in three axis direction (right).</figDesc><graphic url="image-89.png" coords="8,314.04,225.51,99.28,77.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 18 .</head><label>18</label><figDesc>Fig.<ref type="bibr" target="#b17">18</ref>. Robotic arm holding the rotary encoder for rotational interactions according to hand position (left). Select the rotary control in the virtual world and rotate the virtual skeleton around that axis (right).</figDesc><graphic url="image-90.png" coords="8,412.91,225.51,99.48,77.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Fig</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>17</head><label>17</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 19 .</head><label>19</label><figDesc>Fig. 19. Selecting a point data in a scatter plot reveals its value changes with time. Robotic arm holding the slider to match that point data spatial position as well as the gradient of the value change (left) while virtual hand is in touch with point data (right).</figDesc><graphic url="image-93.png" coords="9,64.24,72.88,99.33,77.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>• Lonni besancon is with Linköping University. E-mail:</figDesc><table /><note>lonni.besancon@gmail.com Manuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication xx xxx. 201x; date of current version xx xxx. 201x. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org. Digital Object Identifier: xx.xxxx/TVCG.201x.xxxxxxx</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Ranking of each condition with best and worst highlights.</figDesc><table><row><cell cols="3">Technique Mean Median</cell><cell>SD</cell><cell cols="3">1st 2nd 3rd</cell></row><row><cell>Virtual</cell><cell>2.43</cell><cell>3</cell><cell>0.817</cell><cell>6</cell><cell>5</cell><cell>19</cell></row><row><cell>Physical</cell><cell>1.67</cell><cell>2</cell><cell cols="2">0.711 14</cell><cell>12</cell><cell>4</cell></row><row><cell>Dynamic</cell><cell>1.9</cell><cell>2</cell><cell cols="2">0.759 10</cell><cell>13</cell><cell>7</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://openbuildspartstore.com/ v-slot-mini-v-linear-actuator-bundle/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://www.universal-robots.com/download/ manuals-cb-series/script/script-manual-cb-series-sw33/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://github.com/szdai2021/Universal-Robot.git</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">ACKNOWLEDGEMENTS</head><p>The authors wish to thank their participants. This research was supported under the Australian Research Council's Discovery Projects funding scheme (project number DP180100755) and the Knut and Alice Wallenberg Foundation (grant KAW 2019.0024).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hapticdrone: An encountered-type kinesthetic haptic interface with controllable force feedback: Example of stiffness and weight rendering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abdullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kuroda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jeon</surname></persName>
		</author>
		<idno type="DOI">10.1109/HAPTICS.2018.8357197</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Haptics Symposium (HAPTICS)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="334" to="339" />
		</imprint>
	</monogr>
	<note>Open Access</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scientists rise up against statistical significance</title>
		<author>
			<persName><forename type="first">V</forename><surname>Amrhein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Greenland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcshane</surname></persName>
		</author>
		<idno type="DOI">10.1038/d41586-019-00857-9</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">567</biblScope>
			<biblScope unit="issue">7748</biblScope>
			<biblScope unit="page" from="305" to="307" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Open Access</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Snake charmer: Physically enabling virtual objects</title>
		<author>
			<persName><forename type="first">B</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Perumal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wigdor</surname></persName>
		</author>
		<idno type="DOI">10.1145/2839462.2839484</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. TEI</title>
				<meeting>TEI</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="218" to="226" />
		</imprint>
	</monogr>
	<note>Open Access</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3D mobile data visualization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Besanc ¸on</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Aigner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dwyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Isenberg</surname></persName>
		</author>
		<idno type="DOI">10.1201/9781003090823-4</idno>
	</analytic>
	<monogr>
		<title level="m">Mobile Data Visualization</title>
				<editor>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Dachselt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Isenberg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Choe</surname></persName>
		</editor>
		<meeting><address><addrLine>Boca Raton, FL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>CRC Press</publisher>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">La Différence Significative entre Valeurs p et Intervalles de Confiance</title>
		<author>
			<persName><forename type="first">L</forename><surname>Besanc ¸on</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dragicevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">29ème conférence francophone sur l&apos;Interaction Homme-Machine</title>
				<meeting><address><addrLine>Poitiers, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08">Aug. 2017</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Alt.IHM</note>
	<note>AFIHM</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Continued Prevalence of Dichotomous Inferences at CHI</title>
		<author>
			<persName><forename type="first">L</forename><surname>Besanc ¸on</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dragicevic</surname></persName>
		</author>
		<idno type="DOI">10.1145/3290607.3310432</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI Extended Abstracts</title>
				<meeting>CHI Extended Abstracts<address><addrLine>Glasgow, United Kingdom</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-05">May 2019</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hybrid Tactile/Tangible Interaction for 3D Data Exploration</title>
		<author>
			<persName><forename type="first">L</forename><surname>Besanc ¸on</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Issartel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ammi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Isenberg</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2016.2599217</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comp. Graph</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="881" to="890" />
			<date type="published" when="2017-01">Jan. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mouse, tactile, and tangible input for 3D manipulation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Besanc ¸on</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Issartel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ammi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Isenberg</surname></persName>
		</author>
		<idno type="DOI">10.1145/3025453.3025863</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI</title>
				<meeting>CHI<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4727" to="4740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Definitely maybe: Hedges and boosters in the hci literature</title>
		<author>
			<persName><forename type="first">L</forename><surname>Besanc ¸on</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cockburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dragicevic</surname></persName>
		</author>
		<idno type="DOI">10.31219/osf.io/mjg7h</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Open science saves lives: lessons from the covid-19 pandemic</title>
		<author>
			<persName><forename type="first">L</forename><surname>Besanc ¸on</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Peiffer-Smadja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Segalas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Masuzzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Smout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Billy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Deforet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leyrat</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12874-021-01304-y</idno>
	</analytic>
	<monogr>
		<title level="j">BMC Medical Research Methodology</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Open Access</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hybrid Touch/Tangible Spatial 3D Data Selection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Besanc ¸on</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sereno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ammi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Isenberg</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.13710</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="553" to="567" />
			<date type="published" when="2019-06">June 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The State of the Art of Spatial Interfaces for 3D Visualization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Besanc ¸on</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ynnerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Keefe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Isenberg</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="293" to="326" />
			<date type="published" when="2021-02">Feb. 2021</date>
		</imprint>
	</monogr>
	<note>Open Access</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Interaction for immersive analytics</title>
		<author>
			<persName><forename type="first">W</forename><surname>Büschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dachselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dwyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Görg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Isenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kerren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>North</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Stuerzlinger</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01388-24</idno>
	</analytic>
	<monogr>
		<title level="m">Immersive Analytics</title>
				<editor>
			<persName><forename type="first">K</forename><surname>Marriott</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Schreiber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Dwyer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Klein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Riche</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Itoh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Stuerzlinger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Thomas</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin/Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="95" to="138" />
		</imprint>
	</monogr>
	<note>Open Access</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Technology developments in touch-based accessible graphics: A systematic review of research 2010-2020</title>
		<author>
			<persName><forename type="first">M</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Holloway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reinders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Goncu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Marriott</surname></persName>
		</author>
		<idno type="DOI">10.1145/3411764.3445207</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI</title>
				<meeting>CHI</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Volume slicing display</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cassinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ishikawa</surname></persName>
		</author>
		<idno type="DOI">10.1145/1665137.1665207</idno>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH ASIA Art Gallery &amp; Emerging Technologies</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page">88</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Threats of a replication crisis in empirical computer science</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cockburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dragicevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Besanc ¸on</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gutwin</surname></persName>
		</author>
		<idno type="DOI">10.1145/3360311</idno>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="70" to="79" />
			<date type="published" when="2020-07">July 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Embodied axes: Tangible, actuated interaction for 3d augmented reality data spaces</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cordeil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Montoya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dwyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI</title>
				<meeting>CHI</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Design space for spatio-data coordination: Tangible interaction devices for immersive information visualisation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cordeil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongchao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dwyer</surname></persName>
		</author>
		<idno type="DOI">10.1109/PACIFICVIS.2017.8031578</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Pacific Visualization Symposium (PacificVis)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="46" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Towards intuitive exploration tools for data visualization in VR</title>
		<author>
			<persName><forename type="first">G</forename><surname>De Haan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koutek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Post</surname></persName>
		</author>
		<idno type="DOI">10.1145/585740.585758</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. VRST</title>
				<meeting>VRST<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fair statistical communication in HCI</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dragicevic</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-26633-613</idno>
	</analytic>
	<monogr>
		<title level="m">Modern Statistical Methods for HCI</title>
				<editor>
			<persName><forename type="first">J</forename><surname>Robertson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kaptein</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="291" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A Mean Difference is an Effect Size</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dragicevic</surname></persName>
		</author>
		<idno>RR-9354</idno>
		<imprint>
			<date type="published" when="2020-07">July 2020</date>
		</imprint>
		<respStmt>
			<orgName>Inria Saclay Ile de France</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Research Report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Teslamirror: Multistimulus encounter-type haptic display for shape and texture rendering in vr</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fedoseev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tleugazy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Labazanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsetserukou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2020 Emerging Technologies</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tangi: Tangible proxies for embodied object exploration and manipulation in virtual reality</title>
		<author>
			<persName><forename type="first">M</forename><surname>Feick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bateman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Miede</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Marquardt</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISMAR50242.2020.00042</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="195" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Graspable User Interfaces</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Fitzmaurice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Canada</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
	<note>Open Access</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Jamming user interfaces: Programmable particle stiffness and sensing for malleable and shape-changing devices</title>
		<author>
			<persName><forename type="first">S</forename><surname>Follmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Leithinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Olwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ishii</surname></persName>
		</author>
		<idno type="DOI">10.1145/2380116.2380181</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. UIST, UIST &apos;12</title>
				<meeting>UIST, UIST &apos;12<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="519" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Inform: Dynamic physical affordances and constraints through shape and object actuation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Follmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Leithinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Olwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hogge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ishii</surname></persName>
		</author>
		<idno type="DOI">10.1145/2501988.2502032</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. UIST, UIST &apos;13</title>
				<meeting>UIST, UIST &apos;13<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="417" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mindless statistics</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gigerenzer</surname></persName>
		</author>
		<idno>1016/j.socec.2004.09.033</idno>
	</analytic>
	<monogr>
		<title level="j">The Journal of Socio-Economics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="587" to="606" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A fiducial-based tangible user interface for white matter tractography</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jianu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Laidlaw</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-17274-837</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Visual Computing</title>
				<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="373" to="381" />
		</imprint>
	</monogr>
	<note>Open Access</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reach+ extending the reachability of encountered-type haptics devices through dynamic redirection in vr</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abtahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Follmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. UIST</title>
				<meeting>UIST</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="236" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Human-robot interaction: a survey</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Goodrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Schultz</surname></persName>
		</author>
		<idno type="DOI">10.1561/1100000005</idno>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="203" to="275" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Plane, ray, and point: Enabling precise spatial manipulations with shape constraints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hayatpur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Stuerzlinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wigdor</surname></persName>
		</author>
		<idno type="DOI">10.1145/3332165.3347916</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. UIST, UIST &apos;19</title>
				<meeting>UIST, UIST &apos;19<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1185" to="1195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Can visualization alleviate dichotomous thinking? effects of visual representations on the cliff effect</title>
		<author>
			<persName><forename type="first">J</forename><surname>Helske</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Helske</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ynnerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Besanc ¸on</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2021.3073466</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Visualization &amp; Computer Graphics</title>
				<imprint>
			<date type="published" when="2021-04">apr 2021</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3397" to="3409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Passive real-world interface props for neurosurgical visualization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hinckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pausch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Goble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">F</forename><surname>Kassell</surname></persName>
		</author>
		<idno type="DOI">10.1145/191666.191821</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI</title>
				<meeting>CHI<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="452" to="458" />
		</imprint>
	</monogr>
	<note>Open Access</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Tangible bits: Towards seamless interfaces between people, bits and atoms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ullmer</surname></persName>
		</author>
		<idno type="DOI">10.1145/258549.258715</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI</title>
				<meeting>CHI<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Slicing techniques for handheld augmented reality</title>
		<author>
			<persName><forename type="first">P</forename><surname>Issartel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Guéniat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ammi</surname></persName>
		</author>
		<idno type="DOI">10.1109/3DUI.2014.6798839</idno>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
				<meeting>null<address><addrLine>Los Alamitos</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="39" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A lightweight tangible 3D interface for interactive visualization of thin fiber structures</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Toussaint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Keefe</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2013.121</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comp. Graph</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2802" to="2809" />
			<date type="published" when="2013-12">Dec. 2013</date>
		</imprint>
	</monogr>
	<note>Open Access</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Tangible Remote Controllers for Wall-Size Displays</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dragicevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Fekete</surname></persName>
		</author>
		<idno type="DOI">10.1145/2207676.2208691</idno>
	</analytic>
	<monogr>
		<title level="m">ACM annual conference on Human Factors in Computing Systems. CHI &apos;12</title>
				<meeting><address><addrLine>Austin, TX, United States</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012-05">May 2012</date>
			<biblScope unit="page" from="2865" to="2874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Evaluating the Efficiency of Physical Visualizations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dragicevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Fekete</surname></persName>
		</author>
		<idno type="DOI">10.1145/2470654.2481359</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI</title>
				<meeting>CHI<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013-04">Apr. 2013</date>
			<biblScope unit="page" from="2593" to="2602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Encountered-type haptic display for large vr environment using per-plane reachability maps</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1002/cav.1814</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Animation and Virtual Worlds</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dimpvis: Exploring time-varying information visualizations by direct manipulation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kondo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Collins</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2014.2346250</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comp. Graph</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2003" to="2012" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Points of significance: Error bars</title>
		<author>
			<persName><forename type="first">M</forename><surname>Krzywinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Altman</surname></persName>
		</author>
		<idno type="DOI">10.1038/nmeth.2659</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="921" to="922" />
			<date type="published" when="2013-10">Oct. 2013</date>
		</imprint>
	</monogr>
	<note>Open Access</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Zooids: Building blocks for swarm user interfaces</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Le</forename><surname>Goc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parsaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Fekete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dragicevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Follmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th annual symposium on user interface software and technology</title>
				<meeting>the 29th annual symposium on user interface software and technology</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="97" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Robot telekinesis: application of a unimanual and bimanual object manipulation technique to robot control</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-G</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Bae</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICRA40945.2020.9197517</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE international conference on robotics and automation (ICRA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9866" to="9872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sublimate: State-changing virtual and physical rendering to augment interaction with shape displays</title>
		<author>
			<persName><forename type="first">D</forename><surname>Leithinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Follmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Olwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luescher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hogge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ishii</surname></persName>
		</author>
		<idno type="DOI">10.1145/2470654.2466191</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI, CHI &apos;13</title>
				<meeting>CHI, CHI &apos;13<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1441" to="1450" />
		</imprint>
	</monogr>
	<note>Open Access</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Relief: A scalable actuated shape display</title>
		<author>
			<persName><forename type="first">D</forename><surname>Leithinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ishii</surname></persName>
		</author>
		<idno type="DOI">10.1145/1709886.1709928</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. TEI, TEI &apos;10</title>
				<meeting>TEI, TEI &apos;10<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="221" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Using variable movement resistance sliders for remote discrete input</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lischke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Woundefinedniak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Preikschat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fjeld</surname></persName>
		</author>
		<idno type="DOI">10.1145/3132272.3134135</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. ISS, ISS &apos;17</title>
				<meeting>ISS, ISS &apos;17<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="116" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Towards an understanding of mobile touch navigation in a stereoscopic viewing environment for 3D data exploration</title>
		<author>
			<persName><forename type="first">D</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Oehlberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Doger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Isenberg</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2015.2440233</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comp. Graph</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1616" to="1629" />
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
	<note>Open Access</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Ynnerman. Enabling design and interactive selection of haptic modes. Virtual Reality</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lundin ; Palmerius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">)</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Persson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Evestedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<idno type="DOI">10.1007/s10055-006-0033-7</idno>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Haptic visualization of computational fluid dynamics data using reactive forces</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lundin ; Palmerius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">)</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sillé;N</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ynnerman</surname></persName>
		</author>
		<idno type="DOI">10.1117/12.587029</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Visualization and Data Analysis</title>
				<meeting><address><addrLine>San Jose, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="95" to="102" />
		</imprint>
	</monogr>
	<note>Open Access</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">K</forename><surname>Marriott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schreiber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dwyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Riche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Itoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Stuerzlinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Thomas</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01388-2</idno>
	</analytic>
	<monogr>
		<title level="j">Immersive Analytics</title>
		<imprint>
			<biblScope unit="volume">11190</biblScope>
			<date type="published" when="2018">2018</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fighting for control: Children&apos;s embodied interactions when using physical and digital representations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hornecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yuill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Dalton</surname></persName>
		</author>
		<idno type="DOI">10.1145/1518701.1519027</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI</title>
				<meeting>CHI<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2149" to="2152" />
		</imprint>
	</monogr>
	<note>Open Access</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Alfred: the haptic butler on-demand tangibles for object manipulation in virtual reality using an ethd</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">R</forename><surname>Mercado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Si-Mohammed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Argelaguet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lécuyer</surname></persName>
		</author>
		<idno type="DOI">10.1109/WHC49131.2021.9517250</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. WHC</title>
				<meeting>WHC</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="373" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">It&apos;s just a toolbar!&quot; Using tangibles to help children manage conflict around a multi-touch tabletop</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">C</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Wilensky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Horn</surname></persName>
		</author>
		<idno type="DOI">10.1145/1935701.1935709</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. TEI</title>
				<meeting>TEI<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="29" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Lumen: Interactive visual and shape display for calm computing</title>
		<author>
			<persName><forename type="first">I</forename><surname>Poupyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nashida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maruyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rekimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yamaji</surname></persName>
		</author>
		<idno type="DOI">10.1145/1186155.1186173</idno>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2004 Emerging Technologies, SIGGRAPH &apos;04</title>
				<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Morphees: Toward high &quot;shape resolution&quot; in self-actuated flexible mobile devices</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roudaut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Löchtefeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<idno type="DOI">10.1145/2470654.2470738</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI, CHI &apos;13</title>
				<meeting>CHI, CHI &apos;13<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="593" to="602" />
		</imprint>
	</monogr>
	<note>Open Access</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Tangible globes for data visualisation in augmented reality</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Satriadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Smiley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cordeil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Czauderna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dwyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jenny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM CHI</title>
				<meeting>ACM CHI</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Average task times in usability tests: What to report?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sauro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="DOI">10.1145/1753326.1753679</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI, CHI &apos;10</title>
				<meeting>CHI, CHI &apos;10<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2347" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Immersive design of DNA molecules with a tangible interface</title>
		<author>
			<persName><forename type="first">S</forename><surname>Schkolne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schroder</surname></persName>
		</author>
		<idno type="DOI">10.1109/VISUAL.2004.47</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Visualization</title>
				<meeting>Visualization<address><addrLine>Los Alamitos</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="227" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Using presence questionnaires in virtual reality</title>
		<author>
			<persName><forename type="first">V</forename><surname>Schwind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Knierim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Henze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI</title>
				<meeting>CHI</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Hybrid Touch/Tangible Spatial Selection in Augmented Reality</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sereno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gosset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Besanc ¸on</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Isenberg</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.14550</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="403" to="415" />
			<date type="published" when="2022-06">June 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">The made-axis: A modular actuated device to embody the axis of a data dimension</title>
		<author>
			<persName><forename type="first">J</forename><surname>Smiley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cordeil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Besanc ¸on</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Knibbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dwyer</surname></persName>
		</author>
		<idno type="DOI">10.1145/3488546</idno>
	</analytic>
	<monogr>
		<title level="j">Proc. ACM Hum.-Comput. Interact</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2021-11">nov 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">WYSIWYF: Exploring and annotating volume data with a tangible handheld device</title>
		<author>
			<persName><forename type="first">P</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<idno type="DOI">10.1145/1978942.1979140</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI</title>
				<meeting>CHI<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1333" to="1342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">PaperLens: Advanced magic lens interaction above the tabletop</title>
		<author>
			<persName><forename type="first">M</forename><surname>Spindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stellmach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dachselt</surname></persName>
		</author>
		<idno type="DOI">10.1145/1731903.1731920</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. ITS</title>
				<meeting>ITS<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="69" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Tangible views for information visualization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Spindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tominski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dachselt</surname></persName>
		</author>
		<idno type="DOI">10.1145/1936652.1936684</idno>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Interactive Tabletops and Surfaces, ITS &apos;10</title>
				<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="157" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">The ultimate display</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">E</forename><surname>Sutherland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IFIP Congress</title>
				<meeting>IFIP Congress</meeting>
		<imprint>
			<date type="published" when="1965">1965</date>
			<biblScope unit="page" from="506" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Exploring interactions with physically dynamic bar charts</title>
		<author>
			<persName><forename type="first">F</forename><surname>Taher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hardy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Weichel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hornbaek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Alexander</surname></persName>
		</author>
		<idno type="DOI">10.1145/2702123.2702604</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI, CHI &apos;15</title>
				<meeting>CHI, CHI &apos;15<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3237" to="3246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Development of a surgical simulator for training retraction of tissue with an encountered-type haptic interface using mr fluid</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tsujita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tomita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Konno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nakayama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nakagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Abe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Uchiyama</surname></persName>
		</author>
		<idno type="DOI">10.1109/ROBIO.2018.8665269</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Robotics and Biomimetics</title>
		<imprint>
			<biblScope unit="page" from="898" to="903" />
			<date type="published" when="2018">2018. 2018</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Haptic palpation for medical simulation in virtual environments</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ullrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kuhlen</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2012.46</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comp. Graph</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="617" to="625" />
			<date type="published" when="2012-04">Apr. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Haptic rendering techniques for the interactive exploration of CFD datasets in virtual environments</title>
		<author>
			<persName><forename type="first">T</forename><surname>Van Reimersdahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kuhlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Bischof</surname></persName>
		</author>
		<idno type="DOI">10.1145/769953.769981</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop on Virtual Environments</title>
				<meeting>Workshop on Virtual Environments<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="241" to="246" />
		</imprint>
	</monogr>
	<note>Open Access</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Publication Manual of the American Psychological Association</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Vandenbos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>American Psychological Association</publisher>
			<pubPlace>Washington, DC, 6</pubPlace>
		</imprint>
	</monogr>
	<note>th ed.</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Tips for improving safety, roi for collaborative robots: collaborative robots are becoming more common on the plant floor. deciding if they should be used requires considering safety and business goals and return on investment (roi). see new safety guidance in a technical specification, iso/ts 15066: 2016 robots and robotic devices-collaborative robots</title>
		<author>
			<persName><forename type="first">C</forename><surname>Vavra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Control Engineering</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="26" to="30" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Vrrobot: Robot actuated props in an infinite virtual environment</title>
		<author>
			<persName><forename type="first">E</forename><surname>Vonach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gatterer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kaufmann</surname></persName>
		</author>
		<idno type="DOI">10.1109/VR.2017.7892233</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Virtual Reality</title>
		<imprint>
			<biblScope unit="page" from="74" to="83" />
			<date type="published" when="2017">2017. 2017</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Understanding differences between combinations of 2d and 3d input and output devices for 3d data visualization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Besanc ¸on</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ammi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Isenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Studies</title>
		<imprint>
			<biblScope unit="page">102820</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A virtual reality keyboard with realistic haptic feedback in a fully immersive virtual environment</title>
		<author>
			<persName><forename type="first">C.-M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10055-016-0296-6</idno>
	</analytic>
	<monogr>
		<title level="j">Virtual Reality</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="29" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Open Access</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Are tangibles more fun? Comparing children&apos;s enjoyment and engagement using physical, graphical and tangible user interfaces</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Antle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Motamedi</surname></persName>
		</author>
		<idno type="DOI">10.1145/1347390.1347433</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. TEI</title>
				<meeting>TEI<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
	<note>Open Access</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">A non-grounded and encountered-type haptic display using a drone</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kuroda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kiyokawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Takemura</surname></persName>
		</author>
		<idno type="DOI">10.1145/2983310.2985746</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. SUI</title>
				<meeting>SUI</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="43" to="46" />
		</imprint>
	</monogr>
	<note>Open Access</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
