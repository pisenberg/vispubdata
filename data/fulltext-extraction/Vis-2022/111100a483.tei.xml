<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">D-BIAS: A Causality-Based Human-in-the-Loop System for Tackling Algorithmic Bias</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bhavya</forename><surname>Ghai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Klaus</forename><surname>Mueller</surname></persName>
						</author>
						<title level="a" type="main">D-BIAS: A Causality-Based Human-in-the-Loop System for Tackling Algorithmic Bias</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-03-27T09:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Algorithmic Fairness</term>
					<term>Causality</term>
					<term>Debiasing</term>
					<term>Human-in-the-loop</term>
					<term>Visual Analytics</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. 1. The visual interface of our D-BIAS tool. (A) The Generator panel: used to create the causal network and download the debiased dataset (B) The Causal Network view: shows the causal relations between the attributes of the data, allows user to inject their prior in the system (C) The Evaluation panel: used to choose the sensitive variable, the ML model and displays different evaluation metrics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>When computer systems discriminate based on an individual's inherent characteristic such as gender, or acquired traits such as nationality,</p><p>• Bhavya <ref type="bibr">Ghai and</ref>  which are protected classes under law and are irrelevant to the decision making process, it constitutes algorithmic bias. A simple way to deal with this problem can be to remove the sensitive attribute such as race before training the machine learning (ML) model. However, algorithmic bias can still persist via proxy variables such as zipcode that are correlated with the sensitive attribute. Recent years have seen a huge surge in research papers that deal with this problem. These papers have largely focused on pure algorithmic means to detect and remove bias at different stages of the ML pipeline. However, fairness is contextual and thus cannot be achieved using fully automated methods <ref type="bibr" target="#b39">[40]</ref>. Moreover, existing techniques are largely black boxes, offering only limited insight on the proxy variables and how bias is mitigated. Finally, they are also limited in providing capabilities that allow users to actively steer and control the debiasing process. Given this limited transparency and human control, accountability and trust become a major concern.</p><p>To address these needs, we hypothesize that a human-in-the-loop (HITL) approach is the way forward. A human expert can determine what fairness means in a given context. Such domain knowledge can be incorporated effectively via the HITL approach and hence improve perceived fairness. Introducing a human into the loop will only be effective when a person can understand the underlying state of the system and provide useful feedback. Hence, this approach is naturally inclined towards interpretability. On the trust aspect, people are more likely to trust a system if they can tinker with it, even if it meant making it perform imperfectly <ref type="bibr" target="#b13">[14]</ref>. Human interaction is a core part of the HITL approach, so it might instill more trust. Lastly, this approach should also foster accountability as the human has a much bigger role to play, which can significantly impact the results.</p><p>We present D-BIAS, a visual interactive tool that embodies a HITL approach for bias identification and mitigation. Given a tabular dataset with meaningful column names as input, D-BIAS assists users in auditing the data for different biases and then helps generate its debiased version (see Fig. <ref type="figure" target="#fig_0">2</ref>). It uses a graphical causal model as a medium for users to visualize the causal structures inherent in the data and to inject their domain knowledge. We have made use of causal models since discrimination is inherently causal, and causal models can also be easy to interpret <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b48">49]</ref>. Apart from causal model, D-BIAS also includes multiple statistical fairness metrics to help identify bias. Users can choose to compare between two groups based on a single variable say gender (Male, Female) or a combination of attributes say race and gender (Black Females, White Males). Thereafter, they can inject their domain knowledge by acting on the edges of the causal network, for instance by deleting or weakening biased causal edges. Since the causal model encodes the data-generating mechanism, any user intervention modifies that process. Following each change, the system generates a new dataset based on the current causal model while keeping track and visualizing the impact of the user interventions on utility, data distortion and various fairness metrics. Users can interact with the system until they are satisfied with the outcome and then download the debiased dataset for use in any downstream ML application to achieve fairer predictions. The major contributions of our work are:</p><p>• A novel human-in-the-loop method to debias tabular datasets.</p><p>• An end-to-end visual interactive tool for algorithmic bias identification and mitigation. • A demonstration of the effectiveness of our tool in reducing bias using three datasets. • A user study to evaluate our tool on human-centric measures like usability, trust, interpretability, accountability, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND RELATED WORK 2.1 Bias Identification</head><p>The existing literature on bias identification mostly revolves around different fairness metrics. Numerous fairness metrics have been proposed which capture different facets of fairness, such as group fairness, individual fairness, counterfactual fairness, etc. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33]</ref>. Another way to classify fairness metrics can be on the level they operate on. For eg., dataset based metrics are solely computed using the dataset and are independent of any ML model, such as statistical parity difference. On the other hand, classifier based metrics are computed over the predictions of a trained ML model, such as false negative rate difference. So far, there is not a single best fairness metric. Moreover, some of the fairness metrics can be mutually incompatible, i.e., it is impossible to optimize different metrics simultaneously <ref type="bibr" target="#b26">[27]</ref>. In line with existing visual tools <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b45">46]</ref>, our tool also uses a diverse set of fairness metrics to present a more comprehensive picture.</p><p>Many fairness metrics solely focus on the aggregate relationship between the sensitive attribute and the output variable. This can lead to misleading conclusions as the aggregate trend might disappear or reverse when accounting for other relevant factors. A prime example of this phenomenon, also known as Simpson's paradox <ref type="bibr" target="#b34">[35]</ref>, is the Berkeley Graduate Admission dataset <ref type="bibr" target="#b5">[6]</ref>. There it appeared as if the admission process was biased against women since the overall admit rate for men (44%) was significantly higher than for women (30%) <ref type="bibr" target="#b1">[2]</ref>. However, this correlation/association did not account for the fact that women typically applied for more competitive departments than men. After correcting for this factor, it was found that the admission process had a small but statistically significant bias in favor of women <ref type="bibr" target="#b5">[6]</ref>. Causal models can be an effective tool for dealing with such a situation as they can decipher the different intermediate factors (indirect effects) along with their respective contributions behind an aggregate trend. Hence, our tool also employs causal model for bias identification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Bias Mitigation</head><p>The existing literature on algorithmic bias mitigation can be broadly categorized into the three different stages in which they operate within the ML pipeline, namely pre-processing, in-processing and post-processing. In the pre-processing stage, the dataset is modified such that the bias with respect to an attribute or set of attributes is reduced or eliminated <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b47">48]</ref>. This can be achieved by either modifying the output label <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> or by modifying the input attributes <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b47">48]</ref>. In the in-processing stage, the algorithm is designed to take in biased data but still generate unbiased predictions. This can be achieved by tweaking the learning objectives such that accuracy is optimized while discrimination is minimized <ref type="bibr" target="#b2">[3]</ref>. Finally, in the post-processing stage, predictions from ML algorithms are modified to counter bias <ref type="bibr" target="#b28">[29]</ref>. Our work relates closely with the pre-processing stage where we make changes to the input attributes and the output label.</p><p>There is also a growing set of work at the intersection of bias mitigation and causality <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b49">50]</ref>. The general idea is to generate the causal network, modify it, and then simulate debiased data. These approaches fully rely on automated techniques to yield the true causal network, and assume a priori knowledge about fair/unfair causal relationships. Our work draws inspiration from this line of work and presents a more general solution where human domain knowledge is leveraged to refine the causal network and generate the debiased dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Visual Tools</head><p>Recent years have seen visual tools like Silva <ref type="bibr" target="#b45">[46]</ref>, FairVis <ref type="bibr" target="#b7">[8]</ref>, Fair-RankVis <ref type="bibr" target="#b43">[44]</ref>, DiscriLens <ref type="bibr" target="#b41">[42]</ref>, FairSight <ref type="bibr" target="#b0">[1]</ref>, WordBias <ref type="bibr" target="#b17">[18]</ref>, etc. which are all aimed at tackling algorithmic bias. Although most of these tools are focused on bias identification, a few of them, such as FairSight, also permit debiasing. However, the debiasing strategy used in such tools is fairly basic, like eliminating proxy variable(s). Simple measures like this can lead to high data distortion and can have a high negative impact on data utility. Our work relates more closely with Silva which also features a graphical causal model in its interface. Using an empirical study, it showed that users can interpret causal networks and found them helpful in identifying algorithmic bias. However like most other visual tools, Silva is limited to bias identification. Our work advances the state of the art by presenting a tool that supports both bias identification and mitigation, with our debiasing strategy being more nuanced and sensitive toward data distortion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>The workflow of our system can be understood from Fig. <ref type="figure" target="#fig_1">3</ref>. A detailed discussion for each stage follows next. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Generating the Causal Model</head><p>A causal model generally consists of two parts: causal graph (skeleton) and statistical model. Given a tabular dataset as input, we first infer an initial causal graph (directed acyclic graph) using a popular causal discovery algorithm, namely the PC algorithm <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b38">39]</ref>. This algorithm relies on conditional independence tests and a set of orientation rules to yield the causal graph (for details, see appendix B). Each node in the causal graph represents a data attribute and each edge represents a causal relation. For example, a directed edge from X to Y signifies that X is the cause of Y, i.e., a change in X will cause a change in Y. The PC algorithm provides qualitative information about the presence of causal relationship between variables. To quantitatively estimate the strength of each causal relationship, we use linear structural equation models (SEM) where a node is modeled as a linear function of its parent nodes.</p><formula xml:id="formula_0">y = parents(y) ∑ i β i x i + ε (1)</formula><p>In the above equation, variable y is modeled as a linear combination of its parents x i , their regression coefficients (β i ) and the intercept term (ε). If y is a numerical variable, we use linear regression else we use the multinomial logit model to estimate the values of β i and ε. Here, β i represents the strength of causal relation between x i and y. We repeat this modeling process for each node with non-zero parent nodes. Such nodes that have at least one edge leading into them are termed as endogenous variables. Other nodes correspond to exogenous variables or independent variables that have no parent nodes. In Fig. <ref type="figure" target="#fig_1">3</ref> (Refined Causal Model), s and a are exogenous variables while b, c and y are endogenous variables. Here, y will be modeled as a function of its parent nodes, i.e., a, b and c. Similarly, b and c will be modeled as function of s. After this process, we arrive at a causal graph whose different edges are parameterized using SEM. This constitutes a Structural Causal Model (SCM) or simply causal model (see Fig. <ref type="figure" target="#fig_1">3 (1)</ref>).</p><p>This causal model, generated using automated algorithm, might have some undirected/erroneous causal edges due to different factors like sampling bias, missing attributes, etc. To achieve a reliable causal model, we have taken a similar approach as Wang and Mueller <ref type="bibr" target="#b40">[41]</ref> where we leverage user knowledge to refine the causal model via operations like adding, deleting, directing, and reversing causal edges (see Fig. <ref type="figure" target="#fig_1">3</ref> (2)). For every operation, the system computes a score (Bayesian Information Criterion (BIC)) of how well the modified causal model fits the underlying dataset. Similar to <ref type="bibr" target="#b40">[41]</ref>, our system assists the user in refining the causal model by providing the difference in BIC score before and after the change. A negative BIC score suggests a better fit. After achieving a reliable causal model, we enter into the debiasing stage where any changes made to the causal model will reflect on the debiased dataset (see Fig. <ref type="figure" target="#fig_1">3</ref> (3)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Auditing and Mitigating Social Biases</head><p>From a causal perspective, discrimination can be defined as an unfair causal effect of the sensitive attribute (say, race) on the outcome variable (say, getting a loan) <ref type="bibr" target="#b33">[34]</ref>. A direct causal path from a sensitive variable to the output variable constitutes disparate treatment while an unfair indirect path via some proxy attribute (say, zipcode) constitutes disparate impact <ref type="bibr" target="#b11">[12]</ref>. A direct path is certainly unfair but an indirect path may be fair or unfair (as in the case of Berkeley Admission dataset <ref type="bibr" target="#b34">[35]</ref>). Our system computes all causal paths and lets the user decide if a causal path is fair or unfair. If a causal path is unfair, the user should identify which constituting causal relationship(s) are unfair and act on them. The user can do this by deleting or weakening such biased causal relationships to reduce/mitigate the impact of the sensitive attribute on the outcome variable. For example, in Fig. <ref type="figure" target="#fig_1">3</ref> (Refined Causal Model), s is the sensitive attribute and y is the outcome variable. Here, the user deletes the edge s → b and weakens the edge s → c (shown as a thin edge). Once the user has dealt with the biased causal relationships, we achieve what we call the Debiased Causal Model.  </p><formula xml:id="formula_1">Algorithm 1 Generate Debiased Dataset 1: D ← Original Dataset 2: G(V, E) ← refined causal model 3: E a ← set</formula><formula xml:id="formula_2">D deb [v] ← D[v]</formula><formula xml:id="formula_3">μ, σ 2 ← mean(D[v]), variance(D[v]) 34: μ deb , σ 2 deb ← mean(D deb [v]), variance(D deb [v]) 35: D deb [v] ← μ + (D deb [v] − μ deb )/σ deb * σ 36:</formula><p>end if 37: end for 38: Result D deb</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Generating the Debiased Dataset</head><p>We simulate the debiased dataset based on the debiased causal model (see Fig. <ref type="figure" target="#fig_3">3 (4)</ref>). The idea is that if the user weakens/removes biased edges from the causal network, then the simulated dataset might also contain less biases. The standard way to simulate a dataset based iterations</p><formula xml:id="formula_4">← iterations + 1 19: end loop 20: D deb [v] ← argmax(prob mat) 21: Result D deb [v]</formula><p>on a causal model is to generate random numbers for the exogenous variables <ref type="bibr" target="#b37">[38]</ref>. Thereafter, each endogenous variable is simulated as a function of its parent nodes (variables) in the causal network. In this work, we have adapted this procedure to suit our needs, i.e., simulating a fair dataset while having minimum distortion from the original dataset.</p><p>Our approach to generate the debiased dataset, as illustrated in Algorithm 1, can be broken down into 4 steps. At step 1 (lines 6-9), we focus on the set of edges added during the debiasing stage (E a ). We retrain regression models corresponding to each of the target nodes of E a . This will update the weights (regression coefficients (β i )) for all edges leading into any target node of E a . At step 2 (lines 11-16), we identify the set of nodes (attributes) that need to be simulated (V sim ). Unlike the standard procedure, we only simulate selective nodes that are directly/indirectly impacted by the user's interaction to minimize distortion. This set includes the target nodes of all edges that the user has interacted with along with their descendent nodes. For example, in Fig. <ref type="figure" target="#fig_1">3</ref>(3), the user deletes the edge s → b and weakens the edge s → c. So, we will only simulate variables b, c and y. At step 3 (lines 18-25), we actually simulate all nodes that are a part of V sim using Equation <ref type="formula">2</ref>. All other nodes are left untouched and their values are simply copied from the original dataset into the debiased dataset. It should be noted that all parent nodes must be simulated before their child node as the values for a node are computed using their parent nodes. So, we simulate all endogenous variables in a topological order. For eg., in Fig. <ref type="figure" target="#fig_1">3</ref>(4), nodes b and c will be simulated before node y.</p><formula xml:id="formula_5">y = parents(y) ∑ i α i β i x i + ε + parents(y) ∑ i (1 − α i )β i r i (2)</formula><p>In the above equation, node y is simulated as a sum of 3 terms. The first term is the weighted linear combination of parent nodes. Here, α i is the scaling factor that has a default value of 1 and can range between [0, 2] as determined by user interaction. Strengthening an edge, sets α i &gt;1; weakening an edge, sets α i &lt;1. For example, if the user weakens the edge between node x i and y by -35%, then α i =0.65, strengthening it by +35% will set α i =1.35, and deleting it will set α i =0. The second term is the intercept that was computed when the regression model for y was last trained. The third term adds randomness in proportion to the degree to which the user has altered an incoming edge. Here, r i is a normal random variable that has a similar distribution as</p><formula xml:id="formula_6">x i (r i ∼ N (μ xi , σ 2 xi )</formula><p>). This term adds fairness as it is random and alleviates distortion for y.</p><p>Fig. <ref type="figure" target="#fig_3">4</ref> illustrates the case where the node Job has a single parent node (Gender). Let's say that the user chooses to delete this edge (α=0). Going the conventional route (without the third term), the attribute Job will be reduced to a constant value (the intercept (ε)) which is undesirable. Adding the (third) random term generates the Job distribution below the '+' node which is far more balanced (fair) in terms of Gender than the 'Original' distribution on the top right. However, the number of people getting the job (or not) is distorted compared the 'Original' distribution. This is corrected in Step 4 (lines 27-37) in Algorithm 1, where we rescale each simulated variable so that its distribution is close to its original distribution. For numerical variable(s), we simply standardize values to their original mean and standard deviation. For categorical variable(s), the model returns a set of probability scores corresponding to each possible output label for each data point. We iteratively scale such a probability matrix so that the resulting debiased distribution inches toward the original distribution (see Algorithm 2). We continue this process until a fixed number of iterations or when the difference between original and debiased distribution starts increasing. In Fig. <ref type="figure" target="#fig_3">4</ref>, we observe that the resulting 'Debiased' distribution matches the 'Original' distribution in terms of Job allocation, while maintaining gender parity. It should be noted that simulating each attribute adds a corresponding modeling error to the process. This modeling error is typically small but it can potentially overpower the impact of the user's intervention, especially when a user makes a small change, say weakening an edge by 5%. In such a case, the results may not be in strict accordance with the user's expectations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Evaluation Metrics</head><p>Once the debiased dataset is generated, it can be evaluated using different metrics that operate at the dataset level and the classifier level. For the second case, the debiased dataset is used to train a ML model chosen by the user, and a set of metrics are computed over the model's predictions. Here, the idea is to evaluate the downstream effects of debiasing. All the evaluation metrics can be grouped into three broad categories, namely utility, fairness and distortion. It should be noted that there might be a trade-off among the three categories. For eg., reducing bias might cause high data distortion or lower utility. For comparison, we have used a baseline debiasing strategy which just removes the sensitive attribute(s) from the dataset.</p><p>Fairness. Our tool presents a diverse set of 5 popular fairness metrics, namely statistical parity difference (Parity diff), individual fairness (Ind. bias), accuracy difference (Accuracy diff), false negative rate difference (FNR diff) and false positive rate difference (FPR diff) <ref type="bibr" target="#b4">[5]</ref>. Two of these metrics operate at the dataset level (Parity diff, Individual bias) and the rest operate on the classifier's predictions. Here, Ind. bias is defined as the mean percentage of a data point's k-nearest neighbors that have a different output label. A lower value for Ind. bias is desirable as it means that similar data points have similar output labels. For the 4 other fairness metrics, we compute some statistic for the two groups say males and females, and then report their absolute difference. This statistic can be ML model dependent, such as accuracy, or model independent, such as the likelihood of getting a positive output label. Lower values for such metrics suggests more equality between groups. For computing model based metrics, we omit the sensitive attributes(s) and perform 3-fold cross validation using the user-specified ML model with 50:50 train test split ration, and then report the mean absolute difference between groups across the 3 folds.</p><p>Utility. The utility of ML models is typically measured using met- rics like accuracy, F1 score, etc. In our context, we are interested in measuring the utility of a ML model when it is trained using the debiased dataset instead of the original dataset. To compute the utility for the original dataset, we perform 3-fold cross validation using the user-specified ML model and report the mean accuracy and F1 score.</p><p>For the debiased dataset, we follow a similar procedure where we train the user-specified ML model using the debiased dataset. However, we use the outcome variable from the original dataset as the ground truth for validation. Sensitive attribute(s) are removed from both datasets before training. Ideally, we would like the utility metrics for the debiased dataset to be close to the corresponding metrics for the original dataset. Data Distortion. Data distortion is the magnitude of deviation of the debiased dataset from the original dataset. Since the dataset can have a mix of continuous and categorical variables, we have used the Gower distance <ref type="bibr" target="#b20">[21]</ref> metric. We compute the distance between corresponding rows of the original and debiased dataset, and then report the mean distance as data distortion. This metric is easy to interpret as it has a fixed lower and upper bound ([0,1]). It is 0 if the debiased dataset is the same as the original while higher values signify higher distortion. Lower values for data distortion are desirable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THE D-BIAS TOOL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Generator Panel</head><p>This is the first component of the tool the user interacts with (see Fig. <ref type="figure">1 (A)</ref>). The user starts off by choosing a dataset from its dropdown menu. The user then selects the label variable which should be a binary categorical variable as we are considering a classification setting. Next, the user selects all nominal variables which is required for fitting the SEM model. Lastly, the user chooses a p-value and clicks the 'Causal Model' button to generate the causal network. Here, the p-value is used by the PC algorithm to conduct independence tests. We set p = 0.01 for all our demonstrations. It can be changed to 0.05 or 0.10 for smaller datasets. The 'Debiased Data' button downloads the debiased dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Causal Network View</head><p>This is the most critical piece of the interface where the user will spend most of the time (see Fig. <ref type="figure">1(B)</ref>). The center of this view contains the actual causal network which is surrounded by 4 panels on all sides.</p><p>Causal Network. All features in the dataset are represented as nodes in the network and each edge represents a causal relation. The width of an edge encodes the magnitude of the corresponding standardized beta coefficient. It signifies the feature importance of the source node in predicting the target node. The color of an edge encodes the sign of the corresponding standardized beta coefficient. Green (red) represents positive (negative) influence of the source node on the target node. If an edge is undirected, it does not have a beta coefficient and is colored orange. Finally, gray color encode edges that represent relationships which can not be represented by a single beta coefficient. This occurs when the target node is a categorical variable with more than 2 levels.</p><p>The causal network supports many interactions to enhance the user's overall experience and productivity. It supports operations like zooming and panning. The user can move nodes around if they are not satisfied Right panel. The panel to the right of the causal network offers four functionalities. Going from top to bottom, they represent zooming in, zooming out, reset layout and changing weight of an edge. The slider at the bottom gets activated when the user clicks on an edge during the debiasing stage. It allows the user to weaken/strengthen an edge depending on the selected value between -100% to 100%. Moving the slider changes α i and also impacts the effective beta coefficient for the selected edge (α i β i ). This change manifests visually in the form of proportional change in the corresponding edge width. Moving the slider to -100% will result in deletion of the selected edge.</p><p>Bottom panel. As shown below, the bottom panel offers 4 functionalities. The "Find Paths" button triggers depth first traversal of the causal graph to compute all directed paths between the source and target node as selected in the top panel. This will be especially helpful when the graph is big and complex. All the computed paths are then displayed below the bottom panel. A user can click on a displayed path to highlight it and see an animated view of the path going from the source to the target node. The "Logs" button highlights changes made to the causal network during the debiasing stage (see Fig. <ref type="figure" target="#fig_4">5</ref>). All edges are hidden except for the newly added edges, deleted edges and edges that were weakened/strengthened. Nodes impacted by such operations (V sim ) are highlighted in grey.</p><p>If a user is interested in knowing the exact beta coefficients for all edges, the edge weights toggle will help in doing just that. By default, it is set to 'hide' to enhance readability. If turned to 'show', the beta coefficients will be displayed on each edge. The filter slider helps user focus on important edges by hiding edges with absolute beta coefficients less than the chosen value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Panel</head><p>This panel, at the right (see Fig. <ref type="figure">1(C</ref>)), provides different options and visual plots for comparing and evaluating the changes made to the original dataset. From the top left, users can select the sensitive variable and the ML algorithm from their respective dropdown menus. This selection will be used for computing different fairness and utility metrics. For the sensitive variable, the dropdown menu consists of all binary categorical variables in the dataset along with a Custom Group option. Selecting the Custom Group option opens a new window (see Fig. <ref type="figure" target="#fig_5">6</ref>) which allows the user to select groups composed of multiple attributes. This interface facilitates comparison between intersectional groups say black females and white males.</p><p>Clicking the "Evaluate Metrics" button triggers the computation of evaluation metrics that are displayed on the right half of this panel (Performance View). It also visualizes the relationship between the sensitive attribute or selected groups and the outcome variable using a 4-fold display <ref type="bibr" target="#b16">[17]</ref> on the left half of this panel (Comparison View).</p><p>Comparison View. This view comprises two plots aligned vertically where the top plot represents the original dataset and the bottom plot represents the debiased dataset. This view has two functions. It first aids the user in the initial exploration of different features and relationships. When a node or edge is clicked in the causal network, the summary statistics of the corresponding attributes is visualized (see Fig. <ref type="figure" target="#fig_6">7</ref>(c) for an example). For binary relationships, we use either a scatter plot, a grouped bar chart or an error bar chart depending on the data type of the attributes. The second function of the Comparison View is to visualize the differences between the original and the debiased dataset. Initially, the original data is the same as the debiased data and so identical plots are displayed. However, when the user injects their prior into the system, the plots for the original and debiased datasets start to differ. We added this view to provide more transparency and interpretability to the debiasing process and also help detect sampling bias in the data.</p><p>When the user clicks the "Evaluate Metrics" button, the Comparison View visualizes the binary relation between the sensitive attribute or selected groups and the outcome (label) variable via the 4-fold plot <ref type="bibr" target="#b16">[17]</ref> as shown in Fig. <ref type="figure">1(C</ref>), left panel). We chose a 4-fold display over a more standard brightness-coded confusion matrix since the spatial encoding aids in visual quantitative assessments. The left/right half of this display represents two groups based on the chosen sensitive variable (say males and females) or as defined in the Custom Group interface, while the top/bottom half represents different values of the output variable say getting accepted/rejected for a job. Here, symmetry along the y-axis means better group fairness.</p><p>Performance View. This view houses all the evaluation metrics as specified in Sec.3.4. It uses a horizontal grouped bar chart to visualize 2 utility and 5 fairness metrics. Lower values for the fairness metrics mean better fairness. Higher values for utility metrics means better utility. Data distortion is visualized using a donut chart. On hovering over any of these charts, a tooltip shows the exact values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CASE STUDY</head><p>We demonstrate the utility of our tool for bias identification and mitigation using the Adult Income dataset. Each data point in the dataset represents a person described by 14 attributes recorded from the US 1994 census. Here, the prediction task to classify if a person's income will be greater or less than $50k based on attributes like age, sex, education, marital status, etc. We chose this dataset as it is widely used in the algorithmic fairness literature <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20]</ref>. Here, we have chosen a random sample of 3000 points from this dataset for faster computation.</p><p>Generating the causal network. We start off by selecting the Adult Income dataset from the respective dropdown menu in the Generator panel. We select Income as the label variable and Work class, Marital Status, Race, Gender, Income as the nominal variables. Next, we click on the Causal Model button which generates the default causal model (see Fig. <ref type="figure" target="#fig_6">7 (a)</ref>). Here, we examine different edges of the causal model and act on them as needed to reach to a reliable causal model. We start with the 7 undirected edges encoded in orange. We direct edges based on our domain knowledge like Hours per Week → Income, Education → Income, Education → Hours per week, etc. After each of these operations, we observe a green bar in the left panel of the Causal Network View. This indicates that the resulting causal model is a better fit over the underlying dataset. Next, we examine other directed edges. Many of them align with our domain knowledge like Capital Gain → Income, Age → Income, etc. However, we found a couple of them to be counter-intuitive, namely Capital Gain → Education and Marital Status → Gender. In a causal relation, cause always precedes effect. Hence, immutable personal characteristics like sex, race, etc., which are assigned at birth, can not be the effect of a later life event like marriage or work class. So, in this case, we chose to reverse both these edges to get to the refined causal model (see Fig. <ref type="figure" target="#fig_6">7(b)</ref>).</p><p>Auditing for social biases. Once we have reached a reliable causal model, we start auditing for different kinds of biases. We click on different nodes and edges to explore their distributions. For eg., clicking the Gender node visualizes its distribution in the Comparison View. We observe that females are underrepresented in the dataset (895 females vs 2105 males) (see Fig. <ref type="figure" target="#fig_6">7 (c</ref>)). Given this representation bias and the fact that gender pay gap is a well-known issue, we decided to investigate further. We found an indirect path from Gender to Income via Hours per week. This indicates a possible disparity in income based on gender. To probe further, we click the "Evaluate Metrics" button with Gender as the sensitive variable to compute different fairness metrics. We observe significant gender bias as captured by metrics like Accuracy diff (14%), FPR diff (22%), FNR diff (17%), etc. The 4-fold display for the original data in Fig. <ref type="figure" target="#fig_7">8</ref> (A) reveals that only 12% of the females earn more than $50k compared to 31% for males. Thus there is a significant income disparity based on gender. To have a more comprehensive understanding of the issue, we search for all possible paths by selecting Gender and Income as the source and target from the Top panel and then clicking the "Find Paths" button from the bottom panel. This populates 4 different causal paths below the bottom panel (see Fig. <ref type="figure" target="#fig_6">7</ref> (e)). We will now focus on the different causal relationships in these paths and try to make minimal changes to achieve more fairness.</p><p>Bias mitigation. To mitigate gender bias, we first enter the debiasing stage by flipping the Stage toggle from Refine to Debias. From here on, any changes to the causal network will simulate a new debiased dataset. Among the 4 paths we previously discovered, the top 2 paths have a common causal edge, i.e., Gender → Marital status. On clicking this edge, we find that most males in the dataset are married while most females are single (see Fig. <ref type="figure" target="#fig_6">7(d)</ref>). This pattern indicates sampling bias. Ideally, we would like no relation between these attributes so we delete this causal edge. Next, we assess the remaining two causal paths. Based on our domain knowledge, we find the causal edge Hours per week → Income to be socially desirable, and the edges Gender → Work class and Gender → Hours per week to be socially undesirable. We delete the two biased edges to get to the debiased causal model (see Fig. <ref type="figure" target="#fig_6">7</ref>(f)). To verify all changes made so far, we click on the Logs button. As shown in Fig. <ref type="figure" target="#fig_4">5</ref>, it shows 3 dotted lines for the removed edges and highlight the impacted attributes.</p><p>Lastly, we click on the "Evaluate Metrics" to see the effect of our interventions. The 4-fold display for the debiased data in Fig. <ref type="figure" target="#fig_7">8(C)</ref> shows that the disparity between the two genders has now decreased from 19% to 6% (compare Fig. <ref type="figure" target="#fig_7">8(A)</ref>). The percentage of females who make more than $50k have undergone a massive growth of 75% (from 12% to 21%). Moreover, as shown in Fig. <ref type="figure" target="#fig_6">7</ref>(g), we find that all fairness metrics have vastly improved, with only a slight decrease in the utility metrics and an elevated distortion (12%). These results clearly indicate the efficacy of our debiasing approach.</p><p>Partial debiasing. Considering the tradeoff between different metrics, one might choose to debias data partially based on their context, i.e., weaken biased edges instead of deleting them or keeping certain unfair causal paths from the sensitive variable to the label variable intact. For eg., one might choose to delete the edge Gender → Marital status and weaken the edges Gender → Work class and Gender → Hours per week by 25% and 75%, respectively. On evaluation, we find this setup to sit somewhere between the original and the fully debiased case (see Fig. <ref type="figure" target="#fig_7">8 (B)</ref>). It performs better on fairness than the original dataset (gap: 9% vs 19%) but worse than the full debiased version (gap: 9% vs 6%). Similarly, it incurs more distortion than the original dataset but less than the full debiased version (11% vs 12%).</p><p>Intersectional groups. D-BIAS facilitates auditing for biases against intersectional groups using the "Custom Group" option from the sensitive variable dropdown. Here, we choose Black Females and White Males as the two groups. At the outset, there is a great disparity between the groups as reflected in the 4-fold display and the fairness metrics (see Fig. <ref type="figure">1</ref>). As these subgroups are defined by Gender and Race, we focus on the unfair causal paths from these nodes to the label variable (Income). For debiasing, we first perform the same operations we did for gender debiasing. Thereafter, we reduce the impact of race by deleting the edges Race → Work class and Race → Marital status which we deem as socially undesirable. On evaluation (see Fig. <ref type="figure">1</ref>), we find a significant decrease in bias across all fairness metrics for the debiased dataset compared to the conventional debiasing practice (blue bars) which just trains the ML model with the sensitive attributes (here Gender and Race) simply removed. Finally, the two 4-fold displays reveal that the participation of the disadvantaged group more than doubled, while the privileged group experienced only a modest loss.</p><p>Exacerbating bias. The flexibility offered by D-BIAS to refine the causal model can be misused to increase bias as well. Bias can be exacerbated by strengthening/adding biased causal edges and weakening/deleting other relevant causal edges. For eg., one can exacerbate gender bias by strengthening the edges Gender → Marital status and Marital status → Income by a 100%. On evaluation, we find that the proportion of females making &gt;$50k has shrunk to just 1% while the proportion of males has surged to 38%. In effect, this has broadened the gap between males and females making more than $50k by about 2x from 19% to 37% (see Fig. <ref type="figure" target="#fig_7">8 (D)</ref>).</p><p>Results. Apart from the Adult Income dataset, we also tested our tool using the synthetic hiring dataset and the COMPAS recidivism dataset (see appendix D for details). The evaluation metrics for all 3 datasets after full debiasing are reported in Table <ref type="table" target="#tab_3">1</ref>. As we can observe, our tool is able to reduce bias significantly compared to the baseline approach across the 3 datasets for a small loss in utility and data distortion. These results validate the potential of HITL approach in mitigating bias. It is interesting to observe that the F1 score for the synthetic hiring dataset is slightly higher than the baseline approach.</p><p>However, this is line with the existing literature <ref type="bibr" target="#b19">[20]</ref> where similar instances have been recorded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">USER STUDY</head><p>We conducted a user study to evaluate two primary goals: (1) usability of our tool, i.e., if participants can comprehend and interact with our tool effectively to identify and mitigate bias, (2) compare our tool with the state of the art in terms of human-centric metrics like accountability, interpretability, trust, usability, etc.</p><p>Participants. We recruited 10 participants aged 24-36; gender: 7 Male and 3 Female; profession: 8 graduate students and 2 software engineers. The majority of the participants are computer science majors with no background in data visualization or algorithmic fairness. 80% of the participants trust AI and ML technologies in general. The participation was voluntary with no compensation.</p><p>Baseline Tool. For an even comparison, we looked for existing tools with a visual interface that support bias identification and mitigation. This led us to IBM's AI Fairness 360 <ref type="bibr" target="#b4">[5]</ref> toolkit whose visual interface can be publicly accessed online <ref type="foot" target="#foot_0">1</ref> . However, we didn't go further with this toolkit as the baseline (control group) because it has a significantly different look and feel which is difficult to control for. Instead, we took inspiration from this toolkit and built a baseline visual tool (not to be confused with the baseline debiasing strategy) which mimics its workflow but matches the design of our D-BIAS tool (see Fig. <ref type="figure" target="#fig_8">9</ref>).</p><p>IBM's AI Fairness toolkit allows the user to choose from a set of fairness enhancing interventions with varying impact on the evaluation metrics. However, this study is focused on other important aspects such as trust, accountability, etc. So, in order to have a tightly controlled experiment, we imagine a hypothetical automated debiasing algorithm whose performance exactly matches the peak performance of our tool for all evaluation metrics. Here, peak performance refers to the state where all unfair causal edges are deleted.</p><p>Using the baseline tool is quite simple (see appendix E). The user first selects the dataset, label variable, etc. They can then audit for different biases by selecting the sensitive attribute and then clicking on the 'Check bias' button. This will compute and present a set of fairness metrics in the same fashion as the D-BIAS tool. Lastly, the user can click the 'Debias &amp; Evaluate' button to debias the dataset and generate its evaluation metrics. A small lag is introduced before displaying evaluation metrics to mimic a real debiasing algorithm.</p><p>Study design. We conducted a within subject study where each participant was asked to use the baseline tool and D-BIAS in random order. The study was conducted remotely, i.e., each participant could access and interact with the tools via their own machine. For each tool, a small tutorial was given using the Synthetic Hiring dataset<ref type="foot" target="#foot_1">2</ref> to demonstrate the workflow and features of the tool. Each participant was then given some time to explore and interact with each system. Next, the participants were asked to identify and mitigate bias for the Adult Income dataset. For the D-BIAS tool, participants were also asked to complete a set of 5 tasks to evaluate usability. Each task was carefully designed to cover our testing goals and had a verifiable correct solution. Tasks included: generate a causal network, direct undirected edges, identify if bias exists with respect to an attribute, identify proxy variables and finally debias the dataset. After using each tool, the participants were asked to answer a set of survey questions. Lastly, we collected subjective feedback from each participant regarding their overall experience with both the tools. Throughout the study, participants were in constant touch with the moderator for any assistance. Participants were encouraged to think aloud during the user study.</p><p>Survey Design. Each participant was asked to answer a set of 13 survey questions to quantitatively measure usability, interpretability, workload, accountability and trust. All of these questions can be answered on a 5-point Likert Scale. To capture cognitive workload, we selected two applicable questions from the NASA-LTX task load index <ref type="bibr" target="#b22">[23]</ref>, i.e., "How mentally demanding was the task? and "How hard did you have to work to accomplish your level of performance?". Participants could choose between 1 = very low to 5 = very high. For capturing usability, we picked 3 questions from the System Usability Scale (SUS) <ref type="bibr" target="#b6">[7]</ref>. For e.g., "I thought the system was easy to use", "I think that I would need the support of a technical person to be able to use this system". Participants could choose between 1 = Strongly disagree to 5 = Strongly agree. To capture accountability, we asked two questions based on previous studies <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">19]</ref>. For e.g., "The credit/blame for mitigating bias effectively is totally due to" (1 = System's capability, 5 = My input to the system). To capture interpretability, we consulted Madsen Gregor scale <ref type="bibr" target="#b29">[30]</ref> and adopted 3 questions for our application. For e.g., "I am satisfied with the insights and results obtained from the tool?", "I understand how the data was debiased?" Answers could lie between 1 = Strongly disagree to 5 = Strongly agree. For measuring trust, we referred to McKnight's framework on Trust <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref> and other studies <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19]</ref> to come up with 3 questions for our specific case. For e.g., "I will be able to rely on this system for identifying and debiasing data" (1 = Strongly disagree, 5 = Strongly agree).</p><p>Results. Despite not having a background in algorithmic fairness or data visualization, all participants were able to complete all 5 tasks using the D-BIAS tool. This indicates that our tool is easy to use.</p><p>The survey data was analyzed to calculate usability, interpretability, workload, accountability and trust ratings for each tool by each participant. The mean ratings along with their standard deviations are plotted in Fig. <ref type="figure" target="#fig_9">10</ref>. Using t-test, we found statistically significant differences for all measures with p&lt;0.05. We found that D-BIAS outperforms the baseline tool in terms of trust, accountability and interpretability. However, it lags in usability and cognitive workload. So, if someone is looking for a quick fix or relies on ML algorithms more than humans, automated debiasing is the way to go. Conversely, if trust, accountability or interpretability is important, D-BIAS should be the preferred option. Looking at these results in conjunction with the results reported in Table <ref type="table" target="#tab_3">1</ref>, we find that our tool enhances fairness while fostering accountability, trust and interpretability.</p><p>Subjective feedback. After the study, we gathered feedback from We got comments like "Interface is user friendly", "Causal network gives control and flexibility", "Causal network is very intuitive and easy to understand", "Causal network is a great way to understand relationships between features". Most participants agreed that after a tutorial session, it should be fairly easy for even non-experts to play with the system. Another important aspect which received a lot of attention was our human-in-the-loop approach. Participants felt that they had a lot more control over the system and that they could change things around. One of the participants commented "It feels like I have a say". Some of the participants said they felt more accountable because the system offered much flexibility and that they had a choice to make. Many of the participants strongly advocated for D-BIAS over the baseline tool. For e.g., "D-BIAS better than automated debiasing any day", "D-BIAS hand's down!". Few of the participants had a more nuanced view. They were of the opinion that the baseline tool might be the preferred option if someone is looking for a quick fix. We also received concerns and suggestions for future improvement. Two of the participants raised concern about the tool's possible misuse if the user is biased. Another participant raised concern about scalability for larger datasets. Most participants felt that adding tooltips for different UI components especially the fairness metrics will be a great addon. Two participants wished they could see the exact changes in CSV file in the visual interface itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION, LIMITATIONS, AND FUTURE WORK</head><p>Efficacy. The efficacy of our tool depends on how accurately the causal model captures the underlying data generating process and the ensuing refining/debiasing process. As our tool is based on causal discovery algorithms, it inherits all its assumptions such as the causal markov condition and its limitations like sampling biases, etc. <ref type="bibr" target="#b36">[37]</ref>. For eg., Caucasians had a higher mean age than African Americans in the COMPAS dataset. So, the PC algorithm falsely detected a causal edge between Age and Race (see appendix D.2). From our domain knowledge, we can deduce that this error is due to sampling bias. Ideally, one should use such an insight to gather additional data points for the under sampled group. However, it may not always be possible. In such cases, our tool can be leveraged to remove such patterns in the debiasing stage. We dealt with a similar case (Gender → Marital status) for the Adult Income data (see Sec. 5). It is also worth noting that our tool is able to reduce disparity between the privileged and the unprivileged group but it may not be able to close the gap entirely. This can be due to missing proxy variables whose influence is unaccounted for or maybe because linear models are too simple to capture the relationship between a node and its parents. Future work might use non-linear SEMs and causal discovery algorithms like Fast Causal Inference (FCI) that can better deal with missing attributes.</p><p>Scalability. As the size of the dataset increases in terms of features and rows, scalability can become an issue. With dataset size, the time to generate a causal network, debiasing data, finding paths between nodes and computing evaluation metrics will all increase proportionally. Among all these steps, the process to generate the default causal network might be the most computationally expensive. So, future work should employ GPU-based parallel implementation of PC algorithm like cuPC <ref type="bibr" target="#b46">[47]</ref> or use inherently faster causal discovery algorithms like F-GES <ref type="bibr" target="#b44">[45]</ref> to better scale to larger datasets. On the front end, the causal network will become big and complex as the number of features increase. With limited screen space, the user might find it difficult to comprehend the causal network. To alleviate this issue, we have implemented different visual analytics techniques like zooming, panning, filtering weak edges, finding paths, etc. Future work might optimize graph layout algorithm and explore other visual analytics techniques like node aggregation to help navigate larger graphs better <ref type="bibr" target="#b44">[45]</ref>.</p><p>Applications. In this paper, we have emphasized how our tool can help identify and remove social biases. However, our approach and tool is not limited to social biases. Our tool can incorporate human feedback to realize policy and institutional goals as well (see appendix D.1). For eg., one might strengthen the edge between the nodes Education and Income to implant a policy intervention where people with higher education are incentivized. A ML model trained over the resulting dataset is likely to reflect such policy intervention in its predictions. Next, we plan to extend our HITL methodology to tackle biases in other domains such as word embeddings.</p><p>Human factors. Involving a human in the loop for identifying and debiasing data is a double edged sword. On one hand, it is a key strength of our tool as it provides real world domain knowledge and fosters accountability and trust. On the other hand, it can also be its main weakness if the human operating this tool intentionally/unconsciously injects social biases. A user can misuse the system in two ways. Firstly, the user can choose to ignore the social biases inherent in the dataset by not acting on the unfair causal edges. Such behaviour renders the system ineffective. Secondly, a biased user can explicitly introduce their own biases in the system by adding/strengthening unfair causal edges. Since this is a human aided tool, the biases that are inherent to the human user cannot be avoided. Hence, we recommend choosing the user responsibly. Moreover, we can always check the system logs and hold the person responsible for their action/inaction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. This figure shows how D-BIAS fits into the typical ML pipeline. D-BIAS allows users to act on the raw data and returns its debiased version which can be fed into a ML model for fairer predictions.</figDesc><graphic url="image-12.png" coords="2,75.64,72.88,89.36,69.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The workflow of D-BIAS has 4 parts: (1) The default causal model is inferred using the PC algorithm and SEM. Here, dotted edges represent undirected edges (2) Refine stage -the default causal model is refined by directing undirected edges, reversing edges, etc. (3) Debiasing stage -the causal model is debiased by deleting/weakening biased edges (4) The debiased causal model is used to simulate the debiased dataset.</figDesc><graphic url="image-19.png" coords="3,463.20,72.88,66.16,70.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 2</head><label>2</label><figDesc>Rescale Categorical Variable v 1: D[v] ← categorical variable v in the original dataset 2: prob mat ← probability matrix for v computed using Eq. dist ori ← DPD( D[v] ) 9: dist deb ← DPD( argmax(prob mat) ) 10: diff ← ∑ (dist ori − dist deb )/dist deb 11: scale factor ← 1 + lr * ∑ (dist ori − dist deb )/dist deb ) 12: prob mat ← scale factor * prob mat 13: dist deb ← DPD( argmax(prob mat) ) 14: new diff ← ∑ (dist ori − dist deb )/dist deb 15:if new diff &gt; diff or iterations &gt; 50 then</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Illustration of the data debiasing process using a toy example where the node Job is caused by a single variable, namely gender. Green color marks the proportion of females who got the job (Y) or not (N).</figDesc><graphic url="image-22.png" coords="4,341.00,73.00,86.92,89.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Logs view highlighting the changes made to the causal network for the Adult Income dataset. Dotted lines represent deleted edges and nodes in grey represent the impacted nodes.</figDesc><graphic url="image-24.png" coords="5,99.76,72.99,159.83,105.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The visual interface for selecting subgroups (Group A and B). Each column consists of a list of bar charts/histograms representing all attributes in the dataset. By default, all bars are colored gray. The user can click on multiple bars to select a subgroup. Selected bars are colored blue. Each bar is filled in proportion of their representation in the selected subgroup as a ratio of the entire dataset. In this picture, Group A consists of individuals who went to elite universities and whose age lies between 24-40. It represents 18% of the dataset.with the default layout. On clicking a node, all directly connected edges and nodes are highlighted. Similarly, on clicking an edge, its source and target nodes are highlighted. Moreover, clicking a node or an edge also visualizes their distribution in the Comparison View (see Sect. 4.3).</figDesc><graphic url="image-25.png" coords="5,342.93,72.88,173.21,171.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Case study: Adult Income dataset (a) Causal model generated using automated techniques (b) Refined causal model (c) Clicking the Gender node visualizes its distribution as a bar chart (d) Bivariate distribution between Gender and Marital Status (e) All paths from Gender to Income in the refined causal model (f) Debiased causal model (g) Evaluation metrics to compare our results against the baseline debiasing approach.</figDesc><graphic url="image-32.png" coords="6,84.90,163.57,98.90,91.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. The above picture shows the impact of 3 types of user interaction as captured by the 4-fold display. Due to space constraints, we have only shown a subset of the causal network which connects the Gender node with the Income node. For details, please refer to the description in Section 5.</figDesc><graphic url="image-37.png" coords="7,79.39,72.99,438.63,182.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Baseline visual tool used as a benchmark in the user study.</figDesc><graphic url="image-38.png" coords="9,103.40,72.66,150.65,226.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Mean user ratings from the survey data along with their standard deviation for different measures.</figDesc><graphic url="image-43.png" coords="9,314.00,129.17,66.34,56.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Rescale values for the simulated attributes 28: for each v in V sim do</figDesc><table><row><cell>24:</cell><cell>end if</cell></row><row><cell cols="2">25: end for</cell></row><row><cell>26:</cell><cell></cell></row><row><cell>27: // 29:</cell><cell>if v is a categorical variable then</cell></row><row><cell>30:</cell><cell>D deb [v] ← rescale values based on Algorithm 2</cell></row><row><cell>31:</cell><cell>else</cell></row><row><cell>32:</cell><cell>// v is a numeric variable</cell></row><row><cell>33:</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Evaluation metrics to compare the debiased dataset generated using our tool against the baseline debiasing approach for different datasets.</figDesc><table><row><cell>Dataset</cell><cell>Sensitive attribute</cell><cell>version</cell><cell>ML model</cell><cell>Accuracy</cell><cell>F1</cell><cell>Parity difference</cell><cell>Individual Bias</cell><cell cols="4">Accuracy difference difference difference Distortion FNR FPR Data</cell></row><row><cell>Synthetic Hiring</cell><cell>Gender</cell><cell>baseline debiased</cell><cell>SVM</cell><cell>77% 77%</cell><cell>0.59 0.60</cell><cell>11.12 1.66</cell><cell>19.09 12.93</cell><cell>4.14 2.99</cell><cell>14.26 1.37</cell><cell>6.82 3.63</cell><cell>0% 6%</cell></row><row><cell>Adult Income</cell><cell>Gender</cell><cell>baseline debiased</cell><cell>Logistic Regression</cell><cell>82% 75%</cell><cell>0.69 0.63</cell><cell>19.32 6.24</cell><cell>17.92 4.8</cell><cell>14.35 0.88</cell><cell>17.98 2.33</cell><cell>22.53 1.9</cell><cell>0% 12%</cell></row><row><cell>COMPAS</cell><cell>Race</cell><cell>baseline debiased</cell><cell>Random Forest</cell><cell>67% 63%</cell><cell>0.64 0.59</cell><cell>12.07 11.12</cell><cell>33.9 2.19</cell><cell>0.17 0.44</cell><cell>23.07 0.68</cell><cell>16.89 1.55</cell><cell>0% 13%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://aif360.mybluemix.net/data</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">We generated a synthetic hiring dataset fraught with gender and racial bias to better evaluate our tool. For details, please refer to appendix C .</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was partially funded by NSF grants CNS 1900706, IIS 1527200, IIS 1941613, and NSF SBIR contract 1926949.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fairsight: Visual analytics for fairness in decision making</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-R</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1086" to="1095" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fairness in machine learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nips tutorial</title>
		<imprint>
			<date type="published" when="2017">1:2017, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning fair classifiers: A regularizationinspired approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bechavod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ligett</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.00044</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Penalizing unfairness in binary classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bechavod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ligett</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.00044</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Ai fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Bellamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hind</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Hammel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>O'connell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sex bias in graduate admissions: Data from berkeley</title>
				<imprint>
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Sus-a quick and dirty usability scale. Usability evaluation in industry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Brooke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">189</biblScope>
			<biblScope unit="page" from="4" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Á</forename><forename type="middle">A</forename><surname>Cabrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Epperson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Morgenstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
		<title level="m">Fairvis: Visual analytics for discovering intersectional bias in machine learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The effects of example-based explanations in a machine learning interface</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jongejan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Holbrook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Intelligent User Interfaces</title>
				<meeting>the 24th International Conference on Intelligent User Interfaces</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Optimized data pre-processing for discrimination prevention</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Calmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Ramamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Varshney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03354</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Path-specific counterfactual fairness</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chiappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A causal bayesian networks viewpoint on fairness</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chiappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Isaac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IFIP International Summer School on Privacy and Identity Management</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Order-independent constraint-based causal structure learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maathuis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Overcoming algorithm aversion: People will use imperfect algorithms if they can (even slightly) modify them</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Dietvorst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Simmons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Massey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Trust in automl: exploring information needs for establishing trust in automated machine learning systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Drozdal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weisz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent User Interfaces</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Group fairness under composition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ilvento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Fairness, Accountability, and Transparency</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A fourfold display for 2 by 2 by k tables</title>
		<author>
			<persName><forename type="first">M</forename><surname>Friendly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Wordbias: An interactive visual tool for discovering intersectional biases encoded in word embeddings</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ghai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Hoque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM CHI, Extended Abstracts</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Explainable active learning (xal) toward ai explanations as interfaces for machine teachers</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ghai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bellamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM CSCW</title>
				<meeting>ACM CSCW</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Cascaded debiasing: Studying the cumulative effect of multiple fairness-enhancing interventions</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ghai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mueller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.03734</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A general coefficient of similarity and some of its properties</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Gower</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="page" from="857" to="871" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A methodology for direct and indirect discrimination prevention in data mining</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hajian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Domingo-Ferrer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on knowledge and data engineering</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Development of nasa-tlx (task load index): Results of empirical and theoretical research</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Staveland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Psychology</title>
				<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="139" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Outcome-explorer: A causality guided interactive visual interface for interpretable algorithmic decision making</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Hoque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mueller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00633</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Classifying without discriminating</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kamiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Calders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009. 2009</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
	<note>Computer</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Data preprocessing techniques for classification without discrimination</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kamiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Calders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Inherent trade-offs in the fair determination of risk scores</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mullainathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raghavan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Counterfactual fairness</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Loftus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Bias mitigation post-processing for individual and group fairness</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Lohia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Ramamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bhide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Puri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.06135</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Measuring human-computer trust</title>
		<author>
			<persName><forename type="first">M</forename><surname>Madsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gregor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th australasian conference on information systems</title>
				<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Developing and validating trust measures for e-commerce: An integrative typology</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Mcknight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kacmar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Systems Research</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Initial trust formation in new organizational relationships</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Mcknight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Chervany</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Academy of Management Review</publisher>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="473" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">21 fairness definitions and their politics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Fairness, Accountability, and Transparency</title>
				<meeting><address><addrLine>NYC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Causality: Models, Reasoning, and Inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Causal inference in statistics: An overview</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<idno type="DOI">10.1214/09-SS057</idno>
	</analytic>
	<monogr>
		<title level="j">Statistics Surveys</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="96" to="146" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Tabfairgan: Fair tabular data generation with generative adversarial networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rajabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">O</forename><surname>Garibay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Challenges and opportunities with causal discovery algorithms: Application to alzheimer&apos;s pathophysiology</title>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vemuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Simon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Scientific Reports</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Simcausal r package: conducting transparent and reproducible simulation studies of causal effect estimation with complex longitudinal data</title>
		<author>
			<persName><forename type="first">O</forename><surname>Sofrygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Van Der Laan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Neugebauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Statistical Software</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Causation, Prediction, and Search</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Why fairness cannot be automated: Bridging the gap between eu non-discrimination law and ai</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wachter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mittelstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Law &amp; Security Review</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">105567</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Visual causality analysis made practical</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE VAST</title>
				<meeting>IEEE VAST</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<title level="m">Visual analysis of discrimination in machine learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">On discrimination discovery and removal in ranked data using causal graph</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fairrankvis: A visual analytics framework for exploring algorithmic fairness in graph mining models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Maciejewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="368" to="377" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A visual analytics approach for exploratory causal analysis: Exploration, validation, and applications</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Visualization and Computer Graphics</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1448" to="1458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Interactively assessing machine learning fairness using causality</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rzeszotarski</surname></persName>
		</author>
		<author>
			<persName><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM CHI Conference on Human Factors in Computing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">cupc: Cuda-based parallel pc algorithm for causal structure learning on gpu</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zarebavani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jafarinejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Salehkaleybar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="530" to="542" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning fair representations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intern. Conf. on Machine Learning</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="325" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Anti-discrimination learning: a causal modelingbased framework</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intern. J. Data Science and Analytics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A causal framework for discovering and removing direct and indirect discrimination</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Artificial Intelligence</title>
				<meeting>the 26th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
