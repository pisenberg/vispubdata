<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visualizing the Passage of Time with Video Temporal Pyramids</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Melissa</forename><forename type="middle">E</forename><surname>Swift</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wyatt</forename><surname>Ayers</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sophie</forename><surname>Pallanck</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Scott</forename><surname>Wehrwein</surname></persName>
						</author>
						<title level="a" type="main">Visualizing the Passage of Time with Video Temporal Pyramids</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-03-27T09:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Distill Timescales (Hourly</term>
					<term>Daily</term>
					<term>Monthly</term>
					<term>etc) Time</term>
					<term>time-frequency</term>
					<term>video visualization</term>
					<term>multi-scale</term>
					<term>webcam</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. <ref type="figure">1</ref>. Given months or years of recorded webcam footage, our approach builds a Video Temporal Pyramid consisting of different length shorter videos, each of which visualizes the events happening at a particular timescale. Our Video Spectrogram is a visualization for the pyramid that provides both overview and drill down functionality to aid in interactive exploration.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The world around us is constantly changing at many speeds at once, but the human visual system can only perceive a narrow range of dynamic phenomena in real time. Some things move too slowly for us to register, such as a glacier flowing, and some things move too quickly for us to register, such as a bee's wings in flight. Though we</p><p>• Melissa E. Swift conducted this research while at Western Washington University and is currently with Pacific Northwest National Laboratory. E-mail: melissa.swift@pnnl.gov. • Wyatt Ayers is at Western Washington University. E-mail: ayersw2@wwu.edu. • Sophie Pallanck was at Western Washington University. E-mail: sophierosepallanck@gmail.com. • Scott Wehrwein is at Western Washington University. E-mail: scott.wehrwein@wwu.edu. cannot see these motions as they occur, we can visualize them after the fact. For fast events, we can use a high-speed camera and slow down the footage to a more human-friendly speed (i.e., slow-motion). For slow events, the most common method of visualizing these changes is timelapse photography, where frames are taken at a regular intervals over time and then assembled into a video that plays much faster. The key observation that motivates our work is that many scenes exhibit interesting phenomena at multiple timescales: in a single scene, we might be able to observe foot and vehicle traffic on a road, movement of clouds in the sky, diurnal changes in illumination, and a building being constructed over the course of months or years.</p><p>A natural way to capture these multi-timescale phenomena is to begin with an input video with sufficiently high framerate to capture the fastest-moving phenomena. However, months of raw video cannot be viewed in a reasonable amount of time, so we might subsample it to create a series of timelapse videos that show different rates (e.g., one frame per minute, one frame per hour, etc.). However, straightforward timelapse sampling exhibits aliasing due to high-frequency content. Consider sampling one frame per month; although longer-term changes happening at or around a months-long timescale will be natu-rally viewable, shorter-term changes, such as a person that happened to be walking through the scene at the moment a frame was sampled, will appear as a distracting single-frame blip. This paper proposes (1) Video Temporal Pyramids, a more principled, alias-free approach for visualizing changes at different timescales; and (2) the Video Spectrogram, a visualization tool for navigating and exploring the pyramids.</p><p>The algorithm that forms the basis for creating a Video Temporal Pyramid takes inspiration from the common image processing techniques of Gaussian and Laplacian image pyramids, but applied to the temporal domain. The result is a collection of new videos, each of which distills the changes happening at a particular timescale (e.g., hourly, daily, monthly, yearly). Though each pyramid level is similar to a timelapse with a particular sampling rate, they feature a smoother viewing experience with no aliasing or flickering effects.</p><p>The Video Temporal Pyramid captures information about the changes over many timescales, but the volume of video data is (approximately 2×) larger than the original. To help a user navigate and explore the pyramid and surface more information about events and patterns in the scene, we propose a visualization tool called the Video Spectrogram. We quantify the magnitude of changes happening at each time and in each timescale and plot those data as a heatmap, analogous to the spectrogram used in audio processing <ref type="bibr" target="#b9">[10]</ref>. The resulting spectrogram plots time vs. timescale, showing clear patterns for strong cyclic changes such as day/night and seasonal periodicity. Anomalies such as significant weather events and corrupted data can also be discovered. The Video Spectrogram facilitates connecting the video footage to specific dates and times when events occurred. This enables the user to quickly do an overview scan and then drill down to lower timescales to view more details for a particular day or time, in keeping with Shneiderman's information seeking mantra <ref type="bibr" target="#b37">[37]</ref>. This is key to making the large volume of video data manageable without arbitrarily discarding information.</p><p>To validate our contributions we have processed multiple longduration webcam datasets of diverse outdoor scenes, including a construction site, a ski slope, and a mountain lake, among others. The time periods covered by our datasets range from 1 month to 16 years, with base temporal resolutions ranging from 30 frames per second to 1 frame per hour (Fig. <ref type="figure">6</ref>). In our exploration of these datasets, especially in direct comparison to a timelapse baseline, we found that our pyramid videos and spectrogram tool allowed us to rapidly learn a lot of detailed information about each scene, from how the seasons changed all the way down to the exact time a particular object appeared in the scene. Please refer to our supplementary materials to view selected Video Temporal Pyramid videos from each of our datasets as well as a demo of the Video Spectrogram.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND RELATED WORK</head><p>The proposed Video Temporal Pyramid and Video Spectrogram are closely related to work in several subdisciplines. This section provides a brief overview of the most relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Timelapse and Related Techniques</head><p>Timelapse has been in existence since the late 1800's <ref type="bibr" target="#b45">[46]</ref> and is a popular way to visualize the passage of time on a small scale (e.g., a pineapple rotting <ref type="bibr" target="#b39">[39]</ref>) or a large scale (e.g., Google Earth Timelapse <ref type="bibr" target="#b11">[12]</ref>). Of particular interest, Martin-Brualla et al. <ref type="bibr" target="#b25">[25]</ref> drew from internet imagery to create years-long timelapse videos, but the noise from internet photos captured by different cameras and at different times requires heavy smoothing so that shorter-term changes are not visible and long-term changes can be hard to detect.</p><p>Several techniques have been proposed to smooth out the aliasing artifacts that result from timelapse sampling after the fact. With consumer video applications (e.g., hyperlapse and timelapse) in mind, Zhang et al. <ref type="bibr" target="#b48">[49]</ref> propose a method for smoothing transitions between frames in temporally subsampled videos. Whereas their method interpolates and smooths after subsampling, our method necessitates fewer modeling assumptions because we smooth discontinuities before temporal subsampling; starting with the densely sampled input also allows us to produce smooth visualizations of any timescale. Their method is also tested only on videos that span minutes or hours of time, and are subsampled to seconds or minutes, whereas ours is designed to work with years-long video streams. Finally, their method is also more computationally expensive, operating around 0.5 frames per second, whereas ours runs at 6 frames per second. While both methods can benefit from parallelization, this is nonetheless a significant difference when considering datasets such as ours that have on the order of 1 billion frames. Details on runtime and datasets can be found in the supplemental material.</p><p>In remote sensing, it is often desirable to visualize long-term changes related to a variety of phenomena caused by humans or natural events. The data is often of low temporal frequency, sometimes only beforeand-after satellite pictures or landscape photographs taken far apart in time, such as the U.S. Geological Survey repeat photography project <ref type="bibr">[41]</ref>. Animation techniques have been proposed which can help create a smooth transition between images. Lobo et al. <ref type="bibr" target="#b24">[24]</ref> does this by simulating plausible intermediary frames, while Harrower <ref type="bibr" target="#b14">[15]</ref> provides the user with control over the spatial and temporal resolution to allow for optimal visualization of a given phenomenon. While our work shares the same goals of visualizing changes happening over long timescales, we work with datasets with high temporal resolution and do not rely on interpolation to smooth transitions. That said, interpolation methods such as Lobo et al. <ref type="bibr" target="#b24">[24]</ref> could be complementary to ours as a possible way to fill in segments of missing data from our datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Temporal Resampling and Video Visualization</head><p>Most existing techniques for video visualization are designed for videos no longer than a few hours, and their end goals often differ significantly from ours. In fact, over ten years ago, Borgo et al. <ref type="bibr" target="#b4">[5]</ref> published a survey of different video visualization techniques; while our work is related to many of the techniques they describe, the authors clearly assumed the use of relatively short videos. This section highlights some of the most closely related work in this area.</p><p>Various works adapt the frame rate, or temporal sampling rate, of a video over time based on its content <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b50">51]</ref>. The most closely related technique is Computational Timelapse <ref type="bibr">[4]</ref>, which uses temporal differences in video frames to dynamically speed up the frame rate when little is changing and slow it down when more changes are occurring. While this is effective as an automatic fast-forward tool, it requires a chosen output video length; furthermore, long-term changes appear choppy and broken up due to sudden changes in the frame rate. Several works have proposed various non-axis-aligned manipulations of spacetime cubes such as videos; Rav-Acha et al. <ref type="bibr" target="#b30">[30]</ref> explore this idea from in a graphics/vision context, while Bach et al.give a thorough visualization-oriented review of the possibilities in this space. These techniques are generally incompatible with cuboids with significantly longer time extent, such as months-long videos.</p><p>Video summarization techniques take another approach-rather than maintaining chronological and spatial continuity, these techniques attempt to find frames or clips that encapsulate the activity in a video. These techniques generally approach the problem by automatically detecting "noteworthy" frames or clips either by using unsupervised saliency-based approaches <ref type="bibr" target="#b29">[29]</ref> or by using example-based learning <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b47">48]</ref>. These techniques tend to focus on the real-time timescale and treat longer-term changes as noise; they also make automated decisions that may remove content of interest even at the real-time timescale.</p><p>Video fast-forward techniques are closely related to the ideas supporting timelapse videos and often use frame-skipping. The disadvantages of timelapse were explored by Hoferlin et al. <ref type="bibr" target="#b16">[17]</ref>. Their evaluation of fast-forward techniques also included an interesting experiment with the use of temporal blending-similar in spirit to our temporal filtering method. However, their filtering approach does not generalize to more than one timescale.</p><p>A few prior works have considered multiple timescales of activity in videos. Motion Denoising <ref type="bibr" target="#b34">[34]</ref> separates a video into short-term and long-term components. This technique produces excellent results, but handles only two timescales and is very computationally expensive, making it impractical for months-long videos, much less for yearslong videos. Wehrwein et al. <ref type="bibr" target="#b44">[45]</ref> propose a method to composite clips from different timescales into a single "scene summary," showing, for example, people walking, clouds moving, and the sun crossing the sky all at once. This method begins with a Gaussian temporal pyramid much like the one constructed as a side-effect of our Laplacian pyramid construction, then composites salient clips from different pyramid layers together; though multiple timescales are visualized at once, the vast majority of the lower pyramid levels are discarded from the final output. In contrast, we construct visualizations that assists in exploration of the whole dataset without any assumptions about which timescales or scene elements are of interest to the viewer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Interactive Video Exploration and Retrieval</head><p>In contrast to the automated methods above, a separate category of prior work facilitates interactive browsing, exploration, and retrieval in video. Though this is more closely related to our task, most of these techniques are oriented towards retrieval of specific content rather than discovery, or towards improving the ability to scrub or seek in a shorter video.</p><p>The Video Browser Showdown <ref type="bibr" target="#b33">[33]</ref> is a yearly contest of video browsing tools designed to locate either a specific event or clip in a video, or locate all instances of an event or action. Tools that are successful on this task (e.g., <ref type="bibr" target="#b21">[21]</ref>) tend to leverage the fact that the sequence of interest is known a priori, making them less useful for discovering unknown anomalies; for similar reasons, these tools are also unlikely to generalize well for the purpose of identifying structure at longer timescales.</p><p>Several techniques have been proposed to show a visual overview of an entire video sequence, or improve scrubbing. Gutwin et al. <ref type="bibr" target="#b12">[13]</ref> proposed a spread-loading scheme to load frames at varying intervals when loading a streaming video to improve the scrubbing experience. They showed that this improved users' ability to seek to a particular point in the video quickly, but even with instantaneous availability of all frames, a months-long video would be tedious to explore by scrubbing. Barnes et al. <ref type="bibr" target="#b2">[3]</ref> create a continuous visual overview of automatically selected keyframes to assist in scrubbing around in the video, while Jackson et al. <ref type="bibr" target="#b17">[18]</ref> arrange short clips of a video in an animated grid so a user can shift their focus to any point in a video or watch a single thumbnail as it cycles through the entire video. These techniques work well for shorter clips, but their utility is limited by available screen size for longer-duration videos.</p><p>From a visualization perspective, Romero et al. <ref type="bibr" target="#b32">[32]</ref> also uses computer vision to analyze and visualize video volumes. In particular, their interface proposed a closely related heat map visualization called the Activity Table that is similar in spirit to our Video Spectrogram. The Activity Table displays aggregate motion computed using thresholded frame differences, closely related to the frame differences performed in the construction of our Laplacian Pyramids. Whereas they plot aggregate motion in a single (real-time) timescale across different spatial locations, we are interested in activity at multiple timescales and use the vertical axis of the heatmap to index temporal frequency instead of spatial location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Time Series Visualization</head><p>Although our work relates to the rich literature on time series visualization (e.g., <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>), video has unique properties that benefit from domain-specific techniques. One notable example from this literature is the work by Cakmak et al. <ref type="bibr" target="#b8">[9]</ref> which does visualize time-varying data at multiple timescales; their interface for traversing temporal scales and viewing summaries at different levels is loosely analogous to our Video Spectrogram, but is geared towards the very different domain of time-varying graph data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Pyramids and Spectrograms</head><p>The pyramid computation component of our approach is directly adapted from image pyramid techniques from the computer vision literature. Our adaptation will be described in detail below. In particular, we compute Gaussian <ref type="bibr" target="#b7">[8]</ref> and Laplacian <ref type="bibr" target="#b6">[7]</ref> pyramids along the time dimension, in contrast with their traditional application to the spatial dimensions of images. The idea of generalizing image pyramid techniques to videos is not new; a related generalization of the Gaussian pyramid to video was proposed by <ref type="bibr" target="#b10">[11]</ref> to send videos at variable resolutions in space and time over limited bandwidth network connections.</p><p>Their approach resembles a Gaussian pyramid (in contrast to our Laplacian pyramid), operates across spatial and temporal dimensions, and is designed for efficient coding and variable-resolution transmission of videos under limited bandwidth. Our method aims to visualize longerterm changes with high fidelity. For the problem of human action recognition, numerous works propose variations of pyramids applied temporally for short (e.g., minutes-long) video clips <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b49">50]</ref>.</p><p>To our knowledge, however, our method is the first to extend the temporal Laplacian pyramid concept to extremely long-duration videos with the intent to visualize long timescales. Finally, our Video Spectrogram tool is directly inspired by the idea of time-frequency representations like the spectrogram, which are commonly used in audio visualization and processing <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">VIDEO TEMPORAL PYRAMIDS</head><p>Our approach is inspired by image pyramids from the computer vision literature, which allow for separation and manipulation of spatial frequencies in images. We generalize these same techniques to the temporal domain in videos in order to separate and manipulate temporal frequencies. Specifically, the core of our Video Temporal Pyramid approach is a Laplacian pyramid computed in the time dimension, approximating the output of a bank of band-pass filters applied pixel-wise across time. We first formally define the pyramid's construction, then discuss several adaptations for the domain of long, fixed-camera videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Definition and Construction</head><p>Image pyramids are a classical technique from the computer vision literature <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, widely used to apply image processing and computer vision algorithms at multiple scales or in a scale-invariant fashion.</p><p>The most basic image pyramid is a Gaussian Pyramid <ref type="bibr" target="#b7">[8]</ref> (Fig. <ref type="figure" target="#fig_0">2</ref> (a)), constructed by repeatedly blurring then subsampling an image. Each subsequent level of the pyramid represents what remains after a lowpass filter is applied to the prior level. Each level of a Laplacian pyramid <ref type="bibr" target="#b6">[7]</ref> (Fig. <ref type="figure" target="#fig_0">2 (b</ref>)) represents the result of a high-pass filter applied to the prior level of the Gaussian pyramid, or equivalently a band-pass filter applied to the original image. The resulting Laplacian pyramid levels contain narrow slices of spatial frequency content of the image, thereby resembling the output of a bank of band-pass filters.</p><p>Where Laplacian pyramids are traditionally used to isolate and manipulate spatial frequency content of images, we instead apply the same procedure to the temporal dimension of a video, leaving the spatial dimensions alone. A temporal analog to the Gaussian pyramid consists of videos that have been filtered and subsampled along the time dimension only. The Laplacian pyramid is also constructed analogously, by subtracting the temporally blurred video from the original. In principle, frames of the Laplacian temporal pyramid can be computed by subtracting the computed blur frames from the current level of the Gaussian pyramid before subsampling. In practice, we use the standard pyramid construction approach given by <ref type="bibr" target="#b6">[7]</ref> to avoid quantization errors and ensure that the pyramid levels can accurately reconstruct the input signal. We first filter and downsample the input, then upsample it again to match the prior level's temporal sampling rate; this downsampled-thenupsampled signal is finally subtracted from the input video to calculate the Laplacian pyramid level.</p><p>The pyramid levels are constructed recursively as shown in Fig. <ref type="figure" target="#fig_2">3</ref> and Algorithm 1. We begin with a long input video, which serves as the first level of the Gaussian temporal pyramid. Each subsequent level is computed in one pass through the prior level's video, calculating blurred frames from a sliding window of prior level frames. The resulting pyramid levels are themselves videos of the same spatial resolution and covering the same real-world duration in time, but with a reduced frame count. For this reason, each level of a Gaussian temporal pyramid is similar to a timelapse video with a particular sampling rate. One key difference is that blurring across time before subsampling causes short-term motions to blur out in higher levels of the pyramid, thus eliminating aliased content that would appear in a true timelapse video. The levels of the Laplacian temporal pyramid are less intuitive to watch, as they contain only specific bands of temporal frequency content. Sample Laplacian pyramid frames are shown in Fig. <ref type="figure">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Construct temporal pyramid</head><p>Input: V , an input video of size (H ×W ×C× Frames) Output: G = G 1...N , the Gaussian pyramid Output: L = L 1...N , the Laplacian pyramid for i ← 1 . . . N do F ← f ilter(V ) ▷ apply linear 1D blur in time V ′ ← subsample(F) ▷ e.g., if stride=3, keep every 3rd frame</p><formula xml:id="formula_0">G i ← V ′ F * ← f ilter(upsample(V ′ )) ▷ account for quantization error L i ← V − F * V ← V ′ ▷ set up input for next level end for</formula><p>In a Laplacian temporal pyramid the original source video has been decomposed into multiple component videos. These components can be reassembled to create an exact copy of the source by performing the pyramid-creation steps in reverse order (see <ref type="bibr">Algorithm 2)</ref>. It is also possible to proceed with this reconstruction while leaving out certain Laplacian pyramid levels (specified in Algorithm 2 using the W vector). Reconstructing without the last few Laplacian layers yields a smooth but slower-moving version a Gaussian blur level. We found this smooth temporal upsampling option to be quite useful for some of our videos, as it provides a way to slow down the action so more information can be absorbed by the viewer.</p><p>Each level of the pyramid represents a specific timescale. For instance, the base frame rate of most webcam video is 30 frames per second (fps), or 1/30th of a second per frame. Motions easily visible at this frame rate can be thought of as belonging in the "1/30th of a second" timescale. Higher levels of the pyramid represent changes happening at a frequency of "once per second" or "once per day" or even "once per month."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adaptations for Months of Static-Camera Video</head><p>The prior section described a straightforward generalization of the Gaussian and Laplacian image pyramids to construction of temporal ▷ reconstructs original when k = 0 and W k+1...N = 1 pyramids. We now describe a few simple adjustments we made to adapt these pyramids for the use case of visualizing and exploring long, fixed-camera video streams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Variable Downsampling Rates</head><p>In image pyramids, the filter width and subsampling rate are parameters that are traditionally tuned according to the application. For example, to achieve scale invariance for computer vision algorithms, a subsampling rate smaller than 2 is often desirable to detect objects at a densely sampled range of sizes. In our application, the pyramid is unlikely to miss anything, even with a larger sampling rate because motions and dynamics tend to be visible in a range of timescales. For example, in a 30 frames-per-second (30fps) video of a person walking through a courtyard, the person might take 6 seconds to walk through the scene, and thus would appear, moving increasingly quickly and increasingly blurred, in at least the first four or five levels of the pyramid. While we initially simply used a downsampling rate of 2, problems arise when the temporal sampling rate of each pyramid level is not aligned with intuitive units of time. As discussed in <ref type="bibr" target="#b0">[1]</ref>, modeling time has many complexities to consider. For example, if our input video (Gaussian pyramid level 0) is captured at 1/30 second per frame (30 frames per second) and we chose a fixed scale factor of 2x, then level 5 of the pyramid would cover 1.066 seconds per frame, level 10 would cover 34.133 seconds per frame, and level 22 would cover 1.618 days per frame. In addition to being less intuitive for interpretation, these sampling rates can introduce aliasing at higher sampling rates due to periodic phenomena such as the day/night cycle or seasonal changes. Since our goal is to visualize and discover structure at long time-scales, it is important to have sampling rates lined up with known patterns such the 24-hour cycle of the day and the 365-day cycle of the year. Achieving this alignment requires choosing different subsampling rates for different levels of the pyramid. See Supplementary Material for details of the sampling rates used and timescales represented at all levels of our pyramids. We used strides of 2, 3, and 5, which necessitated the use of different blur filters depending on stride. The one-dimensional blur filter applied across frames was [1,2,2,1] when the stride was 2; [1,2,3,2,1] when the stride was 3; and [1,2,3,4,5,4,3,2,1] when the stride was 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Scaling to Months and Years of Video</head><p>The algorithm as described thus far requires a full pass through each pyramid level to compute the next; because the layers sizes shrink exponentially, this requires the equivalent of roughly 2 passes through the full input video, for an O(n) runtime. However, for months-long input videos such as ours, this is still very slow and can be easily parallelized. To accelerate the computation of pyramids, we compute one-day pyramids in parallel on a cluster, then merge the one-day pyramids to compute the higher pyramid levels. These one-day pyramids are computed up through level 15, where the Gaussian blur video for the entire day is 1 frame, and the corresponding Laplacian pyramid level shows activity changing on a 12-hour timescale. The one-day pyramids are then merged by stitching together each day's 1-frame  Gaussian 'video' into a full blur video for level 15, which is then used as the source video for the construction of the remaining pyramid levels.</p><p>We also parallelize the creation of years-long videos in the same manner, running each year separately and then stitching them together and continuing the construction of multi-year pyramid levels. In addition to being efficient, this also allows us to approximate a 365-(or 366-) day year with only 360 frames, which is necessary in order to use only sub-sampling rates of 2, 3, and 5. For each individual year, after analyzing which days have the most missing frames, we choose 5 days to remove from the pyramid (or 6 days for a leap year). If we used this 360-day year and did not compute each year separately, we would end up with a true 360-day timescale and some aliasing over the course of multiple years, where the year shown would slowly get out of alignment with the calendar year. When we parallelize the years, we end up with 1 frame per year at level 21 (the 1-year timescale), which line up with calendar years, and the higher levels can be built on that solid foundation. We currently sub-sample with powers of 2 for multiple-year timescales, but it would be possible to sample by 2 and then 5 (or 5 and then 2) in order to create a 1-decade timescale.</p><p>Missing data is filled in with all-black frames in our temporal pyramid videos. See the supplementary material for a more detailed description of how we handled missing data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">VIDEO SPECTROGRAMS</head><p>At this point, we have described an approach for parsing out temporal frequency content of a long video stream into timescales by constructing a temporal Laplacian pyramid. Watching just the upper level videos of the pyramid is an efficient way to gain an overview of temporal dynamics and long-term events because they are short but distill important information. However, the pyramid itself does not make it any more tractable to watch the entirety of the lower levels, which still have very long durations. To help address this, we propose a visualization tool called the Video Spectrogram that facilitates interactive navigation and exploration of the pyramid levels.</p><p>The Video Spectrogram user interface evolved to include multiple elements, as shown in Fig. <ref type="figure">5</ref>; however, the main element and key idea is a 2-dimensional plot that provides an overview of the entire pyramid by showing time on the horizontal axis and timescale (frequency) on the vertical axis. Each cell in this time/frequency grid represents a 2D frame from one of the pyramid videos, which would be unwieldy to visualize in such a small space; instead, we abstract the spatial details and display a single quantity that captures aggregate activity.</p><p>By construction, the Laplacian pyramid layers are "difference" frames representing only content that has changed at the corresponding timescale. Therefore, the aggregate activity for a given frame in a timescale can be measured by taking a norm of the Laplacian frame. We chose the L 2 norm (i.e., the square root of the sum of squared pixel values), and display it on a logarithmic scale. We experimented with other norms (L 1 ) and color scales (linear). Because we aggregate across pixels, the L 2 norm gives more weight to spatially smaller changes with larger magnitude versus more widespread, smaller-magnitude changes. The logarithmic color map does a better job of showing contrast in low-activity regions, allowing subtler patterns to be detected when overall activity levels are low.</p><p>We compute the norm for each frame in each Laplacian pyramid level, and display the resulting values as a 2-dimensional heatmap as shown in Fig. <ref type="figure">1</ref> and Fig. <ref type="figure">5</ref>, where each tile in the heatmap is the norm of one Laplacian pyramid frame. Tiles in higher timescales become wider because the same temporal extent is represented using fewer frames at higher pyramid levels.</p><p>The temporal pyramid videos and the spectrogram plot are closely linked; the purpose of the spectrogram is to help explore the pyramid, so we include a large and prominent video player to show the pyramid videos. The full-spectrogram plot is good for an overview; however, we found that since we generally watch the video from one level at a time, it was useful to enlarge the portion of the heatmap corresponding to the level being watched in the video. We visualize this single-level spectrogram below the video, along with a moving vertical line that travels along the plot as the video plays. As the user sees an event unfold in the video they can get a sense of what the spectrogram shows during the event. The full-spectrogram plot always includes a red outline marking the level and/or date being viewed in the current video, for a 'you are here' connection to the bigger picture. Both plots have pan and zoom functionality to assist with overview-to-detail visualization.</p><p>In order to go in the other direction, and see what the video content is like at a particular spot by starting from the spectrogram, we implemented a mouse-over functionality whereby a thumbnail image of the corresponding video frame shows up underneath the plot when the mouse hovers over a cell of the single-level spectrogram. Easy access to those thumbnail images gives the user hints about the reason for the  Fig. <ref type="figure">6</ref>. Sample frames from our datasets, along with the names we are using to refer to them in this paper, their covered timespan, and their base frame rate. More details in supplemental material. structure in the heat map and helps determine whether it's useful to drill down or investigate further in that area. The thumbnail-on-hover functionality also makes it possible to "scrub" through the video for that timescale by dragging the mouse horizontally over the plot at any speed.</p><p>Users can navigate to different levels of the pyramid using a dropdown menu at the top of the user interface. At the 5-minute timescale or lower, the user is given the choice to view a particular date instead of the whole timespan, since those levels are very large and the assumption is that only a portion will be watched. Once a date is selected, it will stay selected while the user navigates down to lower levels, making it easier to 'drill down' on interesting content. Also, the user can stay on one level and easily select a different day on that same level to view, which helps for comparing dynamics across different days. When a user hovers to view a thumbnail, the image also displays the date and time (or timespan) represented by that frame. This information is valuable for following the thread of an event from upper to lower levels to gain greater detail and pinpoint the timing of that event. Quick access to date and time information also allows the user to make use of their own knowledge of specific past events at that location or patterns of life relevant to that scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>To demonstrate our approach, we scraped or downloaded 10 datasets captured by outdoor webcams, with lengths ranging from 30 days to 16 years. Details for these datasets are included in the supplemental material, and a quick visual reference is included in Fig. <ref type="figure">6</ref>. The pyramid videos and spectrograms revealed interesting dynamics and structure at a range of timescales. Below are some general observations, as well as specific findings for several datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Cycles and Visualization of Periodicity</head><p>Events that happen repeatedly stand out with clarity in our pyramid videos. The day/night cycling is the obvious example, and this is noticeable in all of our datasets. However, we found many other examples of cyclic activity that appeared in the videos as distinctive repetitive patterns. Tidal patterns were apparent in the Buxton oceanside dataset as well as the Geiranger ferry dock dataset. Geiranger shows boats rising and falling next to a dock. In the timescales below 1-day, the boats move up and down as the video progresses, and in the timescales longer than 1-day the movement becomes averaged and the video shows a ghostly blur encompassing all of the vertical positions of the boat over time. The cyclic nature of seasons becomes obvious in the datasets with multiple years, such as Hiuchi, Kutcharoko, and Smoky, where we can view the seasons changing fast enough that we understand the similarities of the cycle from year to year. The switching between white snowy winter and green leafy summer becomes a visual rhythmic pulse at the higher pyramid levels, just like the day/night changes at the lower pyramid levels.</p><p>On a smaller scale, the natural cycles of plant growth are nicely visualized in the Mad River dataset. This scene includes a deciduous tree in the foreground, as well as bushes and other trees on the edge of a river. The tree in the foreground loses its leaves and grows them again, and the bushes and other plants can be viewed getting larger in the summer and smaller in the winter. Another very interesting discovery is how the branches of the foreground tree droop at night and perk up during the day, which becomes more noticeable because it happens repetitively. This dataset seems to have its camera recording with infrared at night, which fortunately makes the tree always visible.</p><p>In addition to the pyramid videos themselves, the Video Spectrogram also seems to be especially good at visualizing cyclic events. The day/night cycle is very obvious at the 12-hour timescale, with (usually) more activity and a lighter color on the spectrogram for the half of the day which is mostly daylight, and (usually) a darker color for the lessactive night. The dark and light colors on the spectrogram switch back and forth creating a distinctive pattern at that level of the pyramid for most of our datasets. In the higher levels of the multiple-year datasets, the seasonal pulsing is also clearly visualized with the spectrogram colors.</p><p>We also found other examples where the periodicity of human activity showed up clearly in the spectrogram (Fig. <ref type="figure" target="#fig_5">7</ref>). In the Bryant Park ice rink dataset, the spectrogram had lighter colors during the times the rink was busy with skaters and darker colors for the times when the ice was cleared. This seemed to happen on a cycle of about an hour and a half, presumably a planned timing. Another example is from the Bridge dataset, where the spectrogram shows a light bar every time the drawbridge goes up which happened frequently on Memorial Day in 2021. Since this was a repeating event over the course of that day, its pattern on the spectrogram was more noticeable than it was on the days where the drawbridge only went up once or twice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Multiscale Visualization and Drill-Down Navigation</head><p>The video spectrogram tool connects events at different timescales by virtue of using a common timeline. If the viewer sees a short blip of some interesting or anomalous event at a higher timescale, the user allows for pinpointing the general date/time of the anomaly and drilling down to lower levels at that same time in order to see more detail. For example, one might see a truck appear 'out of nowhere' in the 6-day timescale, then quickly drill down to the day it appeared and view the 5-second timescale on that day in order to see which direction the truck drove in from before it parked (Fig. <ref type="figure" target="#fig_6">8</ref>, top). Most real-world events do not fall neatly into one discrete timescale, and this often means that an interesting event can first be discovered from the bird's-eye perspective of a higher pyramid level and then the full extent of the occurrence can be discovered and viewed by drilling down to lower levels. We found an example of this in the Mad River dataset (Fig. <ref type="figure" target="#fig_6">8</ref>, bottom), when a bright pink object catches the eye briefly and then disappears during the 5-minute timescale on October 6, 2020, in a corner of the scene. Drilling down to the 5-minute level, the pink object appears to be a pink jacket left on a rock but it still disappears quickly. Drilling down to the 1-minute level we can see fast-moving people and we also see that the pink jacket moves from one rock to another. However, it is not until we drill down to the 5-second timescale that we can make out the group of people and their general movements. The fact that they left a bright pink jacket in one place for about 15 minutes, while they themselves moved faster, left a clue to their presence in that longer 15-minute timescale video.</p><p>The multiple timescale visualization also provides a useful and possibly educational demonstration of the role and scope of human activity in a particular scene. In the Bryant Park dataset, the lower level pyramid videos show crowds of people skating. However, the higher level pyramid videos show an eerily empty skating rink, with no humans in sight. At the 4-hour timescale and above, it is mainly the lighting changes, and certain infrastructure changes that stand out (such as a rink-side tent being erected for a while). In the Mid Mountain ski slope dataset, tire tracks and ski tracks in the snow provide a clue to human activity at the lower levels of the pyramid, but we don't see the humans making those tracks unless we drill down. The same thing can be seen with tire tracks and footprints in the sandy beach of the Buxton dataset. With both snow and sand, the evidence of human activity is melted or eroded away fairly quickly. In contrast, the Rane construction dataset shows a scene where the long term effect of human activity is exactly the point, and in that case it is definitely instructive to drill down and see exactly which lower-timescale activities were responsible for the higher-timescale view of a building being constructed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Discovery of Anomalies</head><p>The pyramid videos by themselves, as well as the spectrogram tool, can be useful for surfacing anomalous events. Missing data is the most obvious anomaly to find, and sections of missing data show up as all-black frames in the videos and as solid dark colored areas of the spectrogram, as can be seen in Fig. <ref type="figure">5</ref>. Corrupted data is another kind of anomaly that shows up in the videos and the spectrogram. For instance, in the Buxton coastline dataset the high-level videos have a section that shows a static night scene which is clearly out of place since it lasts longer than a single night both in the pyramid video and on the timeline. We traced the problem back to the original footage, where a static image was looped for a while.</p><p>Many anomalies are related to the camera itself, the most common of which is a sudden change in camera angle or placement. The Mid Mountain dataset includes a few months during ski season where the camera zooms or moves closer to the ski slope. The Buxton beach video includes a section where the camera faces out to the ocean instead of along the coastline. Sometimes camera errors can be seen, such as a day in the Hiuchi dataset which included camera footage of an office ceiling when normally the scene is outdoors in the mountains. That event occurred directly after a long period of missing data, so we suspect the camera was being repaired before being reinstalled outdoors. When there is a rainstorm or snowstorm, the video will often show raindrops on the camera lens for a short while at the lower timescales. The most entertaining camera-related anomalies we found occured when birds perch in front of the camera (Buxton) or a spider builds a web on it (Mad River).</p><p>In contrast with standard timelapse, the pyramid videos are visually less cluttered and they can be upsampled to adjust the rate of change to be easier to absorb. Anomalous events are more distinctive against this backdrop and are thus fairly easy to spot. For instance, when watching the Buxton dataset video for the 4-hour timescale, we noticed that a railing suddenly appears directly underneath the camera. At that level we could localize it to May 14, 2020, using the Video Spectrogram. We went directly to the 5-minute timescale for May 14, 2020, but the railing was there at the beginning of the day, so we switched to May 13, 2020, and drilled down further. We could see the railing put into place in the real-time original video for May 13, 2020. Even then it was very fast and rather anti-climactic since there was no visual of the person putting it in place (see Fig. <ref type="figure" target="#fig_7">9</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Long-term Dynamics and Understanding</head><p>Our pyramid videos provide a window into the reality of long-term dynamics without sacrificing much verisimilitude. There is some level of blurring that occurs at the upper levels of the pyramid, since it has averaged a lot of small changes over time. We also see the blending of day and night scenes. These factors mean we sacrifice some precision and spatial resolution at the longer timescales; however, we found we were still able to discern larger patterns. For instance, in our Mid Mountain dataset, the gradual pattern of snow melt on a ski slope over the course of days or weeks stands out with clarity at the 6-day timescale. In our Buxton oceanside dataset, at the 1-month timescale, the water's edge can be seen slowly changing its position relative to the beach, slowly rising and retreating much slower than the tides.</p><p>Another way these pyramid videos contribute to understanding of long-term dynamics is through the knowledge that anything showing up in a particular timescale must have generally stayed in the same place for a long enough time, related to the timescale. In the 1-hour timescale, a car driving by would not show up but a car parked in one spot for at least an hour would show up, and the duration of its appearance in the video would correspond to how long it stayed parked. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Direct Comparison with Timelapse</head><p>For few datasets, we constructed standard timelapse videos by subsampling at different rates and compiling the resulting frames together into videos for each timescale. At the top levels of the pyramid this resulted in extremely short timelapse videos (less than one second), which were thus not very informative or interesting. However, going down the pyramid levels, once the videos were at least a few seconds long they did provide a good baseline for comparison with our pyramid videos. We found consistent results among all of the datasets which are summarized below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1">High Levels: 1-day Timescale and Above</head><p>The visual smoothness of our pyramid videos stands in stark contrast with the timelapse videos. At the higher levels especially, each frame of the timelapse is far removed in time from its neighboring frames, increasing the likelihood of major discontinuities in lighting, weather, and other large scene elements. The timelapse videos show the viewer all of these images in rapid succession and the effect is visually chaotic.</p><p>Only the most obvious changes can get absorbed by the viewer. The rest of the changes are likely to get lost in the noise. Also, smoothly upsampling our videos to spread out over a longer duration makes them more informative and watchable than the timelapse videos, even after using the video player tools to slow the timelapse videos down to quarter speed. At the 3-day timescale, the timelapse video duration was 4 seconds. Slowing it down to quarter time extended it to 16 seconds. However, our upsampled pyramid video for that timescale was 24 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">Middle Levels: 2-hour to 12-hour Timescales</head><p>The timelapse videos from these levels are almost unwatchable because of the strobe effect caused by rapid switching between day and night as the video progresses. This problem would likely be fixed by the complete removal of night-time frames. We did not test that idea, but we believe that even with night frames removed, the timelapse videos from these levels would still suffer from similar faults as they do in the higher and lower levels. Also, for a fair comparison we would also need to remove the night frames from our pyramid videos, and this would likely improve the watchability of those videos as well. We have currently bypassed the strobe effect problem in our pyramid videos by upsampling them so they take longer to watch while the pulsing from day to night happens at a gentler cadence. If we removed night frames, we would not need to upsample so much and could watch shorter videos. However, for the majority of our datasets there is interesting visual activity during the night hours which we would not want to arbitrarily excise from the video for the sake of watchability or efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.3">Low Levels: 1-hour Timescale and Below</head><p>The timelapse videos are more watchable and informative at lower levels than they are at the higher levels. When comparing a timelapse video with a non-upsampled pyramid video (i.e., of the same length), the pyramid video provides only a slightly better viewing experience because of its smoothness. Upsampling our pyramid video definitely improves its viewing quality.</p><p>The most interesting comparison occurs at the very lowest levels (1-minute timescale and below), where we can watch videos for one day at a time and see fast-moving activity such as people skiing and cars driving. In the timelapse videos, fast moving people and cars end up aliased, meaning they show up as a completely solid object and disappear quickly, without a clear trajectory. By contrast, in the pyramid videos, fast moving people and cars will show up as a line of ghostly versions of themselves, along their trajectory. They will only solidify if they stay in one place for long enough. This provides the viewer with more information than the timelapse videos provide. As an example, in the Rane construction site dataset car traffic can be seen on the road in front of the building site at the 15-second timescale. In the timelapse video, we see cars at night and during the day and we can get a general sense that there is less traffic at night. However, in the pyramid video the day/night traffic difference is clearer. We can barely register a blur for night-time traffic, and there is clearly more traffic during the day. It is blurry for the most part, except for when cars stop at the stoplight at regular intervals, at which point they 'solidify' into a clear line of cars. There is a rhythm to this blurred/not-blurred traffic, presumably corresponding with the traffic light schedule. Also, when the cars are stopped we can see that there are almost always more cars in the right lane (possibly getting ready to turn right) This is more information than we would ever glean from the timelapse videos and provides a sound argument for the very basis of our video temporal pyramid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION, LIMITATIONS, AND FUTURE WORK</head><p>This work proposes a novel way to visualize the passage of time and explore videos that are too large to be practically explored using traditional tools. We also specifically address phenomena that occur at much longer timescales than most existing methods; these phenomena are present and interesting in our application due to the extreme duration of our videos. Our method compares favorably to naive timelapse videos. Even as an imperfect visualization tool, timelapse videos have been put to good use in a diverse range of applications, such as construction site monitoring <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b46">47]</ref>, environmental monitoring <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b35">35]</ref>, art <ref type="bibr" target="#b13">[14]</ref>, education <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b41">42]</ref>, ecological awareness <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20]</ref>, and more. An improvement to existing timelapse techniques could benefit all of these existing applications and possibly lead to interesting new applications. It might also help facilitate a shift in perspective towards long-term thinking. Humanity's short-term or 'real time' bias can make it difficult to tackle long-timescale issues like climate change or urban sprawl. If we don't see it happening, we don't care about it as much. Visualization tools can help us see it <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b27">27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Limitations</head><p>Our method has some important limitations. One is the requirement that the camera viewpoint be fixed. This ensures that changes in the video are due to the scene, rather than camera motion; however, if camera angle changes are infrequent then the spectrogram is minimally affected, and in fact our method is useful for discovering unusual camera events such as a change in viewpoint as discussed in Sect. 5.3.</p><p>Another limitation is that the top two or three levels of the pyramid are usually not very informative because there are not enough frames available for any changes to register when stitching the frames together. In a pyramid built from one year's worth of data, the 90-day timescale will only have 4 representative frames. However, in a pyramid built from 8 year's worth of data, the 90-day timescale will include 32 representative frames. We can discern changes over 32 frames much more easily than over 4 frames.</p><p>We also noticed that as the pyramid levels progress higher, the edges of all scene elements tend to become slightly less sharp with each new level. This is probably caused by very small camera movements that register as brief 'whole scene changes' with an effect in the pyramid that compounds as levels are built recursively. This effect could possibly be mitigated by the addition of a video stabilization preprocessing step. Simple feature matching-based image alignment techniques <ref type="bibr" target="#b38">[38]</ref> could be used to align the frames to minimize movement due to camera shakiness. A similar feature matching technique could be used to manage camera viewpoint changes as well. We found that the noise present in our datasets was small enough that these techniques were not necessary, but they could be used to boost visual quality if desired. They could also help our method generalize to more datasets, such as those with automated and periodic changes in viewpoint, or fixed-viewpoint cameras that exhibit noticeable motion due to wind.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Future Work</head><p>Our temporal pyramid computes a very simple, low-level measure of intensity change from one frame to the next. In the spirit of Viz-a-Vis <ref type="bibr" target="#b32">[32]</ref>, we intend to explore more sophisticated types of analyses that can be aggregated into heatmaps to show more high-level and/or task-specific measures of activity. For example, optical flow could be used to measure motion rather than per-frame intensity change at multiple timescales (similar to <ref type="bibr" target="#b44">[45]</ref>). In scenes with specific object categories of interest (e.g., people, cars etc.), object detection or crowd counting techniques could extract more meaningful trends which could then be visualized in a similar time-frequency spectrogram.</p><p>Although our method was not designed with the intent of video anomaly detection, it could provide the basis for some new techniques in that area. One of the many existing video anomaly detection methods <ref type="bibr" target="#b28">[28]</ref> could possibly be applied to upper level pyramid videos in order to quickly and automatically surface unusual events at those timescales, which might yield insight upon drill-down to lower levels. For instance, detecting the origin of an unattended bag, after the fact, would be likely made easier with the aid of a temporal pyramid.</p><p>Another direction for future work is to explore different types of visualizations for our spectrogram, other than a heatmap. For example, a circular or radial representation might be useful for visualizing periodic events. We would also like to find ways to more easily compare different days with each other (or different years, months, etc.), even if the days chosen for comparison are far removed from each other in time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we presented the Video Temporal Pyramid -a multi-scale lens through which to view the passage of time via a process that distills activity happening at different timescales in long fixed-camera video streams. We also presented the Video Spectrogram, a time-frequency visualization to facilitate exploration and discovery in our pyramids. The pyramid videos present a novel alternative to standard timelapse techniques, providing a smooth viewing experience that allows for the absorption of more information about how a scene changes over time. And the spectrogram visualization is the first example of what we believe is a more general and potentially useful class of time-frequency representations for video visualization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Traditionally, Gaussian and Laplacian pyramids are applied to both spatial dimensions of an image. The left column (a) shows a Gaussian pyramid each layer of which is blurred and subsampled from the prior one. Each level of the Laplacian pyramid (b) is a high-pass filtered version of the corresponding Gaussian level, computed by subtracting the blurred level from the original.</figDesc><graphic url="image-74.png" coords="3,488.98,152.12,70.44,70.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>30 fps (frames / second) -"real time video"</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. An overview of the temporal pyramid construction process. Left: an illustration of the algorithm for computing one level of the Gaussian and Laplacian pyramid. A source video (the input, or a prior level of the Gaussian pyramid) is blurred in the temporal dimension. The subsequent level of the Gaussian pyramid is computed by subsampling this blurred video, while the Laplacian pyramid level is computed by subtracting the blurred video from the source. Right: The resulting pyramids are the collection of videos generated using the above algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. Examples of frames from Laplacian Difference videos at different frequencies, taken from the same webcam but not necessarily from the same days. These show the pixels that changed during that time span. (A) people walking; (B) evidence that the sun peeked out from behind some clouds; (C) a golf cart or similar slow-moving vehicle; (D) evidence of the sun's movement across the sky which has cast shadows of the trees on the ground; (E) outlines of snow patches which likely means that those patches melted over the course of the day; (F) most elements of the scene are visible, including fall colors, which likely means that the autumnal seasonal changes that month affected most pixel values.</figDesc><graphic url="image-83.png" coords="6,49.35,512.41,243.84,138.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Examples of periodic human activity showing in the spectrogram.</figDesc><graphic url="image-98.png" coords="7,125.57,188.55,95.40,53.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. TOP: In Mid Mountain, a truck abruptly appears and quickly disappears in the 3-day timescale. Drilling down to the 5-second level is necessary to learn that the truck drives in from the left and backs into its parking spot. BOTTOM: In Mad River, a pink jacket left on a rock shows up briefly at the 5-minute timescale. Drilling down reveals a group of people moving around.</figDesc><graphic url="image-107.png" coords="7,380.26,267.68,135.07,75.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. In the Buxton dataset, at the 4-hour timescale, a railing quickly and obviously appears directly under the camera. Drilling down, we had to go to the original real-time recording to see it being put into place.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 2 Reconstruct pyramid level videos or upsample Input: k ∈ {0 . . . N − 1}, chosen ending level Input: L k+1...N , Laplacian temporal pyramid videos Input: G N , the top level Gaussian temporal pyramid video Input: W ∈ {0, 1} N , indicator vector of detail levels to reconstruct Output: R k , reconstructed video from level k B ← G N for i ← N . . . k + 1 do</figDesc><table><row><cell cols="2">U ← upsample(B) ▷ e.g., if stride=3, repeat each frame 3 times</cell></row><row><cell>F ← f ilter(U)</cell><cell>▷ use same filter from pyramid construction</cell></row><row><cell>B ← F + (L i ×W i )</cell><cell>▷ add detail layer if applicable</cell></row><row><cell>end for</cell><cell></cell></row><row><cell>R k ← B</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported in part by NASA Award NNX15AJ98H under the Washington NASA Space Grant Consortium, and in part by the National Science Foundation under Grant No. 2105372. The Washington NASA Space Grant Consortium is funded by the NASA Office of Stem Engagement. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of NASA or the NSF.</p><p>The authors wish to thank Ann Tseng and Richie Mohan for their early contributions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Visualization of time-oriented data. Human-computer interaction series</title>
		<author>
			<persName><forename type="first">W</forename><surname>Aigner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Miksch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tominski</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-0-85729-079-3</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Timecluster: dimension reduction applied to temporal data for visual analytics. The Visual Computer</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Williams</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00371-019-01673-y</idno>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Video tapestries with continuous temporal zoom</title>
		<author>
			<persName><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<idno type="DOI">10.1145/1778765.1778826</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2010-07">July 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Computational time-lapse video</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mcmillan</surname></persName>
		</author>
		<idno type="DOI">10.1145/1276377.1276505</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Gr</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">2007</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Survey on Videobased Graphics and Video Visualization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Borgo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Daubney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grundy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heidemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Höferlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Höferlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jänicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<idno type="DOI">10.2312/EG2011/stars/001-023</idno>
	</analytic>
	<monogr>
		<title level="m">Eurographics 2011 -State of the Art Reports. The Eurographics Association</title>
				<editor>
			<persName><forename type="first">N</forename><surname>John</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Wyvill</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Capturing change: the duality of time-lapse imagery to acquire data and depict ecological dynamics</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M B</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Forsberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Caven</surname></persName>
		</author>
		<idno type="DOI">10.5751/ES-09268-220330</idno>
	</analytic>
	<monogr>
		<title level="j">Ecology and Society</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The laplacian pyramid as a compact image code</title>
		<author>
			<persName><forename type="first">P</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adelson</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCOM.1983.1095851</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Communications</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="532" to="540" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast filter transform for image processing</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Burt</surname></persName>
		</author>
		<idno type="DOI">10.1016/0146-664X(81)90092-7</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics and Image Processing</title>
				<imprint>
			<date type="published" when="1981">1981</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="20" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multiscale snapshots: Visual analysis of temporal summaries in dynamic graphs</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cakmak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Schlegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jäckle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Keim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schreck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="517" to="527" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Time-frequency analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Prentice-Hall, Inc., USA</publisher>
			<biblScope unit="volume">778</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multiresolution video</title>
		<author>
			<persName><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
		<idno type="DOI">10.1145/237170.237266</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Gr</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><surname>Google</surname></persName>
		</author>
		<ptr target="https://www.youtube.com/watch?v=5W-zPqrGQWA" />
		<title level="m">Timelapse in Google Earth</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving early navigation in time-lapse video with spread-frame loading</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gutwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Der Kamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Uddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stavness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vail</surname></persName>
		</author>
		<idno type="DOI">10.1145/3290605.3300785</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2019 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Video installation as a creative means of representing temporality in visual data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hansen</surname></persName>
		</author>
		<idno type="DOI">10.1080/14780887.2018.1430011</idno>
	</analytic>
	<monogr>
		<title level="j">Qualitative Research in Psychology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="292" to="297" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visualizing change: Using cartographic animation to explore remotely-sensed data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Harrower</surname></persName>
		</author>
		<idno type="DOI">10.14714/CP39.637</idno>
	</analytic>
	<monogr>
		<title level="j">Cartographic Perspectives</title>
		<imprint>
			<biblScope unit="issue">39</biblScope>
			<biblScope unit="page" from="30" to="42" />
			<date type="published" when="2001-06">Jun. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Digital camera monitoring of recreational fishing effort: Applications and challenges</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Hartill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Weltersbach</surname></persName>
		</author>
		<idno type="DOI">10.1111/faf.12413</idno>
	</analytic>
	<monogr>
		<title level="j">Fish and Fisheries</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="204" to="215" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Evaluation of fast-forward video visualization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Höferlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kurzhals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Höferlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heidemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2012.222</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2095" to="2103" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Panopticon: A parallel video overview system</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nicholson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Stoeckigt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wrobel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thieme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Olivier</surname></persName>
		</author>
		<idno type="DOI">10.1145/2501988.2502038</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology, UIST &apos;13</title>
				<meeting>the 26th Annual ACM Symposium on User Interface Software and Technology, UIST &apos;13</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Realtime hyperlapse creation via optimal frame selection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kienzle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Toelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Uyttendaele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.1145/2766954</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Gr</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">4 environmental time-lapse videos you have to see to believe</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kotack</surname></persName>
		</author>
		<ptr target="https://www.sierraclub.org/sierra/2014-5-september-october/green-life/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m">time-lapse-videos-you-have-see-believe</title>
				<imprint>
			<date type="published" when="2014-10">Oct 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SOM-hunter: Video browsing with relevance-to-SOM feedback loop</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kratochvíl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Veselý</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mejzlík</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lokoč</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-37734-2_71</idno>
	</analytic>
	<monogr>
		<title level="m">MultiMedia Modeling</title>
				<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Temporal extension of scale pyramid and spatial pyramid matching for action recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1408.7071</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Two-year comparative study of snow cover dynamics and its impact factors on glacier surface</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qing</surname></persName>
		</author>
		<idno type="DOI">10.1007/s12665-015-5075-2</idno>
	</analytic>
	<monogr>
		<title level="j">Environmental Earth Sciences</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Animation plans for before-andafter satellite images</title>
		<author>
			<persName><forename type="first">M.-J</forename><surname>Lobo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Appert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pietriga</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2018.2796557</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1347" to="1360" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Time-lapse mining from internet photos</title>
		<author>
			<persName><forename type="first">R</forename><surname>Martin-Brualla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gallup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<idno type="DOI">10.1145/2766903</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Gr</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Reframing temporality in participatory visual research with timelapse video</title>
		<author>
			<persName><forename type="first">B</forename><surname>Monea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stornaiuolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Catena</surname></persName>
		</author>
		<idno type="DOI">10.1177/14687941211019524</idno>
	</analytic>
	<monogr>
		<title level="j">Qualitative Research</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multitimescale education program for temporal expansion in ecocentric education: Using fixed-point time-lapse images for phenology observation</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fujiwara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<idno type="DOI">10.3390/educsci9030190</idno>
	</analytic>
	<monogr>
		<title level="j">Education Sciences</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A comprehensive review on deep learning-based methods for video anomaly detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">C</forename><surname>Pati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Das</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.imavis.2020.104078</idno>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page">2021</biblScope>
			<date>104078</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Nonchronological video synopsis and indexing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rav-Acha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2008.29</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1971" to="1984" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Evolving time fronts: Spatio-temporal video warping</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rav-Acha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Video summarization using fully convolutional sequence networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01258-8_22</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="347" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Viz-a-vis: Toward visualizing video through computer vision</title>
		<author>
			<persName><forename type="first">M</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Summet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Abowd</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2008.185</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1261" to="1268" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Interactive video retrieval in the age of deep learning -detailed evaluation of VBS 2019</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rossetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gasser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lokoč</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schoeffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Muenzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Souček</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bolettieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leibetseder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vrochidis</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMM.2020.2980944</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="243" to="256" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Motion denoising with application to time-lapse photography</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2011.5995374</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition, CVPR &apos;11</title>
				<meeting>the 2011 IEEE Conference on Computer Vision and Pattern Recognition, CVPR &apos;11</meeting>
		<imprint>
			<publisher>IEEE Computer Society, USA</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="313" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Tracking vegetation phenology across diverse biomes using version 2.0 of the phenocam dataset</title>
		<author>
			<persName><forename type="first">B</forename><surname>Seyednasrollah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hufkens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Milliman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Friedl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Frolking</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41597-019-0229-9</idno>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Spatio-temporal laplacian pyramid coding for action recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCYB.2013.2273174</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="817" to="827" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The eyes have it: a task by data type taxonomy for information visualizations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
		<idno type="DOI">10.1109/VL.1996.545307</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings 1996 IEEE Symposium on Visual Languages</title>
				<meeting>1996 IEEE Symposium on Visual Languages</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="336" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Image alignment and stitching: A tutorial. Foundations and Trends® in Computer Graphics and Vision</title>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<idno type="DOI">10.1561/0600000009</idno>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName><surname>Temponaut</surname></persName>
		</author>
		<author>
			<persName><surname>Top</surname></persName>
		</author>
		<ptr target="https://www.youtube.com/watch?v=Sw60Ehm1b5c" />
		<imprint>
			<date type="published" when="2021">10 Timelapses 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sustainable management of construction site big visual data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tibaut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zazula</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11625-018-0595-9</idno>
	</analytic>
	<monogr>
		<title level="j">Sustainability Science</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1311" to="1322" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Slow speed-fast motion: time-lapse recordings in physics education</title>
		<author>
			<persName><forename type="first">M</forename><surname>Vollmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-P</forename><surname>Möllmann</surname></persName>
		</author>
		<idno type="DOI">10.1088/1361-6552/aaa954</idno>
	</analytic>
	<monogr>
		<title level="j">Physics Education</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">35019</biblScope>
			<date type="published" when="2018-03">March 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Temporal pyramid pooling-based convolutional neural network for action recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCSVT.2016.2576761</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2613" to="2622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Spatiotemporal pyramid network for video action recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.226</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2097" to="2106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Scene summarization via motion normalization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wehrwein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2020.2993195</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2495" to="2501" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<ptr target="https://en.wikipedia.org/w/index.php?title=Time-lapse_photography" />
		<title level="m">Time-lapse photography -Wikipedia, the free encyclopedia</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Wikipedia contributors</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Construction performance monitoring via still images, time-lapse photos, and video streams: Now, tomorrow, and the future</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Vela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Golparvar-Fard</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.aei.2015.01.011</idno>
	</analytic>
	<monogr>
		<title level="m">Infrastructure Computer Vision</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="211" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Video summarization with long short-term memory</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46478-7_47</idno>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Photometric stabilization for fast-forward videos</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.13276</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="105" to="113" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Spatial-temporal pyramid based convolutional neural network for action recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ruan</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2019.05.058</idno>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">358</biblScope>
			<biblScope unit="page" from="446" to="455" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Time-mapping using space-time saliency</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2014.429</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
