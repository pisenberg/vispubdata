<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Color-Concept Association via Image Colorization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ruizhen</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">) Colorized image</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ziqi</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">) Colorized image</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bin</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">) Colorized image</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Oliver</forename><surname>Van Kaick</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">) Colorized image</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hui</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">) Colorized image</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Supervised Color-Concept Association via Image Colorization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-03-27T09:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Color-concept association</term>
					<term>colorization</term>
					<term>EMD</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The interpretation of colors in visualizations is facilitated when the assignments between colors and concepts in the visualizations match human's expectations, implying that the colors can be interpreted in a semantic manner. However, manually creating a dataset of suitable associations between colors and concepts for use in visualizations is costly, as such associations would have to be collected from humans for a large variety of concepts. To address the challenge of collecting this data, we introduce a method to extract color-concept associations automatically from a set of concept images. While the state-of-the-art method extracts associations from data with supervised learning, we developed a self-supervised method based on colorization that does not require the preparation of ground truth color-concept associations. Our key insight is that a set of images of a concept should be sufficient for learning color-concept associations, since humans also learn to associate colors to concepts mainly from past visual input. Thus, we propose to use an automatic colorization method to extract statistical models of the color-concept associations that appear in concept images. Specifically, we take a colorization model pre-trained on ImageNet and fine-tune it on the set of images associated with a given concept, to predict pixel-wise probability distributions in Lab color space for the images. Then, we convert the predicted probability distributions into color ratings for a given color library and aggregate them for all the images of a concept to obtain the final color-concept associations. We evaluate our method using four different evaluation metrics and via a user study. Experiments show that, although the state-of-the-art method based on supervised learning with user-provided ratings is more effective at capturing relative associations, our self-supervised method obtains overall better results according to metrics like Earth Mover's Distance (EMD) and Entropy Difference (ED), which are closer to human perception of color distributions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. <ref type="figure">1</ref>: We introduce a method for automatically extracting color-concept associations from natural images. We apply a colorization neural network to predict color distributions for input images. The distributions are transformed into ratings for a color library, which are then aggregated across multiple images of a concept, e.g., blueberry, corn, and glass, to provide the color-concept associations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>People generally find it easier to interpret visualizations such as graphs and diagrams if the categories in the visualizations are represented with colors that match people's semantic expectations <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30]</ref>, since the ability to interpret visualizations also depends on the semantic discrim- inability of the colors in the visualization <ref type="bibr" target="#b18">[19]</ref>. For example, reading a graph of nutritional information where strawberries are represented as red and mangos as yellow requires less conscious effort, as the two categories are associated to colors that match human perception of these fruits. In addition, Mukherjee et al. <ref type="bibr" target="#b18">[19]</ref> showed with their semantic discriminability theory that in certain conditions colors can be associated to concepts usually not considered to be linked with colors. Thus, making color palettes semantically interpretable is useful for visualization and possible in many contexts.</p><p>Semantic palettes can be created from datasets of color-concept associations <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>. However, manually creating such a dataset can be costly, as the dataset has to cover a large variety of concepts, leading to the collection of a large volume of data from human subjects. Thus, previous work has proposed automated methods for the extraction of associations from data. A common idea in many automated approaches is to collect color-concept associations from images asso- (c) Then, we convert the distributions into ratings for a color library with a color mapping module, and aggregate the ratings for all the pixels to obtain an image-wise color rating. The method further aggregates the ratings for all the images of a common concept to provide the final color-concept association. ciated with keywords, where the data can be obtained through image retrieval from search engines according to given keywords <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24]</ref>. The image-keyword pairs are then further processed to provide the color-concept associations. For example, Lin et al. <ref type="bibr" target="#b14">[15]</ref> analyze color distributions of the images and then assign colors to concepts according to an affinity score. Rathore et al. <ref type="bibr" target="#b23">[24]</ref> learn a method to extract color distributions from images to match human color-concept associations, providing high-quality results. Although the core method is in principle unsupervised, in practice it is supervised as it relies on human ratings to guide the feature selection and weight optimization in the learning.</p><p>In this paper, we introduce a self-supervised method based on colorization for extracting color-concept associations from natural images (Fig. <ref type="figure">1</ref>). Our key insight is that human color-concept associations are learned from past visual input <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b35">36]</ref>. Thus, natural images should also be sufficient input for a method that derives color-concept associations, i.e., there should be no need for explicitly collecting human ratings. In this spirit, our main idea is to extract associations from the model learned by a colorization neural network <ref type="bibr" target="#b36">[37]</ref>. Colorization networks learn to assign colors to the pixels of a grayscale image, and thus these networks essentially learn how to associate colors to certain categories of objects in an implicit manner. Our method extracts such knowledge from a colorization network to explicitly discover color-concept associations with a self-supervised learning method.</p><p>Specifically, we train a colorization network to predict color distributions for the pixels of natural images (Fig. <ref type="figure" target="#fig_1">2</ref>). Then, we convert these probabilities into ratings for a color library, which are finally transformed into color-concept distributions that aggregate the ratings of multiple images. Thus, the output of our method is a color probability distribution for each concept. Such color-concept distributions can then be further considered in color assignments for visualizations, e.g., through an optimization scheme that balances the frequency and distinctiveness of the colors assigned to different concepts <ref type="bibr" target="#b14">[15]</ref>.</p><p>The main advantage of our method over previous work is that, since colorization can be posed as a self-supervised learning task, the colorization network can be trained simply with a dataset of images of different concepts. The last step of our method that aggregates the ratings from multiple images requires a concept "label" for each image. However, this is also the common requirement of previous methods, and the assignment of these labels can be automated by retrieving images for a list of required concepts with a search engine, as in previous work <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24]</ref>. In addition, our method does not depend on feature selection based on human ratings, which could be biased towards the small number of classes for which the ratings were collected. Our method also extracts the foreground objects in the input images <ref type="bibr" target="#b21">[22]</ref> to ensure that background regions do not negatively impact the learned color distributions.</p><p>We evaluate our method by comparing our results to the results of the state-of-the-art supervised method <ref type="bibr" target="#b23">[24]</ref> with qualitative and quantitative analyses. Specifically, we evaluate the results quantitatively with the same measures used by previous work (Pearson correlation and total variation) but also use a perceptual measure (Earth Mover's Distance) to provide an evaluation that better reflects the perceptual differences between the results. We show that our self-supervised method provides results that are superior to the state-of-the-art method based on these perceptual measures while less effective according to non-perceptual measures due to the lack of supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>We discuss the literature most related to our work, reviewing methods for automatic extraction of color-concept associations from data and the state-of-the-art methods for image colorization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Color-concept association</head><p>Color-concept associations can be quantified through human judgement <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b31">32]</ref>. However, it can be costly to create such data manually. Methods for automatic extraction of color-concept associations from data can be roughly divided into image-based <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref> and natural language-based methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>Image-based methods extract associations from images paired with keywords. Lin et al. <ref type="bibr" target="#b14">[15]</ref> extract color distributions from tagged images, and determine color-concept affinity scores from the histograms and keywords using an entropy-based method. The method provides an optimal assignment of one color to each concept. Similarly, Lindner et al. <ref type="bibr" target="#b15">[16]</ref> use statistical methods to associate colors to concepts from annotated images.A few methods also compute color distributions for concepts. Lindner et al. <ref type="bibr" target="#b16">[17]</ref> compute distributions in CIELAB space for over 9,000 color names in 10 different languages using images retrieved from a search engine. <ref type="bibr">Bartram et al. [3]</ref> use clustering algorithms to extract palettes for images associated to affective concepts. More recently, Rathore et al. <ref type="bibr" target="#b23">[24]</ref> perform feature selection based on human ratings to learn the best features for retrieving histograms from images, so that the histograms are likely associated to given concepts.</p><p>On the other hand, language-based methods extract color-concept associations from natural language descriptions. Setlur et al. <ref type="bibr" target="#b30">[31]</ref> analyze co-occurrences of concepts and colors in n-grams extracted from a corpus of text, and then cluster images retrieved with the associated colors to create color palettes.</p><p>In summary, earlier work for color-concept extraction makes use of handcrafted automatic methods, which provide results that do not always correlate strongly with human selections <ref type="bibr" target="#b23">[24]</ref>. Recent learningbased methods are more robust and validated on user ratings, but the learning is dependent on user input specific to the problem. In contrast, our work is a learning-based method that does not require data specifically prepared for the concept-color association task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Image colorization</head><p>State-of-the-art colorization methods are based on neural networks learned on over a million images <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37]</ref>. These networks can be trained in a self-supervised manner by taking color images, transforming them into grayscale, and then training the networks to predict the colored version of the image from the grayscale. Iizuka et al. <ref type="bibr" target="#b10">[11]</ref> introduce a CNN-based colorization method that combines global priors with local features extracted from the images. Zhang et al. <ref type="bibr" target="#b36">[37]</ref> provide a colorization method based on a convolutional neural network, which performs significantly better than past methods. This method is then used as the basis of a user-guided colorization which incorporates user hints to direct the coloring output <ref type="bibr" target="#b38">[39]</ref>. Most of these previous methods predict single colors for each pixel in the input image.</p><p>Moreover, colorization has been used as a proxy task for visual understanding <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b37">38]</ref>, and in this paper, we utilize colorization to extract color-concept associations. One interesting aspect of the work of Zhang et al. <ref type="bibr" target="#b36">[37]</ref> is that the output is a pixel-wise color distribution. Thus, we based our colorization network on this method since we also require distributions for computing color-concept associations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>The key idea of our method is to use image colorization as a selfsupervised task to extract color-concept associations automatically from natural images. A colorization network is able to assign correct colors for images with different concepts, which means that this network must implicitly learn color-concept associations. Thus, our goal is to extract the learned color-concept associations explicitly from such a network. Fig. <ref type="figure" target="#fig_1">2</ref> shows an overview of our method. With the image colorization module pre-trained on ImageNet, for each color image associated with a given concept, we can predict a distribution of possible colors for each pixel by taking the lightness (L) channel as input and outputting the probability map over the quantized ab color space. The predicted pixel-wise color distribution is then converted to the color rating over a given color library, e.g., UW-58 colors <ref type="bibr" target="#b23">[24]</ref> shown in this example, by weighted color mapping, and accumulated over all pixels of all images of the same concept, to provide the final color-concept association.</p><p>More details about the image colorization module and the color mapping module are given in the following two subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Image colorization</head><p>We perform the colorization task in the perceptual CIE Lab color space. The goal of our image colorization module is to take the L channel of Fig. <ref type="figure">4</ref>: Mapping from a color c i to the color library { c j }, distributing the probability p i of c i to the colors in the library according to Equation <ref type="formula" target="#formula_1">2</ref>.</p><p>an image as input and predict a distribution of possible colors instead of a single deterministic color for each pixel, so that we are able to quantify color-concept associations over a large range of colors. As the L channel is already given as input, we only need to predict the probability map over the ab color space, which then combined with the input L provides all the information needed to define a full color distribution. With this goal in mind, we adopt the image colorization neural network proposed by Zhang et al. <ref type="bibr" target="#b36">[37]</ref>, where the input and output both fit our setting. The network architecture and how the training data is constructed are illustrated in Fig. <ref type="figure" target="#fig_2">3</ref>.</p><p>Given an input lightness channel L ∈ R H×W of an image, the goal of the colorization module is to learn a mapping to a probability distribution over possible colors P ∈ [0, 1] H×W ×Q , where H and W are the image dimensions, and Q is the number of quantized ab values. In all our experiments, we quantize the ab space into bins with grid size 10 and keep the Q = 313 values which are in-gamut, as in <ref type="bibr" target="#b36">[37]</ref>.</p><p>To define the training data for the network, we convert the ground truth color image I into a quantized ab probability distribution P gt using a soft-encoding scheme. More specifically, for each pixel, we find the 5-nearest neighbors in the color space determined by the quantized ab space and its lightness, and then weight them proportionally to their distance from the ground truth using a Gaussian kernel with σ = 5.</p><p>The loss function is then defined as the multinomial cross entropy loss between the predicted probability distribution P and the ground truth probability distribution P gt : <ref type="bibr" target="#b0">(1)</ref> where v(•) is a weighting term that can be used to rebalance the loss based on color-class rarity as defined in <ref type="bibr" target="#b36">[37]</ref>, which gives less weight to desaturated values as the number of pixels in natural images at desaturated values are orders of magnitude higher than for saturated values due to the appearance of background elements such as clouds, pavement, dirt, and walls.</p><formula xml:id="formula_0">Loss(P, P gt ) = − ∑ h,w v (P(h, w, :)) ∑ q P(h, w, q) log P gt (h, w, q),</formula><p>The network consists of 8 blocks, each of which contains 2 or 3 repeated convolutional and ReLU layers and a BatchNorm layer. The network is pre-trained on 1.3M images from the ImageNet training set <ref type="bibr" target="#b25">[26]</ref>, and fine-tuned on the given images with associated concepts to extract the color-concept associations. More details of the network and training can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Color mapping</head><p>The color mapping module then derives the color-concept associations from the probability distribution P ∈ [0, 1] H×W ×Q predicted by the colorization network. Specifically, for each pixel located at position (h, w), we use its lightness L(h, w) to map the quantized ab space to the full color space with the transferred probability map P h,w = P(h, w, :) ∈ [0, 1] Q . Then for each color c i with probability p i = P h,w i , i = 1,...,Q, we distribute its probability p i to the given color library { c j } j=1,...,N , e.g., UW-58 colors <ref type="bibr" target="#b23">[24]</ref> , UW-71 colors <ref type="bibr" target="#b18">[19]</ref> or BCP-37 colors <ref type="bibr" target="#b20">[21]</ref>, to </p><formula xml:id="formula_1">= Q ∑ i=1 ω i j p i , (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where ω i j are distribution weights based on the perceptual difference between two colors c i and c j . Fig. <ref type="figure">4</ref> illustrates how p i of color c i is re-distributed to the given color library { c j } j=1,...,N .</p><p>To define ω, we first compute the Euclidean distances {d i j } j=1,...,N in Lab color space between c i and all the colors in library { c j } j=1,...,N . Then, we compute their corresponding z-scores {z i j } j=1,...,N , and apply softmax to the z-scores to get the final weights:</p><formula xml:id="formula_3">ω i j = e −z i j ∑ N j=1 e −z i j . (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>To obtain the accumulated color-concept associations { p j } j=1,...,N for the entire image, we simply compute the average probability distribution among all pixels:</p><formula xml:id="formula_5">p j = ∑ (h,w)∈F ph,w j /|F|, (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>where F is the set of pixels inside the foreground region of the image. The foreground mask is obtained with the automatic foreground detection method of Qin et al. <ref type="bibr" target="#b21">[22]</ref>. For a set of images associated with the same concept, we further compute the average probability distribution of all the images to get the accumulated color-concept association p ∈ [0, 1] N . As natural images have great variability in lighting, we find that there can be regions in the images with white glow and dark shadows, which are not common colors associated to concepts and thus less useful for visualization. To reduce the impact of these colors on the computed distributions, we half the probability of the entry in p with both a and b channels equal to zero and then renormalize the distribution to obtain the final color-concept association p.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULTS AND EVALUATION</head><p>In this section, we evaluate our method with qualitative and quantitative analyses on a variety of datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>To evaluate our method, we collect four datasets from previous works with ground truth user ratings, which cover a variety of object categories, to fully explore the capability of our method.</p><p>• Recycling6 <ref type="bibr" target="#b29">[30]</ref> consists of around 70 images for each of 6 types of recycling items, including Compost, Glass, Metal, Paper, Plastic, and Trash. The ground truth user ratings are defined on the Berkeley Color Project 37 (BCP-37) color library <ref type="bibr" target="#b20">[21]</ref>.</p><p>• Fruit12 <ref type="bibr" target="#b23">[24]</ref> consists of 50 images for each of 12 fruit concepts, including Avocado, Blueberry, Cantaloupe, Grapefruit, Honeydew, Lemon, Lime, Mango, Orange, Raspberry, Strawberry, and</p><p>Watermelon. The ground truth user ratings are defined on the University of Wisconsin 58 (UW-58) color library <ref type="bibr" target="#b28">[29]</ref>.</p><p>• Fruit5 <ref type="bibr" target="#b18">[19]</ref> consists of 50 images for each of another 5 fruit concepts, including Apple, Banana, Cherry, Grape, and Peach.</p><p>The ground truth user ratings are defined on the UW-71 color library, an extension of the UW-58 colors. Note that, since only the ground truth ratings are available but without the corresponding example images, we collected corresponding images through Google Image search as in <ref type="bibr" target="#b23">[24]</ref>.</p><p>• Vegetable5 <ref type="bibr" target="#b18">[19]</ref> consists of 50 images for each of 5 vegetable concepts, including Carrot, Celery, Corn, Eggplant, and Mushroom. The ground truth user ratings are defined on the UW-71 color library and example images were retrieved from Google search similarly as in the previous item.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Qualitative results</head><p>Our method first colorizes each concept image to predict a pixel-wise ab probability distribution. Then, our method extracts a color rating from the distributions of each image and finally accumulates ratings for all the images with the same concept in the dataset to provide the final color-concept associations. We provide a qualitative analysis of these three steps of the method as follows. To illustrate the results of the colorization network, we test the learned model on unseen concept images and obtain the colorized results by taking the annealed-mean of the predicted distribution as in <ref type="bibr" target="#b36">[37]</ref>. Fig. <ref type="figure">5</ref> shows several example colorization results for different grayscale concept images from the unseen test set. We see that the colorized images are perceptually quite close to the ground truth. Furthermore, Fig. <ref type="figure" target="#fig_4">6</ref> illustrates the second step of the method, where several example estimated color ratings are extracted from different images of the same concept. Note how the color ratings capture the different color distributions in the images. For example, the distributions for apples have high probabilities for bright colors such as red, yellow, and green, while the distributions for metals have higher probabilities for grayscale colors. Fig. <ref type="figure">7</ref> shows results of the third step of the method (in the middle column), where we see the final color distributions produced for different concepts based on multiple images. Although many colors have non-zero probabilities, the colors with the probabilities that stand out the most clearly correspond to colors commonly associated to the given concepts. Fig. <ref type="figure">1</ref> shows a few extra results with example input images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation metric</head><p>To quantitatively evaluate our final results, that is, to compare an estimated color-concept association p = { pi } N i=1 to the ground truth user rating r = {r i } N i=1 , defined on the color library {c i } N i=1 , where N is the number of colors in the library, we use several different metrics.</p><p>The first metric is the Pearson correlation coefficient (Corr) used in the work of Rathore et al. <ref type="bibr" target="#b23">[24]</ref>, which measures the linear correlation between two distributions. The second metric is the total variation (TV ) used in the work of Mukherjee et al. <ref type="bibr" target="#b18">[19]</ref>, which is defined as half of the L1-distance between two distributions:</p><formula xml:id="formula_7">TV ( p, r) = 1 2 N ∑ i=1 | pi − r i |. (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>The two metrics defined above consider two color distributions as two vectors that are compared in a manner that is oblivious to the association between the vector entries and specific colors. Thus, to take the perceptual difference of the colors into consideration, we propose to use the Earth Mover's Distance (EMD) <ref type="bibr" target="#b24">[25]</ref> as an evaluation metric. EMD has been widely used to compare two probability distributions <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35]</ref> and is known to be closer to user perception than other metrics.</p><p>Let us suppose that p and r are two mass distributions. Then, EMD measures the cost of transporting one heap of mass to another heap, and thus computes the minimal transport effort between p and r:</p><formula xml:id="formula_9">min f i j ∑ N i=1 ∑ N j=1 f i j d i j , s.t. f i j ≥ 0, i, j ∈ {1, 2,...,N} ∑ N j=1 f i j ≤ pi , i ∈ {1, 2,...,N} ∑ N i=1 f i j ≤ r j , j ∈ {1, 2,...,N} ∑ N i=1 ∑ N j=1 f i j = 1,<label>(6)</label></formula><p>where f i j is the amount of mass transported from color c i in p to color c j in r, and d i j is the Lab color distance between c i and c j . Note that as both p and r are normalized, the total amount of the flow is always equal to one as shown in the last constraint above. In addition, EMD is independent of the order in which the colors are stored in the distributions, since the color distances are taken into consideration for finding the optimal mass transport. Once the transportation problem is solved to obtain the optimal flow f * i j , the EMD is defined as the total cost of the flow:</p><formula xml:id="formula_10">EMD( p, r) = N ∑ i=1 N ∑ j=1 f * i j d i j . (<label>7</label></formula><formula xml:id="formula_11">)</formula><p>Besides the three metrics discussed above that compute differences between distributions directly, as indicated in the work of Mukherjee et al. <ref type="bibr" target="#b18">[19]</ref>, specificity is one of the key properties that a color-concept association should possess, which refers to the 'peakiness' of the colorconcept association distribution. Specificity is quantified using the entropy of the distribution, which captures how 'flat' vs. 'peaky' a distribution is, regardless of how many peaks there are. Thus, we also compute the entropy difference (ED) between two distributions as an auxiliary metric to quantify the similarity of specificity of two distributions:</p><formula xml:id="formula_12">ED( p, r) = N ∑ i=1 pi log pi − N ∑ i=1 r i log r i . (<label>8</label></formula><formula xml:id="formula_13">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison to the state-of-the-art method</head><p>In this section, we compare our method to the state-of-the-art supervised method <ref type="bibr" target="#b23">[24]</ref> on different datasets using all evaluation metrics as well as through a user study.</p><p>The results of the supervised method are obtained by cross-validation within each dataset. For example, for each fruit concept inside the Fruit12 dataset, we train the model of Rathore et al. <ref type="bibr" target="#b23">[24]</ref> with the ground truth ratings on 11 fruit concepts and validate the model on the remaining concept by comparing the derived color rating to the ground truth color rating. Note that, in contrast, our method is self-supervised and can be trained directly with the images associated to each concept without any ground truth rating.</p><p>Comparison via evaluation metrics. Table <ref type="table">1</ref> shows the comparison of the average evaluation scores among all the concepts for each dataset. We see that the values of different metrics are inconsistent with each other. The results obtained by the supervised method <ref type="bibr" target="#b23">[24]</ref> have a higher linear correlation with the ground truth rating, while our results have lower EMD and ED values. The TV scores of the two methods are comparable.</p><p>Comparison via a user study. We further conduct a user study to compare the results based on human perception. For each concept in our dataset, we show the ground truth user rating together with the two estimated ratings obtained by our method and the supervised method <ref type="bibr" target="#b23">[24]</ref> in random order, and ask the user to select which result they think is more similar to the ground truth user rating. The user can select either one of the two results or a "not sure" option.</p><p>We invited 30 participants to do the study for all the concepts in our dataset, which resulted in 840 answers in total. The vote percentages for the options "ours/the supervised method <ref type="bibr" target="#b23">[24]</ref>/not sure" are 70.5%/16.9%/12.6%. We see that our method was selected much more frequently than the supervised method <ref type="bibr" target="#b23">[24]</ref> as having better results, which shows that our method provides color-concept associations that are perceptually more similar to the ground truth user ratings.</p><p>Table <ref type="table">1</ref>: Comparison to the state-of-the-art supervised method <ref type="bibr" target="#b23">[24]</ref> on all the datasets with the four different evaluation metrics introduced in Sect. 4.3. Our self-supervised method provides overall better results according to the EMD and ED metrics, but worse results according to the Correlation (Corr) metric. The results according to the total variation (TV) are comparable. For each metric, ↑: the higher, the better; ↓: the lower, the better. (c) GT color-concept association (a) Supervised <ref type="bibr" target="#b23">[24]</ref> (b) Our result Carrot Cherry Cantaloupe Metal Fig. <ref type="figure">7</ref>: Results obtained by the supervised method <ref type="bibr" target="#b23">[24]</ref> (left) and our self-supervised method (middle) compared to the ground truth ratings (right). Each row shows one example concept from one dataset, including Metal in Recycling6, Cantaloupe in Fruit12, Cherry in Fruit5, and Carrot in Vegetable5. The scores of all four evaluation metrics are shown with each result and the best evaluation scores are shown in bold. Analysis of the results. To obtain a better understanding of the results, we show all the metrics together with the user vote percentages for each concept in Table <ref type="table">3</ref> and report the consistency between each metric and the user choices in Table <ref type="table" target="#tab_2">2</ref>. More specifically, for each metric, we consider that it is consistent with the user choice if the result regarded to be better using this metric is selected by the user. Then, we compute the percentages of the consistent results over all the 840 answers we collected as the final consistency score for each metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>From the results shown in Table <ref type="table" target="#tab_2">2</ref>, we see that the EMD and ED metrics are more consistent with the user choices. Especially, the ED metric obtains the highest consistency score, which shows that humans focus more on the peakiness when comparing two distributions. Compared to Corr and TV, the EMD metric considers the perceptual distance between different colors and computes the minimal ground transport effort, which also leads to results more consistent with human perception.</p><p>Fig. <ref type="figure">7</ref> shows several visual examples of the predicted color-concept associations compared to the ground truth rating and the corresponding values of different evaluation metrics. For the results shown in the first three rows, the correlations of the results obtained by the supervised method are always higher than our method, although the distributions obtained by our method are perceptually more similar to the ground truth distributions shown on the right. The EMD and ED metrics better capture this difference. For the result shown on the bottom row, our method performs consistently better than the supervised method with respect to all the evaluation metrics, while some dominant colors do not stand out in the relatively uniform distribution obtained by the supervised method <ref type="bibr" target="#b23">[24]</ref>, which may be caused by the weighted combination of distributions obtained by different features in their method.</p><p>Note that, although our method obtains overall better results accordingly to the EMD and ED metrics, which consider the scale of association with a given concept and are more consistent with human perception, the supervised method <ref type="bibr" target="#b23">[24]</ref> obtains overall better results according to the correlation metric. Correlation assesses the relative pattern of associations, regardless of the scale, which has been shown as the critical factor that influences semantic discriminability of the colors more than the absolute associations <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>. We believe that it would be worthy to explore ways to be able to handle both relative association and range well.</p><p>Table <ref type="table">3</ref>: Comparison to the state-of-the-art supervised method <ref type="bibr" target="#b23">[24]</ref> on each concept in our dataset with the four different evaluation metrics as well as the user study result. For each concept, the results of the supervised method <ref type="bibr" target="#b23">[24]</ref> are the top row and our results are the bottom row. For each metric, ↑: the higher, the better; ↓: the lower, the better. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation studies</head><p>To justify several key design choices of our method, we conduct ablation studies with the following settings:</p><p>• No Fine-tuning: we use the model pre-trained on the ImageNet dataset directly, without fine-tuning it on the concept images;</p><p>• No Pre-training: we train the colorization network directly on the concept images, without using the network pre-trained on the large-scale ImageNet dataset;</p><p>• No Seg-mask: we make use of all the pixels in the concept images without masking out the ones in the background during the color mapping;</p><p>• No Post-processing: we take the mapped distribution directly without halving the probability of colors with both ab channels equal to zero to reduce the effect of lighting in real images.</p><p>Table <ref type="table">4</ref> shows the results of the ablation studies on all four datasets. We see that our method with the full pipeline ("Our method" in the table) provides consistently better results.</p><p>For the No Fine-tuning setting, the network learns information purely from images with various concepts in ImageNet, which makes the distribution extracted not specifically related to the concepts in our datasets and thus leads to the worst results. For the No Pre-training setting, the network learns associations between colors and concepts from the specific concept images in our datasets, which leads to much better results than the No Fine-tuning setting. This shows that our method can obtain reasonable results with a small dataset. However, since each dataset is quite small with only 50 images per concept, this makes the network sometimes biased by the samples given in the datasets and thus does not learn a more general color-concept association. The performance becomes consistently better when the colorization network is pre-trained on ImageNet and fine-tuned on the concept images. Fig. <ref type="figure" target="#fig_5">8</ref> shows an example concept image and the colorization and color ratings obtained under different training settings.</p><p>Table <ref type="table">4</ref>: Ablation studies to justify the key components of our method. Details of the different settings can be found in Sect. 4.5. For each metric, ↑: the higher, the better; ↓: the lower, the better.  For the No Seg-mask setting, we find that the background can add some random noise or bias to the color distribution. Fig. <ref type="figure">9</ref> shows a comparison of several example results with and without using the foreground segmentation mask automatically obtained by the method of Qin et al. <ref type="bibr" target="#b21">[22]</ref>. We can see that for the cantaloupe shown in the first row, the rating of the white color is extremely high due to the white background, and becomes more reasonable when the background is masked out. For the blueberry shown in the third row with a more noisy background, the color rating has clearer peaks after masking out the background, even though the segmentation is not perfect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>For the No Post-processing setting, we see that the correlations of the Recycle6 and Vegetable5 datasets are better than for our full method, while all the other metrics are better in the full method. We find that the main reason is that there are concepts in these two datasets where white or grey colors dominate the images, e.g., plastic and mushroom. Thus, decreasing the probability of colors with both ab channels equal to zero may lead to worse results for these concepts. However, when averaging the performance on the whole dataset among all the concepts, post-processing generally provides better results. Fig. <ref type="figure">10</ref> shows several representative concept images with significant glow and shadows, leading to the high probability of white or black colors, which is alleviated through post-processing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Data visualization application</head><p>To demonstrate how our method can be used for data visualization applications, we follow the method of <ref type="bibr" target="#b14">[15]</ref> to automatically select semantically-resonant colors based on the color-concept associations obtained by our method. We take the set of company brands used Fig. <ref type="figure" target="#fig_1">12</ref>: Semantically-resonant colors selected for different brands using different methods when given different color palettes.</p><p>in <ref type="bibr" target="#b14">[15]</ref>, including Apple, AT&amp;T, HomeDepot, Kodak, Starbucks, Target, and Yahoo!, as the running example, to show the color selection process. More results can be found in the supplementary materials. We start with collecting images for each concept via search engines as we did for the Fruit5 and Vegetable5 datasets. Then, the network pre-trained on ImageNet is fine-tuned to obtain the color-concept association for each concept as described in Sect. 3.1. The color selection method in <ref type="bibr" target="#b14">[15]</ref> is finally applied to select the final color assignment given any pre-defined color palette. Fig. <ref type="figure" target="#fig_1">12</ref> shows the semantically-resonant colors selected for different brands using different methods when given different color palettes. The expert-chosen colors are shown in the first row and the result of <ref type="bibr" target="#b14">[15]</ref> selected from the Tableau20 palette is shown in the second row for comparison. The following four rows show the colors selected based on the color-concept associations obtained by our method but from different color palettes, where "+E" indicates that the set of expertchosen colors is included in the color palettes.</p><p>We see that, due to the limited color choices provided in the Tableau20 palette, most of the colors selected by our method are slightly different from the expert-chosen ones. However, once we add those expert-chosen colors into the palette for selection, all the chosen colors match exactly with expert choices. When given UW-58 as the color palette, the chosen colors are closer to expert choices than those from the Tableau20 palette, which also shows that our method is able to select more appropriate colors for different concepts from a larger color library. Again, when further provided with expert-chosen colors, most of the selected colors are improved other than the one for Yahoo!. When checking the selection process in detail, we find that although the expert-chosen purple color does have a higher association in our method, the color entropy is slightly larger than the final color that our method chooses, which leads to a slightly lower affinity score. It would be interesting to explore other color selection methods based on our color-concept associations instead of using the method in <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We introduced a method for extracting color-concept associations from datasets of natural images via colorization. We showed that our method leads to improved results over the state-of-the-art supervised method according to evaluation metrics more consistent with human perception, while requiring minimal data preparation, since colorization networks can be trained in a self-supervised manner. Thus, the method can be easily applied to extract color associations for concepts beyond the ones tested in our work, since the main data preparation task is to gather images of the new concepts.</p><p>Limitations and future work. Since our method is unsupervised, our results are entirely determined by the data provided as input to the method. As an example of this limitation, a lack of variety in the images of the datasets can lead to failure cases, as shown in Fig. <ref type="figure" target="#fig_7">11</ref>. Thus, it is an interesting direction for future work to explore ways to automatically select the set of representative images for each concept for color rating extraction. Moreover, the simple post-processing applied by our method can reduce the negative effect of glow and shadows caused by certain lighting conditions to some extent, but it may lead to worse results for some specific categories. Thus, it would be interesting to explore more sophisticated ways to resolve this problem.</p><p>Furthermore, although our results are perceptually better, the relative patterns of association are more effectively captured by the state-ofthe-art supervised method <ref type="bibr" target="#b23">[24]</ref>. It would be interesting to explore ways of handling both relative association and range well, for example, by providing light supervision to adjust the color rating obtained by our method to better capture the relative pattern. Last but not least, as the colorization network is pre-trained on ImageNet, which consists of images of real objects, when fine-tuning it using images searched for abstract concepts without specific semantically-resonant visual representations, the final color-concept association might be unreasonable. We believe that this is a common limitation for unsupervised methods relying on images only and it would also be interesting to investigate ways of extending our method to make it work well for abstract concepts. One possible solution could be the automatic selection or even generation of representative images for each abstract concept based on the recent CLIP model <ref type="bibr" target="#b22">[23]</ref> that connects text and images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig.2: Overview of our method for color-concept association. (a) Given the lightness channel (L) of an input image, we use an image colorization network to predict (b) an ab color probability distribution for each pixel. (c) Then, we convert the distributions into ratings for a color library with a color mapping module, and aggregate the ratings for all the pixels to obtain an image-wise color rating. The method further aggregates the ratings for all the images of a common concept to provide the final color-concept association.</figDesc><graphic url="image-61.png" coords="2,50.27,93.02,65.29,65.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Colorization network training. Top row: given an input image provided in perceptual Lab color space, we convert the image into a ground truth probability distribution P gt in ab space. Bottom row: the network is trained to predict the distribution P from the lightness channel (L) via the loss that compares two distributions.</figDesc><graphic url="image-68.png" coords="3,56.09,128.90,55.75,55.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Fig. 5: Colorization results on unseen test images for different concepts. Note the overall plausibility of the colorization results.</figDesc><graphic url="image-78.png" coords="4,60.41,311.30,71.71,71.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Example color ratings for different images with the same concept. Top three rows: Apple. Bottom three rows: Metal.</figDesc><graphic url="image-95.png" coords="4,313.61,377.00,66.67,59.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Comparison of colorization and corresponding color ratings obtained under different training settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 :Fig. 10 :</head><label>910</label><figDesc>Fig. 9: Example color ratings extracted from concept images with and without foreground segmentation masks.</figDesc><graphic url="image-102.png" coords="8,330.41,380.12,57.31,53.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 :</head><label>11</label><figDesc>Fig.11: Failure cases due to the variety of images in the dataset. Top row: our method can capture the dominant green color in the given eggplant image, but as there are only very few images with green eggplants in the datasets, and most of eggplants have dark colors such as purple, the final color-concept rating that our method extracted has low probabilities for green colors compared to the ground truth user rating. Bottom row: most of the images given in the dataset for the Plastic concept are colorful, which leads to a near-uniform distribution of the final color distribution.</figDesc><graphic url="image-108.png" coords="9,62.51,123.98,76.45,51.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Consistency between each metric and the user choices collected in the user study.</figDesc><table><row><cell>Metrics</cell><cell>Corr</cell><cell>TV</cell><cell>EMD</cell><cell>ED</cell></row><row><cell cols="5">Consistency 37.7% 45.6% 54.2% 70.5%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Corr↑ TV↓ EMD↓ ED↓ Corr↑ TV↓ EMD↓ ED↓ Corr↑ TV↓ EMD↓ ED↓ Corr↑ TV↓ EMD↓ ED↓ No Fine-tuning 0.590 0.180 8.626 0.050 0.460 0.279 17.172 0.059 0.347 0.353 22.144 0.214 0.366 0.358 22.549 0.236 No Pre-training 0.717 0.174 8.753 0.043 0.660 0.223 12.905 0.078 0.429 0.326 20.624 0.214 0.492 0.331 20.845 0.209 No Seg-mask 0.687 0.173 8.264 0.048 0.546 0.249 15.120 0.073 0.313 0.347 22.526 0.256 0.333 0.360 23.233 0.262 No Post-processing 0.757 0.166 8.524 0.067 0.642 0.227 13.253 0.059 0.409 0.329 20.188 0.231 0.517 0.326 21.309 0.225 Our method 0.717 0.163 8.043 0.034 0.697 0.210 12.346 0.068 0.497 0.301 18.448 0.204 0.510 0.323 20.432 0.205</figDesc><table><row><cell></cell><cell>Recycling6</cell><cell>Fruit12</cell><cell>Fruit5</cell><cell>Vegetable5</cell></row><row><cell>No Fine-tune</cell><cell></cell><cell></cell><cell></cell></row><row><cell>No Pre-train</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Full pipeline</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Colorization</cell><cell>Estimated image-wise color rating</cell><cell></cell><cell></cell></row><row><cell>Concept image</cell><cell>GT color-concept association</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank the reviewers for their valuable comments. This work was supported in parts by NSFC (61872250, U21B2023, 62161146005), GD Natural Science Foundation (2021B1515020085), GD Talent Plan (2019JC05X328), Shenzhen Science and Technology Program (RCYX20210609103121030, RCJC20200714114435012), Natural Sciences and Engineering Research Council of Canada, and Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Shape, illumination, and reflectance from shading</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1670" to="1687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Affective color in visualization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bartram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Patra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 CHI conference on human factors in computing systems</title>
				<meeting>the 2017 CHI conference on human factors in computing systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1364" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Affective Color in Visualization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bartram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Patra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stone</surname></persName>
		</author>
		<idno type="DOI">10.1145/3025453.3026041</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, CHI &apos;17</title>
				<meeting>the 2017 CHI Conference on Human Factors in Computing Systems, CHI &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1364" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised diverse colorization via generative adversarial networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European conference on machine learning and knowledge discovery in databases</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="151" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning diverse image colorization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-C</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Jin</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6837" to="6845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">In situ distribution guided analysis and visualization of transonic jet engine simulations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heinlein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="811" to="820" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Color and psychological functioning: the effect of red on performance attainment</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Elliot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Moller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of experimental psychology: General</title>
		<imprint>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">154</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Comicolorization: semi-automatic manga colorization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Furusawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hiroshiba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ogaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Odagiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia 2017 Technical Briefs</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automated Color Selection Using Semantic Knowledge</title>
		<author>
			<persName><forename type="first">C</forename><surname>Havasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Holmgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 AAAI Fall Symposium Series</title>
				<imprint>
			<date type="published" when="2010-11">Nov. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep exemplar-based colorization</title>
		<author>
			<persName><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Let there be color! joint end-to-end learning of global and local image priors for automatic image colorization with simultaneous classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ishikawa</surname></persName>
		</author>
		<idno type="DOI">10.1145/2897824.2925974</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The sun is no fun without rain: Physical environments affect how we feel about yellow across 55 countries</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jonauskaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Abdel-Khalek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abu-Akel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Al-Rasheed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Antonietti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Á</forename><forename type="middle">G</forename><surname>Ásgeirsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Atitsogbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bogushevskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Environmental Psychology</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page">101350</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A machine learning approach to quantify the specificity of colour-emotion associations and their cultural differences</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jonauskaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wicker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mohr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Havelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Papadatou-Pastou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oberfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Royal Society open science</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">190741</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Colorization as a proxy task for visual understanding</title>
		<author>
			<persName><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6874" to="6883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Selecting Semantically-Resonant Colors for Data Visualization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fortuna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.12127</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3pt4</biblScope>
			<biblScope unit="page" from="401" to="410" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">What is the Color of Chocolate? -Extracting Color Values of Semantic Expressions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lindner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bonnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Colour in Graphics, Imaging, and Vision</title>
				<imprint>
			<date type="published" when="2012-01">Jan. 2012</date>
			<biblScope unit="volume">2012</biblScope>
			<biblScope unit="page" from="355" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Large-Scale Multi-Lingual Color Thesaurus</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lindner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bonnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Color and Imaging Conference</title>
				<imprint>
			<date type="published" when="2012-01">Jan. 2012</date>
			<biblScope unit="volume">2012</biblScope>
			<biblScope unit="page" from="30" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">W</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Biggam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Paramei</surname></persName>
		</author>
		<title level="m">Progress in colour studies: cognition, language and beyond</title>
				<imprint>
			<publisher>John Benjamins Publishing Company</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Context Matters: A Theory of Semantic Discriminability for Perceptual Encoding Systems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Sherman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lessard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Schloss</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2021.3114780</idno>
	</analytic>
	<monogr>
		<title level="m">Conference Name: IEEE Transactions on Visualization and Computer Graphics</title>
				<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
	<note>IEEE Transactions on Visualization and Computer Graphics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A study of colour emotion and colour preference. part i: Colour emotions for single colours</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Woodcock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Color Research &amp; Application</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="232" to="240" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An ecological valence theory of human color preference</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Schloss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="8877" to="8882" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">U2-Net: Going deeper with nested U-structure for salient object detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">R</forename><surname>Zaiane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2020.107404</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page">107404</biblScope>
			<date type="published" when="2020-10">Oct. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Estimating Color-Concept Associations from Image Statistics</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rathore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Leggon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lessard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Schloss</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2019.2934536</idno>
	</analytic>
	<monogr>
		<title level="m">Conference Name: IEEE Transactions on Visualization and Computer Graphics</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1226" to="1235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The Earth Mover&apos;s Distance as a Metric for Image Retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rubner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1026543900054</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="121" />
			<date type="published" when="2000-11">Nov. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Indexing the earth mover&apos;s distance using normal distributions</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Ruttenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
				<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mapping color to meaning in colormap data visualizations</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Schloss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Gramazio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Silverman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="810" to="819" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semantic discriminability for visual communication</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Schloss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Leggon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lessard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ieee transactions on visualization and computer graphics</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1022" to="1031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Color inference in visual communication: the meaning of colors in recycling</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Schloss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lessard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Walmsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Foley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Research: Principles and Implications</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018-02">Feb. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A Linguistic Approach to Categorical Color Assignment for Data Visualization</title>
		<author>
			<persName><forename type="first">V</forename><surname>Setlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Stone</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2015.2467471</idno>
	</analytic>
	<monogr>
		<title level="m">Conference Name: IEEE Transactions on Visualization and Computer Graphics</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="698" to="707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A systematic investigation of conceptual color associations</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S Y</forename><surname>Tham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Sowden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Grandison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">149</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">1311</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Chromagan: Adversarial picture colorization with semantic class distribution</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vitoria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Raad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ballester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
				<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2445" to="2454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Automated colorization of a grayscale image with seed points propagation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Atiquzzaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1756" to="1768" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Line graph or scatter plot? automatic selection of methods for visualizing trends in time series</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Deussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1141" to="1154" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Object knowledge modulates colour appearance. i-Perception</title>
		<author>
			<persName><forename type="first">C</forename><surname>Witzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Valkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Gegenfurtner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="13" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Colorful Image Colorization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46487-940</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
				<editor>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Split-brain autoencoders: Unsupervised learning by cross-channel prediction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1058" to="1067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Real-time user-guided image colorization with learned deep priors</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="DOI">10.1145/3072959.3073703</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="119" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
