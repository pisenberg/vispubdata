<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">2022 VGTC Visualization Significant New Researcher Award</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">2022 VGTC Visualization Significant New Researcher Award</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-03-27T09:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Arvind Satyanarayan, MIT CSAIL</head><p>The 2022 VGTC Visualization Significant New Researcher Award goes to Arvind Satyanarayan in recognition of his work on new methods for authoring interactive visualizations and insights into how visualizations are used in the public sphere.</p><p>Arvind is an Associate Professor of Computer Science at MIT's Computer Science and Artificial Intelligence Laboratory (CSAIL). He leads the MIT Visualization Group, which uses visualization as a petri dish to study intelligence augmentation (IA), or how computational representations should be designed to amplify our cognitive and creativity while respecting our agency.</p><p>He earned his PhD in Computer Science from Stanford University, working with Jeffrey Heer and the Interactive Data Lab at the University of Washington. Arvind's dissertation focused on declarative interaction design for data visualization-developing novel abstractions for specifying what an interaction technique should accomplish rather than how it should be computed. This approach, following in the tradition of visualization toolkits like Protovis and D3, allows users to focus on design decisions and delegate execution concerns to the underlying runtime. Arvind's work is manifest in three visualization systems: Vega, a lower-level language that extends techniques from Functional Reactive Programming and streaming databases to enable an expressive interaction design space without sacrificing performance; Vega-Lite, a higher-level grammar that enables concise, systematic enumeration of interaction techniques; and, Lyra, a direct manipulation visualization design environment (VDE). Thanks to the hard work and dedication of a talented group of collaborators-including Kanit "Ham" Wongsuphasawat, Dominik Moritz, Jake VanderPlas, Brian Granger, and Ryan Russell-Vega and Vega-Lite have been widely adopted in the data science and data journalism communities, and serve as platforms for further research. Lyra has also influenced a new generation of visualization authoring systems.</p><p>Prior to joining MIT, Arvind spent a year as a Postdoctoral Research Scientist at Google Brain. At Google, he worked with Chris Olah, Shan Carter, and Ludwig Schubert to explore novel interfaces for machine learning interpretability. In a highly cited article, they demon-strated how different interpretability techniques (including feature visualization, attribution, matrix factorization, etc.) can be composed together into rich interfaces, and sketched out how such interfaces could be developed more systematically by approaching interpretability as a structured design space. As part of this effort, Arvind was invited to co-edit Distill, a new academic journal devoted to clear communication of machine learning research through interactive visual media.</p><p>At MIT, Arvind and his students-including PhD researchers Angie Boggust, Crystal Lee, Alan Lundgard, Harini Suresh, Dylan Wootton, and Jonathan Zong, as well as a host of master's and undergraduate researchers-have continued to advance these two research threads. For instance, their recent work has extended Lyra to support interaction design by demonstration, and Vega-Lite to support animated visualizations. Similarly, they have continued to develop methods for intuitive and systematic machine learning interpretability through interfaces for editing model inputs and metrics that assess human-AI alignment.</p><p>Alongside this growing body of work, Arvind's research group has also begun to explore a third research direction: the sociotechnical considerations of visualization. In a series of papers, they have begun to explore strategies for making visualizations accessible to people who are blind or have low vision including through natural language descriptions or through rich semantic hierarchies that can be traversed with screen readers. And, in a widely read study, they described how activist networks on social media mobilized against pandemic policies by authoring and circulating counter-visualizations-a term the researchers coined to describe high quality visualizations used to make arguments that lie outside the mainstream. The study highlights that counter-visualizations reflect a fundamental epistemological divide about how data should be collected, interpreted, and shared, and who participates in this analysis process.</p><p>Arvind Szafir wishes to thank her students, mentors, collaborators, colleagues, friends, and family for their invaluable support and guidance. Their diverse perspectives have shaped her research and continue to provide an interdisciplinary lens for developing datadriven visualization approaches centered around people's perceptual and cognitive abilities. She would also like to acknowledge the NSF, NIH, US Air Force, US Space Force, DHHS, J.P. Morgan-Chase, and the Coleman Institute for Intellectual and Developmental Disabilities for their support of this research. More details about Szafir's work can be found at https://danielleszafir.com/.</p><p>xxx</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>'s research, and work from his group, has been recognized with awards at IEEE InfoVis 2016, ACM CHI 2021 and 2022, ACM IUI 2022, and EuroVis 2022. Significant New Researcher Award Danielle Albers Szafir, University of North Carolina at Chapel Hill The 2022 VGTC Visualization Significant New Researcher Award goes to Danielle Albers Szafir in recognition of her work on perception and vision science principles for visualization. Szafir is an Assistant Professor of Computer Science at the University of North Carolina at Chapel Hill where she directs the VisuaLab. Prior to UNC, she was an Assistant Professor at the University of Colorado, Boulder and completed her Ph.D. at the University of Wisconsin-Madison supervised by Dr. Michael Gleicher. She was a member of the 2018 Forbes 30-Under-30 for Science, served as a General Co-Chair for IEEE VIS 2022, and received a VGTC Best Dissertation Award Honorable Mention, NSF CRII Award, and NSF CAREER Award. Szafir's work bridges vision science and visualization through a combination of empirical modeling, system development, and outreach efforts. Her research characterizes perceptual limitations in existing visualization approaches, then uses experiments to construct models of relevant perceptual capabilities in visualization contexts, and finally embeds these models in systems and techniques for improving visualization scalability, accuracy, and accessibility. Her experimental work constructs data-driven models of visual perception in visualizations. These models characterize a range of topics, including color perception and visual attention. This work was amongst the first to explicitly explore the role of ensemble perception-the rapid extraction of visual statistics from a range of values-in visualization. Her work developing probabilistic models of color perception for visualization received the 2014 CIC Best Student Paper Award and the 2017 IEEE VIS Best Paper Award. These studies have led to novel metrics, design guidelines, and design support tools. Her current work explores the unique visualization constraints introduced by variations in cognitive processing for people with intellectual and developmental disabilities. With a team of students, including Ph.D. student Keke Wu, Szafir's experiments characterizing the limitations of current visualization designs for people with IDD received a Best Paper Award at CHI 2021. Szafir has employed the models developed through this research to design systems for a range of domains, , and environmental science (FieldView). In designing and deploying these systems, she is also examining the role that visualization platforms, such as augmented reality and physical media, play in data representation. In a paper that received an IEEE VR 2020 Best Paper Nomination, Szafir and Ph.D. students Matt Whitlock and Stephen Smart investigated how visualization perceptions change for a range of tasks across AR, VR, and desktop displays. These experiments provided the foundation for subsequent Immersive Analytics systems in environmental science and robotics. In addition to her research contributions, Szafir cofounded VisXVision with Zoya Bylinskii, Madison Elliott, Christine Nothelfer, and Cindy Xiong. This initiative supports events at IEEE VIS and the Vision Science Society's Annual Meeting (VSS) to foster intellectual interchange between visualization and vision science. The team co-authored a design space of vision science methods for visualization evaluation that received an IEEE VIS Best Paper Honorable Mention at VIS 2020. She is additionally a co-editor on an upcoming book on Visualization Psychology.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
