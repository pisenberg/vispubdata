<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NAS-Navigator: Visual Steering for Explainable One-Shot Deep Neural Network Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Anjul</forename><surname>Tyagi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Cong</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Klaus</forename><surname>Mueller</surname></persName>
						</author>
						<title level="a" type="main">NAS-Navigator: Visual Steering for Explainable One-Shot Deep Neural Network Synthesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-03-27T09:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. <ref type="figure">1</ref>. NAS-Navigator is visual analytics (VA) framework for explainable and human-in-the-loop neural network architecture search (NAS). NAS-Navigator implements a one-shot NAS, using an iterative evolutionary search algorithm. The interface supports the visualization of NAS with the human-in-the-loop search control. Analysts start by designing a large template network, through a lego view (A); capable of emulating the search space of candidate neural networks. This network is then trained for a few epochs to initialize meaningful weights, useful for candidate NN search, via a loss chart view (B). Following this, our evolutionary search algorithm evaluates possible candidate NNs iteratively, with iteration counter (C); sampled from the large NN, and these accuracy evaluation results are then presented in the form of a candidate NN projection on a scatterplot, via a search space view (D). Analysts can further pause/stop the search and edit the template NN based on the fitness scores generated by our search algorithm, on the candidate information view (E); to generate the final NN architecture, or to reduce the search space size. The fitness scores are calculated for each node of the candidate neural networks which are sampled from the large template network during the search.</p><p>Abstract-The success of DL can be attributed to hours of parameter and architecture tuning by human experts. Neural Architecture Search (NAS) techniques aim to solve this problem by automating the search procedure for DNN architectures making it possible for non-experts to work with DNNs. Specifically, One-shot NAS techniques have recently gained popularity as they are known to reduce the search time for NAS techniques. One-Shot NAS works by training a large template network through parameter sharing which includes all the candidate NNs. This is followed by applying a procedure to rank its components through evaluating the possible candidate architectures chosen randomly. However, as these search models become increasingly powerful and diverse, they become harder to understand. Consequently, even though the search results work well, it is hard to identify search biases and control the search progression, hence a need for explainability and human-in-the-loop (HIL) One-Shot NAS. To alleviate these problems, we present NAS-Navigator, a visual analytics (VA) system aiming to solve three problems with One-Shot NAS; explainability, HIL design, and performance improvements compared to existing state-of-the-art (SOTA) techniques. NAS-Navigator gives full control of NAS back in the hands of the users while still keeping the perks of automated search, thus assisting non-expert users. Analysts can use their domain knowledge aided by cues from the interface to guide the search. Evaluation results confirm the performance of our improved One-Shot NAS algorithm is comparable to other SOTA techniques. While adding Visual Analytics (VA) using NAS-Navigator shows further improvements in search time and performance. We designed our interface in collaboration with several deep learning researchers and evaluated NAS-Navigator through a control experiment and expert interviews. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index Terms-Deep</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>With the recent advances in computing power, deep learning (DL) has made it possible to automate the problem of feature engineering through neural networks (NNs). Highly complex features can be automatically learned from the data. However, this requires carefully designed DNN architectures, which transform the problem of feature engineering to architecture engineering <ref type="bibr" target="#b19">[20]</ref>. Some well-studied DNNs like AlexNet <ref type="bibr" target="#b28">[29]</ref> and ResNet <ref type="bibr" target="#b23">[24]</ref> have been the results of extensive architecture search studies and required many hours of manual parameter tuning by experts. Most of the current automated approaches find the optimal solution of a NN architecture based on adaptive experiments <ref type="bibr" target="#b41">[42]</ref>, and most of them rely on strong computing power. As a result, these networks are hard to generalize because of the very high hardware equipment demands and associated costs. Network Architecture Search (NAS) techniques aim at alleviating these problems for deep learning researchers by automatically finding the best candidate NN architectures based on validation accuracy. NAS designed methods have outperformed manually curated networks as shown by Zoph et al. <ref type="bibr" target="#b41">[42]</ref>, Real et. al <ref type="bibr" target="#b43">[44]</ref> and the SMASH model <ref type="bibr" target="#b6">[7]</ref>.</p><p>Typical NAS algorithms apply techniques like Reinforcement Learning (RL) <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b62">63]</ref> or Evolutionary Search (EA) <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b43">44]</ref> to search for candidate NNs directly. However, these approaches have been shown to be computationally very expensive. One-Shot NAS techniques aim at reducing the search time for NAS through training candidate neural networks via weight sharing. The idea is that instead of training each candidate NN separately, one trains a large template NN which is a super-set network of all candidates, and then uses the same weights to randomly sample candidate NN from this main network.</p><p>Most NAS algorithms use Recurrent NNs (RNNs) or DNNs as the backbone to run the search. These types of algorithms typically have a low capacity for explaining their actions and strategies <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20]</ref>. Explainability, however, is critical when deploying real-world systems which have a high need for process auditing and often also have high legal liability <ref type="bibr" target="#b51">[52]</ref>. In this work, we talk about explainability from the context of NAS, and NAS-Navigator focuses on adding explainability to the one-shot NAS process. Possible benefits of this procedure include the reasoning behind why a particular NN architecture was chosen over the others. Users can also compare how different candidate NNs behave on the data. Human-in-the-loop (HIL) assistance in NAS approaches can influence the choice of search in high stake decision making <ref type="bibr" target="#b21">[22]</ref> and so assist human users in building trust into the process. The need for explainability and HIL is crucial for many situations and has been the subject of a long debate in the HCI community, commonly referred to as the control and automation trade-off <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56]</ref>.</p><p>Evolving from a formative study done with deep learning and NAS researchers, we developed NAS-Navigator to solve the issues revolving around these problems. NAS-Navigator is a visual analytics (VA) system that reconciles both automation and user control for NAS, where expert knowledge and automated intelligent services can be combined effectively. We present a One-Shot NAS algorithm developed using evolutionary search (EA) to support our explainability and HIL use cases. Evaluation results show that our EA algorithm is fast and more effective than typical One-Shot NAS algorithms. It learns to sample better candidates given the history of selected candidates and their validation accuracy data. We find that our scheme performs better than the random sampling strategy used in the existing One-Shot NAS techniques. Overall, our contributions include solutions to three problems with existing NAS techniques:</p><p>• One-Shot NAS search speed: Our One-Shot EA NAS algorithm provides faster search results • Explainability: Our VA interface NAS-Navigator supports search tracking and progress, search space visualization, candidate ranking, and score visualizations to provide cues to the users • Human-in-the-loop control: NAS-Navigator provides a usercontrollable HIL NAS paradigm, where users can improve the  <ref type="bibr" target="#b31">[32]</ref> SMASH <ref type="bibr" target="#b6">[7]</ref> DARTS <ref type="bibr" target="#b33">[34]</ref> SETN <ref type="bibr" target="#b16">[17]</ref> BEAMES <ref type="bibr" target="#b14">[15]</ref> --REMAP <ref type="bibr" target="#b9">[10]</ref> TREEPOD <ref type="bibr" target="#b37">[38]</ref> --</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NAS-Navigator</head><p>search through VA. Users can control the final NN architecture depending on the resource-accuracy trade-off</p><p>Our evaluation through a user study and expert interviews show that NAS-Navigator is effective in adding explainability and HIL to NAS. We separately evaluate our EA One-Shot NAS algorithm for its efficiency against SOTA methods. The results we obtained show better search convergence at a similar accuracy to the final candidate model compared to existing fully automated NAS techniques.</p><p>Our paper is organized as follows. Section 2 presents related work. Section 3 describes our formative inquiry with deep learning researchers and practitioners. Section 4 presents our Explainable One-Shot NAS method. Section 5 describes our visual interface. Section 6 presents our user study and its findings. Section 7 ends with conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>We summarize several NAS research works by comparing them based on five factors as shown in Table <ref type="table" target="#tab_1">1</ref>. Overall, these methods can be divided into 3 categories; automated NAS, One-Shot NAS, and HIL NAS.</p><p>Automated NAS. Automated NAS has a long history <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b45">46]</ref>. NAS designed methods have outperformed manually curated networks as shown by Zoph et al. <ref type="bibr" target="#b41">[42]</ref>, Real et al. <ref type="bibr" target="#b43">[44]</ref> and the SMASH model <ref type="bibr" target="#b6">[7]</ref>. They use several trained networks to provide the final architecture design after evaluating each network on a validation set. DARTS <ref type="bibr" target="#b33">[34]</ref> is another popular NAS algorithm that searches for good candidate NNs through gradient descent. However, training networks with NAS is expensive since many different networks have to be trained before evaluation. Also, these methods lack the human-in-the-loop control and visual analytics, supporting explainability, as shown in Table <ref type="table" target="#tab_1">1</ref>. To overcome this, another technique called the MorphNet <ref type="bibr" target="#b22">[23]</ref> uses a different approach where the final architecture design is decided directly as a subset of a single hypernetwork where the candidate NNs share the same parameters. Liu et al. <ref type="bibr" target="#b32">[33]</ref> proposed an evolutionary search algorithm for automated NAS without the weight sharing network search method. Evolutionary search is used to generate model architectures by manipulating operations and editing edges in the network.</p><p>One-Shot NAS. Following the work on MorphNets, a slightly different approach known as One-Shot Architecture Search <ref type="bibr" target="#b5">[6]</ref> has evolved, which involves searching for the best neural network architecture as a subset of a largely trained hypernetwork. The hypernetwork in One-Shot NAS has a lesser number of parameters than training several different architectures independently <ref type="bibr" target="#b5">[6]</ref>. Our EA algorithm used in NAS-Navigator is similar to previous one-shot approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b59">60]</ref> where we train a hypernetwork to generate representative weights for every network in the search space (shared parameters). Although these algorithms are less resource-intensive than typical NAS algorithms, they still lack in the explainability and HIL aspects. Our EA algorithm is developed to sample optimal NN candidates along with explainability and HIL support to the typical one-shot NAS pipeline. Real et al. <ref type="bibr" target="#b43">[44]</ref> proposed a one-shot algorithm specifically developed to generate a model AmoebaNet-A for hand sketches. The model performance is evaluated after generation by separate training. This is different in NAS-Navigator where we refer to that information from the template network and hence it is faster. The child models are generated by mutating the NNs from the highest accuracy models, whereas in our algorithm, children are generated from blocks with the highest fitness scores. Fitness scores include the history of that block and how it performed in all the previous models.</p><p>Interactive NAS. There has been significant research in the visualization community to make NN model selection and search more effective. Techniques exist to support NAS where the model parameters are known, and the model has to be evaluated only with a single dataset <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b60">61]</ref>. VA frameworks have also been proposed for HIL ML applications <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b53">54]</ref>. BEAMES <ref type="bibr" target="#b14">[15]</ref> helps the users to find the best regression models for a given dataset iteratively. TreePOD <ref type="bibr" target="#b37">[38]</ref> provides an interface to manage the trade-off between accuracy and interpretability of different existing ML models. REMAP <ref type="bibr" target="#b9">[10]</ref> allows interactive CNN NAS starting with a few pre-trained models. Besides designing NNs, other tools allow interactive design and filtering of clustering techniques <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b44">45]</ref> and dimension reduction <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41]</ref>. However, it is considerably harder to interactively optimize a DNN compared to optimizing a regression, clustering, or dimension reduction model; NAS-Navigator contributes by adding a HIL VA interface to one-shot NAS.</p><p>Out of all these existing techniques, the work by Cashman et al. <ref type="bibr" target="#b9">[10]</ref>, REMAP is the most closely related to NAS-Navigator. However, REMAP does a global search by evaluating some set of models in a given search space where the accuracies of each model on a given dataset are already known. Getting this initial data where accuracy information is known is resource-intensive, and is not easily available for different applications. Datasets like NAS benchmarks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b58">59]</ref> are not available for most applications where deep learning is applied. Hence, in NAS-Navigator, using the one-shot technique, there is no requirement for an initial dataset where every candidate is pre-evaluated.</p><p>Another variation of NAS-Navigator from REMAP is in the ablation and variation phases. REMAP provides the user with options to drop some layers in the original NN for evaluation of different connections in the NN search space. In NAS-Navigator, we implement this operation automatically using the dropout operation during the search procedure. Also, users can still edit layers manually using the lego-view. Similarly, in the variation step of REMAP, users generate variations of different components of a candidate NN. NAS-Navigator handles that automatically through repeated components in the template network. Each layer in the template network consists of differently parameterized components. And the search procedure searches through all these combinations automatically.</p><p>Besides, there are other features in NAS-Navigator which are not available in existing works. The fitness scores of each block visualization allow the users to see which regions in the search space are impactful. For networks with skip connections, the lego view provides better visualization of the architecture. Also, users can easily set parameters using the parameter visualization sidebar, which can be easily viewed for each block. The search space view provides a single view to compare the accuracy and architectural similarities of the full search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">FORMATIVE STUDY</head><p>To systematically evolve our idea of an explainable and HIL NAS framework, we first conducted a formative study to collect requirements from deep learning researchers, their views of explainable NAS, and general workflows. This approach helped concertize our framework and tool design with a user-centered evaluation at an earlier development stage.</p><p>The formative study participants were carefully chosen to be data analysts and researchers with different experiences, working in deep learning applications with a basic understanding of NAS techniques. Out of ten participants, two were deep learning professionals working in the industry, two were professors working in computer vision and NLP, and three were Ph.D. students working in computer vision and NLP, categorized as experts for this study (E1-E7). Three were graduate students in Computer Science studying deep learning with a basic understanding of NAS, categorized as non-experts for this study (NE1-NE3). Each participant was interviewed for about 45 minutes discussing their experience in DNNs and NAS. We covered the following topics during the interview, categorized as who proposed the design ideas.</p><p>• Their experience with the general NAS and One-Shot NAS workflows. • Principles, practices, and difficulties of NAS.</p><p>• Benefits and frustrations of existing VA machine learning tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Key Findings -Design Components</head><p>The purpose of the formative study was to gather a list of requirements from domain experts and potential users, which are expected to be met by our framework. Our many discussions culminated in the following list of requirements. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">OUR EXPLAINABLE ONE-SHOT NAS METHODOLOGY</head><p>In this section, we discuss the technical details of designing a One-Shot NAS technique. NAS problems are often confined to predicting the structure of different subsets of a large template NN (known as cells) instead of the complete architecture designs. This strategy is shown to be more effective than finding complete architectures of DNNs <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b59">60]</ref>. These cells can be combined via an evaluation strategy to form a complete DNN structure, where NAS aims to find the structure of each of these cells in a DNN. A cell (in the context of CNNs) is a fully convolutional structure that maps an input tensor to an output tensor. Following the previous NAS works <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b49">50]</ref>, two types of cell structures have been found useful in the context of designing CNNs, i.e. Normal Cells and Reduction Cells.</p><p>A normal cell has CNN components with a stride of 1, which maps the input size feature map of a given height, width, and the number of feature maps (H, W, F) to the same size output feature maps (H', W', F'). A reduction cell is used to reduce the height and width of the input feature map by a factor of 2, hence (H', W', F') = (H/2, W/2, 2F).</p><p>Each cell contains B number of nodes, with a default value of 4, they can be changed through the interface. Each node is connected to every other node through six operations (O's). These values are kept similar to past works in NAS <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b32">33]</ref> and are: 3x3 Max Pooling, 3x3 Average Pooling, Skip connection, 3x3 Separable Conv, 5x5 Separable Conv, and 1x3 then 3x1 Conv. Each node inside a cell takes two inputs (I 1 , I 2 ) and returns a transformed tensor T o = t 1 (I 1 ) +t 2 (I 2 ) where t refers to the transformed input tensor through an operation O. The task is to find which of these O's work best for every pair of connected nodes for every cell.</p><p>Once the cells are identified, the overall structure of the CNNs is created using the structures identified in <ref type="bibr" target="#b62">[63]</ref>. In a typical NAS algorithm, the cell structures are fixed and users cannot change or skip the search of particular cells/operations based on evaluated candidates. However, NAS-Navigator through a VA interface, allows users to visualize these structures and make changes at any stage of the search. Users can edit the number of normal and reduction cells, and edit nodes (fix, remove and add) inside cells in real-time based on search progression results being displayed on the interface.</p><p>This idea of cell-based construction has been extended to transformer models related to non computer vision tasks. Different templates for transformer models have been suggested <ref type="bibr" target="#b20">[21]</ref>, which can be directly imported into NAS-Navigator to perform the search as the other computer vision model counterpart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Methodology</head><p>Figure <ref type="figure" target="#fig_0">2</ref> shows our methodology for implementing an explainable HIL One-Shot NAS framework with NAS-Navigator. We explain all the five stages of the process in the following text.</p><p>S1: Dataset. Users can choose a dataset using the menu bar on NAS-Navigator which controls the type of template networks the user can choose through the interface. While designing a template network, the structure of the network depends on the dataset and the global structure of the DNN. For example, CNNs for CIFAR10 and ImageNet have different template structures <ref type="bibr" target="#b62">[63]</ref>. With the help of a formative study discussed in Section 3, we have designed template networks for 10 common datasets.</p><p>S2: Design a template NN (T2). Through NAS-Navigator, users can edit the number of normal and reduction cells in the default template network; choose the number of nodes in a cell, and change the number of cells to control the depth of the template NN. To support the tasks where the default template network is not available, users can create their template network by combining components available in the sidebar (See F in Figure <ref type="figure">1</ref>). The sidebar provides template structures for CNN and LSTM components which can be combined to create template networks. This satisfies the design component requirement (T2) discussed in Section 3.1.</p><p>S3: Train the template NN (T2). The next step after selecting the template network structure is to train the template NN for a few epochs. This is important to assign meaningful weights to each node of the network. Users can choose when to stop the training, based on time and resource usage demands. The template network train accuracy affects the search results, hence more training will result in better search results, but also higher training resource consumption. Also, using the cues from the search iterations, users can edit the template network to add or remove nodes or cells. This satisfies the design requirement (T2) from Section 3.1.</p><p>S4: Search Algorithm (T1, T3, T4). Training the template network assigns meaningful weights to each path (node and their respective operations inside a cell) in the network. After training, the purpose of the search algorithm is to evaluate these paths in the template NN cells and choose the operations with the highest fitness scores. This can be presented as a graph search algorithm where each candidate NN is a path in the super-graph connecting the start and end nodes. In every search iteration, a few of these subgraphs (candidate NNs) are evaluated for validation accuracy and corresponding nodes and operations contained in that candidate are ranked. This procedure ranks and helps finalize the best performing cell structure, and hence the candidate NN. Our EA-based search algorithm evaluates the candidate NNs in each iteration and learns from their validation accuracies, thus using this information to choose better candidates in the next iteration. The algorithm updates the candidate information view (E) and the search space view (D) as shown in Figure <ref type="figure">1</ref>. More details on our search algorithm are discussed in Section 4.2. This satisfied the design requirement (T1, T3, and T4) formulated in Section 3.1.</p><p>S5: Final Architecture Evaluation. The last stage of this complete procedure is to evaluate the best candidates found with the help of the search algorithm. Users can choose to stop the search at any iteration based on the intermediate results and the amount of search space explored by the algorithm. The final model consists of one of the final candidates with the highest validation accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evolutionary Search Algorithm.</head><p>We devised an evolutionary search algorithm to search for the candidate NN architectures with HIL and add explainability to the process. The stages of our EA search algorithm are summarized in Figure <ref type="figure">3</ref> and are discussed in detail below.</p><p>EA1: Selecting the candidate models. To select each candidate NN, we generate a bitmask vector with each bit corresponding to a path inside each cell, an example shown in Figure <ref type="figure">3</ref> for candidate NNs A and B. Path refers to the connection between blocks in the network. This mask represents a subgraph from the template NN which includes the paths corresponding to set bits. We generate a candidate NN from the mask by zeroing out the weights of the paths excluded from the template network. This way, only the paths with a corresponding set bit in the mask are activated. The number of candidate NNs in the population is heuristically set to 1.5 times the number of cells in the template neural network based on the studies shown in <ref type="bibr" target="#b11">[12]</ref>, that relate the dataset dimensionality with the population size in EA algorithms.</p><p>EA2: Calculating the fitness scores. To calculate the fitness of a candidate NN (subgraph), we calculate its validation accuracy and use Equation 1 to alter the fitness scores of the paths existing in the candidate NN. α is an accuracy threshold that we fix for our experiments, which serves as a tradeoff on how fast the search algorithm learns from the current accuracy scores. Following this, all the fitness scores for a particular cell are normalized by dividing the sum of all fitness scores from individual node fitness values.</p><formula xml:id="formula_0">individualPathFitness = ValidationAccuracy − α (1)</formula><p>EA3: Choose Parents. The choose parents procedure returns the father and the mother candidate NNs from the fitness probability distributions of the population. This means that there is a higher probability of choosing the models with better fitness scores, as shown in the second block in Figure <ref type="figure">3</ref>.</p><p>EA4: Cross-Over. To generate the child architectures from the father and mother NNs, a cross-over procedure is used, shown in Algorithm 1 (Cross-Over). Firstly, a mask is generated with the procedure described in the state EA1 above. This mask is compared to the masks of the father and mother models to generate a child mask. Both the father and mother models are chosen from the best performing candidates in the population. Also, the mask is generated from the probability density of the fitness scores, i.e. more probability of bits being set at indices pointing to well-performing paths.</p><p>EA5: Mutation. The child mask is mutated as shown in Algorithm 1 (Mutation), to generate a final child candidate. We chose the mutation rate to be 0.05 based on the work by Suganuma et al. <ref type="bibr" target="#b52">[53]</ref>.</p><p>EA6: Get Mask Probabilities. At each search iteration, when the new population is updated, each path in the template network is given a fitness score between 0 and 1, see part EA2 above. These values combined, form the probability distribution of the paths from which paths for the next candidate NNs are sampled.</p><p>Complete Evolutionary Search Algorithm. The complete search algorithm combines all these different stages in each iteration with a goal to find better candidates in each iteration, learning from previous candidates' performances. Algorithm 1 explains our complete EA search, also shown in Figure <ref type="figure">3</ref>.  <ref type="formula">2</ref>). This template network consists of multiple repeated cells (normal and reduction cells, as described in Section 4) which are initially randomly initialized and are assigned meaningful weights after training the template network (3). Then one-shot search generates candidate NNs for evaluation from the template network (4), from which a final candidate is sampled iteratively; with the search process. During the search, users can edit the template NN using the search feedback (5); to make changes to the template NN architecture, which in-turn impacts the search progress and generation of candidate NNs (6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">THE INTERFACE</head><p>To implement our idea of explainable HIL NAS, we developed NAS-Navigator with the help of principles discovered during the formative study (Section 3). NAS-Navigator allows users to interactively control the search algorithm of our framework. As shown in Figure <ref type="figure">1</ref>, our interface consists of six views, which we discuss in the following text. The T# and S# next to each view show which of the formative study requirement and the One-Shot NAS stage that view satisfies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Lego View (T1, T2, T3, S2, S4)</head><p>Shown in Figure <ref type="figure">1</ref>(A), the goal of the Lego view is to allow editing of the template NN. Users can control the network depth, edit different components and visualize how the components are placed in this view. Controlling the template NN gives the power to the users to control the search. Also, users can visualize candidate NNs, which are a subgraph of the template NN in this view, where the cells and the nodes contained inside a candidate NN can be highlighted over the template NN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Loss Chart (T1, T2, S3, S5)</head><p>As shown in Figure <ref type="figure">1</ref> (B), the loss chart allows users to visualize and control the training of template NN before running our EA search algorithm. As discussed in Section 4.1 (S3), training of the template NN is crucial to assign meaningful weights to the nodes. Users can make choices on how much to train the template NN based on this loss chart and depending on the resource availability for training. We can also view the final evaluation of the selected candidate NN in this view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Search Space View (T1, T4, S4)</head><p>As shown in Figure <ref type="figure">1</ref> (D), the search space view is a scatterplot projection of candidate NN space. This is one of the key components of NAS-Navigator since it allows users to visualize and interact directly Fig. <ref type="figure">3</ref>. Our EA One-Shot NAS search algorithm overview. Assuming two candidate NNs A and B sampled from a template NN cell, are evaluated in the current iteration. After the validation accuracies are available for these two candidates, the fitness scores associated with each path of the template NN cell are updated. In this example, all the paths had equal fitness scores before the search iteration, which are updated after the validation of candidate NNs. Based on these new fitness scores, a father and mother candidate NNs are a sample from the distribution of fitness scores, and a child mask is generated using the cross-over algorithm. This child mask is the candidate NN for the next search iteration. This way, our EA algorithm generates more child candidates coming from higher fitness score paths.  f ather, mother ← chooseParents(population, f itness) maskProb ← getMaskProb(newPop) 31: return newPop, loss, maskProb with the candidate NN space. These projections are obtained using t-SNE <ref type="bibr" target="#b35">[36]</ref> and graph edit distance <ref type="bibr" target="#b0">[1]</ref> on a randomly sampled set of candidate NNs. The sampled candidate NNs are subgraphs of the template NN with nodes labeled by the component or operation type, e.g. C for convolution and R for Relu. Using these labeled directed subgraphs, the distances between each of the sampled candidate architectures are calculated using the graph edit distance which is stored in a distance matrix. A t-SNE projection is generated from this distance matrix in 2-D shown as the search space view. This clusters the search space based on the architectural similarity of the candidate models. As the search algorithm progresses, these candidates are colored based on their evaluation accuracy. Hence, the search space view acts as a dual clustered space for architecture and accuracy similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>loss.append([max(loss), mean(loss), min(loss)])</head><p>There are several user interactions supported in the search space view. Hovering over each dot highlights the candidate NN in the lego view. As the search iterations progress, the evaluated candidate NNs are colored based on their evaluation scores, see Figure <ref type="figure">1 (D)</ref>. This is useful to cluster the candidate search space, as more search iterations (shown as C in Figure <ref type="figure">1</ref>) are elapsed, the search space view will be clustered based on the candidate performances. This can help the users to separate the search space into high-scoring and low-scoring candidate regions. Using this information, users can also select a region in the search space, which then limits the search algorithm to select candidates from the selected region for further iterations.</p><p>Another useful piece of information presented with the search space view is the set operations on nodes and operations. Users can drag areas on the scatterplot and find the Union, Intersection or Complement of the nodes and operations in that region. This is a helpful operation to find the most useful components of the search space which allows users to edit the template NN, thus impacting the search algorithm directly. For example, users can remove the most common cells (intersection) from a low-scoring search space region from the template NN, which reduces the search space, thus allowing for faster convergence of the EA algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Candidate Information View (T1, T3, S4)</head><p>As shown in Figure <ref type="figure">1 (E)</ref>, the candidate information view is a scatterplot showing the relationship between frequency and fitness scores of paths in the template NN. The frequency shows how many candidate NNs contain a particular path, hence more frequency score means higher occurrence. The fitness scores are assigned during each search iteration.</p><p>The idea behind the candidate information view is to show confidence in the fitness scores of the paths. For example, removing low-scoring, high-frequency paths from the template NN can prune the search space and help converge the EA algorithm faster. Also, this view helps analyze each operation path in the template NN and their respective fitness scores which have been accumulated over the search progression. Hovering over an operation in the lego view highlights the corresponding dot on the candidate information view. Similarly, dragging an area on the scatterplot highlights all the paths contained in that area on the lego view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Menu Bar, Properties Sidebar (T2, S1, S2)</head><p>Shown in Figure <ref type="figure">1 (F,G</ref>), the menu bar provides buttons for selecting the model, optimizers, loss functions, datasets, saving a model, and set operations (discussed in Section 5.3). The properties sidebar is linked with the lego view and displays the properties of each node and operation in the template NN. Users can change the parameters for each node through this sidebar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATION</head><p>In this section, we evaluate NAS-Navigator for its effectiveness and design efficiency through a fully automated One-Shot NAS comparison with the state-of-the-art (SOTA), followed by case studies to show how HIL and VA can support better NAS. Finally, we evaluate the experience of using NAS-Navigator through a user study and expert user interviews.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Comparison of SOTA and our EA One-Shot NAS algorithms</head><p>In this study, we evaluate the performance of our EA algorithm against the existing NAS techniques on ImageNet <ref type="bibr" target="#b15">[16]</ref>. ImageNet is a largescale image classification dataset that has been extensively used for evaluating computer vision object detection research. The dataset contains 1.28 million training images with 50k validation images. For this study, we ran our EA algorithm for 1k iterations and experimented with different fitness-scores threshold values to create our final NN. Based on our experiments, the fitness-score threshold of 0.68 was used to obtain maximum accuracy on ImageNet with our model. We train our template network with a batch size of 256 for 400 epochs using an SGD optimizer. We use the setting similar to previous NAS training methods <ref type="bibr" target="#b16">[17]</ref> for setting the learning rate to 0.025 and decay to zero using the cosine scheduler. The probability of dropout for our EA algorithm was set to 0.1. Table <ref type="table" target="#tab_5">2</ref> shows the comparison of our algorithm against the existing techniques. We separate the existing techniques based on their efficiency of NAS along with human-derived NNs, shown under Task Category.</p><p>As shown in the results, Table <ref type="table" target="#tab_5">2</ref>, our model obtained the best Top-5 Acc, the same as the previous best performing model GDAS <ref type="bibr" target="#b17">[18]</ref> (best of 5 experiment runs). Other parameters for our model are comparable to the existing NAS techniques. Our search algorithm took about 1.7 GPU days on a Tesla V100 GPU to run for 200 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Case Studies</head><p>Compared to a fully automated study, we separately did case studies with real users to see the impact of HIL and VA on One-Shot NAS. The goal of this study was to compare the efficiency of our framework for search time, resource usage, and usability. We set a baseline accuracy range for our experiments which was devised based on the best performing existing NAS techniques and our automated EA algorithm. For each experiment, we noted the amount of GPU days it requires for our users to achieve that accuracy through our system. For this study, we used the CIFAR10 and CIFAR100 datasets <ref type="bibr" target="#b27">[28]</ref> containing 60k images categorized into 10 and 100 categories for object detection.</p><p>Participants. We worked with six participants for this study, who were chosen based on their experience levels with Deep Learning on Computer Vision tasks with NAS techniques. 3 were categorized as experts and 3 as Non-experts according to their experience, with experts having experience working with NAS and DL for more than 3 years. The experts were Ph.D. students in Computer Vision and non-experts were graduate students working in Computer Science with basic knowledge of Deep Learning and Computer Vision. Out of the six participants, 4 were males and 2 were females. 2 experts and 2 non-experts were the users from the formative study, who proposed the design of the original system (See Sec 3). 3 experts used in the further evaluation are listed as E1-E3.</p><p>Task Description. We initially informed the participants of the concepts of NAS and related terminologies for our framework. Next, we showed them a few examples from the template networks and the filtering steps possible with our interface. The participants were then allowed to experiment with the framework and ask clarifying questions regarding the tasks. The task for the participants in this study was to achieve a baseline accuracy range on both the CIFAR datasets. The accuracy values were decided based on existing NAS techniques and CNN models evaluated on these datasets, as shown in Table <ref type="table" target="#tab_6">3</ref>. To get the accuracy baselines for the case study tasks, we used the previous works from Table <ref type="table" target="#tab_1">1</ref> and Table <ref type="table" target="#tab_5">2</ref> which have published results on CIFAR10 and CIFAR100 datasets, separated into three categories of Human Experts, Older NAS techniques taking &gt; 100 GPU days and newer one-shot techniques taking &lt; 5 GPU days. The task for the users was to use NAS-Navigator and based on their knowledge and cues provided in the interface, get the accuracy results within the baseline range. The time taken for our expert and non-expert users to achieve this task is reported in Table <ref type="table" target="#tab_6">3</ref>.</p><p>Results. Table <ref type="table" target="#tab_6">3</ref> shows the comparison of time and accuracies achieved by NAS-Navigator with Experts and non-expert users. We can see that using NAS-Navigator both the expert and non-expert users achieved the desired accuracies in 0.6 to 1.5 GPU days. For expert users, using domain knowledge and search pruning helped achieve the accuracies in 0.6-0.9 GPU days, which is considerably less than all the existing NAS techniques. This evaluates the efficacy of VA and HIL in NAS and the impact of domain knowledge in reducing the search times. Even for non-expert users, the time taken is less than our fully automated EA algorithm, achieving similar accuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">NAS-Navigator architecture search on Imagenet dataset</head><p>In this section, we discuss the architecture search process followed by one of the experts in the study (E1) for architecture search on Imagenet <ref type="bibr" target="#b28">[29]</ref>. E1 was first given a short demonstration of using our interface followed by an explanation of the task to be performed. All search steps performed by E1 were logged along with the time taken for each search step. E1 started by loading a customized AlexNet into the interface, which has multiple options of blocks to choose from and follows the basic architecture model of AlexNet. This customized version of AlexNet contains the same number of layers as the original network but each layer has multiple blocks. For example, layer 1 has multiple convolutional blocks, i.e. Conv 3x3, Conv 5x5, and Conv7x7, and similarly for other layers. E1 explained that the reason for choosing the AlexNet template as a template network was his experience in using AlexNet for image classification tasks. After a careful understanding of the template AlexNet, E1 started by analyzing the results of the first 20 iterations of the evolutionary search algorithm, which gives a fitness value for each block of the template network. After the search results from the first four iterations, E1 decided to further analyze the 7x7 Conv blocks from Layers 1 and 2 because the fitness values of these blocks dropped to zero. Focusing on the search space view, E1 was able to find a subspace where the most common block was a7x7 Conv block in Layer1. E1 then dragged this region on the search space view which forced the search algorithm to sample candidate architectures from this search space. After 5 more search iterations, it was confirmed that the presence of this block resulted in a below-par performance of the neural networks; E1 decided to remove this block from the search space using the lego view and then continued the search further. Another 5 iterations of the evolutionary search suggested the removal of 3x3 convolution from Layer 1 and 5x5 convolution from Layer 2; these blocks were removed by E1. Additionally, the search results also suggested that 7x7 convolution was the best at Layer 3 on the evaluation dataset but E1 wasnt convinced about this result because of 2.9-4 3.2 ± 0.3 18.6 ± 0.5 his experience. Hence, E1 selected a region in the search space view where the most common block was the 7x7 Conv block at Layer 3 to evaluate more candidate neural networks from this subspace. It was confirmed after a single search iteration that most neural networks from this search space had high accuracy, hence, giving further evidence that 7x7 convolution was the best option among the other blocks at layer 3. The search also suggested that the linear block with 2,304 input and 4,096 output parameters worked the best at layer 6. This yielded the final architecture of the suggested neural network.</p><p>Results: E1 compared the results of the suggested network with a baseline of AlexNet performance on the Imagenet dataset after training for 10 epochs. While the baseline AlexNet has an accuracy of 72.70% on the test data, the network derived from our interface had an accuracy of 74.72%. This accuracy was further improved to 76.34% after E1 used his expertise and added batch normalization layer after every convolutional layer in the network. E1 was satisfied with the final network since it resulted in better accuracy than the baseline AlexNet model. This study confirmed that our tool can help computer vision researchers effectively search for and identify high-performing convolutional neural network architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Interface Evaluation Through User Study</head><p>In this study, we evaluated NAS-Navigator for its support for multiple factors of usability and creativity. Considering that the main goal of creating NAS-Navigator was to add VA and HIL in NAS, we carefully evaluated our interface for its ability to support user thought processes and creativity. Since there are no existing frameworks publicly available that can be used as a suitable baseline for this task (see Table <ref type="table" target="#tab_1">1</ref>), this</p><p>The user study results to evaluate our framework with the Creative Support Index <ref type="bibr" target="#b12">[13]</ref>, on a five-point Likert scale. We asked 7 questions to the participants, covering different aspects of usability.</p><p>user study helped us to explore the strengths and weaknesses of our interface through user feedback.</p><p>Participants. The same participants, as described in Section 6.2 performed this user study. A total of six people participated, out of which three were considered Experts and three were non-experts.</p><p>Task and Procedure. The task was to answer questions regarding the procedure they applied in the Case Study discussed in Section 6.2 and rate their experience on a five-point Likert Scale from 1 (Strongly Disagree) to 5 (Strongly Agree).</p><p>The questionnaire was based on 7 factors, 5 of which are taken from the work by Cherry et al. <ref type="bibr" target="#b12">[13]</ref> for quantifying the creativity support for design tools (Q1 to Q5 in Figure <ref type="figure">4</ref>). We added two additional factors in the questionnaire to rate the domain-specific questions (Q6 and Q7 in Figure <ref type="figure">4</ref>). The whole study took about 45 minutes for each participant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.1">Questionnaire Results</head><p>As shown in Figure <ref type="figure">4</ref>, out of the total 42 ratings on 7 questions, only 4 ratings had a low score of 1 or 2. Overall, 81% of the votes rated the questions with a 4 or 5, with Q4 and Q5 being the highest-rated questions with the most Agree votes. Overall, positive feedback and high ratings for design and usability questions show the efficacy of our interface in supporting user creativity in NAS. However, detailed feedback was collected from the experts about the low-scoring questions and some directions of possible improvement in NAS-Navigator, discussed in the following text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.2">Expert Interview Results</head><p>Besides the general results, we separately collected detailed feedback from two of the experts (E2 and E3) about their experience in working with NAS-Navigator. This interview helped compare NAS-Navigator with the existing works from an expert's perspective and helped us gather deeper insights into the VA aspect of our framework. We discuss our results organized by the themes of the questionnaire in the following text.</p><p>Enjoyment (Q1). Both the participants found NAS-Navigator to be useful in their tasks. E2 explained "Drag and drop on the template NN was a great way to edit the search and get desired results". E3 added "I like that there are template NN provided for major DL tasks, which we can directly load on the interface and start playing with them."</p><p>Exploration (Q2). Both the participants liked the search space view to exploring the candidate NN search space. E2 commented "Search Space view is a great idea and the fact that we can see clusters in the candidate NN search space shows how the structures of candidate NN can change the performance of the NNs." E3 suggested an improvement to NAS-Navigator commenting "A useful feature can be to suggest changes in the NN model based on current fitness scores for some nonexperts or in case the user has no prior idea on what NN will perform better on a given dataset."</p><p>Expressiveness (Q3). Both the experts suggested improvements in the expressiveness aspects of NAS-Navigator. E2 suggested "the user has great power to do pretty much any change with the interface, which can be great if they know what they are doing. However, in many cases, this can be a disaster if the user mistakenly updates something which later turned out to be useful." Adding on to this, E3 suggested, "it's a good idea for future will be to see the impact of user changes on the search results compared to the fully automated EA algorithm. This will allow the users to know the impact of their decision and will make the process more transparent."</p><p>Immersion (Q4). The participants were positive about the immersion part, suggesting a few improvements on top of the existing interactions. E2 commented "NAS processes are slower than the common design tasks, which are more commonly done through dashboards. Hence, the users cannot see immediate results of their actions in this case." E3 added to this comment and mentioned "While the users are waiting for the search iteration to complete, a notification will be useful to see if a particular search iteration has found something useful which can have some impact on the NN performance. Every search result, if it can be linked with user action and its impact, will be useful information to have on the interface."</p><p>Results worth effort (Q5). Both the participants agreed that even small interactions if done correctly, can have a great impact on the search convergence time and the final NN performance.</p><p>Interactions (Q6). The participants had mixed reactions to the interaction effects of NAS-Navigator. E2 commented "It would be great if we could track and visualize how the scores have changed over the search iterations. It'll further add to the decision-making as we will be able to see the history of changing scores and not just the last timestamp." E3 was satisfied with the interactions in NAS-Navigator and commented "The search space interactions are useful in controlling the areas to search from. This is a great idea and the fact that I can see all the candidate NNs in a single space is very useful."</p><p>Results Quality (Q7). Both the participants were satisfied with the quality of the results. An improvement was suggested by E2 who said "Sometimes the clusters in the search space view scatterplot are not very clear, maybe adding supporting plots to show further details of each network architecture would be useful."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION</head><p>Several important lessons were learned while designing this framework.</p><p>Our initial discussion with domain experts was decisive in pinning down the main interface design. After all, tasks were formulated within comprehensive discussions with the experts, it was easier to design the visual interface and its components. Also, we realized that adding strong user interaction facilities was important, as a means to allow users to infuse their domain knowledge into the search process to accelerate convergence to the final solution. This design allows analysts to use their domain knowledge and the one-shot search results to quickly converge to the best performing neural network architecture for a given task. Analysts also have the freedom to apply certain soft constraints at their discretion, for example trading off between neural network size and accuracy, for example, different Resnet <ref type="bibr" target="#b23">[24]</ref> sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>This paper presents a visual analytics framework to assist in deep neural network architecture search. Our interface combines the automated one-shot neural network architecture search approach with a humanin-the-loop design. Our interface is also less resource-intensive than conventional automatic neural network architecture search algorithms. Analysts can quickly load a template neural network along with their dataset and explore different subset neural network architectures to find the best one. Our evolutionary search algorithm allows for quick sampling of well-performing candidate architectures which can then be further evaluated for their performance. A design study was conducted in collaboration with several researchers working in the deep learning domain to lay down the tasks to be performed by our interface. We evaluated our framework for its ability to better search for the best performing neural network architecture with the help of a user study, case studies, and expert interviews. However, besides the effectiveness of our present interface, there remains some scope for improvement, which will be taken on in future work for this project. First, we would like to run comparison experiments to compare the performance of our EA algorithm against other solutions including reinforcement learning and bayesian optimization. Taking some points from our interview evaluation, we will add supporting visualization to the search space view to better present the clusters and candidate NN architectures inside these clusters. We will work on identifying user actions for each search iteration result, that will predict and suggest these actions to the non-expert users. Also, tracking of search iteration results and changes in the fitness scores will be added. We would also like to evaluate NAS-Navigator on language models to extend its applicability. Currently, all our evaluation is based on computer vision networks, but as more NAS algorithms start to come from the language domain, it will be interesting to see how NAS-Navigator compares against the state-of-the-art transformer models. These features are not yet supported and we will continue to work on our interface to incorporate them in the future. We plan to deploy NAS-Navigator for real users as a long-term study to collect in-depth feedback and usage scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. Stages in a one-shot NAS search process are mainly divided into two parts, template network training, and the search algorithm to find possible candidates from the trained template NN. After loading the dataset (1), a template network is trained (shown as the transition from dashed lines to solid lines in the figure) with shared parameters (2). This template network consists of multiple repeated cells (normal and reduction cells, as described in Section 4) which are initially randomly initialized and are assigned meaningful weights after training the template network (3). Then one-shot search generates candidate NNs for evaluation from the template network (4), from which a final candidate is sampled iteratively; with the search process. During the search, users can edit the template NN using the search feedback (5); to make changes to the template NN architecture, which in-turn impacts the search progress and generation of candidate NNs<ref type="bibr" target="#b5">(6)</ref>.</figDesc><graphic url="image-80.png" coords="5,63.91,301.53,126.54,175.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>19 : 20 :</head><label>1920</label><figDesc>procedure EVOLVE(population) 21: f itness ← fitness scores for each NN in population 22: newPop ← Top k NN from population with highest fitness 23: k &lt;length(population) 24:for i=0 to length(population)-k step 1 do 25:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Learning, Neural Network Architecture Search, Visual Analytics, Explainability</figDesc><table /><note>• Anjul Tyagi, Cong Xie and Klaus Mueller are with the Visual Analytics and Imaging Lab at Computer Science Department, Stony Brook University, New York. E-mail: {aktyagi, coxie, mueller}@cs.stonybrook.edu. Manuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication xx xxx. 201x; date of current version xx xxx. 201x. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org. Digital Object Identifier: xx.xxxx/TVCG.201x.xxxxxxx</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison of NAS-Navigator with different NAS algorithms based on five aspects. VA stands for Visual Analytics, showing whether the technique supports HIL interaction with NAS. Eff shows if the search algorithm is efficient, i.e. the search time is less than 5 GPU days for a CNN. Sh stands for shared parameters, meaning whether the algorithm supports search through a shared parameter template network. Exp means explainable, comparing algorithms that support explainability in NAS. No PT means no pre-training of the candidate networks is required for the search algorithm. The table is divided into 4 sections of rows, separating the manually designed DNNs, automated NAS techniques, VA techniques for NAS, and our work (NAS-Navigator).</figDesc><table><row><cell>Method</cell><cell>VA Eff Sh</cell><cell>Exp</cell><cell>No PT</cell></row><row><cell>ResNet [24]</cell><cell>-</cell><cell></cell><cell></cell></row><row><cell>NASNet [63]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>EAS [42]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PNAS</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 .</head><label>2</label><figDesc>Comparing our EA algorithm to existing State-of-the-art NAS techniques and human designed CNNs on ImageNet. Task Category groups the models based on their NAS effectiveness, separately placing the human designed CNNs. We run our EA algorithm on Testla V100 GPUs for 200 iterations, achieving best Top-5 Acc with our experiments, similar to GDAS. The experiments took approximately 5 hrs on NAS-Navigator with 8 GPUs.</figDesc><table><row><cell>Task Category</cell><cell>Method</cell><cell>GPU</cell><cell>Parameters (MB)</cell><cell>Top-1 Acc</cell><cell>Top-5 Acc</cell></row><row><cell></cell><cell></cell><cell>days</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>ResNet [24]</cell><cell>-</cell><cell>11.7</cell><cell>69.8</cell><cell>89.1</cell></row><row><cell>Human Experts</cell><cell>Inception-v1 [3]</cell><cell>-</cell><cell>6.6</cell><cell>69.8</cell><cell>89.9</cell></row><row><cell></cell><cell>Progressive NAS [32]</cell><cell>150</cell><cell>5.1</cell><cell>74.2</cell><cell>91.9</cell></row><row><cell>NAS with more than 100 GPU days</cell><cell>NASNet [43]</cell><cell>2000</cell><cell>5.3</cell><cell>72.8</cell><cell>91.3</cell></row><row><cell></cell><cell>DARTS [34]</cell><cell>4</cell><cell>4.9</cell><cell>73.1</cell><cell>91</cell></row><row><cell></cell><cell>SNAS [57]</cell><cell>1.4</cell><cell>4.3</cell><cell>72.7</cell><cell>90.8</cell></row><row><cell>NAS with less than 5 GPU days</cell><cell>GDAS [18]</cell><cell>0.85</cell><cell>5.3</cell><cell>74</cell><cell>91.5</cell></row><row><cell></cell><cell>SETN [17]</cell><cell>1.8</cell><cell>5.3</cell><cell>74.1</cell><cell>91.4</cell></row><row><cell></cell><cell>NAS-Navigator (T=200)</cell><cell>1.7</cell><cell>5.3</cell><cell>73.9</cell><cell>91.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .</head><label>3</label><figDesc>Evaluating our HIL One-Shot NAS framework with fully automated NAS techniques. We categorize existing NAS techniques and CNNs into three categories shown as first three rows in Methods. The time taken by users through NAS-Navigator to achieve comparable accuracies on CIFAR10 and CIFAR100 datasets is less than existing fully automated techniques. This shows the importance of HIL and VA in reducing the resource footprint of NAS. The experiments took approximately 3.5 hrs on NAS-Navigator with 8 Tesla V100 GPUs.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="2">GPU days range Params (MB)</cell><cell>Error on CIFAR 10</cell><cell>Error on CIFAR 100</cell></row><row><cell></cell><cell></cell><cell></cell><cell>range</cell><cell></cell></row><row><cell>Human Experts</cell><cell></cell><cell>-</cell><cell>24-30</cell><cell>3.38 ± 0.8</cell><cell>21.76 ± 6.14</cell></row><row><cell>NAS &gt; 100 GPU days</cell><cell></cell><cell>150-2000</cell><cell>3.3-10.6</cell><cell>3.52 ± 0.34</cell><cell>21.62 ± 6.34</cell></row><row><cell>NAS &lt; 5 GPU days</cell><cell></cell><cell>0.84-50</cell><cell>3.3-5.7</cell><cell>3.25 ± 0.8</cell><cell>18.5 ± 2.7</cell></row><row><cell cols="2">NAS-Navigator (No VA)</cell><cell>1.4-2.0</cell><cell>3.3</cell><cell>2.6 ± 0.1</cell><cell>17.8 ± 0.3</cell></row><row><cell>(T=1K)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">NAS-Navigator (Experts)</cell><cell>0.6-0.9</cell><cell>3.1 -3.7</cell><cell>2.83 ± 0.2</cell><cell>17.82 ± 0.7</cell></row><row><cell>NAS-Navigator</cell><cell>(Non-</cell><cell>0.8-1.5</cell><cell></cell><cell></cell></row><row><cell>Experts)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">ACKNOWLEDGEMENTS</head><p>We would like to thank the anonymous VIS 2022 reviewers for their valuable comments. This work was partially funded by NSF grants CNS 1900706, IIS 1527200 and 1941613, and NSF SBIR contract 1926949.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">An exact graph edit distance algorithm for solving pattern recognition problems</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Abu-Aisheh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raveaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Ramel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Martineau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Peeking inside the black-box: A survey on explainable artificial intelligence (xai)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Adadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berrada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="52138" to="52160" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning algorithm for autonomous driving using googlenet</title>
		<author>
			<persName><forename type="first">M</forename><surname>Al-Qizwini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Barjasteh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Al-Qassab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Radha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Intelligent Vehicles Symposium (IV)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Guidelines for Human-AI Interaction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vorvoreanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fourney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nushi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Collisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Inkpen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Teevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kikin-Gil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
		<idno type="DOI">10.1145/3290605.3300233</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2019 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>Glasgow Scotland Uk</publisher>
			<date type="published" when="2019-05">May 2019</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Visual pattern discovery using random projections</title>
		<author>
			<persName><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wilkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Dang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Visual Analytics Science and Technology (VAST)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="550" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Smash: oneshot model architecture search through hypernetworks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05344</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Graphs are not enough: using interactive visual analytics in storage research</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kuenning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zadok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th {USENIX} Workshop on Hot Topics in Storage and File Systems</title>
				<imprint>
			<date type="published" when="2019">HotStorage 19. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A user-based visual analytics workflow for exploratory model analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cashman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Humayoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Heimerl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Saket</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mosca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Endert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
				<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="185" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ablate, variate, and contemplate: Visual analytics for discovering neural architectures</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cashman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="863" to="873" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Clustrophile 2: Guided visual clustering analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cavallo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">¸</forename><surname>Demiralp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="267" to="276" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Measuring the curse of dimensionality and its effects on particle swarm optimization and differential evolution</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Montgomery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bolufé-Röhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="514" to="526" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Quantifying the creativity support of digital tools through the creativity support index</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Latulipe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer-Human Interaction (TOCHI)</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An interactive visual testbed system for dimension reduction and clustering of large-scale highdimensional data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visualization and Data Analysis</title>
				<imprint>
			<publisher>International Society for Optics and Photonics</publisher>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="volume">8654</biblScope>
			<biblScope unit="page">865402</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Beames: Interactive multimodel steering, selection, and inspection for regression tasks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cashman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Endert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE computer graphics and applications</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="20" to="32" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2009.5206848</idno>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">One-Shot Neural Architecture Search via Self-Evaluated Template Network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00378</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10">Oct. 2019</date>
			<biblScope unit="page" from="3680" to="3689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Searching for a robust neural architecture in four gpu hours</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1761" to="1770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Nas-bench-201: Extending the scope of reproducible neural architecture search</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.00326</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.05377</idno>
		<title level="m">Neural architecture search: A survey</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Autobert-zero: Evolving bert backbone from scratch</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.07445</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards human-guided machine learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Honaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>D'orazio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Garijo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gadewar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jahanshad</surname></persName>
		</author>
		<idno type="DOI">10.1145/3301275.3302324</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Intelligent User Interfaces, IUI &apos;19</title>
				<meeting>the 24th International Conference on Intelligent User Interfaces, IUI &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019-03">Mar. 2019</date>
			<biblScope unit="page" from="614" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Morphnet: Fast &amp; simple resource-constrained structure learning of deep networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Eban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1586" to="1595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Agency plus automation: Designing artificial intelligence into interactive systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1807184115</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1844" to="1850" />
			<date type="published" when="2019-02">Feb. 2019</date>
		</imprint>
	</monogr>
	<note>Sciences</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Principles of mixed-initiative user interfaces</title>
		<author>
			<persName><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
		<idno type="DOI">10.1145/302979.303030</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI conference on Human Factors in Computing Systems, CHI &apos;99</title>
				<meeting>the SIGCHI conference on Human Factors in Computing Systems, CHI &apos;99<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="1999-05">May 1999</date>
			<biblScope unit="page" from="159" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ipca: An interactive system for pca-based visual analytics</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ziemkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ribarsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
				<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="767" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Task classification model for visual fixation, exploration, and search</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th ACM Symposium on Eye Tracking Research &amp; Applications</title>
				<meeting>the 11th ACM Symposium on Eye Tracking Research &amp; Applications</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Clustervision: Visual supervision of unsupervised clustering</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Eysenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">De</forename><surname>Filippi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="142" to="151" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00559</idno>
		<idno>arXiv: 1712.00559</idno>
	</analytic>
	<monogr>
		<title level="j">Progressive Neural Architecture Search</title>
		<imprint>
			<date type="published" when="2018-07">July 2018</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00436</idno>
		<idno>arXiv: 1711.00436</idno>
		<title level="m">Hierarchical Representations for Efficient Architecture Search</title>
				<imprint>
			<date type="published" when="2018-02">Feb. 2018</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Darts: Differentiable architecture search</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visual exploration of high-dimensional data through subspace analysis and dynamic projections</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Thiagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-T</forename><surname>Bremer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pascucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
				<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Designing neural networks using genetic algorithms</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Todd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">U</forename><surname>Hegde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICGA</title>
				<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="379" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Treepod: Sensitivity-aware selection of pareto-optimal decision trees</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mühlbacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Linhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Möller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Piringer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="174" to="183" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A partition-based framework for building and validating regression models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mühlbacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Piringer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1962" to="1971" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Clustersculptor: A visual analytics tool for high-dimensional data</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zelenyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Imre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE Symposium on Visual Analytics Science and Technology</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="75" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Tripadvisorˆ{ND}: A tourism-inspired highdimensional space exploration framework with overview and detail</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="291" to="305" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03268</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.03151</idno>
		<title level="m">Nasnet: A neuron attention stage-by-stage net for single image deraining</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the aaai conference on artificial intelligence</title>
				<meeting>the aaai conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Somflow: Guided exploratory cluster analysis with self-organizing maps and analytic provenance</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sacha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kraus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Behrisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schreck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="120" to="130" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
		<respStmt>
			<orgName>Technische Universität München</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Integrating data and model space in ensemble learning by visual analytics</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jäckle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Stoffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Diehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Keim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Direct manipulation vs. interface agents. Interactions</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Maes</surname></persName>
		</author>
		<idno type="DOI">10.1145/267505.267514</idno>
		<imprint>
			<date type="published" when="1997-11">Nov. 1997</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="42" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Nasbench-301 and the case for surrogate benchmarks for neural architecture search</title>
		<author>
			<persName><forename type="first">J</forename><surname>Siems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.09777</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<title level="m">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">EXS: Explainable Search Using Local Model Agnostic Interpretability</title>
		<author>
			<persName><forename type="first">J</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
		<idno type="DOI">10.1145/3289600.3290620</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining, WSDM &apos;19</title>
				<meeting>the Twelfth ACM International Conference on Web Search and Data Mining, WSDM &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019-01">Jan. 2019</date>
			<biblScope unit="page" from="770" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A genetic programming approach to designing convolutional neural network architectures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Suganuma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shirakawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nagao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the genetic and evolutionary computation conference</title>
				<meeting>the genetic and evolutionary computation conference</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="497" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Visualization and optimization techniques for high dimensional parameter spaces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.13812</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Ice: An interactive configuration explorer for high dimensional categorical parameter spaces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Estro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zadok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on VAST</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="23" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khurana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mueller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.09904</idno>
		<title level="m">Infographics wizard: Flexible infographics authoring and design exploration</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.09926</idno>
		<title level="m">SNAS: stochastic neural architecture search</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><surname>Renas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01523</idno>
		<idno>arXiv: 1910.01523</idno>
		<title level="m">Relativistic Evaluation of Neural Architecture Search</title>
				<imprint>
			<date type="published" when="2021-09">Sept. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Nas-bench-101: Towards reproducible neural architecture search</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7105" to="7114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05749</idno>
		<idno>arXiv: 1810.05749</idno>
		<title level="m">Graph HyperNetworks for Neural Architecture Search</title>
				<imprint>
			<date type="published" when="2020-12">Dec. 2020</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Manifold: A model-agnostic framework for interpretation and diagnosis of machine learning models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Ebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="364" to="373" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07012</idno>
		<idno>arXiv: 1707.07012</idno>
		<title level="m">Learning Transferable Architectures for Scalable Image Recognition</title>
				<imprint>
			<date type="published" when="2018-04">Apr. 2018</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
