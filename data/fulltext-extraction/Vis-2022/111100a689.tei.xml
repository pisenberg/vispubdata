<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IDLat: An Importance-Driven Latent Generation Method for Scientific Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jingyi</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Haoyu</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jiayi</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ayan</forename><surname>Biswas</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Han-Wei</forename><surname>Shen</surname></persName>
						</author>
						<title level="a" type="main">IDLat: An Importance-Driven Latent Generation Method for Scientific Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-03-27T09:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Latent space</term>
					<term>scientific data representation</term>
					<term>deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning based latent representations have been widely used for numerous scientific visualization applications such as isosurface similarity analysis, volume rendering, flow field synthesis, and data reduction, just to name a few. However, existing latent representations are mostly generated from raw data in an unsupervised manner, which makes it difficult to incorporate domain interest to control the size of the latent representations and the quality of the reconstructed data. In this paper, we present a novel importance-driven latent representation to facilitate domain-interest-guided scientific data visualization and analysis. We utilize spatial importance maps to represent various scientific interests and take them as the input to a feature transformation network to guide latent generation. We further reduced the latent size by a lossless entropy encoding algorithm trained together with the autoencoder, improving the storage and memory efficiency. We qualitatively and quantitatively evaluate the effectiveness and efficiency of latent representations generated by our method with data from multiple scientific visualization applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>As machine learning techniques become increasingly more ubiquitous for scientific visualization and analysis, latent representations generated by autoencoders have attracted great attentions of researchers in recent years. Latent representations have been successfully demonstrated to retain essential information in the original data, and can be used for similarity analysis <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28]</ref>, generation of visualizations <ref type="bibr" target="#b5">[6]</ref>, synthesis of simulations <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>, data reductions <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b43">44]</ref>, and have been applied to multivariate volumetric data <ref type="bibr" target="#b27">[28]</ref>, streamlines and stream surfaces <ref type="bibr" target="#b17">[18]</ref>, isosurfaces <ref type="bibr" target="#b11">[12]</ref>, and particles <ref type="bibr" target="#b24">[25]</ref>.</p><p>Although latent representations for large-scale scientific data have been used extensively, there are still several challenges. First, domain scientists have diverse interests in different data portions, but latent representations trained using unsupervised approaches have limited support for incorporating such domain interests. Given that scientific data complexity varies across space and time <ref type="bibr" target="#b13">[14]</ref>, domain scientists' interests should be taken into account during latent generation so that it is possible to perform importance-driven scientific data explorations as well as to reduce data that are not deemed important. To the best of our knowledge, related works only support generating latent representations associated with simulation parameters <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b42">43]</ref>, time <ref type="bibr" target="#b41">[42]</ref>, and aggregated queries <ref type="bibr" target="#b40">[41]</ref>. Second, how to represent diverse domain interests in a unified way for latent generation is non-trivial. Domain interest in scientific visualization can be defined in many ways, either mathematically related to physical attributes or spatially/temporally related to particular ranges <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b35">36]</ref>. A generalized representation is required to incorporate different types of scientific interests. Third, the costs of importance-driven latent generation can be high. Previous latent representations are tightly coupled with specific scientific visualization applications <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b27">28]</ref>. If scientists change their interests during exploration, re-training the model will be needed but can be prohibitively expensive. Also, current latent generation methods cannot adapt the size of the latent to the domain interest once neural network architecture is determined, leading to high storage and I/O costs.</p><p>In this paper, to generate latent representations guided by scientific interests, we propose an Importance-Driven Latent generation method (IDLat) based on a convolutional autoencoder to combine the power of the convolution operations for extracting local features and the autoencoder for representation learning <ref type="bibr" target="#b4">[5]</ref>. First, to incorporate domain interests into latent representations, we extend the basic autoencoder with a feature transformation network that takes domain interest as an input to guide the mapping from scientific data to latent representations. Second, based on the proposed network, we represent various types of domain interests with discretized spatial importance maps. Every element in the importance map is a real value indicating how vital this spatial location is when generating the latent representation. The importance values can be derived mathematically based on the domain or heuristically based on distances, distributions, locations, etc., depending on the underlying scientific applications. With the location-wise control of spatial importance, we can flexibly represent various types of scientific interests and use them to guide latent generation. Third, our model only needs to be trained once for each dataset, and used even when scientists change the definition of importance. The produced latents are optimized in storage size with the help of feature transformation networks and a lossless entropy encoding module. The motivation for jointly pursuing importance-driven latent and compression can be summarized into two aspects. The first is to further reduce the storage cost of scientific data based on its importance. Latent representations are compact, but their sizes are determined by the network architecture, not the amount of information according to domain interest. To optimize the usage of storage, we quantize and compress latent with importance taken into account, i.e., reduce the size of latent for unimportant data.</p><p>The second is to improve the effectiveness of latent in representing scientific features. The original data may contain unimportant information such as noise or non-feature regions which compromise latent's ability to represent features. However, with importance control and entropy constraint in the latent space, the model will optimize the utilization of limited latent dimensions by preserving more important information and sacrificing the unimportant information. As a result, each latent is instructed to encode important information effectively.</p><p>Our latent generation workflow is as follows. First, spatial importance maps are generated based on scientific interests. Second, both the original scientific data and importance maps are taken as input to our model, which produces latent representations controlled by the importance map. Third, we quantize the generated latents into discrete symbols. Fourth, given that the entropy of discrete latents will be different under different importance settings, we apply lossless entropy encoding on the discrete latent vectors to further reduce the latent size. After the model is trained, we support visualization and analysis in both latent space and data space. In latent space, the discrete latent representations are losslessly recovered through entropy decoding for scientific analysis such as similarity comparison and feature exploration. In data space, the discrete symbols are further decoded to obtain reconstructed data through the autoencoder's decoder.</p><p>Our latent representation is useful for scientific visualization and analysis due to its compactness and effectiveness in preserving domain interests. Each latent representation is forced to focus on representing data of interest instead of all details of the raw data, which amplifies the more salient information and reduces the effect of noise, resulting in more salient and robust data representations. By transforming data into compact latent representations, similarity comparison or distance computation between data becomes efficient and robust. Also, it reduces the storage cost by only saving the compressed latent representations that can later be used for downstream scientific analysis tasks such as projection, retrieval, feature exploration, clustering, query, etc.</p><p>We qualitatively and quantitatively evaluate the usefulness and effectiveness of our importance-driven latent representations through data reconstruction and latent space exploration tasks on three scientific datasets. In summary, the contributions of our work are threefold:</p><p>• First, we present a novel and flexible pipeline for generating importance-driven scientific data representation with an autoencoder model. • Second, we utilize a location-based importance map to incorporate domain interests into the generation of latent representation. • Third, we further reduce the size of latent representation through entropy encoding to reduce the I/O and storage costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>Our study makes use of a deep learning based latent representation for importance-driven data visualization and analysis. We summarize the related works of these two fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Latent Representations in Scientific Visualization</head><p>In scientific visualization, there are three main usages of autoencoders. The first, and the most related one to our work, is to use autoencoders for user-controlled data synthesis. Wiewel et al. <ref type="bibr" target="#b42">[43]</ref> converted raw volume data into latent representations and controlled data properties such as velocity or density through different latent dimensions. Berger et al. <ref type="bibr" target="#b5">[6]</ref> proposed to learn the mapping from transfer functions to rendered volumes with an encoder-decoder architecture. By traversing the latent space and generating rendered images of the volume under various viewpoints and transfer functions, scientists can get a better understanding of the volume features efficiently. Kim et al. <ref type="bibr" target="#b21">[22]</ref> proposed a latent space integration network to learn the mapping of latent representations from the current time step to the next time step. Second, latent representations are also used as feature descriptors of the raw data to select representatives. FlowNet <ref type="bibr" target="#b17">[18]</ref> proposed to identify representative flow lines or surfaces in the lower dimensional latent space by applying density-based clustering on latent representations. To select representative time steps for volumetric time-varying data, instead of using handcrafted features, Porter et al. <ref type="bibr" target="#b27">[28]</ref> adopted autoencoders to learn a representation for each volume and selected representations in the t-SNE projection. The third usage of autoencoders is data reduction. AE-SZ <ref type="bibr" target="#b25">[26]</ref> and multi-branch decoder network <ref type="bibr" target="#b43">[44]</ref> demonstrate the effectiveness of autoencoders for scientific data reduction. However, existing autoencoder-based works assume every data element is equally important without considering scientists' interest when generating the latent. Also, from a data reduction point of view, knowing which region scientists have low interests and thus can afford to have a lower quality will help achieve a better trade-off between the size and the quality of the latent representation. Therefore, we extend the basic autoencoder into one conditioned on user interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Importance-Driven Visualization and Analysis</head><p>For different scientific applications, it is well advised to consider the varying importance throughout the dataset during visualization and analysis. Importance-driven techniques can be classified into two categories: with and without direct user interaction.</p><p>Studies that involve user interaction usually require users to decide the importance. Driven by the visualization goal, Peng et al. <ref type="bibr" target="#b26">[27]</ref> proposed to define mesh importance using transfer functions for interactive isosurface rendering. Burger et al. <ref type="bibr" target="#b8">[9]</ref> proposed to control the shape and density of particles so that scientists can focus on the important regions where the region of interest is either user-defined or feature-based. Viola et al. <ref type="bibr" target="#b31">[32]</ref> defined the object of interest through user selection and smoothly modifies viewpoint and visual parameters when changing the object of interest. Viola et al. <ref type="bibr" target="#b33">[34]</ref> proposed importance-driven volume rendering by manually assigning different importance to the pre-segmented objects in the data to maximize the visual information in the rendered results. Wang et al. <ref type="bibr" target="#b39">[40]</ref> proposed a feature-preserving data reduction method that allows users to magnify regions according to the degree of interest for focus+context visualization.</p><p>Importance-driven visualization without user interaction has predefined importance based on the domain knowledge or is totally datadriven. Wang et al. <ref type="bibr" target="#b35">[36]</ref> incorporated domain knowledge, e.g., salient isosurface and defined the importance of data based on the inverse distance to the surface of interest. To reduce massive visual information during particle tracing, Viola et al. <ref type="bibr" target="#b32">[33]</ref> utilized the object importance to define the sparseness level of each feature for controlling opacity values and rendering styles of the feature. Other works define data importance based on statistical models. For example, Wang et al. <ref type="bibr" target="#b34">[35]</ref> defined importance through conditional entropy by measuring the amount of entropy one block remains given blocks of neighboring time steps. Gosink et al. <ref type="bibr" target="#b15">[16]</ref> introduced a statistical framework to explore variable trends and identify important variables for different regions.</p><p>Our work is related to importance-driven visualization and analysis. The difference is that we use the importance to generate a controlled latent representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BACKGROUND</head><p>Our importance-driven latent generation framework is based on an autoencoder with a quantizer in the latent space. In this section, we introduce this model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Non-linear Transform Coding using Autoencoders</head><p>A recent work <ref type="bibr" target="#b0">[1]</ref> indicates that compared with linear transform coding, nonlinear transform coding is more flexible and can better adapt to the source signal distribution. In our work, we utilize nonlinear transform coding via a convolutional autoencoder. The autoencoder contains two parts, an encoder f which converts the raw data x into a latent representation y and a decoder g which decodes the latent y and gets a reconstruction x of x. The latent size is often smaller than the raw data, which forms a bottleneck to restrict the information flow from the encoder to the decoder. For example, as shown in Fig. <ref type="figure" target="#fig_0">2</ref>, after several convolutional layers, the original data are converted to a latent of size K × 3 × 3 × 3, where K is the number of filters in the last convolutional layer, also known as channel size of the latent. The bottleneck forces the latent to preserve only the most vital information in the data. Thus, the autoencoder is suitable to generate compact data representations.</p><formula xml:id="formula_0">Encoder f Decoder g Quantizer Q x xŷ = Q(y) y ŷ</formula><p>Fig. <ref type="figure">1</ref>. Autoencoder with a quantizer Q in the latent space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Quantized Latent Space for Data Reduction</head><p>To get a better data reduction performance, unlike a basic autoencoder which only minimizes the reconstruction loss, Ballé et al. <ref type="bibr" target="#b2">[3]</ref> further quantize the latent representation by a quantizer Q, as shown in Fig. <ref type="figure">1</ref>. Then, the discrete symbols themselves are losslessly compressed through entropy coding. The optimization goal of this autoencoder is to minimize both the reconstruction loss and the entropy of the quantized latent representations, formulated as:</p><formula xml:id="formula_1">E x [− log 2 p ŷ( ŷ)] R +λ E x [ x − x 2 2 ] D (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>where the quantized latent ŷ = Q( f (x)), the reconstructed data x = g( ŷ), and Q is the quantizer. Equation 1 is aligned with the rate-distortion theory. R is the rate that determines the number of bits per symbol for data reduction; in our case, it is the latent entropy. D is the distortion between the original and the reconstructed data, i.e., reconstruction loss. λ is a tradeoff parameter. A larger λ will focus more on reducing the distortion D during optimization. As a result, more bits are required to maintain the reconstruction quality, and we will have a larger rate R. However, the quantization is not differentiable. To make quantization differentiable and incorporate the quantization error during training, Ballé et al. <ref type="bibr" target="#b1">[2]</ref> replace the quantizer with additive uniform noise. Now instead of the quantized representation ŷ = Q( f (x)), we have a "noisy" representation ỹ = f (x) + Δy, where Δy ∼ U(− 1 2 , 1 2 ). The optimization goal changes into <ref type="bibr" target="#b2">[3]</ref>:</p><formula xml:id="formula_3">E x,Δy [− log 2 p ỹ( ỹ)] R +λ E x,Δy [ x − x 2 2 ] D (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>where x = g( ỹ) is the reconstruction. To remove the constraint on the input size, a non-parametric distribution is used to model the probability density for channels of latent p ỹ, as shown in Fig. <ref type="figure" target="#fig_0">2</ref> (left). During testing, the actual quantization, such as rounding is applied. After that, a lossless entropy encoding on the quantized latents is applied to convert latents into bitstreams. More frequent data will be represented by shorter bits than less frequent data. One limitation of the above method is that the entropy estimation of the latent representation is not accurate. Entropy encoding relies on the probabilistic distributions of discrete latents to decide which codeword will represent which quantized symbol so that the average bit length is minimal. The better the probabilities are modeled, the closer the bit rate approaches the optimal lower bound. However, the above method does not consider spatial and raw data dependencies when estimating the probability <ref type="bibr" target="#b3">[4]</ref>, due to the reason that it only models a channel-wise latent distribution for an ensemble of input.</p><formula xml:id="formula_5">K channels channel 1 channel 2 channel K (μi,σi) (μj,σj) (μk,σk) K channels</formula><p>To improve entropy estimation, one follow-up work <ref type="bibr" target="#b3">[4]</ref> introduces a hyperprior network to extract side information to assist latent probability estimation. The hyperprior network takes side information as input to predict a prior on the parameters of latent's probability distribution. As shown in Fig. <ref type="figure" target="#fig_0">2</ref> (right), each latent dimension is modeled as a Gaussian where the scale and mean of each Gaussian are predicted by the side information <ref type="bibr" target="#b3">[4]</ref>.</p><p>With the quantized latent space and improved entropy estimation, we can achieve a better data reduction performance. In the next section, we will present our latent generation method based on a quantized autoencoder which achieves the data reduction goal, and more importantly, takes domain interest into consideration during latent generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">OVERVIEW AND ALGORITHM REQUIREMENTS</head><p>Algorithm Requirements: We summarize three algorithm requirements to generate domain-interest-guided latent representations.</p><p>• The generated latent representations need to respond to different domain interests. • The algorithm needs to be adaptive to different types of domain interests such that scientists do not need to train multiple neural network models when they vary their interests. • The algorithm needs to generate compact latent representations whose size depends on the domain interest, i.e., low domain interest means a more compact latent representation.</p><p>Overview To generate latent representations for scientific data guided by scientists' interests, we propose an importance-driven latent generation algorithm. An overview of the proposed method is shown in Fig. <ref type="figure" target="#fig_2">3</ref>. The first stage of our method is to properly represent various scientific interests with spatial importance maps, which can be interpreted as, for each spatial location, how much information scientists want to preserve when generating the compact latent representation. Then the second stage is to generate scientific interest-guided latent representations through our autoencoder model. We take a block-wise processing strategy. Volumetric data and corresponding importance maps are divided into blocks and then processed by the model. Conditioned on the importance map, the data blocks are non-linearly encoded and transformed by the autoencoder's encoder into compact latent representations. The third stage is a lossless data reduction component in the latent space. Inspired by autoencoders used for image compression <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> as discussed in Sect. 3.2, our latent representations are further quantized into discrete symbols. After that, an entropy encoding algorithm, e.g., Asymmetric Numeral Systems (ANS), is adopted to losslessly compress quantized latents into bitstreams for saving.</p><p>Analyses can be done in either latent space or the original physical space. Importance-driven latent space has a simpler structure, and therefore tasks like feature extraction can be easily performed in this space. When the precise visualization of the dataset is needed, latent representations can be decoded back into the physical space for various visualization tasks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">METHOD</head><p>There are three main issues to address when generating domain-interestguided latent representations: (1) how to define a unified representation to incorporate various domain knowledge and avoid training multiple models when scientists vary their interests, (2) how to fuse domain knowledge during latent generation, and (3) how to control the latent size and make trade-offs between latent size and latent quality. In this section, we discuss how we address these issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Importance Driven Latent Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Domain Interest Representation</head><p>This section discusses how to represent diverse domain interests with a unified representation. Our latent generation method is domain-interestguided, meaning not all data are treated equally important during latent generation. This strategy is commonly used for many scientific data analysis applications. Given a massive amount of scientific data, scientists often try to identify features of interest by narrowing down their search space, which is defined by how data are relevant to the important features. Generating latent representations for scientific data according to the data importance can not only reduce the size of data, but also allow scientists to focus on the most salient portion of the data.</p><p>In our method, we utilize data importance to assist the process of latent generation. To create a unified representation of importance that can be taken by our autoencoder for a variety of needs, we summarize commonly used importance definitions in scientific visualization literature as below:</p><p>• Location-based: Scientists assign an importance value for every spatial location based on whether it is in the pre-selected region of interest <ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref>. For example, hurricane eye regions are of high interest for hurricane and tropical cyclone research scientists, and as a result, those regions will have higher importance values than non-hurricane eye regions <ref type="bibr" target="#b32">[33]</ref>. • Distance-based: To enhance the understanding of features of interest, scientists define the importance of each data element based on its distance to the feature of interest. For example, based on the distance to object of interest <ref type="bibr" target="#b32">[33]</ref> or salient isosurfaces <ref type="bibr" target="#b35">[36]</ref>. • Value-based: Scientists define the importance based on the differences between a pre-selected reference value and data values or based on transfer functions <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b39">40]</ref>. If the value of a data element is close to the values of interest, high importance will be assigned. We note that here the value can take a variety of forms: scalars, vectors, and tensors, to name a few. properties of data such as conditional entropy <ref type="bibr" target="#b34">[35]</ref>, correlation, or value histogram <ref type="bibr" target="#b6">[7]</ref>.</p><p>In the core of our method, we define a unified representation, i.e., a real-valued spatial importance map, to incorporate various importance definitions. The map is defined in the same domain as the data, and each value in the importance map indicates the scientific interest at that spatial location. Importance maps are taken as an additional input to the neural network model to control the latent representation generation.</p><p>There are two obvious advantages in using spatial importance maps. First, spatial importance maps can inform the neural network which regions are more important so that their information needs to be better preserved in the latent space, and for those regions with low importance, their latents can be simplified or smoothed out during encoding. Second, because we are using a unified representation for various domain interests and our latent generation is conditioned on the input importance map, as a result, we do not need to retrain different neural network models when scientists change their definition of spatial importance. In our paper, the importance value at every spatial location is calculated through a scientist-specified importance mapping function Ψ, mathematically defined as:</p><formula xml:id="formula_6">I p := Ψ(p, F(p))<label>(3)</label></formula><p>where Ψ : R 3 → R is a mapping from spatial location p ∈ R 3 to an importance value I p given the location and its data value F(p). F(p) ∈ R if it is a univariate data, and F(p) ∈ R n if it is a multivariate data with n variables. We evaluate I p on all voxel locations to obtain an importance map I.</p><p>During training, we randomly generate importance maps with different spatial variations such as distance ramps, Gaussian distributions with various centers, data gradients, and random uniform maps. During testing, the trained model is applied to various importance maps derived from different scientific interests. In our evaluation in Sect. 6, we demonstrate that these predefined importance maps are effective to train a generalized model which does not constrain a scientist's importance map specification during testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Autoencoder with Condition Network</head><p>To generate reduced data representations, we utilize a convolutional autoencoder model which converts input data into a latent representation through an encoder and decodes the latent back to data through a decoder. We utilize autoencoder for the reason that, as also discussed in Sect. 3.1, compared with linear coding methods such as discrete cosine transform (DCT), the non-linear coding ability of autoencoders makes them suitable and powerful to represent data.</p><p>To properly fuse domain knowledge into latent representations, we utilize Spatial Feature Transform (SFT) layers which are widely used in computer vision for image super-resolution <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b38">39]</ref>, conditional generation <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref>, compression <ref type="bibr" target="#b29">[30]</ref>, and segmentation <ref type="bibr" target="#b23">[24]</ref>. In these works, SFT layers are used to incorporate conditional knowledge by generating affine transformation parameters for feature modulation. We adopt a similar method as Song et al. <ref type="bibr" target="#b29">[30]</ref> which performs image compression given a classification or text-preserving task.</p><p>In our work, we utilize SFT layers to fuse domain knowledge into latent representations. The reason for using SFT layers is that they can capture rich spatial prior information from prior knowledge, e.g., regions of interest, to modify the intermediate feature maps of the data in the autoencoder. For example, smoothing out details in regions where scientists have low interest. More specifically, we adopt the autoencoder model by connecting it with two SFT-layer-based feature transformation networks. The transformation network connected to the encoder is shown in Fig. <ref type="figure">4</ref> who takes domain interest (i.e., a spatial importance map I) as input to extract conditions of different resolutions to have a layer-by-layer control of the encoder. Each condition Ω produces two sets of affine transformation parameters (i.e., α for scaling and β for shifting) for each encoder layer, formulated as:</p><formula xml:id="formula_7">Ω = conv(I) (4) Φ(Ω) = (α, β )<label>(5)</label></formula><p>where conv are the convolutional layers. Φ is a mapping function from condition Ω to the scaling parameter α and the shifting parameter β . α and β are used to transform the intermediate feature map F generated by the autoencoder:</p><formula xml:id="formula_8">F = SFT(F|α, β ) = F α + β (6)</formula><p>Then, the transformed feature map F is taken as input to the next encoding layer. α and β are of the same size as the feature map F, and denotes element-wise multiplication. Applying multiplication and addition on feature maps is a simple and effective way to gradually fuse two sources of information (i.e., importance and data) from different levels. Scaling the feature map is like gating so that information in regions with high importance are preserved and others are suppressed. Shifting the feature map has a similar effect. Combining these two, we have the flexibility to leverage importance maps for latent generation. Fig. <ref type="figure">4</ref>. Spatial Feature Transform (SFT) <ref type="bibr" target="#b38">[39]</ref> layers take the condition Ω generated from the importance map as input and output affine transformation parameters to scale (α) and shift (β ) feature map F of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Entropy Encoding in Latent Space</head><p>To optimize the size of the latent representation, we apply lossless entropy encoding to the latent vectors generated from the autoencoder.</p><p>We adopt the entropy model widely used for neural-network-based image compression <ref type="bibr" target="#b3">[4]</ref>, as discussed in Sect. 3.2.</p><p>Our proposed importance-driven latent generation method is shown in Fig. <ref type="figure" target="#fig_4">5</ref>. After layers of non-linear coding using autoencoder's encoder f , the input data x is converted into a latent representation y, controlled by a scientist-specified input importance map I, so we have:</p><formula xml:id="formula_9">y = f (x, T f (I))<label>(7)</label></formula><p>where T f is the transformation network connected to the encoder f . Following the technique in Sect. 3.2, we quantize the latent vector y and apply the entropy encoding algorithm, Asymmetric Numeral Systems (ANS) <ref type="bibr" target="#b12">[13]</ref>, on latent vectors. The resulting bitstreams are saved into the disk. During decoding, the saved bitstreams are entropy decoded into discrete latent ỹ and sent to the decoder to get the reconstruction x. As shown in Fig. <ref type="figure" target="#fig_4">5</ref>, to save storage and mainly reduce unimportant information in the latent representation, we do not use importance maps to modulate information during decoding. The decoder only takes latent ỹ as input to the transformation network connected to the decoder to incorporate conditions during reconstruction, formulated as:</p><formula xml:id="formula_10">x = g( ỹ, T g ( ỹ))<label>(8)</label></formula><p>where T g is the transformation network connected to the decoder g. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Loss Functions</head><p>To make trade-offs between size and based on domain interests, we formulate our importance-driven latent generation as an optimization problem of minimizing the rate-distortion Lagrangian as discussed in Sect. 3.2, formulated as:</p><formula xml:id="formula_11">L = L R + λ L D (<label>9</label></formula><formula xml:id="formula_12">)</formula><p>where L R is the quantized latent entropy loss, L D is the reconstruction loss, and λ is the Lagrangian multiplier, a hyperparameter to balance between reconstruction quality and latent size.</p><p>To force the generated latent representation to be guided by scientists' spatial interests, instead of a reconstruction loss with equal importance on every spatial location like a basic autoencoder, we use an adaptive reconstruction loss between the input data x and reconstructed data x:</p><formula xml:id="formula_13">L D = E x,Δy [ N ∑ i=1 w i (x i − xi ) 2 ] (<label>10</label></formula><formula xml:id="formula_14">)</formula><p>where w i is the distortion weight at index i derived from the importance map I by an exponential function with a hyperparameter a, i.e., w i = e aIi . The goal of exponential weighting is to have a finer distortion control across different spatial locations such that different importance values will have significantly different contributions to L D . N is the number of data elements in x. Δy is the additive uniform noise to relax quantization.</p><p>The entropy loss for discrete latent ỹ is:</p><formula xml:id="formula_15">L R = E [− log 2 p ỹ( ỹ)]<label>(11)</label></formula><p>where p ỹ is the latent probability distribution for entropy coding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Latent Space Analysis</head><p>In this section, we discuss how we use block-wise latent representation for feature-related analysis and briefly introduce the visual exploration tool for latent spaces. Feature-related analysis in the latent space originated from the observation that similarity defined in the latent space can better represent the similarity of higher-level features than the similarity of the raw data <ref type="bibr" target="#b4">[5]</ref>. Our latent vector generation method considers domain knowledge provided as an importance map to neural networks. As a result, latent space distribution is conditioned on the provided importance map. Even though the structure of the full latent space of the dataset is complicated and requires a sophisticated tool <ref type="bibr" target="#b24">[25]</ref> to explore, importance-driven latent space is easier to understand and to be made into use for feature analysis. On the saved latent representations for each data block, we perform a hierarchical clustering algorithm to identify blocks that are similar to each other. These clusters can be either used to extract a subset of data for further analysis or for feature-driven visualization like the one presented by Cheng et al. <ref type="bibr" target="#b10">[11]</ref>. It is worth noting that the data block size is a hyperparameter we should choose based on the feature size, data complexity, and desired representation storage size.</p><p>Our latent space visual analysis tool is based on the one designed by Li and Shen <ref type="bibr" target="#b24">[25]</ref>, where clustering is performed on block-wise latent vectors and cluster results are visualized in latent, and physical space for feature-related analysis. There are three main views in our tool, hierarchical clustering view, latent space view, and physical space view, demonstrated in Fig. <ref type="figure" target="#fig_9">9</ref>. Hierarchical clustering view presents each cluster as a node in a tree graph, where clusters can be modified by interacting with the nodes. Latent space view shows projected latent vectors into 2D using t-distributed stochastic neighbor embedding (t-SNE) projection <ref type="bibr" target="#b30">[31]</ref>. This view is updated when the clustering result is modified. Finally, physical space view visualizes data in the selected cluster. Spectral clustering is used in our latent analysis approach since it adapts well to complex spaces with unknown cluster shapes <ref type="bibr" target="#b28">[29]</ref>, which is usually the case for latent spaces generated by neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Implementation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1">Block-based Processing</head><p>Processing large-scale scientific data requires a big convolutional neural network model which has high computational cost and memory consumption. Another problem is that we need a large collection of data for training, but building such training data is prohibitive for scientific simulations due to the high cost of generating and saving large-scale data. To meet GPU memory constraints, some prepossessing steps such as downsampling or cropping <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> are applied to data. However, the drawback of downsampling is that it inevitably introduces errors and uncertainties in the downsampled data.</p><p>To address the above problems, we adopt a block-based processing strategy, i.e., volumetric data are divided into blocks for the neural network model to process. Data blocks can be processed in parallel with a large batch size for speedup. To reduce the reconstruction error at the block boundary introduced by zero-padding or reflection-padding, we pad each block with the actual data for the network to process. During reconstruction, we crop the reconstructed data, and only the central data regions are attached to reconstruct the whole volume. For instance, if the block size is 24 3 and we have a padding size equal to 4, then the data size each latent represents is 16 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">Training Data Sampling</head><p>The most intuitive way to build the training data is to randomly sample large amounts of data blocks to ensure a good coverage of different patterns for training. However, this data size will grow proportionally to the size of the original data and make the training extremely ineffective. To solve this and to force the model to learn complex patterns, we adopt a complexity-aware training data sampling strategy, i.e., the training dataset is designed to include more complex (high entropy) blocks and less homogeneous (low entropy) blocks. For the Hurricane Isabel dataset, the sampling ratio between high and low entropy blocks is 10:1 to ensure the complex data regions are covered in the training data given that a large portion of the original data is homogeneous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RESULTS</head><p>In this section, we evaluate the Importance-Driven Latent generation method (IDLat) both quantitatively and qualitatively from four different perspectives: (1) the quality of latent representations; (2) the size of latent representations; (3) the influence of different important maps; and (4) the use of latent representations for latent space analysis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Dataset and Training Parameters</head><p>We evaluated our importance-driven latent generation method using three scientific datasets for multiple scientific applications.</p><p>Vortex is a simulation of vortex structures with spatial resolution 128×128×128 across 30 time steps. We used the vorticity magnitude scalar field for experiments. We randomly sampled 1000 data blocks from 5 time steps as the training data. Nyx is a cosmological simulation produced by Lawrence Berkeley National Laboratory. We used the log density field with resolution 256×256×256. 5000 data blocks from 5 ensemble members were randomly sampled for training. Hurricane Isabel is a simulation of Hurricane Isabel, produced by the Weather Research and Forecast (WRF) model, courtesy of NCAR and the U.S. National Science Foundation (NSF). The were sliced along the z dimension to remove the special value 1 × 10 35 representing "no data" (the land region). In our experiment, the resolution of data is 512×512×96 with 48 time steps. We chose the pressure field for evaluation. Training data contain 5500 data blocks from 5 time steps.</p><p>Our work consists of two main components: the IDLat model and a latent space visual analysis tool. The IDLat model is implemented based on PyTorch<ref type="foot" target="#foot_0">1</ref> and trained with a single NVIDIA Tesla P100 GPU. For all datasets, we use Adam optimizer <ref type="bibr" target="#b22">[23]</ref>. The learning rate for the autoencoder model and the entropy model is 10 −<ref type="foot" target="#foot_3">4</ref> and 10 −<ref type="foot" target="#foot_2">3</ref> , respectively. Total training time for each dataset is listed in Table <ref type="table" target="#tab_2">1</ref>. Based on a fully convolutional model with block-based training and inference strategy, we can apply IDLat on data of any resolution. The hierarchical clustering view and latent space projection view from the latent space exploration tool are implemented with Vue.js<ref type="foot" target="#foot_1">2</ref> as the frontend framework and Flask 3 as the back-end framework. VTK APIs 4 are used to visualize the extracted blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Quantitative Evaluation</head><p>In this section, we quantitatively evaluate the size and the quality of latent representations generated by IDLat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Evaluation Metric</head><p>To evaluate the size of latents, we use the ratio between the original data size and the saved bitstream file size, i.e., latent size ratio (LSR):</p><formula xml:id="formula_16">LSR =</formula><p>original data size bitstream file size <ref type="bibr" target="#b11">(12)</ref> To evaluate the quality of importance-driven latent representations, we analyze how well the important regions are preserved during reconstruction under various importance maps. We compute the error between the decoder's reconstruction and the raw data. Because we focus more on the quality of important regions, we utilize a weighted Mean Squared Error (wMSE) defined as:</p><formula xml:id="formula_17">wMSE(x, x) = 1 ∑ N i=1 I i N ∑ i=1 I i (x i − xi ) 2 (<label>13</label></formula><formula xml:id="formula_18">)</formula><p>where x i , xi are the original and the reconstructed data at position i, respectively. N is the number of data elements in x. I i is the importance at position i in range [0, 1] defined by Equation 3 for different applications. Locations with a larger importance value will have higher weights in the error estimate. The peak signal-to-noise ratio (PSNR) is defined based on wMSE: PSNR(x, x) = 10 log 10 v 2 wMSE(x, x) <ref type="bibr" target="#b13">(14)</ref> where v denotes the value range in the original data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Evaluation on Different Importance Maps</head><p>As discussed in Sect. 5.1.1, there are various criteria focusing on representing regions of interest for importance-driven visualization. In this section, we evaluate IDLat's quality and quantity under different importance definitions.</p><p>To evaluate the effectiveness of the entropy encoding module in IDLat, especially its ability to generate the latent representation of optimal size with the presence of an importance map, we compare IDLat with a baseline method, i.e., a basic autoencoder without the importance map, quantization, entropy module, and the entropy loss. We train this baseline model with the same training data and parameter setting as IDLat, but only with the reconstruction loss.</p><p>Distance-based importance maps: We evaluate importance-driven latent's quality and quantity conditioned on distance-based importance maps through the Vortex dataset. Vortex data contains vortex structures that have been widely used for isosurface tracking to analyze vortex core regions over time <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. In this evaluation, the importance maps are defined based on distances to the selected isosurfaces where the importance value I p for each spatial location p defined in Equation 3 can be specified as:</p><formula xml:id="formula_19">I p := Ψ Vor (p) = e −0.2|SDF(p,S)| (<label>15</label></formula><formula xml:id="formula_20">)</formula><p>SDF(p, S) represents the signed distance from location p to surface S. We use a negative exponential function to convert absolute SDF distances into importance values in [0, 1] for the model to process. The importance will decrease exponentially as p becomes far from the surface. 0.2 is a parameter that we choose to have a proper slope of the exponential curve. We can also use other functions to convert distances into weights, e.g., the inverse distance function.</p><p>To evaluate the influence of different distance-based important maps, we chose a time step in Vortex data and pre-selected several vorticity magnitude values (e.g., 5.0, 6.0, 7.0, and 8.0) as salient isovalues to be preserved in the latent representations. We used Ψ Vor in Equation 15 to convert the volumetric data into importance maps for IDLat to generate importance-driven latent representations.</p><p>We report the quantitative results, i.e., PSNR and latent size ratio (LSR), of applying different distance-based importance maps on Vortex data in Table <ref type="table" target="#tab_3">2</ref>. From the table, we found that IDLat indeed can generate importance-driven latent representations for Vortex. Given the same data but different importance maps, IDLat generates latents with different quality and size, as shown in the PSNR (IDLat) and LSR (IDLat) columns. As for the baseline model, because the latent size is determined by the input data size, the latent size ratio for the baseline model is fixed at a very small number (7.3143) as shown in LSR (Base). It is clear in the table that the proposed IDLat can largely decrease the latent size (i.e., increase LSR) based on different importance maps without losing much quality. We also found that IDLat can achieve increased latent size ratios as we increase the target isovalue. This is because IDLat is designed to have higher quality on regions with high interests, and as the isovalue increase, the important regions are getting smaller for this dataset, and as a result, LSR is getting larger.</p><p>Value-based importance maps: Given value-based importance maps, we evaluate the size and quality of latents generated by IDLat on Nyx data. One important post-hoc analysis task for Nyx simulation is to find dark matter halos which are related to the high-density field in the data <ref type="bibr" target="#b14">[15]</ref>. In our experiment, the importance value I p of each spatial location p is defined based on the log density value F(p) and a reference log density value F re f as follows: </p><formula xml:id="formula_21">I p := Ψ Nyx (p, F(p)) = 1 if F(p) &gt; F re f 0 else<label>(16)</label></formula><p>To evaluate the effect of different value-based importance maps, driven by the domain interests of the Nyx data discussed above, we conduct experiments on two ensemble members of Nyx with different reference values. In our experiment, we select 9.9, 10.2, and 10.5 as log density reference values and utilize Ψ Nyx defined in Equation <ref type="formula" target="#formula_21">16</ref>to compute value-based importance maps.</p><p>We report the quantitative results in the second and the third blocks of rows of Table <ref type="table" target="#tab_3">2</ref>. The two blocks of rows represent experiments on two different ensemble members, denoted as Nyx (m1) and Nyx (m2). As we can see, for each ensemble member, the latent size ratio (LSR (IDLat)) increases as we increase the log density reference value (i.e., reduce the number of important voxels), but the quality (PSNR (IDLat)) is kept comparable to the baseline (PSNR (Base)). Another thing we notice is that PSNR drops as we increase the log density reference value for both baseline and IDLat models. A possible reason can be that these high-value regions are harder to model due to high data complexity.</p><p>Location-based importance maps: We also evaluate the latent representations' size and quality given location-based importance maps using the Isabel dataset. For this dataset, one task that scientists are interested in is to identify and analyze the hurricane eye region. So the importance maps are built based on the hurricane eye locations where voxels inside the interested region C will have high importance values. The importance value for each spatial location p is defined as:</p><formula xml:id="formula_22">I p := Ψ Isa (p) = 1 if p ∈ C 0 else<label>(17)</label></formula><p>We use the first time step of Isabel and draw a bounding box of the hurricane eye as the region of interest C. Based on the importance mapping function in Equation <ref type="formula" target="#formula_22">17</ref>, we compute the importance map.</p><p>Table <ref type="table" target="#tab_3">2</ref> shows the quantitative results of Isabel data given a locationbased importance map using IDLat and the baseline model. Compared with the baseline, the latent representation generated by IDLat with a location-based importance map is more compact (higher LSR) with slightly higher quality (higher PSNR) than the baseline.</p><p>Essentially, the baseline model is a special case of our IDLat with λ → ∞ in Equation <ref type="formula" target="#formula_11">9</ref>, which achieves an upper bound for reconstruction error and a lower bound for the latent size ratio. Instead of a static model with a fixed latent size, the proposed IDLat can achieve various quality and latent vector sizes based on target scientific applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Qualitative Evaluation</head><p>We qualitatively evaluate the proposed IDLat by visualizing the reconstructions with volume and isosurface rendering.</p><p>In Fig. <ref type="figure" target="#fig_5">6</ref>, we show isosurface rendering of the reconstructed Vortex data generated by the baseline and by IDLat with isosurface-distancebased importance maps. In Fig. <ref type="figure" target="#fig_5">6</ref>, in each row from left to right are isosurface rendering for isovalue 5, 6, 7, and 8. Comparing isosurface rendering of IDLat's reconstruction (third row) with the ground truth (first row) and the baseline (second row), we found that our latent representations can capture the structure and also the details in the ground truth, even though they are much smaller in size as discussed in Sect. 6.2. In some regions, IDLat can have slightly better reconstruction quality compared to the baseline, as shown in the zoom-in regions in Fig. <ref type="figure" target="#fig_5">6</ref>. These results demonstrate that our importance-driven latent representations can capture spatial importance information and have high reconstruction quality in important regions. The reference values are 9.9, 10.2, and 10.5, respectively. In the first two rows, each row shows the result for one ensemble member. We use the same transfer function for each row to ensure the volume rendering difference is caused by the reconstructed data but not the transfer function difference. The transfer function for volume rendering of the reconstructed and ground truth data is the vertical colorbar and for difference map is the horizontal colorbar. From volume rendering images, we found the latent representation has high reconstruction quality on every importance map. From left to right, we can see the differences in the difference map spreads out more when we increase the reference value, which matches the observation that as the reference value increases, the important regions in the dataset are getting smaller and unimportant regions are enlarged so that the difference at unimportant locations becomes more obvious. The third row of Fig. <ref type="figure" target="#fig_6">7</ref> shows zoom-in of ground truth (A), baseline (B) and IDLat's reconstruction (C and D). As shown in the figure, when the reference value increases from 9.9 in Fig. <ref type="figure" target="#fig_6">7</ref> (C) to 10.5 in Fig. <ref type="figure" target="#fig_6">7</ref> (D), the reconstruction loses more details of the unimportant data compared to the ground truth and the baseline, e.g., the red dashed circled regions are more smoothed out in Fig. <ref type="figure" target="#fig_7">7 (D</ref>). The zoom-in of difference maps also shows more obvious spread-out differences in the unimportant regions. The pattern of difference maps and high-quality volume rendering results demonstrate that IDLat is under the guidance of spatial importance when generating latent representations.</p><p>Fig. <ref type="figure" target="#fig_8">8</ref> shows volume rendering images of Isabel dataset. From left to right, they are ground truth, baseline's, and IDLat's reconstruction. IDLat utilizes a location-based importance map where the hurricane eye is the region of interest, as shown in the selected bounding box. Comparing the ground truth and baseline with IDLat's reconstruction, we can see the quality of the hurricane eye is highly preserved, although IDLat has a smaller latent size as discussed in Sect. 6.2.</p><p>The above quantitative and qualitative results validate that IDLat is under spatial importance guidance when generating compact latent representations with high quality in important spatial regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Latent Space Exploration and Analysis</head><p>As discussed in Sect. 5.4, our method produces latent representations that can be used for feature-related exploration and analysis. To show that the proposed importance-driven latent representations are succinct and suitable for representing features of interest, we perform latent space exploration and analysis on Vortex data with two case studies.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.1">Case Study 1: Obtain Insight of Features in Latent Space</head><p>In the first case study, we show the effectiveness of importance-driven latents in identifying features by exploring the latent space. We compare block-wise latent representations generated by a uniform importance map and by a distance-based importance map in Sect. 6.2.2.</p><p>First, we show the exploration results for the importance-driven latent space. The importance map is defined based on distances to the isosurface with isovalue = 8, which reveals the interest features of the vortex cores. In Fig. <ref type="figure" target="#fig_9">9</ref>, we can easily identify four distinct clusters in the t-SNE projection of the latent vectors. The users can also modify the hierarchical clustering view to investigate the detail related to each cluster. On the right side of Fig. <ref type="figure" target="#fig_9">9</ref>, we show the hierarchical clustering results of latent vectors. Cluster A consists of the blocks that contain the vortex cores and can be further separated into two child clusters (A1, A2) based on the separation in the t-SNE projection. The difference between cluster A1 and cluster A2 will be discussed later. Cluster C consists of blocks that do not intersect with the isosurface of interest. Cluster B contains some boundary blocks (e.g., blocks on the edges and in the corners).</p><p>To further investigate cluster A, we show images of volume rendering and isosurfaces for the ground truth data in Fig. <ref type="figure" target="#fig_9">9</ref> (GT) and the blocks from cluster A identified in latent space in Fig. <ref type="figure" target="#fig_9">9 (A)</ref>. It can be observed that Fig. <ref type="figure" target="#fig_9">9</ref> (A) and Fig. <ref type="figure" target="#fig_9">9</ref> (GT) reveal the same isosurfaces (ignoring the fuzzy region in the ground truth image, which is not part of the isosurface), indicating that our latent representations allow us to preserve the important regions with good quality and in the meantime enable us to visualize and separate the features easily in the latent space.</p><p>In the t-SNE projection, cluster A consists of two child clusters A1 and A2. We show the isosurfaces corresponding to these two clusters in Fig. <ref type="figure" target="#fig_9">9 (A1, A2</ref>). Cluster A1's isosurface is in red and cluster A2 in green. We found that except for two boundary vortices (a1, a2), all other vortices are split into two clusters (red and green). The splitting reveals the internal structures of the vortices. Since the splitting happens along one axis, we suspect one possible reason for this is the differences of the scalar values in the block along this direction. For this dataset, in the core of vortices, it has high scalar values and the value is decreasing and the isosurface is getting enlarged from inside vortex core to outside. They are classified into different clusters probably due to the opposite direction of value decreasing on this axis, which is related to the gradient of the values. To validate this hypothesis, we calculate the average gradient distribution along the x-axis using Gaussian kernel density estimation for each data block from these two clusters as shown in Fig. <ref type="figure" target="#fig_10">10</ref>, where we can identify the apparent gradient distribution difference among these two clusters. The further separation of the feature clusters helps visualize and understand the internal structures of the data of interest.</p><p>We also perform latent space exploration for latent vectors generated using a uniform importance map. The t-SNE projection of their latent space is shown on the left side of Fig. <ref type="figure" target="#fig_11">11</ref>. We did not find any visual clusters in the t-SNE projection, and the clustering of the latent vectors splits all blocks into clusters of high and low average values, which is not helpful in feature-related analysis. The process of further splitting of the clusters is tedious and did not bring us anything interesting.</p><p>By comparing the structures of importance-driven and uniform latent spaces, we found that the importance-driven latent space is highly related to the features of interest and is easier to explore. From the clustering result, scientists can reduce the effort of similarity comparison between blocks by quickly filtering out unimportant regions, resulting in fast and scalable data analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.2">Case Study 2: Representative Isosurface Selection</head><p>In the second case study, we show the usefulness of the importancedriven latent for representative isosurface selection.  To quantify and visualize similarities between isosurfaces, Bruckner and Möller <ref type="bibr" target="#b7">[8]</ref> proposed the isosurface similarity map. Each element in this map is the similarity value between two isosurfaces. They use distance fields, i.e., the minimal distance of each point to the surface, to represent isosurfaces and use mutual information between two distance fields as the similarity measure. However, one limitation of this method is the high computation cost. First, representing isosurfaces as distance fields is expensive without acceleration such as approximations. Second, generating the isosurface similarity map needs to calculate mutual information between every pair of isosurfaces, which requires building a joint histogram of every two distance fields. A more effective surface representation and efficient similarity computation is desired.</p><p>To solve this, we utilize IDLat with value-based importance maps to generate isosurface representations. We use value-based importance maps for two reasons. First, the spatial information of each voxel inside each block is encoded in the latent representation, so we do not need implicit distance fields to indicate surface locations. Second, when generating importance-driven latent representations, voxels with higher importance values will have a higher contribution, and voxels with low importance will be suppressed, which helps encode the surface information and zero out non-surface information. These two properties eliminate the heavy computation of distance fields. After we have latent representations for all blocks, we concatenate them into a single latent to represent the whole isosurface. We note that compared to the encoding time reported in Table <ref type="table" target="#tab_2">1</ref>, we have 8 times more blocks due to smaller block size (8 3 instead of 16 3 as in Table <ref type="table" target="#tab_2">1</ref>), so representation generation is about 8 times slower. By changing the value-based importance maps, we can generate compact isosurface representations use different isovalues as the importance measure. Then, isosurface similarities are efficiently computed through cosine similarities between the isosurface-drive latent representations.</p><p>In the isosurface similarity map, we can find clusters of isovalues to select representative isovalues. Given surface similarities, we use the same isosurface selection algorithm as Bruckner and Möller's <ref type="bibr" target="#b7">[8]</ref> to automatically identify representative isovalues. Fig. <ref type="figure" target="#fig_12">12</ref> shows isosurface similarity maps and selected isosurfaces computed by Bruckner and Möller's method <ref type="bibr" target="#b7">[8]</ref> and by ours. Compared to Bruckner and Möller's, our method can generate better results of the top four representative isosurfaces to reveal the structure of Vortex data. The selected isosurface (number 3) in our result is missing in theirs. They may identify it later but need to increase the number of selections. In Table <ref type="table">3</ref>, we show the performance of these two methods. Compared to Bruckner and Möller's <ref type="bibr" target="#b7">[8]</ref>, our method is much more efficient in both representation generation stage (rep) and similarity computation stage (sim).</p><p>Table <ref type="table">3</ref>. Selected isovalues, time (seconds) for all isosurface representation generation and for computing the isosurface similarity map using Bruckner and M öller's <ref type="bibr" target="#b7">[8]</ref> and importance-driven latent representations.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION AND FUTURE WORK</head><p>Even though we have demonstrated that the proposed IDLat can generate latent representations of compact size and correspond well to the importance definition, there are still several limitations to our work. First, the generalizability of our method has not been fully investigated. Our evaluation results demonstrate that a model trained using data blocks from several time steps can generalize well to other time steps and is sensitive to different importance maps. However, to what extent our model can generalize is not fully understood.</p><p>Secondly, in the practical use of our method, the importance definition and dataset itself may not always be in the same resolution. For example, in our isosurface-based importance definition, importance is continuously defined, while scalar data are only defined in the grid points, which forces us to sample the importance field to match the data resolution. How we interpolate and sample the importance field or the dataset can largely influence the latent representation quality.</p><p>Finally, the full potential of latent representation for scientific data analysis has not been extensively studied in this work. For example, the usage of importance-driven latent vectors on time-varying data analysis and feature tracking is one of our future studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>In this paper, we present an importance-driven latent generation method (IDLat) based on an autoencoder model which tightly relates latent representations to specific data of interest, such as salient regions or features of interest. We represent data of interest by spatial importance maps and utilize the location-wise importance information to guide latent generation. With a trained model, scientists can flexibly define various importance criteria and obtain different latent representations. We further reduce the latent size through a lossless entropy coding model. In addition, we develop a visual exploration tool for latent space analysis and demonstrate the efficiency of identifying and analyzing feature regions with importance-driven latent representations. Through quantitative and qualitative evaluations, we validate the effectiveness of our importance-driven latent generation method in representing data under domain interests control.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The probability estimation for every latent channel (left), and probability estimation for every latent dimension (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Our proposed method to generate importance-driven latent representations conditioned on the input importance maps. The resulting latent representation will be entropy encoded into bitstreams for saving.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Our proposed importance-driven latent generation method. Our model is a combination of an autoencoder and entropy encoding model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Comparison of isosurface rendering results of Vortex at time step 6 between ground truth (top row), baseline (middle row) and IDLat's reconstruction (bottom row). The four importance maps for IDLat are defined based on distances to isosurfaces 5, 6, 7 and 8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7</head><label>7</label><figDesc>displays volume rendering images of ground truth (first column), baseline, and IDLat's reconstruction based on value-based importance maps for two ensemble members of the Nyx dataset. The second and the third columns are volume rendering images of the baseline's reconstruction and difference maps between the original and the reconstructed data. Other columns are IDLat's results based on three different value-based importance maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Volume rendering of ground truth, baseline and IDLat's reconstructions on two ensemble members of Nyx data with value-based importance maps.Reference values are 9.9, 10.2 and 10.5. The difference map for each reconstruction is shown on its right. From left to right, in the difference map, the difference spreads out more when we increase the reference value, which matches the fact that as the reference value increases, the important regions are getting smaller and unimportant regions are enlarged so that the difference at unimportant regions becomes more obvious.</figDesc><graphic url="image-59.png" coords="8,138.71,139.25,51.57,52.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Volume rendering of ground truth, baseline and IDLat's reconstruction on Isabel data with a location-based importance map where the hurricane eye in the bounding box is the region of interest.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Latent space projection and hierarchical clustering results given importance map based on distances to isosurface 8.0. Volume rendering and isosurface (isovalue = 8) for the ground truth data (GT), for blocks from cluster A (A) and for child clusters of cluster A (bottom left), where isosurfaces are shown in red for cluster A1 and green for cluster A2.</figDesc><graphic url="image-88.png" coords="8,71.27,364.49,225.02,132.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Gradient distribution for data blocks from two clusters.</figDesc><graphic url="image-89.png" coords="9,93.83,73.25,182.30,60.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Latent exploration of latent vectors generated using uniform importance map. No visual clusters are identified in the t-SNE projection.</figDesc><graphic url="image-90.png" coords="9,118.31,146.69,125.66,76.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Bruckner and M öller's [8] isosurface similarity map (left) and four isosurfaces (right). Bottom Row: Our results of of isosurface similarity map (left) and four selected isosurfaces (right).</figDesc><graphic url="image-91.png" coords="9,340.91,154.37,107.34,85.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>• Time-based: To effectively visualize and analyze large-scale time-varying data, scientists assign different importance values for time steps. For example, based on the relative information a time step contains about its temporal neighbours [35] or assign salient time steps with high importance values. • Multivariate-based: For a multivariate dataset, the importance can be derived from the joint distribution of variables [16]. For instance, to locate interesting regions for turbulent combustion data, multiple variables such as Mixture Fraction (MIX), Mass Fraction of the Hydroxyl Radical (OH), and Heat-Release Rate (HR) are jointly considered. • Statistical-based: Importance can be defined based on statistical</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Dataset name, variable name, data resolution, training epochs and time, training data size (number of blocks), encoding and decoding time on each volume (seconds). Encoding and decoding time do not induce the time used for writing/reading bitstreams files into/from disk.</figDesc><table><row><cell>Dataset</cell><cell>Variable</cell><cell>Size</cell><cell>Epochs</cell><cell>Training time</cell><cell># Training blocks</cell><cell>Enc. time</cell><cell>Dec. time</cell></row><row><cell>Vortex</cell><cell>vorticity magnitude</cell><cell>128×128×128</cell><cell>600</cell><cell>4h 20m</cell><cell>1000</cell><cell>0.0232s</cell><cell>0.0105s</cell></row><row><cell>Nyx</cell><cell>log density</cell><cell>256×256×256</cell><cell>100</cell><cell>3h 14m</cell><cell>5000</cell><cell>0.2272s</cell><cell>0.0794s</cell></row><row><cell>Isabel</cell><cell>pressure</cell><cell>512×512×96</cell><cell>200</cell><cell>5h 30m</cell><cell>5500</cell><cell>0.2000s</cell><cell>0.1191s</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Reconstruction PSNR and latent size ratio (LSR) for IDLat with different importance maps and for the baseline model.</figDesc><table><row><cell>Data</cell><cell>Imp. Def.</cell><cell>PSNR</cell><cell>PSNR</cell><cell>LSR</cell><cell>LSR</cell></row><row><cell></cell><cell></cell><cell>(IDLat)</cell><cell>(Base)</cell><cell>(IDLat)</cell><cell>(Base)</cell></row><row><cell></cell><cell>iso 5.0</cell><cell>34.3987</cell><cell>35.4927</cell><cell>107.3825</cell><cell></cell></row><row><cell>Vortex</cell><cell>iso 6.0 iso 7.0</cell><cell>33.9121 33.2551</cell><cell>35.2154 34.5266</cell><cell>109.8901 111.849</cell><cell></cell></row><row><cell></cell><cell>iso 8.0</cell><cell>33.9757</cell><cell>34.2463</cell><cell>113.6767</cell><cell></cell></row><row><cell></cell><cell>log den &gt; 9.9</cell><cell>33.8422</cell><cell>35.2382</cell><cell>209.4241</cell><cell></cell></row><row><cell>Nyx (m1)</cell><cell>log den &gt; 10.2</cell><cell>30.9899</cell><cell>31.0386</cell><cell>218.8783</cell><cell>7.3143</cell></row><row><cell></cell><cell>log den &gt; 10.5</cell><cell>28.2268</cell><cell>27.1545</cell><cell>225.6700</cell><cell></cell></row><row><cell></cell><cell>log den &gt; 9.9</cell><cell>31.3774</cell><cell>32.2409</cell><cell>172.5998</cell><cell></cell></row><row><cell>Nyx (m2)</cell><cell>log den &gt; 10.2</cell><cell>29.0695</cell><cell>29.2020</cell><cell>179.3722</cell><cell></cell></row><row><cell></cell><cell>log den &gt; 10.5</cell><cell>26.761</cell><cell>26.1568</cell><cell>188.2353</cell><cell></cell></row><row><cell>Isabel</cell><cell>hurricane eye</cell><cell>44.9749</cell><cell>44.6605</cell><cell>199.1288</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://pytorch.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://vuejs.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://flask.palletsprojects.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">https://vtk.org</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work is supported in part by US Department of Energy SciDAC program DE-SC0021360, National Science Foundation Division of Information and Intelligent Systems IIS-1955764, and National Science Foundation Office of Advanced Cyberinfrastructure OAC-2112606. This research was also supported by the Laboratory Directed Research and Development program of Los Alamos National Laboratory under project number 20200065DR (LA-UR-22-23024).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nonlinear Transform Coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ballé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="339" to="353" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-to-End Optimization of Nonlinear Transform Codes for Perceptual Quality</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ballé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Laparra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 Picture Coding Symposium (PCS)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Ballé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Laparra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01704</idno>
		<title level="m">End-to-End Optimized Image Compression</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Ballé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Johnston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01436</idno>
		<title level="m">Variational Image Compression With a Scale Hyperprior</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Representation Learning: A Review and New Perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Generative Model for Volume Rendering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1636" to="1650" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Probabilistic Data-Driven Sampling via Multi-Criteria Importance Analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Patchett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Calhoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ahrens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4439" to="4454" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Isosurface Similarity Maps</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bruckner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Möller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
				<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="773" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Importance-Driven Particle Techniques for Flow Visualization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kondratieva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Westermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE Pacific Visualization Symposium</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="71" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning Spatial Attention for Face Super-Resolution</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-Y</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2020.3043093</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1219" to="1231" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep-Learning-Assisted Volume Visualization</title>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cardone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Krokos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramaniam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Varshney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1378" to="1391" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">IsoExplorer: An Isosurface-Driven Framework for 3D Shape Analysis of Biomedical Volume Data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visualization</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1253" to="1266" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The Use of Asymmetric Numeral Systems as An Accurate Replacement for Huffman Coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tahboub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Gadgil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Delp</surname></persName>
		</author>
		<idno type="DOI">10.1109/PCS.2015.7170048</idno>
	</analytic>
	<monogr>
		<title level="m">2015 Picture Coding Symposium (PCS)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="65" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Feature-Relevant Data Reduction for In Situ Workflows</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Klasky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kurc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 4th International Workshop on Data Reduction for Big Scientific Data (DRBSD-4)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Friesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Almgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lukić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Morozov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Beckner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Day</surname></persName>
		</author>
		<title level="m">Situ and In-Transit Analysis of Cosmological Simulations. Computational Astrophysics and Cosmology</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An Application of Multivariate Statistical Analysis for Query-Driven Visualization</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Gosink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Garth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Bethel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Joy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="264" to="275" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Blind Super-Resolution with Iterative Kernel Correction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1604" to="1613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">FlowNet: A Deep Learning Framework for Clustering and Selection of Streamlines and Stream Surfaces</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2018.2880207</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1732" to="1744" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">V2V: A Deep Learning Approach to Variable-to-Variable Selection and Translation for Multivariate Time-Varying Data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2020.3030346</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1290" to="1300" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Feature Tracking using Earth Mover&apos;s Distance and Global Optimization</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific graphics</title>
				<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Volume Tracking Using Higher Dimensional Isosurfacing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wenger</surname></persName>
		</author>
		<idno type="DOI">10.1109/VISUAL.2003.1250374</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization</title>
				<imprint>
			<date type="published" when="2003">2003. 2003. 2003</date>
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep Fluids: A Generative Network for Parameterized Fluid Simulations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">C</forename><surname>Azevedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thuerey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Solenthaler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
				<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="59" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>ICLR 2015</idno>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
				<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">May 7-9, 2015. 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Segmenting Objects in Day and Night: Edge-Conditioned CNN for Thermal Image Semantic Segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2020.3009373</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3069" to="3082" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Local Latent Representation based on Geometric Convolution for Particle Data Feature Exploration</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Exploring Autoencoder-Based Error-Bounded Compression for Scientific Data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cappello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Cluster Computing (CLUSTER)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="294" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Importance-Driven Isosurface Decimation for Visualization of Large Simulation Data Based on OpenCL</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Yong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing in Science &amp; Engineering</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="24" to="32" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Deep Learning Approach to Selecting Representative Time Steps for Time-Varying Multivariate Data</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Von Ohlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/VISUAL.2019.8933759</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Visualization Conference (VIS)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Normalized Cuts and Image Segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Variable-Rate Deep Image Compression through Spatially-Adaptive Feature Transform</title>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
				<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2380" to="2389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visualizing Data using t-SNE</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">86</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Importance-Driven Focus of Attention</title>
		<author>
			<persName><forename type="first">I</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feixas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Groller</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2006.152</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="933" to="940" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Importance-Driven Feature Enhancement in Volume Visualization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kanitsar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Groller</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2005.62</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="408" to="418" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Importance-Driven Volume Rendering</title>
		<author>
			<persName><forename type="first">I</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kanitsar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Groller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE visualization</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="139" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Importance-Driven Time-Varying Data Visualization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2008.140</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1547" to="1554" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Application-Driven Compression for Visualizing Large-Scale Time-Varying Data</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="69" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Edvr: Video Restoration with Enhanced Deformable Convolutional Networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Towards Real-World Blind Face Restoration with Generative Facial Prior</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9168" to="9178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Recovering Realistic Texture in Image Super-Resolution by Deep Spatial Feature Transform</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Feature-Preserving Volume Data Reduction and Focus+Context Visualization</title>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2010.34</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="171" to="181" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">NeuralCubes: Deep Representations for Visual Data Exploration</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cashman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Scheidegger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Big Data (Big Data)</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="550" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Latent Space Physics: Towards Learning the Temporal Evolution of Fluid Flow</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wiewel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Becher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thuerey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer graphics forum</title>
				<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="71" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Latent Space Subdivision: Stable and Controllable Time Predictions for Fluid Flow</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wiewel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">C</forename><surname>Azevedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Solenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thuerey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
				<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="15" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A Multi-branch Decoder Network Approach to Adaptive Temporal Data Selection and Reconstruction for Big Scientific Simulation Data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Peterka</surname></persName>
		</author>
		<idno type="DOI">10.1109/TBDATA.2021.3092174</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
