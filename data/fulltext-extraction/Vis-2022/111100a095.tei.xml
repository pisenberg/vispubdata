<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Diverse Interaction Recommendation for Public Users Exploring Multi-view Visualization using Deep Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yixuan</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yusheng</forename><surname>Qi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Shi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Qing</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nan</forename><surname>Cao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Siming</forename><surname>Chen</surname></persName>
						</author>
						<title level="a" type="main">Diverse Interaction Recommendation for Public Users Exploring Multi-view Visualization using Deep Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-03-27T09:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Interaction Recommendation</term>
					<term>Visualization for public education</term>
					<term>Mixed-initiative Exploration</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Interaction is an important channel to offer users insights in interactive visualization systems. However, which interaction to operate and which part of data to explore are hard questions for public users facing a multi-view visualization for the first time. Making these decisions largely relies on professional experience and analytic abilities, which is a huge challenge for non-professionals. To solve the problem, we propose a method aiming to provide diverse, insightful, and real-time interaction recommendations for novice users. Building on the Long-Short Term Memory Model (LSTM) structure, our model captures users' interactions and visual states and encodes them in numerical vectors to make further recommendations. Through an illustrative example of a visualization system about Chinese poets in the museum scenario, the model is proven to be workable in systems with multi-views and multiple interaction types. A further user study demonstrates the method's capability to help public users conduct more insightful and diverse interactive explorations and gain more accurate data insights.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Visualization systems have been recently introduced in public cultural centers, museums, and websites, accessible via touch screens, computers, and mobile phones for public education, data journalism, and data storytelling. In the past ten years, museums and other types of public education scenarios have made considerable efforts to create large and advanced visualization systems for audiences to browse and explore <ref type="bibr" target="#b31">[32]</ref>. For example, the EMDialog visualization system <ref type="bibr" target="#b30">[31]</ref>, the Qichang Dong Artwork exhibition from Shanghai Museum <ref type="bibr" target="#b0">[1]</ref>, and the Living Liquid system in the Exploratorium <ref type="bibr" target="#b36">[37]</ref> are popular multiview visualization systems. They all show how interactive visualization systems can help public education by enhancing users' engagements in exhibitions and helping them learn through hands-on practice and free exploration. Visualization is a reliable tool to present socio-cultural, political, scientific, or other topics to public users for its ability to promote a deeper and broader understanding.</p><p>However, visualization systems constructed of multiple linked views and interactions types are often complicated. Public users without expertise in datasets, visualization systems and data analysis can hardly take full advantage of the system. As a result, public users might conduct limited exploration in a confined area or conduct random explorations wandering around the system. In either case, the exploration is unsatisfied and does not fully utilize the system's capabilities.</p><p>To solve this problem, various researchers focus on the interaction recommendation of visualization systems <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b40">41]</ref>. These works laid the foundation for personalized interaction recommendations and inspired our work. However, most of the current research is limited to single-view visualization systems or only consider public user's current step's interaction information to determine the following interaction recommendation, which is unable to support visualization systems with multiple views and interaction types.</p><p>This paper proposes a solution to the interaction recommendations for public-user-oriented interactive visualization systems with multiviews and multiple interaction types. Using deep learning methods, our model aims to recommend interactions for users' subsequent system exploration instantly. We model the interaction behaviors and visual states with the Long Short-Term Memory (LSTM) network. Interactions and visual states are modeled quantitatively and converted into numerical vectors as inputs to the deep learning model. The model evolves by capturing users' actual interactions to fine-tune the deeplearning model. To evaluate our method's performance, we create an interactive visualization system about Chinese poets' biographical data in a museum context. With the collected interaction and visual-states logs, we train the model for prediction and then conduct a user study for evaluation. Our results show that the model works effectively in visualization systems with multi-views and multiple interaction types. Further analysis shows the model's capability to help public users conduct more insightful interactions and facilitate diverse exploration styles with the help of data filters and multiple LSTM models.</p><p>Altogether, we made the following contributions.</p><p>• Numerical Modeling of Interactions: We propose a method that models interaction logs from different views and interaction types into vectors that share the same structure. Visual states in different views are also mapped into vectors with similar methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Interaction Recommendation Model in Visualization Systems:</head><p>To the best of our knowledge, we are the first to propose a mixedinitiative approach for across-view and across-interaction type interaction recommendations based on users' historical interaction logs and the corresponding visual state in visualization systems. This method can provide real-time, diverse, and insightful interaction recommendations, thus improving the system's usability. • An Application Scenario: Our approach is proven to effectively solve the pains faced by public users-oriented interactive systems in public education scenarios such as museums. This model enables public users to gain more knowledge in these scenarios, thereby improving the accessibility of visualization and enhancing the education capabilities of public education institutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Related work is discussed from the following aspects: interaction log analysis in visual analysis, provenance meta-analysis, and interaction recommendations for data exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Interaction Log Analysis in Visual Analysis</head><p>Interaction occurs at all stages of the visualization process, transforming raw data into graphical insights <ref type="bibr" target="#b21">[22]</ref>. As the most critical element in visual analysis and exploration, it determines users' overall experience <ref type="bibr" target="#b30">[31]</ref>. The analysis of the interactions in visualization systems has become particularly important for conducting user studies, improving users' exploration experience, and evaluating visualizations <ref type="bibr" target="#b25">[26]</ref>. For instance, In Brown et al.'s work <ref type="bibr" target="#b10">[11]</ref>, users' interactions with the game Waldo were used to understand users' personalities. Sukumar</p><p>and Martinez investigated how to analyze users' interactions to help users better understand visualizations of their personal data <ref type="bibr" target="#b46">[47]</ref>. Nobre et al. developed reVISit system for evaluating users' performance in complex interactive visualizations for a better design <ref type="bibr" target="#b39">[40]</ref>. Interaction logs are often classified into two types: low-level logs, such as mouse movement information and eye movement data, and high-level logs, such as brushing and zooming <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b58">59]</ref>. The ways of analyzing interaction logs are often divided into qualitative analysis, such as analyzing the interaction patterns from visualizations of interaction logs <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b53">54]</ref> and quantitative analysis, such as converting interaction into numerical forms and using machine learning for data analysis <ref type="bibr" target="#b50">[51]</ref>. Blascheck et al. quantitatively analyzed users' mouse movement and eye movement data to study how people interact with a visualization <ref type="bibr" target="#b7">[8]</ref>. Guo et al. used interaction data to predict users' cognitive abilities during their visualization usage <ref type="bibr" target="#b18">[19]</ref>. Guo et al. <ref type="bibr" target="#b27">[28]</ref> used both qualitative and quantitative methods to analyze interaction logs in visual analysis systems for understanding how interactions guide insight generations. Each interaction action is defined as the tuple of the action name, the component this action applied to, and the timestamp of this action.</p><p>Inspired by these previous work, we propose a new method that captures users' high-level and semantic interaction logs and adopt a quantitative-qualitative combining method to map them into a unified parameter space for subsequent recommendations. Therefore, interaction logs across-views and across interaction types can be well defined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Provenance Meta-Analysis</head><p>Provenance Meta-Analysis <ref type="bibr" target="#b48">[49]</ref> is to present an ensemble of interaction provenance by converting interaction logs into high-dimensional vectors, to understand users' behaviors, evaluate visualization, and conduct model steering <ref type="bibr" target="#b56">[57]</ref>. The work of ModelSpace presented a pipeline that converts the interaction into a high-dimensional vector and then visualizes the vector to show the interaction trails <ref type="bibr" target="#b11">[12]</ref>.Walchshofer et al. improved the method to transform users' interaction provenance into numerical vectors by capturing the application states after each interaction <ref type="bibr" target="#b48">[49]</ref>. However, the structure and length of provenance vectors largely depend on the charts and data, making it hard to be adopted in complex visualization systems. Our inspiration is drawn from the previous methods to represent interaction data as an array. As an improvement, we bring up a brand-new way for encoding interaction provenance with inspirations drawn from declarative visualization grammars, which shows strong capabilities to represent interactions in visualization systems with multiple views and interaction types.</p><p>Satyanarayan et al. developed the Reactive Vega, which aimed to build declarative primitives to interaction. The input interaction events were parameterized and decomposed into JSON-based semantics <ref type="bibr" target="#b44">[45]</ref>. Vega-Lite managed to decode interaction events into a parametric space with a higher-level structural language, making it more convenient and less time-consuming to define an interaction <ref type="bibr" target="#b43">[44]</ref>. Each interaction log is interpreted by a specific set of parameters, ensuring a one-to-one correspondence between the interaction and the decomposition result.</p><p>Our method studied how Vega and Vega-Lite define signals and then constructed our pipeline to represent the interactions as JSON-structure data regarding Vega's declaring format. After that, we used a Context-Free Grammar (CFG) <ref type="bibr" target="#b19">[20]</ref> to convert JSON-structure data into a parse tree and then vectors to facilitate quantitative analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Interaction Recommendations for Data Exploration</head><p>Visual narrative or visual storytelling leverages visualization to tell stories for users <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b33">34]</ref>. Researchers have investigated how to support storytelling from the interactive analysis process <ref type="bibr" target="#b16">[17]</ref>. Recent work shows that interaction guidance is essential for helping users find functionality in visual interfaces and better explore the system <ref type="bibr" target="#b45">[46]</ref>.</p><p>Boy et al. defined suggested interactivity as methods that indicate an area can be interacted with by directing users' attention from the rest of the interface <ref type="bibr" target="#b9">[10]</ref>. Ceneda et al. characterized interaction guidance along the knowledge gap of the user, the input and output of the guidance generation process, and the degree of guidance that is provided to users <ref type="bibr" target="#b14">[15]</ref>. Zhou et al. built a model to represent the focus of a user's analysis through his past interaction behaviors <ref type="bibr" target="#b59">[60]</ref>. Willett et al. created a software structure that added visual cues to the interface to suggest interactions <ref type="bibr" target="#b52">[53]</ref>. In Falcon <ref type="bibr" target="#b38">[39]</ref>, Moritz et al. modeled a user's session with a client-side state and contributed prefetching techniques to eliminate latency for brushing interactions. Most of the work addressed mixed-initiative interaction approaches <ref type="bibr" target="#b2">[3]</ref>, i.e., both machines and users decide the best-suited interaction at each turn.</p><p>Generating interaction recommendations is another hot topic in this scope. Common modeling method including Pattern Analysis and Probabilistic Models. Pattern Analysis detects patterns in user preferences or interaction logs. Milo and Somech presented REACT that generates next-action suggestions by generalizing multiple abstract actions executed in similar n-contexts <ref type="bibr" target="#b37">[38]</ref>. Dabek and Caban <ref type="bibr" target="#b20">[21]</ref> construct a grammar-based model for generating interaction suggestions based on common patterns of user interaction logs. In VisGuide <ref type="bibr" target="#b12">[13]</ref>, Cao et al. modeled user's preferences using an online-learning method to recommend comparison and drill-down charts to help users do both hierarchical structuring and parallel structuring data exploration. The Markov model is a common probabilistic model in this topic. Cetintemel et al. <ref type="bibr" target="#b15">[16]</ref> built a first-order Markov chain to predict users' interaction with the database. Battle's work <ref type="bibr" target="#b4">[5]</ref> created an outgoing transition from each state for every interaction the user can make in the interface using an n-th order Markov chain. Wall et al. <ref type="bibr" target="#b49">[50]</ref> defined a Markov model of "unbiased" interactive behavior in Scatterplots, in which each combination of data point and interaction type comprises a state. Ottley et al. <ref type="bibr" target="#b40">[41]</ref> introduced a hidden Markov model for predicting user's attention by combining visual features in the visual graphs and history interaction behaviors. Deep learning models are also widely used in this field. Fan and Hauser <ref type="bibr" target="#b22">[23]</ref> introduced a convolutional neural network to estimate users' selection in scatterplots by evaluating how data distributes in visualization. Li et al. <ref type="bibr" target="#b34">[35]</ref> derived a model for predicting interaction actions in vertical menu selection based on the Long-Short Term Memory (LSTM) network. Wu et al. <ref type="bibr" target="#b54">[55]</ref> presented MultiVision, a mixed-initiative system for recommending charts based on Siamese and LSTM network structure to help users create analytical dashboards. Embeddings of data columns in visual charts are fed into a bidirectional LSTM model for further charts recommendations. Similarly, we apply LSTM-based models to solve the sequence prediction question but instead of focusing on charts, we focus on interactions.</p><p>In our method, we numerically encode interaction logs and visual states across views and feed the embeddings to LSTM models for interaction recommendations with the mixed-initiative approach addressed. Different from the previous work, our method can present interaction recommendations in systems with multiple views and interaction types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OVERVIEW</head><p>This work aims to provide interaction recommendations to public users during their exploration of visualizations with multiple linked views. To demonstrate the necessity and value, we first present a motivation scenario. Then we outline the design requirements and research problems, providing an overview of Sec. 4 and Sec. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A Motivating Scenario</head><p>The interactive visualization system has become an essential part of the public education digitization process <ref type="bibr" target="#b30">[31]</ref>. In this work, we aim at facilitating users' interactions in interactive visualization systems targeting public users. "Public users" are defined as museum-goers with different education background and ages, have little background about the topic and data analysis but are curious to explore for insights, and do not have a specific targeting goal to achieve when using the systems. We can assume that museum-goers' background is generally in line with the definition of our target user. They may have their own insights, but know little about how to make complex interactions. Users with professional backgrounds are not within our target users. Assumptions of our target users are supported by interviews and existing literature <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b36">37]</ref>. We select the museum as a motivating scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Interview with A Museum Domain Expert</head><p>To understanding the needs of our target scenario and generate reasonable design goals, we first talked with a prestigious expert from the museum informatics and public education field with over 20 years of experience constructing visualization systems for education in many museums. First, she recognized the rising importance of visualization systems in museums because these systems could trigger the audience's own enthusiasm for acquiring new knowledge through hands-on practice instead of teaching the audience a specific principle.</p><p>She also pointed out some current problems.The most critical one would be the low utilization. Many museum visitors either don't interact with the systems, or just randomly click and leave. The main reason is that audiences do not know how they can interact with the system without guidance. She believes there is an urgent need to solve current problems and help public users to explore visualization systems better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Interviews with Public Users</head><p>We also conducted interviews with two students that qualified our target users' profiles. Since visualization systems are still not very common in museums, we asked about their experience in using interactive computer systems. Firstly, they showed willingness to play with these systems because of their attractiveness. Secondly, they would spend more time when the topics were very appealing and the systems were easy to use; otherwise, they just click around and leave. Thirdly, they mentioned the discomfort of learning to use a system on their own in museums. Also, they prefer to spend less than 10 minutes with one system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Challenges in Current Practice</head><p>Researchers have also recognized the problems in the usage of interactive visualization systems in museums. Slingsby et al. found that novice users often failed to discover or used more advanced interaction functionality <ref type="bibr" target="#b45">[46]</ref>. Hinrichs et al. illustrated that since visitor' limited visiting time and the systems' competitions with other exhibits under the scenario of informal learning environments, it is hard for visitors to fully explore these systems without guidance <ref type="bibr" target="#b30">[31]</ref>.</p><p>Through literature studies and interviews, we conclude the challenges in the current practice and construct a scenario for clearer illustration together with this domain expert. A public user is visiting an exhibition about Chinese poets. A digital screen is provided for the audience to explore a visualization system, displaying the travel trajectory and poetry of several Chinese poets. However, due to the following circumstances, her exploration does not go so smoothly.</p><p>Audience's Background Knowledge Lacking Facing a system with multiple views and interaction types, this user has to decide which interaction type to adopt and what data aspects to explore. However, this decision relies on the user's background knowledge and data analysis capabilities. As illustrated in <ref type="bibr" target="#b36">[37]</ref> and <ref type="bibr" target="#b8">[9]</ref>, museum audiences usually arrive without this "pre-requisite" knowledge. Therefore, it is hard for the visitor to initiate their exploration in the system.</p><p>Multi Views and Interaction Types Due to the multiple views and the lack of knowledge, the audience may tend to concentrate on one single view and one interaction type, thus adopting limited interactions to explore in a single view and ignoring other views and data. This results in monotonous data exploration and underutilization of the interactive visualization system.</p><p>No Facilitation Most exhibitions are designed to be used without external facilitation <ref type="bibr" target="#b36">[37]</ref>. Therefore, the audience would only refer to the systems' instructions or guidelines when they encounter obstacles, which are usually non-real-time, and entry-level. Such instructions may not provide continuous assistance and or consider users' actual interactions, making it hard to improve the user's interaction experience.</p><p>It can be concluded that users feel hard to construct interactions in visualization systems without proper guidance and instructions. As a result, facilitating users' exploration in the visualization systems has become a significant priority <ref type="bibr" target="#b30">[31]</ref>. We primarily focus on designing a model to generate interaction recommendations for public users. We addressed mixed-initiative interaction approaches by that both the model and users decide the best-suited interaction at each round.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Design Goals</head><p>Sec. 3.1 illustrates the necessity to provide public users with subsequent interaction recommendations in interactive visualization systems with multiple views and interaction types. We then outline features that provided recommendations should require based on findings from our application scenario.</p><p>G1: Real-time The No Facilitation feature requires the system to provide users with continuous and real-time interaction recommendations to ensure users' using experience. When users face multi-view visualization systems, we believe the most straightforward solution is to recommend interactions right after one interaction is made. Our model should support providing the recommendations of the next interaction immediately after one interaction is made. This can give the users clearer interaction guidance and enhance users' interaction experience.</p><p>G2: Diverse The characteristic of Multi Views and Interaction Types of the systems reflects that the recommendations should support across-views and various types interactions. This can not only help users conduct more in-depth explorations that approach the data from diverse perspectives but also help the system be fully utilized.</p><p>G3: Insightful The users' lack of pre-requisite knowledge suggests that the system offers insightful recommendations that can lead to valuable data insight, to assist users in exploration. The insightful interaction recommendations can lead to valuable exploration that can deepen the user's understanding of the topic she explores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Research Problems</head><p>Our design goal is to provide real-time and diverse interaction recommendations for public users during their exploration of the system and inspire them to obtain more accurate data insights. In order to propose interaction recommendations that support the above requirements, we analyze our task and break down the research problems as follows.</p><p>R1: How to encode the interaction behaviors? We need to bring up a mechanism that can encode interaction logs of multiple interaction types and object views. This is the basis of further study.</p><p>R2: How to construct a model for recommending interactions? This problem decides the structure of our predicting model and greatly influences our method's performance.</p><p>R3: How to quantitatively calculate the degree of diversity and insightfulness? We need to quantitatively define the degree of diversity and insightfulness to filter the collected training data and use these as parts of the evaluation metrics.</p><p>R4: How to present our real-time interaction recommendations to the users during their exploration? It decides our method's usage and whether our method can combine human and machine intelligence.</p><p>Corresponding solutions are carefully illustrated in the following Sec. 4 and Sec. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MODEL FRAMEWORK</head><p>Based on the research problems in Sec. 3.3, we present a model that can give users real-time interaction recommendations when exploring multi-view visualization systems. From literary research <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b51">52]</ref>, we concluded that users choose their interactions based on the previous behaviors and the information displayed by the visualization chart. An interaction log is one record that contains what type the interaction is, what data it interact with and on which view it happens. Based on this conclusion, we decide to encode the interaction logs and visual states numerically as the input and use deep learning methods to make interaction recommendations based on these.</p><p>The workflow presented in Fig. <ref type="figure">1</ref> is constructed with several parts: an interaction converter for modeling interactions, a visual states converter for modeling visual states, and a recommendation generation model for recommending next interactions. A user is shown a visualization system with multiple views and interaction types. Different interaction recommendations will be presented in the UI after the users' current interaction. The user can choose to follow the recommendations or not. The system captures users' actual interaction for further model training. Following subsections introduce how each step works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Interactions Converter (R1)</head><p>To define the encoding of interaction logs, we need to decide how to categorize interaction types and map the logs to numerical space. In this section, we focus on solving R1. Fig. <ref type="figure">1</ref>. A high-level illustration of our workflow for recommending interactions. In the user's workflow, each time the user completes an interaction, the system collects the interactions and visual states and then transfers them to the converters in a sequence. In the predicting workflow, the pre-trained LSTM models receive inputs and then output various predicted interaction vectors. After being converted to JSON data, the corresponding recommended interactions will be presented to the user. The two UIs on the right represent one user's interaction of brushing in the Map View, and the system gives the recommended interaction of selecting one column in the Histogram view for the next step.</p><p>First, we define a discrete-time index t to distinguish each interaction log I t . At the start of a user's exploration, t = 0. Each time the user explores, t increases accordingly, thus forming an interaction sequence of user i S i = [I 0 , I 1 ,...,I k ,...].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Interaction Type</head><p>There are many widely recognized interaction taxonomies for visualization systems <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b52">53]</ref>. To make our method more general and universal, we develop a taxonomy of interaction logs based on the interaction classification method in <ref type="bibr" target="#b57">[58]</ref>. Three interaction types that are high-level and semantic are covered and listed in Table <ref type="table">.</ref> 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1. Interaction Types and Parameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type</head><p>Definition Parameters</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Select</head><p>Choosing interesting points, such as click, brush, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The coordinate range and the index of the content (if any) Filter</head><p>Querying data marks with conditions, such as filtering specific channels</p><p>The conditions and scope of its filtering</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract / Elaborate</head><p>Showing an area of information more or less detailed such as zoom and pan, etc.</p><p>The relationship between the area to be explored and full view</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Interaction Parameters</head><p>In Reactive Vega <ref type="bibr" target="#b44">[45]</ref>, the authors defined specific parameter spaces for different types of interactions for interpretation. Inspired by this work, we determine a set of parameters </p><formula xml:id="formula_0">P T = [p 1 ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Modeling Interactions Numerically</head><p>We first convert the interaction logs into JSON-structure data, then to numerical vectors for further study. This section introduces this pipeline.</p><p>We proposed a rule that transforms each interaction log to a JSON structure of (Interaction Type, Object View, Interaction Parameters), which can define interaction data from across-view and across interaction types. Then we apply an embedding method to the JSON data using a Context-Free Grammar (CFG) representation <ref type="bibr" target="#b19">[20]</ref>. This representation helps to transform data in different JSON structures to parsed trees based on rules extracted by CFG from the dataset. Under this method, data that has different JSON structures can be converted into  the same rule-governed format since the method is conducted over the whole dataset. The parsed-tree consists of the following keys:</p><p>• Object View: Indicating the object view of the interaction log. • Type: Indicating the type of the interaction log. • Behavior: Indicating the parameter set corresponding to the interaction type that can uniquely define an interaction log. • Behavior Parameter Value: Indicating the corresponding value of the parameters in Behavior.</p><p>We use a new mechanism to express both quantitative and qualitative variables in the same vector. For qualitative variables Type, Object View, Behavior, the attribute is represented by the index of the corresponding value; for a quantitative variable Behavior Parameter View, its value is directly filled in the corresponding position of the vector.</p><p>Through this method, we can map each interaction log to a unique numeric vector. Taking the Brush operation in Fig. <ref type="figure" target="#fig_0">2</ref> (a) as an example, the user performs a brush operation in Map view, starting at point (236, 268) and ending at point (378,400). In this case, the first attribute value '3' indicates that the 2nd-5th attributes are the Behavior Parameter Values. The 6th attribute value '4' represents the view 'Map' and the 7th attribute value '7' indicates that the interaction type is 'Brush'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Visual States Converter (Continuing R1)</head><p>Every interaction in the visualization system, in many cases, will cause the visual states in each view to change, thereby bringing new data insights to the users. These visual states and data insights may affect the user's next interaction.</p><p>We consider what information can represent the status of the current visualization, and decompose the data distribution information on a This process is model initialization and is conducted offline (before the recommendation process). Model Updating is conducted during the recommendation process by capturing users' actual interactions and then using this data for regularly fine-tuning the pre-trained LSTM model.</p><p>view into low-level visual channels to form a quantitative information collection V S = (C 1 ,C 2 ,...C k ). <ref type="bibr" target="#b5">[6]</ref> defined the visual channels of marks on a visual chart as the color, position, size, shape, etc., which is able to describe any visual charts <ref type="bibr" target="#b13">[14]</ref>. We believe this information set V S affects the user's choice of the next interaction log. Thus we generate the parameter space of encoding visual states based on the law in <ref type="bibr" target="#b5">[6]</ref>. For instance, a Map View with Point marks distribute on it, the recorded visual state is stored in JSON format with keys (channels) {Points' coordinates, Points' color, Points' size}. We then use the tSNE <ref type="bibr" target="#b47">[48]</ref> method on each channel of each view for dimension reduction. Each view is represented by a numerical vector and the visual states in the systems are represented numerically. The numerical vector is attached to the corresponding interaction for model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">An LSTM Model Approach (R2)</head><p>In this section, we focus on solving R2. We compared the LSTM model, Hidden Markov Model (HMM) and Transformer-based network, which are popular in sequential data analysis. HMM only considers the impact of the timestamp t − 1 at t and does not consider the historical interaction sequence. Also, the transition matrix of visual states is emphasized in the HMM, but this is not what we need to consider, thus increasing the model's complexity. The transformer-based network lacks modeling of the temporal dimension in the sequences, which is important information in our task. The complexity of the Transformerbased model also is unwanted in our scenario. LSTM shows significant advantages among them because it considers the user's historical interaction data, imitate the user's memorizing and forgetting habit and is more convenient and straightforward for experts in museums and other scenarios to train.</p><p>Taking all these into account, we choose the LSTM model. Fig. <ref type="figure" target="#fig_2">3</ref> shows how the model works. We use the time-shifted window approach to process the data and set the window's length as b, i.e., the last b records are used to predict the next interaction log. Multiple models with various parameters are provided to improve the diversity of our method's results (G2). The LSTM model is fed with the input sequence</p><formula xml:id="formula_1">{Z j ; j = n − b + 1, n − b + 2, •••, n}</formula><p>and then outputs a vector of the recommendations for each step. After decoding this vector, we could obtain recommended interactions, which are later presented to the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Model Initializing and Continual Learning (R3)</head><p>Instead of using a static model, we divide our model training into two parts, as shown in Fig. <ref type="figure">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Model Initializing</head><p>The training data is first cleaned by two filters of diversity and insightfulness to ensure the data quality required by G2 and G3. Therefore, the recommending results can also have this nature because of that the Kullback-Leibler divergence <ref type="bibr" target="#b32">[33]</ref> between the distributions of the recommended results and training data should be small. We compute the entropy of users' interaction types and views to measure the degree of diversity and keeps the interaction logs only if the entropy is higher than a threshold. The insightfulness filter regards a series of interactions as insightful if it involves interactions within the insightful data areas. The insightful areas are selected on different views where data are distributed in clusters. For instance, on Map View the insightful areas cover the east, south, and central parts of the map. Based on the above illustration, we employed the following two equations to calculate the Degree − o f − Diversity (DoD, ranging from 0 to 1; a higher DoD is closer to 1), Degree − o f − Insight f ulness (DoI, ranging from 0 to 1; a higher DoI is closer to 1). Here we focus on solving R3. The two filters are controlled by a pair of weight-tuning parameters. A higher weight refers to stronger filtering. These weight-tuning parameters are pre-setted by system managers based on the preferred needs for diversity and insight. For instance, a larger DoI is suggested to obtain a preference for insightfulness if the museums would like visitors to have more task-oriented exploration. And a larger DoD and a smaller DoI can be set to obtain a preference for diversity if more fun is wanted.</p><p>After the filtering, the cleaned data is fed into the LSTM model above and the training will start.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DoD = Entropy o f interactions distribution o f each Interaction Type</head><p>(1) DoI = #Predictions that within the insight f ul data area #Predicting results (2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Model Updating</head><p>To keep high-quality performance over time, our model achieves continual learning by supplementing a steady stream of new training data.</p><p>Training data explicitly calls out attributes in the dataset that represent fundamental facts in the real world, which is continually changing. <ref type="bibr" target="#b28">[29]</ref>. Therefore, a steady stream of new training data is necessary for the model's performance. We recognize this issue and develop a continual learning pipeline regarding this.</p><p>As Fig. <ref type="figure">4</ref> shows, once a user rejects the provided recommendations, we consistently collect the actual interactions and use them to finetune the pre-trained model regularly. For example, a user rejected the recommendations at time t and made his own interaction log I t , then we would feed [(I t−b+1 , ..., I t ] as a new piece of data to the model updating (the width of the LSTM's time-shifted window is b). We design this strategy because when the user rejects the recommendations, it means the recommendations are not satisfying and the user's actual interactions can help correct the models' predicting results. In this way, we realize the model continues learning without manual labor.</p><p>This strategy can also work as a cold start. When transformed to a new visualization system, our model can first generate random recommendations and collect the data once users reject the recommendations. With the gradually incoming data, the model gets fine-tuned several times and will gradually improve its performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Presenting the Predicted Results (R4)</head><p>This section focuses on solving R4. The predicted vector results generated by the LSTM model first need to be transformed into the hierarchical interaction log through the Interaction Converter using similar rules as described in Sec. 4.1.</p><p>The visualization system captures and analyzes the prediction results, and feeds back recommendations on the view. The recommended interaction position or area will be highlighted on the predicted Object View. For example, if the predicted result is brushing, the recommended brushing area will be highlighted in the corresponding object view. This presenting method is selected because it is straightforward for the audience to understand the recommendation when it is shown on the interface around the data marks that are recommended to interact with. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">AN ILLUSTRATIVE EXAMPLE</head><p>This section proposes an interactive visualization system in a museum scenario as an example to implement and test our method completely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">A Multi-view Visualization System</head><p>We selected four different poets in Song Dynasty <ref type="bibr" target="#b1">[2]</ref>. The data contains the Spatio-temporal trajectory data of when and where a literati was created and the content. To present the poets' life from different angles, and to meet our method needs (i.e., multi-view, multi-interaction types), we designed the following visualization system (Fig. <ref type="figure" target="#fig_3">5</ref>).</p><p>For the poetry data, we use the TF-IDF algorithm <ref type="bibr" target="#b42">[43]</ref> to calculate the keywords in each poet's works and apply sentiment analysis <ref type="bibr" target="#b35">[36]</ref> to each work's. For each keyword, we calculate the average of all poems' sentiment value that this keyword appears in; for each location, we calculate all the poems' average sentiment value in that location. We draw the trajectories in chronological order on the map. The points' color is based on its sentiment value. The force-directed map shows the connection between various locations. A location with a larger radius means the author visited this location more often.</p><p>This system consists of five views including (a) Poets button for selecting poets to explore.  We have incorporated seven different interaction logs types based on different views into the system. According to the method in Sec.4.1, we define each Behavior of each type of interaction, and then map the interaction data into a numeric vector. The interaction types and parameter sets P T are listed in Table. We also obtain the corresponding visual states generated during each interaction. The channels we collected for each interaction view are listed in Table <ref type="table">.</ref> 3. The collected visual states are then transformed into low-dimension vectors using the method illustrated in Sec. 4.2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Modeling Users' Interaction and Visual States</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Preparing Predicting Models</head><p>Our method uses deep learning to predict the user's next interaction. Firstly, we need to collect interaction data generated under purposeful and logical data exploration conducted by public users after understanding our system. Then, through the data processing method mentioned above, the data is converted into a numerical format. We construct the training dataset based on these generated numerical vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Training Data</head><p>47 participants that satisfied the definition of "public users" as defined in Sec. 3.1 are invited for data collecting. There are 21 women and 26 men with ages from 18 to 27. All the participants have finished or are in the process of college education. After 5 minutes of reading about the manual of system, they spent 9 minutes exploring the system in Google Chrome browser on their own computers. During the exploration, we collected each user's interaction data and the visualization states.</p><p>The weighted combination of a diversity filter and an insightfulness filter is applied to the training data to ensure the quality. The weight of two filters can be modified according to actual needs as a trade-off of the need for diversity and insightfulness. In this example, we set the two weights both to 0.5, meaning that the diversity and insightfulness are equally important. After mapping the cleaned data using the method in Sec.5.2, we obtained 4,716 interaction log vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Model Training</head><p>The sequence composed of the current interaction and the previous b-1 interaction logs is the input (b is the width of the time-shifted window as defined in Sec. 4.3). When a user performs an interaction, the sequence will be passed into these models simultaneously and generate results. Based on experience, we took the window width from 3 to 5 and used different network structures to obtain 3 models.</p><p>We used several techniques to improve the model.Firstly, we normalized each variable before training. Secondly, since both Type and Object View are qualitative variables, after the predicted result was inversely normalized, we took the values from the value range of Type and Object View that were closest to the actual predicted results as the predicted result. The prediction vector generated by the above method was transformed into structured interaction data.</p><p>We used Bidirectional LSTM (BiLSTM) to build the model <ref type="bibr" target="#b26">[27]</ref>. This structure traverses the input sequence in two directions to make predictions based on past information and uses future information to modify the prediction results, improving the effectiveness of the prediction compared with LSTM. Different choices of hyper-parameter values, including the sliding window's width, the number of BiLSTM layers in the neural network, and the number of cells in each BiLSTM layer, are tuned and compared for better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Performance Results</head><p>The collected data set was divided into the training and testing set at a ratio of 4:1. We compared the training duration, the accuracy and loss value of the training and testing set, and the mean square error of each model. The three models with the best prediction effects were selected as the final models. The results are shown in Table <ref type="table">.</ref>4. After the comparison and selection, we retrained the models using the entire dataset and obtained three prediction models for further usage.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4">Impact of the Weighted Filters and Model Updating</head><p>To evaluate the weighted filters and the model updating, we conducted an ablation experiment to compare the models' performance. We trained four different sets of models (with or without filters and with or without model updating), and used these to make predictions of the dataset we collected in Sec. 5.3.1. We then calculated the DoD and DoI using the equations in Sec. 4.4.1. The results are listed in Table <ref type="table">.</ref> 5. It shows that the filters of diversity and insightfulness significantly increased the results' DoD and DoI, but decreased the accuracy very slightly, which measures the difference between the users' actual interactions and the recommendations, because the filters decreased the size of the training dataset by filtering out unsatisfying data. The results justify the effectiveness of the two filters. The model updating method also increased the results' accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Visual Exploration With Recommendations</head><p>After data collection and model building, we obtain three models for interaction recommendations. This allows our method to provide users with three different recommendations for the next interaction choice. Fig. <ref type="figure" target="#fig_6">6</ref> shows an illustrative example of the three different recommendation results according to one input interaction log. For each round, the visualization system makes and presents predictions and feeds back recommendations for the next operation. The user could choose to follow one of the recommendations or freely operate. Both previous users and the current users' interaction logs impact the provided interactions in two ways. Firstly, the pre-trained model generates recommendations based on this user's previous two interaction logs. Secondly, the model updating strategy helps the pre-trained model fine-tune itself based on the new cases for a better predicting result. It reflects the concept of human-machine coordination in our method.  the Force-directed View and Histogram View, compared with Group A and B. The distribution in Group B is broader than that in Group C on the Map View. This could be caused by the insightfulness filter, which may reduce the diversity to some degree reflecting the trade-off between diversity and insightfulness.</p><p>Effect on Obtaining Data Insights To evaluate whether our method could guide participants to obtain more data insights, we analyzed the questionnaire result. A higher acceptance of the correct perceptions and a lower acceptance of the incorrect ones means more accurate insights gained from the system. Fig. <ref type="figure" target="#fig_7">7 (a)</ref> shows the distribution of the agreement of insight statements. Participants in Group C showed more agreement to good insights (Questions 1-3) and less to pseudo-insights (Questions 4) than those in Group A and B. The results have been verified by two paired t-tests between Group A &amp; C and Group B &amp; C. This shows our model's ability to help participants obtain a more insightful understanding of the target topic, satisfying our G3.</p><p>Usability The 5-point Likert Scales rating focuses on the acceptance rate of the interactive system's recommendations (Reasonablity), whether the system can guide participants to obtain more data insights (G3), whether the system can inspire users to adopt various interactions (G2) and the satisfaction of the system's usage. The averaged results are shown in Fig. <ref type="figure" target="#fig_7">7 (b</ref>). The acceptance rate is 52.58% in Group B and 72% in Group C, which shows that the model's recommendations are reasonable. Also, participants in both groups reported that recommendations could inspire them to adopt various interactions types. Group C users with score= 3.92 showed more agreement of this statement compared to Group B (score = 3.43). Group C also reported a score of 3.8 to our systems' performance, much higher than that in Group B. When recommendations were turned down, some thought the recommendations could be distracting when they wanted to explore using similar actions around a specific area. Some would reject the recommendations when they could not figure out what outcomes they could get after interactions. Possible solutions are discussed in Sec. 7.</p><p>Run Time Based on the deep learning framework, our model can generate recommendation results immediately. The time interval from the user's current interaction to the display of recommendations is 0.8s, which is close to no waiting time, ensuring the G1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">In-depth Interview</head><p>In this session, in-depth interviews with several participants were conducted for further opinions on our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Method</head><p>We invited three participants and conducted interviews about their attitudes towards the exploring experience.</p><p>Participants We invited a 26-year-old male student in the field of data science, a 21-year-old female student in the field of museum study, and the domain expert in Sec. 3 to evaluate our system.</p><p>Procedure and Tasks The study is divided into an observation part and an interview part. For the first part, participants follow procedures in Study 1 with one researcher around. After the experiment, a semistructured interview was conducted for additional feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Results and Analysis</head><p>Participants reported spending about 90s to get familiar with the systems and 8.5 minutes for an in-depth exploration. According to their claims, their exploration interest changed during the usage of the system, from the poet's activities in the capital to the poet's trajectory and the relationship between his creative style and the places. Participants hoped to obtain multiple insights on these topics through exploration.</p><p>Reasonability The participants' acceptance of interaction recommendations accounted for 70%, 55% and 59%, that is, on average, users have the possibility of 61% to accept the provided recommendations. One participant reported that although there were some repetitive recommendations, more than 65% of the system's interaction recommendations were reasonable, meaningful, and coherent with previous interaction behaviors. One participant reported that, since she found many of the recommendations were reasonable, she sometimes struggled to choose one from the three provided at the same time. She suggested the recommendations be provided in priority.</p><p>Diversity The interaction recommendations were very diverse (in more than 190 interactions, 54 interaction recommendations are on three different views, 68 recommendations are on two different views), and not intermittent (78% of recommendations include views based on the current interaction). The expert also agreed that the recommendations are helpful to explore more detailed information from a broader aspect. But it was sometimes difficult for her to immediately understand why a recommendation was made when the previous and current recommendations were very different. She suggested our method provide more explanation. Another participant experienced a similar issue, but he chose to trust the recommendations and would pick the recommendations to explore another area. In his case, he showed a relatively higher degree of trust with the system compared with his judgment, which is a limitation that also discussed in other previous work <ref type="bibr" target="#b29">[30]</ref>  <ref type="bibr" target="#b55">[56]</ref>.</p><p>Insightfulness The participants recognized that the recommendations could lead users to more data insights. One participant gave an example. When he selected the poet's travel experience in the eastern coastal area and wished to explore further, the system recommended he explore the Wordle View and filter different map point colors. This made him realize that he could pay more attention to the emotional characteristics of works in the region, which led him to draw new data conclusions. The domain expert also recognized this point but further suggested that some simple explanations of the visualization results could be provided after one interaction was conducted.</p><p>Usability All participants liked the current way how the system interacted with the users and how the recommendations were provided. One participant pointed out that the recommendations provided by our method were easy to follow and deepened his understanding of how to interact with visualization systems. The domain expert pointed out that our method is able to give users the freedom to explore what they like without museums' need to design a curation to teach the knowledge. However, one participant suggested involving the "UNDO" feature in the system since it reflected the users' second thoughts. This suggestion might be further developed in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">LIMITATIONS AND DISCUSSION</head><p>The illustrative example shows that our system can model interaction behaviors and visual states and make recommendations to facilitate and inspire users' next step interaction. The evaluation results suggest that our method can help public users explore complex visualization systems by combining human and machine intelligence together. However, we admit that this method does not come without any limitations.</p><p>Firstly, explanations about why the recommendations were generated and how to interpret the visualization can be provided when giving recommendations. We have tackled the functionality side of the provided recommendations (reasonability, diversity, and insightfulness). The next step would be focusing on interpretability and usability to further help the results be more understandable and acceptable.</p><p>Secondly, although we have designed strategies to mimic the environment of museums by presenting virtual exhibits to participants in the in-lab evaluation, a user study under a real scenario would be more persuasive. Also, participants should cover students from middle and primary school, and adults with lower degrees to better fit the distribution of museum visitors. And in the museum, the audience interact with most visualization systems with a touch screen, but in our experiment, they used a mouse to make interactions. A more rigorous evaluation will be conducted within the real museum environment for future work.</p><p>Thirdly, there is still room to improve the model even though our model is able to provide diverse and insightful interaction recommendations now. One way would be to collect more training data. Another approach would be to integrate different methods of parsing visual charts and extracting data insights, such as auto insights. In future work, we will try to experiment with other methods of analyzing visual states and compare their effects on the model's performance.</p><p>We admitted that showing one case in our paper limits the generalization and extensibility of our method. In this work, we emphasized more on giving a clear explanation and evaluation of our designed workflow. To reduce this limitation, we have thoroughly discussed the implementation and evaluation of this case. Moreover, the modeling updating strategy mentioned in Sec. 4.4.2 as a cold start and the encoding strategy reduce the cost of transforming the method to a new system. In the future, we would like to compact our method into a toolkit that can be added to the existing visualization systems to improve the reusability of our methods in more systems under the public education scenarios.</p><p>The trade-off between diversity and insightfulness deserves further discussion. Since every user might exhibit homogeneous exploration behaviors, different users show diverse exploration patterns that our method can learn. "There are a thousand Hamlets in a thousand people's eyes", "insightful" focused on helping audiences obtain a "Hamlet" after exploration, while "diverse" focused on "how each individual audience obtain a distinct Hamlet in their way". Therefore, the relationship between diversity and insightfulness is actually the unity of opposites. Better ways to deal with this relationship can be added, such as dynamically modification of filters' weights according to users' exploration tasks and interaction patterns. When users' tasks are clear, the insightfulness filter has a higher weight and vice versa. In future work, a module for recognizing users' tasks might be added, and the recognized users' tasks would be combined with interactions log, visual states and the weighted filters for generating predictions.</p><p>Recommendations provided to professions should be different. Domain experts might have clear tasks to achieve during their usage, so the recommendations should be more task-oriented. Also, since exports have more background knowledge, providing interaction hints or adding triggers to start recommending might be more helpful. New strategies are required to provide the recommendations to expert users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>This work introduces a method for making real-time interaction recommendations to users when exploring interactive visualization systems with multiple views and interaction types. This method adopts a deep learning structure to learn the patterns in historical users' interaction paths and corresponding visual states and make subsequent interaction recommendations to the current user based on the learned patterns. The results of an illustrative example and the user study found that the method is capable of giving real-time recommendations to public users when they are exploring interactive visualization systems. The recommendations generated by our method are proven to be reasonable, diverse, and insightful, which is consistent with our design motivation and effectively solves the tasks faced by novice users-oriented interactive systems in public education scenarios. We believe our method can contribute to interaction studies and visualization accessibility, helping visualization exploration better support public users.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Examples the encoding process of (a) Brush, (b) Click, and (c) Zoom. These interactions are examples of interactions in Table.1. They are converted to JSON representations, then to CFG Parsed Tree, and then to numerical vectors based on the rules generated by CFG Tree.</figDesc><graphic url="image-5.png" coords="4,314.07,303.76,113.35,108.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig. 2. Examples the encoding process of (a) Brush, (b) Click, and (c) Zoom. These interactions are examples of interactions in Table.1. They are converted to JSON representations, then to CFG Parsed Tree, and then to numerical vectors based on the rules generated by CFG Tree.</figDesc><graphic url="image-6.png" coords="4,426.97,303.76,113.24,108.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The Data Structure of LSTM Model. The first k {Interaction, Visual States} are sequenced together to form an input value Z k , and the corresponding label value is the next interaction I k+1</figDesc><graphic url="image-7.png" coords="5,88.02,69.79,183.27,86.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The Interface of the System. (a) Select one poet to explore. (b) Wordle View. (c) Map View. (d) Force Directed Graph View. (e) Histogram View.</figDesc><graphic url="image-10.png" coords="6,197.24,73.00,97.24,131.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(b) Wordle View that presents keywords with their sentiment value and frequency. It supports interaction types of Hover and Filter. The x-axis represents the frequency of each word. (c) Map View that shows the places a poet visited. It supports Hover, Brush, and Zoom. (d) Force Directed Graph View showing the connection between the locations the poet visited in the selected area. It supports Click. (e) Histogram View that shows the frequency of poets' visits in the selected area at different ages. It supports Click. The x-axis represents the age. Views are connected through interactions like select, hover, zoom, etc. We also compile a set of data insights from the dataset with the guidance of a domain expert in Sec. 3.1.1. as a validation for our model's performance of insightfulness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2 .</head><label>2</label><figDesc>The numerical vector's structure after mapping is [ Behavior, Parameter values, Object view, Type].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. One example of recommendation results of one input sequence of interaction logs. 1 click a column on the Histogram View 2 brush an area on the Map View 3 zoom into an area on the Map View, compose a sequence of user interactions and the corresponding visual states. a brush on the Map View b click on the Force-Directed View c click on the Histogram View are the recommendations given by our model.</figDesc><graphic url="image-12.png" coords="7,77.10,216.09,209.85,173.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Data Insights. Questionnaire Rating (a) shows the distribution of the acceptance degree of insight statements, with the results of paired t-tests' p-value between our model and two control groups (*, ** and *** denote p &lt; .05, .01 and .001, respectively). The higher degree of acceptance of the correct statement (Creation and Trajectory Characteristic Q1-Q3), the better the data insight obtained by the user. 5-Points Likert Scales (b) shows users' reviews of random recommendations and our model's recommendations. Bar Chart (c) shows user's interactions' entropy in the aspect of interaction types, object views, and interaction behaviors among three different groups, along with the results of t-tests.</figDesc><graphic url="image-13.png" coords="8,91.07,73.00,418.83,223.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Interaction Diversity Comparison. Interaction Heat-map shows the distribution of interactions on the interface in three groups. It is notable that the distribution of the interactions is broader in Group With Our Model's Recommendation, especially on Force-directed View and Wordle View.</figDesc><graphic url="image-14.png" coords="8,63.97,344.61,63.48,64.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>• Y. Li, Y. Qi and S. Chen are with School of Data Science, Fudan University. S. Chen is the corresponding author. E-mail: simingchen@fudan.edu.cn. • Y. Li is now also with UC Santa Barbara . • Y. Shi, Q. Chen and N. Cao are with Tongji University. Manuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication xx xxx. 201x; date of current version xx xxx. 201x. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org. Digital Object Identifier: xx.xxxx/TVCG.201x.xxxxxxx</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>p 2 ,..., p n ] of each interaction type T in Sec. 4.1.1. Different interaction types have different sets of P T , and one P T has various values for each time interaction log. The last column in Table. 1 shows the details.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Interaction Types</figDesc><table><row><cell cols="2">Taxonomy Type</cell><cell>Object View</cell><cell>Behavior</cell></row><row><cell>Select</cell><cell cols="2">Hover Wordle</cell><cell>Coordinates of points</cell></row><row><cell></cell><cell cols="2">Hover Map</cell><cell>Coordinates of points</cell></row><row><cell></cell><cell cols="2">Hover Force-Directed</cell><cell>Coordinates of points</cell></row><row><cell></cell><cell>Click</cell><cell>Histogram</cell><cell>Index of columns</cell></row><row><cell></cell><cell cols="2">Brush Map</cell><cell>Coordinates of bound-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ing boxes</cell></row><row><cell>Abstract</cell><cell cols="2">Zoom Map</cell><cell>Scale, Δ x and Δ y</cell></row><row><cell>Filter</cell><cell>Filter</cell><cell>Map</cell><cell>Value</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>The Visual States of Each View Type</figDesc><table><row><cell>View Type</cell><cell cols="2">Mark Type Visual Channel</cell></row><row><cell>Wordle</cell><cell>Points</cell><cell>Points' coordinates and color</cell></row><row><cell>Map</cell><cell>Points</cell><cell>Points' coordinates and color</cell></row><row><cell cols="2">Force-Directed Points</cell><cell>Points' coordinates and radios</cell></row><row><cell>Histogram</cell><cell>Columns</cell><cell>Columns' index and height</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Model Performance</figDesc><table><row><cell cols="2">Model Structure</cell><cell>Loss</cell><cell>Valid-</cell><cell>Acc</cell><cell>Valid-</cell><cell>RMSE Valid-</cell><cell>Trianing</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Loss</cell><cell></cell><cell>Acc</cell><cell>RMSE</cell><cell>Duration</cell></row><row><cell>A</cell><cell>BiLSTM(100)+Dropout(0.2)</cell><cell cols="5">0.0142 0.0151 0.6915 0.6702 0.1188 0.1229</cell><cell>15s/epoch</cell></row><row><cell>B</cell><cell cols="6">BiLSTM(50)+Dropout(0.2)+BiLSTM(50)+Dropout(0.2) 0.0118 0.0128 0.7132 0.7001 0.1036 0.1133</cell><cell>26s/epoch</cell></row><row><cell>C</cell><cell cols="6">BiLSTM(100)+Dropout(0.2)+BiLSTM(100)+Dropout(0.2) 0.0090 0.0127 0.7406 0.7155 0.0952 0.1122</cell><cell>25s/epoch</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Impacts on DoD, DoI and accuracy of filters and model updating</figDesc><table><row><cell>Model Types</cell><cell>DoD</cell><cell>DOI</cell><cell>Accuracy</cell></row><row><cell cols="2">Filters(NO), Update (NO) 0.329</cell><cell>0.659</cell><cell>0.6814</cell></row><row><cell cols="2">Filters(Yes), Update (NO) 0.776</cell><cell>0.873</cell><cell>0.6656</cell></row><row><cell cols="2">Filters(NO), Update (Yes) 0.663</cell><cell>0.947</cell><cell>0.7242</cell></row><row><cell cols="2">Filters(Yes), Update (Yes) 0.838</cell><cell>0.963</cell><cell>0.7151</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work is supported by Shanghai Municipal Science and Technology Major Project (No. 2018SHZDZX01, 2021SHZDZX0103), General Program (No. 21ZR1403300), Sailing Program (No.21YF1402900) and ZJLab. This work is also supported by NSFC No. 61972278 and No.62061136003.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATION</head><p>In order to evaluate the effectiveness of our method, we further conducted a controlled user study and a set of in-depth interviews.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">A controlled study</head><p>A controlled study was conducted for the investment of evaluating our method's performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Method</head><p>Participants were invited to use our visualization systems and complete a questionnaire at the end of the evaluation.</p><p>Participants 42 Participants were selected based on the target users definition in Sec. 3.2 with backgrounds ranging from computer science, data science, mathematics to museum study, psychology, and philosophy, and with ages ranging from 18 to 30 (M = 22.45, SD = 3.00). 31 have little experience in data analysis and 11 have basic data analysis knowledge less than 1 year. They all reported visiting 3 or more times in the last six months and having interests in Chinese poets. All the participants in this study are different from those in Sec. 5.3.1.</p><p>Participants were randomly assigned to three groups, of which 14 people without the interaction recommendation function as a control group (Group A), 14 with interaction recommendations randomly generated as another control group (Group B), and the remaining 14 people explored the visualization system with interaction recommendations generated by our model as the experimental group (Group C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure and Tasks</head><p>We first gave participants some digital exhibits about the poets to imitate the background knowledge provided by exhibits in museums. Then, we briefly introduced the visualization system. After that, participants explored our system on their own within 8 minutes. After that, a questionnaire about the poets was sent to participants to test data insights they gained from the exploration. The questions were selected from the data insights we constructed as mentioned in Sec. 5.1, including six correct insights (Creation and Trajectory Characteristic Questions 1-3), and two fake insights (Questions 4). Participants were asked to rate out of 5 their degree of agreement of the statements. For Group B and C, the questionnaires also contained some 5-point Likert Scales questions about users' experience toward the recommendations. A questionnaire sample and detailed procedure record are attached to the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Results and Analysis</head><p>Effect on Interaction Diversity We obtained 1368 interaction logs from Group A, 1660 from Group B, and 1480 from Group C. Then, we calculated the entropies of different interaction types (4 in total), interaction object views (4 in total), and interaction behaviors (7 in total) of each participant's interaction logs. A bar chart is presented in Fig. <ref type="figure">7</ref> (c) to illustrate the average entropy of each criteria using the following formula HGroupX = ∑i(Hi)</p><p>Size o f GroupX . As shown in the Fig. <ref type="figure">7(c</ref>), the values of user interactions in group C are significantly greater than those in group A in terms of Interaction Type, and significantly greater than those in groups A and B in terms of Object View and Interaction Behavior. It illustrates our approach is able to provide recommendations with different interaction types and object views, thus increasing the diversity of user interactions, which satisfies our G2. Also, we captured the coordinates of the interaction logs on the interface and drew a heat-map to compare their distributions. As Fig. <ref type="figure">8</ref> displays, users' interaction range in Group C is broader, especially on</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://www.shanghaimuseum.net/museum/dongqichang/index.html" />
		<title level="m">Dong qichang artworks exhibition</title>
				<imprint>
			<biblScope unit="page" from="2022" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="https://sou-yun.cn/poetlifemap.aspx" />
		<title level="m">Souyun map of poems in tang and song dynasty</title>
				<imprint>
			<biblScope unit="page" from="2021" to="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mixed-initiative interaction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Horvtz</surname></persName>
		</author>
		<idno type="DOI">10.1109/5254.796083</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems and their Applications</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="14" to="23" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Low-level components of analytic activity in information visualization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Amar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
		<idno type="DOI">10.1109/INFOVIS.2005.24</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Proceedings of the 2005 IEEE Symposium on Information Visualization, INFOVIS &apos;05</title>
				<meeting>the the 2005 IEEE Symposium on Information Visualization, INFOVIS &apos;05<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dynamic prefetching of data tiles for interactive visualization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stonebraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 International Conference on Management of Data</title>
				<meeting>the 2016 International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1363" to="1375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Semiology of graphics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bertin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>University Wisconsin press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Va2: A visual analytics approach for evaluating visual analytics applications</title>
		<author>
			<persName><forename type="first">T</forename><surname>Blascheck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kurzhals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2015.2467871</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="70" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploration strategies for discovery of interactivity in visualizations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Blascheck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Vermeulen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vermeulen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Perin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Willett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carpendale</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2018.2802520</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1407" to="1420" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Investigating aspects of data visualization literacy using 20 information visualizations and 273 science museum visitors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Börner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maltese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Balliet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heimlich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Visualization</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="198" to="213" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Suggested interactivity: Seeking perceived affordances for information visualization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Boy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eveillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Detienne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fekete</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2015.2467201</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="639" to="648" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Finding waldo: Learning about users from their interactions</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ottley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Souvenir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Endert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2014.2346575</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1663" to="1672" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Modelspace: Visualizing the trails of data models in visual analytics systems</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yarlagadda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Endert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Machine Learning from User Interaction for Visualization and Analytics Workshop at IEEE VIS</title>
				<meeting>the Machine Learning from User Interaction for Visualization and Analytics Workshop at IEEE VIS</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visguide: User-oriented recommendations for data event extraction</title>
		<author>
			<persName><forename type="first">Y.-R</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1145/3491102.3517648</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems, CHI &apos;22</title>
				<meeting>the 2022 CHI Conference on Human Factors in Computing Systems, CHI &apos;22<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Human-computer interaction: Design issues, solutions, and applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Card</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mackinlay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">181</biblScope>
		</imprint>
	</monogr>
	<note>Information visualization</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Characterizing guidance in visual analytics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ceneda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gschwandtner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Miksch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Streit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tominski</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2016.2598468</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="111" to="120" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Query steering for interactive data exploration</title>
		<author>
			<persName><forename type="first">U</forename><surname>Cetintemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cherniack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Debrabant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dimitriadou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kalinin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Papaemmanouil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Zdonik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIDR</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Supporting story synthesis: Bridging the gap between visual analytics and storytelling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Turkay</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2018.2889054</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2499" to="2516" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the semantics of interactive visualizations</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Chuah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.1109/INFVIS.1996.559213</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Symposium on Information Visualization &apos;96</title>
				<meeting>IEEE Symposium on Information Visualization &apos;96</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="29" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Comparing and combining interaction data and eye-tracking data for the real-time prediction of user cognitive abilities in visualization tasks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Conati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lallé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Toker</surname></persName>
		</author>
		<idno type="DOI">10.1145/3301400</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Interact. Intell. Syst</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020-05">May 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Context-free grammar forms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ginsburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="86" to="117" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A grammar-based approach for modeling user interactions and generating suggestions during the data exploration process</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dabek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Caban</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2016.2598471</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="50" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">What is interaction for data visualization?</title>
		<author>
			<persName><forename type="first">E</forename><surname>Dimara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Perin</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2019.2934283</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="129" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast and Accurate CNN-based Brushing in Scatterplots</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hauser</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.13405</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Personalized sketch-based brushing in scatterplots</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hauser</surname></persName>
		</author>
		<idno type="DOI">10.1109/MCG.2018.2881502</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="28" to="39" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Narrative visualization: A case study of how to incorporate narrative elements in existing visualizations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Figueiras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 18th International Conference on Information Visualisation</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="46" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Characterizing users&apos; visual analytic activity for insight provenance</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gotz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Visualization</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="42" to="55" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional lstm and other neural network architectures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A case study using visualization interaction logs and insight metrics to understand how analysts arrive at insights</title>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ziemkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Laidlaw</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2015.2467613</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="60" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Embracing change: Continual learning in deep neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2020.09.004</idno>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1028" to="1040" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Agency plus automation: Designing artificial intelligence into interactive systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1844" to="1850" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Sciences</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Emdialog: Bringing information visualization into the museum</title>
		<author>
			<persName><forename type="first">U</forename><surname>Hinrichs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carpendale</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2008.127</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1181" to="1188" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Visualizing biological data in museums: Visitor learning with an interactive tree of life exhibit</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Block</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Diamond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Research in Science Teaching</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="895" to="918" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">On information and sufficiency. The annals of mathematical statistics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kullback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Leibler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1951">1951</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A deeper understanding of visualization-text interplay in geographic data-driven stories</title>
		<author>
			<persName><forename type="first">S</forename><surname>Latif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Beck</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.14309</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="311" to="322" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Predicting human performance in vertical menu selection using deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bailly</surname></persName>
		</author>
		<idno type="DOI">10.1145/3173574.3173603</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Sentiment analysis and subjectivity. Handbook of natural language processing</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="627" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Living liquid: Design and evaluation of an exploratory visualization tool for museum visitors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Frazier</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2012.244</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2799" to="2808" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Next-step suggestions for modern interactive data analysis platforms</title>
		<author>
			<persName><forename type="first">T</forename><surname>Milo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Somech</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="576" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Falcon: Balancing interactive latency and resolution sensitivity for scalable linked visualizations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Howe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2019 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">revisit: Looking under the hood of interactive visualization studies</title>
		<author>
			<persName><forename type="first">C</forename><surname>Nobre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wootton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lex</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2021 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Follow the clicks: Learning and anticipating mouse interactions during exploratory data analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ottley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
				<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="41" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Personality as a predictor of user strategy: How locus of control affects search strategies on tree visualizations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ottley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1145/2702123.2702590</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems, CHI &apos;15</title>
				<meeting>the 33rd Annual ACM Conference on Human Factors in Computing Systems, CHI &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3251" to="3254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Software framework for topic modelling with large corpora</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rehurek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sojka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC 2010 workshop on new challenges for NLP frameworks</title>
				<meeting>the LREC 2010 workshop on new challenges for NLP frameworks</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Vega-lite: A grammar of interactive graphics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Satyanarayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wongsuphasawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2016.2599030</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="341" to="350" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Reactive vega: A streaming dataflow architecture for declarative interactive visualization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Satyanarayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2015.2467091</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="659" to="668" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Designing an exploratory visual interface to the results of citizen surveys</title>
		<author>
			<persName><forename type="first">A</forename><surname>Slingsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dykes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Radburn</surname></persName>
		</author>
		<idno type="DOI">10.1080/13658816.2014.920845</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Geographical Information Science</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2090" to="2125" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Characterizing exploratory behaviors on a personal visualization interface using interaction logs</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Sukumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>D'mello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mattingly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Striegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EuroVis</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Provectories: Embedding-based analysis of interaction provenance data</title>
		<author>
			<persName><forename type="first">C</forename><surname>Walchshofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hinterreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Streit</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A markov model of users&apos; interactive behavior in scatterplots</title>
		<author>
			<persName><forename type="first">E</forename><surname>Wall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Arcalgud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE visualization conference (VIS)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="81" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Interaction spaces in data and information visualization</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VisSym</title>
				<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="137" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Putting the &quot;i&quot; in interaction: Interactive interfaces personalized to individuals</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wenskovitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dowling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Endert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1109/MCG.2020.2982465</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="73" to="82" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Scented widgets: Improving navigation cues with embedded visualizations</title>
		<author>
			<persName><forename type="first">W</forename><surname>Willett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2007.70589</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1129" to="1136" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Lifeflow: Visualizing an overview of event sequences</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wongsuphasawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Guerra Gómez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Plaisant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taieb-Maimon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
		<idno type="DOI">10.1145/1978942.1979196</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI &apos;11</title>
				<meeting>the SIGCHI Conference on Human Factors in Computing Systems, CHI &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1747" to="1756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Multivision: Designing analytical dashboards with deep learning based recommendation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="162" to="172" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Cai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.01691</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Survey on the analysis of user interactions and visualization provenance</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ottley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Walchshofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Streit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wenskovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
				<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="757" to="783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Toward a deeper understanding of the role of interaction in information visualization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">A</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Jacko</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2007.70515</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1224" to="1231" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Annotation graphs: A graph-based visualization for meta-analysis of data based on user-authored annotations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Glueck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Breslav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chevalier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khan</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2016.2598543</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="261" to="270" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Modeling and leveraging analytic focus during exploratory visual analysis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gotz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2021 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
