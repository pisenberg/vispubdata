<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FoVolNet: Fast Volume Rendering using Foveated Deep Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">David</forename><surname>Bauer</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kwan-Liu</forename><surname>Ma</surname></persName>
						</author>
						<title level="a" type="main">FoVolNet: Fast Volume Rendering using Foveated Deep Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-03-27T09:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Volume data</term>
					<term>volume visualization</term>
					<term>deep learning</term>
					<term>foveated rendering</term>
					<term>neural reconstruction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. 1: We propose a novel rendering pipeline for fast volume rendering using optimized foveated sparse rendering and deep neural reconstruction networks. FoVolNet can faithfully reconstruct visual information from sparse inputs. With FoVolNet, developers are able to significantly improve rendering time without sacrificing visual quality.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Since its beginnings, volume rendering has been an integral part of the scientific and biomedical visualization community. Over time, tremendous improvements have been made to the quality and performance of volume rendering algorithms. Yet, with advances in high-fidelity rendering comes increased computational cost. Many state-of-the-art techniques like path tracing or global illumination have outpaced the capabilities of consumer hardware, putting these techniques out of reach for interactive applications. There are also other relevant issues in this area, such as data management and storage. However, the visualization community has already produced various methods to mitigate these problems.</p><p>For instance, prior work <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b47">49]</ref> offers solutions for rendering extremely large volumes that do not fit the main memory of a system. Visualization of these data became viable through the introduction of streaming techniques such as out-of-core rendering which eliminate the need for the whole dataset to be present in memory. Such methods are a means of emancipation. They help us depend less on specific characteristics of the data-in this case, its size. When it comes to visual quality, state-of-the-art rendering techniques lack similar means. The higher the visual quality a technique produces, the more computational resources are generally needed to compute it. Although ongoing research has produced more efficient methods over the years, we are still bound by factors like the number of rays, sampling rates, or the type of illumination. Therefore, visualizing volume data using high-quality shading techniques at interactive framerates remains challenging-especially for demanding applications like immersive visualization.</p><p>This work pries open the tight coupling between rendering technique and computational cost. We introduce FoVolNet-a complete volume rendering pipeline that aims to loosen the relation between technique and cost. By skipping the majority of screen-space pixel processing and replacing it with constant-time neural image reconstruction, we can achieve drastic performance improvements without sacrificing visual quality. We take inspiration from literature on foveated rendering and deep learning based image denoising. Prior work <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b50">52,</ref><ref type="bibr" target="#b56">58]</ref> has shown that taking the human visual system (HVS) into account when rendering data can yield excellent results for performance without perceptible quality loss. Utilizing the characteristics of the HVS is a crucial part of our design, as it allows us to concentrate computational resources. Accordingly, FoVolNet renders sparse images with dense foveated areas. A neural image reconstruction network restores the missing visual information, allowing us to skip the majority of screen-space pixel processing and replacing it with a constant-time inference step. This makes it possible to visualize volumes in high quality at a much lower computational cost than conventional rendering methods. In turn, this decoupling allows us to achieve faster and more consistent frame rates in various rendering setups.</p><p>We conduct thorough tests involving the system's overall rendering performance, image quality, and other properties such as effective compression rate to evaluate our approach. The results show that FoVolNet faithfully reconstructs full frames at a fraction of the time it takes conventional methods to produce the same output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Our work is related to topics in volume rendering, optical flow estimation, and deep learning methods in computer graphics and image processing. In this section, we discuss related works in these fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Advanced Volume Rendering</head><p>Optical models for computing advanced illumination effects (e.g., ambient occlusion, global shadows, multi-scattering) in volume rendering were first outlined by Max in the late 1990s <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>. However, since these models are generally expensive to compute, a large body of work has focused on how performance can be improved. The ambient occlusion model <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b43">[45]</ref><ref type="bibr" target="#b44">[46]</ref><ref type="bibr" target="#b45">[47]</ref><ref type="bibr" target="#b49">51]</ref> simulates the occlusion effect within a small neighborhood of the sample point, estimating the local extinction within a small spherical region. More recently, deep neural networks have been used to generate ambient occlusion effects for volume rendering <ref type="bibr" target="#b6">[7]</ref>. However, this model only accounts for local shadows and lacks cues for large-scale occlusions. Computing global shadows require considering attenuation between light sources and the sample points. To efficiently compute global shadows, many different approaches have been proposed, including half-angle slicing <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>, plane sweep <ref type="bibr" target="#b51">[53]</ref>, shadow volume <ref type="bibr" target="#b42">[44]</ref>, light volume <ref type="bibr" target="#b64">[66]</ref>, or voxel cone tracing <ref type="bibr" target="#b46">[48]</ref>. However, these methods still cannot calculate realistic multi-scattering effects. More recently, the use of ray tracing presents a new trend of volume visualization algorithms that implement a highly realistic multi-scattering model <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b38">40]</ref>. By combining them with production ray-tracing software <ref type="bibr" target="#b55">[57]</ref> and realistic BRDF classification techniques <ref type="bibr" target="#b18">[19]</ref>, unbiased volume rendering can finally be achieved. However, these algorithms are computationally costly when high-resolution data-which requires a high sampling rate-and complex lighting conditions are combined. Thus, the rendering performance of these methods can quickly deteriorate. In this work, we lift a sizeable portion of the computational burden that such techniques incur on modern hardware. We reduce the screen space sample count and replace the skipped computations with a constant-time neural network inference step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Foveated Rendering</head><p>Recent advances in eye-tracking technology and the market push towards augmented and virtual reality applications have intensified research in foveated rendering techniques. Computational power is crucial for high-fidelity rendering applications and most of today's immersive content. It is therefore of paramount importance to distribute resources efficiently. The capacity of the human visual system to perceive high levels of detail is limited to a relatively small focal area <ref type="bibr" target="#b0">[1]</ref>. The fovea, which is the area of the visual field with the highest acuity, only makes up about 5.2 degrees around the optical axis of the eye <ref type="bibr" target="#b57">[59]</ref>. Foveated rendering approaches use that fact by focusing computational resources on these areas. Guenter et al. <ref type="bibr" target="#b11">[12]</ref> were one of the first to develop such an approach by rendering scenes in multiple levels of detail in concentric circles around the focus point. Later approaches <ref type="bibr" target="#b50">[52]</ref> use variable sampling rates that prioritize the focal area. Weier et al. <ref type="bibr" target="#b56">[58]</ref> combine this approach with frame reprojection to reduce peripheral flickering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Deep Learning for Image Denoising</head><p>One of the prominent uses of deep learning in the computer graphics field is image denoising. It is the process of refining noisy images, which are usually the result of Monte Carlo (MC) renderings with a low number of samples per pixel (SPP). A primitive approach to improving image quality is to increase SPP. However, this method requires significantly longer processing times per frame. Recent work has leveraged deep learning to refine low SPP images without this computational overhead. Early approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref> already achieve impressive results using CNNs. These works have established the two fundamental philosophies of image denoising in today's literature. On the one hand, there is direct prediction <ref type="bibr" target="#b3">[4]</ref>. A method to produce denoised images as a direct result of network inference. On the other hand, kernel prediction <ref type="bibr" target="#b1">[2]</ref> approaches use CNNs to produce image filters. The denoising operation is performed by applying these filters to the input image in a separate step.</p><p>Subsequent work followed in these footsteps, furthering the potential of these two concepts. Wong et al. <ref type="bibr" target="#b60">[62]</ref> introduce residual connections for direct prediction networks to improve single-frame image quality. To the same end, Xu et al. <ref type="bibr" target="#b62">[64]</ref> and Lu et al. <ref type="bibr" target="#b31">[32]</ref> conducted experiments on using adversarial networks <ref type="bibr" target="#b10">[11]</ref> to train direct prediction models. More recently, Hofmann et al. <ref type="bibr" target="#b16">[17]</ref> have applied direct prediction to the domain of volume path tracing. Similarly, Weiss et al. <ref type="bibr" target="#b58">[60]</ref> explore the utility of direct prediction for reconstructing adaptive volume ray marching. Along with works like Kettunen et al.'s gradient-space denoising <ref type="bibr" target="#b21">[22]</ref> and Wong et al.'s ResNet approach <ref type="bibr" target="#b60">[62]</ref>, they investigate the effect of various auxiliary input features on final image quality. Following the kernel prediction path <ref type="bibr" target="#b1">[2]</ref>, we see work by Vogels et al. <ref type="bibr" target="#b54">[56]</ref> who extend the approach by incorporating neighboring frames into training to facilitate temporal stability. Hasselgren et al. <ref type="bibr" target="#b14">[15]</ref> build on this notion, creating temporally stable image sequences using predictive adaptive sampling and temporal blending. Gharbi et al. <ref type="bibr" target="#b9">[10]</ref> solve problems involving motion blur and depth-of-field using a kernel prediction approach with splatting.</p><p>Neural architectures used in these projects vary. However, there are certain identifiable trends. The U-Net architecture <ref type="bibr" target="#b41">[43]</ref>, initially developed for medical image segmentation tasks, has proved to be a practical choice for denoising tasks. Many works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22]</ref> base their design on the U-Net's image encoder-decoder principle. Extensions often include skip connections and recurrent feedback, which tend to increase image quality and temporal stability. Other works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b54">56,</ref><ref type="bibr" target="#b60">62,</ref><ref type="bibr" target="#b62">64]</ref> use more conventional CNN or RNN models. Interestingly, there is no apparent connection between chosen architecture (U-Net, CNN, RNN) and the denoising approach (direct prediction, kernel prediction). Several works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b62">64]</ref> also use an additional critique network for their adversarial training. Named after its shape, the W-Net poses an extension to the U-Net and was introduced by Thomas et al. <ref type="bibr" target="#b53">[55]</ref>. It comprises two U-Nets in sequence. One is used for feature extraction, while the other serves to generate and apply convolutional filters to the input. This design facilitates optimization through selective quantization without significant image quality loss. For further reading on this topic, we refer the interested reader to Huo et al.'s survey on deeplearning-based image denoising techniques <ref type="bibr" target="#b17">[18]</ref>. Our network design is based on the W-Net architecture. We introduce a hybrid approach combining direct and kernel prediction to achieve the best results for sparse image inputs. This differs from conventional image denoising approaches in that missing visual information needs to be generated by the network. Therefore, pure kernel prediction networks are not suitable for this task as kernels only operate on existing pixel values. DeepFovea <ref type="bibr" target="#b20">[21]</ref> is the current state-of-the-art for such sparse frame reconstructions using solely direct prediction. Our approach translates this initial idea to the domain of scientific rendering and significantly improves visual quality and performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Optical Flow</head><p>Perceived motion in video sequences results from incremental changes in the positions of objects in a scene or by camera movement. Estimating the optical flow of elements in adjacent frames is an active area of research, and various approaches have been proposed in recent years.  An early approach by Farnebäck et al. <ref type="bibr" target="#b7">[9]</ref> introduces a motion estimation algorithm that characterizes pixel neighborhoods as polynomials and uses those to find a mapping between frames. They propose a multiscale approach that uses a priori motion estimation. This allows the algorithm to iterate and refine the estimation by considering differently sized search windows. This increases the robustness and quality of the results. Subsequent works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b52">54]</ref> tend to emulate this iterative, hierarchical approach. Most recently, Hanika et al. <ref type="bibr" target="#b13">[14]</ref> have introduced a method based on this scheme. Unlike previous approaches, this algorithm sacrifices some quality in favor of speed. It also manages disocclusions gracefully.</p><p>For our work, we utilize Hanika et al.'s approach <ref type="bibr" target="#b13">[14]</ref> to reproject frames during training. By warping a previous frame, we can gain more visual information about the current image, which can be used to construct a loss function that requires the network to match reprojected frames <ref type="bibr" target="#b20">[21]</ref>. This additional information helps increase image quality and supports retaining temporal stability between frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODS</head><p>FoVolNet is a complete raymarching volume rendering pipeline that is supported by a neural network (Figure <ref type="figure" target="#fig_0">2</ref>). The overall rendering process consists of two critical steps. First, the volume needs to be rendered. Instead of rendering the whole frame, we selectively render a subset of pixels. A neural network is then used to reconstruct the full frame from this subset. The following sections contain details on our approach. There, we discuss the implementation and design of each stage of FoVolNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Foveated Rendering</head><p>We implement a ray marching system that facilitates sparse, foveated rendering. The renderer is implemented in CUDA and OptiX and supports global ray marched shadows. For the shadow computation we cast one shadow ray per sample step towards the light source using 1  4 of the main sample rate. This renderer allows us to reduce rendering time as overall pixel density decreases. Our foveated rendering technique is based on binary sample maps generated from noise patterns that determine which pixels in screen space should be sampled by the volume renderer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Noise Patterns</head><p>The noise patterns used in this work (Figures <ref type="figure" target="#fig_1">3, 4</ref> (left)) can be tiled seamlessly which allows us to cover an arbitrarily large frame. In our experiments, we tested noise map tile sizes that ranged from 16 × 16 to 256 × 256 pixels; however, there was no noticeable difference in the final image quality.</p><p>A comparison of different sources of noise is shown in Figure <ref type="figure" target="#fig_1">3</ref>. Using noise sampled from a uniform distribution, like the one shown on the left, can result in energy spikes across the pattern. This is generally not desirable as it causes samples in the sampling map to be unevenly distributed. The temporal mean of uniform noise exhibits similar problems. Blue noise (Figure <ref type="figure" target="#fig_1">3 (b)</ref>) is rich in high frequencies and generally does not suffer from low-frequency energy spikes in the spatial domain. Its distribution closely models that of the visual receptors on our retina and is therefore ideal for creating perceptually unobtrusive sampling patterns. However, conventional blue noise suffers from temporal instability, as can be seen in Figure <ref type="figure" target="#fig_1">3 (e)</ref>.</p><p>We use spatio-temporal blue noise (STBN) <ref type="bibr" target="#b59">[61]</ref> to generate temporally stable sample patterns while preserving the perceptual advantages of conventional blue noise (Figure <ref type="figure" target="#fig_1">3</ref> (c), (f)). As opposed to sequences of independent 2D blue noise or 3D blue noise patterns, STBN is not only blue in the spatial domain -every pixel is blue over time. This property is desirable since it helps our reconstruction network to produce stable and perceptually clean image sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Sample Maps</head><p>To generate a sample mask M, we compare the noise value N(u, v) at position u, v against a threshold τ. If N(u, v) &lt; τ we set M(u, v) to 1; otherwise, it is set to 0. The value for τ can be adjusted to vary the sampling density. We define the density of the base noise pattern as P b (u, v). The foveated area is generated by modulating the value of τ using an exponential function around the focal point (Figure <ref type="figure">4</ref> (middle)). Changing the variance σ , changes the size of the foveated area. The density of the foveated area is denoted by P f (u, v).</p><formula xml:id="formula_0">P f (u, v) = e −0.5( f 2 x + f 2 y )σ<label>(1)</label></formula><p>where f x and f y are the current focal position. Combining both components, we calculate τ as follows.</p><formula xml:id="formula_1">τ(u, v) = (1 − P b (u, v)) • P f (u, v) + P b (u, v) (2)</formula><p>With the resulting sampling mask, the volume is selectively rendered at all positions (u, v) where M(u, v) = 1 (Figure <ref type="figure">4</ref> (right)).</p><p>Fig. <ref type="figure">4</ref>: Sampling maps are generated using an STBN <ref type="bibr" target="#b59">[61]</ref> (left). The area around the focal point is sampled more densely using an exponential fall-off to preserve details (middle). The volume is sparsely sampled using the sampling map (right).</p><p>This process is repeated for every new frame that is rendered. To guarantee uniform sampling and maximize the amount of visual information that can be accumulated over time, the underlying blue noise maps are changed every time. Due to the computational complexity of blue noise generation, we use a pre-calculated series of 64 noise tiles. We loop the series to render frame sequences of arbitrary length.</p><p>Sampling only a small subset of rays can drastically reduce the computation time per frame. We define the maximum possible compression rate C max of the technique as follows.</p><formula xml:id="formula_2">C max = 1 h • w h,w ∑ u=0,v=0 τ(u, v)<label>(3)</label></formula><p>where h and w are the spatial dimensions of the framebuffer. P b and P f are the probability of casting a ray at u, v as described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Rendering</head><p>In a naive implementation of the renderer in OptiX and CUDA, invalid pixels would simply be discarded on the kernel level. However, using one kernel thread per full-size framebuffer pixel will yield only negligible performance gains. This is because GPU kernel calls are grouped in warps. Results are available only after all threads in a warp conclude. We develop two methods to circumvent this issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Direct Sampling</head><p>For this method, we create a stochastic function P that incorporates both P b and P f . It adapts over time as the noise pattern and the location of the fovea change. The function can be called to generate a position u, v within the bounds of the framebuffer. Specific values for u and v are dependent on both probability functions. Therefore, it is more likely to generate a position in and around the foveated area. When rendering a new frame, we allocate a small framebuffer in which the number of pixels corresponds to the desired sampling compression C max which means that each kernel thread contributes calculations without potentially being discarded. For each entry in this buffer, the renderer queries P to determine which pixel to render to the compact frame buffer. The values for (u, v) relate to a pixel's position in the full-size framebuffer. Therefore, the result is a compact representation of the full frame. The contents of this compact buffer are projected back to their respective positions in the full-size framebuffer to generate the sparse image. Direct sampling is relatively unintrusive in terms of pipeline integration. Instead of issuing a CUDA kernel run across the full image dimensions, we call it on a smaller buffer. Function P acts as a proxy when accessing the screen-space coordinates in each GPU thread. The downside of this method is that P does not reliably produce unique sampling positions. Although direct sampling performs better than the naive approach, we encounter duplicate coordinates quite frequently, making the renderer recompute existing samples, forfeiting potential compression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Render Compaction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Volume</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Projection</head><p>Rendered Foveated Frame Sparse Sampling Map Fig. <ref type="figure">6</ref>: The renderer first creates a sampling map which is compacted into a small framebuffer. The volume is sampled according to each pixel's ray direction in the full-size frame. After rendering, the resulting color values are projected back to the initial frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.5">Stream Compaction</head><p>To alleviate the problems with direct sampling, we separate the sample map generation from the actual sampling. In the first step, the sampling map is generated using the approach described in Section 3.1.2. Next, a stream compaction algorithm is used to remove sparsity in the sampling map (Figure <ref type="figure">6</ref>). This allows us to pack all valid pixels in the map into a smaller framebuffer similar to the direct sampling approach. The rendering is performed on this smaller framebuffer, and the same back-projection mechanism restores the full frame image (Figure <ref type="figure">6</ref>).</p><p>The mask generation and compaction steps are implemented in CUDA to accelerate the process. Using this approach, we effectively circumvent the problems encountered in direct sampling and are therefore able to further approach the theoretical C max .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Reconstruction Network</head><p>The core of FoVolNet is a deep neural network. We have developed a two-stage hybrid architecture that is based on the W-Net architecture <ref type="bibr" target="#b53">[55]</ref>. It draws ideas from both direct prediction and kernel prediction approaches. We specifically design this network to accommodate sparse frames with minimal input features. Images are reconstructed solely from RGB input-optical flow or other auxiliary features are not required. The network's components are described in detail here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Overall Design</head><p>The core idea of our design is to split the reconstruction process into two steps. The split is realized by running two networks in sequence (Figure <ref type="figure" target="#fig_3">7</ref>).</p><p>This hybrid architecture employs both direct and kernel prediction. Without the initial reconstruction step, the kernel prediction method fails to perform due to the absence of rich pixel neighborhoods from which to draw visual information. On the other hand, performing only the direct prediction step would result in sub-par image quality, as we show in the evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Direct Prediction: Coarse Image Reconstruction</head><p>Network D (Figure <ref type="figure" target="#fig_3">7</ref>) reconstructs the image using direct prediction. That is, its output O d is directly interpretable as an image. This step reconstructs the overall features of the frame and fills the blank spots between valid pixels in the input. In addition to this, we preserve the decoder's hidden state H d for further processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Kernel Prediction: Image Refinement</head><p>In the second reconstruction step, network K predicts convolutional kernels on multiple scales in both encoder and decoder stages. They are then applied in sequence to O d . The kernel prediction stage takes  the hidden states H d of D s decoder stage (Figure <ref type="figure" target="#fig_3">7</ref>) and forwards them to K s convolution blocks in both the encoder and decoder stages. The convolution blocks are used to predict filter kernels from H d . Network K s input image is passed through the network by applying each block's predicted filter to the image in sequence. This process is analogous to the original W-Net filtering approach <ref type="bibr" target="#b53">[55]</ref>. This step allows us to remove any remaining artifacts and blurriness that might result from direct prediction. Using the pre-filtered output O d , we provide visual context for the filters to refine the image meaningfully. When using the original sparse image as input for this stage, we saw blotchy artifacts in areas with insufficient visual information to cleanly filter the image, and more densely sampled areas generally looked blurrier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Recurrence</head><p>We introduce recurrent connections in multiple parts of the network to accumulate state. The aggregated information aids the reconstruction of temporally stable image sequences.</p><p>Decoder blocks in network D are connected by recurrent connections that pass down the block's output hidden state back to its input in the next training step. On a broader scale, the output O d of network D is passed back to its input layer as part of the data of the subsequent run. Current and recurrent states are combined using a concatenation operation along the channel dimension, and appropriate up-or down-sampling is applied to make all inputs compatible for subsequent operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Loss</head><p>Model optimization was performed using a linear combination of multiple loss components. We split the training loss into a spatial and a temporal component which we label L s and L t , respectively. The losses are defined as follows.</p><formula xml:id="formula_3">L = λ s L s + λ t L t (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where λ s,t denote the linear weights assigned to the loss. In our training we choose λ s = 0.8 and λ t = 1.0 which-given the losses' different magnitudes-weights spatial and temporal components at a ratio of 10:1. This balance of image quality and temporal stability was ideal for our trainings but might vary per training dataset. For L s we use a combination of L 1 and VGG19-based LPIPS perceptual loss <ref type="bibr" target="#b63">[65]</ref> terms (Figure <ref type="figure" target="#fig_4">8</ref>). The temporal loss is a combination of L 1 loss and optical flow (OF) loss as used by Kaplanyan et al. <ref type="bibr" target="#b20">[21]</ref>.</p><formula xml:id="formula_5">L s = λ 1 LPIPS + λ 2 L 1 and L t = λ 3 L 1 + λ 4 OF<label>(5)</label></formula><p>We choose λ 1 = 0.9, λ 2 = 0.1, λ 3 = 1.0, and λ 4 = 0.1 for the linear weights to equalize the components' magnitudes. Overall, the perceptual loss alone provides good reconstruction quality; however, adding a small L 1 term helps preserve some more high-frequency details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Spatial Loss</head><p>During training, both L s and L t are applied across the whole series of images in a sequence. Both losses are computed for each time step and the model weights are updated once after a full sequence of loss values has accumulated. Given any pair of predictions y p and ground truth targets y g with t total time steps, we calculate L s as follows.</p><formula xml:id="formula_6">L s (y g , y p ) = t ∑ i=0 (1 − e −0.5i ) • (λ 1 LPIPS(y gi , y pi ) + λ 2 ||y pi − y gi || 1 )<label>(6</label></formula><p>) Early images in the sequence are exponentially down-weighted to account for errors due to the lack of recurrent state at the beginning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Temporal Loss</head><p>Using L s alone provides good single frame reconstruction quality. However, as previous studies <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b54">56]</ref> have noted, it results in temporal flickering. Hasselgren et al. <ref type="bibr" target="#b14">[15]</ref> have shown that adding a simple L 1 term helps reduce temporal flickering drastically, but we have found their method to be prone to tearing artifacts when there is fast movement between adjacent frames. We add a small optical flow term, as used by Kaplanyan et al. <ref type="bibr" target="#b20">[21]</ref> to stabilize such cases. The first component forces the network to produce adjacent frames with finite differences similar to the output. The optical flow loss works by comparing the current frame against its predecessor. The previous frame is warped using the optical flow φ (i−1)→i with the warping operator ω to match the current frame. The network has to match this warped frame, leading to less tearing in the final output as consecutive frames become similar to their respective predecessors. For a sequence of t images L t is defined as follows.</p><formula xml:id="formula_7">L t (y g , y p ) = t ∑ i=1 i−1 ∑ j=0 λ 3 ||(y pi − y p j ) − (y gi − y g j )|| 1 + t ∑ i=1 λ 4 ||y pi − ω(y pi−1 , φ (i−1)→i )|| 1 (7)</formula><p>In our training, the first loss term is defined over the whole sequence of prior frames. This way of constructing the loss emphasizes later image pairs in the sequence which entices training to use recurrent connections. On the other hand, the optical flow loss is only applied to a frame's direct predecessor as warping frames becomes harder and more prone to errors the farther they are apart temporally. We use Hanika et al.'s fast reprojection algorithm <ref type="bibr" target="#b13">[14]</ref> to estimate optical flow between frames during training. For both components of L t , we do not consider the first frame of the sequence as it has no viable predecessor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Precision &amp; Optimization</head><p>Initially, we train the network using full 32-bit floating-point precision. However, the computational cost of running a full-precision network is often unnecessarily high. We truncate the network's weights to 16-bit half precision format as a first optimization step. This operation does not cause any noticeable performance loss.</p><p>We also experiment with post-training quantization on a pre-trained model. In this process, we initially train the model in full-precision mode. After this, the precision of the network is reduced, and training continues using half-precision. In contrast to similar works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b53">55]</ref>, we do not simulate integer quantization <ref type="bibr" target="#b25">[26]</ref>, as we observed drastic deterioration of image quality using this approach on our data. This is likely due to the sparsity of the input and the reliance on network D to contribute to the final output instead of just extracting features.</p><p>Post-training quantization is realized using TensorRT <ref type="bibr" target="#b37">[38]</ref>. Models are trained in PyTorch [8] and exported to ONNX format. We then transform the ONNX network to a CUDA inference engine using the tools provided by TensorRT <ref type="bibr" target="#b37">[38]</ref>. In our trials, we experiment with different levels of optimization. Namely, we choose from different numerical precisions: float32, float16, int8, and mixed-mode. We compare different settings in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training</head><p>We provide information about network configuration, hyperparameters, and details regarding the dataset used for training in this section. The training was performed on two NVIDIA Quadro RTX 8000 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Model Configuration</head><p>Both sub-networks D and K of our reconstruction network are based on the U-Net design <ref type="bibr" target="#b41">[43]</ref>. Each network has four encoder and three decoder blocks. Skip connections connect the blocks. In network D, each block consists of two convolutional layers of equal depth, followed by a ReLU activation. On the other hand, each block in network K only has a single convolution to predict the kernels. Both networks follow the same progression of block configurations which is defined as:</p><formula xml:id="formula_8">e64 − e64 − e80 − d96 − d80 − d64 − d64 (8)</formula><p>where e and d denote encoder/decoder blocks followed by the convolution depth used for Conv2D layers in the block. Blocks in the encoder stage conclude with an average pooling layer to downscale the image. Analogously, all except the final block in the decoder part up-sample their outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Dataset</head><p>FoVolNet is trained on short video sequences of several pre-rendered volume datasets. The data covers CT scans of humans, animals, mechanical parts, and large-scale simulation data from astronomy and material sciences (Table <ref type="table">1</ref>). We render the datasets at a resolution of 800x800 using the previously described renderer. Video sequences consist of a continuous camera fly-through around the volume to cover most angles of the data. For training, the video dataset is sliced into 16-frame segments with no overlaps, and images are tiled at a resolution of 256x256. We find that these spatiotemporal dimensions offer the best trade-off between training time and quality. Sequences with eight or fewer frames resulted in under-utilization of recurrent connections and, therefore, bad temporal coherence. Batches consist of 15 such 16x256x256 sequences. In total, the network is trained on 16000 unique images. Our validation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Dimensions Data Type SKULL <ref type="bibr" target="#b29">[30]</ref> 256x256x256 uint8 CHAMELEON <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b33">34]</ref> 1024x1024x1080 uint8 MECHANICAL HAND 640x220x229 float32 VORTICES 1 <ref type="bibr" target="#b48">[50]</ref> 128x128x128 float32 VORTICES 2 <ref type="bibr" target="#b48">[50]</ref> 128x128x128 float32 SUPERNOVA <ref type="bibr">[39]</ref> 432x432x432 float32</p><p>Table <ref type="table">1</ref>: Datasets used to train and evaluate FoVolNet. Using our ray marching renderer, each volume was rendered as a camera fly-through sequence around the object. The skull and chameleon datasets were used for evaluation and were not part of the training datasets.</p><p>data consists of 1600 unique images from the same datasets. Training, validation, and test datasets were split randomly at a 10:1:1 ratio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.3">Data Augmentation</head><p>We use data augmentation, which effectively increases the number of unique training sequences we provide to the network. During training, sequences of frames are subject to random augmentations to improve training effectiveness. Table <ref type="table">2</ref> shows the list of augmentations used during training. Here, P(x) denotes the independent probability of each augmentation occurring for any given batch of data.  <ref type="table">2</ref>: List of image augmentations applied during training. All augmentations affect the whole sequence of images to not introduce any unwanted combinations of effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Name</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.4">Hyperparameters</head><p>During development, we experimented with different sets of hyperparameters to empirically determine the best settings for training. These include initial learning rate (LR), learning rate schedule, optimizer, weight decay, and length of training. For the final version, we use the following setup. The LR is set to an initial value of − 3, and a cosine annealing LR schedule is applied to gradually reduce the LR to a minimum value of 1e − 8. We use the Ranger <ref type="bibr" target="#b61">[63]</ref> optimizer with a weight decay set to 1e − 2 to stabilize training. The model usually converges at around epoch 60-80. All trainings are stopped after 120 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION</head><p>To show the potential of FoVolNet, we conduct several tests to evaluate different aspects of the system. All evaluations are conducted using our custom foveated rendering pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">System Setup</head><p>We use C++ backends for both PyTorch [8] and TensorRT <ref type="bibr" target="#b37">[38]</ref> for inference. All evaluations were performed on an end-user machine with an Intel Core i7-6900K CPU with 128 gigabytes of RAM and an NVIDIA Titan RTX GPU. The system runs Ubuntu 20.04 LTS, and all parts of FoVolNet were developed and compiled on Linux. All frames are rendered at a resolution of 1280 × 720. We use our ray marching renderer and enable global shadows using a single light source per scene. Layer weights are truncated to fp16 precision, unless otherwise specified. All output was produced using images that were not in the training set.</p><p>Some results show comparisons to DeepFovea-the current stateof-the-art of foveated sparse frame reconstruction <ref type="bibr" target="#b20">[21]</ref>. We train a model of this architecture on our data in our training pipeline using hyperparameters as suggested by the authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Inference Speed &amp; Pipeline Throughput</head><p>To test the rendering throughput of our system, we use a fixed-path camera fly-through in our rendering pipeline. The camera path consists of a pattern of oscillating zoom with continuous rotation in two axes. This allows us to cover most aspects of the data using a small number of frames. Please refer to the supplemental video for more details. Fig. <ref type="figure">9</ref>: The per-frame timings of different pipeline components during a camera fly-through of VORTICES 2. We compare times for using fp16 precision with conventional DVR as the reference. Thumbnails show the camera position at that specific point in the run.</p><p>A comparison of conventional raymarching with FoVolNet is shown in Figure <ref type="figure">9</ref>. We perform the fly-through mentioned above on the Vortices 2 dataset. Baseline rendering time is drastically reduced due to sparse, foveated sampling of the volume. A constant, sceneindependent inference time adds to the total frame time. The result is a sequence of fast and stable frame times that is less dependent on camera angle or scene configuration. As the thumbnails in Figure <ref type="figure">9</ref> suggest, the more screen space is occupied by data, the larger the benefit of using our technique. However, as the left-most image shows, we achieve roughly two times faster end-to-end rendering performance even from a far-away viewing position.</p><p>A more comprehensive analysis of rendering performance is shown in Figure <ref type="figure">10</ref>. Here, we compare against DeepFovea <ref type="bibr" target="#b20">[21]</ref>. The sequences were created at two different quality settings, which differ in their configuration of sampling density. The hatched parts of the deep learning based runs indicate the inference times. We report the resulting average end-to-end speedups for all datasets in Table <ref type="table">3</ref>. Note that the fly-through sequences are all composed of roughly equal parts far-away and close-up viewing positions. This is due to the oscillating zoom of the camera. Therefore, our results represent speedups that can be expected in the average case. However, if the data is viewed at a reasonably close angle like shown in Figure <ref type="figure">1</ref>, speedups are generally much higher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Image Quality</head><p>Final image quality is at least as important as inference speed when it comes to image reconstruction. For all datasets, we calculate structural similarity (SSIM) and peak signal to noise ratio (PSNR) on single frames and image sequences. The image matrix in Figure <ref type="figure">11</ref> shows results for both foveated areas and the periphery on single frames. Our architecture can reconstruct fine details even in peripheral areas of the frame. Notice how pure direct prediction methods like the recurrent U-Net used in DeepFovea <ref type="bibr" target="#b20">[21]</ref> fail to preserve high-frequency details as the number of samples decreases. Fig. <ref type="figure">10</ref>: Average frame times from fly-through renderings of different datasets. Each dataset was rendered for 500 frames. The camera movement was framerate-independent. We compare against DeepFovea <ref type="bibr" target="#b20">[21]</ref> as specified by the authors. Note that the x-axis scales logarithmically past frame 100 to accommodate long frame times.</p><p>Table <ref type="table">3</ref>: Relative speed-up times when compared to the baseline raymarching renderer. The data is based on that shown in Figure <ref type="figure">10</ref>. P b is given and P f was calculated using the listed σ values.  For the video analysis, we create a camera fly-through video of the CHAMELEON dataset. We split the video into multiple 50-frame sequences (of which we show two) with a jump-cut between them. Figures <ref type="figure" target="#fig_8">12 and 13</ref> show the reconstruction quality over time. A red line indicates the cut. Both FoVolNet and direct prediction improve their quality over a short ramp-up period in which state is accumulated. Similarly, the network needs several frames after the jump cut to recover full quality. However, the hybrid architecture outperforms direct prediction by a constant offset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Temporal Stability</head><p>The quality of temporal coherence is evaluated on the same fly-through clips. The sequences were constructed so that they each start and end in fast camera movement while slowing down towards the middle. This three-act setup allows us to see how the network uses accumulated state to retain temporal consistency when there is (1) plenty of movement with little prior state, (2) little movement but lots of state, and lastly, (3) lots of movement and lots of state. We compute the temporal PSNR (tPSNR) as used by <ref type="bibr">Hasselgren et al. [15]</ref>. This value is the PSNR of finite differences between frames. Instead of computing the PSNR on the image itself, we compute it on the difference between the current and previous frame. The aforementioned fast-slow-fast pattern is reflected by data in Figure <ref type="figure">14</ref>. It shows that FoVolNet is able to retain stability throughout most of the sequences. Both models achieve peak quality when there is little movement. This is unsurprising since, without movement, the network acts as a simple accumulation buffer. However, FoVolNet is able to retain quality under much faster   movement than direct prediction-especially in Phase (3) when there is enough state available to the layers. Please refer to the supplemental material for the source clips.</p><p>In addition to this metric, we provide just-objectionable-difference (JOD) <ref type="bibr" target="#b40">[42]</ref> scores for the whole video. This score is similar to justnoticeable-difference (JND), but instead of quantifying the difference between pairs of images, it is better suited to compare multiple degraded images to the reference. That means that while the results of different reconstruction methods might look degraded in different ways, they will still have similar JOD scores as they are equally different from the ground truth. The data was produced using FovVideoVDP <ref type="bibr" target="#b34">[35]</ref>. We use the default settings for a 4K screen viewed under office light levels. Detailed settings were chosen as follows: 75.4 [pix/deg], Lpeak=200, Lblack=0.5979 [cd/m 2 ], non-foveated, (standard 4k). Data is produced for the video at different quality settings as shown in Table <ref type="table" target="#tab_8">4</ref>. Note that higher scores are better, with 10 being the maximum score.</p><p>Sequence 2 Sequence 1 Fig. <ref type="figure">14</ref>: Temporal stability as measured by tPSNR <ref type="bibr" target="#b14">[15]</ref> over the course of two 50-frame clips from the CHAMELEON dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Model Precision</head><p>During training, we configure the weights to use full 32-bit precision. By default, this level of precision is retained during inference. However, reducing the precision of certain weights in the network can drastically improve the performance during inference. We examine the effects that such adjustments have on image quality in practice (Figure <ref type="figure" target="#fig_9">15</ref>).  In our tests, quantization artifacts were most apparent on homogeneous surfaces, subtle gradients, and transparent regions. Two examples are shown in Figure <ref type="figure" target="#fig_9">15</ref>. The difference in quality is especially apparent in the lower dataset shown in Figure <ref type="figure" target="#fig_9">15</ref>. The fading color towards the top shows a much more abrupt cut-off in quantized precisions (int8 and mixed-precision int8/fp16) compared to their unquantized counterparts (fp32 and fp16). The brightness of the lower frame was increased to emphasize the subtle differences. There was no noticeable difference between the non-quantized precisions fp32 and fp16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Effective Compression Rate</head><p>Reducing the number of total pixels that the volume needs to be sampled at immediately affects rendering performance. In the ideal case, a sparse rendering algorithm would achieve rendering times that scale linearly with the number of pixels. We termed this ideal case C max -the maximum possible rendering performance at any given sparsity level. To test how well our stream compaction sparse renderer performs, we record frame times along the whole spectrum of sparsity as represented by τ ranging from 1.0 (full frame) to 0.0 (no samples). Data is recorded for both the stream compaction method and a naive rejection approach which simply skips computations for certain threads on a full-frame kernel run. The results of this test are shown in Figure <ref type="figure" target="#fig_10">16</ref>. The data shows that our compaction method maps well to C max . As τ approaches sparsity rates of around 10%, the performance starts to diverge slightly from C max . Due to hardware limitations and computational overhead in the pipeline, the curve starts to flatten at around 1%. In our experiments, values for τ reside in the range of 0.1 to 0.01, which equates to roughly 10× to 25× compression rates compared to the naive approach, which is almost ten times less efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">LIMITATIONS &amp; FUTURE DIRECTIONS</head><p>FoVolNet works well on various data, as we have shown in the evaluation. It can benefit from more specialized training to address some of the edge cases we encountered during development, like high-frequency visual content or more pronounced transparency. Beyond this, there are several interesting extensions that we suggest here.</p><p>High-frequency Content. Regions of a volume that contained highfrequency intensity shifts resulted in increased temporal flickering when using our network. In most cases, increasing the renderer's volume sampling rate would alleviate such issues. A more cost-effective approach is to purposely introduce such artifacts into the training data, which would reduce the overall severity of the issue. Another approach is to emphasize temporal loss terms by increasing their weight (sacrificing visual quality) or by introducing more adaptable terms like a GAN critique <ref type="bibr" target="#b10">[11]</ref>.</p><p>Beyond Raymarching. In this work, we showcase our technique on the example of a raymarching renderer. However, FoVolNet is easy to extend to Monte Carlo methods like volume path tracing. Here we see potential to stabilize framerates by cutting short long-running threads due to multiple bounces and reconstructing their results using constanttime neural networks. Support for other data types like particle volumes or flow fields could also be added. We encourage further research to explore the specifics of such extensions.</p><p>Neural Adaptive Sampling. The approach presented here can be improved by predicting adaptive sample maps. Similar to Stengel et al.'s approach <ref type="bibr" target="#b50">[52]</ref>, both adaptive and foveated maps can be merged to maximize visual quality. Creating more off-focus sampling density could also improve the remaining issues with temporal stability. This extension increases sampling efficiency by replacing the naive uniform sampling in the periphery with an overall smarter approach.</p><p>Beyond the Screen. High-fidelity immersive visualization of volume data is still out of reach today. However, with FoVolNet we achieve higher and more consistent framerates (Figure <ref type="figure">9</ref>). With further improvements to the network, this goal could be attained much sooner than with conventional rendering techniques. The inference overhead could be reduced to a point where real-time, high-fidelity rendering becomes possible. This would open up opportunities to utilize FoVolNet for immersive experiences of volume data in VR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We presented FoVolNet-a foveated neural reconstruction system for volume visualization. FoVolNet accelerates conventional volume rendering techniques by sparsely sampling the data and reconstructing the full-frame using deep learning. Our novel network design reconstructs the final rendering at high quality using a hybrid of direct and kernel prediction mechanisms. We show that FoVolNet is able to provide tremendous speed-ups at compression rates as high as 25× over the state-of-the-art control technique DeepFovea <ref type="bibr" target="#b20">[21]</ref> while preserving image quality close to the original. Our uncomplicated design makes it easy to be integrated into existing rendering pipelines.</p><p>It is our plan to combine this technique with neural representation compression techniques and streaming technology to push the field further towards real-time high-fidelity volume visualization on consumer hardware. There are numerous opportunities to use and extend this technique, and we hope to entice the visualization community to take up this pursuit.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Overview of FoVolNet's components. The rendering pipeline loads a volume and renders it sparsely-saving time by skipping pixels in the periphery. Then, the sparse rendering is reconstructed by a neural network which takes constant time. The output of the rendering pipeline is the full-frame rendering.</figDesc><graphic url="image-4.png" coords="3,286.08,72.69,53.24,53.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: We compare multiple noise patterns to create our sampling masks. The top row (a)-(c) shows the different types of noise. In the bottom row we show an 8-image sequence of patterns averaged across time to emphasize their temporal stability (d)-(f).</figDesc><graphic url="image-11.png" coords="3,331.43,595.97,194.18,63.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 Fig. 5 :</head><label>55</label><figDesc>Fig.5: For direct sampling, we use a stochastic function P to generate sample points. A small frame buffer is filled with samples that correspond to real locations on the full-size framebuffer. Color values are reprojected to the full frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: The reconstruction network comprises two U-Net stages D and K. All Conv2D layers are configured with a stride and padding of 1 and no dilation. Network D uses a 3 × 3 kernel size while K uses 1 × 1 kernels. Upsample2D layers use bilinear interpolation for filtering. Skip connection are not shown in favor of readability. Network D performs coarse reconstruction through direct prediction while the second stage K uses D's hidden state to predict convolutional kernels which are subsequently applied to D's output to produce the final frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 :</head><label>8</label><figDesc>Fig.8: The hidden states H1-H5 of the VGG19 image classification network are used to measure the perceptual quality of our image reconstruction<ref type="bibr" target="#b63">[65]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>FoVolNet P b = 0.03 σ = 0.02 FoVolNet P b = 0.07 σ = 0.06 DeepFovea P b = 0.03 σ = 0.02 DeepFovea P b = 0.07 σ = 0.06 VORT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 :Fig. 12 :</head><label>1112</label><figDesc>Fig. 11: Visual comparison of reconstruction quality using our method. For each dataset, we show the area around the fovea in blue and a part of the periphery in yellow. All images were generated with P b = 0.03 and σ = 0.02 for P f .</figDesc><graphic url="image-51.png" coords="8,66.71,249.65,69.83,69.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 13 :</head><label>13</label><figDesc>Fig. 13: Structural similarity (SSIM) over the course of two 50-frame clips from the CHAMELEON dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 15 :</head><label>15</label><figDesc>Fig. 15: Comparison of reconstruction quality at different model precisions. Differences are most apparent on homogeneous surfaces and along subtle color gradients.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 16 :</head><label>16</label><figDesc>Fig. 16: Compression efficiency of our stream compaction rendering technique. Data was recorded for the whole value spectrum of sampling threshold τ. A curve for C max shows the maximum achievable efficiency at any given threshold value.The data shows that our compaction method maps well to C max . As τ approaches sparsity rates of around 10%, the performance starts to diverge slightly from C max . Due to hardware limitations and computational overhead in the pipeline, the curve starts to flatten at around 1%. In our experiments, values for τ reside in the range of 0.1 to 0.01, which equates to roughly 10× to 25× compression rates compared to the naive approach, which is almost ten times less efficient.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="1,95.99,161.57,424.70,138.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>• David Bauer, Qi Wu, and Kwan-Liu Ma are with the University of California</figDesc><table /><note>at Davis. E-mail: davbauer,qadwu,klma@ucdavis.edu. Manuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication xx xxx. 201x; date of current version xx xxx. 201x. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org. Digital Object Identifier: xx.xxxx/TVCG.201x.xxxxxxx</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>53 PSNR 11.41 SSIM 0.90 PSNR 28.71 SSIM 0.92 PSNR 33.37 SSIM 0.33 PSNR 11.7 SSIM 0.86 PSNR 26.29 SSIM 0.93 PSNR 34.74</head><label></label><figDesc></figDesc><table><row><cell>Mechanical Hand</cell><cell>Input</cell><cell>DeepFovea</cell><cell>FoVolNet</cell><cell>Reference</cell><cell>Supernova</cell><cell>Input</cell><cell>DeepFovea</cell><cell>FoVolNet</cell><cell>Reference</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Fovea</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Fovea</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Periphery</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Periphery</cell></row><row><cell>SSIM 0.Chameleon</cell><cell>Input</cell><cell>DeepFovea</cell><cell>FoVolNet</cell><cell>Reference</cell><cell></cell><cell>PSNR 9.75 SSIM 0.07</cell><cell>PSNR 23.87 SSIM 0.86</cell><cell>SSIM 0.9</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Fovea</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">SSIM 0.15</cell><cell>SSIM 0.68</cell><cell>SSIM 0.86</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">PSNR 10.06</cell><cell>PSNR 25.04</cell><cell>PSNR 30.41</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Periphery</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">SSIM 0.02</cell><cell>SSIM 0.85</cell><cell>SSIM 0.89</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">PSNR 11.67</cell><cell>PSNR 28.90</cell><cell>PSNR 34.18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>PSNR 34.16 SSIM 0.18 PSNR 11.38 SSIM 0.86 PSNR 26.53 SSIM 0.91 PSNR 34.57</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Skull</cell><cell>Input</cell><cell>DeepFovea</cell><cell>FoVolNet</cell><cell>Reference</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Fovea</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SSIM 0.14</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PSNR 4.59</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Periphery</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SSIM 0.34</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PSNR 4.56</cell></row><row><cell>Vortices 1</cell><cell>Input</cell><cell>DeepFovea</cell><cell>FoVolNet</cell><cell>Reference</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Fovea</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Periphery</cell></row><row><cell>SSIM 0.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>17 PSNR 10.85 SSIM 0.91 PSNR 29.97 SSIM 0.98 PSNR 40.58 SSIM 0.3 PSNR 10.02 SSIM 0.93 PSNR 33.2 SSIM 0.97 PSNR 43.29</head><label></label><figDesc></figDesc><table><row><cell>Vortices 2</cell><cell>Input</cell><cell>DeepFovea</cell><cell>FoVolNet</cell><cell>Reference</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Fovea</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Periphery</cell></row><row><cell>SSIM 0.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>22 PSNR SSIM 0.95 PSNR 31.73 SSIM 0.97 PSNR 39.07 SSIM 0.25 PSNR 13.97 SSIM 0.94 PSNR 32.74 SSIM 0.97 PSNR 40.33</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Just-objectionable-difference scores<ref type="bibr" target="#b34">[35]</ref> of reconstruction output from the CHAMELEON dataset computed at different thresholds.</figDesc><table><row><cell>τ τ τ</cell><cell cols="3">JOD Score FoVolNet</cell><cell cols="3">JOD Score DeepFovea</cell></row><row><cell cols="2">0.10 9.35</cell><cell></cell><cell></cell><cell>8.53</cell><cell></cell></row><row><cell cols="2">0.07 9.28</cell><cell></cell><cell></cell><cell>8.47</cell><cell></cell></row><row><cell cols="2">0.03 8.86</cell><cell></cell><cell></cell><cell>8.21</cell><cell></cell></row><row><cell cols="2">0.01 8.26</cell><cell></cell><cell></cell><cell>7.51</cell><cell></cell></row><row><cell></cell><cell>Full Frame</cell><cell>FP32</cell><cell>FP16</cell><cell>INT8 + FP32</cell><cell>INT8 + FP16</cell><cell>Reference</cell></row><row><cell></cell><cell></cell><cell>SSIM 0.95</cell><cell>SSIM 0.95</cell><cell>SSIM 0.94</cell><cell>SSIM 0.93</cell></row><row><cell></cell><cell></cell><cell>PSNR 34.9</cell><cell>PSNR 35.0</cell><cell>PSNR 33.3</cell><cell>PSNR 33.2</cell></row><row><cell></cell><cell></cell><cell>SSIM 1.00</cell><cell>SSIM 1.00</cell><cell>SSIM 0.99</cell><cell>SSIM 0.99</cell></row><row><cell></cell><cell></cell><cell>PSNR 45.7</cell><cell>PSNR 45.8</cell><cell>PSNR 37.2</cell><cell>PSNR 37.0</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research is sponsored in part by the Department of Energy through grant SC-DE0019486 and Intel's oneAPI Centers of Excellence grant.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Adler&apos;s Physiology of the Eye</title>
		<author>
			<persName><forename type="first">F</forename><surname>Adler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Elsevier Health Sciences</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Kernel-predicting convolutional networks for denoising monte carlo renderings</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bako</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vogels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Novák</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Harvill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Derose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rousselle</surname></persName>
		</author>
		<idno type="DOI">10.1145/3072959.3073708</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A survey of GPU-based large-scale volume visualization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hadwiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<idno type="DOI">10.2312/eurovisstar.20141175</idno>
	</analytic>
	<monogr>
		<title level="m">The Eurographics Association</title>
				<editor>
			<persName><forename type="first">R</forename><surname>Borgo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Maciejewski</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Viola</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="105" to="123" />
		</imprint>
	</monogr>
	<note>EuroVis-STARs</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Interactive reconstruction of monte carlo image sequences using a recurrent denoising autoencoder</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R A</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Kaplanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schied</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lefohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nowrouzezahrai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<idno type="DOI">10.1145/3072959.3073601</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="98" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cinematic rendering-an alternative to volume rendering for 3d computed tomography imaging</title>
		<author>
			<persName><forename type="first">E</forename><surname>Dappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Higashigaito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fornaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leschka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wildermuth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Alkadhi</surname></persName>
		</author>
		<idno type="DOI">10.1007/s13244-016-0518-1</idno>
	</analytic>
	<monogr>
		<title level="j">Insights Into Imaging</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="849" to="856" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Real-time ambient occlusion and halos with summed area tables</title>
		<author>
			<persName><forename type="first">J</forename><surname>Díaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-P</forename><surname>Vázquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Navazo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Duguet</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cag.2010.03.005</idno>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="337" to="350" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep volumetric ambient occlusion</title>
		<author>
			<persName><forename type="first">D</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ropinski</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2020.3030344</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1268" to="1278" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Two-frame motion estimation based on polynomial expansion</title>
		<author>
			<persName><forename type="first">G</forename><surname>Farnebäck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Analysis</title>
				<editor>
			<persName><forename type="first">J</forename><surname>Bigun</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Gustavsson</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/3-540-45103-X50</idno>
		<imprint>
			<date type="published" when="2003">2003</date>
			<pubPlace>Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sample-based monte carlo denoising using a kernel-splatting network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<idno type="DOI">10.1145/3306346.3322954</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="125" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.1145/3422622</idno>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="139" to="144" />
			<date type="published" when="2020-10">oct 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Foveated 3d graphics</title>
		<author>
			<persName><forename type="first">B</forename><surname>Guenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Finch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snyder</surname></persName>
		</author>
		<idno type="DOI">10.1145/2366145.2366183</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2012-11">nov 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Interactive volume exploration of petascale microscopy data streams using a visualizationdriven virtual memory approach</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hadwiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-K</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2012.240</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2285" to="2294" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast temporal reprojection without motion vectors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hanika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tessari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dachsbacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Graphics Techniques (JCGT)</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="19" to="45" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural temporal adaptive sampling and denoising</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hasselgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Munkberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Patney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lefohn</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.13919</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="147" to="155" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Local ambient occlusion in direct volume rendering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hernell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ljung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ynnerman</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2009.45</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="548" to="559" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural denoising for path tracing of medical volumetric data</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martschinke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<idno type="DOI">10.1145/3406181</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Computer Graphics and Interactive Techniques</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A survey on deep learning-based monte carlo denoising</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
		<idno type="DOI">10.1007/s41095-021-0209-9</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="169" to="185" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-material volume rendering with a physically-based surface reflection model</title>
		<author>
			<persName><forename type="first">O</forename><surname>Igouchkine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2017.2784830</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3147" to="3159" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Quantization and training of neural networks for efficient integer-arithmetic-only inference</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kligys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00286</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2704" to="2713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">DeepFovea: neural reconstruction for foveated rendering and video compression using learned statistics of natural videos</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Kaplanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sochenov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leimkühler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Okunev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Goodall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rufo</surname></persName>
		</author>
		<idno type="DOI">10.1145/3355089.3356557</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="212" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep convolutional reconstruction for gradient-domain rendering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kettunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Härkönen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3306346.3323038</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="126" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Open scientific visualization datasets</title>
		<author>
			<persName><forename type="first">P</forename><surname>Klacansky</surname></persName>
		</author>
		<ptr target="https://klacansky.com/open-scivis-datasets/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Interactive translucent volume rendering and procedural modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kniss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Premoze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ebert</surname></persName>
		</author>
		<idno type="DOI">10.1109/VISUAL.2002.1183764</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="109" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A model for volume lighting and modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kniss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Premoze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shirley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mcpherson</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2003.1196003</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="150" to="162" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Quantizing deep convolutional networks for efficient inference: a whitepaper</title>
		<author>
			<persName><forename type="first">R</forename><surname>Krishnamoorthi</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1806.08342</idno>
		<idno type="arXiv">arXiv:1806.08342</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Exposure render: an interactive photo-realistic volume rendering framework</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kroes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Botha</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0038586</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">e38586</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Smooth probabilistic ambient occlusion for volume rendering</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kroes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Eisemann</surname></persName>
		</author>
		<idno type="DOI">10.1201/9781351052108-17</idno>
	</analytic>
	<monogr>
		<title level="j">GPU Pro</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">475</biblScope>
			<date type="published" when="2015">2015</date>
			<publisher>Advanced Rendering Techniques</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dense optical flow by iterative local window registration</title>
		<author>
			<persName><forename type="first">G</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Besnerais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Champagnat</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICIP.2005.1529706</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">137</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
		<ptr target="https://graphics.stanford.edu/data/voldata/" />
		<title level="m">The Stanford volume data archive</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Progressive light volume for interactive volumetric illumination</title>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.1002/cav.1706</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Animation and Virtual Worlds</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="394" to="404" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">DMCR-GAN: adversarial denoising for monte carlo renderings with residual attention networks and hierarchical features modulation of auxiliary buffers. SIGGRAPH Asia Technical Communications</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">F</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3410700.3425426</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Out-of-core multi-resolution volume rendering of large data sets</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lundell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
		<respStmt>
			<orgName>Linköping University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Chamaeleo calyptratus</title>
		<author>
			<persName><forename type="first">J</forename><surname>Maisano</surname></persName>
		</author>
		<ptr target="http://digimorph.org/specimens/Chamaeleocalyptratus/whole" />
	</analytic>
	<monogr>
		<title level="j">Digital Morphology</title>
		<imprint>
			<biblScope unit="page">2003</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">FovVideoVDP: a visible difference predictor for wide field-of-view video</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Mantiuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Denes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kaplanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rufo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bachy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Patney</surname></persName>
		</author>
		<idno type="DOI">10.1145/3450626.3459831</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Optical models for direct volume rendering</title>
		<author>
			<persName><forename type="first">N</forename><surname>Max</surname></persName>
		</author>
		<idno type="DOI">10.1109/2945.468400</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="108" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Local and global illumination in the volume rendering integral</title>
		<author>
			<persName><forename type="first">N</forename><surname>Max</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.4230/DFU.SciViz.2010.259</idno>
	</analytic>
	<monogr>
		<title level="m">Scientific Visualization: Advanced Concepts</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Hagen</surname></persName>
		</editor>
		<meeting><address><addrLine>Dagstuhl, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="259" to="274" />
		</imprint>
	</monogr>
	<note>of Dagstuhl Follow-Ups</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<author>
			<persName><surname>Tensorrt</surname></persName>
		</author>
		<ptr target="https://developer.nvidia.com/tensorrt" />
		<imprint>
			<date type="published" when="2022-03">2022. March-2022</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Optimization techniques for cloud based interactive volumetric monte carlo path tracing. Industrial Talk</title>
		<author>
			<persName><forename type="first">G</forename><surname>Paladini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Petkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Engel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>EG/VGTC EuroVis</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Massively parallel Lucas Kanade optical flow for real-time video processing applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Plyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Le</forename><surname>Besnerais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Champagnat</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11554-014-0423-0</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Real-Time Image Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="713" to="730" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">From pairwise comparisons and rating to a unified quality scale</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pérez-Ortiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mikhailiuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hulusic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Valenzise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Mantiuk</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2019.2936103</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1139" to="1151" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">U-net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-428</idno>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention -MICCAI 2015</title>
				<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Interactive volumetric lighting simulating scattering and shadowing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ropinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Döring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rezk-Salama</surname></persName>
		</author>
		<idno type="DOI">10.1109/PACIFICVIS.2010.5429594</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Pacific Visualization Symposium (PacificVis)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Hinrichs. Interactive volume rendering with dynamic ambient occlusion and color bleeding</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ropinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meyer-Spradow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Diepenbrock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mensmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-8659.2008.01154.x</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="567" to="576" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Obscurance-based volume rendering framework</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Boada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bruckner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feixas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sbert</surname></persName>
		</author>
		<idno type="DOI">10.2312/VG/VG-PBG08/113-120</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/ EG Symposium on Volume and Point-Based Graphics</title>
				<editor>
			<persName><forename type="first">H.-C</forename><surname>Hege</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Laidlaw</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Pajarola</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">O</forename><surname>Staadt</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
	<note>The Eurographics Association</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A directional occlusion shading model for interactive direct volume rendering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pegoraro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Boulanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bouatouch</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-8659.2009.01464.x</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="855" to="862" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Parallel distributed, GPU-accelerated, advanced lighting calculations for large-scale volume visualization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rizzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Insley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Uram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hereld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Papka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1109/LDAV.2016.7874309</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE 6th Symposium on Large Data Analysis and Visualization (LDAV)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="47" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Out-ofcore visualization of time-varying hybrid-grid volume data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sitaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mavriplis</surname></persName>
		</author>
		<idno type="DOI">10.1109/LDAV.2014.7013209</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE 4th Symposium on Large Data Analysis and Visualization (LDAV)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="93" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<ptr target="https://mbs.rutgers.edu/staff/deborah-silver/" />
		<title level="m">Vortices volume dataset</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A multidirectional occlusion shading model for direct volume rendering</title>
		<author>
			<persName><forename type="first">V</forename><surname>Šoltészová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bruckner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Viola</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-8659.2009.01695.x</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="883" to="891" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Adaptive image-space sampling for gaze-contingent real-time rendering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stengel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grogorick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eisemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Magnor</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.12956</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="129" to="139" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Image plane sweep volume illumination</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sundén</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ynnerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ropinski</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2011.211</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2125" to="2134" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">TV-L1 optical flow estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sánchez Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Meinhardt-Llopis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Facciolo</surname></persName>
		</author>
		<idno type="DOI">10.5201/ipol.2013.26</idno>
	</analytic>
	<monogr>
		<title level="j">Image Processing On Line</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="137" to="150" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A reducedprecision network for image reconstruction</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vaidyanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liktor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Forbes</surname></persName>
		</author>
		<idno type="DOI">10.1145/3414685.3417786</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="231" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Denoising with kernel prediction and asymmetric loss functions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Vogels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rousselle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Röthlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Harvill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Novák</surname></persName>
		</author>
		<idno type="DOI">10.1145/3197517.3201388</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">OSPRay-a CPU ray tracing framework for scientific visualization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Amstutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Brownlee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Knoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jeffers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Günther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Navrátil</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2016.2599041</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="931" to="940" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Foveated real-time ray tracing for head-mounted displays</title>
		<author>
			<persName><forename type="first">M</forename><surname>Weier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kruijff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hinkenjann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pérard-Gayot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Slusallek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.13026</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="289" to="298" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Perception-driven accelerated rendering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Weier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stengel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Didyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Eisemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eisemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grogorick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hinkenjann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kruijff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Magnor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Myszkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Slusallek</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.13150</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="611" to="643" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning adaptive sampling and reconstruction for volume visualization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Is ¸ik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Westermann</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2020.3039340</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2654" to="2667" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Spatiotemporal Blue Noise Masks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wolfe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Morrical</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Akenine-Möller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<idno type="DOI">10.2312/sr.20221161</idno>
	</analytic>
	<monogr>
		<title level="m">Eurographics Symposium on Rendering</title>
				<editor>
			<persName><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L.-Y</forename><surname>Wei</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
	<note>The Eurographics Association</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Robust deep residual denoising for monte carlo rendering</title>
		<author>
			<persName><forename type="first">K.-M</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-T</forename><surname>Wong</surname></persName>
		</author>
		<idno type="DOI">10.1145/3283254.3283261</idno>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia Technical Briefs, SA &apos;18</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Ranger-a synergistic optimizer</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wright</surname></persName>
		</author>
		<ptr target="https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Adversarial monte carlo denoising with conditioned auxiliary feature modulation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3355089.3356547</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="224" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00068</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Fast global illumination for interactive volume visualization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1145/2448196.2448205</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games</title>
				<meeting>the ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="55" to="62" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
