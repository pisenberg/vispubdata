<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visualizing Ensemble Predictions of Music Mood</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zelin</forename><surname>Ye</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Min</forename><surname>Chen</surname></persName>
						</author>
						<title level="a" type="main">Visualizing Ensemble Predictions of Music Mood</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-03-27T09:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>time-series visualization</term>
					<term>ensemble learning</term>
					<term>music mood classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. 1. An ensemble of 210 machine learning (ML) models was used to predict music mood. The music shown is a segment from Partita V, Clavierubung part I, Bach BWV 829. From the original ThemeRiver (top right) and stacked line graph (bottom right), it is not always easy to see the ordering of the moods according to the votes, nor the points where the mood changes. A new visual design, dual-flux ThemeRiver (left), enables more effective observation of such information. The upper flux shows the dominant mood, and the lower flux depicts the other moods in the descending order of received votes. In this figure, votes by ML models are weighted by the squared accuracy of ML models. The weighted 50% lines depict a critical threshold over time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Music classification is an important family of techniques for enabling music categorization, recognition, segmentation, and retrieval. In comparison with typical classification problems (e.g., genre and composer <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b51">52]</ref>), mood classification appears to be more challenging <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b30">31]</ref> without using lyrics features, as some of the music has relatively vague or complicated moods, while the genre can be clearly defined as using certain chords, instruments or rhythm pattern. The problem of music mood classification entails three challenges:</p><p>(a) Mood labels are typically assigned to a long piece of music rather than its parts because segmenting music paradoxically would need correct mood labelling for individual parts. Extrapolating "global" ground truth labels to individual parts will inevitably lead to erroneous parts labels. For many music pieces, there is a flow of mood change across the music, and machine learning (ML) workflows have difficulties (i) to train a part-based model using "global" ground truth labels and (ii) to convey the uncertainty in mood classification effectively.</p><p>(b) The mood of music is inherently uncertain as a similar sequence of music scores A3: may convey different moods. Sometimes the same or similar notes can impress the listeners with different moods, e.g., by introducing an extra chord, playing the same notes in a different key, or changing the speed of a few notes. Some music pieces have more recognizable mood, e.g., delightful, sad, angry, and calm, while many other pieces may tingle between two moods or convey a mixture of moods as mood judgement can be subjective <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b81">82]</ref>. In many ways, such uncertainty is also common in other mood classification problems, such as blog posts <ref type="bibr" target="#b52">[53]</ref> or lyric text mining <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>(c) The ensemble ML models can potentially address challenges (a) and (b) partly by allowing different models to offer their predictions as "opinions" or "votes". However, it is common for ensemble ML techniques to infer a single prediction from different opinions or votes, e.g., using weighted aggregation or a meta-model <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b68">69]</ref>. Such over-abstraction creates difficulties in observing how different parts of the music are voted by the ensemble of ML models or how individual model has performed for different music patterns.</p><p>The above three challenges suggest that the ML technology can benefit from the assistance of visualization and visual analytics (VIS) techniques. To address challenge (a), VIS can reveal the performance of ML models in predicting part-labels after being trained with only "global" labels. To address (b), VIS can convey multiple moods that may potentially be present in individual parts of a music piece as well as the relative levels of their presence. To address (c), VIS can provide ensemble models that deliver multi-predictions with more complex visual representations for the model outputs. While VIS solutions for (a, b, c) can support ML experts, a VIS solution for (b) can reach out to a broader audience who are interested in the music mood.</p><p>In this work, we propose to visualize the multiple predictions by the ensemble of ML models, offering visual information that the traditional statistical aggregation cannot offer. In particular, we considered three visual designs, stacked line graph and ThemeRiver <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> for aggregated opinions and pixel-based visualization for constituent predictions of individual models. During our effort for using such visual designs to support our ensemble ML workflows, we discovered a variant of ThemeRiver, which is more effective than the original ThemeRiver and stacked line graph. Fig. <ref type="figure">1</ref> shows an example of such a variant, and a section of the music is also shown in a stacked line graph and an original ThemeRiver plot. This variant allows viewers to observe the change in the opinions of ML models more easily, and perceive whether the majority opinion has passed a critical threshold more precisely. We refer to this new variant as dual-flux ThemeRiver.</p><p>While we use the dual-flux ThemeRiver variant to address the need for temporal and collective mood visualization in challenges (a) and (b), we use pixel-based visualization to address the need for visualizing the performance of many individual models in the above challenge (c).</p><p>The combination of dual-flux ThemeRiver and pixel-based visualization also enables the juxtaposition of the overall mood flow and the contribution of individual models.</p><p>The overall contribution of this work is the introduction of appropriate visualization techniques into ensemble ML workflows. In particular, we firstly introduce dual-flux ThemeRiver (Section 4.1) as a novel visual representation to enable effective visual communication of multiple mood predictions by many ML models. This is crucial to address the above challenges (a) and (b) and to support pixel-based visualization in terms of challenge (c). Secondly, we provide ML model developers with investigative visualization for evaluating the performance of many models in a context sensitive to individual parts of the music (Section 4.2). This allows ML developers to observe a huge amount of information that statistical indicators cannot convey, facilitating the transformation from actionable information to ML-developmental decisions (e.g., weighting schemes in ensemble ML). Hence, this addresses challenge (c) that is routinely encountered in developing an ensemble ML technique involving many models.</p><p>We present two case studies (Sections 5.1 and 5.2) to demonstrate the usability of the proposed techniques. We conduct an analytical evaluation to reason the shortcomings of the existing solutions and the advantages and potential limitations of the proposed techniques (Section 5.3). We report the evaluation by music and ML experts, confirming the merits of this work (Section 5.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>This paper relates to visualization and visual analytics (VIS), music mood classification and machine learning (ML) in several aspects. In this section, we give a brief overview of each aspect. Time-series Visualization. The survey in the book <ref type="bibr" target="#b0">[1]</ref> offers a comprehensive list of visual designs up to 2011. Recently Fang et al. provided an updated survey on the topic <ref type="bibr" target="#b19">[20]</ref>. There are many versions of line graphs for visualizing time series. Harris <ref type="bibr" target="#b24">[25]</ref> proposed a visual design using area graph stacking lines on top of each other as a variant of the original layer area graph. Among variants of line graphs, ThemeRiver is a popular visual design and is highly relevant to this work. Harve et al. <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> proposed the ThemeRiver design for visualizing textual data or large document collections. There are many variants of ThemeRiver, including the spiral variant by Jiang et al. <ref type="bibr" target="#b38">[39]</ref>, a multi-dimensional variant by Kalouli et al. <ref type="bibr" target="#b39">[40]</ref>, a variant with trend line and change indicators by Shi et al. <ref type="bibr" target="#b71">[72]</ref>, variants with labelling and wiggling deformation by Byron and Wattenberg <ref type="bibr" target="#b9">[10]</ref>, a variant with heatmap by Hashimoto and Matsushita <ref type="bibr" target="#b25">[26]</ref>. With all these designs, viewers have to determine the ordering of different layers by visually estimating the thickness of each layer at each time step.</p><p>To depict ordering change explicitly, Javed et al. <ref type="bibr" target="#b37">[38]</ref> introduced an overlapped design, where multiple layers are superimposed and ordered from lower values (at front) to higher values (at back). However, while the change of ordering can be perceived easily, the visual estimation of the thickness of each layer at each step requires cognitive reasoning about the perceived color-filled shapes as well as the occluded shapes. Ensemble Data Visualization. Ensemble data visualization is a collection of visual designs and techniques for visualizing data derived by ensemble modelling and simulation. Wang et al. provided a survey on VIS techniques for ensemble data <ref type="bibr" target="#b79">[80]</ref>. A variety of VIS techniques have been developed, including a collection of interactive visualization techniques by Potter et al. <ref type="bibr" target="#b60">[61]</ref>; topology preserving mappings by Baruque et al. <ref type="bibr" target="#b1">[2]</ref> an approach called Noodles for visualizing uncertainty of ensemble weather modelling by Sanyal et al. <ref type="bibr" target="#b69">[70]</ref>, a cluster-based ensemble visualization technique by Kumpf et al. <ref type="bibr" target="#b42">[43]</ref>, several visual representations under a Lagrangian framework by Hummel et al. <ref type="bibr" target="#b33">[34]</ref>, contour boxplots for uncertainty visualization by Whitaker et al. <ref type="bibr" target="#b80">[81]</ref>, 3D animation of integrated ensemble data by Phadke et al. <ref type="bibr" target="#b58">[59]</ref>, a multi-view tool with continuous parallel coordinate plots and ensemble bars by Chen et al. <ref type="bibr" target="#b10">[11]</ref>, a multi-view tool with geospatial visualization, spaghetti plots, cluster-based parallel coordinates by Jarema et al. <ref type="bibr" target="#b36">[37]</ref>, and a multi-view tool with plots for displaying time-hierarchical clustering, stacked spatiotemporal data, time-step animation, and space-time surface by Ferstl et al. <ref type="bibr" target="#b20">[21]</ref>.</p><p>Because almost all these works deal with complex outputs (e.g., 3D geometry and spatiotemporal data), much focus was placed on displaying and exploring the output data resulting from ensemble simulation. Our work differs from these works in two aspects. (i) While the predictions in each time series are simple nominal values, the temporal dimension contains much more semantically-meaningful information. (ii) There is a stronger need to observe individual model's predictions at individual time steps and there are 210 such models. For these two reasons, we focused on the ThemeRiver and pixel-based visualization.</p><p>Visualization for Machine Learning. Sacha et al. <ref type="bibr" target="#b67">[68]</ref> provided an ontology for showing various places in ML workflows which can benefit from visualization, followed by Tam et al. <ref type="bibr" target="#b74">[75]</ref>, who used information theory to estimate the benefit of visualization in ML workflows. We are going to apply a similar strategy for ML. Rauber et al. <ref type="bibr" target="#b63">[64]</ref> proposed an overview visual design for comparing model structures, in which we find the ensemble models for time-series data in music should also have similar functions and to compare within the model. In the VIS4ML ontology outlined by Sacha et al. <ref type="bibr" target="#b67">[68]</ref>, this work is part of the step of "Evaluate-Model" in ML workflows, including the process blocks "Model-Testing", "Quality-Analysis", and "Understanding-Model". Our contributions include a new general-purpose visual design as well as new application-specific solutions and experiences.</p><p>Ensemble Machine Learning Models. Ensemble ML techniques have been developed to address the problem that a single ML model may not be able to reach the required accuracy. Dietterich et al. <ref type="bibr" target="#b17">[18]</ref>, Polikar et al. <ref type="bibr" target="#b59">[60]</ref>, and Sagi et al. <ref type="bibr" target="#b68">[69]</ref> provided surveys on ensemble ML techniques. These surveys showed that the majority of ensemble ML models were designed to improve model prediction by combining different opinions offered by models in the ensemble. The aggregated decision is commonly obtained using a weighting function or a machinelearned meta-model. Opitz and MaClin <ref type="bibr" target="#b54">[55]</ref> studied how each classifier and dataset would affect the results of ensemble classification. Rokach et al. <ref type="bibr" target="#b65">[66]</ref> discussed various ensemble classifiers techniques, including their weighting functions. Many weighting functions have been proposed, including distribution summation by Clark et al. <ref type="bibr" target="#b15">[16]</ref> and performance weighting by Opitz <ref type="bibr" target="#b55">[56]</ref>. For meta-learning, "stacking" is a popular approach developed by Wolpert et al. <ref type="bibr" target="#b82">[83]</ref>, and "mixture of experts" is another popular method proposed by Jacobs et al. <ref type="bibr" target="#b34">[35]</ref>. Omari and Figueiras-Vidal <ref type="bibr" target="#b53">[54]</ref> proposed a post-aggregation method with combined weighting and meta-learning.</p><p>In the ensemble ML literature, some researchers reported the applications of multi-label classification, e.g., semantic annotation <ref type="bibr" target="#b61">[62]</ref> and music mood classification <ref type="bibr" target="#b81">[82]</ref>. There are mainly two approaches for multi-label classification <ref type="bibr" target="#b76">[77]</ref>: algorithm adaptation and problem transformation. With the algorithm adaptation approach, multi-label predictions are generated directly by multi-label classification models, e.g., kNN <ref type="bibr" target="#b87">[88]</ref>, adaboost <ref type="bibr" target="#b70">[71]</ref> and neural network <ref type="bibr" target="#b86">[87]</ref>. With the problem transformation approach, a multi-label problem is decomposed into multiple single-label problems <ref type="bibr" target="#b5">[6]</ref>. The Binary Relevance (BR) method considers each label as an independent class and trains a single-label model independently <ref type="bibr" target="#b50">[51]</ref>. The Label Powerset (LP) method considers different a new combination of labels as a new class <ref type="bibr" target="#b77">[78]</ref>). The RAkEL method reduces the complexity of LP by using an ensemble of multiple LP classifiers <ref type="bibr" target="#b78">[79]</ref>. The Classifier Chain (CC) method adapts BR by chaining a set of independent models in a particular order <ref type="bibr" target="#b64">[65]</ref>.</p><p>In this work, we use ensemble ML for multi-label classification and we adopted the problem transformation approach.</p><p>Machine Learning for Music and Music Mood Classification. Basili et al. <ref type="bibr" target="#b2">[3]</ref> proposed several ML methods for music classification and explored the impact of different music features on the model accuracy. In the literature, ML techniques used for music classification include: support vector machine by Xu et al. <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b84">85]</ref>, adaboost by Bergstra et al. <ref type="bibr" target="#b3">[4]</ref>, and transformer-based methods by Zhuang et al. <ref type="bibr" target="#b89">[90,</ref><ref type="bibr" target="#b91">92]</ref>. Choi et al. <ref type="bibr" target="#b14">[15]</ref> proposed to use topic modeling techniques for categorizing users' interpretation of song lyrics for music classification.</p><p>Laurier et al. <ref type="bibr" target="#b45">[46]</ref> used support vector machine for music mood classification. Hu et al. <ref type="bibr" target="#b32">[33]</ref> showed that lyrics features could be useful for music mood classification. In the field of music, it is widely known that mood classification can be subjective and experts may have different opinions about the same piece of music <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b81">82]</ref>. Hence, the labels in the training data are treated as the closest labels rather than the ground truth. There are mainly two approaches for mood modelling: categorical and dimensional <ref type="bibr" target="#b40">[41]</ref>. Hevner <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref> studied the categorical approach and clustered 67 moods into 8 groups. Because the adjectives for mood labelling could be ambiguous and misleading, the alternative models by Rusell <ref type="bibr" target="#b66">[67]</ref> and Thayer <ref type="bibr" target="#b75">[76]</ref> are more widely used. Both models have been used in ML-based mood classification. For example, Jim et al. <ref type="bibr" target="#b40">[41]</ref> used Thayer's model, while a few others used Rusell's model <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b72">73]</ref>. Our work followed Rusell's model and used the 4-mood model dataset <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b57">58]</ref> associated with this model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ENSEMBLE MUSIC MOOD CLASSIFICATION</head><p>Music classification is to determine if a piece of music falls into a certain category in relation to a concept. Example concepts are genre, composer, period, mood, and instrument. Many ML techniques have been applied to music classification, including support vector machine <ref type="bibr" target="#b16">[17]</ref>, convolutional neural networks <ref type="bibr" target="#b47">[48]</ref>, recurrent neural networks <ref type="bibr" target="#b35">[36]</ref>, and transformer <ref type="bibr" target="#b91">[92]</ref>. Some concepts, such as genre, composer, and period, are associated with a whole piece of music. For these classification problems, it is relatively easy to obtain accurate labels for training and testing ML models. Many recent techniques have already achieved very good results. Our own experiments showed that it is not too challenging to attain 90% accuracy for such problems.</p><p>However, the mood classification is not so easy as a "global" label for a piece of music is not necessary to be applicable to every constituent part. Music is a narrative, where moods may change over time. Meanwhile, it is difficult to label individual parts of music, as it requires accurate segmentation of parts according to mood. Hence, paradoxically, mood classification and segmentation are two mutually dependent problems. This work is concerned with training ML models for mood classification without ideal mood labels. The ML models are required to determine music mood at individual parts, but can only learn to do so using the less accurate "global" labels.</p><p>Our overall approach is to train many models using different ML techniques and training settings. From the perspectives of ML and the two challenges, (a) and (b), in Section 1, different ML techniques and training settings can enable models to learn different temporal features from "global" labels so that these models may judge individual parts differently. An ensemble of model predictions resembles multiple opinions in the human judgement of music mood. In the following subsections, we describe the training data, the ML methods used, the ensemble strategy, and the problems encountered in our ML workflow.</p><p>Training and Testing Dataset. For training our models, we used the public domain dataset, 4Q Audio Emotion Dataset <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b57">58]</ref>. It includes about 900 pieces of clips, with each associated with a mood label from delighted, angry, sad, and calm according to Rusell's model <ref type="bibr" target="#b66">[67]</ref>. Because the ML models are required to determine music mood at individual parts, we defined the smallest part as one second. For the music clips in the dataset, we used the slide-window mechanism to transform each clip into parts of i ∈ <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b29">30]</ref> seconds. For example, for a 30s clip, there will be one part of 30s, two parts of 29s, three parts of 28s, and so on. The "global" label of the 30s clip is extrapolated to all parts. Our first premise is that if a "global" label of a clip indicates a mood Q, it normally applies to most parts of the clip. Hence, probabilistically, mood Q is mostly correct among these parts. Our second premise is that learning from inaccurate labels is common in many ML applications, such as topic modelling <ref type="bibr" target="#b14">[15]</ref> and text mood classification <ref type="bibr" target="#b32">[33]</ref>.</p><p>To apply and evaluate our ML models, we also used another public domain dataset, Multimodal Sheet Music Dataset <ref type="bibr" target="#b18">[19]</ref>. These music clips are longer and are accompanied by music scores, but do not have mood labels. The music in Fig. <ref type="figure">1</ref> is from this dataset.</p><p>Feature-based Machine Learning. In this work, we use one of our ML workflows as an example to demonstrate the need for data visualization. This workflow focuses on feature-based ML using techniques belonging to the decision tree (DT) family. The basic DT algorithms include CART <ref type="bibr" target="#b7">[8]</ref> and C4.5 <ref type="bibr" target="#b62">[63]</ref>. More sophisticated techniques in the family include bagging <ref type="bibr" target="#b6">[7]</ref>, adaboost <ref type="bibr" target="#b22">[23]</ref>, gradient boosting decision tree (GBDT) <ref type="bibr" target="#b23">[24]</ref>, XGBoost <ref type="bibr" target="#b13">[14]</ref>, random forest (RT) <ref type="bibr" target="#b46">[47]</ref>, and Deep Forest (gcForest) <ref type="bibr" target="#b90">[91]</ref>. These techniques use different strategies to improve the basic DT. We are particularly interested in how their performance is affected by training with music parts of different interval lengths.</p><p>Feature extraction is an integral part of all algorithms and techniques in the DT family. We extracted 55 features ( f 1 ,..., f 55 ) from spectrogram from all music parts used for training and testing, including:</p><p>• chroma stft: For each feature extracted (except the tempo), we compute the mean and variance for feature integration.</p><formula xml:id="formula_0">f 1 , f 2 .</formula><p>The common wisdom in the area of ML for music <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b88">89]</ref> is that features of a shorter temporal window are suitable for conveying information about timbre, and those of a longer temporal window are suitable for conveying information of modulation, beat, mood, vocal, and so on. However, there has not been a thorough investigation of the size of the window. Some previous work used window sizes from 10ms to 10s <ref type="bibr" target="#b21">[22]</ref>. It was found that 10ms segments were too short to have any information for classification <ref type="bibr" target="#b51">[52]</ref>. The uncertainty about window sizes motivated us to explore as many reasonable options as possible and to train ML models with window sizes of 1 to 30 seconds.</p><p>Ensemble Music Models. In our ML workflow, we train ML models using seven different methods in the DT family. They are basic DT (CART), bagging, adaboost, gradient boosting decision tree (GBDT), XGBoost, random forest (RF), and Deep Forest (gcForest). With each method, we train 30 ML models for features extracted from different interval lengths (i.e., i ∈ <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b29">30]</ref>). Our ML workflow thus generates 210 models for mood classification. Our initial goals for ensemble ML were (i) to identify the best model from the 210 models and (ii) to use these models to compute aggregated ensemble predictions.</p><p>As shown in Fig. <ref type="figure" target="#fig_0">2</ref>, the basic DT has the lowest accuracy with an average accuracy of 56.8% for ML models corresponding to the 30 interval lengths. The next is adaboost, with an average accuracy of 58.4%. This is followed by bagging trees (60.8%), GBDT (61.6%), Random Forest (62.2%), and XGBoost (62.3%). The best is gcForest with an average accuracy of 63.1%. In particular, the joint best individual models are gcForest operating at 3s and 5s intervals respectively and both achieving 67.4% overall accuracy. The second best is GBDT at 5s interval (66.7%), and the third-best is GBDT at 4s (65.9%).</p><p>We tried many typical ensemble strategies, including the maximum number of (a) unweighted votes, and (b) votes weighted some quality measures of individual models, including (b 1 ) by the F1-score, (b 2 ) by the overall accuracy, (b 3 ) by the squared overall accuracy, (b 4 ) by mood-specific class-accuracy (based on confusion matrices), and (b 5 ) by squared mood-specific class-accuracy. The performance of these ensemble models was disappointing. We found that these statistical measures did not help ML model developers to think, and especially those with music knowledge could not use their knowledge in analyzing the model performance in relation to individual music clips at different temporal locations. As discussed in Section 1, one major reason is that the accuracy measures themselves are not reliable because the given global labels for 30s intervals are not the ground truth for the individual music clips of 1s to 29s intervals.</p><p>Requirements for Data Visualization. According to Chen and Ebert's methodology for improving a workflow, when statistic measures abstract information too quickly, data visualization can offer a remedy <ref type="bibr" target="#b11">[12]</ref>. Having realized that the statistical measures could not enable the two initial goals mentioned earlier, the ML developer working on music mood prediction refocused the development effort on VIS. While the ML developer had an intuition about several VIS requirements, a relatively full set of requirements, as listed below, were identified iteratively during the development life-cycle, including the analytical evaluation and expert evaluation reported in Section 5. We consider two groups of target users, music experts and ML developers.</p><p>• R 1 . Both would like to observe how ensemble models collectively voted on individual sections of music, so someone with music knowledge can reason if the voting results are sensible or not. • R 2 . Both would like to observe how ensemble models are collectively influenced by the less accurate "global" ground truth labels in parts of the music where the mood changes. • R 3 . Both would like to locate where ensemble models voted for a mood change so we can relate such changes with the corresponding music score. • R 4 . Both would like to see the dominant opinion of ML models, the second dominant opinion, the third, and so on, and music experts would like to exercise their own interpretations of the different predictions generated by an ensemble of ML models. • R 5 . Both would like ideally to identify visual representations that can be used to accompany music for non-experts. • R 6 . ML developers would like to observe sub-groups of models (e.g., by methods and interval length) to compare their performance with the ensemble group. • R 7 . ML developers would like to observe individual models' performance to compare their performance with the ensemble group and related subgroups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">VISUALIZING ENSEMBLE MODEL PREDICTIONS</head><p>As mentioned in Section 3, our ML workflow results in 210 ML models for mood classification. Given a piece of music, these models will scan sections of the music and offer their predictions section-by-section.</p><p>While the accuracy analysis in Section 3 may be indicative as to the overall performance of a model, they have the following shortcomings:</p><p>(1) The term "accuracy" is in fact rather misleading. As mentioned earlier, each mood label available to the ML workflow is for the whole piece of music rather than individual sections. When a piece of music is labelled as "sad", it may be safe to consider most parts are "sad" but rather unsafe to conclude every section is "sad". Therefore, when a ML model fails to assign a "sad" prediction to a section of the music concerned, it does not necessarily mean that the ML model made an error.</p><p>(2) The "accuracy" measure (or any other commonly-used quality measure, e.g., F1-score) is thus uncertain. The accuracy measure cannot convey the "opinions" of different models for each section of music. Some of the model predictions might be considered as errors in calculating the "accuracy" measure, but are actually contributing meaningful "opinions" in judging the mood of individual sections, for which the ground truth is unknown.</p><p>(3) The "accuracy" measure is not detailed enough for ML modeldevelopers to investigate the behaviour of each model in relation to different music patterns.</p><p>One major underlying cause of ( <ref type="formula" target="#formula_2">1</ref>) and ( <ref type="formula">2</ref>) is that uncertainty is inherently part of music and other forms of arts. It may be better for us to embrace the uncertainty by conveying different "opinions" instead of an aggregated prediction. Another major cause that underlies (2) and ( <ref type="formula">3</ref>) is that statistical measures compress information too quickly <ref type="bibr" target="#b11">[12]</ref>, and visualization can alleviate such shortcomings by slowing down the pace of information loss.</p><p>In this section, we examine the use of two families of visualization techniques, stacked line graph and pixel-based visualization for bringing back some lost information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Stacked line graph, ThemeRiver, Dual-flux ThemeRiver</head><p>As discussed in Section 3, different ML models process a piece of music in different section lengths. We define the unit-section length as the maximal common denominator of all section lengths. For our ML models, it happens to be 1 second. Given a piece of music M , we can first divide it into n unit-sections, M = {s 1 , s 2 ,...,s n }, and then translate the predictions of each model to the predictions for these unitsections. Given m ML models, we have m model-specific time series (with categorical values) as</p><formula xml:id="formula_1">T i = {c i,1 , c i,2 ,...,c i,n }, i = 1, 2,...,m.</formula><p>For each time interval s j , we can compute the total number of models that predicted a particular mood, choosing from "delighted", "angry", "sad", and "calm". This results in four summary time series (with numerical values for ensemble classification results).</p><formula xml:id="formula_2">T delighted ={v d,1 , v d,2 ,...,v d,n } T angry = {v a,1 , v a,2 ,...,v a,n } T sad ={v s,1 , v s,2 ,...,v s,n } T calm = {v c,1 , v c,2 ,...,v c,n }<label>(1)</label></formula><p>The time series in the Eq. 1 are naturally suitable for the stacked line graph and ThemeRiver as shown in Figs. <ref type="figure" target="#fig_2">3(a,b</ref>). With these two visual designs, it is not easy for one to observe which mood is the most popular opinion, especially at the temporal locations where two or more moods received similar numbers of votes from ML models.</p><p>We thus introduce a new visual design, which is referred to as dualflux ThemeRiver, the basic version of which displays each time step as several stacked bars. As shown in Fig. <ref type="figure" target="#fig_2">3</ref>(c), we divide the "river" into two main fluxes. The upper flux always displays the data of the mood receiving the most popular votes. The lower flux displays the data of other moods, ordered by the number of votes received from ML models. We can also depict critical thresholds using grid-lines in both fluxes, allowing viewers to perceive whether the most popular opinion has passed the critical threshold. For example in Fig. <ref type="figure" target="#fig_2">3(c</ref>), two grid-lines indicate the 50% threshold of 210 votes. Note that it is not easy to introduce such grid-lines in the original ThemeRiver representation (e.g., in Fig. <ref type="figure" target="#fig_2">3(e)</ref>). As demonstrated in Figs. <ref type="figure" target="#fig_2">3(c,d,g,j</ref>), with the dualflux ThemeRiver, one can observe the change in the popularity of the opinions and whether they pass the threshold line more easily than the stacked line graph and the original ThemeRiver. To achieve the same smoothed effect as the stacked line graph and ThemeRiver in Figs. <ref type="figure" target="#fig_2">3(a,b</ref>), we also developed an algorithm (see Supplemental Materials) that can handle any potential change of ordering between time steps as shown in Fig. <ref type="figure" target="#fig_2">3(d)</ref>, which improves the visual design in Fig. <ref type="figure" target="#fig_2">3(c)</ref>.</p><p>In ensemble modelling, it is common to assign different weights to votes of models according to a quality measure, e.g., accuracy values, F1-scores, or the squared version <ref type="bibr" target="#b43">[44]</ref>. Once such weights are introduced, the width of the river (in y direction) may vary temporally (in x-direction). In this work, we consider that the accuracy of each model depends on what decision it is making. Hence we obtain the classaccuracy of each model based on its confusion matrix. Fig. <ref type="figure" target="#fig_2">3(e,f,g</ref>) shows three examples of ensemble predictions for the three visual designs, where each vote is weighted by the squared class-accuracy of the model. One may observe that the delighted mood (yellow) at the left-bottom of (c,d) has shrunk and moved downwards in Fig. <ref type="figure" target="#fig_2">3(g)</ref>. This is because the delighted mood in that music segment was largely In (a,b,c,d), the width of each "prediction" stem is the unweighted sum of the number of models that made that prediction. The river width is thus the total number of models (i.e., 210 in this case). In (e,f,g) the width of each stem is the weighted sum by the squared class-accuracy of those models that made the prediction. In (h,i,j), the weights are normalized, i.e., the total number of models for each second is normalized to 1.</p><p>returned by models that are less accurate in predicting delight. Often weighting by class-accuracy can reduce the frequency of order switching caused by "voting noise". One may also notice that for the dual-flux ThemeRiver in Fig. <ref type="figure" target="#fig_2">3</ref>(g), the critical threshold is defined as 50% of the sum of all weighted votes, and it also varies temporally.</p><p>Comparing Figs. <ref type="figure" target="#fig_2">3(a,b,c,d</ref>) and Figs.</p><p>3(e,f,g), we notice the change of the popular opinion in the middle of the plot (i.e., 13∼15th seconds), where the most popular angry opinion (red) was replaced by the calm opinion (green). Because the "global" label for the whole excerpt is angry, we consider that the weighted aggregation is more meaningful than the unweighted one. A close look at the sample shows that the first half has a lot of strong drum beats and sound, with low frequency at a fast speed without melody. This ends up being classified as angry. The second half has synthesized sound only without a meaningful melody. The calm opinion is not necessarily incorrect. Many ML models must have made such a prediction based on their learning from other music pieces. This also explains why angry opinion becomes the second least popular opinion in the second half of the music.</p><p>Naturally, we may consider to normalize the weighted votes such that the total number of votes will remain uniform temporally. Figs. <ref type="figure" target="#fig_2">3(h,i,j</ref>) show the normalized versions of (e,f,g) respectively. Generally normalization makes the stacked line graph and the original ThemeRiver into rectangular boxes, while the dual-flux ThemeRiver still displays a fair amount of dynamics. While the normalized version may appear to some as more intuitive or less complicated, it may be mistaken as "one model, one vote" as with Fig. <ref type="figure" target="#fig_2">3(d)</ref>. Meanwhile, the unnormalized version in Fig. <ref type="figure" target="#fig_2">3(g</ref>) not only indicates the presence of weighting but also conveys some uncertainty information. For example, the second section in Fig. <ref type="figure" target="#fig_2">3(g</ref>) is narrower than the first section, which indicates the mood classification is less certain because some highly-weighted models are minority voters. In the rest of the paper, we will focus on the smoothed, weighted, and unnormalized version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pixel-based Visualization with Sorted/Clustered Models</head><p>In an ensemble modelling workflow, inevitably, we are interested in observing individual models' performance as well as comparing groups of models with similar attributes. Pixel-based visualization can provide an effective overview for observing the performance of many models in the context of different music. For example, as discussed in Section 3, the models in the ensemble were trained with different ML techniques, or sound features used by models are of different time intervals. Fig. <ref type="figure" target="#fig_3">4</ref> shows three sets of pixel-based visualization, all corresponding to the weighted ensemble predictions in Fig. <ref type="figure" target="#fig_2">3(g)</ref>. In each set of pixel-based visualization, the first pixel-map shows the prediction results. In the pixel-map, each pixel is a model's prediction for a time step of the music concerned (1 second in our case), and each row depicts a model's predictions per unit-section (30 seconds in our case). Each pixel-map essentially depicts the data structure of T i = {c i,1 , c i,2 ,...,c i,n }, except that the rows are sorted differently to enable the comparison of models with different grouping strategies.</p><p>The second pixel-map shows the class-accuracy of each prediction. For example, the top-left and bottom-left pixels in the first pixel-map indicate two angry predictions, while the corresponding pixels in the second pixel-map indicate two very different levels of class-accuracy. When we produce the weighted dual-flux ThemeRiver on the top of these two pixel-maps, the two angry predictions are weighted differently and contribute differently to the width of the first time step of the river.</p><p>Based on the discussion in the previous subsection, we consider that the popular predictions of angry (red) in the first section and the predictions of calm (green) in the second section are all correct.</p><p>In Fig. <ref type="figure" target="#fig_3">4</ref>(a), models (i.e., rows) are sorted according to accuracy. The accuracy values of models are depicted using a separate column on the right of the two pixel-maps. We can observe that most models in the upper half of the plot (i.e., more accurate models) can separate angry and calm well. A good number of models predicted the whole section as calm. We consider that their predictions in the second half are correct, while their votes in the first half are questionable since the 'global" ground truth label is angry. In the lower part of the plot (less accurate models), the predictions seem to be much less consistent. However, a few models that predicted the whole section as angry are all located in the lower half of the plot, suggesting that these are "accidental" correlations with the 'global" ground truth label.</p><p>In Fig. <ref type="figure" target="#fig_3">4</ref>(b), models are sorted by the ML methods and then interval length (from 1 to 30). We can observe that gcForest, RF, and XGB methods offer opinions consistent with the popular views depicted in the upper flux of the dual-flux ThemeRiver (top), while adaboost is the least consistent, followed by DT and bagging. The more accurate models of GBDT are consistent, but not with the less accurate ones.</p><p>Using Fig. <ref type="figure" target="#fig_3">4</ref>(b), we can also observe the impact of interval length. The shape of the red pattern is similar across all seven ML methods, indicating that models with short interval lengths are more accurate. For the first half of the music, interestingly, the pixel block for the bagging method shows a pattern similar to gcForst, RF, XGB, and GBDT, but has more sad opinions (blue) in the middle (interval lengths 7∼12), suggesting that the bagging method may likely confuse calm with sad if they are trained with certain interval lengths. Meanwhile, DT and adaboost also made many sad predictions. However, they appear less coherent than bagging when they are sorted by interval length in (c). Since models produced by these two ML methods are least accurate in the traditional sense (Section 3), pixel-based visualization confirms that they are generally not as good as the other 5 ML methods by having the average accuracy of 56.8% and 58.4% while others are all above 60%. Fig. <ref type="figure" target="#fig_3">4</ref>(c) further confirms the impact of interval length. This plot also allows us to make a critical judgement about training ML models with relatively long interval lengths. On the surface, if an interval length is closer to the length used for "global" ground truth labelling, the model evaluation appears to be more "trustworthy". However, we consider the "global" ground truth labels are not necessarily correct for every "local" part of the music piece concerned and our goal is to obtain models that can collectively make more correct predictions for the "local" parts. Hence the pixel-based visualization in Fig. <ref type="figure" target="#fig_3">4</ref> would suggest trusting more the models trained with short interval lengths. Of course, such a suggestion needs to be validated by the testing data for other music examples. Pixel-based visualization allowed ML model developers to perform such validation quickly. In Fig. <ref type="figure" target="#fig_3">4</ref>(c), models are sorted by the time interval length first, then the accuracy among the 7 ML models for each time interval. Here we can observe that the result of (c) is perfectly matched with the previous analysis from (b). Models with 22, 23, 25, 26, 27, 29, and 30 second time intervals have absolutely no red color. Only a few long segments are giving the angry mood, while most of the red color comes from the shorter segments, and there is a decreasing trend as the time interval length increases. This result also matches the dual-flux ThemeRiver above the pixel-based visualization, as the dominant mood started with angry, and was replaced by calm. In terms of the model accuracy, there is no huge difference among models of different time intervals, and the only slightly darker section is around 4 to 6 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CASE STUDIES AND EVALUATION</head><p>In this section, we first describe two case studies, one focuses on the use of dual-flux ThemeRiver, and another focuses on the combined use of pixel-based visualization and dual-flux ThemeRiver. We then provide an analytical evaluation of the visual designs and the VIS4ML workflow as well as a human-centered evaluation involving six independent experts from ML and music disciplines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Case Study: Piano Sonata Op 57</head><p>Fig. <ref type="figure" target="#fig_4">5</ref> depicts the ensemble predictions of the music mood by 210 ML models for the 3 rd Movement in Piano Sonata Op 57 by Beethoven, which is also known as Appassionata (Italian: passionate). This is one of Beethoven's most famous and technically difficult sonatas. Three sections of mood changes were highlighted, and the corresponding music scores are also shown in the figure.</p><p>In the first example, the upper flux shows that a segment receiving votes of sad ends at bar 211, and the dominant votes change to calm. From the music theory, the segment of sad is part of development, and the segment of calm is part of recapitulation that leads to the end. Before bar 211, the music is still slow, consisting of bass notes and chords. When the recapitulation starts, it suddenly turns into a series of notes at a normal speed from bar 211. This is when the switching happens in Example 1. Here the ensemble models collectively and precisely detected the change of music content and the mood, from a slow, sad segment into a segment played at a normal speed, which leads to the dominance of calm. Since it is still played in F minor, there is a considerable amount of sad mood, which is meaningfully shown as the second dominant mood in the lower flux.</p><p>The music in the second example in Fig. <ref type="figure" target="#fig_4">5</ref> is a connecting episode and played in D flat major, which could cause an increase of delighted. The dominant votes in the upper flux have not changed, as the playing style is similar to the previous segment (bar 212 ∼ 255, in F minor). The only change is the key being transposed to a major key. Therefore, although the music does not change much, the change of the second dominant mood in the dual-flux ThemeRiver shows that it is different.</p><p>In the third example, as the music approaches the end as part of the Coda, there is a slight change before the finish. The first 3 bars from 347 still repeat the same pattern as in Example 2 on the right hand from bar 256 to 267. From bar 350, it is suddenly played in a higher octave, which might cause the dominant mood to be delighted. In Fig. <ref type="figure" target="#fig_4">5</ref>, we also find the delighted mood does not appear from nowhere, but gradually becomes dominant in bar 350. This matches the score, which indicates a continuous sequence of notes played by the right hand.</p><p>This case study shows that the collective predictions of ensemble models are meaningful and consistent with the music score. The dualflux ThemeRiver enables the identification of music pattern changes, which are conveyed in the sense of mood changes. The ability to see the first and second dominant opinions is the major merit of dual-flux ThemeRiver over the original ThemeRiver and stacked line graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Case Study 2: Hypothesis Evaluation</head><p>Recall our discussion in Section 4.2 about Fig. <ref type="figure" target="#fig_3">4</ref>, the visual observation suggests to "trust more the models trained with short interval lengths." To an ML model-developer, this is an important hypothesis. The traditional evaluation methods based on statistical analysis cannot be used to evaluate this hypothesis easily because one would need an algorithm to find all mood change parts in the dataset to calculate some statistics, but the "global" ground truth labels cannot support such an algorithm.</p><p>On the other hand, it is rather easy to create a dual-flux ThemeRiver and pixel-based visualization for any music. One can quickly identify parts of music featuring noticeable mood changes using the dual-flux ThemeRiver. Fig. <ref type="figure" target="#fig_5">6</ref> shows 12 such examples. One can observe the pixelbased visualization corresponding to each change to see how models cast their votes. Same as Fig. <ref type="figure" target="#fig_3">4</ref>(d), the pixel-based visualizations in Fig. <ref type="figure" target="#fig_5">6</ref> are sorted by interval length first and then accuracy.</p><p>For each example, we identify which vertical parts of the pixel-based visualization correlate with the corresponding dual-flux ThemeRiver better. For example, interval lengths 4 ∼ 12 for ex1, 4 ∼ 14 for ex9, 3 ∼ 10 for ex10, and 1 ∼ 9 for ex12 provide positive evidence to support the hypothesis. The visualizations for ex3, ex4, ex7, and ex11 show evidence leaning towards positive, while the visualizations for ex2, ex5, and ex6 are not indicative. Meanwhile, the pixel visualization for ex8 seems to provide negative evidence against the hypothesis.</p><p>Given such observation, in conjunction with other sorting strategies as shown in Fig. <ref type="figure" target="#fig_3">4</ref>, the ML model developers can make combined use of different knowledge to reason these visualizations. It is particularly helpful when the model developers have some music knowledge or have access to such expertise to evaluate the dominant votes in the dual-flux ThemeRiver plots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analytical Evaluation</head><p>Chen and Ebert proposed a systematical design and evaluation methodology <ref type="bibr" target="#b11">[12]</ref>, which was adopted to analyze the symptom, cause, remedy, and side-effects of different approaches in this paper. We first examine the three visual designs discussed in Sections 4.1 and 5.1 and then the combined use of pixel-based visualization and dual-flux ThemeRiver discussed in Sections 4.2 and 5.2.</p><p>Three Visual Designs. For the requirements described in Section 3, the three line-graph based visual design (i.e., stacked line graph, original ThemeRiver, and dual-flux ThemeRiver) cannot support R 6 or R 7 easily. Although all three visual designs convey more or less the same amount of information, they have different strengths and weaknesses in supporting R 1∼5 .</p><p>1. Symptom: With the stacked line graph and the original The-meRiver, one cannot see easily the changes in the dominant opinion, the ordering of other opinions, and the place where ordering changes. Observing such information is an essential part of R 1∼5 . 2. Cause: Although such information is depicted implicitly, the cognitive cost for gaining it is very high, as it would involve perceptual estimation of the heights of different cross-sections, and cognitive comparison of such height measures <ref type="bibr" target="#b4">[5]</ref>. The stacked line graph has some advantages over the original ThemeRiver in estimating the total height and that of the bottom stream. 3. Remedy: Introduce a more explicit depiction of such information to reduce the cognitive cost. With the dual-flux ThemeRiver, the dominant opinion, the ordering, and the places of mood changes are all explicit, ready to be perceived. 4. Side-effect: The mood streams are no longer continuous, and it may take extra effort to re-connecting the same stream e.g., to quantify the amount of mood change. With only four moods and appropriate color-coding, the side-effect is not a big issue. It could become more serious if there were many streams.</p><p>Pixel-based Visualization and dual-flux ThemeRiver. This combined use of two visual designs is for supporting requirements R 6 and R 7 .</p><p>To address the issue, we have to go back to the traditional methods for observing ML models' performance. When one has a few models to compare, one might be able to ensure the demanding effort for is difficult with an arbitrary list of models, and grouping models visually is even harder. 5. Cause: Labelling small pixels is not easy. Visual grouping demands extra cognitive load for remembering and formulating groups mentally. 6. Remedy: Using different sorting schemes. 7. Side-effect: There could be an issue if the sorting scheme is unfamiliar to a user. For ML model-developers, this is unlikely.</p><p>The above analytical evaluation shows that the merits of the proposed dual-flux ThemeRiver and the combined use of pixel-based visualization and dual-flux ThemeRiver can be reasoned analytically according to information theory and cognitive theories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Expert Evaluation</head><p>To evaluate the effectiveness of our visual designs, we conducted informal interviews with 6 independent experts from music and ML domains. We met the experts individually via video conferencing. We first introduced the background of our research, then showed the video with audio for music as discussed in Section 5.1 and the comparisons with 12 examples as discussed in Section 5.2.</p><p>Three music experts (E1, E2, E3), who major in piano, cello, and Erhu, received professional music training for over 20 years. Three ML experts (E4, E5, E6) are specialized in different ML areas. E4 is specialized in natural language processing (NLP) and was able to relate ML applications in music with languages. E5 is an expert in convolutional neural networks and is aware of ensemble modelling and the benefits of VIS4ML techniques. E6 is an expert in ML and optimization and is mathematically knowledgeable. We summarize their main points from three perspectives, music, visualization, and ML. Each point is labelled with a circled letter (e.g., a ), transcribed comments are in italic, added clarifications are in [], and the authors' feedback and actions are marked with .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>From the Music Perspective:</head><p>a E1 considered that the dual-flux ThemeRiver avoids confusion. The original ThemeRiver can make people think that the bottom color is the base mood or the main mood, and the top mood is minor.</p><p>b E2 noted that the dual-flux ThemeRiver can be applied to a music ensemble with multiple instruments so that we can see how the individual instrument produces the mood and how the mood is created when multiple instruments are played simultaneously. It will be interesting for music analysis.</p><p>c Five experts were in favor of keeping the minor mood, even if a mood attracts only a few votes. We need to have some flexibility. And the music itself is subjective. I have seen people from other cultures considering the major key to be sad and the minor key to be happy. So if we were to use their labeling data for training, the results would be the opposite. The only exception is E2, who considered scenarios where some music is designed to be sad or has a specific mood. ... So these opinions are noise and are okay to be ignored.</p><p>There is a general debate about the principle that different moods can co-exist. We concluded that E2's specific scenarios should not restrict the flexibility favored by others. In a scenario where only one mood is allowed per time step, the musicians usually have the knowledge to ignore the other opinions. This can be done easily by focusing on the dominant stream at the top half of a dual-flux ThemeRiver. d E3 noticed the fact that the labelled data has only 4 classes, and would like to see more musical words, such as Affettuoso (excited), Con moto (vivid), and Con tenerezza (gentle).</p><p>There are currently no labelled data for the over 60 music words mentioned by E3. Nevertheless, this will be an important future direction for all of us working on music-related ML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>From the Visualization Perspective:</head><p>e In general, all experts agreed that the dual-flux ThemeRiver can depict the flow of mood changes while conveying the uncertainty of mood classification.</p><p>f Five experts considered that the dual-flux ThemeRiver helped determine the dominant mood more easily. The only exception was E1 who considered that it was difficult to learn. E2 and E3 commented on the learning aspect: As we are not professional visual analysts, we need to be trained to read the dual-flux ThemeRiver as it seems a bit more complicated with lines and switches. Once we know how to read it, it is much easier to identify the dominant mood and the uncertainty.</p><p>g E3 suggested a new variant of the dual-flux ThemeRiver. "Maybe we can have ... one that has the same time scale as the music score.".</p><p>We researched this suggestion. In most visual representations of sound, the temporal length is encoded as spatial length, while in a music score, the temporal length of a music note is encoded symbolically (e.g., semibreve, minim, crotchet, quaver). If one wishes to synchronize both representations, one could distort either the spatial lengths of the time steps in a sound representation or the spatial gaps between music notes in a music score. Perceptually, the relative merits might be in favor of the latter. We believe that further cognitive research will be needed before releasing E3's variant as a new music representation.</p><p>h E4 and E5 suggested that to reduce the number of mood switches by setting a threshold of maybe 5% before letting it switch.</p><p>We investigated this suggestion carefully. We consider that such a threshold would undermine the interpretation of the dominant mood as well as the ordering of the moods. The threshold will likely be a hidden parameter, and there may need an additional parameter for determining how long a switching may be delayed. We went back to E4 and E5 with our investigation. They both agreed that this could lead to new problems, confusion, and extra learning cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>From the ML Perspective:</head><p>i ML experts (E4, E5, and E6) are positive about the dual-flux ThemeRiver for conveying ML predictions with uncertainty and its combination with the pixel-based visualization, as the combined design enables ML developers to extract information more efficiently for understanding and optimization of ensemble learning. j There were two opinions as to whether less accurate models should be eliminated. E6 found that it was easy to observe poor performance and reason about the causes using the pixel-based visualization, and was in support of elimination. E4 argued for keeping all the models as part of the ensemble learning.</p><p>k Both E4 and E6 agree on moderating using weights as votes. E4 commented: It is okay to adjust the weights, but we should keep them as it is already the trained results. E6 commented: We may set some conditions to adjust the weights.</p><p>We adopted the suggestion and extended the uses of pixel-based visualization for showing the weights as detailed below.</p><p>As described in sections 4.1 and 4.2, dual-flux ThemeRiver can handle weighted votes. As shown in Fig. <ref type="figure" target="#fig_3">4</ref>, one approach is to assign the weight to each model decision based upon the class-accuracy of the model. As shown in Fig. <ref type="figure" target="#fig_6">7</ref>, we can also use pixel-maps to compare different weighting schemes. For example, let α i, j be the class-accuracy associated with each pixel. The first wide pixel-map on the left in Fig. <ref type="figure" target="#fig_6">7</ref> shows the prediction of each pixel. The second, third, and Fourth wide pixel-maps show the values of α i, j , α 2 i, j , and α 3 i, j respectively. Between these pixel-maps are the narrow pixel-bars for showing the overall accuracy of each model, and its squared and cubed versions in the second and third narrow pixel-bars. The dual-flux ThemeRiver plots on the top show the effects of different weighting schemes, i.e., from left: unweighted, α i, j , α 2 i, j , and α 3 i, j . Experts can evaluate different weighting schemes and identify a suitable one for describing the mood.</p><p>We showed this result to E3 and E6. Both quickly noticed the reduction of calm in the second half of the music. They commented: We understand the increase in "angry". We might say that it is a "stronger angry" mood, but not "stronger calm". E3 commented further:</p><p>I would say the weights of squared [class-]accuracy fit better. I assume that the calm mood itself is weaker, so when it is calm, it should be kept at a moderate level as "standard" instead of the plots shown with the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>In this work, we have developed visualization solutions for supporting the development and analysis of ensemble ML models for music mood classification as well as for communication of ensemble model predictions. To address the challenge of training models with less accurate "global" labels, we appreciate the need for users to make sense of the predictions by a large collection of ML models, for which VIS is essential. As shown in Section 5.1, those with music knowledge can reason the ensemble ML predictions using the dual-flux ThemeRiver plot, which therefore provides a means for enabling expressive AI or the understanding of AI. As exemplified in Sections 4.2 and 5.2, when ML model-developers examine the behaviours of one or a few ML models, they will frequently formulate hypotheses about the models (e.g., poorly-performed training methods, unsuitable time intervals, and optimal weighting function). One major advantage of using VIS is that the ML model-developers can quickly inspect the behaviours of many models. Such visual evaluation of a hypothesis is highly cost-effective if the VIS techniques are readily available, despite it may not be the final step for confirming or falsifying the hypothesis concerned.</p><p>Any slightly complex visualization incurs some information loss in comparison with the original data <ref type="bibr" target="#b12">[13]</ref>. Inevitably it will have some limitations for some tasks. The analytical and expert evaluation in Sections 5.3 and 5.4 enabled us to identify such limitations and sideeffects. In some cases, we have improved our work. In other cases, the benefits out-weighed the limitations.</p><p>We are in the process of using the VIS techniques developed in this work as well as others in the literature to study a much larger ensemble of ML models for music mood classification, including models developed using other ML frameworks. We also anticipate the potential use of the dual-flux ThemeRiver design in other applications, such as visualizing the results of opinion polling and the emotion featured in texts and videos (e.g., <ref type="bibr" target="#b83">[84,</ref><ref type="bibr" target="#b85">86]</ref>). In such visualization, it is important to observe the ordering of different opinions or emotion streams. We hope that future work will explore such opportunities.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Accuracy of the 210 ML models trained in this work.</figDesc><graphic url="image-2.png" coords="3,324.77,72.77,209.70,91.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>The normalized energy. • rms: f 3 , f 4 . The root-mean-square values. • spectral centroid: f 5 , f 6 . The mean of the normalized magnitude of the spectrogram (a distribution over frequency bins). • spectral bandwidth: f 7 , f 8 . The frequency bandwidth. • spectral rolloff: f 9 , f 10 . The centre frequency where 85% of the energy of the spectrum is contained in this bin and the bins below. • zero crossing rate: f 11 , f 12 . The zero-crossing rate of the data. • tempogram: f 13 , f 14 . Local auto-correlation of the onset strength envelope. • MFCCs: f 15 − f 54 . Decompose the waveform into different frequency bands. The first 20 MFCCs. • tempo: f 55 . Estimation of beats per minute.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Three visual designs for depicting time-varying ensemble predictions, namely stacked line graph (a), original ThemeRiver (b), and dual-flux ThemeRiver(c,d). In (a,b,c,d), the width of each "prediction" stem is the unweighted sum of the number of models that made that prediction. The river width is thus the total number of models (i.e., 210 in this case). In (e,f,g) the width of each stem is the weighted sum by the squared class-accuracy of those models that made the prediction. In (h,i,j), the weights are normalized, i.e., the total number of models for each second is normalized to 1.</figDesc><graphic url="image-10.png" coords="5,102.76,132.20,88.83,68.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Pixel-based visualization of the predictions by individual models. Each model corresponds to a row of pixels, while models are sorted along the y-axis using different sorting schemes. Color-mapped model accuracy is shown on the right of each plot. The same weighted dual-flux ThemeRiver (as Fig. 3(g)) is shown above each pixel-based plot as a reference to how class-accuracy may affect the dual-flux ThemeRiver.</figDesc><graphic url="image-24.png" coords="6,216.15,369.39,70.81,65.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Example 1 to 3: dual-flux ThemeRiver for Piano Sonata Op 57 (3 rd Movement).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Pixel-based visualization with the aid of dual-flux ThemeRiver, for evaluating a hypothesis that ML models trained with features of smaller intervals can deal with mood change better. The 12 dual-flux ThemeRiver show examples of mood changes and each corresponding pixel visualization shows the predictions of individual models, sorted by interval lengths. To evaluate the hypothesis, one can observe the correlation between the upper flux of the dual-flux ThemeRiver and different parts of the pixel-based visualization. observing their performance against individual data objects (music clips in this work) by reading classification logs. However, this would not scale up to 210 ML models. 1. Symptom: It is almost impossible to observe a large number of ensemble models against individual data objects by reading classification logs. 2. Cause: It incurs very high cognitive costs of reading numbers, remembering them for building up a mental overview model, and performing comparative tasks mentally. 3. Remedy: Both pixel-based visualization and dual-flux The-meRiver provide external memorization, substantially reducing the cost of repeated reading-remembering. By removing the burden of memorization, the users can devote more cognitive resources to the patterns depicted. 4. Side-effect (new symptom): Identifying individual ML modelsis difficult with an arbitrary list of models, and grouping models visually is even harder. 5. Cause: Labelling small pixels is not easy. Visual grouping demands extra cognitive load for remembering and formulating groups mentally. 6. Remedy: Using different sorting schemes. 7. Side-effect: There could be an issue if the sorting scheme is unfamiliar to a user. For ML model-developers, this is unlikely.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The first column is from Fig 4(d). On its right, the three wide pixel-maps show the weighted class-accuracy α of prediction (i.e., a pixel in the first column). The three weighting functions are α, α 2 , and α 3 respectively. They were used to compute the weighted dual-flux ThemeRiver above these wide pixel-maps. On the left of each pixel-map, a narrow pixel-bar shows the weighted model accuracy. other two weighting schemes. I am happy with the flexibility to have different options. For now, the squared [class-]accuracy should fit the best."</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>• Zelin Ye and Min Chen are with University of Oxford.</figDesc><table /><note>E-mail: zelin.ye@linacre.ox.ac.uk, min.chen@oerc.ox.ac.uk. Manuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication xx xxx. 201x; date of current version xx xxx. 201x. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org. Digital Object Identifier: xx.xxxx/TVCG.201x.xxxxxxx</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>Part of this work was made possible by the Network of European Data Scientists (NeEDS), a Research and Innovation Staff Exchange (RISE) project under the Marie Skłodowska-Curie Program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Survey of visualization techniques</title>
		<author>
			<persName><forename type="first">W</forename><surname>Aigner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Miksch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tominski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visualization of Time-Oriented Data</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="147" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ensemble methods for boosting visualization models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Baruque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Corchado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Corchado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Work-Conference on Artificial Neural Networks</title>
				<meeting>the International Work-Conference on Artificial Neural Networks</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="165" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Classification of musical genre: a machine learning approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Basili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Serafini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stellato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ISMIR</title>
				<meeting>the ISMIR</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Aggregate features and adaboost for music classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Casagrande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kégl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="473" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Evaluating the impact of task demands and block resolution on the effectiveness of pixel-based visualization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Borgo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Proctor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jaenicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Thornton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="963" to="972" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning multi-label scene classification</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Boutell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1757" to="1771" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Classification and Regression Trees</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Routledge</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A decision tree approach for the musical genres classification</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Bressan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>De Azevedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lizzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Mathematics &amp; Information Sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1703" to="1713" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Stacked graphs-geometry &amp; aesthetics</title>
		<author>
			<persName><forename type="first">L</forename><surname>Byron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1245" to="1252" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Uncertainty-aware multidimensional ensemble data visualization and exploration</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1072" to="1086" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An ontological framework for supporting the design and evaluation of visual analytics systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Ebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Graphics Forum</title>
				<meeting>the Computer Graphics Forum</meeting>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="131" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">What may visualization processes optimize?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Golan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2619" to="2632" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Xgboost: extreme gradient boosting</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Benesty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Khotilovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
	<note>R package version 0.4-2</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Topic modeling users&apos; interpretations of songs to inform subject access in music digital libraries</title>
		<author>
			<persName><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Willis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Downie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM/IEEE-CS Joint Conference on Digital Libraries</title>
				<meeting>the 15th ACM/IEEE-CS Joint Conference on Digital Libraries</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="183" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rule induction with cn2: Some recent improvements</title>
		<author>
			<persName><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Boswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Working Session on Learning</title>
				<meeting>the European Working Session on Learning</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="151" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Classification of audio signals using svm and rbfnn</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dhanalakshmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Palanivel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramalingam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="6069" to="6075" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ensemble methods in machine learning</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Multiple Classifier Systems</title>
				<meeting>the International Workshop on Multiple Classifier Systems</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning audio-sheet music correspondences for cross-modal retrieval and piece identification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dorfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hajič</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Arzt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Frostel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Widmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the International Society for Music Information Retrieval</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A survey of time series data visualization research</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IOP Conference Series: Materials Science and Engineering</title>
				<meeting>the IOP Conference Series: Materials Science and Engineering</meeting>
		<imprint>
			<publisher>IOP Publishing</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">782</biblScope>
			<biblScope unit="page">22013</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Timehierarchical clustering and visualization of weather forecast ensembles</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ferstl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kanzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rautenhaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Westermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="831" to="840" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The beat spectrum: A new approach to rhythm analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Foote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Uchihashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Multimedia and Expo</title>
				<meeting>IEEE International Conference on Multimedia and Expo</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="224" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A decision-theoretic generalization of on-line learning and an application to boosting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="139" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Greedy function approximation: a gradient boosting machine</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="1189" to="1232" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Information Graphics: A Comprehensive Illustrated Reference</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Harris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Oxford University Press</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Heat map scope technique for stacked time-series data visualization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Matsushita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Information Visualisation</title>
				<meeting>the 16th International Conference on Information Visualisation</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="270" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Themeriver: Visualizing theme changes over time</title>
		<author>
			<persName><forename type="first">S</forename><surname>Havre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hetzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nowell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Symposium on Information Visualization</title>
				<meeting>IEEE Symposium on Information Visualization</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="115" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Themeriver: Visualizing thematic changes in large document collections</title>
		<author>
			<persName><forename type="first">S</forename><surname>Havre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hetzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nowell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="20" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The affective character of the major and minor modes in music</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hevner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="118" />
			<date type="published" when="1935">1935</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The affective value of pitch and tempo in music</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hevner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="621" to="630" />
			<date type="published" when="1937">1937</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A framework for evaluating multimodal music mood classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Downie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Association for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="273" to="285" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">When lyrics outperform audio for music mood classification: A feature analysis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Downie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ISMIR</title>
				<meeting>the ISMIR</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="619" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Lyric text mining in music mood classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Downie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Ehmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>American Music</publisher>
			<biblScope unit="volume">183</biblScope>
			<biblScope unit="page" from="2" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Comparative visual analysis of lagrangian transport in cfd ensembles</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hummel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Obermaier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Garth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Joy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2743" to="2752" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adaptive mixtures of local experts</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="87" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Evaluation of gated recurrent neural networks in music classification tasks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jakubik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Information Systems Architecture and Technology</title>
				<meeting>the International Conference on Information Systems Architecture and Technology</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="27" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Comparative visual analysis of transport variability in flow ensembles</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jarema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kehrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Westermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of World Society for Computer Graphics</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Graphical perception of multiple time series</title>
		<author>
			<persName><forename type="first">W</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcdonnel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="927" to="934" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Spiral theme plot</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Grannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EuroVis (Short Papers)</title>
				<meeting>EuroVis (Short Papers)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="109" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Parhistvis: visualization of parallel multilingual historical data</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Kalouli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kehlbeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sevastjanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Butt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st International Workshop on Computational Approaches to Historical Language Change</title>
				<meeting>the 1st International Workshop on Computational Approaches to Historical Language Change</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="109" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Music mood classification model based on arousal-valence values</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Advanced Communication Technology</title>
				<meeting>the 13th International Conference on Advanced Communication Technology</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="292" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Large-scale midi-based composer classification</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.14805</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Visualizing confidence in cluster-based ensemble weather forecast analyses</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumpf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baumgart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riemer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Westermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rautenhaus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="109" to="119" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A probabilistic classifier ensemble weighting scheme based on cross-validated accuracy estimates</title>
		<author>
			<persName><forename type="first">J</forename><surname>Large</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bagnall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1674" to="1709" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multimodal music mood classification using audio and lyrics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Laurier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Grivolla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Conference on Machine Learning and Applications</title>
				<meeting>the Seventh International Conference on Machine Learning and Applications</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="688" to="693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Audio music mood classification using support vector machine. MIREX Task on Audio Mood Classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Laurier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mandel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ellis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="2" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Classification and regression by randomforest</title>
		<author>
			<persName><forename type="first">A</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wiener</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">R News</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="18" to="22" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Parallel convolutional neural networks for music genre and mood classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lidy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schindler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIREX</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cultural style based music classification of audio signals</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<meeting>IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="57" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Automatic mood detection and tracking of music audio signals</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="18" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">An extensive experimental comparison of methods for multi-label learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Madjarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kocev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gjorgjevikj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Džeroski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3084" to="3104" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Improving music genre classification by short time feature integration</title>
		<author>
			<persName><forename type="first">A</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ahrendt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Larsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
				<meeting>IEEE International Conference on Acoustics, Speech, and Signal Processing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">497</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Experiments with mood classification in blog posts</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGIR Workshop on Stylistic Analysis of Text for Information Access</title>
				<meeting>ACM SIGIR Workshop on Stylistic Analysis of Text for Information Access</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="321" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Post-aggregation of classifier ensembles</title>
		<author>
			<persName><forename type="first">A</forename><surname>Omari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Figueiras-Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="96" to="102" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Popular ensemble methods: An empirical study</title>
		<author>
			<persName><forename type="first">D</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Maclin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="169" to="198" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Feature selection for ensembles</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Opitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">379</biblScope>
			<biblScope unit="page">384</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Musical texture and expressivity features for music emotion recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Malheiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Paiva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Society for Music Information Retrieval Conference</title>
				<meeting>the 19th International Society for Music Information Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="383" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Novel audio features for music emotion recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Malheiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Paiva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="614" to="626" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Exploring ensemble visualization</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Phadke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Alabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">I</forename></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Bass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Healey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Visualization and Data Analysis</title>
				<meeting>the Visualization and Data Analysis</meeting>
		<imprint>
			<publisher>International Society for Optics and Photonics</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">8294</biblScope>
			<biblScope unit="page">82940B</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Ensemble based systems in decision making</title>
		<author>
			<persName><forename type="first">R</forename><surname>Polikar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Circuits and Systems Magazine</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="21" to="45" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Ensemble-vis: A framework for the statistical visualization of ensemble data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Potter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Bremer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Doutriaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pascucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Data Mining Workshops</title>
				<meeting>IEEE International Conference on Data Mining Workshops</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Correlative multi-label video annotation</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM International Conference on Multimedia</title>
				<meeting>the 15th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="17" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
		<title level="m">C4. 5: Programs for Machine Learning</title>
				<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Visualizing the hidden activity of artificial neural networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Fadel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Falcao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Telea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="101" to="110" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Classifier chains for multi-label classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="333" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Ensemble-based classifiers</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rokach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="39" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A circumplex model of affect</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1161</biblScope>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Vis4ml: An ontology for visual analytics assisted machine learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sacha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kraus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="385" to="395" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Ensemble learning: A survey</title>
		<author>
			<persName><forename type="first">O</forename><surname>Sagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rokach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">e1249</biblScope>
			<date type="published" when="2018">2018</date>
			<publisher>Wiley Interdisciplinary Reviews</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Noodles: A tool for visualization of numerical weather model ensemble uncertainty</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Amburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Moorhead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1421" to="1430" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Boostexter: A boosting-based system for text categorization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="135" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Rankexplorer: Visualization of ranking changes in large time series data</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2669" to="2678" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Ground truth for automatic music mood classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Skowronek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Mckinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van De Par</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ISMIR</title>
				<meeting>the ISMIR</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="395" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Harmonicity and dynamics-based features for audio</title>
		<author>
			<persName><forename type="first">H</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
				<meeting>IEEE International Conference on Acoustics, Speech, and Signal Processing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="iv" to="iv" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">An analysis of machine-and humananalytics in classification</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="80" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Thayer</surname></persName>
		</author>
		<title level="m">The Biopsychology of Mood and Arousal</title>
				<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Multi-label classification: An overview</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Katakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Data Warehousing and Mining</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Mining multi-label data. Data Mining and Knowledge Discovery Handbook</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Katakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Vlahavas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="667" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Random k-labelsets for multilabel classification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Katakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Vlahavas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1079" to="1089" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Visualization and visual analysis of ensemble data: A survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2853" to="2872" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Contour boxplots: A method for characterizing uncertainty in feature sets from simulation ensembles</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Whitaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirzargar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Kirby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2713" to="2722" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Multi-label classification of emotions in music</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wieczorkowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Synak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">W</forename><surname>Raś</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Information Processing and Web Mining</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="307" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Stacked generalization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Wolpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="241" to="259" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Seqdynamics: Visual analytics for evaluating online problem-solving dynamics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Computer Graphics Forum</title>
				<meeting>Computer Graphics Forum</meeting>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="511" to="522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Automatic music classification and summarization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Maddage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="441" to="450" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Emotioncues: Emotion-oriented visual summarization of classroom videos</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-C</forename><surname>Pong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3168" to="3181" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Multilabel neural networks with applications to functional genomics and text categorization</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1338" to="1351" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Ml-knn: A lazy learning approach to multi-label learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2038" to="2048" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Audio segmentation based on multi-scale audio classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
				<meeting>IEEE International Conference on Acoustics, Speech, and Signal Processing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="iv" to="iv" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Musicoder: A universal music-acoustic encoder based on transformer</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Multimedia Modeling</title>
				<meeting>the International Conference on Multimedia Modeling</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="417" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Deep forest</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">National Science Review</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="74" to="86" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Music genre classification with transformer classifier</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Digital Signal Processing</title>
				<meeting>the 4th International Conference on Digital Signal Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="155" to="159" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
