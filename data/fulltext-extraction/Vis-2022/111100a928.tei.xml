<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sporthesia: Augmenting Sports Videos Using Natural Language</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhutian</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Qisen</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiao</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Johanna</forename><surname>Beyer</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Haijun</forename><surname>Xia</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yingcai</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
						</author>
						<title level="a" type="main">Sporthesia: Augmenting Sports Videos Using Natural Language</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-03-27T09:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Augmented Sports Videos</term>
					<term>Language-driven Authoring Tool</term>
					<term>Video-based Visualization</term>
					<term>Sports Visualization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sporthesia</head><p>Video Processor "…gets him close to the backhand court" Video Commentary "Federer looks to be covering the crosscourt, which gets him close to the backhand court. " + "Feder looks to be covering the crosscourt…" a b c</p><p>Fig. <ref type="figure">1</ref>: Sporthesia takes raw video footage and commentary text of racket-based sports as input, and outputs an augmented video. To achieve this, three key steps are taken: 1) detecting the visualizable entities in the text, 2) mapping the entities to visualizations, and 3) scheduling the visualizations to play with the raw video.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Augmented sports videos are becoming increasingly popular as a form to present sports data. In recent years, an increasing amount of data has been collected during sports activities thanks to the advances in high-speed cameras and computer vision (CV) techniques. While this data plays a central role in understanding players' performance and developing winning strategies, it can be challenging to understand the data without presenting it in its physical context. Augmented sports videos can present sports data directly in actual scenes through embedded visualizations and video effects, communicating insights and explaining player strategies in an intuitive and engaging manner. Thus, augmented sports videos have been widely used by TV channels <ref type="bibr" target="#b18">[19]</ref>, fan clubs <ref type="bibr" target="#b59">[60]</ref>, and analysts <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b53">54]</ref> to present sports data, influencing billions of sports enthusiasts around the world.</p><p>However, creating augmented sports videos is a demanding and time-consuming task <ref type="bibr">[12]</ref>, as it requires skills in areas such as data analysis, data visualizations, and video editing. The gap between the difficulty in creating augmented sports videos and the strong market demand for augmented sports videos has spawned very successful commercial systems, such as Viz Libero <ref type="bibr">[61]</ref> and Piero <ref type="bibr" target="#b4">[5]</ref>. These systems, however, target expert video editors and require the manipulation of low-level graphical elements. This leads to a high entry barrier for sports analysts, who usually focus on presenting analytical insights and lack sufficient video editing skills. Recently, Chen et al. <ref type="bibr">[12]</ref> presented VisCommentator, a fast prototyping tool for augmenting table tennis videos, enabling analysts to augment the video by interacting with the data of video objects instead of low-level graphical elements. However, analysts often express their findings as high-level insights, such as "Federer hits a backhand down the line", rather than data (e.g., the player's position and the ball trajectory). Consequently, to visualize an insight, an analyst usually needs to "translate" the insight into data equivalents and then map them to visualizations embedded in the videos, which is tedious and has a heavy cognitive load.</p><p>However, the tedious "translation" process implies a latent mapping between the high-level insights and the visualizations. Such a latent mapping provides an opportunity to facilitate the creation of augmented sports videos by directly creating visualizations to augment the videos based on the user's insights. In sports, perhaps the most common way to express insights is using natural language, e.g., commentators give realtime comments on live games, analysts report their analytical findings of sports videos in oral presentations, and journalists summarize key events in textual documents. This work, thus, explores how to leverage natural language to facilitate the creation of augmented sports videos.</p><p>This work aims to augment a sports video clip based on a given commentary text. To achieve this goal, we first identify three tasks inspired by existing text-to-visuals systems <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b70">72,</ref><ref type="bibr" target="#b71">73]</ref>: 1) detecting the visualizable entities in the text, 2) mapping these entities into visualizations, 3) scheduling these visualizations to play with the sports video. To tackle these three tasks, we collected and analyzed 155 sports video clips, as well as their corresponding commentaries, of six different sports with three main questions in mind -What text entities can be visualized (Q1)?; How can we visualize these entities (Q2)?; and How do we schedule these visualizations with the video (Q3)? Based on our formative study, we have identified that four categories of entities, namely, object, action, data, and emotional cue, in the commentaries can be visualized, entities in different categories can be visualized by different embedded visualizations, and the scheduling of visualizations depends on the style of the commentaries, i.e., analyst or play-by-play, in which the video is paused or not, respectively.</p><p>Based on the findings in our formative study, we have designed and developed Sporthesia (Fig. <ref type="figure">1</ref>), a proof-of-concept system that takes textual commentaries, racket-based sports (e.g., table tennis, tennis) videos, and sports data (e.g., player and ball positions, key events) as the input to produce augmented sports videos. The inputted sports data can be extracted from the videos by using CV models or manually prepared. In contrast to most existing language-driven visualization creation tools, Sporthesia sees the text as an information source instead of a command (e.g., "show me the bar chart!") to create the visualizations. Sporthesia features three components, i.e., Entity Detector, Entity Visualizer, and Visualization Scheduler, to complete the aforementioned three tasks. Behind these three components is a set of state-of-the-art natural language processing (NLP) models.</p><p>Sporthesia is a technique that can be applied in different application scenarios. To demonstrate the usage of Sporthesia, we exemplify two application scenarios, including authoring augmented racket-based sports videos using text and augmenting historical sports videos based on auditory comments. To evaluate the effectiveness of Sporthesia, we first conducted a technical evaluation that focuses on the accuracy of the Entity Detector, which is the foremost step in the pipeline, and achieved a good performance of an F1 score of 0.9. A task-based expert evaluation with eight sports analysts confirmed the utility, effectiveness, and high satisfaction of our language-driven creation method. We also discuss promising future opportunities implied by observations and feedback from the study.</p><p>In summary, our main contributions are threefold: First, we conduct a formative study and subsequent identification and discussion of design considerations for augmenting sports videos based on commentary text. Second, we present the design and implementation of Sporthesia, a proof-of-concept system that augments racket-based sports videos based on a piece of commentary text. Third, we demonstrate two exemplar applications based on Sporthesia and report on a technical evaluation and expert feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Embedded Visualization in Sports Videos. The advances in highspeed cameras and CV techniques have made sports data from videos increasingly available <ref type="bibr" target="#b40">[41]</ref>. To present the data in a meaningful context, researchers have proposed methods that visualize the data together with the videos, such as side-by-side <ref type="bibr" target="#b24">[25]</ref> and embedded views <ref type="bibr" target="#b53">[54]</ref>. This work focuses on embedded visualizations in sports videos.</p><p>The idea of embedding sports data in its physical context is not new. In the early stage, researchers embedded visualizations in court diagrams, which can be seen as a simplification of the real-world scene <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b61">63]</ref>. Recently, with more powerful graphics cards and advanced CV techniques, researchers have started to explore ways to embed visualizations in sports videos directly <ref type="bibr" target="#b23">[24]</ref>. For example, Stein et al. <ref type="bibr" target="#b53">[54]</ref> introduced a system that takes raw footage of soccer games as the input and automatically visualizes relevant analytic measures of the players in the video. Stein et al. <ref type="bibr" target="#b52">[53]</ref> further extended their work with a framework that semi-automatically decides what measures should be presented at a specific moment. However, most of these research systems were developed for exploration purposes and thus do not allow users to visualize their insights in the videos for communication purposes. On the other hand, industry companies have developed commercial systems to assist in creating embedded visualizations in sports videos. For instance, Piero <ref type="bibr" target="#b4">[5]</ref> and Viz Libero [61] are powerful video editing tools that have been used for annotating sportscasting and producing TV programs. CourtVision <ref type="bibr" target="#b13">[14]</ref>, developed by Second Spectrum <ref type="bibr" target="#b48">[49]</ref>, is a basketball watching system that automatically embeds players' status information in basketball videos to engage the audience. However, these industrial products target proficient video editors, who focus on the manipulations of graphical marks, leading to complex interface actions and a steep learning curve for sports analysts.</p><p>Perhaps the most closely related work to the present research is Vis-Commentator <ref type="bibr">[12]</ref>, which is an application that enables sports analysts to create augmented sports videos by selecting sports data. Our work aims to further ease the creation process and support users to augment sports videos by directly expressing sports insights in natural language. Compared to VisCommentator, our system is a modular technique that provides a higher abstraction level, leading to a different system design, interactions, and technical implementations.</p><p>Natural Language Interfaces for Visualization. Recent achievements in NLP have reignited interest in using natural language interfaces (NLIs) for creating data visualizations. Compared to traditional visualization creation tools, systems with NLIs enable users to express their intentions via natural language rather than interface actions or coding, thereby lowering the barrier to visualizing data.</p><p>Existing NLI systems can be roughly divided into explicit and implicit approaches. Explicit NLI systems <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b51">52]</ref> treat the natural language as commands and require users to illustrate their intentions explicitly. For example, DataTone <ref type="bibr" target="#b25">[26]</ref> allows users to create visualizations of their desired data by typing, e.g., "Show me medals for hockey and skating by country". The NL4DV toolkit <ref type="bibr" target="#b37">[38]</ref> takes a tabular dataset and a text query as the input and outputs visualizations in the form of JSON specifications. On the other hand, implicit NLI systems view the text descriptions as another representation of the visual content, automatically converting the text to visual content and thus enabling users to create visual content implicitly. Extensive research in computer vision, computer graphics, and human-computer interaction has explored the automatic conversion of descriptive text into visual content, such as images <ref type="bibr" target="#b70">[72,</ref><ref type="bibr" target="#b71">73]</ref>, 3D shapes <ref type="bibr" target="#b8">[9]</ref> and scenes <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15]</ref>, documents <ref type="bibr" target="#b10">[11]</ref>, and short video clips <ref type="bibr" target="#b34">[35]</ref>. In recent years, with the development of generative adversarial networks, a plethora of systems <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b67">69,</ref><ref type="bibr" target="#b72">74,</ref><ref type="bibr" target="#b74">76,</ref><ref type="bibr" target="#b75">77]</ref> have been proposed to generate visual content based on text descriptions. However, none of those works investigates generating augmented sports videos from textual commentaries.</p><p>Our work employs an implicit approach and enables users to create augmented videos by expressing insights in natural language. In the visualization community, only few implicit NLI systems for visualization creation exist. A representative example is Text-to-Viz <ref type="bibr" target="#b15">[16]</ref>, which extracts semantic information from the user's description of insights and maps it into static infographics. We share a similar spirit but focus on creating visualizations from text to augment sports videos, which poses extra challenges as both the text and video need to be considered in the creation process and the resulting visualizations need to be dynamic.</p><p>Auto-Generation Techniques for Data Visualization. Creating data visualizations usually involves exploring data to discover insights and mapping them into proper visualizations, both of which are demanding and time-consuming. Thus, to ease the creation process, researchers have developed techniques to automate or semi-automate the data exploration and visual mapping steps. For example, to facilitate the data exploration step, DataShot <ref type="bibr" target="#b62">[64]</ref> employs an auto-insight technique to suggest interesting data patterns from spreadsheets and generated factsheets. By using a pattern detection engine, DataToon <ref type="bibr" target="#b29">[30]</ref>, an authoring tool for data comics, automatically suggested salient patterns of the input network data. To ease the visual mapping step, prior research has proposed template-based methods to automatically map a subset of data to different types of visualizations, including charts <ref type="bibr" target="#b66">[68]</ref>, infographics <ref type="bibr" target="#b62">[64]</ref>, and data animations <ref type="bibr" target="#b1">[2]</ref>. The templates in these systems are usually hand-crafted based on prior knowledge and empirical studies. Research systems, such as DeepEye <ref type="bibr" target="#b33">[34]</ref>, Draco <ref type="bibr" target="#b35">[36]</ref>, VizML <ref type="bibr" target="#b28">[29]</ref>, and AutoTimeline <ref type="bibr" target="#b9">[10]</ref>, have used machine learning models to automatically learn and extract templates from existing visualizations.</p><p>In contrast to these systems, our work focuses on extracting information from natural language rather than structured datasets and converting it to embedded visualizations for augmenting sports videos. In this sense, our system is closer to Text-to-Viz <ref type="bibr" target="#b15">[16]</ref>, which generates infographics based on text, but has a very different output, augmented sports videos, which is more difficult and constrained. Immersive Sports Visualization. Fundamentally, augmented sports videos share a similar spirit with immersive sports visualization, which leverages virtual or augmented reality devices to visualize sports data in either simulated <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b73">75]</ref> or real courts <ref type="bibr" target="#b32">[33]</ref>. While the immersive visualizations in these systems have proven to be effective for sports data analysis, most of them are predefined and lack customizability. Thus, users cannot flexibly create new immersive visualizations to express their intentions. Our work adds to the direction of immersive sports visualization but focuses on helping users create visualizations to present data in the physical context shown in a video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TEXT TO AUGMENTED VIDEOS: A THREE-STEP APPROACH</head><p>The goal of this work is to augment a sports video clip based on a given commentary text by automatically converting the text into embedded visualizations. To achieve this goal, inspired by existing text-to-visuals systems <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b70">72,</ref><ref type="bibr" target="#b71">73]</ref>, we propose a three-step approach (Fig. <ref type="figure">2</ref>) that decomposes the problem into three tasks: 1) detecting the visualizable entities in the text, 2) mapping the entities to visualizations, and 3) scheduling the visualizations to play with the video. These tasks lead us to the following three questions: Q1-What text entities can be visualized?; Q2-How can we visualize these entities?; Q3-How do we schedule these visualizations with the video? To understand these questions, we conducted a formative study by collecting and analyzing 155 sports video clips and their accompanying commentaries. Nole crosscourt with a sharp angle. He is brilliant! Fig. <ref type="figure">2:</ref> A three-step approach to augment sports videos with embedded visualizations based on text commentary. The three steps include detecting visualizable entities in the text, mapping them to visualizations, and scheduling the visualizations in the video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Collection and Analysis</head><p>There are many publicly available text sources that comment on sports videos, including sports commentaries, game-related reports, articles, and open discussions (e.g., posts in online forums). In this work, we decided to collect sports commentaries since they are usually given by sports experts and contain rich insights.</p><p>Collection. Following the methodology in [12], we harvested a collection of commentaries that cover three team-based sports (i.e., basketball, soccer, and American football) and three racket-based sports (i.e., tennis, badminton, and table tennis) from the internet. Specifically, we searched sports videos with English audio commentaries from Google Videos by using the keywords "SPORT + full match", where SPORT is one of the six ball sports. We downloaded five videos for each sport from the top query results, totally gathering 30 sports videos lasting over 3600 minutes. Our collection considered both the quality (i.e., millions of views) and diversity (i.e., from various TV channels, including member-only ones such as ESPN+). Most of the videos were final games of famous sports events, such as the Olympic Games, FIFA world cups, NBA games, Grand Slams, and the Super Bowl. Note that our unit of analysis was not an entire game but a specific meaningful moment in the game (e.g., a goal, a rally). Thus, for each video, we manually sampled at least four clips following two criteria: 1) the clip should cover a highlighted moment (curated by TV channels) of the game; 2) the commentaries of this clip should be closely related to the sports event happening in the clip (i.e., commentaries about player anecdotes were thus excluded). Finally, our dataset included 155 clips lasting 92 minutes, which aligns with similar visualization research <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b56">57]</ref>. All videos were transcribed manually by native English speakers, resulting in 12545 words. Figure <ref type="figure" target="#fig_1">3a and b</ref> show the average duration of the videos and the average number of words in the commentaries of different sports.</p><p>Analysis. We analyzed the dataset both qualitatively and quantitatively. Three of the authors first followed an open coding process to analyze the 155 clips independently, with the three questions (Q1-3) in mind. The codes were then refined through multiple rounds of discussions with other co-authors. The investigation also referred to prior research on text-driven visual content generation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b71">73]</ref>, sports visualizations <ref type="bibr" target="#b40">[41]</ref>, data-driven videos and animated graphics <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b56">57]</ref>, and augmented sports videos <ref type="bibr">[12]</ref>. Findings revealed that there are four categories of entities that can be visualized in the text (Q1), entities in different categories can be visualized with different embedded visualizations (Q2), and these visualizations can be scheduled with the video in two different ways (Q3). We further conducted a quantitative analysis to count the occurrences of the four categories of entities in the dataset (Fig. <ref type="figure" target="#fig_1">3c</ref>). The findings are detailed in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Q1: What Text Entities Can Be Visualized?</head><p>Different from traditional visualization systems that generate visualizations based on structural data (e.g., spreadsheets), our goal is to generate visualizations from sports commentaries, which can be unstructured, messy, and full of uncertainty. Thus, the first step to achieving our goal is to recognize the visualizable entities in the text. According to our analysis of the dataset, we identified four categories of visualizable entities (highlighted in typewriter font), which are introduced from concrete, objective to abstract, subjective: E1 Objects are physical entities that can be seen in the video, which usually serve as the referent for other visualizations. Among all objects, we found two kinds of objects were mentioned most frequently, namely, players (68.83%) and places in the court (10.73%). Players were usually mentioned by their names and pronouns. All objects are noun words in the commentaries. E2 Actions are performed by objects and can also be seen in the video. In the dataset, we found that actions can be roughly divided into sports-general (80.27%) and sports-specific (19.63%). Sports-general actions, such as hit, run, and cover, are usually verbs and exist in all the six ball sports. In contrast, sports-specific actions are terms used in specific sports, such as down the line in tennis, pick and roll in basketball. Domain-specific actions can be nouns (e.g., crosscourt) or adjectives (e.g., backhand), both of which can be used as verbs in the commentary, such as "Federer backhand on the run," "Djokovic down the line." E3 Data is usually generated by actions and cannot be seen in the video. Prior research <ref type="bibr" target="#b40">[41]</ref> categorized data in sports videos into tracking data, which is inherently spatial and temporal, and non-tracking data, which is rather abstract. We also found these two kinds of data were brought up in the commentaries (16% and 84% for tracking and non-tracking data, respectively). Data in the commentaries usually are noun words or numbers, such as speed, 5 meters, winning rate, and won 64%. E4 Emotional Cues are the subjective feeling of a game expressed by the commentators, adding to the exciting atmosphere of the game. Emotional cues cannot be seen in the video but can be felt in the game and the commentaries. In the dataset, emotional cues can be adjectives (e.g., "Phenomenal!"), interjections (e.g., "Wow!"), or analogies (e.g., "make it just like Quidditch.") Relationships Among Entities. In a commentary, the visualizable entities are usually not isolated -instead, they are inherently connected.</p><p>As discussed above, an object can perform an action that generates data. We noticed that such kinds of relationships are embedded in the linguistic structures of the natural language, which can, and should, be extracted and leveraged to specify the visualizations. For example, by identifying the subject (i.e., an object) of an action, we can visualize the generated tracking data in the correct position in the video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Q2: How to Visualize the Entities in the Video?</head><p>After recognizing the visualizable entities in the text, the next step is to map them into proper visualizations that can be embedded in the video. While prior work <ref type="bibr">[12]</ref> studied the visualizations of sports data (i.e., tracking or non-tracking) in videos, how to visualize commentary entities in videos remains unclear. Based on our analysis of the commentaries and the videos, multiple rounds of discussion and iterations with a domain expert (a sports science professor who provides data analysis and consultancy services for national sports teams), and the previous research on video-based sports visualizations, we propose the following methods to visualize each category of entities:</p><p>• Object entities can be directly seen in the video. Thus, to visualize object entities, we can directly highlight the corresponding objects in the video. Various types of objects can be highlighted differently. For example, we can use spotlights and rectangle marks to highlight players and places on the court, respectively. • Action entities can also be seen in the video, but they do not have a physical existence. On the other hand, actions are performed by objects and generate data. Hence, to visualize an action entity, we can highlight the object when she/he/it is performing the action or visualize the data generated by the action.</p><p>Compared to highlighting the subject of an action, visualizing the invisible data of the action can reveal more insights and better engage the audience. However, one must know what data is generated by the action to achieve the visualization, which is relatively easy for sports-general actions but can be challenging for sports-specific ones. • Data entities, according to prior research <ref type="bibr">[12]</ref>, can be visualized in the video by mapping them to different visual representations based on their types. Specifically, we can naturally embed track-ing data into the video as it is always associated with a specific space and time in the video. For non-tracking data, we can annotate the video with labels to show it. • Emotional Cues are the most abstract entities and are sometimes not directly associated with the objects in the video. To visualize emotional cue entities, a straightforward way is to display semantic-related pictograms, such as emojis, in the video. While other more advanced methods are possible, we consider exploring them as beyond the scope of this work, as the research of affective visualization is still in its infancy <ref type="bibr" target="#b30">[31]</ref>.</p><p>In summary, entities in different categories can be visualized differently, such as highlighting the entities in the video (object, action), visualizing the data generated by the entities (action), embedding or annotating the entities into the video (data), or presenting the entities using emojis (emotional cue). All these methods map the entities into visualizations embedded in the video. To select the specific visualizations, we follow previous work <ref type="bibr">[12]</ref> that summarized a design space of embedded visualizations in augmented sports videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Q3: How to Schedule the Visualizations in the Video?</head><p>After mapping the entities into visualizations, the last step is to schedule the visualizations in the video. This step entails deciding when to display a visualization and how long to display it for. This, intuitively, depends on several factors, including the text, the video, and the visualizations themselves. In our analysis, we noticed that this question is particularly related to the commentary style of the text. Specifically, there are two major types of sports commentators -play-by-play commentators, who need to articulate each play and event of an often fast-moving sports game, and analyst commentators, who provide expert analysis and background information of the game. These two types of commentaries lead to two different rendering methods, wherein the visualizations are scheduled differently:</p><p>• Play-by-play mode renders the visualizations without pausing the video, since the visualizations are generated by commentaries that describe the ongoing content of the video. In this mode, the scheduling of the visualizations depends on both the text and the video. For example, the visualization of an action should be displayed when it is mentioned in the text and disappear when the action is finished in the video. • Analyst mode usually renders the visualizations by pausing the video, since the commentaries are often given during a break or replay of the game and contain too much information to be visualized in a short moment. In this mode, the scheduling of the visualizations only depends on the text since the video is paused. Thus, the start time and duration of a visualization should be decided by when and how it is mentioned in the text.</p><p>While we divide the rendering and scheduling methods of visualizations into two types based on the commentary styles, they are not meant to be the only solution. For example, an analyst-style commentary may also be able to be rendered without pausing the video. We leave the comprehensive exploration of the rendering and scheduling of visualizations in augmented sports videos for future research. Fig. <ref type="figure">4</ref>: Sporthesia detects the visualizable entities in the text (a1) and groups them into semantic units (a2). Next, the entities are mapped to visualizations (b1) with arguments specified by the semantic units (b2). Finally, the system initializes and calibrates the schedules of the visualizations based on the reading time of the text and the video events (c). All three steps are built upon the video processing components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SPORTHESIA: SYSTEM DESIGN AND IMPLEMENTATION</head><p>To realize the three-step approach, we design and implement Sporthesia, a proof-of-concept system that creates augmented videos for racketbased sports. Sporthesia consists of three major components -Entity Detector, Entity Visualizer, and Visualization Scheduler -for each step in the approach, respectively. It takes a piece of text, a video clip, and sports data extracted from the video clip as the input, and outputs an augmented video. Figure <ref type="figure">4</ref> displays the pipeline of Sporthesia. We first introduce the video processing and rendering techniques Sporthesia is built upon, followed by details of the three components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Video Data Extraction and Rendering</head><p>While this research particularly focuses on leveraging natural language to create augmented sports videos, the implementation of Sporthesia is built based on advanced CV techniques. We follow previous work <ref type="bibr">[12]</ref> and use several machine learning models to extract data from the videos. Specifically, we use Detectron2 <ref type="bibr" target="#b69">[71]</ref>, a detection and segmentation platform that features a mask-rcnn <ref type="bibr" target="#b26">[27]</ref> with an ImageNet <ref type="bibr" target="#b16">[17]</ref> pretrained RestNet-50 <ref type="bibr" target="#b27">[28]</ref> as the backbone to detect the bounding boxes of the players and ball, the skeletons of the players, and the court lines. We also use TTNet <ref type="bibr" target="#b60">[62]</ref> to detect ball events, including ball bounce and net hit. For player events, we utilize the distance between the ball and the player's right hand to detect stroke events. The extracted data is used in the following steps for creating visualizations to augment the videos. We refer readers to <ref type="bibr">[12]</ref> for the technical details.</p><p>To render visualizations in the videos, we further need to know the camera parameters and the player identities (i.e., who is far from or near the camera). While the camera parameters can be obtained using camera calibration techniques <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b76">78]</ref> on the court lines detected from the video, we treat them and the player identities as known meta information in the current implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Entity Detector</head><p>The first step of the framework is to detect the visualizable entities in the text (Fig. <ref type="figure">4a1</ref>), i.e., objects, actions, data, and emotional cues. Additionally, we need to extract their relationships and group them into semantic meaningful units (Fig. <ref type="figure">4a2</ref>). To this end, we leverage a series of state-of-the-art NLP techniques to process the input text: Detecting Visualizable Entities. Detecting entities in a piece of text is a fundamental task in NLP called Name Entity Recognition (NER) <ref type="bibr" target="#b36">[37]</ref>, which locates and classifies segments in the text into predefined categories, such as person, location, organization, etc. To detect the entities in a sentence, three steps are taken: 1) tokenizing the sentence into a word sequence, 2) converting the tokens into feature vectors, and 3) classifying the feature vector of each token into categories. We achieved these three steps by using Spacy <ref type="bibr" target="#b19">[20]</ref>, an industrial-strength NLP toolkit that provides pre-trained transformer-based <ref type="bibr" target="#b17">[18]</ref> language models to tokenize, featurize, and recognize entities in a sentence. To improve the recognition performance of Spacy, we fine-tuned its pre-trained model with the commentary examples we collected and extended its pattern matching step with sports glossaries collected from Wikipedia <ref type="bibr" target="#b63">[65,</ref><ref type="bibr" target="#b64">66]</ref>.</p><p>We found that some expressions, such as he and here, refer to other entities in the text and need to be resolved to be able to visualize them. Thus, to solve this issue, we employed a neural co-reference resolution model <ref type="bibr" target="#b21">[22]</ref>, which can find all expressions that refer to the same entity in a text. In sum, the output of this step is a list of entities with their categories and pointers to their referents if they exist.</p><p>Grouping the Entities into Semantic Units. As discussed in Sec. 3.2, the entities in the text are usually connected together at the semantic level, e.g., an object performs an action that generates data. Such semantic relationships are critical to specify the visualizations. Take "Federer hits the ball to the backhand court" as an example, in which four entities are detected, i.e., Federer, hits, ball, and backhand court. We cannot visualize hits and backhand court without knowing the subject and possessive noun. To extract the semantic relationships among the entities, we leverage the Semantic Role Labeling (SRL) <ref type="bibr" target="#b38">[39]</ref> technique, which detects the latent predicateargument structure of a sentence and classifies the roles of each word. For example, hits will be detected as a predicate with Federer, ball, and backhand court as argument 0, 1, and 2, respectively. With this structure, we can easily group the entities into units like who (argument 0) did what (predicate) to whom (argument 1) in what ways (argument 2). In our implementation, we use a BERT-based model <ref type="bibr" target="#b44">[45]</ref> to detect and classify the semantic roles of the entities. The output of this step is a list of units organized in the predicate-argument structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Entity Visualizer</head><p>The next step is to visualize the detected entities, which we achieve by mapping the entities to visualizations (Fig. <ref type="figure">4b1</ref>) and specifying the visualizations' arguments based on the text (Fig. <ref type="figure">4b2</ref>):</p><p>Mapping the Entities to Visualizations. As discussed in Sec. 3.3, different categories of entities can be mapped to different visualizations. To achieve the visual mappings, we developed a dictionary based on a design space of augmented sports videos [12] and an emoji searching engine. For example, players (object) are mapped to spotlight highlight effects; ball angles (data) are mapped to embedded visualizations in the court; "brilliant" (emotional cue) is mapped to a celebration emoji. More details can be found in the supplemental material. A particular challenge is to map actions to visualizations, especially for the sports-specific actions, such as crosscourt, which are abstract and usually require case-by-case designs. Two steps are thus taken to tackle this challenge:</p><p>1. Mapping sports-general actions to tracking-data: In the dictionary, we manually specify the mappings from sports-general actions to the tracking data they generate, i.e., run and hit are mapped to the player trajectory and ball trajectory, respectively. The mappings are initialized with 21 sports-general actions and then extended with their synonyms using word embedding <ref type="bibr" target="#b19">[20]</ref>, e.g., hit is extended with stroke, shoot, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Mapping sports-specific actions to sports-general ones:</head><p>Sports-specific actions are terminologies that are difficult to be extended from other actions using synonyms matching.</p><p>To map sports-specific actions to the tracking data in a generalizable way, we leverage sports glossaries that explain these terminologies in plain text. For instance, crosscourt is explained as "Hitting the ball into the diagonal court" in a tennis glossary <ref type="bibr" target="#b65">[67]</ref>. Based on this explanation, we can use synonym matching to map crosscourt to hit and then map to ball trajectory in our dictionary. With these two steps, we can map the actions to the tracking data they generated, which will be mapped to embedded visualizations. Fig. <ref type="figure">5</ref>: a) The visualization of hit is manually specified, which takes two arguments, i.e., from and to. b) The visualization of crosscourt can be generated based on its text explanation in the tennis glossary, which is a variant of hit with a default argument, diagonal court.</p><p>Specifying the Arguments of the Visualizations. To visualize the entities in the video, we further need to specify the arguments of the visualizations, which can be extracted from the text based on the semantic relationships among entities. Take "Federer hits the ball to the backhand court" as an example. The visualization of the hit action needs two arguments, i.e., from and to (Fig. <ref type="figure">5a</ref>), which can be the argument 0 (e.g., Federer) and argument 2 (e.g., backhand court) in the text. Some sports-specific actions can infer some arguments for their visualizations based on the text explanation. For example, by applying the SRL technique to the text explanation of crosscourt (Fig. <ref type="figure">5b</ref>), we can set the diagonal court as the default value for the argument to. In this sense, the visualization of crosscourt can be seen as the one of hit with a default argument, diagonal court. More details of visualization arguments are in the supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Visualization Scheduler</head><p>Lastly, to embed the visualizations into the video, we need to decide when a visualization should appear and disappear in the video. We initialize the time schedules of the visualizations based on the text (Fig. <ref type="figure">6</ref> left). The initialized schedules can be used to render the visualizations in analyst mode. When rendering in play-by-play mode, we further calibrate the schedules based on the video events (Fig. <ref type="figure">6 right</ref>). Initialization. Based on our analysis in Sec. 3.4, we use the text to initialize the schedules of the visualizations. Intuitively, a visualization should be displayed when its corresponding entity is read in the sentence. Thus, we employed a text-to-speech neural network <ref type="bibr" target="#b43">[44]</ref> to generate natural speech audio for the input sentence. With the speech audio, we can obtain the start reading time of each entity, which is used as the appearance time for the visualization of the entity. Next, visualizations within the same semantic unit are scheduled to disappear at the end of reading the last word of the unit to emphasize their connections. In analyst mode (i.e., when visualizations are shown as an inserted animation while pausing the video), this initial scheduling is sufficient. However, when rendering in play-by-play mode, the scheduling needs to be further calibrated. Nole hits to the court.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Audio Appearance of Vis</head><p>Fig. <ref type="figure">6</ref>: Left: The text is converted into audio that initializes the appearance time of each visualization. This initialized schedule can be used to render the visualizations in analyst mode. Right: When rendering in play-by-play mode, the appearance times of some visualizations are further calibrated based on video events.</p><p>Calibration. When rendering in play-by-play mode, the visualizations of action and its argument entities must match the corresponding events. For example, the visualization of hit and court (Fig. <ref type="figure">6</ref> left) should only appear when the player hits the ball and the ball touches the ground, respectively. Thus, we calibrate the schedules of actions and their arguments based on the events detected in the video (Fig. <ref type="figure">6</ref> right). Specifically, for each action and its argument entities, we look up the corresponding event in the video by examining the type and time interval of the video events. If a corresponding event is found, we use the start time of the event as the appearance time for the visualization. Nevertheless, the schedules could still be sub-optimal, which should be manually refined by the users via external validity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">External Validity</head><p>As an intelligent system, Sporthesia inevitably might derive sub-optimal results due to the imperfect underlying machine learning models and the limited mapping dictionary. To support error recovery and visualization personalization, several methods for external validity can be introduced to the system. First, we can leverage the text entities as an representation to allow users to modify the system outputs. Specifically, the users can select an entity and open a context menu to modify its corresponded visualization, as well as the arguments and time schedule of the visualization. Second, the dictionary of Entity Visualizer should be configurable so that users can modify the mappings persistently. Last, the time schedules of visualizations can be visualized along with the video timeline, enabling users to understand and adjust the schedules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">APPLICATION SCENARIOS</head><p>Sporthesia is a technique that can be employed in different scenarios to augment racket-based sports videos. In this work, we implemented two application prototypes to exemplify the usage of Sporthesia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Application I: Authoring Augmented Sports Videos</head><p>VisCommentator [12] is an authoring tool for creating racket-based augmented sports videos. We integrated Sporthesia into VisCommentator as a sub-system to enable analysts to create augmented sports videos by directly expressing high-level insights in natural language.</p><p>To achieve the integration, we modify the user interface (UI) of VisCommentator to allow text input and connect its data extractor and renderer to Sporthesia. Specifically, when a user brushes on the timeline (Fig. <ref type="figure" target="#fig_5">7b1</ref>), a text input field will show up on the right panel (Fig. <ref type="figure" target="#fig_5">7c</ref>). The user can then type in the input field to comment on the video. Once the user presses the play button, the text and the data extracted from the video will be passed to Sporthesia, which generates and schedules embedded visualizations through the three components (Fig. <ref type="figure">4</ref>). The scheduled embedded visualizations will be rendered into the video by the renderer of VisCommentator.</p><p>Additionally, we also provide UI to support external validity of Sporthesia. The text entities visualized in the video will be highlighted in the text input field (Fig. <ref type="figure" target="#fig_5">7c</ref>). Users can right click on an entity to assess and modify how it is visualized. For example, a user can configure the visualization and schedule of the entity distance in Fig. <ref type="figure" target="#fig_5">7d</ref>. Users can also manually create or remove an entity to create their own visualizations if the detection is incorrect, and switch to the "edit" panel to modify other settings such as the visual mapping dictionary. After integrating with Sporthesia, VisCommentator enables users to create augmented sports videos more efficiently. For example, users can simply highlight objects or actions in the video through text (Fig. <ref type="figure" target="#fig_5">7d  and e</ref>). Besides, users can comment on a moment using a more complex sentence (Fig. <ref type="figure">1</ref>) or on multiple moments (Fig. <ref type="figure" target="#fig_5">7b1 and b2</ref>) to create a series of augmented clips (Fig. <ref type="figure" target="#fig_5">7a</ref>, f, and g).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Application II: Augmenting Archive Sports Videos</head><p>Many sports events, such as the Wimbledon Championships, are broadcast on TV, recorded as videos with audio commentaries, and released on online platforms (e.g., YouTube). While these videos are widely used for analysis or entertainment purposes, the audio commentaries are usually not fully exploited. A promising use case is to leverage the audio commentaries to generate visualizations to augment the video, thereby facilitating the analysis of the games and increasing the engagement of watching experiences.</p><p>To augment racket-based sports videos by leveraging the audio commentaries, we implemented an examplar application that integrates a speech-to-text (STT) neural network with Sporthesia. Specifically, the application takes a video clip with audio commentaries as the input, separates the audio track from the video, converts the audio commentaries into text by using Silero <ref type="bibr" target="#b55">[56]</ref>, an enterprise-grade pre-trained STT model, and finally generates the augmented video by using Sporthesia. Figure <ref type="figure" target="#fig_6">8</ref> presents an example produced by the application. The input video is a BBC sports clip of the women's final in the 2019 Wimbledon Championships. In the generated augmented video, the ball trajectory and the court inside the baseline are visualized when the commentator describes that Halep (i.e., the player far from the camera) hits the ball "just inside the baseline" (Fig. <ref type="figure" target="#fig_6">8a</ref>). Next, the ball trajectory is visualized in yellow color to represent "Williams crosscourt." (Fig. <ref type="figure" target="#fig_6">8b</ref>) Right after Halep's forehand down the line, the commentator complimented that "this is brilliant", which is visualized as several celebration emojis around Halep in the augmented video (Fig. <ref type="figure" target="#fig_6">8c</ref>). Finally, the augmented video displays Halep's running trajectory followed by the ball trajectory and trophy emojis (Fig. <ref type="figure" target="#fig_6">8d</ref>) when the commentator said "She's now running onto the backhand and got a crosscourt for the winner."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATION</head><p>This section reports technical and expert evaluations on Sporthesia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Technical Evaluation</head><p>We evaluated the accuracy of recognizing the four categories of text entities (Fig. <ref type="figure">4a1</ref>), which is the foremost step of our approach. We did not technically evaluate other components because they are either offthe-shelf components with high accuracy (e.g., the SRL model achieves a 0.86 F1-score on benchmark datasets) or the ground truth is not available (e.g., the visualizations and schedules). Instead, we conducted expert interviews to collect qualitative feedback on the overall system. Dataset. We prepared a dataset for entity recognition by labeling the entities in each sentence of our collected commentaries (Fig. <ref type="figure" target="#fig_1">3c</ref>). Specifically, we manually labeled the start and end index of each entity as well as its category according to our analysis in Sec. 3. The label of each entity was represented as (𝑠𝑡𝑎𝑟𝑡, 𝑒𝑛𝑑, 𝑐𝑎𝑡𝑒𝑔𝑜𝑟 𝑦). We randomly split the dataset into 10 participants for 10-fold cross-validation. Model. To recognize the entities in a sentence, we use our dataset to fine-tune the pre-trained en core web trf <ref type="bibr" target="#b47">[48]</ref> model provided by Spacy <ref type="bibr" target="#b19">[20]</ref>. The en core web trf model is trained on written text such as blogs, news, comments using the transformer structure <ref type="bibr" target="#b17">[18]</ref>. We also extend its pattern matching step <ref type="bibr" target="#b20">[21]</ref> with sports glossaries. For each  Results. Table <ref type="table">.</ref> 1 shows the mean precision, recall, and F1-scores of the 10-fold cross-validation. Overall, the model achieves high accuracy across the four categories. The accuracy of object and action is comparatively higher than those of data and emotional cue since the latter have fewer data points. Note that the model, as well as its pattern matching, is data-driven, which means our Entity Detector can be improved by and generalized to other larger datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Expert Evaluation</head><p>To assess the utility and effectiveness of Sporthesia, we used Vis-Commentator as a technology probe to conduct a qualitative expert evaluation. The study aimed to evaluate whether sports analysts can create augmented videos with our system, observe their creation process, reflect on future improvements, and collect feedback about the three-step approach and how language-driven authoring can facilitate their overall workflows of presenting analytical findings.</p><p>Participants: We recruited 8 sports analysts (P1-P8; 3 female; age: 20-58) from a university sports science department. All experts majored in Sports Training with proficient experience in analyzing racket-based sports matches. P1-P4 particularly focused on analyzing tennis matches, while P5-P8 focused on table tennis. P8 was a senior sports analyst lead who had more than ten years of experience in providing consulting services for national sports teams. All the experts only had experience with lightweight video editors, e.g., Tiktok <ref type="bibr" target="#b57">[58]</ref>, rather than advanced video editing tools such as Adobe Premiere. Each participant received a gift card worth $16 at the beginning of the session, independent of their performance.</p><p>Tasks: The participants were asked to finish a training task and two creation tasks by using VisCommentator. For the first creation task, we curated a raw video T1 that contained more than five turns, in which a player lost the rally due to an unforced error, and required the experts to augment the video using commentaries in play-by-play style. The second creation task required the experts to use analyst-style comments on a video T2, in which a player lost the rally due to a forced error in less than five turns. The training task was prepared to cover all the features in the two creation tasks by providing a video T0 with more than five turns and a player lost due to a forced error. In total, we prepared six raw videos, each three for tennis and table tennis, respectively. The original commentaries of each video were removed.</p><p>We also provided the experts with a document of the vocabulary and example sentences supported by the system.</p><p>Procedure: The study began with the introduction (10 min) of the study purpose, the concept of augmented sports videos, the motivation of language-driven authoring, and the concepts in our three-step approach.</p><p>Next, we proceeded to the training task (15 min). We demonstrated the features of the system with video T0 and asked the experts to reproduce the augmented videos themselves.</p><p>After the training, we provided the experts with two raw videos (T1 and T2) for the two creation tasks (15min for each). We encouraged the experts to watch and ask questions about the videos before the creation. For each creation task, we asked the experts to comment on at least three segments of the video and to create eight visual effects. Finally, the session ended with a semi-structured interview (15 min) and a post-study questionnaire (5-Point Likert Scale). Each session was run in-lab using a 27-inch monitor, following a think-aloud protocol. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Results</head><p>All experts successfully created multiple augmented videos by using VisCommentator in the creation tasks. Figure <ref type="figure" target="#fig_7">9</ref> shows two examples of augmented table tennis videos created by the experts during the study. Unsurprisingly, all experts spoke highly of the usability of the system, considering it "easy to learn and use". These results qualitatively demonstrated the efficiency and usability of our system. The experts' feedback is summarized as below:</p><p>Usefulness: All experts confirmed the usefulness (𝜇 = 4.88) of our language-driven authoring method for sports analysts to create augmented sports videos, as it "significantly lowers the entry barriers to augmented sports videos for analysts" (P4). The experts expressed that the usefulness of our method is rooted in its "intuitiveness" and "efficiency" (P1-8), which allows analysts to create augmented sports videos in a short period without being tangled in the details of video editing. Particularly, the experts lauded that our language-driven method "occupies a unique niche" (P2) of fast prototyping augmented videos for day-to-day usages, such as discussion, presentation, and demonstration. The experts pointed out that in these scenarios, augmented videos can significantly facilitate the communication of sports insights but are unnecessary to be high-fidelity. Thus, existing video editing tools are too "heavy" while our method "can perfectly fill this gap" (P8).</p><p>Effectiveness: The design of our three-step approach was rated as effective by the experts (𝜇 = 4.65). The experts thought that the four categories of entities purposed by us were "reasonable" (P3-8) and "sufficient to present sports insights" (P3, 6-8). Some experts (P1, 2, 5) suggested that the system should detect some deep semantic meanings, such as the players' emotions and the situation of the games. The experts also agreed with our proposed visualization methods for each category and suggested that the players' emotions can be visualized by highlighting their actions (P7). As for the two scheduling methods, while most of the experts appreciated our proposed designs, some experts who focused on table tennis (P7-8) thought that play-by-play mode is not that useful as in table tennis usually both the players and ball move too fast to be augmented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Satisfaction:</head><p>The rating also reflects positive user satisfaction for the implementation of Sporthesia (𝜇 = 4.23). The experts indicated that the detector could correctly recognize the key information in their comments and the scheduler could properly arrange the visualizations by incorporating both the text orders and video events. However, comments also suggested that the dictionary that maps entities to visualizations was not comprehensive enough so that "I have to find alternative expressions to describe a tactic." (P6) We considered this can be mitigated in future improvement by extending the dictionary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Observations, Feedback, and Future Opportunities</head><p>Visualizing Deep Semantic Information in Physical Contexts. Some experts suggested that the system should be able to detect and visualize deep semantic meanings in the text. For example, P6 wanted to visualize the "tense situation" in the game; P1 noted that although he "comments the same [kinds of] actions", the visualizations should be different since he may have "different tendencies." However, extracting and visualizing the deep semantic information are both challenging tasks, which requires further study in NLP and visualizations. From the perspective of visualization, visualizing such kind of highly abstract information in a physical context remains underexplored. Recently, research in Situated Visualization <ref type="bibr" target="#b6">[7]</ref>, an emerging research topic, has conducted preliminary exploration in this direction.</p><p>Collaborative Interactions Across Abstraction Levels. While natural language can allow users to convey high-level insights, we notice that sometimes the expert wants to express low-level information that can hardly be expressed in language. For example, P1 found that it was difficult to express a specific court location in language. Instead, he would like to type "Federer moves to ..." and then use the mouse to directly click the location in the video. This interesting observation implies that when authoring augmented videos, users need interactions with different expressiveness ranging from low to high abstraction levels. However, how to unify the interactions across various abstraction levels remains an open question. Recently, Srinivasan et al. <ref type="bibr" target="#b50">[51]</ref> explored consistent multimodal interactions for data visualization on tablets, providing relevant knowledge in this interesting direction.</p><p>Opportunities Enabled by NLIs to Bridge Data Analysis and Communication. Surprisingly, the experts suggested that the NLI system not only facilitates the creation of augmented videos but also their analysis process. P5 provided that the augmentations generated based on comments can be seen as visual notes of the video, which helps him externalize and organize his thoughts. "At the beginning [of analysis,] my thoughts are fragmented..", P5 detailed that, "...visualizing them can help me to think." Such feedback suggests a promising opportunity to bridge the gap between data analysis and communication. Specifically, with our technique, a visual analytics system for sports videos can enable users to take textual notes on the videos, visualize their notes to facilitate the analysis, and gradually shift to authoring augmented sports videos to present the analytical findings. We discussed this new workflow with the experts and received very positive feedback. Thus, we suggest further exploration in this direction.</p><p>Suggestions. The experts also identified some limitations, most of which were related to system engineering maturity. For example, compositions of multiple augmentations were not supported. The experts did come across certain issues rooted in the design of our languagedriven method. First, we noticed that the experts were hesitant to type when guessing which words the system could understand. Such an issue is a long-standing challenge for users of NLI systems <ref type="bibr" target="#b49">[50]</ref>. One plausible solution is to integrate auto-complete features into the system. Second, the experts showed that the visual mappings should adapt to the specific sports. For example, the visualizations of distances between a player and a place should be different in tennis and table tennis (i.e., stand on the court vs. behind the table). Finally, the reading times of entities and the video events can be mismatched. The future system should provide interactive functions for manual corrections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION</head><p>Failure Cases Due to ML Models. During our study, we observed some failure cases in which Sporthesia cannot create augmented videos correctly. One major source of these cases is the imperfect NLP models, especially the SRL model. For some complex and long sentences, such as "Fan, now with the side of the racket that makes it spring off the rubber fast", the SRL model cannot extract the desired predict-arguments structures, leading to problematic visualizations. Moreover, the SRL model can only extract the shallow semantic meanings of the sentence but cannot understand the deep semantic relationships among entities or sentences. For example, "Dema takes a swatting backhand after this inside-out forehand from Zhendong", will lead to an error order of what commentators want to convey. In addition, the underlying CV models may also cause inappropriate rendering results due to, for example, incorrect object detection, tracking, or segmentation. Nevertheless, these issues can be addressed or mitigated with more advanced models, larger datasets, and better implementations <ref type="bibr" target="#b68">[70]</ref>. Generalizability-beyond racket-based sports. While Sporthesia is designed and implemented for tennis and table tennis, it can be generalized to other racket-or team-based sports as it is built based on a formative study of both racket-and team-based sports. Among the three components, the bottleneck for generalization lies in the Entity Visualizer, as the other two-Entity Detector (data-driven) and Visualization Scheduler (domain-agnostic)-are naturally generalizable. Entity Visualizer needs to be extended with a domain specific dictionary that maps sports actions to embedded visualizations. Such a verb-visual dictionary will increase the expressive power of Sporthesia, but also contribute to the visualization community in many directions, such as data animations and NLI systems. Another bottleneck of extending Sporthesia to team-based sports is the underlying CV models, which require detecting and tracking multiple objects. Recent advances in transformer-based models <ref type="bibr" target="#b5">[6]</ref> for video processing can be a solution.</p><p>Applicability-broader application scenarios. We have showcased that Sporthesia can be employed in various application scenarios. In the user study, the experts also suggested multiple interesting applications. On one hand, the experts believed that our technique could benefit presenters in scenarios such as teaching, group discussion, and TV broadcasting. The experts commented that when presenting insights about sports videos, the generated visualizations can "reduce the ambiguity [of spoken language] and facilitate the communication" (P8). On the other hand, some experts also indicated that our technique can be used in game-viewing systems for audiences. P8 explained that "fans can type comments to highlight players or visualize data in game watching." We consider leveraging interactive embedded visualizations to improve game watching experiences as an interesting future direction. Study Limitations. Since Sporthesia was built based on findings derived from English spoken commentaries and was only implemented for tennis and table tennis and covered the words discovered from the collected commentaries. Further studies or adaptations may still be necessary when generalizing it to other scenarios. Besides, the expert evaluations only provided qualitative feedback since the sample size is small due to the limited nature of access to experts. Finally, although the experts were satisfied with the created augmented videos, we didn't evaluate the videos from the audience's perspective. Followup empirical evaluation in real-world settings is thus suggested.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>This work aims to facilitate the creation of augmented sports videos using insights expressed in natural language. To achieve this goal, we proposed a three-step approach inspired by existing research in textto-visuals. We conducted a formative study to analyze 155 augmented sports videos and their commentaries to answer three key questions in the approach. Informed by the analysis results, we designed and implemented Sporthesia, a proof-of-concept system that creates augmented videos for racket-based sports using textual comments. To demonstrate the applicability of Sporthesia, we presented two application scenarios, i.e., authoring augmented sports videos and augmenting historical sports videos based on auditory comments. A technical evaluation showed that Sporthesia can successfully detect visualizable text entities. A user study with eight sports analysts revealed the utility, effectiveness, and high satisfaction of the system. Feedback and observations from the study suggest promising future research directions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: The average a) duration of videos and b) number of words of commentaries per sport in the collected dataset. c) The number of entities per category in different sports.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>:, to: ) "Crosscourt: Hitting the ball into the diagonal court "</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>"Fig. 7 :</head><label>7</label><figDesc>Fig. 7: The user interface of VisCommentator after being integrated with Sporthesia. Users can brush on the timeline and comment on the brushed period to e) highlight the players or places, f) visualize tracking data, and g) -h) explain insights. Users can also d) assess and modify the visualization and schedules of the text entities.</figDesc><graphic url="image-94.png" coords="7,352.33,163.68,93.17,73.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>"Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Sporthesia leverages audio commentaries to augment historical sports videos with embedded visualizations.</figDesc><graphic url="image-134.png" coords="7,66.12,605.94,115.81,72.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>"Fig. 9 :</head><label>9</label><figDesc>Fig. 9: Two examples, a-b and c-d, created by the experts in the study.</figDesc><graphic url="image-166.png" coords="8,324.89,161.78,106.72,90.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Precision, recall, and F1-score of the entity recognition.</figDesc><table><row><cell>Entity</cell><cell cols="4">Object Action Data Emotion Cue</cell></row><row><cell>Precision</cell><cell>0.92</cell><cell>0.90</cell><cell>0.86</cell><cell>0.84</cell></row><row><cell>Recall</cell><cell>0.95</cell><cell>0.95</cell><cell>0.90</cell><cell>0.91</cell></row><row><cell>F1-Score</cell><cell>0.93</cell><cell>0.92</cell><cell>0.88</cell><cell>0.88</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors wish to thank the sports experts from Zhejiang University for their time and expertise. A special thanks to Salma Abdel Magid for her beautiful voice and help on the video narration. This research is supported in part by the NSF award III-2107328, NSF award IIS-1901030, NIH award R01HD104969, and the Harvard Physical Sciences and Engineering Accelerator Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Understanding Data Videos: Looking at Narrative Visualization through the Cinematography Lens</title>
		<author>
			<persName><forename type="first">F</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Riche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hurter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CHI</title>
				<meeting>of CHI</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1459" to="1468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Authoring Data-Driven Videos with DataClips</title>
		<author>
			<persName><forename type="first">F</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Riche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Monroy-Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="501" to="510" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Constructing Spaces and Times for Tactical Analysis in Football</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Anzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Budziak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wrobel</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2019.2952129</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2280" to="2297" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploring pressure in football</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Budziak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Landesberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Weber</surname></persName>
		</author>
		<idno type="DOI">10.1145/3206505.3206558</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of AVI</title>
				<meeting>of AVI</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="1" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><surname>Bbc</surname></persName>
		</author>
		<author>
			<persName><surname>Piero</surname></persName>
		</author>
		<ptr target="https://www.bbc.co.uk/rd/projects/piero" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Is Space-Time Attention All You Need for Video Understanding?</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
				<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2021-07">July 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">What&apos;s the Situation with Situated Visualization? A Survey and Perspectives on Situatedness</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bressa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Korsgaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tabard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Houben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vermeulen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="117" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning Spatial Knowledge for Text to 3D Scene Generation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/d14-1217</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2028" to="2038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Text2Shape: Generating Shapes from Natural Language by Learning Joint Embeddings</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-20893-67</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACCV</title>
				<meeting>of ACCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11363</biblScope>
			<biblScope unit="page" from="100" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards Automated Infographic Design: Deep Learning-based Auto-Extraction of Extensible Timeline</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="917" to="926" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">CrossData: Leveraging Text-Data Connections for Authoring Data Documents</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<idno type="DOI">10.1145/3491102.3517485</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of CHI</title>
				<meeting>of CHI</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Augmenting Sports Videos with Viscommentator</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="824" to="834" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">TIVEE: Visual Exploration and Explanation of Badminton Tactics in Immersive Visualizations</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2021.3114861</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="118" to="128" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
		<ptr target="https://www.clipperscourtvision.com/" />
	</analytic>
	<monogr>
		<title level="j">Clippers. Court vision</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">WordsEye: An Automatic Text-to-Scene Conversion System</title>
		<author>
			<persName><forename type="first">B</forename><surname>Coyne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sproat</surname></persName>
		</author>
		<idno type="DOI">10.1145/383259.383316</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGGRAPH</title>
				<meeting>of SIGGRAPH</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="487" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Text-to-Viz: Automatic Generation of Infographics from Proportion-Related Natural Language Statements</title>
		<author>
			<persName><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2019.2934785</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="906" to="916" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ImageNet: A Largescale Hierarchical Image Database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
				<meeting>of ACL</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><surname>Espn</surname></persName>
		</author>
		<author>
			<persName><surname>Detail</surname></persName>
		</author>
		<ptr target="https://www.espn.com/watch/catalog/f48c68af-f980-4fcb-8b59-2a0db01f50cf/_/country/us" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName><surname>Explosion</surname></persName>
		</author>
		<author>
			<persName><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><surname>Spacy</surname></persName>
		</author>
		<ptr target="https://spacy.io/" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><surname>Explosion</surname></persName>
		</author>
		<author>
			<persName><surname>Ai</surname></persName>
		</author>
		<ptr target="https://spacy.io/usage/rule-based-matching" />
		<title level="m">SpaCy -Pattern Matching</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">H</forename><surname>Face</surname></persName>
		</author>
		<ptr target="https://github.com/huggingface/neuralcoref" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Robust Camera Calibration for Sport Videos Using Court Models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Farin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krabbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H N</forename><surname>De With</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Effelsberg</surname></persName>
		</author>
		<idno type="DOI">10.1117/12.526813</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of Storage and Retrieval Methods and Applications for Multimedia</title>
				<meeting>of Storage and Retrieval Methods and Applications for Multimedia</meeting>
		<imprint>
			<publisher>SPIE</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">5307</biblScope>
			<biblScope unit="page" from="80" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Video-based Analysis of Soccer Matches</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Workshop on Multimedia Content Analysis in Sports</title>
				<meeting>of International Workshop on Multimedia Content Analysis in Sports</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Supporting data-driven basketball journalism through interactive visualization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Stasko</surname></persName>
		</author>
		<idno type="DOI">10.1145/3491102.3502078</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of CHI</title>
				<meeting>of CHI</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="volume">598</biblScope>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">DataTone: Managing Ambiguity in Natural Language Interfaces for Data Visualization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dontcheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Karahalios</surname></persName>
		</author>
		<idno type="DOI">10.1145/2807442.2807478</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of UIST</title>
				<meeting>of UIST</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="489" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICCV</title>
				<meeting>of ICCV</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
				<meeting>of CVPR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">VizNet: Towards A Large-Scale Visualization Learning and Benchmarking Repository</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N S</forename><surname>Gaikwad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hulsebos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zgraggen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kraska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Satyanarayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">¸</forename><surname>Demiralp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CHI</title>
				<meeting>of CHI</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">662</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">DataToon: Drawing Dynamic Network Comics With Pen + Touch Interaction</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Riche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brehmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hinckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pahud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Mcguffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CHI</title>
				<meeting>of CHI</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">105</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Kineticharts: Augmenting Affective Expressiveness of Charts in Data Stories with Animation Design</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="933" to="943" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Video Generation From Text</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
				<meeting>of AAAI</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7065" to="7072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Towards an Understanding of Situated AR Visualization for Basketball Free-Throw Training</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nobre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<idno type="DOI">10.1145/3411764.3445649</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of CHI</title>
				<meeting>of CHI</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="volume">461</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">DeepEye: Towards Automatic Data Visualization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICDE</title>
				<meeting>of ICDE</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="101" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attentive Semantic Video Generation Using Captions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Marwah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Balasubramanian</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.159</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICCV</title>
				<meeting>of ICCV</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1435" to="1443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Formalizing Visualization Design Knowledge as Constraints: Actionable and Extensible Models in Draco</title>
		<author>
			<persName><forename type="first">D</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Howe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="438" to="448" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A Survey of Named Entity Recognition and Classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sekine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Lingvisticae Investigationes</publisher>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">NL4DV: A Toolkit for Generating Analytic Specifications for Data Visualization from Natural Language Queries</title>
		<author>
			<persName><forename type="first">A</forename><surname>Narechania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2020.3030378</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="369" to="379" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semantic Role Labeling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="103" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">SoccerStories: A Kick-off for Visual Soccer Analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Perin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vuillemot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fekete</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2013.192</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2506" to="2515" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">State of the Art of Sports Data Visualization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Perin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vuillemot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Stolper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Stasko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carpendale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CGF</title>
				<meeting>of CGF</meeting>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="663" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dynamic Visual Abstraction of Soccer Movement</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sacha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Al-Amoody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schreck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Janetzko</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.13189</idno>
	</analytic>
	<monogr>
		<title level="j">CGF</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="305" to="315" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Eviza: A Natural Language Interface for Visual Analysis</title>
		<author>
			<persName><forename type="first">V</forename><surname>Setlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Battersby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gossweiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1145/2984511.2984588</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of UIST</title>
				<meeting>of UIST</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="365" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Natural TTS Synthesis by Conditioning Wavenet on Mel Spectrogram Predictions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Skerrv-Ryan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
				<meeting>of ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4779" to="4783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Simple BERT Models for Relation Extraction and Semantic Role Labeling</title>
		<author>
			<persName><forename type="first">P</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno>ArXiv, abs/1904.05255</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Sports Training System for Visualizing Bird&apos;s-Eye View from First-Person View</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Surni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of VR</title>
				<meeting>of VR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1156" to="1158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<title level="m">What Makes a Data-GIF Understandable? IEEE TVCG</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1492" to="1502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName><surname>Spacy</surname></persName>
		</author>
		<ptr target="https://spacy.io/models/en_core_web_trf" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>English transformer pipeline</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Second spectrum</title>
		<author>
			<persName><forename type="first">S</forename><surname>Spectrum</surname></persName>
		</author>
		<ptr target="http://secondspectrum.com/" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Discovering Natural Language Commands in Multimodal Interfaces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dontcheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<idno type="DOI">10.1145/3301275.3302292</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of IUI. ACM</title>
				<meeting>of IUI. ACM</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">InChorus: Designing Consistent Multimodal Interactions for Data Visualization on Tablet Devices</title>
		<author>
			<persName><forename type="first">A</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Riche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hinckley</surname></persName>
		</author>
		<idno type="DOI">10.1145/3313831.3376782</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of CHI</title>
				<meeting>of CHI</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Collecting and Characterizing Natural Language Uterances for Specifying Data Visualizations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nyapathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1145/3411764.3445400</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of CHI. ACM</title>
				<meeting>of CHI. ACM</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Revealing the Invisible: Visual Analytics and Explanatory Storytelling for Advanced Team Sport Analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Breitkreutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Häussler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Seebacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Niederberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schreck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grossniklaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Janetzko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of BDVA</title>
				<meeting>of BDVA</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Bring It to the Pitch: Combining Video and Movement Data to Enhance Team Sport Analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Janetzko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lamprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Breitkreutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Goldlücke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schreck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grossniklaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="22" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Scope of Manipulability Sharing: A Case Study for Sports Training</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shiokawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shiokawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of VR</title>
				<meeting>of VR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="701" to="702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Team</surname></persName>
		</author>
		<ptr target="https://github.com/snakers4/silero-models" />
		<title level="m">Silero Models: Pre-trained Enterprise-grade STT / TTS Models</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Understanding the Design Space and Authoring Paradigms for Animated Data Graphics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Stasko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CGF</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="207" to="218" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName><surname>Tiktok</surname></persName>
		</author>
		<author>
			<persName><surname>Tiktok</surname></persName>
		</author>
		<ptr target="https://www.tiktok.com" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Feasibility Study on Virtual reality Based Basketball Tactic Training</title>
		<author>
			<persName><forename type="first">W.-L</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-C</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Tv</surname></persName>
		</author>
		<ptr target="https://www.facebook.com/watch/TennisTV" />
		<title level="m">Tennis tv</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">TTNet: Real-Time Temporal and Spatial video Analysis of Table Tennis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Voeikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Falaleev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Baikulov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
				<meeting>of CVPR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3866" to="3874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Tac-Miner: Visual Tactic Mining for Multiple Table Tennis Matches</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2021.3074576</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2770" to="2782" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">DataShot: Automatic Generation of Fact Sheets from Tabular Data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="895" to="905" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<author>
			<persName><surname>Wikipedia</surname></persName>
		</author>
		<ptr target="https://en.wikipedia.org/wiki/Glossary_of_table_tennis" />
		<title level="m">Glossary of table tennis</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><surname>Wikipedia</surname></persName>
		</author>
		<ptr target="https://en.wikipedia.org/wiki/Glossary_of_tennis_terms" />
		<title level="m">Glossary of tennis terms</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Glossary of Tennis Terms</title>
		<author>
			<persName><surname>Wikipedia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Voyager: Exploratory Analysis via Faceted Browsing of Visualization Recommendations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wongsuphasawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mackinlay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Howe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="649" to="658" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Multimodal analysis of video collections: Visual exploration of presentation techniques in ted talks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2429" to="2442" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Ai4vis: Survey on artificial intelligence approaches for data visualization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><surname>Detectron2</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Crosspower: Bridging Graphics and Linguistics</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<idno type="DOI">10.1145/3379337.3415845</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of UIST</title>
				<meeting>of UIST</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="722" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Crosscast: Adding Visuals to Audio Travel Podcasts</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
		<idno type="DOI">10.1145/3379337.3415882</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of UIST</title>
				<meeting>of UIST</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="735" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">AttnGAN: Fine-Grained Text to Image Generation With Attentional Generative Adversarial Networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00143</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
				<meeting>of CVPR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1316" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Shut-tleSpace: Exploring and Analyzing Movement Trajectory in Immersive Visualization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="860" to="869" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">StackGAN: Text to Photo-Realistic Image Synthesis with Stacked Generative Adversarial Networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.629</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICCV</title>
				<meeting>of ICCV</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5908" to="5916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2018.2856256</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1947" to="1962" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">A Flexible New Technique for Camera Calibration</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/34.888718</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1330" to="1334" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
