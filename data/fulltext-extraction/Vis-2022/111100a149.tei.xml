<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Animated Vega-Lite: Unifying Animation with a Grammar of Interactive Graphics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jonathan</forename><surname>Zong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Josh</forename><surname>Pollock</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dylan</forename><surname>Wootton</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Arvind</forename><surname>Satyanarayan</surname></persName>
						</author>
						<title level="a" type="main">Animated Vega-Lite: Unifying Animation with a Grammar of Interactive Graphics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-03-27T09:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Information visualization</term>
					<term>Animation</term>
					<term>Interaction</term>
					<term>Toolkits</term>
					<term>Systems</term>
					<term>Declarative Specification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present Animated Vega-Lite, a set of extensions to Vega-Lite that model animated visualizations as time-varying data queries. In contrast to alternate approaches for specifying animated visualizations, which prize a highly expressive design space, Animated Vega-Lite prioritizes unifying animation with the language's existing abstractions for static and interactive visualizations to enable authors to smoothly move between or combine these modalities. Thus, to compose animation with static visualizations, we represent time as an encoding channel. Time encodings map a data field to animation keyframes, providing a lightweight specification for animations without interaction. To compose animation and interaction, we also represent time as an event stream; Vega-Lite selections, which provide dynamic data queries, are now driven not only by input events but by timer ticks as well. We evaluate the expressiveness of our approach through a gallery of diverse examples that demonstrate coverage over taxonomies of both interaction and animation. We also critically reflect on the conceptual affordances and limitations of our contribution by interviewing five expert developers of existing animation grammars. These reflections highlight the key motivating role of in-the-wild examples, and identify three central tradeoffs: the language design process, the types of animated transitions supported, and how the systems model keyframes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Rapid prototyping is critical to the visualization authoring process. When making an explanatory graphic, rapid prototyping allows a visualization author to evaluate candidate designs before committing to refining one in detail. For exploratory data analysis, rapid prototyping is equally key as visualization is just one part of a broader workflow, with analysts focused on producing and analyzing a chart to yield insight or seed further analysis. However, consider the friction of visualizing faceted data: an author might choose between depicting facets as a small multiples display, on-demand via interaction (e.g., dynamic query widgets), or played sequentially via animation. These designs make different trade-offs between time and space and, as a result, research results suggest they afford readers different levels of clarity, time commitment, and visual interest <ref type="bibr" target="#b32">[33]</ref>. Despite these differences, the designs express a shared goal -to visualize different groupings of the dataand a visualization author might reasonably expect to be able to easily move between the three to make the most appropriate choice.</p><p>Unfortunately, existing visualization toolkits can present a highly viscous <ref type="bibr" target="#b43">[44]</ref> specification process when navigating this time-space trade-off. One class of toolkits supports either interaction or animation, but not both. Such systems include Vega <ref type="bibr" target="#b37">[38]</ref> and Vega-Lite <ref type="bibr" target="#b35">[36]</ref> -which offer interaction primitives in the form of signals and selections but do not provide abstractions for animation -as well as gganimate <ref type="bibr" target="#b42">[43]</ref>, Data Animator <ref type="bibr" target="#b45">[46]</ref>, Canis / CAST <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, and Gemini/Gemini 2 <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> -which express animation in terms of transitions between discrete visualization states known as keyframes but do not provide treatment for interaction. As a result, these systems force visualization authors to prematurely commit <ref type="bibr" target="#b43">[44]</ref> to either an interactionor animation-friendly abstraction when choosing their prototyping tool, and thus limit authors' ability to explore alternative designs. A second class of toolkits (including D3 <ref type="bibr" target="#b2">[3]</ref> and Plotly <ref type="bibr" target="#b0">[1]</ref>) support both modalities but do so via largely distinct abstractions (namely, transitions or frames for animation, and event handlers or a typology of techniques for interaction). Thus, an author must often either restructure or rewrite their specifications to consider interaction and animation in parallel.</p><p>In this paper, we present Animated Vega-Lite: extensions to Vega-Lite to support data-driven animation. Its design is motivated by the key insight that interaction and animation are parallel concepts (Sect. 3). Whereas interactions transform data (e.g. filtering) and update visual properties (e.g. re-coloring marks) in response to user input, animations do the same in response to a timer. From this perspective, interactive and animated visualization techniques occupy a spectrum of dynamic, event-driven behaviors. Thus, with Animated Vega-Lite, animated visualizations (like their interactive counterparts) are modeled as timevarying data queries -an approach that allows us to provide a unified set of abstractions for static, interactive, and animated visualizations. Animated Vega-Lite offers two abstractions of time that allow animations to compose with Vega-Lite's existing grammars of static and interactive visualizations (Sect. 4). From the perspective of interaction, time is an event stream: a source of events analogous to clicks and keypresses produced by a user. These events drive Vega-Lite selections, which apply dynamic data queries to visual encodings. Thus, by modeling time as an event stream, users can seamlessly specify and move between interactive and animated behavior in the same specification. From the perspective of Vega-Lite's grammar of graphics, time is an encoding channel. Just as x and y encodings map data values to spatial positions measured in pixels, a time encoding maps data values to temporal positions measured in elapsed milliseconds. Compared to the event stream abstraction, the encoding channel abstraction is lighterweight, but less expressive. This allows a visualization author to get started quickly with an animated chart and to move easily between an animated and a faceted visualization by switching a time channel for a row or column one. And, for added customizability, users can always turn a time-as-encoding specification into a time-as-event-stream one.</p><p>We implement a prototype compiler that synthesizes a low-level Vega specification with shared reactive logic for interaction and animation (Sect. 5). Following best practices <ref type="bibr" target="#b31">[32]</ref>, we assess our contribution with multiple evaluation methods. Through a diverse example gallery (Sect. 6), we demonstrate that Animated Vega-Lite covers much of Yi et al.'s interaction taxonomy <ref type="bibr" target="#b50">[51]</ref> and Heer &amp; Robertson's animation taxonomy <ref type="bibr" target="#b11">[12]</ref> while preserving Vega-Lite's low viscosity and systematic generativity. We also interview five expert developers of four existing animated visualization grammars <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b45">46]</ref> to critically reflect <ref type="bibr" target="#b34">[35]</ref> on the tradeoffs, conceptual affordances, and limitations of our system (Sect. 7). We discuss the important role example visualizations play in grammar design and analyze three areas of tradeoffs: the language design process, support for animations within vs. between encodings, and models of animation keyframes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Our contribution is motivated by perceptual work on the value of combining interaction and animation, and is informed by the design of existing toolkits for authoring animated data visualizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Animation in Information Visualization</head><p>In a classic 2002 paper, Tversky et al. <ref type="bibr" target="#b46">[47]</ref> question the efficacy of animated graphics. In reviewing nearly 100 studies comparing static and animated graphics, the authors were unable to find convincing cases where animated charts were strictly superior to static ones. Visualization researchers have since contributed a body of studies that have identified reasons to be both optimistic and cautious about the value of animation in visualization. For instance, several studies have demonstrated advantages when animating chart transitions <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18]</ref> or directly animating data values to convey uncertainty <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17]</ref>. However, these studies have also echoed concerns from Tversky et al. that animations are often too complex or fast to be perceived accuratelyfor instance, Robertson et al. found that animated trend visualizations are outperformed by static small multiples displays <ref type="bibr" target="#b32">[33]</ref>.</p><p>To ameliorate these limitations of animation, Tversky et al. suggest composing animation with interactivity, particularly through techniques that allow reinspection or focusing on subsets of depicted data. Robertson et al. began to probe this question by testing an interactive alternative alongside the static and animated stimuli -here, clicking an individual mark adds an overlaid line that depicts its trajectory over time. They find that although participants are no more accurate under this interactive condition, they perform faster when using this visualization for data analysis <ref type="bibr" target="#b32">[33]</ref>. In follow-up work, Abukhodair et al. <ref type="bibr" target="#b1">[2]</ref> further contextualize Robertson's results, finding that interactive animation can be effective and significantly more accurate than animation alone when users want to drill down into the data or have specific questions about points of interest. More recent results are similarly promising: in eye-tracking studies, Greussing et al. <ref type="bibr" target="#b10">[11]</ref> find that interactive animated graphics not only received more attention than static or interactive-only equivalents, but these charts also produced higher knowledge acquisition in participants. The authors believed that the enhanced affects on memory and performance resulted from an increase in engagement and attention on the visualization, which is in line with additional research on attention <ref type="bibr" target="#b3">[4]</ref>. Our work is motivated by these results. By providing a unified abstraction of interaction and animation, Animated Vega-Lite allows analysts to rapidly switch between the two modalities, or compose them together to best suit their needs. Moreover, as our abstractions preserve Vega-Lite's generative properties, we believe our contribution lowers the threshold for conducting future such studies by allowing researchers to more systematically isolate, vary, and compare individual interaction and animation techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Authoring Interaction and Animation</head><p>In Sect. 3.1, we describe the conceptual similarities between Animated Vega-Lite and Functional Reactive Programming (FRP). Moreover, in Sect. 7 we conduct a detailed comparison between Animated Vega-Lite and gganimate <ref type="bibr" target="#b41">[42]</ref>, Data Animator <ref type="bibr" target="#b45">[46]</ref>, Gemini/Gemini 2 <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, and Canis/CAST <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. Here, we instead survey other systems for authoring interaction and animation that have informed our approach.</p><p>Visualization toolkits such as D3 <ref type="bibr" target="#b2">[3]</ref>, Plotly <ref type="bibr" target="#b0">[1]</ref>, and Matplotlib [14] offer a number of facilities for authoring and composing interaction and animation including typologies of techniques (e.g., brushing, hovering, and animation frames) through to event callbacks and transition functions. Technique typologies can help foster a rapid authoring process, allowing designers to easily instantiate common techniques, but also present a sharp abstraction cliff <ref type="bibr" target="#b43">[44]</ref>. If designers wish to produce more custom interaction or animation techniques, they must turn to an entirely different notation: authoring low-level, imperative event callbacks or transition functions. This abstraction cliff also increases the viscosity of the authoring process <ref type="bibr" target="#b43">[44]</ref>. For instance, to switch between the static, interactive, and animated displays of faceted data described in the introduction using D3 would involve restructuring the specification code in non-trivial ways -a problem that is exacerbated</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example</head><p>Interaction intent <ref type="bibr" target="#b50">[51]</ref> Animation type <ref type="bibr" target="#b11">[12]</ref>  if HTML templates are used to generate the SVG rather than the d3selection, as is increasingly the case when working with modern frontend frameworks such as Svelte, Vue, or React.</p><p>In contrast, Animated Vega-Lite, like its predecessor, prioritizes concise high-level declarative specification. As Sect. 3 describes, users can make atomic edits (i.e., changing individual keywords, or adding a localized handful of lines of specification code) to rapidly explore designs across the three modalities. The tradeoff, however, is one of expressiveness. Animated Vega-Lite users are limited to composing language primitives; while these primitives are sufficient to broadly cover interaction and animation taxonomies (Sect. 6), their expressive range will necessarily be smaller than their lower-level counterparts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MOTIVATION: UNIFYING INTERACTION AND ANIMATION</head><p>In this section, we discuss similarities between interaction and animation that we observe. These similarities drive our design decisions, allowing us to extend Vega-Lite with only minimal additional language primitives, and yielding a low-viscosity grammar that makes it easy to switch between static, interactive and animated modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Conceptually Bridging Interaction and Animation</head><p>We observe that interaction and animation share conceptual similarities at both low and high levels of abstraction. At a low level of abstraction, Functional Reactive Programming (FRP) languages like Flapjax <ref type="bibr" target="#b25">[26]</ref> and Fran <ref type="bibr" target="#b7">[8]</ref>, as well as FRP-based visualization toolkits like Vega <ref type="bibr" target="#b36">[37]</ref>, have shown that interaction and animation can both be modeled as event streams. The Vega example gallery demonstrates how this unified abstraction offers consistency, with similar semantics expressed through similar syntactic forms <ref type="bibr" target="#b43">[44]</ref>. Namely, the gallery recreates the Gapminder global health scatter plot, originally an animated visualization produced by Hans Rosling <ref type="bibr" target="#b33">[34]</ref>, but as an interactive visualization driven by the DimpVis direct manipulation technique <ref type="bibr" target="#b20">[21]</ref>. We observe that, although it would be tedious to do manually, a user could convert this interactive visualization back to the original animated one by replacing signals near the top of the dataflow, which react to incoming drag events, with signals that respond to timer events instead: where these signals map the drag event's position to a year value, the timer signals would simply emit the next year value on each event. The rest of the downstream reactive logic would remain unchanged. However, as the Vega authors found <ref type="bibr" target="#b37">[38]</ref>, additional language design is necessary to ensure FRP primitives compose together with grammar of graphics constructs and to facilitate higher-level authoring of dynamic visualizations.</p><p>To analyze conceptual similarities between interaction and animation at a higher-level of abstraction, we look to Yi et al. <ref type="bibr" target="#b50">[51]</ref> and Heer and Robertson <ref type="bibr" target="#b11">[12]</ref> that taxonomize techniques for each modality respectively. These taxonomies are defined by drawing on example visualizations, and although they have been defined separately, share many motivating techniques (Table <ref type="table" target="#tab_1">1</ref>  panning as an example of view transformation because it changes the reader's viewpoint while leaving data schemas and encodings intact. Yi et al. also consider panning, categorizing it as an example of an explore interaction, because it involves showing a new subset of data as points shift in and out of the viewport. Zooming, another example of view transformation, is also described as an abstract/elaborate interaction because it can be used to show data at different levels of detail.</p><p>As we show in Table <ref type="table" target="#tab_1">1</ref>, we observe substantial overlap in techniques referenced by both taxonomies. Though select interactions lack an explicitly defined corresponding animation type, conditional encoding is a commonly used technique in animated visualizations. Similarly, though there is no corresponding category in Heer and Robertson's taxonomy for connect interactions, animations applied to shared backing data across multiple views can fulfill the same purpose of highlighting relationships between related points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Low-Viscous Authoring: An Example Usage Scenario</head><p>A unified abstraction for static, interaction and animation also promotes a low-viscous authoring process (i.e., being able to easily switch between modalities, or compose them together). To illustrate the affordances of this approach, we present an example walkthrough following Imani, an orthonologist, as she plans a new birdwatching expedition. Imani has a bird migration dataset comprising the average latitudes and longitudes for a variety of bird species, for every day of the year <ref type="bibr" target="#b21">[22]</ref>.</p><p>To ensure a productive trip, Imani wants uncover how migration patterns correspond to different times of the year and geographic regions. Static (Fig. <ref type="figure" target="#fig_0">1A</ref>). Imani begins her analysis with a static visualization to get an overview of the dataset. She plots a map, and visualizes migration paths using line marks: each bird species is depicted as a single, uniquely-colored line, connecting the individual daily points along their given latitudes and longitudes. However, Imani is quickly overwhelmed as the size of the dataset produces too many overlapping lines for this static view to be useful, even after adjusting mark opacity.</p><p>Interactive (Fig. <ref type="figure" target="#fig_0">1B</ref>). To pick out individual bird species, and begin a cycle of generating and answering hypotheses, Imani thinks to layer some interactivity on the static display. She adds a point selection named highlight and driven by mouseover events. By default this selection is populated with the data tuple underneath the mouse cursor, and additional tuples are added or toggled when the shift modifier key is pressed. Imani writes a conditional encoding to interactively adjust mark appearance: selected paths are drawn at full opacity and in a larger size, while unselected paths are drawn with lower opacity and at a smaller size. Thus, as Imani moves her mouse across the visualization, she is able to better trace individual paths, and she adds a tooltip encoding channel to surface and note species' names.</p><p>This interactive view gives Imani a better sense of migration paths.</p><p>But, to be able to plan her expedition, she needs to understand where different bird species may be on any given day. Until this point, Imani has used vanilla Vega-Lite abstractions. In the subsequent steps, we show how features of Animated Vega-Lite help Imani deepen her analysis. Time Encoding Channel (Fig. <ref type="figure" target="#fig_0">1C</ref>). Imani swaps to a circle mark and maps day (a field that encodes the day of the year from 0 to 365) to the new time encoding channel. With these two edits, each bird species is drawn as a circle indicating its location on a particular day, and the visualization animates through day values. Imani can now follow the path bird species travel over the course of a year.</p><p>Time Event Stream (Fig. <ref type="figure" target="#fig_0">1D</ref>). Imani, however, is keenly aware that her dataset only contains average values for each species. Birds tend to appear at a given location within a small window of time around the average day in the dataset. Thus, to ensure she does not make an erroneous conclusion, Imani wants to visualize this variability as a path trail. To do so, she adds a new point selection named spread window, which contains a custom predicate -a function that identifies which data tuples should be considered as falling within the selection. In this case, Imani writes a predicate to select data from the five days previous to the current day. She does this by writing inequality expressions referencing the reserved name anim value, which stores the current data value of the animation. In contrast to the existing highlight point selection, which is updated on user input events, spread window is instead populated and re-populated on every timer tick. She uses spread window to dynamically filter the circle marks, ensuring only data values that lie within the selection are displayed and animated. To visually distinguish the current day's points, she also elaborates the time encoding into an explicit selection called current frame and uses it to drive a conditional opacity encoding. She renders current points at full opacity while rendering the trailing points at less opacity.</p><p>Composing Interaction + Animation (Fig. <ref type="figure" target="#fig_0">1E</ref>). While watching this path-trail animation, Imani notices that a cluster of birds appear to visit Pensacola, Florida during late March and notes this region as a potential location for her expedition. However, before she lets her colleagues know, she wants to investigate the migration patterns of the birds that come through the area -if these species tend to co-locate in other parts of the world, there is less of a reason for birders to travel to Pensacola specifically. To answer this question, Imani needs finer control over the animation state. She binds the current frame selection to an interactive range slider, and can now toggle between animating and interactively sliding the day field. She scrubs the slider to the day when the birds pass through Pensacola, and to track these species in the visualization, she modifies the interactive highlight selection to fire on click instead of hover. Imani multi-selects (i.e., clicking with the shift key pressed) the birds that pass through the area, and then scrubs to a different day. Here Imani can see that these birds come from 5 unique nesting sites across the mid-west US to eastern Canada. This is promising as it indicates that these species uniquely overlap in Pensacola, making it a prime viewing destination.</p><p>Summary. With Animated Vega-Lite, Imani was able to move between static, interactive, and animated visualizations through a series of atomic edits or otherwise localized changes rather than larger-scale refactoring or restructuring of code. Moreover, we have extended Vega-Lite's high-level affordances to animation: Imani was able to express animation as data selections and transformations, rather than manipulating keyframes or specifying transition states; and, the Animated Vega-Lite compiler synthesized appropriate defaults and underlying machinery for the animation to unfold correctly. Finally, as Animated Vega-Lite offers a unified abstraction, Imani was able to reuse Vega-Lite's existing primitives to author mixed interactive-animated visualizations as well as custom techniques without the need for special-purpose functionse.g., combining animations with on-click highlighting and composing selections with a window data transform to draw trailing marks, rather than using a shadow function as with gganimate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">A GRAMMAR OF ANIMATION IN VEGA-LITE</head><p>In Animated Vega-Lite, users specify animation using a time encoding channel and timer-driven selections. Time encodings provide a light-weight way to convert faceted static visualizations into animations. To further customize the animation design or easily add interaction, users can specify animations as selections instead. Selections express dynamic data queries, and are now populated either by input events (as with vanilla Vega-Lite) or, now, via timer ticks. Defined selections can then be used to drive data transformations, scale functions, or conditionally encode visual properties. Our animation model expressively extends existing abstractions for static and interactive visualizations while minimally increasing language surface area and complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Time Encoding Channel</head><p>In Vega-Lite, encodings determine how data values map to the visual properties of a mark (also known as channels). Vega-Lite includes two channels for spatial position, x and y. Animated Vega-Lite adds a new channel for temporal position, called time. A user specifies a time encoding by providing a field property, which is a string of the name of a data column. The field can be any measure type with a sort order (quantitative, temporal, ordinal), and does not necessarily need to represent a timestamp. The system uses distinct values from this column to group data rows into temporal facets called keyframes. Over the duration of the animation, each keyframe is shown sequentially.</p><p>Fig. <ref type="figure" target="#fig_1">2A</ref> shows the Animated Vega-Lite specification for Rosling's Gapminder animation <ref type="bibr" target="#b33">[34]</ref>. The time encoding, highlighted in yellow, maps the dataset's year field to the time encoding channel. The system uses the distinct values of year to group rows into keyframes. In other words, there is one keyframe per possible value of year in the dataset (i.e. 1955, 1960, 1965, ..., 2005) (Fig. <ref type="figure" target="#fig_1">2C</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Key Field</head><p>In-betweening, more commonly called tweening, is a standard animation technique that involves generating additional frames to smoothly transition between two keyframes. By adding tweening, the animation will give the visual impression of continuous change over time even when data represents discrete measurements. In data visualization, tweening takes on additional meaning as it requires generating and interpolating between values that are not present in the dataset. In Animated Vega-Lite, to specify tweening between keyframes, the user specifies a key property in the time encoding channel, which references a field name. This key field is used to group rows together across keyframes. For two given successive keyframes, rows that share the same value for the key field are treated as the start and end states for a single mark instance. Key values should be unique within a keyframe to prevent ambiguity; otherwise, a single mark instance might have multiple start or end states, resulting in undefined behavior. If the user does not specify a key field, the Animated Vega-Lite compiler attempts to infer a sensible default based on the mark type and other specified categorical channels such as color or detail -an approach that follows Vega-Lite's existing inferences.</p><p>In the Gapminder example, Fig. <ref type="figure" target="#fig_1">2B</ref> shows the Gapminder spec from Fig. <ref type="figure" target="#fig_1">2A</ref> with default values specified explicitly. Here, country is used as the default key field as it is also encoded on the color encoding channel. Consider the successive keyframes with year values 1955 and 1960. For each year, each scatterplot point is identified by a unique country value. Therefore, to tween from 1955 to 1960, the system interpolates the two rows for each country to produce the corresponding in-between point at each animation frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Time Scale</head><p>An encoding uses a scale function to map from the data domain to a visual range. For spatial encoding channels, this range is measured in pixels relative to the bounding box of the rendered visualization. For the time encoding channel, we measure the range in milliseconds elapsed from the start of the animation. Users specify the timing of the animation using a time scale (for example, by specifying either an overall animation duration or the amount of time between keyframes as a step). As with existing encoding channels, if a scale is not specified by the user, Vega-Lite infers default scale properties. By default, scales for the time encoding channel use the unique values of the backing field as the scale domain, and create a default step range with 500ms per domain value. For example, the Gapminder domain is a list of every fifth year between 1955 and 2005, inclusive. The default range maps 1955 to 0ms, 1960 to 500ms, 1965 to 1000ms, and so on. A user can override this default range to slow down or speed up the animation.</p><p>Though the default domain is sufficient to express most common animations, a user may want to override the domain. Supplying a custom domain is useful for specifying non-keyframe-based animations that require direct reference to in-between values, or require animating through values that are missing from the dataset. For example, Fig. <ref type="figure" target="#fig_2">3</ref> shows an example of such a use case. The animation should advance through 24-hour time span at a constant rate. However, the dataset does not contain a field that has values that are evenly spaced in the desired domain. So, with a default scale domain, the animation would appear to jump between time stamps rather than move through them smoothly.</p><p>To achieve the desired behavior, the user instead specifies a custom domain representing the continuous interval between 00:00 and 23:30.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Re-scale</head><p>By default, the visualization's data rectangle (or viewport) is fixed to the initial extents of the x-and y-scales calculated from the full dataset. However, for keyframe animations, only a subset of data is shown at any given time. If a user wants to re-calculate the viewport bounds based on only the data included in the current keyframe, rather than the original full dataset, they can set a flag in the time encoding called rescale. When rescale is true, the viewport's bounds are recomputed at each step of the animation. We refer to this concept as re-scaling because re-calcuating the viewport bounds involves updating the domains of the x and y scales at each keyframe.</p><p>Fig. <ref type="figure" target="#fig_3">4</ref> demonstrates the use of rescale. Rescale is enabled in Fig. <ref type="figure" target="#fig_3">4A</ref>, where the viewport updates according to the current selection. The visualization remains tightly zoomed on the currently displayed bars, with the longest bar always scaled to nearly the full width of the viewport. In contrast, Fig. <ref type="figure" target="#fig_3">4B</ref> has rescaling disabled. The viewport is initially calculated with the full dataset and remains fixed. This would be appropriate for Gapminder, because we want to show the countries moving along a fixed scale. However, it is less helpful for bar chart race. Instead of enabling positional comparisons to a fixed scale, the animation prioritizes making the ordering of the top-ranked bars salient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Selections with a Timer Event Stream</head><p>Selections are subsets of data points that are populated when updates occur in an event stream. In Vega-Lite's interactive grammar, selections are defined using streams of user input events (e.g., clicks, mouse movements, or keyboard presses). The system uses the event's properties to query a set of data points. The selected data can then be applied to update downstream primitives in the visualization specification including data transformations, scale functions, or conditional visual encodings. For example, a selection defined using the mouseover event may be used to highlight marks that a user hovers over with their cursor. Under the hood, the selection receives a stream of mouseover events with x and y coordinates in pixels. It uses the scales associated with the x and y encoding channels to invert these screen coordinates back to data coordinates (i.e. values in the domain of the corresponding scale). A default predicate function iterates over all rows in the dataset, and includes the rows matching those data values in the selection.</p><p>Animated selections are analogous to interactive selections. However, instead of reacting to input events, animated selections use a timer event stream to advance an internal clock representing the elapsed time of the animation in milliseconds (ms). This clock resets to 0ms when it reaches the end of the range defined by the time encoding's scale (i.e. the animation loops the duration of the time scale's range). As the clock updates, the elapsed time value is mapped to a value in the time domain (i.e. the time encoding's field values). The animation selection updates to include all data points matching that value.</p><p>As selections rely on scales to convert map time to data values, selection-based animations still require a time encoding channel to be defined. In fact, all animations that can be expressed with only a time encoding can be elaborated into selection-based animations. In other words, selection-based animations are strictly more expressive than animations using only time encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Applying Selections</head><p>In Vega-Lite, selections can be applied to other language constructs, including conditional mark encodings, scale domains, or data transformations <ref type="bibr" target="#b51">[52]</ref>. This property of composition continues to hold with Animated Vega-Lite: animated and interactive selections can be used interchangeably wherever selections are supported in the Vega-Lite language. Therefore, selections driven by timer events inherit the expressiveness of interactive selections in terms of Yi et al.'s taxonomy of interaction techniques <ref type="bibr" target="#b50">[51]</ref>. Animations can be used to: select marks of interest; explore subsets of data (panning and zooming); reconfigure data into different transformed states, connect related items; abstract/elaborate through overview and detail; and filter data dynamically. However, they cannot be used to change the properties of visual encodings on the fly, which is an interaction technique that falls outside of the selection-based model and is a limitation of base Vega-Lite.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Predicate</head><p>As the animation's elapsed time advances, the selection uses the scale defined in the time encoding to invert elapsed milliseconds (in the scale's range) to a data value (in the scale's domain). As a result, at any given time, there is an internal variable that has a data value corresponding to the animation's current time. When the Vega-Lite specification is compiled into Vega, this variable is represented as a Vega signal called anim value. In the Gapminder example, anim value starts at 1955 at 0ms, and advances to 1960, 1965, ..., 2005.</p><p>To construct keyframes, the selection queries a subset of data tuples to include in the keyframe based on the current value of anim value. By default, tuples are included in the keyframe if their value in the time encoding's field (e.g. year for Gapminder) is equal to anim value. However, to define alternate inclusion criteria for determining keyframes, users can specify custom predicate functions. For example, if at every step of the animation, a user wished to show all points with year less than or equal to anim value, they would use the following predicate:</p><p>{"field": "year", "lte": "anim value"} Previously, Vega-Lite did not allow users to customize the selection predicate because the majority of interactions could be expressed using a combination of default predicates and selection transformations. Nonetheless, enabling predicate customization in the selection specification also increases the expressiveness of the interactive grammar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Input Element Binding</head><p>Using the bind property, a user can populate a selection using a dynamic query widget (such as an HTML slider or checkbox). For animated selections, input element binding offers a convenient way to add interactive playback control to the animation. For instance, the user can bind an animated selection to a checkbox to toggle whether the animation is playing or paused. Similarly, they can bind a selection to a range slider and drag to scrub to a specific time in the animation.</p><p>Scrubbing the animation with the slider surfaces an interesting design challenge when combining animation and interaction: how should the system delegate control between the animation timer and user interaction? Initially, the animation is driven by the timer, with the slider visualizing timer updates. When the user starts dragging the slider, the system pauses the animation and delegates control to user interaction. Pausing is necessary so that the slider does not continue to advance forward while the user is currently scrubbing. When the user is done scrubbing, they may want to give control back to the animation. To facilitate this, Animated Vega-Lite automatically includes a play/pause checkbox alongside bound sliders. The user can simply re-check the box to give control over the animation back to the timer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Pausing</head><p>Animated Vega-Lite supports pausing in two ways: by interaction, and by data value. Interactive pauses are specified using the filter property of Vega-Lite event streams. Users can provide the name of a Vega-Lite parameter to the filter property of a timer event stream. Parameters can be either selections or variables. When the provided parameter evaluates to true (i.e. is a non-empty selection or a true boolean variable), the filter will capture incoming events, preventing the animation clock from advancing. When the paramater evaluates to false, the events will resume propagating and the animation will continue. For example, a user can bind a checkbox to a parameter named is playing, and use the following event stream definition to pause the visualization when the box is checked: "on":{"type": "timer", "filter": "is playing"} Pausing by data value is specified using the pause property of an animated selection definition. The user provides a list of data values to pause on, and the duration of each pause. For example, a user can specify that the Gapminder animation should pause on the year 1995 for 2 seconds, to draw attention to the data for that year:</p><p>"pause": [{"value": 1995, "duration": 2000}]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.5">Global Easing</head><p>Easing is a common animation technique that involves controlling the rate that the animation timer advances. Easing is typically implemented using a palette of pre-defined functions that map an animation time domain to a transformed time domain. For example, an exponential easing function might cause the animation clock to begin advancing slowly, and then exponentially accelerate as the animation progresses. In Animated Vega-Lite, the animation clock advances linearly by default. However, users can use the easing property of a selection to specify an easing function to apply to the whole duration of the animation. Animated Vega-Lite exposes D3's named easing functions <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">IMPLEMENTATION</head><p>We implement Animated Vega-Lite using a prototype compiler, wrapping the existing Vega-Lite compiler to ingest Animated Vega-Lite specifications and output a lower-level Vega specification. The Animated Vega-Lite prototype compiler begins by expanding a user-supplied specification into a "normalized" format with all implicit default values filled in explicitly. This step includes generating default selections and transforms for animations specified using only time encodings, and filling in default scale and key definitions. This normalized specification is passed to the next compiler step to simplify processing.</p><p>To convert Animated Vega-Lite into low-level Vega, we use the existing Vega-Lite compiler to make the initial conversion into Vega (using a copy of the specification with animation removed), and then call a series of functions to compile animation-specific parts of the spec and merge them with the output Vega. Because Vega-Lite's high-level abstractions do not have a one-to-one mapping to low-level Vega concepts, seemingly-isolated Vega-Lite fragments will typically make changes in many different parts of the Vega spec. Each of these functions takes in fragments of Animated Vega-Lite and standard Vega, and outputs a partial Vega specification that includes dataset, signal, scale, and mark definitions to merge into the output.</p><p>Compilation happens in six steps. First, compileAnimation-Clock uses definitions of animated selections and time encoding channels to create Vega signals and datasets for controlling the current state of the animation, handling pausing, and interfacing with interactive playback controls. Next, compileTimeScale takes in a definition of a time encoding alongside Vega marks and scales. It creates Vegalevel scales for the time encoding, and signals to handle inversions between the animation clock and the corresponding data value at that time. It also applies rescaling to mark encodings if applicable. com-pileAnimationSelections then ingests definitions of animated selections to produce Vega signals and datasets that implement custom predicates, pausing and easing, and input element binding. Fourth, compileFilterTransforms takes animation selections and any filter transforms that reference those selections, and materializes the selections as filtered datasets in Vega. These datasets provide the backing data for rendering marks at each keyframe. compileKey then uses the time encoding specification to generate datasets and signals that handle tweening between keyframes. Finally, compileEnterExit supports top-level enter and exit encoding definitions in Animated Vega-Lite, converting them into Vega-level enter and exit encodings. Because of existing limitations in Vega, enter and exit currently are not well-supported for animation. However, pending Vega support, designers should be able to control the behavior of visual encodings as marks enter and exit the current keyframe.</p><p>We chose to implement our compiler as a wrapper around the existing Vega-Lite compiler in order to facilitate rapid prototyping. However, our current approach faces performance challenges that could be improved with internal changes to Vega and Vega-Lite. For example, we currently support tweening by creating three separate datasets: the current keyframe, the next keyframe, and a joined dataset with tweens computed as a derived column. This expensive operation causes noticeable lag on large datasets. In future implementations, we can instead create a Vega dataflow operator that leverages the animation's semantics to compute tweens more efficiently. For example, instead of computing multiple datasets independently and performing a join, the operator can create a single dataset backed by a sliding window over the time facets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATION: EXAMPLE GALLERY</head><p>To evaluate Animated Vega-Lite's expressiveness, we created an example gallery to demonstrate coverage over both Yi et al.'s taxonomy of interaction intents <ref type="bibr" target="#b50">[51]</ref> and Heer &amp; Robertson's taxonomy of transition types in animated statistical graphics <ref type="bibr" target="#b11">[12]</ref>. As Fig. <ref type="figure" target="#fig_4">5</ref> shows, we support 6 / 7 interaction categories and 5 / 7 animation categories.</p><p>Fig. <ref type="figure" target="#fig_4">5a</ref> demonstrates an overview + detail visualization. A selection controls a brush over the bottom view, which sets the zoomed viewport of the top view. This selection is defined using a predicate that defines a sliding window over the x-axis field. When the brush is driven by animation, the selection is updated on each timer event. When the brush is driven by interaction, the selection is instead updated on drag events. Because the original Vega-Lite selection model unifies panning and zooming as selections applied to a scale domain, this approach can be adapted to animate arbitrary geometric panning and In Fig. <ref type="figure" target="#fig_2">3</ref> and Fig. <ref type="figure" target="#fig_4">5b</ref>, we apply a conditional filter over the whole dataset, with filter parameters changing over time. In contrast to faceting, filtering can leverage custom selection predicates to show and hide data -a single data point can appear in multiple groups. Both taxonomies contain a category for filtering, shown here by adding or removing elements from the display. Fig. <ref type="figure" target="#fig_2">3</ref> additionally demonstrates a select intent by using conditional encoding to highlight selected data.</p><p>Fig. <ref type="figure" target="#fig_3">4</ref> and Fig. <ref type="figure" target="#fig_4">5c</ref> show examples with a sorted axis. When a key is specified in a time encoding, the system automatically tweens an element's position even when its sort index has changed in the next keyframe. Continually sorting elements as the underlying data changes demonstrates an ordering transition, as well as a reconfigure intent. Time encodings transition between sequential time values by default in Animated Vega-Lite (e.g. Fig. <ref type="figure" target="#fig_1">2</ref>). Fig. <ref type="figure" target="#fig_4">5d</ref> demonstrates an additional example of this animation. A default animated point selection is applied to a data transform that re-normalizes a stock price time-series chart on each tick. The original Vega-Lite paper contains an interactive version of this example, which instead populates the point selection on mouse hover events <ref type="bibr" target="#b35">[36]</ref>. These examples demonstrate timestep transitions, which also fulfill the explore intent by showing new data points at each step. Axis re-normalization is also an example of a reconfigure intent.</p><p>In addition to achieving broad coverage over the two taxonomies, our system also supports simulation techniques including hypothetical outcome plots (Fig. <ref type="figure" target="#fig_4">5e</ref>) <ref type="bibr" target="#b12">[13]</ref>. And, as previously discussed in Sect. 4.2.1, animated selections can be applied to the same set of dynamic visual behaviors as interactive selections. Consequently, users can easily switch between timer and input event streams when prototyping existing interaction techniques in Vega-Lite. For example, Fig. <ref type="figure" target="#fig_4">5a</ref> and Fig. <ref type="figure" target="#fig_4">5d</ref> show animated selections driving common interaction techniques -panning and re-normalizing, respectively. Users can also easily compose interaction techniques with animated visualizations by defining additional selections. For example, Fig. <ref type="figure" target="#fig_4">5f</ref> demonstrates an interactive brush used to highlight a region of an animated Gapminder visualization. Points of interest are conditionally colored as they enter or exit the brush region.</p><p>Discussion and Limitations. Like the original Vega-Lite, Animated Vega-Lite intentionally trades some limits to expressivity for gains in concise, high-level, declarative specification. In Sects. 7.2.1 &amp; 7.2.2, we detail this expressiveness tradeoff in terms of the classes of animation techniques (Animated Vega-Lite primarily supports scene techniques instead of segue) as well as the implications on how keyframes are modeled and generated (Animated Vega-Lite supports non-parametric keyframe transitions, and offers some limited support for parametric keyframe transitions). Thus, lower-level and imperative languages will necessarily be more expressive: for instance, D3 can express both scene and segue animations, but using different language constructs (timer event loops and transition functions, respectively). As these sections describe, offering high-level declarative specification that unifies not only these distinct conceptual models of animation, but also interaction and static charts, remains a compelling direction for future work.</p><p>By extending Vega-Lite, Animated Vega-Lite also inherits its predecessor's limitations. For instance, Vega-Lite selections cannot alter visual encodings or data transformation pipelines at runtime (the encode interaction type in Yi et al.'s taxonomy <ref type="bibr" target="#b50">[51]</ref>); thus, Animated Vega-Lite cannot support the visualization change or data schema change transition types in the Heer &amp; Robertson taxonomy <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">EVALUATION: CRITICAL REFLECTION</head><p>To identify our grammar's design tradeoffs, we compared our approach to existing animated visualization grammars following the critical reflections evaluation method <ref type="bibr" target="#b34">[35]</ref>. We recruited five developers of existing grammars: John Thompson and Leo Zhicheng Liu<ref type="foot" target="#foot_0">1</ref> of Data Animator <ref type="bibr" target="#b45">[46]</ref>, Tong Ge of Canis <ref type="bibr" target="#b9">[10]</ref> and CAST <ref type="bibr" target="#b8">[9]</ref>, Thomas Lin Pedersen of gganimate <ref type="bibr" target="#b42">[43]</ref>, and Younghoon Kim of Gemini <ref type="bibr" target="#b18">[19]</ref> and Gemini 2 <ref type="bibr" target="#b19">[20]</ref>. We focused on animation grammar developers because the interactive grammar was evaluated in the original Vega-Lite paper. With each participant, we conducted a one-hour pre-interview. We then asked them to asynchronously engage with our grammar for an extended time by reading a system walkthrough and grammar documentation similar to Sect. 3 and Sect. 4, respectively, and run examples similar to those found in Sect. 6. We further suggested participants write new specifications and/or port other examples, including examples from their own tools. We encouraged participants to take notes and reflect on the design of Animated Vega-Lite during the process. Finally, we conducted post-interviews with each participant that lasted 30-60 minutes. Each participant was offered a $125 gift card as compensation.</p><p>Our goals were to (i) compare and contrast their design processes with ours, (ii) understand differences and design tradeoffs between their grammars and ours, and (iii) generate insights about the direction of future animation grammars. During the interviews, three of the authors of this paper began developing initial thematic hypotheses. After the interviews, we independently conducted a thematic analysis before finally coming together and synthesizing our insights, which we summarize below. These themes provide insight into the design of our grammar, and animated visualization grammars more generally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Grammar Design Process</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1">Specific Examples Motivate Grammar Design</head><p>When scoping their research projects, our interviewees prioritized motivating examples that they found personally compelling. For example, the authors of Data Animator and Gemini were both motivated in part by R2D3 <ref type="bibr" target="#b39">[40]</ref>. As we discuss in the following subsections, the choosing examples to support leads to design tradeoffs, e.g. between scene-and segue-dominant abstractions (Sect. 7.2.1). Thus, a handful of compelling in-the-wild examples can significantly influence the grammars developers build. Other examples that were cited across multiple interviews included Gapminder <ref type="bibr" target="#b33">[34]</ref>, Periscopic's Gun Deaths <ref type="bibr" target="#b30">[31]</ref>, and animations in the New York Times (NYT) and the Guardian.</p><p>On the other hand, a lack of existing examples may also motivate a grammar developer. For example, to gain more insight into the popularity of animated visualization techniques, Kim scraped NYT and Guardian articles from 2018 as well as YouTube videos from the same year. He noticed that about 90% of the animated visualizations he studied updated data, but kept the encoding fixed. R2D3 was a notable exception. A similar imbalance can be found in the Data-Gifs example gallery <ref type="bibr" target="#b38">[39]</ref>, where over half of the examples have fixed encodings. Kim hypothesized that the imbalance is influenced by the affordances of existing tools, and decided to optimize Gemini for transitions between changing encodings.</p><p>With Animated Vega-Lite, we were motivated by the large collection of existing examples with static encodings, such as those in the Data-Gifs example gallery. This category includes many prominent designs like Gapminder and bar chart races. Rather than focus on developing an expressive language of transitions between keyframes, we focused on an expressive language of keyframe generation via selections. Our abstractions facilitate the design of visualizations that must produce many keyframes backed by a fixed encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2">Natural Programming vs. Core Calculus Design</head><p>To make their systems easy to use for their target audiences, the authors of Data Animator and Gemini aimed to develop grammars that matched the existing mental models of animation designers. To that end, both groups conducted interviews prompting experienced animators to sketch interfaces or write pseudocode to recreate exemplar animated visualizations <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b44">45]</ref>. Fundamental abstractions emerged from these formative studies. For instance, Gemini's studies yielded the concepts of synchronizing ('at the same time') and concatenating ('then', 'after') while Data Animator's studies surfaced designers' familiarity with keyframes in Adobe After Effects. This design process is known as natural programming, where a developer aims "for the language and environment to work the way that nonprogrammers expect" <ref type="bibr" target="#b29">[30]</ref>.</p><p>In contrast, we set out to develop a small core calculus <ref type="bibr" target="#b5">[6]</ref> of abstractions for Animated Vega-Lite, which we outlined in Sect. 4. Our design was motivated by the desire to explore whether interaction and animation could be unified. This unification would likely not have been elicited by a target user. Because the key idea of our paper is to identify a unified abstraction, this difference in approach results in a design tradeoff. As Kim explained, Animated Vega-Lite may seem natural to a Vega-Lite user, but might present a steeper learning curve to someone familiar with animation tools like Adobe AfterEffects, as Animated Vega-Lite has no explicit concept of a keyframe.</p><p>Analyzing these processes via the Cognitive Dimensions of Notation <ref type="bibr" target="#b43">[44]</ref>, we find that iterating closely with end users in a natural programming process yields a grammar that closely maps to common user mental models. On the other hand, by distilling abstractions to a reduced set of orthogonal concepts, a core calculus process better emphasizes a consistent API that has low viscosity. Over-emphasizing one process or the other may drag a language design too far to one side. With PLIERS, Coblenz et al. <ref type="bibr" target="#b5">[6]</ref> offer suggestions for how developers may integrate and balance between these approaches. They recommend a developer iterate between developing the theoretical foundations of their language (core calculus) and the user-facing language (surface language). Moreover, Coblenz et al. suggest adapting natural programming by progressively prompting a user with incrementally more information about a language's proposed API. This additional scaffolding can help scope how natural programming studies explore mental models, and also lets a language developer gain insights even when the core calculus significantly departs from a user's familiar models. Integrated design processes, like PLIERS, are likely to be valuable methods for assessing future unified grammars, because these systems must balance significant conceptual unifications with end-users' ease-of-use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Animation Abstractions and Design Considerations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1">Scene-vs. Segue-Dominant Abstractions</head><p>Several interviewees noted that Animated Vega-Lite's abstractions appear complementary to their systems. For example, Kim noted his conceptual distinction between Animated Vega-Lite and Gemini is "[Animated Vega-Lite] animates the internal state within Vega-Lite, and Gemini doesn't care about the internal state. It just transforms between two static states of Vega-Lite." Similarly, Thompson said "if you compare [Animated Vega-Lite] directly to Data Animator, the two of them together would be really nice. What one doesn't have, the other does really well." For instance, he highlighted Animated Vega-Lite's ability to automatically generate keyframes from data (e.g., each year keyframe in Gapminder) and Data Animator's ability to precisely specify transitions between keyframes (such as staggering) as complementary components of the two systems. He also appreciated Animated Vega-Lite's ability to create overlapping keyframes via layering, as in our bar chart race example (Fig. <ref type="figure" target="#fig_3">4</ref>). Pedersen provides one explanation for why our approach is complementary to the existing systems we studied. In his useR! 2018 keynote, Pedersen introduced the concepts of a scene and a segue animation <ref type="bibr" target="#b40">[41]</ref>. A scene animation, such as Gapminder, is one where the data is changing (such as countries ranging over years), but the visual encoding is not. One can imagine a scene playing within a fixed stage (i.e., a static visual encoding). In contrast, a segue animation -such as a pie chart transitioning to a bar chart -is one where the visual encoding is changing, but the data is fixed. In practice, the line between a scene and segue is not always clear. For example, transitioning from a strip plot to a box and whiskers plot involves both a change to the data (computing aggregate quantities) and a change to the visual encoding (converting to box-and-whiskers).</p><p>Using this scene and segue distinction, Animated Vega-Lite and gganimate may be categorized as scene-dominant grammars. Both systems aim to cover a large space of animated visualizations with fixed encodings, such as Gapminder and bird migrations. Both systems support an additional collection of visual encoding transformations. For example, Animated Vega-Lite supports rescaling, panning, and zooming while gganimate supports transitions that can interpolate between different shapes with the same underlying data. Though both Animated Vega-Lite and gganimate are scene-dominant systems, Pedersen highlighted the expressiveness of Animated Vega-Lite's selection model for generating arbitrary keyframes from data (as shown with the Dunkin example in Fig. <ref type="figure" target="#fig_2">3</ref>) as a key conceptual distinction between the two.</p><p>On the other hand, Data Animator, Canis, and Gemini are seguedominant. These systems have focused primarily on connecting two distinct keyframes that may have distinct visual encodings and data. To construct a transition, Data Animator, Canis, and Gemini each construct a mapping between two keyframes. This approach works well when the data set is fixed, and there are only a few keyframes (as is typical when showing a small handful of segues). But as identified by Thompson and Liu, to support an animation like Gapminder, these systems must produce a keyframe for every year in the dataset.</p><p>As discussed in Sect. 6, Animated Vega-Lite inherits Vega-Lite's inability to represent complex runtime changes to visual encodings and data transformations. We suspect that extending Vega-Lite with these capabilities could enable segue animations in a future version of Animated Vega-Lite. To support complex runtime changes, Vega-Lite's conditional encodings could be extended from just mark properties to mark types and data transforms as in Ivy <ref type="bibr" target="#b24">[25]</ref>. And our support for enter and exit could be extended to operate not just on data, but also on these more expressive encoding changes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2">Modeling Transitions Between Keyframes</head><p>Keyframes were the most salient animation abstraction in our interviews. We discussed keyframe concepts with every interviewee, and they would often use keyframes to pose comparisons between different systems' abstractions. Every tool had to make decisions about (i) how to generate keyframes and (ii) how to transition between them. Moreover, keyframes and transitions are useful abstractions for both sceneand segue-dominant systems. In this subsection we surface an axis of the keyframe design space: modeling transitions between keyframes.</p><p>Non-parametric transitions. The simplest kind of transition between keyframes is a non-parametric transition. Consider a linear sequence of keyframes, where each keyframe describes an entire scenegraph. Transitions between these keyframes are non-parametric in that the same transition is applied to every data point. For example, changing every bar to a point in 0.5 seconds (a segue animation) is a non-parametric transition because the transition's definition is independent of the mark's encoded data -i.e. its duration is a constant value. Similarly, animating countries in Gapminder (a scene animation) is also a non-parametric transition because the transition applied to each mark is identical (moving between two points in a fixed time interval).</p><p>Animated Vega-Lite supports non-parametric transitions via its timer, easing, and interpolation abstractions, which implicitly specify a transition across keyframes. The other libraries also support non-parametric transitions between pairs of keyframes, but only scene-dominant systems (gganimate and Animated Vega-Lite) support non-parametric transitions across many keyframes. In scene-dominant animations, the same transition specification can be reused across a sequence of keyframes sharing a fixed encoding.</p><p>Parametric transitions. In contrast to non-parametric transitions, parametric transitions involve transition definitions that depend on the backing data. A common use case for this model is to stagger transitions -a common segue technique that applies a small delay to each animated element to make them easier to track <ref type="bibr" target="#b11">[12]</ref>. Because parametric transitions depend on data, individual marks can have different timing properties during the same transition.</p><p>Segue-dominant systems Data Animator, Canis, and Gemini all support parametric transitions. But, as Thompson identified in his postinterview, parametric transitions also increase the expressive gamut of scene animations. For example, Fig. <ref type="figure" target="#fig_5">6</ref> shows "Swimming World Records Throughout History" from the Data Animator example gallery. This animated scatterplot shows replays of world record swimmers. The input data includes swimmers and their final race times. When Thompson tried to port this example to Animated Vega-Lite, he realized he "had no clue how to do it. The two keyframes in this example are very simple. All of the circles at one x position, and then all of the circles like 200-400 pixels to the right. For us, you change the speed of each individual shape based on a data property." Animated Vega-Lite could support this animation by allowing users to explicitly define a transition, with its speed parameterized by a data value.</p><p>To support parametric transitions, future versions of Animated Vega-Lite could use Lu et al.'s concept of "dynamic functions" <ref type="bibr" target="#b23">[24]</ref>. These functions use mappings between data and transitions to specify rateof-change properties of transitions over time (e.g., encoding transition speed instead of mark position). Adapting this segue-dominant concept to Animated Vega-Lite could increase expressivity, though further work is required to understand its composition with and implications for static and interactive language constructs. For instance, segue transition properties may more easily compose with existing static and interactive Vega-Lite constructs if translated back into scene keyframes as direct encodings instead of rates (e.g. instantiating transition speed as additional position keyframes). However, this would trade off the memory efficiency of the segue representation.</p><p>Connecting transitions in series and parallel. Some of the most compelling animated examples cannot be represented as a linear sequence of transitions, parametric or not. For instance, Periscopic's Gun Deaths animation <ref type="bibr" target="#b30">[31]</ref>, a visualization frequently cited by our interviewees, cannot easily be represented even by parametric transitions. When discussing this example, Thompson remarked: "This was one that I had on my list of 'oh it would be so cool if we could create this,' and then I could just not figure out a way of doing it. [...] How do you have the circle appear and then drop, and then the line keeps going? I have no clue how to do that [in Data Animator]". Authoring this animation is difficult because there is no linear transition specification: the animation splits in two when the circle drops and the line continues. We are not certain that any of the grammars we have discussed in our critical reflections can easily express this animation, because it involves both scene and segue animation.</p><p>Gemini's composition rules offer a promising path for the transitions necessary to support the Gun Deaths animation. Gemini's concat primitive allows a user to specify animations in series, while its sync primitive allows a user to specify animation components that play in parallel. Using these primitives, one could specify a sync that splits the animation into the circle and the line, and then concat the many stages of the Gun Deaths animation together. More generally, concat and sync allow a user to model transitions as a series-parallel graph <ref type="bibr" target="#b47">[48]</ref>.</p><p>However, this abstraction alone is not enough. While Gemini has a rich transition language, it cannot generate keyframes automatically from data like Animated Vega-Lite. This generation is necessary for the Gun Deaths animation to visualize individual points. Combining Gemini's segue abstractions with Animated Vega-Lite's scene abstractions is a promising future direction for expressive animation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION AND FUTURE WORK</head><p>Animated Vega-Lite contributes a low viscosity, compositional, and systematically enumerable grammar that unifies specification of static, interactive, and animated visualizations. Within a single grammar, authors can now easily switch between the three modalities during rapid prototyping, and also compose them together to effectively communicate and analyze faceted and time-varying data.</p><p>Our grammar takes a promising step in helping authors develop visualizations that leverage the dynamic affordances of computational media. During interviews, Pedersen described unification as the "holy grail" of data visualization APIs: "A grammar of graphics that defines how things look, a grammar of animation that defines how things react, and a grammar of interaction that defines how things interact. Having all of that in one unified theoretical framework would simply be awesome." Future work might more deeply explore the distinctions and tradeoffs we surfaced between transition and keyframe models, or study the implications of unification at the lower-level of reactive programming semantics and data stream management.</p><p>Beyond language design, we hope that Animated Vega-Lite facilitates future work on interactive and animated visualization akin to the role the original Vega-Lite has played. For instance, how might we leverage Animated Vega-Lite's ability to enumerate static, interactive, and animated visualizations to study how these modalities facilitate data analysis and communication -replicating and extending prior work <ref type="bibr" target="#b32">[33]</ref> more systematically? Similarly, how might study results be codified in the Draco knowledge base <ref type="bibr" target="#b28">[29]</ref>, or exposed in systems like Voyager <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50]</ref> or Lux <ref type="bibr" target="#b22">[23]</ref> to recommend animated visualizations during exploratory data analysis? To support this future research, we intend to contribute our work back to the open source Vega-Lite project.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An analyst's workflow with Animated Vega-lite. A) Static visualization of bird migrations. B) Adding interaction to hover over a migration path and view a tooltip. C) Switching from static lines to animated circle marks. D) Adding animated path trails for the previous 5 days. E) Adding an interactive slider to scrub through the animation.</figDesc><graphic url="image-57.png" coords="3,158.81,154.62,76.44,84.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Animated Vega-Lite specification of the influential Gapminder animation [34]. (A) A minimal specification using only time encoding. (B) The same specification elaborated to show default encoding properties and a default selection. (C) Selected keyframes from the resulting animation.</figDesc><graphic url="image-96.png" coords="4,398.44,130.48,68.92,67.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Animation of Dunkin' Donuts stores' opening and closing times. With a custom domain and predicate, the animation advances through 24 hours at a constant rate and conditionally colors each store if the current time is between the store's open and close times.</figDesc><graphic url="image-119.png" coords="5,64.03,72.14,159.50,70.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Demonstration of the rescale time encoding property recreating a D3 bar chart race example [28]. (A) rescale is true: the viewport is recalculated on each keyframe. (B) rescale is false: the viewport is calculated on the whole dataset, and does not update with the selection.</figDesc><graphic url="image-145.png" coords="6,348.40,86.59,77.44,81.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig.5. Animated Vega-Lite examples demonstrating coverage over interaction and animation taxonomies<ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b50">51]</ref> (see Fig.4for an example substrate transform and Fig.3for select). A) View transform via panning, abstract/elaborate via overview + detail, and connecting multiple views. B) Filtering data via a predicate. C) Ordering / reconfiguring a sorted axis in a bump chart. D) Exploring sequential timesteps of an index chart. E) A hypothetical outcome plot in the style of the New York Times<ref type="bibr" target="#b14">[15]</ref>. F) An interactive brush selection over Gapminder.zooming behavior. This visualization demonstrates a transformation, changing the reader's viewpoint by panning and zooming the top view. It also demonstrates an abstract/elaborate intent by showing the data at different levels of detail in the top and bottom view, and the connect intent by showing corresponding data across multiple views.Fig.4shows a bar chart's x-scale dynamically recalculating on each frame using the rescale property of a time encoding (Sect. 4.1.3). This animation technique demonstrates a substrate transformation through scale manipulations. It also demonstrates the reconfigure intent by showing a new spatial arrangement of the data.In Fig.3and Fig.5b, we apply a conditional filter over the whole dataset, with filter parameters changing over time. In contrast to faceting, filtering can leverage custom selection predicates to show and hide data -a single data point can appear in multiple groups. Both taxonomies contain a category for filtering, shown here by adding or removing elements from the display. Fig.3additionally demonstrates a select intent by using conditional encoding to highlight selected data.Fig.4and Fig.5cshow examples with a sorted axis. When a key is specified in a time encoding, the system automatically tweens an element's position even when its sort index has changed in the next keyframe. Continually sorting elements as the underlying data changes demonstrates an ordering transition, as well as a reconfigure intent. Time encodings transition between sequential time values by default in Animated Vega-Lite (e.g. Fig.2). Fig.5ddemonstrates an additional example of this animation. A default animated point selection is applied to a data transform that re-normalizes a stock price time-series chart on each tick. The original Vega-Lite paper contains an interactive version of this example, which instead populates the point selection on mouse hover events<ref type="bibr" target="#b35">[36]</ref>. These examples demonstrate timestep transitions, which also fulfill the explore intent by showing new data points at each step. Axis re-normalization is also an example of a reconfigure intent.In addition to achieving broad coverage over the two taxonomies, our system also supports simulation techniques including hypothetical outcome plots (Fig.5e)<ref type="bibr" target="#b12">[13]</ref>. And, as previously discussed in Sect. 4.2.1, animated selections can be applied to the same set of dynamic visual behaviors as interactive selections. Consequently, users can easily switch between timer and input event streams when prototyping existing interaction techniques in Vega-Lite. For example, Fig.5aand Fig.5dshow animated selections driving common interaction techniques -panning and re-normalizing, respectively. Users can also easily compose interaction techniques with animated visualizations by defining additional selections. For example, Fig.5fdemonstrates an interactive brush used to highlight a region of an animated Gapminder visualization. Points of interest are conditionally colored as they enter or exit the brush region.Discussion and Limitations. Like the original Vega-Lite, Animated Vega-Lite intentionally trades some limits to expressivity for gains in concise, high-level, declarative specification. In Sects. 7.2.1 &amp; 7.2.2, we detail this expressiveness tradeoff in terms of the classes of animation</figDesc><graphic url="image-188.png" coords="7,74.75,145.44,147.77,71.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Swimming World Records example from Data Animator [16].</figDesc><graphic url="image-211.png" coords="9,69.70,72.77,188.14,69.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>technique Techniques common to interaction and animation taxonomies.</figDesc><table><row><cell>Conditional</cell><cell>Select</cell><cell>-</cell></row><row><cell>encoding</cell><cell></cell><cell></cell></row><row><cell>Panning</cell><cell>Explore</cell><cell>View transformation</cell></row><row><cell>Zooming</cell><cell>Abstract / Elaborate</cell><cell>View transformation</cell></row><row><cell cols="2">Axis re-scaling Reconfigure</cell><cell>Substrate</cell></row><row><cell></cell><cell></cell><cell>transformation</cell></row><row><cell>Axis sorting</cell><cell>Reconfigure</cell><cell>Ordering</cell></row><row><cell>Filtering</cell><cell>Filter</cell><cell>Filtering</cell></row><row><cell>Enter/exit</cell><cell>Explore</cell><cell>Timestep</cell></row><row><cell>Multi-view</cell><cell>Connect</cell><cell>-</cell></row><row><cell>Changing</cell><cell>Encode</cell><cell>Visualization change,</cell></row><row><cell>encodings</cell><cell></cell><cell>Data schema change</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>). For example, Heer and Robertson cite</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Thompson &amp; Liu also co-authored the original critical reflections paper<ref type="bibr" target="#b34">[35]</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank our critical reflections interlocutors and anonymous reviewers. This work was supported by NSF grants #1942659 and #1900991 and by the NSF's SaTC Program. This material is based upon work supported by the National Science Foundation under Grant No. 1745302.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><surname>Plotly³graphing³libraries</surname></persName>
		</author>
		<ptr target="³https://plotly.com/graphing-libraries/.³" />
		<title level="m">³2012</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">³I.³Erhan,³and³C.³D.³Shaw.³Does³inter-active³animation³control³improve³exploratory³data³analysis³of³animated³ trend³visualization?³ In³Visualization and Data Analysis</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">³a</forename><surname>³abukhodair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³b</forename><forename type="middle">³e</forename><surname>³riecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³h</forename></persName>
		</author>
		<idno type="DOI">³10.1117/12.2001874³</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="211" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">M</forename><surname>³bostock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³v</forename><surname>³d³³data-Driven³documents</surname></persName>
		</author>
		<idno type="DOI">³10.1109/TVCG.2011.185³</idno>
	</analytic>
	<monogr>
		<title level="j">³IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2301" to="2309" />
			<date type="published" when="2011-12">Dec.³2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">³Schumacher.³The³relevance³of³attention³for³selecting³ news³content.³An³eye-tracking³study³on³attention³patterns³in³the³reception³ of³print³and³online³media</title>
		<author>
			<persName><forename type="first">³ H.-J</forename><surname>³bucher³and³p</surname></persName>
		</author>
		<idno>³Jan.³2006.³doi:³ 10.³ 1515/COMMUN.2006.022³</idno>
	</analytic>
	<monogr>
		<title level="j">³ Communications</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The³Not-so-Staggering³ Effect³of³Staggered³Animated³Transitions³on³Visual³Tracking</title>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">F</forename><surname>³chevalier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³p</forename></persName>
		</author>
		<idno type="DOI">³10.1109/TVCG.2014.2346424³</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2241" to="2250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">M</forename><surname>³coblenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³g</forename><surname>³kambhatla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³p</forename><surname>³koronkevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³j</forename><forename type="middle">³l</forename><surname>³wise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³c</forename><surname>³barnaby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³j</forename><surname>³sun-Shine</surname></persName>
		</author>
		<idno type="DOI">³10.1145/3452379³</idno>
	</analytic>
	<monogr>
		<title level="j">³J.³Aldrich,³and³B.³A.³Myers.³ PLIERS:³A³Process³that³Integrates³ User-Centered³Methods³into³Programming³Language³Design.³ACM Transactions on Computer-Human Interaction</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2021">³July³2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">³ Temporal³ distortion³ for³ animated³ transitions</title>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">P</forename><surname>³dragicevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³a</forename><surname>³bezerianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³w</forename><surname>³javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³n</forename></persName>
		</author>
		<author>
			<persName><forename type="first">.-D</forename><surname>³fekete</surname></persName>
		</author>
		<idno type="DOI">³10.1145/1978942.³1979233³</idno>
	</analytic>
	<monogr>
		<title level="m">In³ Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,³pp.³2009-³ 2018.³ACM,³Vancouver³BC³Canada,³May³2011</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">³elliott³and³p</forename><surname>³hudak</surname></persName>
		</author>
		<idno type="DOI">³10.1145/258948.258973³</idno>
		<title level="m">³Functional³reactive³animation.³In³Proceedings of the second ACM SIGPLAN international conference on Functional programming,³ICFP³&apos;97,³pp.³263-273.³Association³for³Computing³Machinery,³ New³York,³NY,³USA,³Aug.³1997</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">T</forename><surname>³ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³b</forename><surname>³wang</surname></persName>
		</author>
		<idno type="DOI">³10.1145/3411764.3445452³</idno>
		<title level="m">³CAST:³Authoring³Data-Driven³Chart³Anima-tions.³In³Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems,³CHI³&apos;21,³pp.³1-15.³Association³for³Computing³Ma-chinery,³New³York,³NY,³USA,³May³2021</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">T</forename><surname>³ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³y</forename><surname>³zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³b</forename><surname>³lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³b</forename><surname>³d.³ren</surname></persName>
		</author>
		<author>
			<persName><surname>Canis</surname></persName>
		</author>
		<idno type="DOI">³10.1111/cgf.14005³</idno>
		<title level="m">³A³High-Level³Language³for³Data-Driven³Chart³Animations.³Computer Graphics Forum,³2020.³Publisher:³The³Eurographics³Association³and³John³Wiley³&amp;³ Sons³Ltd</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">E</forename><surname>³greussing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³s</forename><surname>³h</surname></persName>
		</author>
		<author>
			<persName><surname>³kessler</surname></persName>
		</author>
		<idno type="DOI">³10.1177/1075547020962100³</idno>
		<title level="m">³and³H.³G.³Boomgaarden.³ Learning³From³ Science³News³via³Interactive³and³Animated³Data³Visualizations:³An³Inves-tigation³Combining³Eye³Tracking,³Online³Survey,³and³Cued³Retrospective³ Reporting.³Science Communication</title>
				<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="803" to="828" />
		</imprint>
	</monogr>
	<note>³Dec.³2020.³Publisher:³ SAGE³Publications³Inc</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">³ Animated³ Transitions³ in³ Statistical³ Data³ Graphics.³IEEE Transactions on Visualization and Computer Graphics</title>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">J</forename><surname>Heer³</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Robertson</surname></persName>
		</author>
		<idno type="DOI">³10.1109/TVCG.2007.70539³</idno>
	</analytic>
	<monogr>
		<title level="j">³</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1240" to="1247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">³P.³Resnick,³and³E.³Adar.³ Hypothetical³Outcome³Plots³Out-perform³Error³Bars³and³Violin³Plots³for³Inferences³about³Reliability³of³ Variable³Ordering</title>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">J</forename><surname>³hullman</surname></persName>
		</author>
		<idno type="DOI">³10.1371/journal.pone.0142444³</idno>
	</analytic>
	<monogr>
		<title level="m">e0142444,³Nov.³2015.³Publisher:³ Public³Library³of³Science</title>
				<imprint>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">³d</forename><surname>³hunter</surname></persName>
		</author>
		<idno type="DOI">³10.1109/MCSE.2007.55³</idno>
		<title level="m">³Matplotlib:³A³2D³Graphics³Environment.³Computing in Science Engineering</title>
				<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="90" to="95" />
		</imprint>
	</monogr>
	<note>³May³2007.³Conference³Name:³Computing³ in³Science³Engineering</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">³Irwin³and³K.³Quealy.³How³Not³to³Be³Misled³by³the³Jobs³Report.³The New York Times</title>
		<imprint>
			<date type="published" when="2014">³May³2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><surname>John³thompson</surname></persName>
		</author>
		<author>
			<persName><surname>³swimming³world³records³throughout³history</surname></persName>
		</author>
		<idno>³2020.³</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">A</forename><surname>³kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³m</forename><surname>³f.³nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Hypothetical³outcome³ Plots³help³untrained³observers³judge³trends³in³ambiguous³data</surname></persName>
		</author>
		<idno type="DOI">³10.1109/TVCG.2018.2864909³</idno>
	</analytic>
	<monogr>
		<title level="j">³IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="892" to="902" />
			<date type="published" when="2019-01">Jan.³2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">Y</forename><surname>³kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³m</forename><surname>Designing³animated³transitions³to³ Convey³aggregate³operations</surname></persName>
		</author>
		<idno type="DOI">³2019.³doi:³10.1111/cgf.13709³</idno>
	</analytic>
	<monogr>
		<title level="j">³ Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="541" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">³kim³and³j</forename><surname>³heer</surname></persName>
		</author>
		<author>
			<persName><surname>Gemini</surname></persName>
		</author>
		<idno>10.³ 1109/TVCG.2020.3030360³</idno>
	</analytic>
	<monogr>
		<title level="m">A³Grammar³and³Recommender³System³ for³Animated³Transitions³in³Statistical³Graphics.³IEEE Transactions on Visualization and Computer Graphics</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="485" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">³kim³and³j</forename><surname>³heer</surname></persName>
		</author>
		<idno type="DOI">³10.1109/VIS49827.2021.9623291³</idno>
		<title level="m">³Geminiˆ2:³Generating³Keyframe-Oriented³Animated³ Transitions³Between³Statistical³Graphics.³ In³2021 IEEE Visualization Conference (VIS),³pp.³201-205.³IEEE,³New³Orleans,³LA,³USA,³Oct.³2021</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">³DimpVis:³Exploring³Time-varying³Information³ Visualizations³by³Direct³Manipulation.³IEEE Transactions on Visualization and Computer Graphics</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">³kondo³and³c</forename><surname>³collins</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2014.2346250³</idno>
	</analytic>
	<monogr>
		<title level="m">Dec.³2014.³ Conference³ Name:³IEEE³Transactions³on³Visualization³and³Computer³Graphics</title>
				<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="2003" to="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">³M.³Hochachka,³and³S.³Kelling.³Convergence³ of³broad-scale³migration³strategies³in³terrestrial³birds</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">³a</forename><surname>³la³sorte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³d</forename><surname>³fink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³w</forename></persName>
		</author>
		<idno type="DOI">³10.1098/rspb.2015.2588³</idno>
	</analytic>
	<monogr>
		<title level="m">20152588,³Jan.³2016.³ Publisher:³Royal³Society</title>
				<imprint>
			<date type="published" when="1823">1823</date>
			<biblScope unit="volume">283</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<author>
			<persName><forename type="first">D</forename><surname>³j.-L.³lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">D</forename><surname>³tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">K</forename></persName>
		</author>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">T</forename><surname>³boonmark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">C</forename><surname>³chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">J</forename><surname>³kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">U</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">M</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">M</forename></persName>
		</author>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³</forename><surname>.³ G.³ Parameswaran</surname></persName>
		</author>
		<author>
			<persName><surname>Lux</surname></persName>
		</author>
		<idno type="DOI">³Nov.³2021.³doi:³10.14778/3494124.3494151³</idno>
	</analytic>
	<monogr>
		<title level="m">³always-on³visualization³recommendations³for³ex-ploratory³dataframe³workflows.³ Proceedings of the VLDB Endowment</title>
				<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="727" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">³Cohen-Or,³and³H.³Huang.³Enhancing³ Static³Charts³With³Data-Driven³Animations</title>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">M</forename><surname>³lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³n</forename><surname>³fish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³s</forename><surname>³wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³j</forename><surname>³lanir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³d</forename></persName>
		</author>
		<idno type="DOI">³July³2022.³Conference³Name:³IEEE³Transactions³on³Visualization³and³Computer³Graphics.³doi:³10.1109/TVCG.2020.3037300³</idno>
	</analytic>
	<monogr>
		<title level="j">³IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2628" to="2640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">³m</forename><surname>³mcnutt³and³r</surname></persName>
		</author>
		<author>
			<persName><surname>³chugh</surname></persName>
		</author>
		<idno type="DOI">³10.1145/3411764.3445356³</idno>
		<title level="m">³Integrated³Visualization³Editing³via³Parame-terized³Declarative³Templates.³In³Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems,³pp.³1-14.³ACM,³Yokohama³ Japan,³May³2021</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">³a</forename><surname>³meyerovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³a</forename><surname>³guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³g</forename><forename type="middle">³h</forename><surname>³j.³baskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³m</forename><surname>³cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">A</forename><surname>³greenberg</surname></persName>
		</author>
		<author>
			<persName><surname>³bromfield</surname></persName>
		</author>
		<idno type="DOI">³10.1145/1640089.1640091³</idno>
		<title level="m">³and³S.³Krishnamurthi.³Flapjax:³a³programming³language³for³ Ajax³applications.³In³Proceedings of the 24th ACM SIGPLAN conference on Object oriented programming systems languages and applications,³ OOPSLA³&apos;09,³pp.³1-20.³Association³for³Computing³Machinery,³New³ York,³NY,³USA,³Oct.³2009</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName><surname>Mike³bostock</surname></persName>
		</author>
		<ptr target="³2015.³https://github.com/d3/d3-ease.³" />
		<imprint/>
	</monogr>
	<note>³d3-ease</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Mike³</forename><surname>Bostock</surname></persName>
		</author>
		<ptr target="https://observablehq.com/@d3/bar-chart-race-explained.³" />
		<title level="m">Bar³ Chart³ Race</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">D</forename><surname>³moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³c</forename><surname>³wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³g</forename><surname>³l.³nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³h</forename><surname>³lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³a</forename><forename type="middle">³m</forename><surname>³smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³b</forename></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>³heer</surname></persName>
		</author>
		<idno type="DOI">³Jan.³2019.³Conference³Name:³IEEE³Transactions³on³Visualization³and³Computer³Graphics.³doi:³10.1109/TVCG.2018.2865240³</idno>
		<title level="m">³ Formalizing³Visualization³Design³Knowledge³as³Constraints:³ Actionable³and³Extensible³Models³in³Draco.³IEEE Transactions on Visualization and Computer Graphics</title>
				<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="438" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">³a</forename><surname>³myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³j</forename><forename type="middle">³f</forename></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">³j</forename><surname>³ko</surname></persName>
		</author>
		<idno type="DOI">³10.1145/1015864.1015888³</idno>
	</analytic>
	<monogr>
		<title level="j">³ Natural³programming³languages³ and³environments.³Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="47" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><surname>Periscopic</surname></persName>
		</author>
		<title level="m">³United³States³gun³death³data³visualization,³2013.³</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">D</forename><surname>³ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³b</forename><surname>³lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³m</forename></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">³h</forename><surname>³riche</surname></persName>
		</author>
		<idno type="DOI">³10.1109/BELIV.2018.8634297³</idno>
		<title level="m">³Reflecting³on³the³Evaluation³ of³Visualization³Authoring³Systems³:³ Position³Paper.³ In³2018 IEEE Evaluation and Beyond -Methodological Approaches for Visualization (BELIV),³pp.³86-92,³Oct.³2018</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">G</forename><surname>³robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³r</forename><surname>³fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³d</forename><surname>³fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³b</forename><surname>³lee</surname></persName>
		</author>
		<idno type="DOI">³Nov.³2008.³doi:³10.1109/³TVCG.2008.125³</idno>
	</analytic>
	<monogr>
		<title level="j">³and³J.³Stasko.³Effectiveness³ of³Animation³in³Trend³Visualization.³IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1325" to="1332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The³ best³ stats³ you&apos;ve³ ever³ seen</title>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">H</forename><surname>Rosling</surname></persName>
		</author>
		<ptr target="https://www.ted.com/talks/hans³rosling³the³best³stats³you³ve³ever³seen.³" />
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">A</forename><surname>Satyanarayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">D</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">J</forename><surname>Heer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">J</forename><surname>Stasko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">J</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">M</forename><surname>³brehmer</surname></persName>
		</author>
		<idno type="DOI">³2019.³doi:³10.1109/TVCG.2019.2934281³</idno>
		<title level="m">³and³Z.³Liu.³Critical³Reflections³on³Visualization³Authoring³ Systems.³IEEE Transactions on Visualization and Computer Graphics,³pp.³ 1-1</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">A</forename><surname>³satyanarayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³d</forename><surname>³moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³k</forename><surname>³heer</surname></persName>
		</author>
		<idno>1109/TVCG.2016.2599030³</idno>
		<title level="m">³Vega-Lite:³ A³Grammar³of³Interactive³Graphics.³IEEE Transactions on Visualization and Computer Graphics</title>
				<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="341" to="350" />
		</imprint>
	</monogr>
	<note>³Jan.³2017.³Conference³Name:³ IEEE³Transactions³on³Visualization³and³Computer³Graphics.³doi:³ 10</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">A</forename><surname>³satyanarayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³r</forename><surname>³russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³j</forename></persName>
		</author>
		<idno type="DOI">³10.1109/TVCG.2015.2467091³</idno>
		<title level="m">³Hoffswell,³and³J.³Heer.³Reactive³Vega:³A³ Streaming³Dataflow³Architecture³for³Declarative³Interactive³Visualization.³ IEEE Transactions on Visualization and Computer Graphics</title>
				<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="659" to="668" />
		</imprint>
	</monogr>
	<note>³Jan.³2016.³Conference³Name:³IEEE³Transactions³on³Visualization³ and³Computer³Graphics</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">A</forename><surname>³satyanarayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³k</forename><surname>³new³york</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³usa</forename><surname>³ny</surname></persName>
		</author>
		<author>
			<persName><surname>Oct</surname></persName>
		</author>
		<idno type="DOI">³2014.³doi:³10.1145/2642918.2647360³</idno>
		<title level="m">³Heer.³Declarative³interaction³ design³for³data³visualization.³ In³Proceedings of the 27th annual ACM symposium on User interface software and technology</title>
				<imprint/>
	</monogr>
	<note>³UIST³&apos;14,³pp.³ 669-678.³Association³for³Computing³Machinery,</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">X</forename><surname>³shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">A</forename><surname>³wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">J</forename><surname>³tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">B</forename><surname>³bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">Y</forename><surname>³wu</surname></persName>
		</author>
		<author>
			<persName><surname>And³h</surname></persName>
		</author>
		<idno type="DOI">³10.1109/TVCG.³2020.3030396³</idno>
		<title level="m">³Qu.³ What³Makes³ a³Data-GIF³Understandable?³ IEEE Transactions on Visualization and Computer Graphics</title>
				<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1492" to="1502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName><surname>Stephanie³yee³and³tony³chu</surname></persName>
		</author>
		<author>
			<persName><surname>³a³visual³introduction³to³machine³learning</surname></persName>
		</author>
		<author>
			<persName><surname>Part³ii</surname></persName>
		</author>
		<ptr target="³2015.³http://www.r2d3.us/visual-intro-to-machine-learning-part-2/.³" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">The³ Grammar³ of³ Animation,³ July³</title>
		<author>
			<persName><forename type="first">Lin³</forename><surname>Thomas³</surname></persName>
		</author>
		<author>
			<persName><surname>Pedersen</surname></persName>
		</author>
		<ptr target="https://www.youtube.com/watch?v=21ZWDrTukEs.³" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">³</forename><surname>³ Thomas³lin³pedersen</surname></persName>
		</author>
		<author>
			<persName><surname>Gganimate³has³transitioned³to³a³state³of³release</surname></persName>
		</author>
		<ptr target="³https://www.data-imaginist.com/2019/gganimate-has-transitioned-to-a-state-of-release/.³" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">³ A³Grammar³of³Animated³ Graphics</title>
		<author>
			<persName><surname>Thomas³lin³pedersen³and³david³robinson</surname></persName>
		</author>
		<ptr target="³2019.³https://gganimate.com/.³" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">³</forename><surname>³ Thomas³rg³green</surname></persName>
		</author>
		<author>
			<persName><surname>Cognitive³dimensions³of³notations</surname></persName>
		</author>
		<title level="m">In³A.³Sutcliffe³ and³L.³Macaulay,³eds.,³People and Computers V,³pp.³443-460.³Cambridge³ University³Press,³Cambridge,³UK,³1989</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">J</forename><surname>³thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³z</forename><surname>³liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³w</forename><surname>³li</surname></persName>
		</author>
		<idno type="DOI">³2020.³doi:³10.1111/cgf.13974³</idno>
	</analytic>
	<monogr>
		<title level="j">³and³J.³Stasko.³ Understanding³the³Design³ Space³and³Authoring³Paradigms³for³Animated³Data³Graphics.³Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="207" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">³r</forename><surname>³thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³z</forename><surname>³stasko</surname></persName>
		</author>
		<idno>1145/3411764.3445747³</idno>
		<title level="m">³Data³Animator:³Authoring³Expres-sive³Animated³Data³Graphics.³In³Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems,³CHI³&apos;21,³pp.³1-18.³Association³ for³Computing³Machinery,³New³York,³NY,³USA,³May³2021.³doi:³ 10</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Animation:³ can³it³ facilitate?</title>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">B</forename><surname>³tversky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">³b</forename><surname>³morrison</surname></persName>
		</author>
		<author>
			<persName><surname>And³m</surname></persName>
		</author>
		<idno type="DOI">³Oct.³2002.³doi:³10.1006/ijhc.2002.1017³</idno>
	</analytic>
	<monogr>
		<title level="j">³ International Journal of Human-Computer Studies</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="247" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><surname>Wikipedia³contributors</surname></persName>
		</author>
		<idno>³2022.³</idno>
		<title level="m">³ Series-parallel³graph³-³Wikipedia,³The³Free³ Encyclopedia</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">³Voyager:³Exploratory³Analysis³via³Faceted³Browsing³of³Visualiza-tion³Recommendations</title>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">K</forename><surname>³wongsuphasawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³d</forename><surname>³moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³a</forename><surname>³anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³j</forename><surname>³mackinlay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³b</forename></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>³heer</surname></persName>
		</author>
		<idno type="DOI">³10.1109/TVCG.2015.³2467191³</idno>
	</analytic>
	<monogr>
		<title level="m">³Jan.³2016.³Conference³Name:³IEEE³Transac-tions³on³Visualization³and³Computer³Graphics</title>
				<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="649" to="658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">K</forename><surname>³wongsuphasawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³z</forename><surname>³qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³d</forename><surname>³moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³r</forename><surname>³chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³f</forename><surname>³ouk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³a</forename><surname>³anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">J</forename><surname>³mackinlay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³b</forename><surname>Voyager³2</surname></persName>
		</author>
		<idno type="DOI">³10.1145/3025453.3025768³</idno>
		<title level="m">Augmenting³Visual³ Analysis³with³Partial³View³Specifications.³ In³Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems,³CHI³&apos;17,³pp.³ 2648-2659.³Association³for³Computing³Machinery,³New³York,³NY,³USA,³ May³2017</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">³s</forename><surname>³yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³y</forename><surname>³kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³j</forename></persName>
		</author>
		<idno type="DOI">³10.1109/TVCG.2007.70515³</idno>
		<title level="m">Toward³a³Deeper³Under-standing³of³the³Role³of³Interaction³in³Information³Visualization.³ IEEE Transactions on Visualization and Computer Graphics</title>
				<imprint>
			<date type="published" when="2007-11">Nov.³2007</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1224" to="1231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">³</forename><forename type="middle">J</forename><surname>³zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">³d</forename></persName>
		</author>
		<author>
			<persName><forename type="first">³r</forename><surname>³neogy</surname></persName>
		</author>
		<idno>³Feb.³2021.³doi:³10.³ 1109/TVCG.2020.3030367³</idno>
		<title level="m">³and³A.³Satyanarayan.³Lyra³2:³Designing³ Interactive³Visualizations³by³Demonstration.³ IEEE Transactions on Visualization and Computer Graphics</title>
				<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="304" to="314" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
