<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Scanner Deeply: Predicting Gaze Heatmaps on Visualizations Using Crowdsourced Eye Movement Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sungbok</forename><surname>Shin</surname></persName>
						</author>
						<author>
							<persName><roleName>Sanghyun</roleName><forename type="first">Sunghyo</forename><surname>Chung</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Niklas</forename><surname>Hong</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><surname>Elmqvist</surname></persName>
						</author>
						<title level="a" type="main">A Scanner Deeply: Predicting Gaze Heatmaps on Visualizations Using Crowdsourced Eye Movement Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-03-27T09:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Gaze prediction</term>
					<term>visualization</term>
					<term>webcam-based eye-tracking</term>
					<term>crowdsourcing</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pipeline for developing a gaze prediction model that, given an image as an input, produces a saliency map for an image containing a visualization. First, we collect more than 10,000 images that contain one chart per image (A). Second, using the collected images we conduct a crowdsourced study on Amazon Mechanical Turk to gather gaze logs using a webcam eyetracker (B). Third, using the image collection with gaze log annotations, we train a model called SimpleNet, a CNN-based neural network model (C). Gaze predictions are shown in (D) in the form of saliency maps.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Your eyes are not just windows to your soul, but also to your ability to read and perceive the visual content on a computer screen. Thus, the ability to detect, track, and predict one's eye movements is used in numerous fields such as computer vision <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b62">63]</ref>, human-computer interaction <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b47">48]</ref>, and natural language processing <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12]</ref>. This is doubly true for data visualization, where tracking the user's gaze on an interactive chart is key to understanding sophisticated mechanisms behind how humans perceive them <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b34">35]</ref>. However, current eyetracker hardware is bulky and costly, thus preventing widespread use, and the resulting data is often large in scope and difficult to interpret. Recent progress in machine learning and computer vision has made web-based eyetracking using standard webcams feasible <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b54">55]</ref>. However, such techniques tend to be less accurate than specialized eyetracking hardware and are sensitive to varying lighting conditions, webcam placement, and even the user's appearance. At the same time, universally available eyetracking would undoubtedly be highly useful during the design and development phases of a data visualization, allowing the designer to quickly gauge the appearance and salience of a visual representation. What if you could, for example, figure out whether a peak in data stands out, or if a relationship between two items can be seen in the visual clutter? However, to the best of our knowledge, such functionality does not yet exist.</p><p>In this paper, we propose a virtual eye tracker using deep learning-a SCANNER DEEPLY-that, given an image of a visualization as an input, automatically generates a gaze heatmap for the image. Figure <ref type="figure">1</ref> outlines our approach for developing this deep learning-based eye tracker. We first gathered a large corpus of some 11,000 visualization images from the web. Then we collected eyetracking data at scale by conducting a crowdsourced user study on Amazon Mechanical Turk that leverages an existing webcam-based eyetracking technique <ref type="bibr" target="#b54">[55]</ref> to collect gaze logs for our visualization corpus. We used these images and the annotated Table <ref type="table">1</ref>: Taxonomy of prior work. We categorize the 30 papers that collect eye movement data with three dimensions: (1) the tracker, (2) image stimulus, and (3) collection methodology. Note that the papers are listed in chronological order. We found that there is no work that employs the same collection approach as ours, i.e., collecting webcam-based crowdsourcing eye movements on chart images. Predicting Affect from Gaze Data During Interaction... <ref type="bibr" target="#b27">[28]</ref> 2014 eyetracker texts lab experiment 10 A Crowdsourced Alternative to Eye-tracking for... <ref type="bibr" target="#b33">[34]</ref> 2015 mouse/cursor charts crowdsourcing 11 TurkerGaze: Crowdsourcing Saliency with Webcam... <ref type="bibr" target="#b70">[71]</ref> 2015 webcam natural crowdsourcing 12 Constructing Models of User and Task Characteristics... <ref type="bibr" target="#b15">[16]</ref> 2015 eyetracker charts lab experiment 13 SALICON: Saliency in Context <ref type="bibr" target="#b28">[29]</ref> 2015 mouse/cursor natural lab experiment 14 Predicting Confusion in Information Visualization... <ref type="bibr" target="#b39">[40]</ref> 2016 eyetracker charts lab experiment 15 Do graph readers prefer the graph type most suited to... <ref type="bibr" target="#b66">[67]</ref> 2016 eyetracker charts lab experiment 16 WebGazer: Scalable Webcam Eye Tracking Using User... <ref type="bibr" target="#b54">[55]</ref> 2016 webcam natural crowdsourcing 17 Eye Tracking for Everyone <ref type="bibr" target="#b36">[37]</ref> 2016 phone camera eye gaze crowdsourcing 18 Beyond Memorability: Visualization Recognition and... <ref type="bibr" target="#b2">[3]</ref> 2016 eyetracker charts lab experiment 19 Zone out no More: Mitigating Mind Wandering... <ref type="bibr" target="#b13">[14]</ref> 2017 eyetracker texts lab experiment 20 Learning Visual Importance for Graphic Designs and ... <ref type="bibr">[</ref> gaze logs to train a state-of-the-art convolutional neural network-based model <ref type="bibr" target="#b57">[58]</ref> that predicts gaze heatmap when an image is given as an input. Based on the images created from Scanner Deeply, we present its qualitative features and evaluate our work using a model trained on the Salicon dataset and the DVS model. We also present a preprocessing method that effectively removes noisy gaze dots that improves the quality of webcam-based gaze dots. Our work demonstrates that gaze patterns on visualizations are task-and domain-specific. Contributions. To sum up, the contributions of our work are • Our Scanner Deeply pipeline, which collects large-scale chart images and uses webcam-based eyetracker, a low-technology requirement, to collect gaze dots and train the model on a neural network.</p><p>Our approach is based on a taxonomy of 30 prior works. • A preprocessing technique to effectively remove noise from raw eye movement data. • Qualitative and quantitative evaluations of our work compared with other gaze prediction models. • The disclosure of the dataset upon the acceptance of our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">OVERVIEW</head><p>We review works that study visual perception via human eye movements. As summarized in Table <ref type="table">1</ref>, diverse apparatus and techniques have been proposed to collect eye movements data. We taxonomize those works based on three axes: tracker technology, image stimulus, and collection methodology. We detail why some choices are not practically desirable given the recent research trends. We conclude this section with a discussion on our choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Understanding Visual Perception via Eye Movements</head><p>Research on understanding human perception using eye movement started more than three decades ago <ref type="bibr" target="#b25">[26]</ref>, and the methods for collecting eye movements have made advances over time. One form of early works that involve studies with eye movement data explore different human reactions and patterns as a lab study. Eye movement data are typically collected using eyetracking hardware. Examples of these experiments include understanding mind wandering patterns when reading text <ref type="bibr" target="#b58">[59]</ref>, how people react under tabular visualizations <ref type="bibr" target="#b34">[35]</ref>, and how people make sense of unfamiliar visualizations <ref type="bibr" target="#b42">[43]</ref>. Another form of early work aims at developing gaze prediction models. First introduced by Itti et al. <ref type="bibr" target="#b25">[26]</ref>, there has been many attempts to build models predicting gaze views of natural images, e.g., models using concepts from information theory to predict salience on natural images <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>As eyetracker started to gain more popularity in human perception research than the past, there has been growing demand for gaze prediction models with high quality. This brought light to numerous data-driven gaze prediction models <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b52">53]</ref> and also to large-scale gaze logs on natural image datasets <ref type="bibr" target="#b29">[30]</ref>. Two most commonly used image datasets for training and evaluating saliency models are (1) CAT2000 benchmark <ref type="bibr" target="#b7">[8]</ref> and (2) Salicon <ref type="bibr" target="#b28">[29]</ref>. CAT2000 contains 2,000 images from 20 different categories, and Salicon contains 10,000 images drawn from MS COCO <ref type="bibr" target="#b43">[44]</ref>. Salicon provides highly varied and natural images along with ground-truth fixation annotations. With rapidly developing deep learning techniques, the performance of saliency maps on the benchmark datasets have also rapidly ameliorated <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>. Furthermore, as crowdsourced platforms became available, various methods have been developed to democratize the eyetracking process, such as cursor-based eyetrackers <ref type="bibr" target="#b32">[33]</ref>, or webcam-based eyetrackers <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b70">71]</ref>.</p><p>Initial work in the visualization community on gaze prediction builds models using natural image datasets <ref type="bibr" target="#b18">[19]</ref>. Inspired by the works showing that humans focus on texts while analyzing charts, Matzen et al.</p><p>proposed DVS <ref type="bibr" target="#b49">[50]</ref>, a gaze prediction model that utilizes a linear combination of a model based on natural images <ref type="bibr" target="#b21">[22]</ref> and text optimizers <ref type="bibr" target="#b49">[50]</ref>. However, it has been unclear whether those models based on natural images are effective on visualization images. In evaluation ( §5.1), we Fig. <ref type="figure">2</ref>: Calibration and gaze collection. Participants click on 8 dots that are placed near the boundaries of the screen. Each dot turns yellow once the user has clicked on it five times (A-1). Then, the calibration accuracy is measured by making users look at the middle dot at the center of the screen (A-2). If the accuracy is above 70%, then participants can proceed to the gaze collection phase (A-3). In the gaze collection phase, participants are asked to look at the chart for 7 seconds, during which time the user's gaze is tracked (B-1). Then, they are asked to answer what type of chart the image is (B-2). After they submit the answer, participants look at the next image by clicking "Go" button at the question "Ready for the next image?" We customize the code WebGazer developed by Papoutsaki et al. <ref type="bibr" target="#b54">[55]</ref> in creating the website.</p><p>show that such a model is less effective than our models trained on a visualization dataset, which necessitates developing a task-specific-i.e., focusing on data visualization-data corpus. It also has been unknown whether neural networks that improve the performance of gaze prediction tasks in natural image domains, are effective for visualization images, especially for charts. Our work aims at addressing this knowledge gap by collecting a large-scale gaze dataset of charts and training neural networks for gaze prediction on the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Choice of Apparatus and Techniques in Prior Work</head><p>Building gaze prediction models requires a corpus of eye movement data. Prior work therefore utilizes diverse apparatus and techniques for collecting datasets. Here, we review 30 prior works and assess the advantages and disadvantages of their collection methods. We collect those papers from visualization (IEEE TVCG, IEEE VIS, etc.), computer vision (IEEE CVPR, IEEE ICCV, etc.), and human-computer interaction (ACM CHI, ACM UIST, etc.). We consider studies comprehensively, i.e., they deal with eye movements on visualizations, natural images, texts, and webpages. We then evaluate their choices based on three axes: (1) the tracker that is used to track eye movement of a participant, (2) the image stimulus, or the type of images that is shown to participants, and (3) the collection methodology that is used to gather the data from participants. We have listed those works in Table <ref type="table">1</ref>.</p><p>Eyetracker. Understanding one's eye movement patterns provide us with various information about the attention the person is focusing on. For example, the fixation data about an image play as indicators of important regions within the image. In the field of visualization, by knowing which part of the chart would people's attention be most focused, the visualization designer can slightly change her design to better meet her intentions. There exist three types of methods that are used for eyetracking: specialized eyetracking hardware, indirect measurement methods, and general-purpose webcams.</p><p>To begin with, the benefit of eyetracking hardware is that it provides an accurate measurement of one's eye gaze. As a result, it has been extensively used in lab experiments. However, the quality of gaze dots are largely dependent on their prices. The performance of some of the inexpensive ones is somewhat questionable, and high-end eyetrackers can cost upwards of $20,000. For this reason, and also because of the difficulty in recruiting participants for laboratory sessions, experiments involving eyetracking devices are generally known to be expensive.</p><p>As an alternative, there have been attempts to collect gaze information without directly tracking the user's eyes. One approach is the cursor-based gaze collection technique. This is based on the assumption that the movement of cursor is correlated with the eye movement when looking at a screen <ref type="bibr" target="#b23">[24]</ref>-or at least that participants can reliably be asked to use the mouse cursor in this way. Cursor-based eyetrackers are effective in conveying the person's location of visual attention <ref type="bibr" target="#b23">[24]</ref>. Because of this characteristics, various research collects gaze dots using this approach <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>. However, cursor-based approaches do have limits and are not a completely accurate replacement for eyetracking devices. It loses some of its information, because it cannot track the person's eye when she is moving her eyes without moving the cursor.</p><p>There is another alternative that is being studied-the use of generalpurpose webcams as an eyetracking method. However, webcams have one infamous hurdle that needs to be addressed: consistent calibration of the eye to the screen. This is mainly because calibration in webcams are very sensitive to even small head motions. Jiang et al. <ref type="bibr" target="#b28">[29]</ref> argue that collecting large-scale dataset via general-purpose webcams is not possible, especially in an uncontrolled setting. Because of this, with current technology, gaze collection can only be maintained for a short period of time after calibration <ref type="bibr" target="#b70">[71]</ref>. Furthermore, although it is still imperfect technology, we think that it is by far the best method to measure directly one's eyes, affordably, and still yield large-size data.</p><p>Image Stimulus. Two types of data can be used to predict gaze for visualization studies: visualization images, and natural images.</p><p>For the former, stimulus is often confined to a particular type of chart (e.g., node-link diagrams, parallel coordinate diagrams, etc.), and the size of the data corpus is typically small, not exceeding 300 images. Recently, a large-scale visualization dataset, called the MASSVIS dataset <ref type="bibr" target="#b6">[7]</ref>, has been developed. The dataset contains more than 5,000 different types of static visualizations from four topics: government, infographic, news, and science.</p><p>Natural image datasets are also referenced to imitate gaze dots about chart images. For example, despite being collected on a different domain of images and not intended to be used for visualization images, the reaction to low-level features (e.g., color, contrast, motion, etc.) of an image in natural image datasets and chart images is similar on certain tasks (e.g., exploration tasks) <ref type="bibr" target="#b55">[56]</ref>. Furthermore, since early large-scale gaze datasets are collected using natural image datasets as the stimulus, their results have often been deployed in predicting saliency in visualizations.</p><p>Collection Methodology. There are two main choices for collection methodology: lab experiment and crowdsourcing.</p><p>Lab experiments are optimal for collecting small-scale but highquality eye movement data. As mentioned before, these lab experiments are mainly done using eyetracking hardware. The accuracy and quality of eyetracking dots are high as it deploys a specialized eyetracker and the experiment can be fully controlled. However, collecting gaze dots via lab experiments is expensive, impractical and time-consuming to involve a large number of participants.</p><p>Crowdsourced platforms such as Amazon Mechanical Turk (AMT) or Innocentive are beneficial in that it is easier to recruit at a relatively affordable price. Consequently, many researchers use crowdsourced platforms to obtain human-involved datasets in a large scale. However, existing technologies for tracking eye movements are not as accurate as real eyetracking hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Our Approach</head><p>Based on our assessments, the most desirable approach is to develop a gaze prediction model that provides the highest accuracy, but at the same time using the least possible resources (i.e., reducing time and cost). Neural network models trained on large-scale natural image datasets achieves superior performance over other methods. We therefore aim to apply this approach to visualization in order to construct a gaze prediction model that we call a SCANNER DEEPLY.</p><p>To do this, we choose to gather chart images as our image stimulus, as we hypothesize this will lead to a more specialized model with better performance for chart input. For the collection methodology, we choose to use a crowdsourced platform to be able to collect sufficiently large number of data samples for training neural network models. To reduce the confusion of a model, we only collect eyetracking data for images that contain a single static chart.</p><p>We set a task that can provide answers on the generability of tasks. Matzen et al. <ref type="bibr" target="#b49">[50]</ref>, while describing the DVS, mention the possibility of a general-purpose gaze prediction model. However, several researchers provide empirical evidence that gaze patterns are task-specific. For example, Yarbus' <ref type="bibr" target="#b71">[72]</ref> and Michal and Franconeri's <ref type="bibr" target="#b50">[51]</ref> works on gaze research suggest that human attentions are guided by the task she is conducting. Prior work <ref type="bibr" target="#b49">[50]</ref> showed that the ideal visualization has a strong overlap between the regions (1) that are most likely to draw the viewer's attention (bottom-up) and ( <ref type="formula">2</ref>) the regions that convey important information (top-down) about a task. We choose a task that satisfies both conditions and can also be evaluated with a gaze dataset collected in a short time (7 seconds). Specifically, we ask participants to figure what type of chart it is after they view the chart for 7 seconds.</p><p>Another question remains-it is not clear how large the image stimulus dataset should be to yield a sufficient amount of eye movement data. To the best of our knowledge, the largest known visualization dataset is the MASSVIS dataset <ref type="bibr" target="#b6">[7]</ref> which contains 5,000 static visualization images. However, the dataset is not suitable for our task as images in the infographics category have more than two charts per image, and the subset includes images far smaller than 5,000. For comparison, the Salicon dataset <ref type="bibr" target="#b28">[29]</ref> has gaze annotations for 10,000 images, while they are natural images. It means the amount of images in MASSVIS is not sufficient for training neural networks. To that end, we decide to create our own image dataset that contains 10,000 images.</p><p>For the tracker, we choose a low-technology requirement consistent with our crowdsourced platform. Between cursor-based method and webcams, we choose to go with webcams. The reason is two-fold. First, webcams are the only method to collect gaze dots that are actually directly measured from one's eyes in a crowdsourced platform. The and verified in the literature. This may be because of the intrinsic issue that there may be noise in gaze dots collected from webcams and that consequently the result is unpredictable. However, if we can devise a method that can successfully ease the impact of noise and successfully train the model, then this can also lead to another contribution. Given the gaze dataset obtained from crowdsourcing, we train Sim-pleNet, a neural network devised by Reddy et al. <ref type="bibr" target="#b57">[58]</ref>. SimpleNet is a state-of-the-art saliency map generating model that utilizes convolutional neural network architectures (e.g., ResNets <ref type="bibr" target="#b22">[23]</ref>), to generate gaze heatmap. It requires less computational resources to train or predict than existing networks designed for gaze prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TASK-SPECIFIC CROWDSOURCED DATA COLLECTION</head><p>Here, we delineate the steps taken to collect task-specific gaze dots using webcams on a crowdsourced platform. We first describe how we crawled chart images from the web. Then we describe the processes taken to conduct the crowdsourced study on Mechanical Turk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Visualization Image Collection</head><p>The goal in collecting chart images is to collect a dataset that aligns with the distribution of charts that can be found in the web, so that the trained model becomes as versatile as possible. To collect crowdsourced eyetracking data at scale, we first needed a large-scale dataset of visualization images. While the large-scale MASSVIS dataset contains 5,000 images, this may not be sufficient for training a neural network. Furthermore, the dataset also contains images containing multiple individual charts. We base our scale on the Salicon dataset, which contains 10,000 gaze dots. Accordingly, we choose 10,000 images as our target quantity.</p><p>We gather images by querying Google Image<ref type="foot" target="#foot_0">1</ref> using three keywords each time, where two keywords are chosen from 379 keywords derived from the topical analysis <ref type="bibr" target="#b0">[1]</ref> of papers in scientific communities over the past 10 years (2011-2020), and the last keyword stands for the type of charts (e.g., line chart, bar chart, or heatmap). Examples of these keywords are shown in Table <ref type="table" target="#tab_3">2</ref>. Through a series of search queries, we gathered 280,000 images from the web. These images contain not only charts, but also natural images pertaining to the topic. We only kept the images that (1) have one chart in an image, (2) both width and height of an image are larger than 400 pixels, (3) have heights or widths less than 4 times of the other, and (4) are without texts whose sizes are unreadably small. We also removed all duplicates.</p><p>In the end, we were able to retrieve 10,960 chart images. To roughly identify the distribution of charts, we randomly sampled 1,000 of the images, and counted them by their chart types. Fig. <ref type="figure">3</ref> shows the 10 most frequent chart types in a random sample of 1,000 images in our Fig. <ref type="figure">3</ref>: Chart type popularity. We show the 10 most popular charts from a set of 1,000 samples randomly chosen from our image collection. This provides an estimate on the chart type distribution in our dataset. collection. We can observe that bar charts, line charts, and pie charts are the three most commonly used graphs on the web.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Gaze Collection Setup</head><p>We collect eyetracking data using crowdsourcing on AMT. We design the procedure with three goals in mind: (1) obtaining high quality gaze dots, (2) obtaining large dataset, and (3) lessening the burden of participants. The task involved participants identifying the kind of chart after looking at the image for a few seconds; while they did so, we tracked their gaze using a webcam eyetracker.</p><p>Eyetracking Mechanism We created our experimental platform with the help of WebGazer, an eyetracking technique developed by Papoutsaki et al. <ref type="bibr" target="#b54">[55]</ref>. Prior to finalizing our experiment environment, we conducted pilot studies with members of our research group at the University of Maryland to find the optimal settings. We identified two issues in deploying general-purpose webcams as eyetrackers: (1) the sensitivity and accuracy for webcam eyetrackers, and (2) maintaining a high level of concentration from participants.</p><p>Calibration in webcams is sensitive to slight movements of the head. Even state-of-the-art webcam eyetrackers exhibit poor robustness against changing head posture. This is exacerbated when testing in the wild, i.e., using a crowdsourcing platform on the internet. During their experiment with webcams, Xu et al. <ref type="bibr" target="#b70">[71]</ref> note that eyetracking requires frequent calibration, and to minimize the number, the viewing time must be short. To alleviate this problem, we limit our eyetracked tasks to 7 seconds per image and conduct calibration every 6 images.</p><p>Secondly, to check if participants are concentrating, we add attention trials throughout the experiment to determine if participants are concentrating. We select 400 attention images. These attention images are all bar charts given the assumption that all participants know what a bar chart is, and hence can answer questions without difficulty. Furthermore, as Figure <ref type="figure">3</ref> shows, the bar chart is also the most common visual representation in our dataset. If the user does not correctly answer the attention trial, then we stop the participant from further proceeding with the experiment. This fact-that participants remain focused on the trials-is clearly communicated at the outset of the experiment.</p><p>Participants. We recruit participants who are fluent in English, do not have color blindness or any other kind of color vision deficiency, and are at least 18 years old. We let participants be aware that the experiment is about charts. Furthermore, we ask them to use computing device with a webcam, have monitors with a screen resolution or 1280× 720 or higher, and use Google Chrome or Microsoft Edge web browsers (these requirements are imposed by our eyetracking software).</p><p>Apparatus. We build a website for collecting data while participants perform pre-defined tasks. While they are doing so, we track their eye movements. We use JavaScript and jQuery to build our website and use Python Flask v1.1.1 to run it on our university-hosted virtual machine (VM). We allocate 2 GB RAM and 32 GB storage to the VM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Participant Task</head><p>Below we describe steps taken for collecting gaze dots from participants on a crowdsourced platform.</p><p>Getting Consent. Participants are first presented with a consent form, including a brief introduction to what the study is about and how the study is run. Afterwards, we lead participants to a website hosted on our server. The server was created by the authors and is run on a Linux-based virtualization environment provided by the department of Computer Science at the University of Maryland.</p><p>Calibration. Prior to conducting the experiment we provide several suggestions to participants to help them get past the accuracy threshold during the calibration phase: (1) conducting the experiment in a well-lit environment, (2) situating their heads within the green box shown on the camera view, and (3) trying not to change their head posture during the experiment, as it will decrease the calibration accuracy.</p><p>After participants allow access to the camera, the calibration can proceed. Fig. <ref type="figure">2</ref> (A) illustrates the steps required for calibration. Calibration takes place in the following manner: Participants are asked to hold their head steady and place their head on the box shown in the camera view. Then, they are asked to synchronously look at and click at 8 dots placed near the margins of the browser five times each. The gaze dot, or the estimated gaze location of the participant, is represented by a small moving red-colored dot. Every time a target button is clicked, the opacity of it gets lower until it eventually turns into yellow to signal that it has been fully clicked 5 times. These clicks adjust the calibration between the participant's eyes and the browser window. After all dots turn yellow, a new red-colored button shows up at the center of the browser to measure the accuracy of the calibration. The participants are required to look at the dot until it turns yellow. Accuracy is measured by the proportion of gaze dots that are placed near the center and those that are not. As described in the previous part, if that proportion is higher than 70%, then participants can proceed to the gaze collection phase. If the accuracy is below 70%, then participants must repeat the process until the accuracy rises above the threshold.</p><p>Data Collection. The next step is where the gaze log collection process starts. Fig. <ref type="figure">2 (B</ref>) explains the steps taken to collect gaze logs. Once participants click the button "Go" from the question "Are you ready?," an image appears at the center of the screen. The image appears for 7 seconds. During the 7-second period, gaze logs are collected at a refresh rate of 20 Hz. After 7 seconds have passed, the screen changes into a 20-multiple choice question that polls the chart type. The participants can choose to answer between the choices, select "I don't know," or provide a new answer after choosing "other."</p><p>As stated before, we compose one unit block as one calibration followed by 6 gaze trials. The intention is to keep one human intelligence task (HIT) under 3 minutes, including the calibration. From our pilot study, collecting gaze logs from a full block averaged around 1 minute and 10 seconds. Among the 6 trials in a block, one is an attention trial and 5 are used for the experiment. After each HIT is complete, we collect a packet from the server that contains information on (1) the gaze log information, (2) width, height, and size of the browser (e.g., the exact displacement of the image, the size of the screen, etc.), (3) answers to the questions asked, and (4) the name of the image. We do not capture nor reference videos from the webcam; the latter is clearly stated to participants prior to participating in the experiment.</p><p>One HIT is composed of three blocks. Based on our pilot study, it took approximately 3 minutes and 33 seconds to complete one HIT. Based on the targeted rate of $15 per hour upon successful completion of the experiment, we pay each participant $0.90 per HIT. For those that perform more than one HIT, we compensate them in the form of bonuses, at the rate of $0.30 per block. We gave compensation to any participants that conducted the experiment even if only partially. We paid participants no more than five days after the date of submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Collection Results</head><p>For the 10,960 images, we ran this collection process until we had at least one annotation per image, resulting in 12,504 gaze dots. It took 60 days to collect the data. We exclude the gaze dots from participants Fig. <ref type="figure">4</ref>: Illustration of procedures for constructing oracle heatmaps. We use the final heatmaps for training our neural networks. Gaze dots collected by using webcams contain rich information that enables further analysis, e.g., temporal changes in human visual perception (see §5.4 for the analyses), but they are also noisy to learn. We address this challenge by carefully pre-processing gaze dots. The second column shows the dots in the raw data from webcams, and we blur dots and remove some of them not in the human's area of focus (see §4.2 for details). who submitted incorrect identification numbers. The dataset consists of gaze logs from 9,157 correct responses and 3,347 incorrect responses. Examples of gaze logs collected from our study are shown in Fig. <ref type="figure">4</ref>. The success rate of each HIT task was 71.2%. The unsuccessful HITs are due to (1) incompleted tasks (21.3%) and (2) false answers (7.5%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">A SCANNER DEEPLY</head><p>We now propose a SCANNER DEEPLY, a virtual eyetracker that utilizes a deep neural network (DNN) for gaze prediction. Scanner Deeply takes a visualization image as an input and automatically generates a gaze heatmap for the image. We first discuss our choice of a DNN: we employ SimpleNet <ref type="bibr" target="#b57">[58]</ref>, a compact DNN architecture designed for gaze prediction. We then describe how we preprocess our dataset to prepare the training data, i.e., data filtration and saliency map generation. We finally describe how we train SimpleNet on the preprocessed data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">SimpleNet: A DNN for Gaze Prediction</head><p>We have two criteria for choosing a DNN architecture. First, the architecture should be fast and computationally efficient at inference time. Most DNN architectures that are used in prior work <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b68">69]</ref> for gaze prediction are complex, i.e., they contain millions of model parameters, which increases the operational costs. To run inferences with those models, we require special hardware (e.g., GPUs or hardware accelerators). Reducing a DNN's post-training operations thus allows users and practitioners to deploy the Scanner Deeply to diverse computing environments, ranging from servers and personal computers to devices with limited computational resources, e.g., IoT or mobile devices. Second, while reducing the costs of post-training operations, we choose the architecture that provides state-of-the-art performance in gaze prediction tasks. Deep and complex architectures <ref type="bibr" target="#b44">[45]</ref> typically offer better performance. However, we aim to find a shallow, simpler network that can achieve on-par performance. Considering the criteria, we employ SimpleNet to implement the Scanner Deeply.</p><p>Figure <ref type="figure">5</ref> illustrates the SimpleNet architecture adapted for our pipeline. SimpleNet employs an encoder-decoder architecture. The encoder extracts latent representations (often referred to as features) from an input, and the decoder reconstructs the input from the latent vectors. In the figure, SimpleNet extracts a 2048-dimensional vector from a visualization image as a latent representation. From this vector, the decoder architecture generates a gaze map. Multiple image classification models, such as VGGNet <ref type="bibr" target="#b64">[65]</ref>, ResNet <ref type="bibr" target="#b22">[23]</ref>, and PNASNet <ref type="bibr" target="#b45">[46]</ref>, can be used as an encoder; we choose ResNet because it offers the highest performance. The decoder is composed of two deconvolutional layers. SimpleNet utilizes the U-Net structure <ref type="bibr" target="#b61">[62]</ref>, which helps to improve the performance further by incorporating the information from earlier layers when the decoder reconstructs a gaze map. Fig. <ref type="figure">5</ref>: Our SimpleNet architecture. The pipeline accepts image input and produces an output image. In our case, the input is a visualization and the output is the gaze map. SimpleNet follows an encoder-decoder structure similar to U-Net <ref type="bibr" target="#b61">[62]</ref>. Compared to DNNs from prior work <ref type="bibr" target="#b28">[29]</ref>, SimpleNet requires less computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Preprocessing Webcam-based Eyetracking Data</head><p>Even with our efforts to maintain a high level of calibration during gaze collection process, the resulting gaze dots from webcam-based eyetrackers contain noise. Noise is mainly attributed to the dispersion of gaze dots. Such dispersion happens as participants make diminutive movements of their heads, or because poor or shifting light conditions introduce errors in the eyetracking software. At worst, noisy ones exist in locations far from where the chart is located within an image, and they prevent the DNNs from learning patterns in the data. (see Table <ref type="table">3</ref>)</p><p>To address this issue, we filter out gaze dots not in a saliency zone. Following the prior work <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b59">60]</ref>, we define the saliency zone as areas in an image that are highly disparate in RGB values in contrast to its neighboring pixels. We preprocess gaze dots as follows: At each pixel location, we first compute color differences between the location and all the neighboring pixels within a 5-pixel radius. We perform this step over all the pixels in an image and remove gaze dots at each pixel location where the sum of the color differences is less than 10 (10 is the threshold we empirically find). We then blur the fixations by using a Gaussian filter that has a standard deviation of 5 and set the boundaries of the salient zone. In Fig. <ref type="figure">4</ref>, we show examples of our preprocessed gaze dots. We exclude 160 images that this filtration process removes all the gaze dots from our dataset, leading to 12,344 gaze maps, consisting of 9,040 correct and 3,304 are incorrect ones. Note that our preprocessing does not change the input resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training SimpleNet on Our Crowdsourced Dataset</head><p>We implement Scanner Deeply with Python 3.9 and PyTorch v1.10. <ref type="foot" target="#foot_1">2</ref>To train our SimpleNet models, we use a machine equipped with Intel i7-6700K CPU with 32GB RAM and 2 NVIDIA GTX 1080 Ti GPUs. Note that, once trained, we only use CPUs to offer gaze prediction.</p><p>Datasets. We compose our dataset as follows: Among the 9,040 correctly-responded visualization images, we randomly pick 70% of them as our training data and use the rest 30% as the testing set. This split leads to 6,328 training and 2,712 testing images. Next, we pair each visualization image with a gaze map preprocessed by the method described in §4.2. Those maps are the oracles the model should generate. All the visualization images and saliency maps are scaled to 256×256. Our model produces gaze predictions with the same size, and we re-scale them back to the original resolutions for visualization.</p><p>Objective Function. We train SimpleNet to minimize the perceptual difference between gaze predictions and the oracle saliency maps. To measure the difference, we employ the Kullback-Leibler (KL) divergence, an objective function commonly-used in literature <ref type="bibr" target="#b16">[17]</ref>. We also examine other metrics for measuring perceptual differences, e.g., p -distances or normalized scanpath saliency (NSS) <ref type="bibr" target="#b41">[42]</ref>, proposed by prior work <ref type="bibr" target="#b60">[61]</ref>. While our model minimizes those metrics, we observe in our manual analysis that gaze predictions generated with KL divergence are better than the cases of using others.</p><p>Hyper-parameters. We use the Adam <ref type="bibr" target="#b35">[36]</ref> optimizer to train our models. We set the batch size to 32 and the learning rate to 10 −4 . We also set the weight decay to 10 −4 . We train our models for 40 epochs; at each epoch, we compute the KL divergence of our model the testing set and store the one that minimizes the metric over the 40.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION</head><p>Here, we evaluate the Scanner Deeply. We first show the prediction performance of the Scanner Deeply and the baselines ( §5.1). We then compare the gaze predictions qualitatively to illustrate unique characteristics the Scanner Deeply captures ( §5.2 and §5.3). We lastly show the benefit of using webcam-based eyetackers by analyzing temporal changes in human visual perception with the Scanner Deeply ( §5.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Prediction Performance of the Scanner Deeply</head><p>We evaluate the performance of the Scanner Deeply. The purpose of this experiment is to show that for predicting gaze heatmaps for a specific task, it is desirable to train models on a task-specific dataset. To this end, we train (1) SimpleNet on Salicon <ref type="bibr" target="#b28">[29]</ref>, gaze maps for a set of natural images and (2) use the DVS model <ref type="bibr" target="#b49">[50]</ref> <ref type="foot" target="#foot_2">3</ref> designed to perform generally well on predicting gaze maps for visualizations. We employ five metrics for measuring perceptual similarity between gaze predictions and gaze maps on our 2,712 testing images: two location-based metrics, i.e., AUC-Judd (AUC-J) <ref type="bibr" target="#b29">[30]</ref> and NSS, and three similarity-based metrics, i.e., KL-Div, SIM, and CC <ref type="bibr" target="#b8">[9]</ref>. Except for KL-Div, the higher a metric is, the more gaze predictions are similar to oracles. Table <ref type="table">3</ref> shows our results.</p><p>Scanner Deeply shows better performance over the two baselines. We first observe that the Scanner Deeply exhibits 5-15% higher performance than SimpleNet trained on the Salicon dataset. Considering that the training dataset is the only difference between the two models, it is important to use the dataset collected from the same domain (i.e., visualization) for high-quality gaze prediction. Compared with the DVS model, we find that the Scanner Deeply achieves 8-18% improvements. This result implies that within the same domain, a model trained on a task-dependent dataset can perform better on a specific task than a model built for a general-purpose prediction.</p><p>We also assess the impact of the preprocessing step on the performance of the Scanner Deeply (Ours vs. Ours † ). Across the board, the similarity metrics from the models trained on preprocessed gaze maps are better than those trained on un-preprocessed maps. This confirms our hypothesis that filtering out gaze dots in the image area with no stimulus helps a model focus more on the important low-level features. We find that the improvements are larger for the location-based similarity metrics (i.e., AUC-J and NSS), which implies that preprocessing encourages a model to ignore the unimportant parts in visualization.</p><p>Table <ref type="table">3</ref>: Performance evaluation of Scanner Deeply by comparing it with gaze prediction models proposed by prior work. We show the performance using five metrics. We report each metric's mean and standard deviation over the five runs. The value shown on top is the mean, and on the bracket below is the standard deviation over the five runs. We compare with DVS <ref type="bibr" target="#b49">[50]</ref> and Salicon <ref type="bibr" target="#b28">[29]</ref>. The only difference between Salicon and the Scanner Deeply is in the dataset. We show that a model using datasets collected for the specific task and domain on a specific task can show improved performance over a general-purpose gaze prediction model. We also compare ours with Scanner Deeply before pre-processing. This is represented as Ours † .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">How Does the Scanner Deeply Perceive Charts?</head><p>We now present unique characteristics of how the Scanner Deeply perceives visualization images compared with our baselines. We make this comparison by analyzing how those three models perceive three different stimuli within an image: (1) charts (visualized data information), (2) textual information, and (3) low-level features <ref type="bibr" target="#b55">[56]</ref>. In Fig. <ref type="figure" target="#fig_0">6</ref>, we illustrate gaze predictions generated by three models for four images.</p><p>Chart Perception. A predominant characteristic of gaze predictions produced by the Scanner Deeply is that it focuses on the shape (or structures) of charts within images, while the baseline models focus more on the texts. Fig. <ref type="figure" target="#fig_0">6</ref> shows that gaze predictions from DVS and SimpleNet models, trained with Salicon, highlight legends, numbers, or titles. In contrast, the Scanner Deeply focuses on a slice in the pie chart (A), peaks in the line chart (B), branching points in the sankey tree (C), and edges in the geographical maps (D). This means that the Fig. <ref type="figure">7</ref>: Gaze predictions from ScannerDeeply for different types of visualizations. We show examples of the six common chart types in our datasets, i.e., (from the left) a line chart, a bar chart, a population pyramid, a heatmap on a geographic map, a scatterplot on a geographic map, and a scatterplot. We analyze them in §5.3.</p><p>Scanner Deeply has the ability to emphasize different parts of images depending on the visualization tasks. The task here is to determine the type of a chart. It also means that in our data collection, participants have looked at the entire chart to perform the task, not focusing only on a specific part (see in §5.3 and 5.4 for our detailed analysis).</p><p>If we take a closer look at the gaze predictions from the DVS model (see the figures in the middle column), the model explicitly captures texts the most. The result implies that DVS is designed to capture both the low-level features (via the GBVS model) and texts (via text optimizers), but in fact, the text optimizers seem to be a dominant component. Surprisingly, from the figures in (B), DVS predicts that humans will mainly focus on reading the x-/y-axis. However, it is unlikely to be true as we use charts for reducing the perceptual complexity.</p><p>We further show that the model learned from natural image datasets (e.g., Salicon) focuses on low-level features the most. If the shape of the chart is easily noticeable with respect to its environment, the model focuses on the chart (see the figures in rows (A) and (C)); otherwise, it emphasizes other areas, e.g., texts. Here, being noticeable means how much contrast the chart is in terms of its color or shapes compared to the other parts of an image, which accounts for human perception of natural images. However, our results suggest that it might not be suitable for understanding human perception for a specific task.</p><p>Text Perception. Our findings are in stark contrast to the prior work <ref type="bibr" target="#b48">[49]</ref> showing humans mostly focus on textual information in a visualization. In Scanner Deeply, the concentration of gaze on text is somewhat similar to, or sometimes even weaker than that on the shape of the chart. We hypothesize that the importance of textual information in visualization may differ by the designated task. If the task is to read visualization closely and comprehensively, textural information will carry a lot of importance. But, the overall structure of visualization becomes more critical if the task is to classify visualization types. This finding is in line with the results presented by Polatsek et al. <ref type="bibr" target="#b55">[56]</ref>.</p><p>Oftentimes, the model trained on the SALICON dataset also produces gaze predictions that focus on textual information. However, we argue that it is because the saliency is largely determined by the level of contrast in the low-level features (e.g., colors) of the text with respect to the chart background. Note that we typically use distinct colors in texts for perceptual clarity. Thus, neural networks-that are good at capturing such input differences-will learn them during training and reflect the differences in the gaze predictions.</p><p>Perception of Low-level Features. Charts in images are generally drawn so that they can stand out from the backgrounds visually. As we discussed above, this is the main reason that all three models capture low-level features. But, there is a noticeable difference between the Scanner Deeply and the baseline models. Scanner Deeply utilizes different levels of prioritization in capturing low-level features.</p><p>Suppose our model only emphasizes the low-level features that are similar to the baselines (see gaze predictions conducted by SimpleNet with Salicon dataset in Fig. <ref type="figure" target="#fig_0">6</ref>). In (A), the prediction should focus on every component in the image with white color (as it has the highest contrast to the black background). However, our model focuses on a specific slide in the pie charts. It does not mean that we de-prioritize low-level features; the Scanner Deeply does prioritize low-level features when it is needed. In (C), humans need to focus on the texts at branching points to understand the sankey tree chart. In this case, our model emphasizes the texts in black, which contrasts with the yellow-colored background the most.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">What Structure Does the Scanner Deeply Focus?</head><p>We now analyze further the gaze predictions generated by the Scanner Deeply. We specifically focus on the structures (or shapes) our model primarily focuses on visualization images. We present examples for five most popular chart types: line chart, bar chart, pie chart, geographics map, and scatterplot. We show those examples in Fig. <ref type="figure">7</ref>.</p><p>Bar Charts. For bar charts in Fig. <ref type="figure">7</ref> (B), the model mainly focuses on the bars of the chart, though the gaze is focused on the center of the chart. This is not an unusual phenomenon, as chart images are generally located at the center of the image. Variants of bar charts also follow a similar pattern. For example, in population pyramids (see Fig. <ref type="figure">7 (C</ref>)), the gaze covers the entire shape of the chart.</p><p>Pie Charts. In Fig. <ref type="figure" target="#fig_0">6</ref> (A), the gaze prediction is on the center of the chart. It checks the angles of each pie and looks at the boundaries. Most pie charts have labels next to the pie, attracting the gaze. However, the concentration of gaze is usually less than that of the chart.</p><p>Line Charts. The model reads the line that passes through the chart for line charts. When the color of the line stands out from its environment, the saliency follows the line until it ends (see Fig. <ref type="figure" target="#fig_0">6 (B)</ref>). When there are multiple lines in the chart, it follows all of these lines. However, if the line is similar to the background color, then it often fails to follow the line. For example, when a line is drawn on a grid plane with similar color, it confuses distinguishing between the grid and the line, and it follows both the grid and the line.</p><p>Geographic Maps. We have two types of geographic maps: (1) maps containing textual information and (2) superimposed visual components. In the first one (in Fig. <ref type="figure">7 (D)</ref>), the gaze prediction focuses on the boundaries and texts with large font. In the second one (in Fig. <ref type="figure">7 (E)</ref>), the prediction focuses on parts of the map with more visual components than parts with fewer components, e.g., if there are dots on the map, the gaze is focused on where the concentration is high.</p><p>Scatterplots. Generally, in scatterplots (see Fig. <ref type="figure">7</ref> (F)), our model focuses on each dot, with the highest concentration of interest in areas with the highest concentration of dots. , we train our model on the gaze maps collected from the first 0.7 seconds, while we train our models on the gaze maps for the entire 7 seconds. Our model predicts that human eyes will focus first on the distinct areas in charts and then focus on text components (e.g., legends) later on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Analyzing Temporal Patterns in Human Perception</head><p>Our webcam-based data collection offers unique information over other datasets: we collected eye movements over time. We recorded eye movements every 0.05 seconds over 7 seconds in total, which allows us to analyze temporal patterns in human visual perception. Using this advantage, we evaluate an intriguing hypothesis: It has been known that humans first scan the overview of a visualization image and then perceive its details. To this end, we train our neural network on different datasets containing 0-t seconds, where t ∈ <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>Fig. <ref type="figure" target="#fig_1">8</ref> illustrates the gaze predictions for an image from our model trained with 0.7s period data (left) and the model trained with the full 7s data (right). We first observe that the Scanner Deeply, trained on the 0.7s data, looks at the center of the charts. We found two reasons: <ref type="bibr" target="#b0">(1)</ref> We made participants reset their eyes at the center; thus, they started by looking at the center. <ref type="bibr" target="#b1">(2)</ref> We also see that to understand the chart type (i.e., bar), participants do not need to look at other information.</p><p>However, when trained with the entire data, the Scanner Deeply starts focusing on other details, such as legends or texts in the x-axis where the highest bars are. This confirms our (and also a hypothesis well-known to the visualization community) that human perceives an overview first, and then details come. Our findings also raise questions about the community's practice in gaze predictions. Most prior work <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b49">50]</ref> offers a single gaze map for each image, which may lead to missing opportunities to understand human perception in more depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>In this section, we discuss the implications of our work in two ways:</p><p>(1) an assessment of our data collection methodology, (2) two possible approaches one can consider for gaze estimation. Finally, we conclude by discussing the limitations and plans for our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Assessing Crowdsourced, Webcam-based Gaze</head><p>We collected a large-scale dataset of gaze fixation points on visualization images using a crowdsourced platform through general-purpose webcams. There is clearly a benefit in terms of time and cost when using a crowdsourced platform than when conducting a lab study. We spent approximately $900 to collect around 12,000 gaze logs. This yielded approximately $0.07 to $0.08 per gaze log. This is about 40% more expensive than cost expectations using TurkEyes <ref type="bibr" target="#b51">[52]</ref>. However, one must take into account that our payment was based on a targeted rate of $15 per hour, whereas that of TurkEyes was $10.</p><p>There was an obstacle during the experiment-the time it takes to collect sufficient amount of data. While our initial expectation of collecting period was 30 days, it took more than 50 days to collect 10,000+ gaze logs. We attribute this to the fact that the task of collecting eye movement data on webcams is a challenge when using general-purpose webcams. Participants are required to conduct frequent calibrations, potentially multiple times, if they did not reach a particular threshold. We struggled to collect large-scale data at a fast, constant pace despite compensating at the relatively high rate of $15 per hour.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">A Priori or A Posteriori? A Better Method for Gaze Estimation</head><p>Our results show that gaze patterns on visualizations are task-specific and domain-specific. This goes in line with the experiment conducted by Polatsek et al. <ref type="bibr" target="#b55">[56]</ref>. In developing a gaze prediction model, they propose a gaze prediction model based on an a priori approach where the model is made as a combination of an image-and an object-based saliency model, similar to the DVS model that combines a model for natural images with a text identifier. In contrast, our work takes a posteriori approach that trains a neural network on empirical gaze data. Here, we discuss advantages and disadvantages for both approaches. Compared to a prior approach, our data-driven, a posteriori approach requires a large-scale dataset and substantial computing power to train a sophisticated neural network model. However, there is no guarantee that a priori-based model will provide solid performance in the general case. For simple low-level tasks, a priori-based models can predict gaze views as well as a data-driven a posteriori model without difficulty. Issues will arise when dealing with tasks that require a priori knowledge about human perception where no such knowledge exists. This may occur when the model encounters an entirely new type of chart not covered by existing perceptual models. It also becomes a problem when balancing between known factors is required. For example, while conventional knowledge suggests that people focus on textual information in a chart, the overview task in our experiment yielded more focus on chart shape than text. For both of these examples, a data-driven a posteriori model uses sheer computing power to learn from the large-scale dataset.</p><p>On the other hand, an a priori model for gaze prediction is built on operational and interpretable perceptual knowledge. Such knowledge can be informed by existing and ongoing research in perceptual psychology <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b58">59]</ref>. The neural network implementing our a posteriori model, in contrast, is opaque and not understandable to humans <ref type="bibr" target="#b19">[20]</ref>. Furthermore, new training datasets and models may be needed to cover all conceivable tasks, datasets, charts, and domains to guarantee robust performance for the deep learning method. Nevertheless, one important question we raise for future work is whether neural networks, such as our Scanner Deeply model presented in this paper, can teach us anything about perceptual psychology in humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Limitations and Future Work</head><p>As shown in our qualitative and quantitative experiments, gaze prediction models that are task-specific and domain-specific are desirable. Even if several works (e.g., Yarbus' <ref type="bibr" target="#b71">[72]</ref> and Michal and Franconeri's <ref type="bibr" target="#b50">[51]</ref>) suggest that the task guides human attention she is conducting, most of the recent work (shown in §2) on predicting gaze heatmaps focuses on collecting and developing models generally working well. That being said, building such models for individual tasks may require iterative efforts. However, we highlight that our work presented a method and desirable choices for the crowdsourced data collection approach in conjunction with the low-technology requirement.</p><p>We emphasize that our paper's datasets and methodology shed some light on interesting future work directions. As gaze prediction on visualization is an active area of study, we first envision extending our approach for various tasks as future work. We also want to highlight that the scalability issues in building prediction models for different tasks may inspire interesting future work. For example, one could build foundational models for gaze predictions, similar to models developed in computer vision (CLIP <ref type="bibr" target="#b56">[57]</ref> or GPT-3 <ref type="bibr" target="#b3">[4]</ref>), and then fine-tune those models for multiple downstream tasks. We further envision that exploring the feasibility of few-shot learning in this context to tackle data scarcity issues could be promising for future work.</p><p>Separately, in contrast to the datasets presented by the prior work, our dataset contains gaze dots collected over 7 seconds, which may reflect temporal human eye movements. We further envision future work using our dataset to conduct in-depth analysis of temporal eye movements to understand human perception better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HOMAGE</head><p>Once a guy stood all day shaking bugs from his hair. The doctor told him there were no bugs in his hair. After he had taken a shower for eight hours, standing under hot water hour after hour suffering the pain of the bugs, he got out and dried himself, and he still had bugs in his hair; in fact, he had bugs all over him. A month later he had bugs in his lungs.</p><p>-A Scanner Darkly (1977), Philip K. Dick )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DATA AVAILABILITY</head><p>Our crowdsourced dataset and source code for reproducing all of our experiments can be found in https://osf.io/spw49/?view_only= 7fde2fc5e51f4d6682805c1dbb7420f6.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Visualization of gaze predictions. We show the gaze predictions of three from ScannerDeeply, DVS, and SimpleNet trained on the Salicon dataset. DVS and Salicon focus more on text, while our model focuses more on distinct areas in a chart.</figDesc><graphic url="image-141.png" coords="7,314.17,418.80,132.20,62.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Temporal movements in gaze By training SimpleNet on datasets representing different time scale, ScannerDeeply can predict human visual perception over time. In (A), we train our model on the gaze maps collected from the first 0.7 seconds, while we train our models on the gaze maps for the entire 7 seconds. Our model predicts that human eyes will focus first on the distinct areas in charts and then focus on text components (e.g., legends) later on.</figDesc><graphic url="image-148.png" coords="8,313.85,527.16,99.24,73.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Example queries for collecting chart images from the web. Each query (on the right) is a combination of two keywords from our topical analysis of papers (on the left) and a chart type (in the middle). decision is not at all in manifesting the superiority of current webcam-based gaze collection over other methods. Our intention is just to reinforce that it is a choice worth studying because of the benefits we would gain if conducted successfully. Second, the use of webcams as the tracker, chart images as the stimulus on a crowdsourced platform is a choice that has not yet been investigated</figDesc><table><row><cell>Topic Keywords</cell><cell>Chart</cell><cell>Search Query</cell></row><row><cell>culture, anomaly</cell><cell>bar</cell><cell>culture anomaly bar chart</cell></row><row><cell>normal, politics</cell><cell>line</cell><cell>politics normal line chart</cell></row><row><cell>artist, history</cell><cell>pie</cell><cell>history artist pie chart</cell></row><row><cell>global, media</cell><cell>tree</cell><cell>global media tree diagram</cell></row><row><cell>meeting, crime</cell><cell>heatmap</cell><cell>meeting crime heatmap</cell></row><row><cell>sports, outlier</cell><cell>spider</cell><cell>sports outlier spider map</cell></row><row><cell>newyork, time</cell><cell>bubble</cell><cell>newyork time bubble chart</cell></row><row><cell>market, medical</cell><cell>scatter</cell><cell>market medical scatterplot</cell></row><row><cell>museum, weather</cell><cell>violin</cell><cell>museum weather violin</cell></row><row><cell>argument in this</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://images.google.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://pytorch.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://github.com/LauraMatzen/DVS</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank the anonymous reviewers for their constructive feedback. This work was partially supported by the U.S. National Science Foundation award IIS-1908605. Any opinions, findings, and conclusions expressed in this material are those of the authors and do not necessarily reflect the views of the funding agency.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno type="DOI">10.5555/944919.944937</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01">Jan. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Combined eye tracking and fMRI reveals neural basis of linguistic predictions during sentence comprehension</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Bonhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Friederici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Fiebach</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cortex.2015.04.011</idno>
	</analytic>
	<monogr>
		<title level="j">Cortex</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="33" to="47" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Beyond memorability: Visualization recognition and recall</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Borkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bainbridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2015.2467732</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="519" to="528" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
				<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Saliency based on information maximization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
				<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Evaluation of traditional, orthogonal, and radial tree diagrams by an eye tracking study</title>
		<author>
			<persName><forename type="first">M</forename><surname>Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Konevtsova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hoeferlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2011.193</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2440" to="2448" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Eye fixation metrics for large scale evaluation and comparison of information visualizations</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Borkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-47024-514</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Eye Tracking and Visualization</title>
				<editor>
			<persName><forename type="first">M</forename><surname>Burch</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Chuang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Fisher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Schmidt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</editor>
		<meeting>the Workshop on Eye Tracking and Visualization<address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="235" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<ptr target="http://saliency.mit.edu/" />
		<title level="m">MIT saliency benchmark</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">What do different evaluation metrics tell us about saliency models?</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2018.2815601</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="740" to="757" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning visual importance for graphic designs and data visualizations</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>O'donovan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Alsheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Madan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<idno type="DOI">10.1145/3126594.3126653</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Symposium on User Interface Software and Technology</title>
				<meeting>the ACM Symposium on User Interface Software and Technology<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="57" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Visual saliency model based on crowdsourcing eye tracking data and its application in visual design. Personal and Ubiquitous Computing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00779-020-01463-7</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Using eye-tracking in applied linguistics and second language research</title>
		<author>
			<persName><forename type="first">K</forename><surname>Conklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pellicer-Sánchez</surname></persName>
		</author>
		<idno type="DOI">10.1177/0267658316637401</idno>
	</analytic>
	<monogr>
		<title level="j">Second Language Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="453" to="467" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pushing the limits of saliency prediction models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cornia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName><surname>Sam</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPRW.2018.00250</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops<address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1971" to="19712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Zone out no more: Mitigating mind wandering during computerized reading</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>D'mello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bixler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bosch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Educational Data Mining. ERIC</title>
				<meeting>the International Conference on Educational Data Mining. ERIC</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Predicting visual importance across graphic design types</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fosco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Casser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Bedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>O'donovan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<idno type="DOI">10.1145/3379337.3415825</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Symposium on User Interface Software and Technology</title>
				<meeting>the ACM Symposium on User Interface Software and Technology<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Constructing models of user and task characteristics from eye gaze data for user-adaptive information highlighting</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gingerich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Conati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence<address><addrLine>Palo Alto, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1728" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An efficient image similarity measure based on approximations of KL-divergence between two Gaussian mixtures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2003.1238387</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="487" to="493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fauxvea: Crowdsourcing gaze location estimates for visualization analysis tasks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jianu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cabeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Laidlaw</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2016.2532331</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1042" to="1055" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modeling human comprehension of data visualizations</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Haass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Matzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Divis</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-39907-212</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Virtual, Augmented and Mixed Reality</title>
				<meeting>the International Conference on Virtual, Augmented and Mixed Reality<address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="125" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Evaluating &quot;graphical perception&quot; with CNNs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Haehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tompkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2018.2865138</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="641" to="650" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Eye-tracking research in autism spectrum disorder: What are we measuring and for what purposes?</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hamner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vivanti</surname></persName>
		</author>
		<idno type="DOI">10.1007/s40474-019-00158-w</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="37" to="44" />
		</imprint>
	</monogr>
	<note>Current Developmental Disorders Reports</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Graph-based visual saliency</title>
		<author>
			<persName><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
				<meeting>the Advances in Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">User see, user point: Gaze and cursor alignment in web search</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Buscher</surname></persName>
		</author>
		<idno type="DOI">10.1145/2207676.2208591</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Human Factors in Computing Systems</title>
				<meeting>the ACM Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1341" to="1350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Using eye tracking to investigate graph layout effects</title>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1109/APVIS.2007.329282</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Asia-Pacific Symposium on Visualization</title>
				<meeting>the International Asia-Pacific Symposium on Visualization<address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007-02">Feb 2007</date>
			<biblScope unit="page" from="97" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
		<idno type="DOI">10.1109/34.730558</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Commentary on section 4 -eye tracking in human-computer interaction and usability research: Ready to deliver the promises</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Karn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Mind&apos;s Eye</title>
				<meeting><address><addrLine>North-Holland, Amsterdam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="573" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Predicting affect from gaze data during interaction with an intelligent tutoring system</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jaques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Conati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Azevedo</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-07221-04</idno>
	</analytic>
	<monogr>
		<title level="m">Intelligent Tutoring Systems</title>
				<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">SALICON: Saliency in context</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298710</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1072" to="1080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to predict where humans look</title>
		<author>
			<persName><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2009.5459462</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision<address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2106" to="2113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scale saliency: a novel approach to salient feature and scale selection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brady</surname></persName>
		</author>
		<idno type="DOI">10.1049/cp:20030478</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Visual Information Engineering</title>
				<meeting>the International Conference on Visual Information Engineering</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="25" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Eye tracking assisted extraction of attentionally important objects from videos</title>
		<author>
			<persName><forename type="first">S</forename><surname>Karthikeyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thuyen</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298944</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3241" to="3250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">BubbleView: An interface for crowdsourcing image importance maps and tracking visual attention</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Borkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Z</forename><surname>Gajos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<idno type="DOI">10.1145/3131275</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer-Human Interaction</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A crowdsourced alternative to eye-tracking for visualization understanding</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Borkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Z</forename><surname>Gajos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<idno type="DOI">10.1145/2702613.2732934</idno>
	</analytic>
	<monogr>
		<title level="m">Extended Abstracts of the ACM Conference on Human Factors in Computing Systems</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1349" to="1354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Does an eye tracker tell the truth about visualizations?: Findings while investigating visualizations for decision making</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Upatising</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Yi</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2012.215</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2421" to="2430" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Eye tracking for everyone</title>
		<author>
			<persName><forename type="first">K</forename><surname>Krafka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kellnhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhandarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.239</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2176" to="2184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep Gaze I: Boosting saliency prediction with feature maps trained on ImageNet</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kümmerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
				<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Understanding low-and high-level contributions to fixation prediction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kümmerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S A</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.513</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision<address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4799" to="4808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Predicting confusion in information visualization from eye tracking and interaction data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lallé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Conati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carenini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence</title>
				<meeting>the International Joint Conference on Artificial Intelligence<address><addrLine>Palo Alto, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2529" to="2535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Gaze-driven links for magazine style narrative visualizations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lalle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Conati</surname></persName>
		</author>
		<idno type="DOI">10.1109/VIS47514.2020.00040</idno>
	</analytic>
	<monogr>
		<title level="m">Short Proceedings of the IEEE Visualization Conference</title>
				<meeting><address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Methods for comparing scanpaths and saliency maps: strengths and weaknesses</title>
		<author>
			<persName><forename type="first">O</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Meur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Baccino</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-012-0226-9</idno>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="251" to="266" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">How do people make sense of unfamiliar visualizations?: A grounded model of novice&apos;s information visualization sensemaking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-A</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Yi</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2015.2467195</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="499" to="508" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10602-148</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
				<meeting>the European Conference on Computer Vision<address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">DeepGaze IIE: Calibrated prediction in and out-of-domain for state-of-the-art saliency modeling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Linardos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kümmerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV48922.2021.01268</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
				<meeting>the IEEE/CVF International Conference on Computer Vision<address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01246-52</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
				<meeting>the European Conference on Computer Vision<address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="19" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Inferring human gaze from appearance via adaptive linear regression</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2011.6126237</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision<address><addrLine>NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Eye Tracking and Eye-Based Human-Computer Interaction</title>
		<author>
			<persName><forename type="first">P</forename><surname>Majaranta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bulling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="39" to="65" />
			<pubPlace>London, London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Patterns of attention: How data visualizations are read</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Matzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Haass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Divis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Stites</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-58628-115</idno>
	</analytic>
	<monogr>
		<title level="m">Augmented Cognition. Neurocognition and Machine Learning</title>
				<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="176" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Data visualization saliency model: A tool for evaluating abstract data visualizations</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Matzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Haass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Divis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2017.2743939</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="563" to="573" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Visual routines are associated with specific graph interpretations</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Michal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Franconeri</surname></persName>
		</author>
		<idno type="DOI">10.1186/s41235-017-0059-2</idno>
	</analytic>
	<monogr>
		<title level="m">Cognitive research: principles and implications</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">TurkEyes: A web-based toolbox for crowdsourcing attention data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcnamara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fosco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sukhum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<idno type="DOI">10.1145/3313831.3376799</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Human Factors in Computing Systems</title>
				<meeting>the ACM Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Shallow and deep convolutional networks for saliency prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sayrol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Giro-I-Nieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>O'connor</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.71</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="598" to="606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">SearchGazer: Webcam eye tracking for remote studies of web search</title>
		<author>
			<persName><forename type="first">A</forename><surname>Papoutsaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laskey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3020165.3020170</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Human Information Interaction and Retrieval</title>
				<meeting>the ACM Conference on Human Information Interaction and Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="17" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">WebGazer: Scalable webcam eye tracking using user interactions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Papoutsaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sangkloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laskey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Daskalova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence</title>
				<meeting>the International Joint Conference on Artificial Intelligence<address><addrLine>Palo Alto, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3839" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Exploring visual attention and saliency modeling for task-based visual analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Polatsek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Waldner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kapec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Benesova</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cag.2018.01.010</idno>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="26" to="38" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
				<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Tidying deep saliency prediction architectures</title>
		<author>
			<persName><forename type="first">N</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yarlagadda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gandhi</surname></persName>
		</author>
		<idno type="DOI">10.1109/IROS45743.2020.9341574</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
				<meeting>the IEEE/RSJ International Conference on Intelligent Robots and Systems<address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Eye movements during mindless reading</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Reichle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Reineberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Schooler</surname></persName>
		</author>
		<idno type="DOI">10.1177/0956797610378686</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1300" to="1310" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">An information maximization model of eye movements</title>
		<author>
			<persName><forename type="first">L</forename><surname>Renninger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Coughlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Verghese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
				<meeting>the Advances in Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Saliency and human fixations: State-of-the-art and study of comparison metrics</title>
		<author>
			<persName><forename type="first">N</forename><surname>Riche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Duvinage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mancas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gosselin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dutoit</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2013.147</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision<address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1153" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-428</idno>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention</title>
				<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Gaze-based object segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Ngan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/LSP.2017.2739200</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1493" to="1497" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Visual perception of parallel coordinate visualizations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Siirtola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laivo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Heimonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Räihä</surname></persName>
		</author>
		<idno type="DOI">10.1109/IV.2009.25</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Information Visualisation</title>
				<meeting>the IEEE International Conference on Information Visualisation<address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="3" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
				<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">User-adaptive information visualization: Using eye gaze data to infer visualization tasks and user cognitive abilities</title>
		<author>
			<persName><forename type="first">B</forename><surname>Steichen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Conati</surname></persName>
		</author>
		<idno type="DOI">10.1145/2449396.2449439</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Intelligent User Interfaces</title>
				<meeting>the ACM Conference on Intelligent User Interfaces<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="317" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Do graph readers prefer the graph type most suited to a given task? insights from eye tracking</title>
		<author>
			<persName><forename type="first">B</forename><surname>Strobel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Lindner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Köller</surname></persName>
		</author>
		<idno type="DOI">10.16910/jemr</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Eye Movement Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Saliency revisited: Analysis of mouse movements versus fixations</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Tavakoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laaksonen</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.673</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6354" to="6362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Large-scale optimization of hierarchical features for saliency prediction in natural images</title>
		<author>
			<persName><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2014.358</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the Conference on Computer Vision and Pattern Recognition<address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2798" to="2805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Revisiting video saliency: A large-scale benchmark and a new model</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00514</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition<address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4894" to="4903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.06755</idno>
		<title level="m">TurkerGaze: Crowdsourcing saliency with webcam based eye tracking</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Eye movements during perception of complex objects</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yarbus</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4899-5379-78</idno>
	</analytic>
	<monogr>
		<title level="m">Eye Movements and Vision</title>
				<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1967">1967</date>
			<biblScope unit="page" from="171" to="211" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
