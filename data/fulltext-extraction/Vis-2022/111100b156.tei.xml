<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Benjamin</forename><surname>Hoover</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Johanna</forename><surname>Beyer</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
						</author>
						<title level="a" type="main">Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-03-27T09:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Natural language processing</term>
					<term>language modeling</term>
					<term>zero-shot models</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. 1. Example of PromptIDE's interface to explore variations of different prompts. Each variation is tested against up to twenty data examples and represented as a template card (a). For each variation, rich detail can be tracked by using the detail stripes (b). If performance and qualitative detail are convincing, a user can collect the prompt in the prompt cart (c).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Machine learning models for natural language processing (NLP) have shown impressive results on benchmark tasks; however, translating this success from model architects into specific applications for model users H. <ref type="bibr">Strobelt</ref>  remains a challenge. One challenge is that benchmarks typically assume a supervised train-test workflow. The underlying task is carefully designed top-down with annotated training data. However, a significant portion of use-cases of NLP does not easily fit this workflow. For example, consider a journalist, covering legal proceedings, who is interested in finding all cited instances of a precedent <ref type="bibr" target="#b27">[28]</ref>, or a financial analyst looking through past company financial statements to find cases of debt obligations <ref type="bibr" target="#b6">[7]</ref>. Annotating enough data, training a model, and then applying it to their task requires time and expertise that may not be available for a user in this setting.</p><p>In recent years, an alternative bottom-up approach, known as prompting, has become popular for developing ad-hoc end-user tasks in NLP <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32]</ref>. To solve an ad-hoc task, the user provides, in nat-ural language, a prompt template that describes the task along with target answer choices. For example, simply providing a prompt template such "Is the case {case} referenced in this text? {text}" could alone provide a classification model for an ad-hoc task with no explicit train and test data needed. This approach is possible through advances in training large general-purpose models for language.</p><p>The promise of prompting is that it allows domain experts to solve new tasks with only natural language inputs. However, while there are prompts that can achieve high accuracy on specific tasks, there is a large amount of variance in the choice of the prompt template itself. Recent papers have described how task accuracy is dependent on specifics of prompt choices <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b44">45]</ref>. This leads to a brute-force procedure under which dozens of prompts are written, evaluated, and compared to find the best fit for a task. In this sense, prompting transfers similar burdens of curating expert labels to prompt construction.</p><p>This work explores how interactive visualization can support prompt construction for domain experts. Unlike aspects of model training, such as hyperparameter tuning, prompting is not constrained to brute-force exploration. Because prompt templates are written in natural language, users can craft and customize them for their tasks of interest and refine their answer choices based on the dataset. Outputs are legible to domain experts who can observe the process to stop it early or adjust it based on failures. They can rewrite prompts based on system observation to find the best expressions for their task. The tool is agnostic to the underlying model or datasets used but aims to support the expert in their goals.</p><p>This work makes the following novel contributions. (1) PromptIDE automates the creation and evaluation of many prompt templates simultaneously and supports different underlying models, tasks, and datasets.</p><p>(2) PromptIDE encourages a principled and repeatable workflow for prompt engineering. Users are guided through the process, with opportunities for iterations at each step. <ref type="bibr" target="#b2">(3)</ref> We demonstrate the utility of PromptIDE and our workflow for several real-world use cases for ad-hoc NLP models. We end the paper by exploring future challenges and avenues for follow-up work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Prompting as an Interface</head><p>The use of large language models as a replacement for supervised learning was popularized by the GPT series of language models <ref type="bibr" target="#b28">[29]</ref>. Prompting both in the zero-short and few-shot settings has been explored widely in NLP tasks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b42">43]</ref>. We consider only the case of human-readable prompts to large models, which contrasts with methods such as prompt-tuning <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20]</ref>, which learns a continuous prompt embedding, and auto-prompting <ref type="bibr" target="#b32">[33]</ref>, which attempts to search for prompts from scratch, both of which require a training step. For more examples see a recent survey by Liu et al. <ref type="bibr" target="#b20">[21]</ref>. Current prompt-based models are primarily based on Transformers <ref type="bibr" target="#b37">[38]</ref>, which has become the de-facto model architecture for NLP models; however, nothing about our visual analysis is specific to the model used or task considered, only the prompting interface.</p><p>Prompting as a means to interact with generative models is prevalent in online demos that often deploy Transformers as assistive agents. For instance, OpenAI released a (closed-source) API with a simple demo to interact with their proprietary GPT-3 model using text <ref type="bibr" target="#b24">[25]</ref>. 1  Another notable example is Github CoPilot <ref type="bibr" target="#b9">[10]</ref>, which uses OpenAI's Codex <ref type="bibr" target="#b3">[4]</ref>, a GPT-like language model trained on code, to allow everyday programmers to turn natural language prompts into code through an IDE like VSCode. With this model, a handful of well-crafted natural language instructions can code entirely functional browser-based video games <ref type="bibr" target="#b22">[23]</ref>. The flexibility and effectiveness of prompt-based approaches encourage the use of prompting as the preferred way to interact with powerful NLP models.</p><p>The flexibility of prompts as an interface also comes with a cost, as downstream performance is closely tied to prompt wording. Writing good prompts to extract the desired information from a model is usually 1 Many demos built around this API can be found at https://gptcrush. com/ left to trial and error by the user, with few exceptions. One tool that seeks to solve this problem is Prompts.ai <ref type="bibr" target="#b45">[46]</ref>, a visual tool that uses the GPT-3 API to explore how a user-specified prompt template affects the behavior of GPT-3 on individual examples, in conversations, and with different generation parameters. However, its input space is limiting, allowing only one prompt template with limiting syntax to be tested at a time. PromptSource <ref type="bibr" target="#b1">[2]</ref> increases the power of the templating language used to write general prompts. It also provides a platform for the community to create, evaluate, and explore new prompts. Promp-tIDE extends this work, providing a principled workflow to automate the time-intensive process of creating and evaluating many prompt templates on different models, datasets, and tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Visualization for NLP</head><p>Visualization tools to interact with language models have grown increasingly popular alongside the rise of the models themselves. These tools can serve several functions: (1) to expose the internals of a particular architecture (e.g., Transformers, RNNs, LSTMs) to understand how it encodes knowledge of the language <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>; <ref type="bibr" target="#b1">(2)</ref> to explore and compare the behavior of a model's internal distributions during text generation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b34">35]</ref>; and (3) to understand how model behavior differs under controlled input or parameter changes <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b35">36]</ref>. PromptIDE is agnostic to the underlying language model and thus serves the functions of (2) and <ref type="bibr" target="#b2">(3)</ref>.</p><p>Other visualization tools that treat NLP models as black boxes focus on visualizations of output distributions and model performance with single custom input or static sets of inputs. For example, GLTR <ref type="bibr" target="#b8">[9]</ref> focuses on visualizing output probability distributions to support humans in detecting whether a provided text was generated by a model. LMdiff <ref type="bibr" target="#b34">[35]</ref> extends these visualizations to support comparisons between different language models on user-provided inputs and static corpora. Neither of these provides an exploration of the input space. The Language Interpretability Tool (LIT) <ref type="bibr" target="#b35">[36]</ref> is a comprehensive toolkit that enables rapid exploration and error analysis for a model on a larger input dataset. However, LIT's comprehensive analyses are not conducive for rapid, iterative improvements of a prompt on general NLP tasks. NLIZE <ref type="bibr" target="#b21">[22]</ref> serves as a debugging tool that evaluates how a language model outputs changes as a result of perturbations to its hidden state rather than its input. Unlike these existing tools, PromptIDE enables exploration and evaluation of the infinitely large space of possible input prompts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MODEL: PROMPTING FOR NLP</head><p>Prompting is a paradigm for solving ad-hoc NLP tasks. We particularly focus on zero-shot prompting that assumes we do not have access to any training examples. Prompting is typically used in conjunction with large pre-trained language models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b28">29]</ref>. These models are powerful, but their size makes them difficult to train directly, which further encourages this style of zero-shot prompting.</p><p>Prompting assumes access to a large language model (LM) pretrained on generic text. An LM is a probabilistic model over text. Given an input text x, it gives the probability of output text y. The idea of the prompting technique has been facilitated by recent improvements in these models, primarily deriving from scaling Transformer neural networks <ref type="bibr" target="#b37">[38]</ref>. Recently, researchers have trained LMs that are directly targeted for the end-use of prompting <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b41">42]</ref>. These language models can all be queried in a standard way. In this work, we utilize three different LM queries (Fig. <ref type="figure" target="#fig_0">2</ref>):</p><p>Generation -Sample a target output for a given input, ỹ ∼ p(y | x).</p><p>Ranking -Compare the rank score of different texts, where f is a function based on p(y | x)( <ref type="bibr" target="#b2">[3]</ref>, details in Sect. 5.5),</p><formula xml:id="formula_0">f (y 1 , x) &lt; f (y 2 , x).</formula><p>Top-K -Find the k highest probability outputs from the model,</p><formula xml:id="formula_1">topk y ′ p(y = y ′ | x).</formula><p>We represent an NLP task as a table of examples, each associated with a fixed set of fields and a label. As a running example, we consider the task of document topic classification. For this task, there is one field, the article text, and one label, the topic of the article. The document text might consist of, Authorities have halted oil export flows from the main pipeline in southern Iraq after intelligence showed a rebel militia could strike infrastructure, an oil official said on Saturday... whereas the corresponding topic label would be World.</p><p>Contrast the standard ML approach for this task to prompting. A standard approach would collect supervised labels for the task and train a model on these examples. In zero-shot prompting, we do not have access to a training set. Prompting facilitates ad-hoc models by converting each test example directly to a form natural language input to which a large LM can respond. A user introduces a prompt template and a set of answer choices. The prompt template describes how to map the example fields to an input string x, whereas the answer choices describe how to convert potentials outputs y back to labels for the task.</p><p>We follow the work of PromptSource <ref type="bibr" target="#b1">[2]</ref>, where researchers introduced a format for describing prompt templates.</p><p>In which section of a newspaper would the text appear? {document} with answer choices given as a dictionary of labels:</p><p>[World, Sports, Business, Science and Technology].</p><p>Utilizing the prompt template, we can construct an example prompt x for model conditioning and answer choices y 1 , . . . , y n . Each of the choices can then be ranked under the model to provide an evaluation score for the dataset.</p><p>We also support other task formats such as multiple-choice tasks that allow different answers for each example depending on specific fields in the data set.</p><p>{question} Choose between the following answers:</p><formula xml:id="formula_2">(A) { answerA } (B) { answerB } ...</formula><p>Here the answer choices could either be the corresponding letters or the answers themselves. We discuss this decision in Section 6.2.</p><p>Throughout, we assume a small set of labeled validation data for quantitative evaluation, which differs from the research on true zeroshot learning <ref type="bibr" target="#b26">[27]</ref>. We note though that this is not enough data to attempt to automatically generate prompts directly. The main elements of prompting can be summarized as: M1 -Prompt Template. A user writes a prompt template consisting of a task description in natural language that utilizes the fields from the task in a situated context. This leads to the construction of the input x that is used for conditioning of the large LM. M2 -Answer Choices. A user provides a dictionary of answer choices paired with the original labels for a given task that offer different possible output wordings y to be considered by the model. The underlying model uses ranking to determine which of these answer choices to select. The original label paired with this answer choice is then the classification choice selected. M3 -Evaluation. A user can evaluate the current version of the system under a known prompt for a set of validation data. This step will provide a proxy score for how well the given wording of the prompt is at capturing the underlying task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">GOALS AND TASKS</head><p>We held discussions with our NLP researchers on the team to determine the functionality for a minimal viable prototype. We imagined how our tool could enable our example personas journalist and analyst to work with prompting. In the following, we summarize the insights of these discussions.</p><p>While prompting is a promising approach, it is still too workintensive for many use cases. The problem is that performance is highly dependent on the specific wording choices for templates (M1, M2), which is reflected in a high variance in accuracy (M3). For example, previous work has shown that different choices of prompts often lead to a more than 10-point spread in task accuracy between the best and worse choices at stage M1, even though both were approved by human editors <ref type="bibr" target="#b30">[31]</ref>.</p><p>Brute force approaches for prompt search require a user to write a large set of possible prompts and validate them empirically on a task. However, these approaches are both computationally expensive and slow. An interactive tool should provide an approach for using fewer resources and allow fast iterations for prompt engineering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Goals</head><p>The high-level aim of PromptIDE is to provide a better approach for prompt development by domain experts in terms of four targeted goals: G1 -Support a broad set of ad-hoc NLP tasks. It is essential that the system is generic, as the user may not know beforehand the nature of the end-user task. The tool should present a single interface for multiple different potential downstream tasks. In this way, a tool is not targeted specifically to model trainers but also to the end-user's application goals. G2 -Faster and more informed prompt writing through feedback from data. The process should let users target new language tasks that arrive during their projects. A tool should enable the user to develop prompts efficiently. It should also provide feedback on what effect prompt variations have. The goal is to make prompt search less automated by giving the user the human-in-the-loop ability to edit and construct prompts based on their domain knowledge. G3 -Ground prompt choices in quantitative measures. Prompt customization replaces training for ad-hoc systems, but it is still critical that choices be grounded in task evaluation metrics. A key element of interaction in the system is that task scores be directly available in the tool itself. G4 -Ease deployment of models to end uses. The tool should provide a testbed for the wording and usage of the prompts, but for actual usage, the prompts must be able to be run and used on actual data cases. Our goal is for the user to be able to directly export the prompts written in the system to a full system for use on the final task itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Tasks</head><p>Given these goals, we identified a series of tasks that guided the development of PromptIDE: Fig. <ref type="figure">3</ref>. PromptIDE UI is organized like a notebook with foldable sections that follow the order of the main workflow but also allow quick iterations within a section or between neighboring sections. Fig. <ref type="figure">4</ref>. Dataset navigation. Browsing through dataset samples and dataset schema lets the user get acquainted with the data. In this case, "label" refers to the index position within a list of potential labels.</p><p>Task 1 -Formulating and trying out prompts and prompt variations.</p><p>To allow users to quickly explore and run different prompt templates for a specific NLP task, the tool provides an interactive interface for formulating and trying out many different prompts in a manner that provides feedback on a small set of real data examples. This interface is agnostic to the NLP task. [G1, G2] Task 2 -Encoding prediction details of the model. Even after finding a good prompt template, it is critical to ensure that the answer choices provided ensure that the prompt is useful and leads to successful results on realistic NLP problems. The tool exposes the predictions of the model beyond ranking to allow for other choices that could be selected. [G2, G3] Task 3 -Testing promising prompts on task performance. From Task 1 and Task 2, there are many combinations of prompt templates and answer choices to be considered, and each may be run on many different examples. Testing provides insight to the user on how choices for promising candidates lead to different resulting outcomes in a larger data regime and informs them which elements of their design have been successful. [G3] Task 4 -Export prompt for concrete deployment. The end goal of the system is to provide packaged prompts that can be used in real tasks after the exploration phase has concluded. For this to be useful, the final system must collapse the exploration steps and provide a full prompt for deployment. [G4]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DESIGN</head><p>The visual interface of PromptIDE provides means to address the aforementioned tasks. With the text being the main carrier of information for the tasks, we designate the most screen real estate to text and interaction with text while introducing visual encodings for abstraction when useful.</p><p>At a high level, PromptIDE appears as a continuous notebook of four foldable sections (Fig. <ref type="figure">3</ref>) that lead to the following workflow: a dataset navigation section to select and browse data, a prompt variation section to broadly explore prompt variations, a prompt refinement section to help fine-tune a specific prompt, and a prompt testing section to explore results of testing on a larger scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">The Four Sections of PromptIDE</head><p>The dataset navigation section (Fig. <ref type="figure">4</ref>) enables browsing and selection of a reference dataset needed for testing prompt templates against tasks T1-T3 and respective goals G1-G3. It provides access to many standard NLP data using the huggingface datasets <ref type="bibr" target="#b17">[18]</ref>. In addition, a user can provide their own data as CSV or JSON files. While describing the dataset schema is syntactically sufficient, the tool enables browsing to help get a better understanding of the encoded semantics behind each data dimension. E.g., in Fig. <ref type="figure">4</ref>, the dataset glue/sst2 (Stanford Sentiment Treebank v2) is selected, and the schema indicates the presence of three fields: sentence, label, and idx. The naming of fields and the respective datatypes match the intuition of how a sentiment classification dataset could look like. But browsing shows that sentences can be very short and of low quality. The prompt variation section (Fig. <ref type="figure" target="#fig_1">5</ref> and Fig. <ref type="figure">1</ref>) allows formulating a prompt experiment for broad variations of prompt templates (M1 in Sect. 3) using up to three template variables q 1 , q 2 , q 3 , and spanning their combinatorial space Q1 × Q2 × Q3. The user can formulate a prompt template (M1) using dataset fields (e.g., {{text}}), template variables q x , and plain template text. The list of answer choices (M2) can be formulated as a static or dynamic list (e.g.,</p><formula xml:id="formula_3">World | | | Sports | | | Business)</formula><p>. The correct answer (M3) can be dynamically retrieved from the dataset and the answer choices (e.g., answer choices[label]).</p><p>After defining the space of all variations &lt; M1(q), M2, M3 &gt; for q ∈ Q1 × Q2 × Q3, the model can now be asked to predict answers for all variations on a small set of data items. As soon as the experiment is started, each variation of M1(q) is represented as a template card, highlighting the template variables q x by a preassigned color (q 1 in red, q 2 in blue, q 3 in gold) and showing the sum of correctly evaluated samples as a bar chart. Then, progressively, the full set of variations is tested against the set of data items, adding results for one data item at a time. For each step in the progression, the order of template cards is updated, keeping the stack sorted by decreasing performance against the ad-hoc task. At any time, when the user has gathered sufficient evidence of what could be promising candidates, they can stop the progression and re-iterate before the experiment would have been finished. This procedure of iteratively formulating prompt variations and trying them out on a small dataset addresses task 1 (G1, G2). Fig. <ref type="figure">6</ref>. Prompt refinement section. To incorporate small optimizations for promising prompts, the user can test against a small subset of data frequently. For each data item, the evaluation chip indicates the prediction, the ground truth, if they match (green or gray), and the normalized distribution of probabilities as a bar chart on top.</p><p>The prompt refinement section (Fig. <ref type="figure">6</ref>) takes one of the template variations and enables quick iterations for fine-tuning this template. Using only one variant M1 allows the user to try on one batch of data items at interactive rates (T1, G1, G2). For each iteration, a performance overview is shown in an evaluation chip that indicates if the task was evaluated successfully for a data item (green background) or not (grey). It shows the predicted answer in black and the correct answer in green. If they match, only one is shown. On top of each evaluation chip, a bar chart indicates the relative probability of all possible answers sorted by rank and normalized to maximum probability.</p><p>The green bar highlights the ground truth, and the leftmost bar indicates the current prediction. This encoding allows, e.g., insights if a wrongful prediction was close to being a coin flip (similar height for most left bar and green bar) or if a correct prediction was a good choice (significant difference between the leftmost bar in green and the second bar from the left), targeting task T2. Besides running quick iterations using a small data regime, the user can trigger an experiment of testing against a larger test set (T3).</p><p>The results of this testing are shown in the prompt testing section (Fig. <ref type="figure">7</ref>) of PromptIDE. After the model completes the testing against a larger dataset, the results are presented such that a user might be able to answer the questions: How well did my prompt perform against the task (T3, G3)? What did the model confuse using my prompt (T2, T3, G3)? How could I potentially tweak the answer choices (T3, G3)?</p><p>For all three questions, PromptIDE provides a visual encoding that can help find an answer. A stacked bar chart indicates the share of correct predictions vs. incorrect ones to indicate prompt performance at the highest abstraction level (Fig. <ref type="figure">7(a)</ref>). If the answer choices allow, a confusion matrix shows across class scores ((Fig. <ref type="figure">7(b)</ref>) and if the in-class performance was good (large values on the diagonal axis). If the answer choices are dynamic but still form groups, the tool shows the top ten most abundant ground truth groups in the confusion matrix. For datasets where the answer choices do not fall into groups, nothing is shown.</p><p>Finally, to help answer the question about potential answer choice tweaks, the tool records the top five ranked generation tokens for each data item independently. Then, for each group of answer choices, these tokens are accumulated, and the number of appearances in the top five is recorded. Additionally, the average rank they had per answer group is calculated (value between 1 and 5). The list of the most frequent and highest-ranked answer tokens is shown in Fig. <ref type="figure">7</ref>(c), with tokens sorted by decreasing appearance count. The green background indicates the best average rank in the group (which does not need to be the most appearing item). The use case in Sect. 6.1 gives an example of the practical use of this feature. Similar to the confusion matrix, nothing is shown if the answer choices do not form groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Visual Encoding and Interactions</head><p>Across the foldable sections, a user can investigate more detail about the prompted data items using the detail stripes (Fig. <ref type="figure">1</ref>). Each detail stripe consists of an upper part highlighting the answer options, the predicted answer (on top), and the ground truth answer (encoded in green). The panels below show details about the prompted text, the ground truth answer, and what the model has generated. Upon request, each ranked result can also show the probability of the respective answer option.</p><p>Detail stripes can be shown when tracking a specific template variation (Fig. <ref type="figure" target="#fig_1">5</ref>) in the prompt variation section. They can be unfolded beneath the performance chips in the prompt refinement section (Fig. <ref type="figure">6</ref>). They show detail about the respective subsets when the user clicks on the performance bars or the cells of the confusion matrix in the prompt testing section (Fig. <ref type="figure">7</ref>). Detail stripes target task T2 (G2).</p><p>To collect and store promising prompts, PromptIDE provides a shopping cart (Fig. <ref type="figure">8</ref>). From the cart, templates can be sent to the prompt variation section or the prompt refinement section. Templates can be exported from the cart to deploy them seamlessly with the tooling provided in the PromptIDE repository (T4, G4). To add prompts, a user can trigger buttons from the template cards in the prompt variation section (Fig. <ref type="figure" target="#fig_1">5</ref>) or the refinement section (Fig. <ref type="figure">6</ref>). If a prompt has been evaluated against the larger testing set, the performance result will be automatically added to the corresponding item in the cart for comparison. Furthermore, a read-only PromptSource shopping cart (Fig. <ref type="figure">6</ref>) shows templates that have been created for a specific dataset by the global community. These prompts could serve as inspiration for starting a user's own prompt idea.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Example Interactive Workflow</head><p>A prototypical interaction workflow using PromptIDE starts with opening the data browser section to investigate the schema and concrete examples of own or globally available data. The user clicks on one item that serves as a good exemplar and is shown for reference in the prompt variations and prompt refinement sections. The user then opens the prompt variation section and writes down a prompt template using data and template variables. Alternatively, they open the prompt source cart and scan for examples that could serve as inspiration, copy them over, and add template variables. They run the experiment and stop it early because one prompt variation seems to perform very well. They use the shortcut to copy the specific prompt over to the refinement section, where, through multiple small edits and along multiple data portions, the prompt seems to be performing well. During this, the users observe the performance chips and occasionally the detail stripes. Later, the user triggers the larger-scale testing to see if they over-optimized the prompt for a local data range. After a short period of time, they note a high confusion between labels A and D. Inspired by the most common top five predictions, they iterate over their answer choices and run the test again, resulting in better performance. The prompt (and maybe some intermediate steps) are saved in the shopping cart. The best-performing prompt is exported to a JSON file. The user can now run the newly created ad-hoc model (original LM plus prompts in the JSON file) with a simple input-output interface or as a batch processing script on new data for their customers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Design Iterations and Rationale</head><p>While consulting with our co-authors, who are NLP domain experts, we went through multiple design iterations on different parts of the tool.</p><p>In this section, we highlight some of them and provide a more in-depth design rationale for certain parts of PromptIDE. Early version of prompt variation testing. The interface is less powerful than the final version: only one template variable q 1 that can either be prefixed or appended to the prompt. Testing is only against one dataset item. Each answer option has its own color, which was not considered useful by early users.</p><p>The overall design as a notebook with foldable sections was inspired by the popular use of Jupyter notebooks in the NLP community. It allows occupying the screen with different views while keeping the views connected in a natural order. This allows the user to build a mental map that is established by scrolling up and down. If, instead, we had mapped the steps of the interactive workflow to independent views, the user might not be able to build this mental model due to the constant context switches. To assist with navigating to a specific subsection, the menu bar acts as a table of content that scroll-animates to the respective sections.</p><p>Our use of progressive visualization methods <ref type="bibr" target="#b36">[37]</ref> was driven by technical design considerations. We started thinking about what the modes of interaction during a progression would be. We decided to use progressive updates for the prompt variation testing because early stopping in this phase of the exploration has proven useful. For example, testing on data with just one label class may reveal local effects relatively quickly, in which case the user can stop the test. On the other hand, we did not use the early stopping interaction during prompt testing on a larger dataset. The goal for this part of the workflow is to observe more global effects by running the test to completion. Early stopping might increase the chance of observing a subset of the data that has local effects, negating the intention of larger-scale testing. Furthermore, the progression in the prompt variation section is controlled by the client and stops if the tool is closed. The larger-scale testing is controlled by the server and continues running even when PromptIDE is being closed on the client-side.</p><p>A fundamental technical design decision was which language to use for formulating the prompts with data and template variables (for T1). The three options for our decision were: (1) invent a new language, (2) use a templating language, or (3) use a general programming language. We quickly decided against inventing our own language that would have to be explained and maintained. The choice for using a templating language (Jinja) over a general programming language (Python) was based on the observation that PromptSource <ref type="bibr" target="#b30">[31]</ref> used the same language. This made the PromptSource parsing work immediately available to us and also allowed us to build the read-only shopping cart with prompt examples gathered by the partner project. Another concern was that a general programming language could be a prime target for malicious server attacks once the tool is released to the public.</p><p>The detail stripes underwent several design iterations. An early version assigned a color to each answer choice (Fig. <ref type="figure" target="#fig_3">9</ref>). But our domain experts did not express interest in being able to track ranks of answer choices across data items and found it more distracting than useful. The switch to a simpler color encoding was also enforced by the required capability to allow dynamic answer options that change per data item and repeat a few times. This would have required an impractically large number of categorical colors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Implementation</head><p>Our PromptIDE prototype consists of a backend in Python that communicates with a frontend written in Typescript and Svelte. We use the openly available pre-trained T0-3B large language model <ref type="bibr" target="#b30">[31]</ref> provided through the huggingface platform<ref type="foot" target="#foot_0">2</ref> . For long-running queries, the backend provides a custom-built queuing and execution system that keeps the memory footprint for the model low. In our implementation, answer options are ranked by the decreasing average log-likelihood:</p><formula xml:id="formula_4">( i&lt;la ∑ i=0 log(p i a ))/l a ,</formula><p>with p i a being the probability for the i-th token of answer a and l a being the answer's token length.</p><p>The demo system is available at http://prompt.vizhub.ai. We will make the source code available upon acceptance of the paper. The open-source version allows easy use of custom data that is either provided as a CSV file or by the Huggingface dataset interface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">USE CASES</head><p>We illustrate how we can use PromptIDE to interactively prompt a diversity of NLP tasks, compare these prompts, save them and export them outside of PromptIDE. We experiment with a range of standard tasks in NLP, including document classification, reading comprehension, and natural language inference. The tool enables seamless development of prompts for a wide variety of tasks and formats [G1] while quickly providing feedback on prompts patterns that generalize to many instances [G2].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Document Classification</head><p>The most common end-user task for NLP <ref type="bibr" target="#b16">[17]</ref> is document classification for an end-domain. The task of document classification is to determine the label of a document from a fixed set. It can be used in adhoc models for filtering a large set of documents or collecting statistics about a large collection. Domain expertise is critical in classification since the specific wording may lead to different results.</p><p>As an example use case we consider a prototypical version of this task with the goal of classifying the topic of a document. The AG News dataset <ref type="bibr" target="#b43">[44]</ref> is often used for benchmarking this task and consists of text documents and labels that indicate which topic they are associated with (labels are canonically listed as "World, Sports, Business, Sci/Tech"). This task (introduced in Sect. 3) is representative of an ad-hoc classification task that a user might consider for prompt development with a language model [G1].</p><p>We can first explore this dataset through the dataset navigation section and then initialize the process through the prompt variation section. This section allows the user to write several different prompt templates <ref type="bibr">[T1]</ref> as well as answer templates for these prompts [T2]. Fig. <ref type="figure">1 (a)</ref> shows some examples of these templates and the way they use fields from the underlying data set and template variables q1 and q2. We select one of the better-performing prompts to investigate further in the prompt refinement section.</p><p>Figure <ref type="figure" target="#fig_4">10</ref>(a) shows the output of the selected prompt for some data, and we observe that there is some confusion about the labels. Upon detailed inspection (see example in Fig. <ref type="figure" target="#fig_4">10(b</ref>)), we observe that the labels can be ambiguous even under human evaluation. So, we send the prompt to larger-scale testing [T3,G3].</p><p>The testing reveals the specific problem. The ground truth label "Technology" is not a great choice and gets confused with other phrases (Fig. <ref type="figure" target="#fig_4">10(c)</ref>). Upon inspection of the Top 5 Rank Predictions, we observe that the token "Science" is a very frequent wording for this ground truth, even more than the term "Technology" itself. We use this insight to refine the answer options in the refinement section by substituting "Technology" with "Science". In this case, the wording of the answer changed, but other feedback would lead to changes in the wording of the prompt itself.</p><p>We run the testing again. After receiving the results, we see that the modification increased performance substantially (Fig. <ref type="figure" target="#fig_4">10(d)</ref>). We can now go to the shopping cart and export the new model [T4,G4].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Multiple-Choice Answering</head><p>The task of reading comprehension is to answer a question from a complex document. It can be used for ad-hoc models to find documents that provide answers to specific questions. The RACE dataset <ref type="bibr" target="#b13">[14]</ref> is a multiple-choice reading comprehension dataset built from English examinations used for benchmarking this task. For a given text extract and a question about that text, the model has to choose the correct answer among four possibilities. Unlike document classification, the possibilities are different for each sample.</p><p>We explore prompts that introduce four answer choices, associating them with letters (A, B, C, and D): "Possible answers:", "Choose between A, B, C and D:", or nothing. Figure <ref type="figure" target="#fig_5">11</ref> shows that "Choose between A, B, C, and D:" consistently gives worse results than the two other variations, independently of how the input is introduced (q1). We discard this variation, and following <ref type="bibr" target="#b41">[42]</ref>, we try out prompts that explicitly state the instruction at the beginning of the prompt. We note that even though the performance remains the same for most of the prompt variations, some variations degrade the performance (for instance "Please select the correct answer among all of the options.").</p><p>We select one of these prompts for refining and test it on 100 examples. From the confusion matrix and the most common top 5 predictions (Figure <ref type="figure" target="#fig_6">12</ref>), we notice that the model is often predicting "E" even though it is not among the answer choices and that it tends to predict "C", as it is always the second most frequent prediction when it is not the correct label. This observation hints at potential class imbalances in the training mixture of the underlying model and warrants more investigation. We find that the training mixture contains a variety of multiple-choice question answering datasets that contain from 3 (COS-E <ref type="bibr" target="#b29">[30]</ref>) to 8 (QASC <ref type="bibr" target="#b12">[13]</ref>) answer choices, which leads to the high frequency of labels A, B, and C.</p><p>The interactive nature of PromptIDE makes it easy to develop a lot of prompts, save them and export them to a different environment. Once the best prompt has been identified, the end-user can use them outside of PromptIDE, for instance, to deploy a prompted language model in production [G4]. The JSON export format makes the whole interactive development extremely versatile and compatible with most of the standard codebase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Sentence Similarity</head><p>The task of natural language inference is to determine the semantic relationship between two similar sentences. An ad-hoc domain expert would use a similar task to check whether documents agree with or contradict specific target statements. Recognizing Textual Entailment <ref type="bibr" target="#b4">[5]</ref> is an inference dataset where a model is asked to assume that one piece of text (the "premise") is true and to classify whether another piece of text (the "hypothesis") must also be true, i.e., whether the premise "entails" the hypothesis. For example, if the premise is Steve Jobs was attacked by Sculley and other Apple executives for not delivering enough hot new products and resigned from the company a few weeks later. and the hypothesis states that Steve Jobs worked for Apple., then the data is labeled as "true" or "entailment".</p><p>Past work <ref type="bibr" target="#b40">[41]</ref> has used prompting for this task but found that it was difficult to find a prompt wording that performed robustly on different examples. Starting from the prompts in <ref type="bibr" target="#b40">[41]</ref> we can use PromptIDE for estimating the models' robustness to different wordings in the templates.  In particular, the prompt variable variants (q1, q2, q3) allow us to easily control and test wording variations.</p><p>To start finding good prompts for RTE, we copy and paste the main instructive variations of <ref type="bibr" target="#b4">[5]</ref> into q1. In previewing 20 examples, we see T0-3B's performance across prompts ranges from a respectable 70% to 50% (Fig. <ref type="figure" target="#fig_7">13a</ref>). We add the best and the worst prompts to carts for further investigation under the "Refine Prompt" section. From the confusion matrix (Fig. <ref type="figure" target="#fig_7">13b</ref>), see that most mistakes are models under-predicting entailment for sentence pairs where the premises do, in fact, entail their hypotheses (i.e., ground truth is "Yes").</p><p>Another noteworthy finding from the "Most Common Top 5 Predictions" is that, for the entailment class, the model generates "No" with a higher rank more frequently than "Yes" (Fig. <ref type="figure" target="#fig_7">13c</ref>), yet it also generates "yes" and "True" with higher rank more frequently than "no" and "False" (Fig. <ref type="figure" target="#fig_7">13d</ref>). Similar to Sect. 6.1, we could simply change the desired target words for a performance boost. But we can also study the effect of explicitly providing the answer choices in the input sequence by appending "True or false?" to every template, which further improves performance for all templates. However, explicitly providing the answer choices in input sequences does not always control model behaviors in accord with human intuition. Prepending the answer choices at the beginning of input sequences does not confer any performance boost.</p><p>Finally, PromptIDE is also prime for studying adversarial conditions such as irrelevant and misleading prompts. For example, using the misleading-extreme prompts from <ref type="bibr" target="#b40">[41]</ref>, Fig. <ref type="figure">14a</ref> shows that "is this Fig. <ref type="figure">14</ref>. Using PromptIDE to study adversarial conditions by using misleading-extreme prompts from <ref type="bibr" target="#b40">[41]</ref>. Details in Sect. 6.3</p><p>grammatically correct" outperforms just instructive prompts such as "does this imply that". However, if we remove the question marks in the global template, then the misleading and irrelevant prompts consistently underperform the instructive ones (Fig. <ref type="figure">14b</ref>), suggesting that models may be using question marks as some kind of heuristic feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">EARLY FEEDBACK AND LESSONS LEARNED</head><p>After internal deployment within the prompting expert group we received initial qualitative feedback from two colleagues. When being asked about most helpful PromptIDE features, they answered: "visualizing the best verbalizer in the confusion matrix form and the Most Common Top 5 Predictions format. Furthermore, it's really helpful to preview the prompt variations and their performances and use refine to copy to the Prompt Refine section." Main criticism came from the limitation of prompt variant variables: "In agnews dataset, when I add q3 and q4, the Prompt variable variants field doesn't have q4. . . . we could have some indicator of maximum prompts allowed." Interestingly, there where also recommendations about UI improvements: "I am confused about the use of red underlines . . . I would prefer use something other than red because red indicates something is wrong." or simplifications like "I think using some 'i' popup icon for storing some information (with more descriptions) is much better than outright displaying technical texts like (frequency &gt; 5 | avrg rank &lt; 5)."</p><p>The lesson we learned from working on this project are manifold with some of them are already indicated in Sect. 5.4. The surprise to us was how much constructive feedback we got about the user interface during the development and after deployment, often started with an apology about being a non expert in UI. We recommend to establishing early on that all UI feedback is very welcome and not a overstepping in competences.</p><p>A constant fight for compromising between flexibility vs simplicity became a major task. E.g., determining how many prompt variation variables we should support (q1, q2, q3) vs how many can we handle with combinatorical explosion of variants.</p><p>Another lesson learned was that the more complex charts tend to be ignored whilst the simple charts with complex algorithmic ideas got much faster appreciated and accepted. E.g., the most common "Aha!" moment was caused by top 5 tokens idea in the prompt testing section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSIONS AND FUTURE WORK</head><p>We present PromptIDE, a system for domain experts to customize models for ad-hoc tasks without requiring training expertise. The system adopts the prompting framework that has recently been developed for NLP tasks while developing an interactive visualization environment for customizing prompts for new tasks. The approach extends beyond brute-force prompt trial-and-error to facilitate the exploration of prompt language and the development of new prompt templates and answer choices. The system is open-source http://prompt.vizhub.ai/ and can work with any available language model backend.</p><p>As the methodology of prompting develops, there are many areas of extension for PromptIDE. Currently, the tool supports tasks with a known set of choices, but there are many NLP tasks where the correct label may be a free-form response. These could be incorporated through support for new metrics such as BLEU in the prompt refinement section. PromptIDE also assumes that the user is able to deduce how to update prompts based on task metrics. Ideally, the system could provide direct advice on how to change prompts or highlight text spans that would lead to better results through methods like gradient saliency. These methods are currently too computationally expensive to run on large promptable models, but will likely improve with more research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. A language model can generate text by ranking all possible events (tokens) at each time step based on their probability. Often, only top-k tokens are considered as generation output. In PromptIDE, only the answer ranks of defined answer choices are compared against each other.</figDesc><graphic url="image-2.png" coords="2,476.00,155.65,62.88,91.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 .</head><label>5</label><figDesc>Fig.5. Prompt variation section. The user can quickly generate and progressively test variations of prompts to identify promising candidates. Each prompt variation is represented as a template card highlighting the values for template variables q x in unique colors and showing the number of correct answers vs. data tested.</figDesc><graphic url="image-8.png" coords="4,312.75,74.38,230.76,157.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Fig. 7. Prompt testing section. Showing results of testing against a mid-sized subset of data to get quantitative measures to help answer questions: How well did my prompt perform against the task (a)? What did the model confuse using my prompt (b)? How could I potentially tweak the answer choices (c)?</figDesc><graphic url="image-10.png" coords="6,85.08,73.00,422.46,126.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 9 .</head><label>9</label><figDesc>Fig.9. Early version of prompt variation testing. The interface is less powerful than the final version: only one template variable q 1 that can either be prefixed or appended to the prompt. Testing is only against one dataset item. Each answer option has its own color, which was not considered useful by early users.</figDesc><graphic url="image-14.png" coords="6,314.17,256.91,230.73,98.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Use Case Document Classification for a news dataset. Details in Sect. 6.1</figDesc><graphic url="image-17.png" coords="7,62.60,73.00,238.60,232.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 11 .</head><label>11</label><figDesc>Fig.11. Mapping dynamic answer choices to simple outputs such as letters can help the model by simplifying its output space.</figDesc><graphic url="image-21.png" coords="8,314.26,360.92,230.74,55.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. The label "C" is systematically the second most frequent prediction of the model, which hints at a potential class imbalance in the training set of the underlying model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Use case Natural Language Inference on RTE dataset [5]. Details in Sect. 6.3</figDesc><graphic url="image-27.png" coords="9,63.54,262.46,228.63,55.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="1,119.59,167.53,349.97,258.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-9.png" coords="5,135.01,73.00,330.93,213.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and B. Hoover are with IBM Research. A. Webson is with Brown University. V. Sanh and A. Rush are with Huggingface. J. Beyer and H. Pfister are with Harvard SEAS. A. Rush is with Cornell Tech. Manuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication xx xxx. 201x; date of current version xx xxx. 201x. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org. Digital Object Identifier: xx.xxxx/TVCG.201x.xxxxxxx</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">https://huggingface.co/bigscience/T0_3B</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">ACKNOWLEDGEMENTS</head><p>We would like to thank the anonymous reviewers for their constructive feedback and helpful comments. This work was partially funded through NSF grant IIS-1901030 and the MIT-IBM Watson AI Lab.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Interfaces for explaining transformer language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Alammar</surname></persName>
		</author>
		<ptr target="https://jalammar.github.io/explaining-transformers/" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Promptsource: An integrated development environment and repository for natural language prompts</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-X</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fevry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.01279</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. December 6-12, 2020, virtual, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P D O</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03374</idno>
		<title level="m">Evaluating large language models trained on code</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The pascal recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Challenges Workshop</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention flows: Analyzing and comparing attention mechanisms in language models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Derose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1160" to="1170" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The finsim 2020 shared task: Learning semantic representations for the financial domain</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">El</forename><surname>Maarouf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mansar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mouilleron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Valsamou-Stanislawski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Financial Technology and Natural Language Processing</title>
				<meeting>the Second Workshop on Financial Technology and Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="81" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Making pre-trained language models better few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.295</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<title level="s">Long Papers</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-08">Aug. 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3816" to="3830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">GLTR: Statistical detection and visualization of generated text</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-3019</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07">July 2019</date>
			<biblScope unit="page" from="111" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Github copilot • your ai pair programmer</title>
		<author>
			<persName><surname>Github</surname></persName>
		</author>
		<ptr target="https://copilot.github.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformer Models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hoover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-demos.22</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07">July 2020</date>
			<biblScope unit="page" from="187" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visqa: X-raying vision and language reasoning in transformers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Jaunet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kervadec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vuillemot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Antipov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="976" to="986" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Qasc: A dataset for question answering via sentence composition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guerquin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sabharwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11473v2</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">RACE: Large-scale ReAding comprehension dataset from examinations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1082</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09">Sept. 2017</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">How many data points is a prompt worth?</title>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.208</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-06">June 2021</date>
			<biblScope unit="page" from="2627" to="2636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The power of scale for parameterefficient prompt tuning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.243</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana</title>
				<editor>
			<persName><forename type="first">M</forename><surname>Moens</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Specia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Yih</surname></persName>
		</editor>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana<address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11-11">7-11 November, 2021</date>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Datasets: A community library for natural language processing</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Del Moral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tunstall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sasko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chhablani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brandeis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Patry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Matussière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Goehringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mustar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lagunas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-demo.21</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, EMNLP 2021</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Adel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Shi</surname></persName>
		</editor>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, EMNLP 2021<address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11-11">7-11 November, 2021</date>
			<biblScope unit="page">2021</biblScope>
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Datasets: A community library for natural language processing</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Villanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Del Moral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tunstall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Šaško</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chhablani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Le</forename><surname>Brandeis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Patry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matussière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Goehringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mustar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lagunas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><surname>Wolf</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-demo.21</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
				<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Dominican Republic</publisher>
			<date type="published" when="2021-11">Nov. 2021</date>
			<biblScope unit="page" from="175" to="184" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">T3-vis: visual analytic for training and fine-tuning transformers in NLP</title>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carenini</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-demo.26</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
				<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11">Nov. 2021</date>
			<biblScope unit="page" from="220" to="230" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.353</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</title>
		<title level="s">Long Papers</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Zong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Navigli</surname></persName>
		</editor>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">August 1-6, 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<idno>CoRR, abs/2107.13586</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Nlize: A perturbation-driven visual interrogation tool for analyzing and interpreting natural language inference models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Srikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pascucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-T</forename><surname>Bremer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="651" to="660" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Building games and apps entirely through natural language using openai&apos;s code-davinci model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mayne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022-03">Mar 2022</date>
			<publisher>Andrew Mayne Blog</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Understanding hidden memories of recurrent neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Visual Analytics Science and Technology (VAST)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Open</surname></persName>
		</author>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<ptr target="https://beta.openai.com/playground" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">What have language models learned? VISxAI Workshop</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pearce</surname></persName>
		</author>
		<ptr target="https://pair.withgoogle.com/explorables/fill-in-the-blank/" />
		<imprint>
			<date type="published" when="2021-07">2021. Jul 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">True few-shot learning with language models</title>
		<author>
			<persName><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Natural language processing meets journalism</title>
		<author>
			<persName><forename type="first">O</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Strapparava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 EMNLP Workshop</title>
				<meeting>the 2017 EMNLP Workshop<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Explain yourself! leveraging language models for commonsense reasoning</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">F</forename><surname>Rajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the Association for Computational Linguistics (ACL2019)</title>
				<meeting>the 2019 Conference of the Association for Computational Linguistics (ACL2019)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Multitask prompted training enables zero-shot task generalization</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chaffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stiegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Thakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Szczechla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chhablani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename></persName>
		</author>
		<author>
			<persName><forename type="first">.-J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Manica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">X</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bawden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Neeraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rozen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Santilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fevry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Fries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Teehan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">It&apos;s not just size that matters: Small language models are also few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.185</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter</title>
				<meeting>the 2021 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-06">June 2021</date>
			<biblScope unit="page" from="2339" to="2352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Autoprompt: Eliciting knowledge from language models with automatically generated prompts</title>
		<author>
			<persName><forename type="first">T</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L L</forename><surname>Iv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.346</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
				<editor>
			<persName><forename type="first">B</forename><surname>Webber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Cohn</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</editor>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11-16">2020. November 16-20, 2020. 2020</date>
			<biblScope unit="page" from="4222" to="4235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Lstmvis: A tool for visual analysis of hidden state dynamics in recurrent neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="667" to="676" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">LMdiff: A visual diff tool to compare language models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hoover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Satyanaryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-demo.12</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
				<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11">Nov. 2021</date>
			<biblScope unit="page" from="96" to="105" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The language interpretability tool: Extensible, interactive visualizations and analysis for NLP models</title>
		<author>
			<persName><forename type="first">I</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bastings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bolukbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coenen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pushkarna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Radebaugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="107" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Progressive data science: Potential and challenges</title>
		<author>
			<persName><forename type="first">C</forename><surname>Turkay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pezzotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Binnig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Fekete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Palpanas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rusu</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1812.08032</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
	<note>Attention is all you need</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A multiscale visualization of attention in the transformer model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vig</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-3007</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07">July 2019</date>
			<biblScope unit="page" from="37" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dodrio: Exploring transformer models with interactive visualization</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Turko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-demo.16</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations</title>
				<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-08">Aug. 2021</date>
			<biblScope unit="page" from="132" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Do prompt-based models really understand the meaning of their prompts?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pavlick</surname></persName>
		</author>
		<idno>ArXiv, abs/2109.01247</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Finetuned language models are zero-shot learners</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>CoRR, abs/2109.01652</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.11903</idno>
		<title level="m">Chain of thought prompting elicits reasoning in large language models</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Calibrate before use: Improving few-shot performance of language models</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<idno>CoRR, abs/2102.09690</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Zhidkov</surname></persName>
		</author>
		<ptr target="https://prompts.ai/" />
		<title level="m">Advanced GPT-3 playground</title>
				<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
