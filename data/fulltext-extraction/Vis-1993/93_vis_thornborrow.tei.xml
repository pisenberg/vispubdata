<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Developing Modular Application Builders to Exploit MIMD Parallel Resources</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thornborrow</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Edinburgh Parallel Computing Centre University of Edinburgh Edinburgh</orgName>
								<address>
									<postCode>EH9 352</postCode>
									<country key="GB">Scotland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J S</forename><surname>Wilson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Edinburgh Parallel Computing Centre University of Edinburgh Edinburgh</orgName>
								<address>
									<postCode>EH9 352</postCode>
									<country key="GB">Scotland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Faigle</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Northeast Parallel Architectures Center Syracuse University Syracuse</orgName>
								<address>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Developing Modular Application Builders to Exploit MIMD Parallel Resources</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Modular application builders (MABs), such as AVS and Iris Explorer[6, 71 are increasingly being used in the visualisation community. Such systems can already place compute intensive modules on supercomputers in order to utilise their power. This paper details two major projects at EPCC which attempted to fully integrate the M A B concept with a distributed memory MIMD (DM-MIMD) environment. The work presented was driven b y two goals, e f icient use of the resource and ease of use by programmer and end user. W e present a model of MABs and describe the maj o r problems faced, giving solutions to them through two case studies.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction 2 Abstract model</head><p>Many modular application builders (MABs) have been built recently and their popularity is growing. It is often the case that the users of MABs wish to utilise parallel supercomputers within the environment. A few years ago, this was only possible by writing a special module that, typically, used socket connections to talk to a remote supercomputer and executed a single module on that platform. There is though, much demand for a closer integration of parallel supercomputers and networks of workstations running a MAB.</p><p>Recently, there have been developments in these areas. CM/AVS allows users of Thinking Machines CM5 Supercomputer to use array data types with AVS in a parallel manner. Modules may be written that describe data distribution in arrays and, if required, the layout of this data over the processors. Within an individual module, the support for parallel processing 1070-2385/93 $3.00 0 1993 IEEE In order to discuss current MABs we will first nerd to define our terminology. This is a new field with few accepted definitions. The following are for ease of discussion within this paper and are not, necessarilj, generally accepted terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MVE (Modular Visualisation Environment): A</head><p>package for data visualisation, consisting of a ~1 s t~ interface allowing linking of modules in a pipeline. MVEs are a subset of MABs specifically geared to 3cientific visualisation. MODULE: An entity which may be utilised in a pipeline. This can be thought of as a filter perforniing a function on input data to produce output d a t a .</p><p>It may also have a user interface. It may itself he a pipeline of modules. USER: The person utilising the MAB. They may iise the modules available or write new ones. A user wlio writes new modules is termed the writer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PIPELINE:</head><p>This is a set of modules and links between these modules. This implicitly defines the types of input, the functions performed on this input and the resulting types of output, e.g. an Explorer map definition.</p><p>PARALLEL MODULE: This is a module whose implementation consists of more than one concurrent process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TRIGGERING:</head><p>The firing of a module. This may be explicit with a given signal or implicit with certain inputs determining the firing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SYNCHRONISATION:</head><p>The method by which a MAB guarantees association of logical groups of data flowing through the pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Serial model of MABs</head><p>The MAB is considered to be a set of modules which can be connected to form pipelines. The function of the pipeline is implicitly defined by the function of each module and their connectivity. We refer to data flowing through the pipeline as meaning that it passes from one module to another in the order defined by the entry point and the links between modules. Typically there is a module in the pipeline, usually the last module, which renders the data.</p><p>There are two forms of parallelism in a serial MAB. The first, functional parallelism, is a facet of the fact that different modules may concurrently operate on different workstations. MABs are designed such that the user is encouraged to write modules that perform one function, in this way, different modules have differing functionality. Thus, we refer to a pipeline as having functional parallelism.</p><p>The second is the ability to 'hook' a parallel platform into a serial MAB. This is usually accomplished as described in the introduction to this paper. It is not a natural feature of the serial MAB and is considered serial as the data communication into and out of the modules is through one point and thus serial in nature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Parallel environment</head><p>The parallel environment is considered to be a multiple instruction, multiple data surface with distributed memory (DM-MIMD). A parallel module is considered to be a collection of processes, each running identical code but, potentially, operating on a different area of the overall data. We refer to this as data parallelism.</p><p>There are two useful broad categories of coinmunication within a parallel MAB utilising an SPMD model of programming. Iiatranzodule is the communication type of two processes of the same module whilst it)iermodule is the communication type of two processes of different modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Serial to parallel migration</head><p>There are three main issues to be addressed when moving from the serial model of MABs to the parallel model. Data Distribution: The method by which data is split up into smaller chunks for processing in parallel and then gathered again for further serial processing. Data distribution also covers the splitting and gathering of the data a s it flows between parallel modules. Synchronisation: The method by which the MVE implicitly triggers modules upon the arrival of data at the module. Keeping logically associat,ed blocks of data (such as a 'frame'in apE <ref type="bibr">[lo]</ref>) associated a s they flow through the pipeline is also part of synchronisation. These are intimately linked for it is usually t,he case that if frames of data become mixed or lost then it is due to incorrect triggering of modules within t,he pipeline. In the parallel environment, the sychronisation is complicated by the splitting of a module into processes. Data distribution must not lead to franws of data becoming seperated. Mapping: The problem of how a description of module placement on hardware resources can be achieved. In other words, mapping covers the issues of how to describe and implement the parallelism of a module. The mapping of modules to resources will have a great influence on how efficiently the parallel pipeline will execute as a bad mapping may substantially increase the amount of message passing and decrease processor utilisation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The MVE project</head><p>The MVE project at EPCC is a prototype library aimed at existing MVE systems to facilitate the iise of DM-MIMD resources within such systems. The work was based on previous work done hy Chris Thornborrow[l, 21. The project firstly examined the design of existing systems in order to sublimate a model of the systems and thus attempt to address generic issues common to most systems. The library was named the NEVIS library <ref type="bibr" target="#b2">[3]</ref>. There was, at development time, no existing MVE which was stable, freely available and small. It was thus necessary to write libraries, or wrappers, to imitate most of the common features of such systems. Into these prototype libraries, the NEVIS library would be slotted, thus demonstrating that, given a full system, DM-MIMD integration would be possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The design criteria</head><p>The two main criteria were ease of use and efficiency. Ease of use applies both for the writer and the user of the system. That the project was to fit within existing MVEs, or at least potentially do so, meant it had several other design criteria.</p><p>It was to be expected that seperate modules would be precompiled to executable object code and use libraries to communicate. The effect is that all the MVE project could aim to do was to extend the libraries. Existing systems encouraged the breakdown of functionally parallel units into seperate modules. It thus seemed natural to support an SPMD programming model within a module. The library must support the triggering rules of modules within existing MVEs and multiple frames of data. There should be fan-in and fan-out from as many input ports and output ports as required by the user.</p><p>In <ref type="bibr" target="#b1">[2]</ref>, Chris Thornborrow categorised the commonly occuring types of data in MVEs and demonstrated that these can be automatically distributed in a parallel environment. However, only arrays and single valued entities, known as parameters, were implemented in the prototype.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The efficiency criterion</head><p>The design should attempt to keep processors busy constantly. In order to keep processors busy, it is necessary to supply them with data as quickly as possible. A naive approach to data distribution within a parallel MAB is shown in diagram 1. It can be seen that the data must be gathered after each module and then split again for the next module. The task of the source is to divide data up amongst the workers and the task of the sink is to gather output data and make it into one coherent whole for the next modules source to distribute again. It is obvious there is a bottleneck and unnecessary inefficiency as parallel communication links are available.</p><p>To avoid this, it was decided to split the source and sink processes up, associating one process with each of the worker processes. In practice, it was possible to make these processes linked libraries and thus avoid context switching. The scheme is depicted in figure 2.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parallel Module</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2: Splitting the Source and Sink</head><p>Thus each sink is responsible for splitting data from its associated process to forward to the correct sources of the next module. These sources then gat,her all such inputs from the previous module into one chunk of data. This gathering enables the data, t.0 be forwartlcd to the normal triggering routines of t,he MVE.</p><p>There are three interest,ing consequences of t.his scheme : Role Swapping: The source and sink have now swapped roles. The sink library now splits data for the next module and the source library now gat&gt;hers data into one chunk ready for forwarding to the worker process. Loose Synchrony: As each process is essentidly distinct from others in the same module, they trigger at, slightly different. times. This enables us to keep processors busier than in the naive a.pproach. Synchronisation: As long as the splitting and gat,Iiering of data work correctly, correct synchronisat.ion of the module is guaranteed. This is tricky to see, hut. relies on the fact that,the data that is forwarded to the existing MVE libraries in a serial form (i.e. ea.ch process only sees what it would, if it were running singly on a smaller data set).</p><p>In practice, the pipeline was found to execute correctly and to run faster witmh the dist,ribut.ed source and sink.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Analysis of the MVE project 4.3 Ease of use criterion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data distribution</head><p>It was decided the code should look as similar to serial code as possible. The number of processors was to be input by the user, not the writer who could only supply a default. Assigning numbers of processes to each module automatically is impossible without some knowledge of the data and the execution rate of modules as Thornborrow shows in [l], thus no automation was attempted. It was decided to attempt to hide as much message passing as possible.</p><p>Intermodule: The NEVIS library was designed to hide all such communication, as existing MAB wrappers hide socket connections between workstations. Thus, the library becomes part of the communications wrapper of an existing MVE. Intramodule: The NEVIS library was not originally intended to deal with this, another project at EPCC, called the Parallel Utilities Library Key Technology Project (PUL) <ref type="bibr" target="#b4">[5]</ref> developed libraries to support the parallel programming paradigms of regular domain decomposition and scattered spacial decomposition. These hide the message passing totally from the user.</p><p>Thus, using the PUL and NEVIS libraries in conjunction within the MVE project, all message passing could be hidden from the writer of parallel modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Module writers interface</head><p>It was possible to use few parallel calls. In the pseudo-code below, extra lines of code are highlighted with a plus sign at the start. Note that the line that indicates processing of the data has a code dependent number of message passing calls as processing the data requires. If the PUL libraries were utilised then there would be no message passing calls here.</p><p>Only arrays have to be mapped across processes in t.he prototype. There are two problems t,o be solved: Specification: Suppose a module performs a reduction operation on a 2D data set, down to a 1D data. set. If all the values in the Y axis are to be reduced to one value for each entry in the X axis, it makes lit,t,le sense to split the dat,a into square chunks across processes and introduce the need for communication of partial sums. Instmead we would wish to assign ships of data. to processes and avoid communication t,ot.a.lly. To this end, the writer needs to specify t,he data distribution over processes. In the NEVIS library this was achieved by allowing the writer to specify which dimensions data should not be split across. This t.echnique was used because the default is to split across all dimensions as equa.lly as possible. The da.t,a split is decided dynamically, given the prefered type of d a h split by the writer and the number of processes by t.lie user. A piece of code can be writt,en that. det,erministically returns a mapping of data space t,o processes given the number of processes, the dimensions and size of the data in each dimension and the type of data split required, specified from a fixed set of possibilities. Re-mapping: As data flows from one module t.o another, it is re-mapped to the correct processes in t,he next module. This is achieved by each module having knowledge about its immediate downstream modules. For each of these modules, the code described above is called with the size and dimensions of the d a h t.o be output, the number of processes of the downstrmin module and the prefered data split of the downstream module. This is fine if the size of the output d a h is known apriori. If it is not, then we effectively have 1D data (it is impossible to assign dimensions of da.t.a if we do not know their size). This does not mean the dat.a has no spacial co-ordinates, simply that, the data shape itself is 1D. Data may thus he split into chunks by each process individua.lly and forwarded on a round-robin basis to the next module. If t,he nuinber of processes of the modules is different., t,hen each process would begin it's round robin distribution at. a different process. This scheme allows loose synchrony of processes even when the dat,a. size is unknown, and, given small enough chunks of dat,a, ensures a fa.irly even distribution of data, in the downstream module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Synchronisation</head><p>It has already been shown that synchronisation is guaranteed by the correct operation of the splitting and gathering libraries. The standard firing libraries of the existing MVE may be utilised. Loose synchrony of the processes is achieved, which is useful for efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mapping</head><p>Some work has gone into automatic efficient mapping of processes and is detailed <ref type="figure" target="#fig_1">in [2]</ref>. This is still in the theoretical stage. In the prototype, mapping was one process per processor (excepting processes for the message passing and PUL libraries).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">The Euphrates project</head><p>Euphrates is one of a series of collaborative projects between EPCC and the petroleum industry. A previous project developed a number of new 3D seismic processing techniques and implemented these on DM-MIMD platform [8]. The goal of the Euphrates project was to take these developments and present them in a form suitable for use in operating companies.</p><p>The approach taken was to build a system in which a scientist could interactively prototype a processing sequence, running small jobs to test that the desired effect was achieved, and then submit a batch job, which would then execute on a much larger, production, data set.</p><p>In order to achieve this within the time available it was decided to take advantage of existing MABs. A number of modules were prepared for such systems to allow users to interactively prototype their processing sequence which may then be saved in the map file format specific to that particular MAB. This is then translated into Euphrates Map Language (EML) a MAB-independent format. Finally, a batch job is created from this map description. This is achieved by generating source code which links against separate libraries which provide the required application functionality and a framework in which to parallelise this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">The design criteria</head><p>Once again the two main criteria for design were ease of use, for writer and user, and efficiency. Ease of use was particularly important as the system was for use by geoscientists who had no wish to become programmers, let alone parallel programmers. It must be possible to process very large seismic images (a typical 3D seismic survey of around 10,000 Kni lines will occupy around 920 Gbytes). Infact, arbatraraly large data sets must be coped with. Modules do not communicate through libraries, but rather are linked, in sequence, from a single piece of source code, generated at the time the pipeline is instanced. Also, it was only necessary to support a srngle frame of data flowing through the system. It must be possible to have fan-in and fan-out between modules. The target architectures are networks of workstations such as Suns and RS~OOOS, and more closely coupled machines such as Meiko Computing surfaces, so the system had to be portable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation</head><p>Euphrates t.ransla.tes a description of an applica.t.ion in EML into an equivalent program for a DM-RIIIMD computer. In common with many geophysical applications, the functionality initially targeted feat,ured processing of a regular mesh of values by fii1it.e difference operations. Previous work at EPCC indicated that the most effective method for parallelising such operations was to introduce data parallelism.</p><p>Clearly this task is too difficult to be tackled in t.he general case since this would amount. t,o the const,ruction of an all-purpose parallelising compiler for DAI-MIMD computers. 1nst.ea.d the a.pproa.ch taken was t.o define a class of operations which would be supported and a module developers interface. Any opera.t.ion which falls int&gt;o this class, and which is programmed in accordance with the module developers int,erface, will be automatically, and correctly, parallelised hy Euphrates.</p><p>Characterising operations 0pera.tions on meshes can be discussed in t.erms of the task which must be performed at each site on t,lie mesh and the perspecti,ue of the operation as a whole. Three properties of tasks can he identified which are key indicators of the efficiency which can he expect.ed from a parallel implementation. These propert,ies ate the spatial dependence of each t,ask, which describes the dependence of a task at one site upon informa.t.ion from other sites; the activity of the operation which tlescribes the distribution of tasks across the mesh; and precedence which describes the order in which t,a.sks must be executed. Operations which have local spatial dependence, global activit.y and no precedence re- The perspective of an operation may be global or local. Global operations, such as a global sum or maximum, require all tasks to be completed before another operation can begin. This is not the case for operations with a local perspective and so a sequence of these operations may be applied to a large data set on a tile by tile basis allowing overlap of file 1/0 and computation and greatly reducing memory requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">The Efficiency criterion</head><p>Euphrates chose to tackle regular operations only, since these were the easiest to parallelise, and use could be made of the PUL library <ref type="bibr" target="#b4">[5]</ref> which supports regular decomposition. Only fully occupied 3D rectangular meshes are currently supported, although voxels may have any aspect ratio.</p><p>Regular operations may be effectively parallelised by the SPMD approach. Euphrates follows this approach and, in order to minimise context switching, combines the entire functionality of the pipeline into one application process per processor. This approach is best illustrated in comparison to the MVE project approach and is depicted in figure 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Providing arbitrary connectivity</head><p>The order in which calls to application modules are made is determined during the translation by analyzing the topology of the input map. Arbitrary con-nectivity can be achieved by ensuring that the modules are sorted into an order where each module only uses data output from a module which occurs earlier in the list. This process also checks for cyclicity; maps which contain feedback loops cannot be support.ed since there is no user interaction and such an app1ica.tion, once set running, would run forever.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ease of use criterion</head><p>The module developers interface is remarkably simple. Developers are required to provide a C function call, and a plain text file which describes the arguments to this call. In this plain text file arguments are associated with ports and are described as either input or output, parameter or data. Parameters may be of any type, but arguments corresponding to data ports may only be of one type; a C data structure which describes an array, and an arbitrary region of this array. Modules must not alter input data arrays, and must write data to fill the indicated region of output, data arrays.</p><p>In terms of our two cat,egories of message passing found in SPMD code, we find the following:</p><p>Intramodule: The writer is forced to use the PIJL libraries and thus no explicit message passing code need be written, simply calls t,o manipulate areas of data.</p><p>Intermodule: These calls cease to exist as they are implicit in the source code generated from the EhlL description of the pipeline. Data is simply passed as pointers from one C function call (module) to anot.lier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Processing arbitrarily large images</head><p>The need to process large images with limit,ed amounts of memory a.va.ilable a.t ea.ch node, and no virtual memory syst.em, led to a decision not, to support operations with a gloha.1 perspective. Such operations would have required Euphrates to implement. an equivalent to a global memory paging system for DM-MIMD systems. This was considered imppropriate since current and future DM-MIMD syst,eins seem likely to based around commodit,y processors and will either have virtual memory systems on ea.ch node or have a global address space.</p><p>Instead Euphrates pre-calculates the memory requirements of an application in terms of the inaxiiniiin number of images which will exist at a.ny one t,ime during one run of the application. Given this informat,ion, and an upper limit on the a.mount of memory availa.hle at each node, it is possible to process arbitrarily large images using regular operations with local perspective. This is achieved by dividing the input images into regions and then processing each region in turn. This decomposition is termed the primary decomposition in order to distinguish it from the secondary decomposition in which a primary region is distributed over a number of processors. Both the primary and secondary decomposition are performed at run-time, allowing the same DM-MIMD executable to run on a systems with varying numbers of processors and with varying amounts of memory per processor, without requiring retranslation or recompilation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Analyzing Euphrates</head><p>By moving out of the MAB environment before parallelising, Euphrates sidesteps many of the complications that the MVE project has to deal with.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Mapping</head><p>Euphrates assumes that each module employs the same data distribution. This allows Euphrates to perform the primary decomposition and to manage all associated file I/O. Because the functionality of an entire application is combined into one process there is only one splitting and one gathering event per primary region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Synchronisation</head><p>Issues of module triggering do not arise, since each module is executed only once and in a predetermined order. There is no triggering library level, as triggering is implicitly defined by the source code generated from the EML which calls each of the modules in turn. Similarly it is not possible for multiple frames of data to become disassociated, since only one frame of data is processed at a time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mapping</head><p>Mapping issues disappear completely since there is only one kind of application process and Euphrates assumes one application process will be placed on each processor. However, the user may easily modify this, so that a more powerful processor receives more than one process. Intramodule message passing is handled by the PUL libraries. These support non-blocking boundary exchange which enforces loose synchrony among the processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and conclusions</head><p>A model of MABs has been present,ed. Tlsing this the issues relevant to parallelising an MAB have been discussed. Two projects at EPCC with differing design considerations, but both addressing t.liese same issues have been presentmed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Synchronisation</head><p>It was surprising to find that there appear to he few synchronisation problems when parallelising M A Bs. In the Euphrates project, this is a natural function of the fact that source code calls are used t,o t,rigger modules, which are in effect, simply libraries. In t.he MVE project, an extra layer of libra.ry was used t.0 forward data to existing libraries that, trigger modriles upon da.ta a.rriving or being present a.t a port,. As long as the new 1ibra.ry guarantees t.hat it has gathered all data necessary to begin execution on a per process basis, then the processes may execut.e wit.h loose synchrony.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Data distribution</head><p>It would seem t.hat enforcing an SPMD model of parallel programming is both sufficient, for the needs of writers and convenient for syst,em builders. Assuming this, there are two factors, splitting of data and gathering. In the MVE project, it was discovered that as long as data sizes and shapes a.re known, t.0gether with the number of processes of each module then data distribution can be automa.t.ed. The writ.er can give guidelines for splitting of data over processc's within a module.</p><p>During the AlVE project it became a.pparent, t.liat if the size of d a h t.0 be out.put. by a module is not. known, then data distribution becomes complica.t.cd if we wish to mainta.in loose synchrony of t.he nest. module's processes. M'e suggested that an intelligent round-robin scheme of distribut,ion be used to forward dat.a to the next module or modules. When d a h ordering is important,, such a s the d a h for a hist,ogram where each entry has a meaning depending on its position, we had to collecte a complete data set, before forwarding to the next module when the PUL libraries were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Mapping</head><p>When dealing with SPMD programming models, there are two broad cat,egories of message passing. Intramodule message passing cannot easily be controll(d by the system. If intramodule data access patterns are of a fixed kind (as in the Euphrates project) then assumptions can be made and libraries developed to support this. The second category of message passing is intermodule. Again, if the patterns are known optimisations can be made, however, once again, if we are dealing with a general parallel MAB then the situation is less clear. Tests using a package called D3 <ref type="bibr" target="#b8">[9]</ref>, written by Mike Norman of EPCC, strongly suggest that co-location of processes from different modules on the same processor would improve efficiency.</p><p>If the model of possible SPMD processing is limited, then great efficiency can be achieved by ordering the pipeline, removing cycles and then compiling down the pipeline into a single process which is then replicated once per processing element. This scheme presupposes that the data distribution is the same in each module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Dynamic module addition</head><p>Neither the Euphrates project, nor the MVE project were dynamic in nature. In other words, once a pipeline was configured, it was impossible to add a module to the pipeline, or remove one. This is reminiscent of apE but neither AVS, nor Iris Explorer work in this way. In order to facilitate this within the MVE project, it would be necessary to use a dynamic message passing system, one that supported the creation and deletion of processes. Interestingly, MPI appears to have no support for this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Future work</head><p>There are a number of issues left to resolve. The first problem for the MVE project is that it does not deal with arbitrarily large data sets. This was not a big priority because it was felt that future machines would have virtual memory at each node, or that there would be a global address space which would mean that the system as a whole might run out of memory but that an individual process would not.</p><p>It is not yet clear how co-location of processes of the MVE project would improve throughput. The prototype used a small number of processes for each module, but each had a dedicated processor. This meant that whenever data was split or gathered, it was sent in a message, rather than being a pointer passed from one module to another, as would be the case should two processes be celocated. This would reduce intermodule communication costs. However, the more co-location that occurs, the more competition there is for memory between the modules. Thus the modules themselves must be split into more processes, spread over more processing elements. This the intramoclule and intermodule message passing occuring. A proper investigation of this would be useful.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Three Parallel Modules Demonstrating Possible Bottlenecks ,.-._.-.-..-.-. -.-, ..-._.__.__.I ..... -......... L .-.-......--......-.-...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2</head><label>2</label><figDesc>Parallel Module L ._...-.._.-.-..... 1 . 1 .....</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>A Comparison of a 16 Processor Configuration of a 4 module Pipeline by the MVE project and Euphrates lations are the easiest to parallelise and are termed regular since a regular geometric decomposition may be expected to provide an efficient parallelisation.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgements</head><p>The Euphrates team were Andrew   </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Utilising MIMD Paralleli.stn in Modular Visualisation Enziironmenls</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Thornborrow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 10th Eurographics</title>
		<meeting>10th Eurographics<address><addrLine>U.K. Confer~nce</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Thornborrow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">"epcc-Ktp-Ne Vis-Mve-Concepts</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EPCC Technical Report</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The NEVIS Protofype User&apos;s m d Developer&apos;s Guide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Faigle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EPCC Summer Scholarship Programme Report</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Euphrates-P Coiicep1.s Document</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
	<note type="report_type">EPCC Internal Report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">PUL-RD User&apos;s Guide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Chapple</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EPCX Technical Report</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">IRIS Explorer User&apos;s Guide (Beta Draft)</title>
	</analytic>
	<monogr>
		<title level="m">SGI Confidential Document</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">IRIS Explorer Module Writer&apos;s Guide (Bela Ora&amp;)</title>
	</analytic>
	<monogr>
		<title level="m">SGI Confidential Document</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bodyscan: A Transputer Based .3D Image Analysis Package</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J S</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Mills</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EPCC Technical Report</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Parallel 3D Graphics Ulility for Parallel Programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Norman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applica.tions of Trmsputers</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The Ohio Supercomputer Graphics Project &quot;apE Version 2.0 Users Manzial</title>
	</analytic>
	<monogr>
		<title level="m">Ohio Sta.te TTiiiversity</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
