<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visualization of Acoustic Lens Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><forename type="middle">J</forename><surname>Bladek'</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Applied Physics Laboratory</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<addrLine>Washington. 1</addrLine>
									<settlement>Seattle</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Currently with Tera Computer Company</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>Washington</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Visualization of Acoustic Lens Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>3-dimensional data visualization from any input source involves the study and understanding of several steps. These steps include data acquisition, signal processing, image processing and image generation. Using a forward-looking high frequency sonar system (which focuses sound much like the eye focuses light), stMdard and non-standard data processing algorithms, and industry &quot;stadard visualization algorithms, this project produced accurate 3dimensional representations of several underwater objects.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite large amounts of noise in the data, the images produced did represent, in large part, the size and general shape of the objects ensonified (For complete details, see <ref type="bibr">[5,9,10,14,15.19,20]</ref>).</p><p>For the c m n t study, the data was collected by the APL Autonomous Underwater Vehicle (AW) acoustic lens from the bottom of Lake Union, Seaule, Washington <ref type="bibr" target="#b7">[6]</ref>. This study also used the AVSTM visualization tools.</p><p>Several custom data processing and imaging algorithms were used to prepare the data for the "standard visualization methods provided by AVSTM <ref type="bibr">[2.</ref> 3.41.</p><p>Although many objects were ensonified, the main target was a sunken tugboat on the bottom of the south end of Lake Union. The boat is approximately 30m long by 7m wide and lies with its bow pointing approximately 30' Northeast. Scuba divers [ 1 J determined bat it had no pilothouse or engine. The deck has 3 holes where the pilothouse had been situated. Further, the tow winch (a device used to distribute cabling) was mostly intact. The gathering of the data by the lens tested its ability to image objects on the bouom of bodies of fresh water.</p><p>Section 1.1. Preview.</p><p>Section 2 briefly describes the operation and characteristics of acoustic lenses. Section 3 explains the nature of the data as received by the A W lens and how it relates to the process of visualization. Section 4 considers the subject of data registration which was determined to be the crucial obstacle to accurate visualization. Section 5 reviews some of the filtering methods used to enhance the images. Finally, Section 6 discusses the imaging methods used, results and conclusions reached.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Section 2. Acoustic lens operation.</head><p>The most succinct way to describe an acoustic lens is to discuss the many similarities between an eye focusing light and how an acoustic lens focuses sound. An acoustic lens fccuses sound onto sound detectors (transducers) in a manner very similar to the way a human eye focuses light onto light detectors (rods and cones). The entire area where the sound/light focuses is referred to in both cases as the The major difference between an eye and am acoustic lens lies in the fact that an acoustic lens is an active system. It does not passively receive whatever sound it may intercept. Instead, it sends out a sound pulse of a known frequency and duration and then waits to receive any returning sound that may have Scattered off objects in its path. The lens' range is limited to the spatial volume enveloped by the sound pulse during the sampling period. The lens "sees" those items in the volume that reflect or scatter enough energy such that some of it makes it back to the lens.</p><p>The spatial resolution of an acoustic lens represents one of the main factors in determining the resolution of any output images. Note that the transducers on the retina have very specific transmission and reception patterns. Each msducer produces rhrough the lens a focused beam of sound 161. Three parameters describe the spatial resolution of many lenses: 1) radial resolution (or range resolution), 2) the azimuthal angle, 0. resolution and 3) the elevation angle, CD, resolution. For example, the system used here has a resolution of O.lm range resolution and   the lens, and other radiant sources such as boat traffic or bursting bubbles. In the APL lens system, the transducer electronics represent the main background noise source.</p><p>Their high input impedance causes them to "pick up" any noise in the electronics as real returns.</p><p>Each ambient noise source possesses its own recognizable signature. For example, the "pick-up" noise from the transducers is represented as a low intensity signal covering every beam and every sector. Ideally, this signal type could be considered merely nuisance data and separated from the "real" signals by raking a very low threshold. Unfortunately, the returns from complex objects vary from brilliant intensities to intensities very near this system noise. Thus, taking almost any threshold could eliminate important information and distort any images produced.</p><p>Another source of interference comes from a phenomenon known as acoustic crosstalk. For the most part, the beams of sound produced and focussed by the APL lens do not overlap. However, situations arise in which the signals received interfere with each other. This phenomenon is caused most frequently by the sound beams from the lens bouncing off a particularly hard or close object. The transducer reception patterns can best be described as an axially symmewic function with non-zero sidelobes [91. The overlapping sidelobes pick up the extra intense returns and cause a non-object to appear in the adjacent beams. The received sidelobe signal appears to represent a real object for visualization, and without additional clues, it would be impossible to distinguish the apparent from the real. Fortunately, other clues to distinguish these signals as clutter do exist (see below).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Section 3.2. Bright objects.</head><p>The differences between bright objects and other return types quickly becomes apparent. Signal strength rejxesents one clue with which to differentiate signals from noise. Another method looks at the duration of signals; rpal objects usually provide reasonably consistent returns over a number of data cycles. A third method looks for acoustical shadows. A zone of very low intensity returns just behind an object (as viewed from the lens) characterizes these shadows. The shadow's shape roughly corresponds to the object's shape and height as seen from the acoustic source or lens (See <ref type="figure">Figure 3</ref> for a typical plot of a single frame of Bright objects are initially found due to the high levels of backscatter. However, as noted above, once an object is identified, all information in the immediate vicinity of an object becomes impoxtant. All returns, no matter their strength. can represent some portion of an ensonified object. This mixing of interference and "real" data sources made the visualization task difficult and guided many data processing algorithm designs. data).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3. Plot of a Typical Bright Obect</head><p>Return from the APL AUV lens. represents one way to get more information about the objects in the view volume. The process of putting two or more data fiames together is called frame compounding.</p><p>The APL A W lens system does not record the velocity vector of the lens as it records the data. This means that the spatial relationship between successive frames is not directly hown or ascertainable. The process that provides this information and relates successive frames is defined as frame registration. Accurate registration substantially controls the quality of the final images. In fact, without reasonably accurate registration, the advantage gained by compounding multiple frames is lost. As mentioned in the introduction, these registration and compounding stages therefore came to represent the most crucial steps in the visualization process. In all previous work, the investigators controlled and therefore knew the motion of the lens. They then accurately registered and compounded many data frames. Unfortunately, the current study data was not acquired under such controlled circumstances [61. Under the best circumstances, we did not know the speed of the lens, let alone the velocity vector, to within 1 knot. Because the lens itself is a relatively low resolution device (especially in the 8 and # dimensions) the resulting images could not be very detailed without accurate registration.</p><p>The lens currently provides no form of data other than the envelope detected reflectance returns. Any information to detect the motion of the lens must come from the reflectance data itself. Under these circumstances, it became difficult to find a general. automatic registration method. However, using the theory of 3-dimensional moment invariants and previous work of Kamgar-Parsi in the area of registration <ref type="bibr">[13. 14, 15. 16, 19, 213</ref>, both the translational and rotational motions of the lens between data frames can be automatically predicted under certain This method of motion prediction performs only as well as the ability of the algorithm to mck the centroid of an object. Because the lens can receive a radically different intensity return from the very same place on an object, using the raw intensity returns can cause the object's centroid to vary radically within the object itself. Because this method depends heavily on a consistent if not accurate prediction, in general, this means the algorithm needs a series of consistently ensonified objects in the lens field of view f" frame to frame. If not, the prediction will not track the actual motion of the lens.</p><p>The data collected by the APL lens rarely met all the conditions for accurate registration. This method worked well enough on these data sets to produce a few high quality accurate images but only on a very small subset of all the data collected.</p><p>Beyond translational motion prediction, the theory of 3dimensional moment invariants can be used to predict the rotational motion of an object as well <ref type="bibr" target="#b18">[13,</ref><ref type="bibr">14,</ref><ref type="bibr">15</ref> </p><formula xml:id="formula_0">-"0 -mOIO -"1 z, -- Y, -- x, -- "</formula><p>" " Once the data was collected, registered, and compounded, it continued to be noisy and sparse for all the reasons stated above (Le., interference sources and poor registration over a minimum set of frames). In order to combat these problems we used simple shape-based filters to improve the final images. The term "shape" in this context refers to the field of mathematical morphology using the dilation and erosion operators <ref type="bibr" target="#b10">[8,</ref><ref type="bibr" target="#b0">11,</ref><ref type="bibr" target="#b16">12,</ref><ref type="bibr" target="#b22">171</ref>. Equations 5.1 and 5.2 demonstrate these operations in terms of the maximum and minimum operators. In these equations, I represents the input set, K represents the structuring element <ref type="bibr" target="#b10">[8,</ref><ref type="bibr">17]</ref> and x and z represent the indices of the values within each set. The structuring element, K, is a 1-, 2-or 3-dimensional matrix, which is usually simple in shape and small in size. Using the values within the structuring element, it is possible to look for many shapes within the data. The possible shapes include circles, cylinders, and even spheres <ref type="bibr">[8, 11.12, 171.</ref> In this case, we were not looking for any particular shape within the data, but we did know the approximate intensity and spatial frequency of the noise we were attempting to eliminate. Thus, using an all zero value structuring element of the appropriate size, it was possible to eliminate many noise sources. (Note that an all zero value structuring element reduces the morphological operations down to only a maximum or minimum filter thus reducing the amount of computation significantly) <ref type="bibr" target="#b7">[6,</ref><ref type="bibr" target="#b10">8,</ref><ref type="bibr" target="#b0">11,</ref><ref type="bibr" target="#b16">12,</ref><ref type="bibr">17]</ref>. The next two operations in morphology (and the ones most commonly used) are openings and closings [8. 171. Equations 5.3 and 5.4 describe these operations in terms of the dilation and erosion operations. Filtering with these operations using our simple structuring element involves a minimum filter followed by a maximum filter and vice versa. Although filtering with these non-linear operators can involve an infinite number of combinations, experimenting with a small subset disclosed that a 3 by 3 by 3 closing was the most effective at eliminating the noise and enhancing the data for the imaging methods attempted (i.e., volume and isosurface rendering) <ref type="bibr" target="#b7">[6]</ref>. This section contains two sub-parts, one for each of the imaging methods used. Section 6.1 discusses and shows the best volume rendering image and Section 6.2 performs the same task for the isosurface rendering method. This last section not only shows the best isosurface rendering images, it also demonstrates the best overall images. Section 6.1. Volume rendering.</p><p>'This section contains one image. It represents a single pass over the sunken tugboat taken in June 1992. We found that the particular logical space imaging method provided by the AVSm system [2, 3, 41 did not consistently produce adequate results. Each particular situation calls for a certain resolution based on factors such as the sampling geometry of the lens, the direction and speed of the lens over the target, and the size and shape of the object. These factors often produce resolutions that do not match the overall proportions of objects (i.e., a 40 by 40 by 40 sample array for a 5m by 5m by 30m object). This logical space algorithm could not depict many objects correctly because it depends entirely upon the resolution of the output grid to determine the proportions.</p><p>Two situations occurred in which the resolution of the output grid closely matched the proportions of the tugboat.  multi-valued data over the entire dynamic range of the lens, some sort of processing must take place in order for this imaging method to find and produce coherent and continuous surfaces.</p><p>Bright objects distinguish themselves by both bright reflectivity and a higher point density above a minimum threshold. These data features led to the following algorithm:</p><p>The first step finds, for every i by j by k region in the input data matrix, the number of points which have an intensity above a minimum value. If the first step finds a minimum number of points above a minimum threshold (i.e.. a minimum density), then all the values in the region receive the same value. This value represents a scaled version of the above-thethreshold point density. <ref type="figure" target="#fig_5">Figure 4</ref> shows several twodimensional examples. In the first example, 3 out of 9, or 33%, of the values are above the threshold of 6. Thus, the algorithm gives each corresponding place in the output grid the value 3.3 which is 33% of the way between 0 and 10. In the second example, only 2 out of 9. or 22%, remain above the threshold, thus the algorithm zeros out all values in the region. This algorithm, in effect, creates a scaled density surface for the isosurface rendering algorithm. It condenses the once disparate intensity values down to a manageable range with which to find an isosurface.</p><p>The images in this section come from two different data sets. These files represent the most detailed data processed and thus consistently produce the best results, no matter the imaging method. <ref type="figure" target="#fig_12">Figure 6</ref> depicts an isosurface rendering of a single South to North pass over the tugboat taken in June 1992.</p><p>The ensonified volume per frame of data was smaller in this data set leading to a higher sample density and thus more detailed images. Data used for this figure contains registration errors caused by a had fix on the centroid of the boat, but the results still show many accurate 3dimensional details. Note how it reveals many details about the surface of the boat including 2 of the 3 known holes and the tow winch near the center of the stem. Through shadows, perspective and the use of the density mapping explained above, this figure reveals many correct details not seen in the volume rendering image discussed above. <ref type="figure" target="#fig_13">Figure 7</ref> shows the first image of a single North to South Pass over the tugboat from June 1992. Note the large number of accurate three dimensional details revealed by this figure. It accurately depicts the three holes (one slightly obscured by a railing and the viewpoint), the position and general shape of the tow winch near the middle of the stem, and even the railing around the bow of the boat.  <ref type="figure" target="#fig_13">Figure 7</ref> is an adjustment to the isosurface level. This operation helps to enhance different features of the boat than those in <ref type="figure" target="#fig_13">Figure  7</ref>. It helps to bring out a more accurate depiction of the tow winch and the shape and structure of the decking and two of the three holes on the surface of the boat. As above, the combination of the density mapping, the lighting, and perspecti.ve cues allow this image to reveal many accurate and demled features not previously seen through any of the other images or imaging methods.</p><p>Section 63. Conclusions.</p><p>Acoustic lenses represent an important tool for looking, detecting and, now, imaging objects which cannot be viewed using light. This study, using the AUV lens developed at the Applied Physics Lab at the University of Washington, Seattle, Washington, has demonstrated the feasibility of accurately visualizing acoustic lens data using certain industry standard visualization techniques along with standard and non standard data and image processing algorithms.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tow Winch Tow Winch</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Humans use light as their primary information source about the world. Situations arise, however, in which light cannot penetrate and another visualization method is necessary. In certain situations, such as turbid water, sound waves can penetrate and thus represent a viable alternative. The act of seeing with sound, as with light, includes looking, detecting, and then forming some sort of image. Many past and present sonar systems can look, detect and image 3-dimensional objects using %dimensional imaging techniques. Imaging those same 3-dimensional objects using 3-dimensional imaging techniques remains a challenge. Personnel at the University of Washington Applied Physics Laboratory (APL) have played an active role in that challenge [5,6,7,9,10,201. In one of their most recent projects with the Naval Research Laboratory (NRL), [14, 15, 191 a forward-looking active acoustic lens (to be explained below) and several different data visualization techniques were used to produce accurate 3-dimensional representations of underwater objects. Data visualization remains an area of current research interest Current processing techniques were 6rst developed for medical imaging technologies such as Magnetic Resonance Imaging (MRI), Positron Emission Tomography (PET) and Ultra-sound. Because many such data sources provide 2-dimensional data slices, current imaging techniques handle this type of data best. Despite this fact, adapting 2-dimensional methods to the acoustic lens data, which is truly 3-dimensional. presented itself as a reasonable come of action. Of a number of possible methods. volume rendering and isosurface rendering were chosen as the most likely candidates to effectively display the data from an acoustic lens. The previous e f f m on this project were among the first to post-produce high resolution images from the acoustic lens data It involved imaging a series of known objects in known configurations under highly controlled conditions. The main point of the efforts at this stage focussed more on the lens data itself and less on the best data visualization method. The data visualization methods used include a "point cloud" representation, a "standard" mesh surface representation, and a volume rendering method supplied by the AVSm software package [2, 3. 4, 9, 14, 15, 191.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>retina. (See Figure 1 and [5]). Further, just as the eye's retina converts the objects it sees into electrochemical signals, the acoustic lens' retina converts objects it "sees" into electrical signals. Finally, just as the eye uses the change in the index of refraction from the air outside the lens to the comea and aqueous humor (fluid inside the lens) to focus light, the acoustic lens uses the change in the index of refraction from the fluid outside to the fluid inside the lens to focus sound (See Figure 1 for a simple diagram of the operation of an acoustic lens [SI.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1. 5 "</head><label>5</label><figDesc>azimuthal and elevation angle resolution. Although not an exact mapping to the standard Cartesian coordinate system</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 .</head><label>1</label><figDesc>Bask Operation of an Acoustic Lens [5 . Using the change in refractive the fluid inside,the acoustic lens focuses the sound wave onto the transducers on the retina. indices 1 ~m the fluid outs.ide the lens to these resolutions correspond to approximately 0.15m 2 resolution, 0.5m to 4.0m X resolution and 0.25m to 1. Om Y resolution (See Figure 2 for axis definition). Note that these parameters describe a system which has decreasing resolution as the range in the azimuthal and elevation directions kreases. As the sound beams move away from the lens their energy diverges in space and thus the echoes received from objects farther away represent samples fiom a larger and larger area.Section 3. Nature of the data.This section reviews some of tl;e characteristics of the data as received by the APL AUV lens. It shows the nature of the data and what features enhance and detract from the visualization process. Two main dam categories are used in the discussion: (1) Interference sources, and (2) bright object returns. The distinction between these categories remains somewhat arbitrary, but, in general, this classification system is adequate for the task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Section 3. 1 .Figure 2 .</head><label>12</label><figDesc>Interference sources. As noted above, acoustic lenses represent active devices that first send out a sound pulse and then accept echoes. Non-zero, non+bject return values are referred to as ambient noise (or background noise). Ambient noise sources include thermal noise in the electronics, slight temperature or density differences in the fluid surrounding Side View Note that the resolution of the lens decreases with range. kens [TJ Objects to be Imaged Lie within the View Volume A Single Frame of Data Contains Echoes from Objects within the Ensonified Volume Typical Configuration of the APL AUV Lens [SI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Section 4 .</head><label>4</label><figDesc>Registration.As the lens moves forward and takes data, successive data h m e s overlap in space. This overlap means that at least some portion of most objects remains within successive dam frames. Combining frames in some manner</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>restricted circumstances. An object's centroid is its center of mass. Whether measured in absolute or relative world coordinates, it represents the origin of its coordinate system. Theoretically, an objects centroid should not change unless the object itself changes. Assuming static objects for the period of observation, it became possible to predict an object's translational motion by calculating its centroid over successive frames. n i s was done using the srandard moment equation (Eiqn. 4.1) for both the zeroth and first order moments and then finding the objects centroid coordinates (x,.y,.zJ using equation 4.2. (The zeroth order moment is a measure of an object's volume.) In equation 4.1. I(x,y,z) represents the intensity of the reflectance data;x, y, and z represent the coordinates of that data; m i ? represents the ith, jth, kth order moment; and the integral is taken over the view volume.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>( 1 6 Section 6 .</head><label>66</label><figDesc>3~) (x) = m a x z E K { r ( x -z ) +k(z))} K ) (x) = m i n z E k { I ( x + z ) --k ( z ) } Imaging methods and results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5</head><label>5</label><figDesc>depicts one of those passes over the scuttled tugboat on the bottom of Lake Union. It depicts a North to South pass over the boat taken in June 1992.Figure 5depicts the volume rendering method's version of the top surface. It reveals many details such as the three holes in the deck and the overall shape. It does not, however, clearly show the tow winch or. in fact, the correct size of the tugboat. The proportions of the output grid are a close match to the true physical dimensions of the tugboat, but are not exact. This represents one of the best attempts using this algorithm, but it does not compare to the image detail obtained using the algorithm explained in the next section.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Section 6 . 2 .</head><label>62</label><figDesc>Isosurface rendering.Isosurface algorithms attempt to construct surfaces through a single constant data value. Just as curve fiuing interprets single valued lines between points in two dimensions, an isosuface interprets single valued surfaces between points in three dimensions. Given the fact that data emerging from the morphological filters still contains Opening I o K = ( I 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8</head><label>8</label><figDesc>represents a second rendering of the North to South pass over the tugboat from June 1992. The only processing difference between this figure and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 5 :</head><label>5</label><figDesc>Volume rendering of one pass over the sunken tugboat<ref type="bibr" target="#b7">(6)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 6 :</head><label>6</label><figDesc>lsosurface rendering of a south to north pass over the sunken tugboat<ref type="bibr" target="#b7">(6)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 7 :</head><label>7</label><figDesc>lsosurface rendering of a north to south pass over the sunken tugboat (6).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 8 :Figure 5 :Figure 7 :</head><label>857</label><figDesc>Second isoswface rendering of a north to south pass over the sunken tugboat<ref type="bibr" target="#b7">(6)</ref>.(See color mtes, p. CP-34.)Visualization of Acoustic Lens Data, A.J. Bladek, pp. 3 16-323. Volume rendering of one pass over the sunken tugboat<ref type="bibr" target="#b7">(6)</ref>. lsosurface rendering of a north to south pass over the sunken tugboat<ref type="bibr" target="#b7">(6)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 6 :Figure 8 :</head><label>68</label><figDesc>lsosurface rendering of a south to north pass over the sunken tugboat<ref type="bibr" target="#b7">(6)</ref>.Tow Winch Second isosurface rendering of a north to south pass over the sunken tugboat<ref type="bibr" target="#b7">(6)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>211. For our purposes, this method was far too inaccurate to use with the data as received. Although this method was shown to be accurate under conuolled data collection conditions[14,15,19], it was not a viable option under our operating circumstances.</figDesc><table><row><cell>Section 5. Filtering.</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments:</head><p>The author wishes to thank Dr. </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Applied Physics Lab Underwater Video Tape</title>
		<imprint>
			<date type="published" when="1992-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Develoder's Guide</surname></persName>
		</author>
		<title level="m">Advanced Visual Systems. Inc., Release</title>
		<imprint>
			<date type="published" when="1992-05" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Advanced Visual Systems</title>
	</analytic>
	<monogr>
		<title level="j">AVS Module Reference</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Release 4</title>
		<imprint>
			<date type="published" when="1992-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Advanced Visual System, Inc.. Release4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Avs User's Guide</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belchex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Edward</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Forward-Looking Active Acoustic Lens</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rosenblum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Larry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">APL Technical Report-APL-UW TR9113</title>
		<imprint>
			<date type="published" when="1991-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bladek Anthony</surname></persName>
		</author>
		<title level="m">Visualization of Acoustic Lens Data</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Thesis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992-12" />
		</imprint>
		<respStmt>
			<orgName>University of Washington</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A Real-Time Imaging And Data Acquisition System For An Acoustic Lens Sonar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clark</forename><surname>Bodyfelt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
		<respStmt>
			<orgName>University of Washington</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">M.S. Thesis.</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Introduction to Momholoeical</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Dougherty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">SPIE Optical Engineering Press</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imaee</forename><surname>Processhe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<pubPlace>BeUingham. WA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Acoustic Imaging: The Reconstruction of Underwater Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ola</forename><surname>Engelsen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
		<respStmt>
			<orgName>University of Washington</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">M.S. Thesis</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">An Acoustic Imaging System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Glasgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
		<respStmt>
			<orgName>University of Washington</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">M.S. Thesis</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Robert</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image Analysis Using Mathematical Morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Stemberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhua</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions On Pattern Analvsis and Machine Intellieence</title>
		<imprint>
			<date type="published" when="1987-07" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="532" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Computer and Robot Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">G</forename><surname>Linda</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visual Pattem Recognition by Moment Invariants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Kuei</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J J</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="179" to="187" />
			<date type="published" when="1962-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Underwater Acoustic Imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kamgar-Parsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Behzad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rosenblum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">O</forename><surname>Belcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comuuter Grauhics and Auulications</title>
		<imprint>
			<biblScope unit="page" from="11" to="13" />
			<date type="published" when="1992-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">An Imaging Sonar h i Underwater Vision</title>
		<imprint>
			<date type="published" when="1992-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3D Moment Forms: Their Construction and Application to Object Identification and Positioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chong-Huah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Don</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JEEE Transactions on Pattem Analvsis and Machine Jntelligence</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1053" to="1064" />
			<date type="published" when="1989-10" />
		</imprint>
	</monogr>
	<note>Hon-Son</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Morphological Systems for Multidimensional Signal Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Margos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Petros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">W</forename><surname>Schafer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedines of the</title>
		<meeting>eedines of the</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Acoustics: An Intro duction and ADDlications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><forename type="middle">D</forename><surname>Pierce</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Acoustical Society of Amenca</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Acoustic Imaging: The Reconstruction of Underwater Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Rosenblum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kamgar-Pars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Behzad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belcher</surname></persName>
		</author>
		<idno>Engelsen. 01%</idno>
		<imprint>
			<date type="published" when="1990-04" />
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="690" to="709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m">Visualization &apos;91</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A Digital Signal Processing Hardware System For An Acoustic Lens Sonar Array</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>San Diego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shawn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="94" to="101" />
		</imprint>
		<respStmt>
			<orgName>University of Washington</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">M.S. Thesis</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scene Matching Using Invariant Moments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernest</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Comuuter Gra~hjcs and Image Processing</title>
		<imprint>
			<date type="published" when="1976-08" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="16" to="24" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
