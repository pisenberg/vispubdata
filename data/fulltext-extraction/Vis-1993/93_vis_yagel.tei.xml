<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Accelerating Volume Animation by Space-Leaping</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roni</forename><surname>Yagel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhong</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Accelerating Volume Animation by Space-Leaping</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>In this paper we present a method for speeding the process of volume animation. It exploits coherency between consecutive images to shorten the path rays take through the volume. Rays are provided with the information needed to leap over the empty space and commence volume traversal at the vicinity of meaningful data. The algorithm starts by projecting the volume onto a C-buffer (Coordinates-buffer) which stores the object-space coordinates of the first non-empty voxel visible from a pixel. Following a change in the viewing parameters, the C-buffer is transformed accordingly. Next, coordinates that possibly became hidden are discarded. The remaining values serve as an estimate of the point where the new rays should start their volume traversal. This method does not require 3D preprocessing and does not suffer from any image degradation. It can be combined with existing acceleration techniques and can support any ray traversal algorithm and material modeling scheme.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1: Introduction</head><p>Visualization is the process of converting complex information to a format that is amenable to human understanding while maintaining the integrity and accuracy of the information. One type of information that can benefit from visualization techniques is volumetric data which consists of information on three-dimensional phenomena. The process of determining, for a given view point, which parts of the volume are visible is called volume viewing. Algorithms for volume viewing operate by scanning through the data points (voxels) in a strict sequence designed to achieve hidden voxel removal.</p><p>The simplest way to achieve hidden voxel removal is to traverse the volume regarding each voxel as a 3D point, that is, each voxel is transformed by the viewing transformation matrix and then projected onto the screen. Besides its computational burden, another severe limitation of the above method (also called forward projection) is that it is hard to accelerate because any forward viewing algorithm must process all voxels in the dataset. We take the alternate approach, termed backward viewing, and show that we can avoid processing a large portion of the data without affecting image quality.</p><p>The ray casting algorithm (which is considered a backward projection), casts a ray from each pixel on the screen into the volume along the view vector. While following a ray, the voxels it pierces are sampled and the resulting colors and opacities are accumulated to yield the ray's final color <ref type="bibr">[7]</ref>. The simplest volume sampling method performs zero-order interpolation to locate the nearest voxel and uses a 3D integer-based line algorithm to generate the ray. A more precise algorithm is based on a continuous line algorithm that can take several samples per voxel. It also uses higher order interpolation to compute the sample value. If the ray is traced as it reflects from surfaces (in order to simulate reflections, refractions and shadows) we call it ray tracing <ref type="bibr">[14]</ref>. In this paper, however, we deal only with raycasting.</p><p>Several methods, originating from surface-based ray tracing algorithms, have been suggested to reduce the computation involved in volumetric ray casting. Methods have been described that reduce the cost of tracing each ray by employing adaptive termination [5, 71 or by adaptively reducing the sampling rate along the ray [5, 101. Alternatively, acceleration can be achieved by reducing the number of rays <ref type="bibr">[8]</ref>, or by exploiting coherency between rays in orthographic viewing [15].</p><p>Another set of acceleration techniques provide the ray traversal algorithm with the means to efficiently traverse or even skip the empty space surrounding the volumetric objects. We call this capability space-leaping. The method described in this paper (Section 4) provides space-leaping capabilities when rendering a sequence of images.</p><p>Commonly, the outcome of the viewing process consists of, for each screen pixel, the depth and the value (color) of the first opaque voxel encountered by the ray emitted from that pixel. However, it is possible to apply different operators while following the ray. For example, the weighted additive reprojection produces an X-ray-like image by averaging the intensities along the ray. Alternatively, one could display the maximum value encountered along the ray passage in a method called maximum projection. Assigning opacities to voxel values will enable a compositing projection to simulate semitransparent volumes. The algorithm presented in this paper does not assume a specific scheme and can support any one of them.</p><p>In this paper we describe a volume viewing algorithm that exploits coherency between images in a sequence of frames such as those commonly generated by animation systems. Such a sequence of frames is generated by rendering the same volumetric dataset while changing some rendering parameters or object attributes. For example: light source moves or changes its color, objects change their colors or opacities, or the observer changes his position or viewing direction. Our method is based on saving, in each pixel, the coordinates of the first non-empty voxel encountered by the ray emitted at that pixel. This information can often be used for the efficient generation of the next image in a sequence of images. One example of the use of this type of coherency can be found in the case when the second image is to be rendered using the same viewing parameters as the first one except for a change in some material attributes (e.g., transparency, color). The coordinates value stored in each pixel indicates the volume coordinates where ray traversal could start, as opposed to starting at the volume boundarythereby avoiding the repeated traversal of the empty space. The idea of exploiting frame coherency was explored in the case of surface-rendering. In [2] a search-based method was proposed that extends the idea of adaptive sampling to the temporal axis. Images are not generated in order and suffer from some level of image degradation. More recently a method was suggested that exploits coherency in ray-object intersections between frames [3]. This method is limited and does not extend to volume rendering. In this paper we present a volume rendering algorithm that exploits frame coherency to perform efficient rendering without any image degradation.</p><p>In the next section we describe in more detail the general method of volume rendering by ray casting. Section 3 surveys existing methods that employ space-leaping for accelerating volume rendering. In Section 4 we describe our method for accelerating multi-image volume rendering. We conclude with implementation results that show the effectiveness and utility of our method. </p><formula xml:id="formula_0">I l l l l l l i l l l i l l i l l</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2: Volume Viewing by Ray Casting</head><p>The algorithm for volumetric ray casting is simple: for each pixel on the screen', send a ray into the scene ((a) in <ref type="figure" target="#fig_0">Figure  1)</ref>. Starting at the point where the ray enters the volume ((b) in <ref type="figure" target="#fig_0">Figure 1</ref>), follow the ray while sampling the volume at constant distances ((d) in <ref type="figure">Figure I</ref>). Accumulate (composite) the colors and opacities of these sample values. Stop following the ray when it is known that it cannot significantly change its value, that is, when it had accumulated an opaque color or when it is no longer inside the volume ((c) in <ref type="figure" target="#fig_0">Figure 1)</ref>. <ref type="figure">Figure 2</ref> shows the passage of rays through a 16x16 grid that contains a semitransparent object. It is assumed that accumulating the values of two voxels yields an opaque value which causes the ray to stop. It is also assumed that only I . In practice, rays are sent only from pixels that are potentially affected by the volume ((e)-(f) in <ref type="figure" target="#fig_0">Figure 1</ref>).</p><p>rays that hit the volume are traced (the range (e) to (f) in <ref type="figure" target="#fig_0">Figure 1</ref>). From <ref type="figure" target="#fig_0">Figure 1</ref> and <ref type="figure">Figure 2</ref> it is obvious that the speed of a ray-caster depends on the total number of voxels it traverses. Therefore, the rendering time of a volume containing many objects will be shorter than that of a sparse dataset since in the first case rays will tend to stop after traversing a small number of voxels. This phenomenon leads us to observe that the passage of a ray through the volume has two phases. In the first phase, the ray searches through the empty space looking for an object. In the second phase the ra integrates colors and opacities as it penetrates the steps, depending on the object's opacity. That is, most of the rendering time is spent in the search for an object while traversing the empty space. Since the passage of the empty space does not contribute to the final image, efficiently traversing, or altogether skipping the empty space could provide significant speedup without affecting image quality. object 1 . Commonly, the second phase involves one or few</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3: Existing Methods for Space-Leaping</head><p>A most widely used and effective branch of acceleration techniques for volume rendering deals with methods to efficiently traverse or altogether skip the empty space. We call this capability space-leuping. One well known method that delivers space-leaping capabilities is based on the hierarchical representation (e.g., octree, pyramid) of volumes. This approach decomposes the volume into uniform regions that can be represented by nodes in a hierarchical data structure. An adjusted ray traversal algorithm skips the (uniform) empty space by maneuvering through the hierarchical data structure [8, 91. It was also observed [4] that instead of traversing the hierarchical data structure (which is less efficient, compared to regular volume traversal), the uniformity information implied by the octree can be stored in the empty space of the regular 3D volume grid, as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. In this type of grid, calledjlatpyramid, each empty voxel is assigned a value that indicates the level of the octree it belongs to. Rays are then sent into the volume. When the ray encounters a data voxel (dark squares in <ref type="figure" target="#fig_2">Figure 3</ref>), it is handled as usual. Encountering a voxel with uniformity information instructs the ray to perform a leap forward that will bring it to the first voxel beyond the uniform region, as shown in <ref type="bibr">Figure 3</ref>. This approach saves the need to perform neighbor search which is the most time consuming operation in rendering hierarchical data structures.</p><p>When a volume consists of one or few objects surrounded by empty space, a common and simple method to provide space-leaping uses bounding-boxes. The PARC (Polygon Assisted Ray Casting) method [ 11 strives to have a better fit by allowing a convex polyhedral envelope to be constructed around the object. These methods however are not suitable for a scene occupied by many small objects and 2. In the case of multiple or concave objects, these two phases may repeat. in most cases require user intervention for the specification of a tight bounding envelope. It is obvious that the empty space does not have to be sampledit has only to be crossed as fast as possible. Therefore, we have proposed [ 121 to utilize a crude but fast line algorithm in the empty space (e.g., integer-based 26connected line algorithm) and another, more accurate (e.g., integer-based 6 connected or 3D DDA floating point line algorithm), in the vicinity and interior of objects. The effectiveness of this approach depends on its ability to efficiently switch back and forth between the two line algorithms and its ability to efficiently detect the proximity of occupied voxels.</p><p>We have shown elsewhere the details of how to switch between line connectivities [12] and how to efficiently switch between any number of line algorithms when employing the template-based ray casting algorithm <ref type="bibr">[ 131.</ref> Sensing the proximity of data voxels can be achieved by having a volume buffer called d7 which is maintained in addition to the data volume. This buffer contains, in each voxel, the maximum value in the immediate neighborhood of the correspondin voxel in the data volume [5]. Rays are traced in the d volume. When a value is encountered that is not transparent the ray algorithm switches into a more accurate algorithm and continues its traversal in the data volume. An alternate solution, called the vicinity-voxels approach, does not require any additional space. It is based on surrounding the occupied voxels with a one-voxeldeep "cloud" of flag-voxels. That IS, all empty voxels neighboring an occupied voxel are assigned, in a prepro-B cessing stage, a special "vicinity flag". As shown in <ref type="figure" target="#fig_3">Figure  4</ref>, each ray rapidly traverses the empty space (dashed lines in <ref type="figure" target="#fig_3">Figure 4</ref>) until it encounters a vicinity voxel (light-grey squares in <ref type="figure" target="#fig_3">Figure 4</ref>) which flags the need o switch to the more accurate ray traversal algorithm (solid lines in <ref type="figure" target="#fig_3">Figure  4</ref>). Encountering an empty voxel later (un-occupied and not carrying the vicinity flag) can signal a switch back to the rapid traversal algorithm which is employed in the empty space. The proximity-clouds method [4, 161 is based on the extension of this idea even further. Instead of having a onevoxel-deep vicinity cloud this method computes, in a preprocessing stage, for each empty voxel, the distance to the closest occupied voxel, as shown in <ref type="figure" target="#fig_4">Figure 5</ref> where the isovalue clouds are displayed in different shades of gray. When a ray is sent into the volume it can either encounter an occupied voxel, to be handled as usual, or a "proximity voxel" carrying the value n. This suggests that the ray can make a leap forward that skips n voxels, while being assured that there are no objects in that range of voxels. <ref type="figure" target="#fig_4">Figure 5</ref> shows the sample points (small squares) for some rays. The effectiveness of this algorithm is obviously dependent on the ability of the line traversal algorithm to efficiently jump arbitrary number of steps <ref type="bibr">[4]</ref>. We have shown elsewhere how to efficiently accommodate this feature in the templatebased ray casting algorithm <ref type="bibr">[ 131.</ref> All existing space-leaping methods require 3D preprocessing that needs to be repeated for any change in the volume data. They rely on a special ray traversal algorithm (octree, flat pyramid, proximity clouds), or require additional 3D memory (max volumes). In the next section we present a new method for applying space-leaping in the case of volume animation. Our method does not require additional 3D memory or 3D preprocessing while supporting any ray traversal algorithm and sampling scheme. </p><formula xml:id="formula_1">L ......... 1 .......... I ......... 1 ,.._._.._.I .......... I .......... i .......... i .......... I .......... 1 ......... 1 .......... 1 .......... I ......... i .......... i .......... U</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4: Employing Space-Leaping when Rendering a Sequence of Images</head><p>Our method is based on the observation that when generating a sequence of images, such as in animation, there is a great deal of coherency between any pair of consecutive images: usually both images are almost identical and only a few rendering parameters slightly change between the two. This observation have been exploited in surface-based ray tracing (see Section 1) but not so in volume rendering. The attribute we chose to store and utilize is the coordinates of the first non-transparent voxel encountered by the ray emitted at each pixel. This information is stored in a special Cbuffer (for Coordinates-buffer) when the first image is generated. When the next image is to be rendered we should be able to slightly transform the C-buffer data and reuse it to our advantage. We described, in Section 1, an example dealing with a change in material attributes. Another example can be found in the case where the second image is to be rendered using the same viewing parameters as the first one, except for a change in the position, color, or intensity of a light source. Then, we can also use the C-buffer data and start following the ray from the coordinates specified there as opposed to starting at the volume boundary. <ref type="figure">Figure 6</ref> shows the path rays take through the volume when employing our method. </p><formula xml:id="formula_2">1 I t i 1 I l l i l l l i l l l i l l l l ]</formula><p>Figure 6: The passage of rays through the grid when employing space-leaping.</p><p>We now show how the C-buffer can be utilized to spaceleap when producing a more challenging set of imagesthose that involve rotation of the world space. First, the Cbuffer coordinates are transformed by the desired rotation. That is, the coordinates <ref type="bibr">[x,y,z]</ref> stored in the C-buffer[i,j] are mapped to C-buffer[i', j ' ] . However, it could be the case that several coordinates values are mapped by the transformation to the same C-buffer entry. It can also happen that a C-buffer entry remains empty, that is, without being mapped any coordinate value. In the second case we simply conclude that there is no space-leaping information and a ray traced from that pixel has to start stepping from the volume boundary. The first case is more complex although one would think that depth-sorting (e.g., using Z-buffer) between the coordinates mapped to the same pixel could provide the most conservative and safe estimate. Unfortunately, although being correct in principle and in many cases, this approach fails due to discretization artifacts. For example, in the case of three objects, A, B, and C that project to the same area on the screen, such that, A hides B and B hides C. Due to discretization artifacts some pixels are being mapped coordinates values that belong to objects B and C only (i.e., at that pixel there is a "hole" in A). This will cause the depth-sorting mechanism to conclude that the first visible object at this pixel is B. This will drive it to commence following the ray near object Baltogether skipping over object A.</p><p>Our solution to this problem (see <ref type="figure" target="#fig_6">Figure 7)</ref> is to employ the following principle, suggested by <ref type="bibr">Gudmundsson and Randen [6]</ref>: Let us assume that a voxel's coordinates <ref type="bibr">[x,y,z]</ref> are_st_ofed in C-buffer <ref type="bibr">[i,j]</ref> and the coordinates of the voxel <ref type="bibr">at [x,y,z]</ref> are stored in C-buffer <ref type="bibr">[i,k]</ref> such that k &gt; j , as depicted in <ref type="figure" target="#fig_6">Figure 7(a)</ref>. Now let us assume that we perform a positive rotation around the X-axis, that is, the coordinates of both voxels are mapped to entries in the ith C-buffer column -j ' and k'. If we observe that the relative position between <ref type="bibr">[x,?,~]</ref> and <ref type="bibr">[&lt;,&gt;,i]</ref> has changed (i.e., j ' t k ' ) then the point <ref type="bibr">[x,y,z]</ref> is potentially hidden as shown in <ref type="figure" target="#fig_6">Figure 7(b)</ref>. The extension of this logic for the other cases of rotation (positive and negative rotations around the X and Y axis, and combinations thereof), is straightforward. Another possible solution involves a variation of the splatting idea [l 11. When a C-buffer coordinates value is transformed it is not written to the target pixel only. Instead, it is written into a neighborhood of C-buffer pixels. In each pixel, a Z-buffer mechanism is activated for each coordinates value that is mapped to it. The size of the footprint must guarantee that the phenomenon described above will not occur, that is, it must guarantee that all pixels in the area covered by an object are mapped with coordinate values of that object. Unlike the previous solution, this one can also correctly handle viewing changes in perspective transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b)</head><p>As a result of the previous step, the C-buffer entry at [i,j] is either empty or it contains some object-space coordinate. In the first case we now have to find and assign Cbuffer <ref type="bibr">[ij]</ref>  <ref type="figure">with the point [x,y,z]</ref> where the ray from [i,j] enters the volume. Finally, we shoot rays from each and every pixel on the screen in the same way as any other ray casting method does. However, we start our volume traversal from the coordinates found in the appropriate C-buffer entry. The actual ray traversal mechanism and the sampling method used is really of no interest to the space-leaping mechanism described here. The only value of interest is a potential update of the C-buffer. The ray traversal algorithm is assumed to return the object-space coordinates of the first non-transparent voxel it encountered. This value replaces the information in the C-buffer.</p><p>Another difficulty arises when the projected objects occupy a small portion of the image. In other words, most pixels are assigned the background color. Since the background does not consist of any object, there will be no coordinates values in the corresponding C-buffer entries. Therefore, for any background pixel an expensive and fruitless ray traversal must be launched. One solution is to employ bounding boxes or PARC (see Section 3) in conjunction with space-leapingan approach that requires preprocessing. Alternatively, one could shoot rays only near the silhouettes of the projected objects [6]a method with limited accuracy. Our solution is to regard the back faces of the volume as objects. When a ray exits the volume it returns the coordinates of the exit point. These points are then treated just like any other coordinate value stored in the Cbuffer with the only exception that these coordinates should be invalidated and removed from the C-buffer when the face they belong to becomes a front face. The results, reported in Section 5, demonstrate the performance of this solution. <ref type="figure" target="#fig_8">Figure 8</ref> shows the pseudo code for the space-leaping assisted rendering. The function shoot-ray receives the starting point of <ref type="figure">the ray [u,v,w]</ref> and returns the coordinates of the first non-transparent voxel it encountered. If no such voxel was found, shoot-ray returns the coordinate of the point where it exited the volume. This function is assumed to compute the ray's final color and display it on the image at pixel [i,j]. Transform receives a 3D coordinates value and rotates it. The T-buffer data structure is a temporary storage that can store multiple coordinates in each of its entries. The function elim-hidd goes over these coordinates and eliminates the potentially hidden ones, employing one of the methods described above.</p><p>Compared to the method proposed by <ref type="bibr">Gudmundsson and Randen [6]</ref>, which suffers from image degradation and can only support first-opaque ray traversal, our method does not accumulate any image degradation since the values in the C-buffer are refreshed in each view. Our method can support semitransparent rendering as well as maximum-value and weighted summation rendering paradigms. Gudmundsson and Randen deal only with rotation and their method cannot support changes to shading and material parameters while our method includes these capabilities. Finally, as described above, we provide an accurate and efficient solution to the background problem. Compared to existing space-leaping algorithms (see Section 3) our method is the only one that does not require 3D preprocessing.</p><p>--Initializationrender theJirst image in the sequence.</p><p>foreach pixel (ij) in the screen do <ref type="bibr">[u,v,w</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5: Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scene density</head><p>The software of the space-leaping algorithm was implemented and evaluated on Hewlett Packard 9000/700 series workstations. We have implemented the solution depicted in <ref type="figure" target="#fig_6">Figure 7</ref> while regarding the volume back faces as objects, as describe in Section 4.</p><p>In order to verify the performance of the software implementation the following experiment was conducted: a number of random sized boxes were voxelized at random locations in the volume. Measurements were taken for rendering an empty scene, scenes with 2 boxes, 5 boxes, 20 boxes, 90 boxes, and a completely occupied scene, where rays stop immediately. In <ref type="table" target="#tab_0">Table 1</ref> we have listed these by the percentage of space occupied by the boxes -0%. 25%, 44%, 62%, 80% and loo%, respectively. We note that the "random objects" type of scenes represents the worst case scenario for space-leaping algorithms such as bounding boxes, PARC, proximity-clouds, and adaptive screen and ray sampling, since objects are uniformly scattered in 3D and 2D spaces. Rendering time was measured by averaging the time of multiple projections from arbitrary directions when viewing a 1283 volume. The leftmost column in <ref type="table" target="#tab_0">Table  1</ref> shows the time it takes to render the image without any space-leaping acceleration. The second leftmost column in <ref type="table" target="#tab_0">Table 1</ref> shows the average time it takes to render one image in an anibmation sequence generated by incrementally rotating by 1 along one axis. The rest of theocoiumns !how the times when the incremental rotation is 2 , 4 and 8 .</p><p>Ray 1 lo , s~e~~~g , 8o 1 casting To better understand the source of speedup, we provide <ref type="table" target="#tab_1">Table 2 and Table 3</ref> that show the number of rays and total number of voxels visited by the rays, when rendering without space-leaping (first column) and when rendering a sequence of images with various rotation increments. <ref type="table" target="#tab_1">Table 2</ref> shows that, as expected, the number of rays emitted by the ray-casting algorithm is constant while our method, in the worst case, shoots rays at only 9.1 % of the screen-pixels.</p><p>The empty case (first line) and the full case (last line) are the easiest to handle and rays are needed only in areas of discrete holes. The amount of new rays needed truly depends on the scene, as shown in <ref type="figure" target="#fig_6">Figure 7</ref> and demonstrated in the case of 2 boxes in <ref type="table" target="#tab_1">Table 2</ref>. As the number of boxes grows, rays tend to bump into a box relatively soon, that is, since most of the empty space is now occupied, the average ray length decreases. This is obvious in the case of ray-casting in which, although the number of rays remains constant, the number of voxels traversed shrinks. In fact, the number of visited voxels in the 90 boxes scene (80% density) is 13% of that visited in the emptyscene case. A similar phenomenon happens in the spaceleaping algorithm, where the number of voxels traversed in the 90 boxes scene is 13%-17% of that traversed in the worst case of the 2 boxes scene (and 20%-26% of the voxels traversed in empty scene case). It should be observed that although the number of voxels traversed bx the space-leaping apprzach varies between 1% (in the 1 case) and 12% (in the 8 case) of the number traversed by the ray casing algorithm, the speedup gained is only a factor of 10 and 2, respectively. This is because the space-leaping approach consists of the phase devoted to the transformation of the Cbuffer and the elimination of potentially hidden coordinates. This phase requires 0.47 seconds, a time that is constant as long as the image size does not change. When rendering small volumes the time spent in this phase is rath-er significant compared to the time required by the whole algorithm. When the volume resolution is increased (increasing the number of traversed voxels by a cubic function), the C-buffer mapping (which increases by a square function only) is expected to play a much smaller role. We have compared the algorithm performance on a CT dataset of a human head in 12g3 resolution and 2563 resolution. The volume is 16% occupied and in the 1283 case it performed as expected from <ref type="table" target="#tab_0">Table 1</ref>, showing 3-4 speedup. In the 2563 case the advantage of skipping empty space is even more obvious and a speedup of 4-8 was achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6: Summary</head><p>In this paper we presented a method that accelerates the process of generating a sequence of volume-rendered images. Our method is based on exploiting coherency between images to extract information that is used to shorten the passage of rays through the volume. This is achieved by computing a starting point for the ray traversal that is closer to the rendered objects in the volume, thereby saving the need to traverse the empty space. This space-leaping technique can support any ray-traversal paradigms and can be combined with other acceleration techniques. We have also demonstrated the utility of our method by providing test results which show a minimum factor of two speedup and up to an order of magnitude speedup, without any degradation in image quality. We believe that this algorithm can be incorporated into most ray-casting-based volume rendering systems and provide significant speedup with no quality or performance penalties.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The principles of ray-casting. The #D volume is represented by a 2D grid while the 2D screen is depicted by a 1D line. At a screen pixel (a) a ray is traversed and sampled (d) from its entry to the volume (b) to its exit point (c). Rays are emitted only from pixels that are in the region affected by volume (e)-(f).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure</head><label></label><figDesc>Figure 2: The passage of rays through a semitransparent object. Rays are assumed to accumulate full opacity, and stop, after sampling two voxels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The passage of rays through a 16x16 flat pyramid. The passage of some rays, that is, the voxels these rays visit, are shown by the small squares.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The passage of rays through the grid when employing adaptive ray traversal where the dashed line represents a fast ray algorithm and the light-grey areas denote voxels with proximity flag turned on.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>The passage of rays when employing proximity clouds. The iso-value clouds are shown by different shades while the sample points along some rays are shown by the small squares.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>........~..I .... i. ......... ~ ..I............I.. r_ .-..__....I.....</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Removing potentially hidden coordinates from the C-buffer. Since the relationship between the two voxels in (a) changed, it serves as an indicator that the voxel at [x, y, z ] IS potentially hidden (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>] := ray-enter-vol([i,j]); Cbuffer[ij] := shoot-ray([u,v,w], [ijl); end-for ; foreach change that requires re-rendering do if change is rotation then --. Transform into a temporary buffer foreach pixel (ij) in the screen do [i ', j ' ] := Transform(C-buffer[ i,j]); add-coord(C-buffer[i,j], T-buffer[i ', j ' ] ) ; end-for; --Eliminate potentially hidden coordinates. foreach pixel (ij) in the screen do C-buffer[ij] := elim-hidd(T-buffer[ij]); --Assign to empty entries the point where if C-buffer[i,j] is empty then end-if; the ray enters the volume. C-buffer[ij] := ray-enter-vol([i,j]); end-for; end-* --Use the C-buffer to render the next image. foreach pixel (ij) in the screen do end-for; C-buffer[i,j] := shoot-ray(C-buffer[i,j], [ij]); end-for;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Pseudo code for the space-leaping based ray-casting algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 : CPU time (in seconds) of ray casjing a ro- tation sequence of various scenes in 128 resolu- tion.</head><label>1</label><figDesc>I .56 I .61 I .69 I .91 I</figDesc><table><row><cell cols="2">25% 44% 0% 1 5.79 62% 4.19 3.25 1.88</cell><cell>.63 .58 .56</cell><cell>.69 .66 .61</cell><cell>.8 .76 .66</cell><cell>1.12 1.04 .79</cell></row><row><cell>80%</cell><cell>1.34</cell><cell>.56</cell><cell>.57</cell><cell>.58</cell><cell>.61</cell></row><row><cell>100%</cell><cell>.94</cell><cell>.55</cell><cell>.56</cell><cell>.55</cell><cell>.52</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 : Number of rays cast when volume-render- ing without and with space-leaping.</head><label>2</label><figDesc>100% 1 27842 1 304 I 489 I 787 I 1479 1 Average 1 27842 1 413 1 760 1 1277 1</figDesc><table><row><cell cols="6">I -I I O c -I 27842 I 333 I 561 I 949 I 2034 125% 1 27842 1 : l 3: i !X5; 1 1671 1 3317 44% 27842 1018 1582 3243</cell></row><row><cell>62%</cell><cell>27842</cell><cell></cell><cell></cell><cell>1488</cell><cell>2951</cell></row><row><cell>80%</cell><cell>27842</cell><cell>426</cell><cell>727</cell><cell>1182</cell><cell>2228</cell></row><row><cell cols="6">I 2542</cell></row><row><cell></cell><cell>100%</cell><cell cols="4">1.5% 2.7% 4.6% 9.1%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 : Number of voxels (in thousands) visited when rendering without and with space-leaping.</head><label>3</label><figDesc></figDesc><table><row><cell>Scene density</cell><cell>Ray casting</cell><cell>lo</cell><cell cols="3">Space Leaping 2' 4' I 8'</cell></row><row><cell>0%</cell><cell>1481</cell><cell>15</cell><cell>27</cell><cell>51</cell><cell>128</cell></row><row><cell>25%</cell><cell>1052</cell><cell>23</cell><cell>42</cell><cell>78</cell><cell>190</cell></row><row><cell>44%</cell><cell>838</cell><cell>14</cell><cell>36</cell><cell>65</cell><cell>171</cell></row><row><cell>62%</cell><cell>385</cell><cell>9</cell><cell>19</cell><cell>38</cell><cell>95</cell></row><row><cell>I 80%</cell><cell>I 197</cell><cell></cell><cell></cell><cell>I 12</cell><cell>I 26</cell></row><row><cell>100%</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Kim Ciula, David Ebert, and Raghu Machiraju for reading the manuscript and contributing use-ful suggestions. This work has been supported by the National Science Foundation under grant CCR-92 1 1288.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards a Comprehensive Volume Visualization System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Avila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Sobierajski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Kaufman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Visualization &apos;92</title>
		<meeting>Visualization &apos;92<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992-10" />
			<biblScope unit="page" from="13" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploiting Temporal Coherence in Ray Tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">W</forename><surname>Calvert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Graphics Interface&apos;90</title>
		<meeting>Graphics Interface&apos;90</meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="196" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spatio-Temporal Coherence in Ray Tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">W</forename><surname>Calvert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Graphics Interface&apos;91</title>
		<meeting>Graphics Interface&apos;91</meeting>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="101" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Proximity Clouds -An Acceleration Technique for 3D Grid Traversal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shefer</surname></persName>
		</author>
		<idno>FC 93-01</idno>
		<imprint>
			<date type="published" when="1993-02" />
		</imprint>
		<respStmt>
			<orgName>Department of Mathematics and Computer Science, Ben Gurion University of the Negev</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast Algorithms for Volume Ray Tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Danskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 1992 Workshop on Volume Visualization</title>
		<meeting>1992 Workshop on Volume Visualization<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992-10" />
			<biblScope unit="page" from="91" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Incremental Generation of Projections of CT-Volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gudmundsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Randen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Visualization in Biomedical Computing</title>
		<meeting>the First Conference on Visualization in Biomedical Computing<address><addrLine>Atlanta, GA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990-05" />
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Display of Surfaces from Volume Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics and Applications</title>
		<imprint>
			<date type="published" when="1988-05" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="29" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Volume Rendering by Adaptive Refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Visual Computer</title>
		<imprint>
			<date type="published" when="1990-02" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="2" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hierarchical Data Structures and Algorithms for Computer Graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Webber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="59" to="75" />
			<date type="published" when="1988-07" />
		</imprint>
	</monogr>
	<note>Part</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient Hybrid Rendering of Volume data and Polygons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Van Walsum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J S</forename><surname>Hin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Versloot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Post</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second Eurographics Workshop on Visualization in Scientific Computing</title>
		<meeting><address><addrLine>Delft, Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Footprint Evaluation for Volume Rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>I] Westover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="367" to="376" />
			<date type="published" when="1990-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Volumetric Ray Tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yagel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<idno>TR 91.01.09</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Science, SUNY at Stony Brook</title>
		<imprint>
			<date type="published" when="1991-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient Methods for Volume Graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yagel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Doctoral Dissertation</title>
		<imprint>
			<date type="published" when="1991-12" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, SUNY at Stony Brook</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Discrete Ray Tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yagel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kaufman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics &amp; Applications</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="19" to="28" />
			<date type="published" when="1992-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="153" to="157" />
			<date type="published" when="1992-09" />
			<publisher>Blackwell Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Acceleration of Ray Casting Using 3D Distance Transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zuiderveld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H J</forename><surname>Koning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Visualization in Biomedical Computing</title>
		<meeting>Visualization in Biomedical Computing</meeting>
		<imprint>
			<publisher>SPIE</publisher>
			<date type="published" when="1808-10" />
			<biblScope unit="page" from="324" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yagel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kaufman</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
