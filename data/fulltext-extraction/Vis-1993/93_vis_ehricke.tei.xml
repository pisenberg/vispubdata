<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Vision Camera: An Interactive Tool for Volume Data Exploration and Navigation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Heino</forename><surname>Ehricke</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universi t at Tubingen Wilhelm-Schickard-Insti t ut fur Informati k Tubingen</orgName>
								<address>
									<country>F.R. G</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Daiber</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universi t at Tubingen Wilhelm-Schickard-Insti t ut fur Informati k Tubingen</orgName>
								<address>
									<country>F.R. G</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Vision Camera: An Interactive Tool for Volume Data Exploration and Navigation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>In this paper we focus on one of the key problems of Scientific Visualization, the object recognition dilemma. The necessity to pre-interpret application data in order to classify object surface voxels prior to rendering has prevented many visualization methods from becoming practical. W e propose the concept of vision by visualization which integrates Computer Vision methods into the visualization process. Based on this, we present the vision camera, a new tool allowing for interactive object recognition during volume data walkthroughs. This camera model is churucierized by a flexible front-plane which, under the control of user-specified parameters and image features elastically matches to object surfaces, while shifted through a data volume. Thus, objects are interactively carved out and can be visualized by standard volume visualization methods. Implementation and application of the model are described. Our results suggest that by the integration of human and machine vision new perspectives for data exploration are opened up.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In Scientific Visualization (SV) a variety of applications focus on the analysis of three-dimensional scalar or vector fields, also known as volume data. Examples are tomographic image stacks in medicine, seismic data in geophysics and simulation vector fields in thermodynamics. The involvement of a variety of disciplines has led to a great number of volume visualization techniques. In principle they fall into two categories:</p><p>e Segmentation-based methods with an intermediate object representation.</p><p>e Methods operating on the original or a contrastenhanced datavolume.</p><p>The former rely on a delineation of interesting object surfaces and their reconstruction e.g. by surface patches <ref type="bibr">[l]</ref>. These geometrically well defined surface models are then rendered with standard surfacegraphics methods. Since the automatic recognition of object surfaces in many situations cannot be performed with sufficient reliability, these techniques often lack practicality. Volume-based approaches try to circumvent this problem by avoiding a binary presegmentation and by integrating part of the recognition process with the actual rendering. The idea is, that the user may interactively identify objects of interest by manipulating rendering parameters, such as a density threshold or the minimum number of successive voxels exceeding the threshold <ref type="bibr" target="#b1">[2]</ref>, or the shape of an opacity function <ref type="bibr" target="#b2">[3]</ref>. Regarding the complexity of volume datasets which are used in the disciplines mentioned above, it becomes obvious that by the use of these local voxel-based models the discrimination between relevant and obscuring voxels is often not possible. Thus, visualization results often fail to reveal relevant properties of the datasets. So again, we are confronted with the object recognition problem. One way to get out of this dilemma is to transfer the main steps of the recognition process t o the user of a visualization system. This seems rather promising, since man's visual system has reached a high degree of perfection. The idea has been expressed repeatedly by researchers engaged in SV, but nevertheless the literature lacks precise proposals for its implementation. In this paper we describe an extension of the visualization pipeline, allowing for the merging of the disciplines SV and Computer Vision (CV). In CV over certainly more than twenty years now, a variety of model-driven approaches for the interpretation of image data have been proposed <ref type="bibr" target="#b4">[4]</ref>. Their primary deficiency, perhaps is the lack of interfaces in order t o easily adapt the models t o new application areas. The goal of the concept proposed in this paper is to provide the user of a visualization system with vision tools for interactive data exploration. On the basis of this concept we propose a new method, the vision camera (VisiCam). In short, VisiCam is a camera model, which during walkthrough simulations matches its frontplane to object surfaces, thus carving out objects and ignoring obscuring structures. The model parameters are interactively adapted by the user according to the rendering results currently presented to him on the workstation screen. In this way, object recognition and navigation through the dataset is supported by the system, but controlled by the user. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Vision by visualization</head><p>"Visual perception is the relation of visual input to previously existing models of the world" <ref type="bibr" target="#b5">[5]</ref>. The primary goal of SV is the conversion of unstructured input data into a representation which allows visual perception of relevant information. This stepwise analysis process has led to a conceptual model for scientific visualization systems, the visualization pipeline <ref type="bibr" target="#b6">[6]</ref>. Unfortunately this model is very rough and does not adequately address one of the key problems in SV, the object recognition dilemma. Most visualization methods which have been proposed so far, require a pre-interpretation of the input data in order to discriminate between objects of interest and background voxels. This is not an easy task and therefore, highly complex object recognition approaches have been described in the CV literature <ref type="bibr" target="#b4">[4]</ref>. <ref type="figure">Figure l</ref> presents the primary elements of object recognition systems. In CV digital models are fitted to application data or features which have been extracted from them. In an iterative manner features of higher and higher semantical level are derived leading finally t o a symbolic description of the scene. Since the models are unflexible as compared to mental models of a human being, computer vision systems are designed for very specific applications. Dependance on specific types of input data and scene objects is an undesirable feature for visualization systems. One way to get out of this dilemma is to interactively adapt the digital models used in CV to the mental models a visualization system user applies when exploring volume data. This means integration of the CV and SV pipelines, leading to the concept of vision by visualization. The visualization component of such a system allows for immediate visual feedback when modifying model parameters and thus, recognition of relevant data properties is performed partly by the user, partly by the system. If we approach SV with these aspects in mind new perspectives open up, which can be expressed by questions like:</p><p>Which of the CV methods, which originally have been developed with the goal of automatic (noninteractive) object recognition, could be interactively controlled?</p><p>How can we extend CV methods by interaction and visualization interfaces?</p><p>Having in mind the potential of interactive visualization systems, what novel CV methods can we imagine? CV approaches may be introduced not only at the application data level (data pre-interpretation) , but the vision by visualization concept addresses all levels of the SV pipeline. In this paper we focus on the map &amp; render element and propose an original extension, the vision camera (see <ref type="figure" target="#fig_1">fig. 2</ref>).</p><formula xml:id="formula_0">1 mppliution d J . 1 I \ (A,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The vision camera</head><p>One of the key elements of explorative volume data analysis is a freely moveable camera which allows walking through the scene. It has been demonstrated that by walkthrough simulations exploration of volume datasets may substantially be enhanced <ref type="bibr" target="#b8">[7]</ref>. Nevertheless the method has seldom been used. This has mainly two reasons:</p><p>1. Moving through a scene of unlabeled greyvalues may help to circumvent obscuring structures and approach internal objects, but navigation is only possible if relevant objects can be delineated in the projection images by the user. A prerequisite is the ability of the visualization system t o dicriminate, at least roughly, between object and background voxels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Volume rendering approaches are computationally expensive and therefore a frame rate which would be sufficient for interactive work cannot be achieved on standard workstations.</p><p>With respect to the latter argument, new hardware architectures for volume rendering are currently under investigation, also in our institution [8], and we can expect graphics hardware manufactureres to open up their eyes to this new class of algorithms and their perspectives. The vision camera approach focuses mainly on the navigation and recognition problem, addressed by the first argument. Based on the vision by visualization concept, with VisiCam we propose a tool for data exploration with integrated vision capabilities.</p><p>The idea is t o use a flexible camera front-plane which adapts t o surfaces within the dataset. The physical model consists of a rectangular rubber frame which carries an elastic grid. The visualization system user pushes the camera through the scene and the flexible grid adapts to object surfaces via an elastic matching method (see <ref type="figure" target="#fig_2">fig. 3</ref>). 3D visualization is achieved by ray-casting, using the camera front-plane as a clipping plane. Any of the available illumination methods, e.g. semi-transparent volume rendering, threshold-guided greylevel gradient shading or maximum intensity projection may be used. Since the front-plane adapts to surface structures, object discrimination can be easily carried out during rendering. When the user pushes the camera further into the scene the rubber grid finally gets off the current object surface and tends to become planar again. When the next surface is met the adaptation process becomes active again. For the implementation of this physical model we use elastic matching, a technique from the CV area. The method has been widely used for the adaptation of geometric models to actual image data, thus recognizing objects and their boundaries. Our initial geometric model is a planar and regular grid which is fixed to a flexible frame, located at the initial camera position within the volume. The goal is to match this model to relevant surface structures in a stepwise manner. During one step each node of the grid is investigated and one of the alternatives, (1) shift one voxel in camera motion direction, (2) shift one voxel in opposite camera motion direction, (3) keep current position in 3D space, is selected. In order to arrive at a decision, which of the three possibilities to carry out, we use a cost function. This is evaluated for each of the three alternative node positions and the minimum cost position is chosen. Similar to the approach proposed in <ref type="bibr" target="#b10">[9]</ref> the costs are determined according to the following formula:</p><p>where:  The topology funciion Ti prevents the flexible rubber grid from collapsing, dragging it to a more or less planar shape. The positions of the eight orthogonal neighbours of a node (two in each of the four directions) on the grid are averaged and the distance between the nodes position and the neighbour's average is calculated. From this value the minimum distance within the three alternative node positions is subtracted and the resulting value is multiplied by the maximum distance. By the weighting factor bi the motion speed of the flexible frame is augmented, as compared to the speed of the grid nodes: bi &gt; 1.0 for nodes on the frame, bi = 1.0 for other nodes. So, when the user pushes the camera in a certain direction the force is transmitted mainly to the frame which passes it to the flexible grid. After each step the eyepoint(camera) position is updated, based on the average node position within the grid. When an object surface is met, the camera motion speed is decreased by the influence of the image feature function and the flexible grid as well as the rubber frame adapt to it. The user has to increase the motion force to penetrate the surface and drag the front-plane into the object where other surfaces may be detected. In this way camera motion is controlled by the user as well as image data properties.</p><formula xml:id="formula_1">C i ( z , y, z ) is</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We have evaluated our approach for volume data analysis in medicine and biology. Since a camera user interface with four parameters for the adjustment of the front-plane is hardly acceptable for a visualization system user, we have reduced them t o a toggle button and a slider. The toggle button allows switching between high and low flexibility of the front-plane, setting the weighting coefficient of the topology cost function to 0.2 or 0.8 respectively. The slider may be regarded as an accelerator by which the force of mction is controlled. If it is increased, the weighting ccefficient of the driving force function rises while at the same time that of the image feature function is diminished. In this way a front-plane which has converged to an object surface may be triggered t o penetrate it and move further into the object or out of it respectively. <ref type="figure" target="#fig_4">Figure 4</ref> presents three stages during the deformation of the front-plane grid, when the camera approaches the skin surface within a MRI dataset of a human head. In <ref type="figure" target="#fig_5">fig. 5</ref> the analysis of brain surface structures is demonstrated. Approaching the head from outside of the dataset the front-plane ajusted and converged to the skin surface after about 100 iterations. Then the motion force was set to its maximum, switching off the influence of the image feature function. The camera was moved for another 10 steps (voxels) further into the dataset, while keeping the front-plane's shape fixed. Then, the adaptation process was triggered again by increasing the portion of the image feature function. This allowed the flexible grid to get adapted to the brain surface, which is illustrated by applying Z-buffer shading to grid positions. In the left image the original greyvalues a t the grid node positions are depicted. For a better orientation we rendered the remaining parts of the dataset with the initial camera position, using greyvalue gradient shading and thresholding. <ref type="figure">Figure 6</ref> illustrates the analysis of a rat-heart within a noisy MRI-microscopic dataset. The left image presents the result of applying greylevel-gradient shading directly at grid node positions after adaptation of the flexible grid to the heart surface. The shape of the flexible grid is illustrated by the middle and right images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Summary and conclusion</head><p>In this paper we have focused on one of the key problems of SV, the object recognition dilemma. We    <ref type="bibr" target="#b6">6</ref>: Visualization of a rat-heart from noisy MRI-microscopic data: Greylevel-gradient shading directly at grid nodes (left); two views illustarting the grid shape (middle and right).</p><p>have proposed the concept of vision by visualization, which integrates CV methods with the visualization process. Its primary goal is to get rid of highly complex data pre-interpretation steps and transfer difficult object recognition tasks to the user of a visualization system. Thus, an integration of machine and human vision becomes possible. Based on this concept we have proposed a new tool allowing for interactive object recognition, the vision camera. VisiCam is a camera model with an adaptive front-plane. By the use of elastic matching algorithms the camera matches to object surfaces under control of the system user. We have described an implementation of this camera model and its application t o visualization problems in biology and medicine. In spite of our promising results some problems are waiting for solutions:</p><p>1. The cost function possesses a great number of parameters which have to be tuned to a special recognition situation.</p><p>2. If two or more adjacent object surfaces exist the front-plane may not be able to match to only one of them, but may be attracted by the others.</p><p>Regarding the first argument, experience is necessary to tune the four parameters of the cost function to a dataset and a special recognition situation. Here, an intuitive user interface with realtime response to parameter modifications seems to be the best solution. By the pre-computation of the gradients used by the image feature function and the reduction of the number of grid nodes by an interpolation strategy, we are currently trying to achieve interactive feedback on an SGI Indigo. The reason for the second problem may be identified in the simplicity of the cost function, which uses the strength of the greyvalue gradient as the only image feature component. Our preliminary experiments demonstrate that by the introduction of additional components, e.g. the greyvalue gradient direction, a distinction between adjacent surfaces (e.g. decreasing versus increasing greylevel) is often possible.</p><p>We have demonstrated that the vision by visualization concept leads to the development of new interaction tools in the area of SV. VisiCam is only one possibility. We are convinced that with this concept in mind a variety of new techniques will be proposed, bringing together machine and human vision.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Principles of Computer Vision demonstrating the concept of machine perception.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The Scientific Visualization pipeline extended by Computer Vision concepts. The diagram illustrates the integration of the vision camera.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The VisiCam model and the way it approaches an object surface.The front-plane is used as a clipping plane.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>f(Aw) = 0. 5 -0 . 5 A~where</head><label>55</label><figDesc>Aw is the positive or negative voxel distance of the point to be evaluated from the current node position. By the image feature function I ( z , y, r ) nodes are attracted to object surfaces. I ( z , y , z ) has the form 1.0 -G(z, y, z ) , where G ( x , y, z ) is a local greyvalue gradient, normalized to 1.0. We use the Zucker-Hummel operator in a 3*3*3 voxel neighbourhood. In order to get rid of local minima problems we previously apply a median filter with a kernel of 5*5*5 voxels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Stages of the front-plane deformation process when approaching a skin surface (MRI data): The flexible grid adapts very wellleven t o complex surface structures, like nose and eyes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Shifting the camera into an MRI head dataset: rectangular window shows original greyvalues at grid nodes (left); The Z-buffer shaded image of the front-plane clearly reveals brain surface structures (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure</head><label></label><figDesc>Figure 6: Visualization of a rat-heart from noisy MRI-microscopic data: Greylevel-gradient shading directly at grid nodes (left); two views illustarting the grid shape (middle and right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>the cost of node i at position (x, y, z ) , Di is a driving force which pushes nodes in camera motion direction, I ( z , y, z ) is an image feature function, Ti is a toplogy evaluation function, ao, al, a?, bi are weighting factors.</figDesc><table><row><cell>The driving force Di is a term responsible for the push-</cell></row><row><cell>ing of a node in camera motion direction. For linear</cell></row><row><cell>motion we use a simple ramp function of the form</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1070">-2385/93 $3.00 0 1993 IEEE</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Marching cubes: A high-resolution 3D surface construction algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Lorensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Cline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="163" to="169" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Rendering tomographic volume data: Adequacy of methods for different modalities and organs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Hoehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bomans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pommert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riemer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Tiede</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wiebecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Imaging an Medicine</title>
		<editor>K.H. Hoehne, H. Fuchs, and S.M. Pizer</editor>
		<meeting><address><addrLine>Berlin; New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1990" />
			<biblScope unit="page" from="197" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Display of surfaces from volume data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>I E E E C O M P</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Graph. Appl</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="29" to="32" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Computational strategies for object recognition. A CM C o mputing Surveys</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Suetens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Hanson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="5" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Brown</surname></persName>
		</author>
		<title level="m">Computer Vision</title>
		<meeting><address><addrLine>Englewood Cliffs</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Haber</surname></persName>
		</author>
		<title level="m">Visualization in engineering mechanics: Techniques, systems and issues. Siggraph&apos;88</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Visualization Techniques in Physical Science</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
	<note>Course Notes</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Interactive 3D-graphics workstations in stereotaxy: Clinical requirements, algorithms and solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-H</forename><surname>Ehricke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Daiber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sonntag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Strasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lochner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Schad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Lorenz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visualization in Biomedical Computing</title>
		<editor>R.A. Robb</editor>
		<meeting><address><addrLine>Bellingham</addrLine></address></meeting>
		<imprint>
			<publisher>SPIE Publisher</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="548" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">VERVE -Voxel Engine for Real-time Visualization and Examin at ion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Knittel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Eurographics&apos;93, accepted for publication</title>
		<meeting>Eurographics&apos;93, accepted for publication</meeting>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Geometrically deformed models: A method for extracting closed geometric models from volume data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Breen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Lorensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>O'bara A Nd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="217" to="226" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
