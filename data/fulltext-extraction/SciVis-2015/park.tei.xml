<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Auto-Calibration of Multi-Projector Displays with a Single Handheld Camera</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghun</forename><surname>Park</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunggoog</forename><surname>Seo</surname></persName>
							<email>hyunggoog.seo@kaist.ac.kr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Cha</surname></persName>
							<email>seunghooncha@kaist.ac.kr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyong</forename><surname>Noh</surname></persName>
							<email>junyongnoh@kaist.ac.kr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">ยง</forename><surname>Kaist</surname></persName>
						</author>
						<title level="a" type="main">Auto-Calibration of Multi-Projector Displays with a Single Handheld Camera</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Uncalibrated state, (b): Projected patterns, (c): Calibration result, (d): Panoramic content on display I.3.3 [Computer Graphics]: Picture/Image Generation-Display algorithms</term>
					<term>I.4.1 [Image Processing and Computer Vision]: Digitization and Image Capture-Imaging geometry</term>
					<term>I.4.1 [Image Processing and Computer Vision]: Digitization and Image Capture-Camera calibration</term>
					<term>I.3.3 [Hardware]: Input/Output Devices-Image display</term>
				</keywords>
			</textClass>
			<abstract>
				<p>We present a novel approach that utilizes a simple handheld camera to automatically calibrate multi-projector displays. Most existing studies adopt active structured light patterns to verify the relationship between the camera and the projectors. The utilized camera is typically expensive and requires an elaborate installation process depending on the scalability of its applications. Moreover, the observation of the entire area by the camera is almost impossible for a small space surrounded by walls as there is not enough distance for the camera to capture the entire scene. We tackle these issues by requiring only a portion of the walls to be visible to a handheld camera that is widely used these days. This becomes possible by the introduction of our new structured light pattern scheme based on a perfect submap and a geometric calibration that successfully utilizes the geometric information of multi-planar environments. We demonstrate that immersive display in a small space such as an ordinary room can be effectively created using images captured by a handheld camera.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A spatial immersive display (SID) provides users with an enhanced level of immersion and presence. Thanks to these improved user experiences, SID has rapidly widened the area of its potential applications. Some of the recent applications include entertainment, scientific visualization, medical imaging, aerospace engineering, and virtual reality to name a few.</p><p>An immersive feeling is often created by wide viewing angles and high-resolution displays. Typically, these large displays require the use of a multi-projection system. One notable example is the CAVE Automatic Virtual Environment (CAVE) which was introduced as the first application based on a projection system to achieve SID <ref type="bibr" target="#b5">[6]</ref>. Since then, there have been many studies that facilitate the building of a multi-projection system <ref type="bibr" target="#b1">[2]</ref>. To further increase usability, a technique for building a scalable display given casually positioned projectors has also been introduced <ref type="bibr" target="#b16">[17]</ref>.</p><p>Multi-projection systems entail a complex calibration process in order to create a single imaginary display. This process is divided into geometric and photometric calibration steps.Typically, the user who performs these two calibration steps should possess professional knowledge in order to perform the construction and maintenance of the display. Auto-calibration techniques based on the use of a camera have been researched to expedite this elaborate projector installation process and to minimize user intervention <ref type="bibr" target="#b1">[2]</ref>.</p><p>Most of the existing studies try to establish a relationship between the camera and the projector through the use of an active structured light technique and a camera feedback system <ref type="bibr" target="#b16">[17]</ref> <ref type="bibr" target="#b3">[4]</ref>. Some techniques require that the camera observes the entire projection area <ref type="bibr" target="#b16">[17]</ref>[4] <ref type="bibr" target="#b18">[19]</ref>. An increased number of mounted cameras are often needed for the expansion of the display to satisfy this requirement. In a small space such as an average room, it is difficult to setup multiple cameras without blocking some part of the multiplanar display.</p><p>Recently, researchers have attempted to reduce the number of required cameras by adopting a computer-controlled pan tilt unit (PTU) camera <ref type="bibr" target="#b4">[5]</ref> <ref type="bibr" target="#b9">[10]</ref> <ref type="bibr" target="#b19">[20]</ref>. However, the preparation of a PTU can incur additional hardware and implementation costs. In addition, the installation of the unit has an associated challenge, as the camera must be placed at a location where it can observe the entire projection area just by rotating itself. In small spaces, there is insufficient distance between the camera and the display surface for this to work. Because of these constraints, multi-projector calibration is still considered difficult compared to a typical, single projector installation.</p><p>We propose a novel approach that utilizes a single handheld camera to achieve auto-calibration of multiple projectors in a multiplanar space. Our contribution includes a flexible codification scheme of structured light patterns based on a perfect submap and image stitching techniques that extract geometric information. The utilization of the perfect submap based spatial light pattern can estimate an entire projection area with only partially captured pattern images. Stitching these spatial pattern images reconstructs the geometric information of the multi-planar space. Due to the collaboration of the spatial light pattern and the image stitching technique, the camera does not need to be specially constrained while it cap-tures the pattern images on the display surface. In addition, the field of view of the camera does not need to cover the entire display region. This allows the use of a handheld camera. These days, many handheld devices such as smart phones include a camera. Therefore, the utilization of our method makes the calibration of multiprojectors easy and simple. Our method works well in small spaces and is computationally efficient compared to temporal structured light based approaches employed in existing calibration methods <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Auto-Calibration in Multi-Projection System</head><p>Much research has been conducted and techniques proposed for achieving the automatic calibration of a multi-projection system. Although each work focuses on different issues based on diverse ideas, almost all approaches employ a camera feedback system and structured light patterns to determine the relationship between the projectors and the display surface.</p><p>Geometric registration of multiple projectors using many cameras and active structured light patterns can recover the 3D information of non-planar display surfaces <ref type="bibr" target="#b16">[17]</ref>. Another line of research suggests a homography tree technique to align a large number of projectors <ref type="bibr" target="#b3">[4]</ref>. This approach reduces the constraints on the required field of view of a camera as the camera does not need to observe the entire display region. However, the field of view of the camera should still be large enough to be able to capture the display areas of at least four projectors in a single image. Other methods use simple 2D homography matrices to register multiple projectors on a planar surface <ref type="bibr" target="#b18">[19]</ref>. They extract feature points of a checkerboard pattern and use them as correspondences between the projectors.</p><p>Fully automated calibration techniques often employ a PTU camera, markers to determine the display area, and a gray code to determine the projector-wall homography <ref type="bibr" target="#b9">[10]</ref>. This method completely removes user intervention by using a computer-controlled camera. Another method reconstructs the 3D shape of an arbitrary display surface with a swept surface consisting of profile curves and path curves. This approach achieves multi-projector registration without markers using a grid of Gaussian blob patterns with a binary-encoding scheme <ref type="bibr" target="#b19">[20]</ref>.</p><p>These approaches commonly employ time-multiplexing structured light patterns, which can achieve the maximized pattern resolution within a captured image. However, the employment of temporal structured light patterns requires many captured pattern images and constrains the cameras to be fixed during the capture of the projector patterns. This causes a long processing time and the inconvenience of determining the camera position and direction. To address these issues in the above research, multiple mounted cameras or computer-controlled cameras were employed, resulting in additional hardware cost.</p><p>In contrast to these temporal pattern approaches, an automatic multi-projector registration technique was introduced that utilizes a single spatial pattern image taken with a six-mega pixel handheld camera <ref type="bibr" target="#b14">[15]</ref>. In this method, each projector simultaneously displays randomly generated pattern images with a known structure for a user to capture the whole display area with a hand-held camera. Utilizing the feature points of each pattern extracted from a single captured image, the relationship between the projectors is calculated. This approach still has a constraint on the field of view of the camera, which requires covering the entire projector display region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Structured Light Techniques</head><p>A range of structured light techniques have been developed and each technique employs a variety of their own codification schemes <ref type="bibr" target="#b20">[21]</ref>. These codifications can be classified into time multiplexing and spatial multiplexing schemes, depending on the representation of the codeword. The time multiplexing methods create a codeword by temporally changing illumination through a sequentially projected pattern. This strategy typically achieves high accuracy and resolutions but is inapplicable to dynamic environments. Most multi-projector calibration methods employ this type of active structured light technique with a mounted camera because the display surface is static.</p><p>Conversely, spatial multiplexing techniques project a unique pattern image. The codeword encodes position labels with neighbor information around it. Decoding these patterns is usually a difficult process and relies on a pattern image of a low resolution. However, spatial multiplexing techniques can be applicable to dynamic environments. The perfect map theory, also known as M-arrays, a pseudorandom array, or the De Bruijin tori, is widely used in spatial codification schemes because of its unique mathematical properties. A perfect map is a two-dimensional periodic array of r ร s with symbols from an alphabet of size k. A subarray of dimension u ร v occurs exactly once in the whole array. The perfect map contains all the subarrays except the one filled with 0's. A perfect submap is an aperiodic array of size r ร s where all the subarrays of size u ร v are unique within the array, but does not necessarily contain all of the possible subarrays of size u ร v. This mathematical property makes the decoding stage very robust against pattern occlusion and object shadowing. There are two types of primitive schemes for distinguishing each element in an array. One employs different colors <ref type="bibr" target="#b13">[14]</ref> and the other uses geometric features for symbol <ref type="bibr" target="#b24">[25]</ref> <ref type="bibr" target="#b0">[1]</ref> .</p><p>Our method employs this strategy based on a perfect submap with geometric primitives. Our perfect submap is unique, however, in that we designed an appropriate structured light pattern to allow the use of a handheld camera in calibrating a multi-projection system. The details are described in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OVERVIEW</head><p>Our solution consists of the following steps ( <ref type="figure" target="#fig_0">Figure 2</ref>). First, we generate flexible structured light patterns base on a perfect submap, which can change the alphabet size k adaptively according to the camera resolution. Even though the alphabet size in the pattern changes, our new structured light pattern strategy can identify the correspondences between the projectors and the camera with a consistent decoding scheme. To differentiate the patterns easily, each pattern is represented with a different color channel (Section 4).</p><p>Second, while the projectors project the generated patterns on the display surfaces, the overlapped region of the pattern images is captured with a handheld camera. Then, radial distortions in the captured pattern images are corrected in the camera calibration process <ref type="bibr" target="#b25">[26]</ref>. Colors of the patterns and their distribution in the captured image are utilized to identify a projector from which the pattern is emitted. Then, the captured images are binarized before the decoding process (Section 4.3).</p><p>Third, the vanishing points determined by the perspective of the environment are identified utilizing the edges in the captured images. Because vanishing points offer geometric information of multi-planar spaces, a maximized display area can be obtained (Section 5). Lastly, to achieve uniform display brightness, photometric correction is processed. We adopt an alpha blending technique <ref type="bibr" target="#b16">[17]</ref> based on geometric information that was obtained from the previous step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SPATIAL STRUCTURED LIGHT FOR MULTI PROJECTOR CALIBRATION</head><p>The requirements for a structured light pattern to satisfy our goal are as follows. Patterns that are generated using different colors and perfect submaps across adjacent projectors, are projected on the display surface (the orange box). Then, the captured pattern images are binarized before input to the decoding process (the lower left images). A homography graph determines the relationship between the camera view and the projectors using correspondences from the input images (the green box). Stitching the images then determines the geometric information and the display region (the blue box). Finally, unifying the brightness across the projectors (the purple box) results in an immersive display in a multi-planar space (the right image).</p><p>โข Perfect submap based spatial pattern: Perfect submap based spatial patterns are often used in the field of ranging to handle pattern occlusion and object shadowing. Different from these utilizations, we employ the perfect submap based patterns in order to allow the use of a handheld camera in calibrating a multi-projection system. By exploiting the mathematical properties of the perfect submap, a projector-camera homography can be estimated even if the patterns are partially captured within a camera view. The encoded codeword of the pattern based on a perfect submap contains positions in projector coordinates. Thus, this spatial structured technique can estimate the entire pattern using partially captured images.</p><p>โข Geometric primitive based binary pattern: In a multiprojection system, projectors are placed to be overlapped with each other to create a large display. Active structured light techniques are often employed with this scheme. These techniques require that other projectors are turned off while each projector projects multiple pattern images on the display surface. In our scheme, all of the projectors simultaneously project a unique pattern image onto the display surface. We employ a binary pattern image consisting of different geometric shapes that correspond to the alphabets. The different color of the pattern from each projector plays an important role in the overlapped region formed by the patterns to distinguish between adjacent projectors.</p><p>โข Flexible perfect submap: When structured light patterns are designed, in general, a resolution of the pattern is carefully determined taking into account the circumstances such as the resolution of the light-emitting device, the resolving power of the camera, and the capturing distance. In case of a perfect submap based structured light pattern, the resolution of the pattern and a minimum pattern region which needs to be captured within the camera view for the identification of one correspondence point can be determined by the size r ร s of the perfect submap and the size u ร v of the subarray, respectively. An aperiodic perfect map satisfying the following necessary conditions <ref type="bibr" target="#b12">[13]</ref> is converted to a pattern image.</p><formula xml:id="formula_0">(i) r โฅ u (ii) s โฅ v (iii) (r โ u + 1)(s โ v + 1) = k uv A condition of perfect submap (r โ u + 1)(s โ v + 1)</formula><p>โค k uv can also be obtained from the above conditions. There can be many perfect submaps that satisfy these conditions so that it is tricky to determine the parameters r, s, u, v, k taking the various circumstances into account. Fortunately, structured light patterns are usually used to calculate the depth of the objects in a scene as in 3D scanners. In that case, the capturing distance is somewhat constant. Therefore, many authors designed a perfect submap based spatial pattern with fixed k alphabets and the pre-set parameters r, s, u, v depending on their environments <ref type="bibr" target="#b24">[25]</ref> <ref type="bibr" target="#b0">[1]</ref> .</p><p>To the best of our knowledge, our approach is the first application of a perfect submap based structured light pattern to a multiprojector calibration. Existing approaches that simply fix the parameters cannot be applied to our situation where a variety of handheld camera devices may be used for the capturing patterns. In addition, the capturing distance from the camera to the display surface depends on the scale of space. These factors affect the recognition accuracy for the pattern. For example, if a low resolution camera captures a high resolution pattern, the recognition accuracy would be low. If a camera located too close to the display surface captures a low resolution pattern, the number of point correspondences would be insufficient. Therefore, a codification scheme for the pattern which is consistent regardless of the changes in the size of a perfect submap that is determined by the environment, is required for our purpose.  Our novel flexible spatial pattern is generated according to the following equation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Codification Scheme</head><formula xml:id="formula_1">I combined = I code โ I checker ,<label>(1)</label></formula><p>where I checker is the checkerboard image, and I code is the converted image from a perfect submap. I checker provides the accurate locations of point correspondences with the vertex shared by four adjacent checkerboard squares <ref type="figure" target="#fig_2">(Figure 3c</ref>), while I code provides the mathematical property for point correspondences <ref type="figure" target="#fig_2">(Figure 3b</ref>). The <ref type="figure">Figure 4</ref>: Proposed flexible primitives combined image comprised of these two forms of informative images can be obtained with an exclusive OR operation <ref type="figure" target="#fig_2">(Figure 3d</ref>, Equation 1).</p><p>The size of the perfect submap determines the number of the checker patterns in I checker . Our method first computes r ร s considering the aspect ratio of the projector(ฯ h :ฯ v ) and then u ร v to be a compact subarray. Specifically,</p><formula xml:id="formula_2">r = ฯ v t a โ1, s = ฯ h t a โ1, u = v = t s 2 , If t s is even number u = t s โ1 2 , v = t s โ1 2 + 1. If t s is odd number ,</formula><p>where t a (positive real number) and t s (positive integer for 2 โค t s โค 2 min(r, s)) are scaling factors to adjust the resolution of the pattern and the minimum pattern region needed for the identification of one correspondence point. Lastly, the alphabet size k</p><formula xml:id="formula_3">= uv (r โ u + 1)(s โ v + 1) can be a dependent variable. Instead of parameters r, s, u, v</formula><p>, and k, the user adjusts only t a and t s to ensure accurate recognition of the pattern and to generate the best possible pattern image. Once the parameters are set, a perfect submap is generated with a brute-force algorithm, since there is no optimal solution for the construction of the perfect submap given the arbitrary parameter set r, s, u, v, k and hamming distance h. Please refer to Morano et al. <ref type="bibr" target="#b13">[14]</ref> for a detailed description on the generation of a perfect submap given these parameters. Each element c i j โ {0, 1, . . . , k โ 1} (for 0 โค i &lt; r, 0 โค j &lt; s) in the perfect submap is converted to a symbol in the code pattern <ref type="figure" target="#fig_2">(Figure 3a, 3b</ref>). Our novel spatial pattern contains concentric circles centered on point correspondences. These circles represent the symbols in code pattern I code . The radii of the concentric circles are determined as ฯ,</p><formula xml:id="formula_4">c i j โ1 c i j ฯ, c i j โ2 c i j ฯ, . . . , 1</formula><p>c i j ฯ if c i j is greater than zero, where ฯ is the radius of the outermost circle. A blank is employed as a symbol if c i j equals zero. We set ฯ โค min( W ฯ h t a , H ฯ v t a ) / 2 to prevent adjacent symbols from overlapping with each other, where W /H are the width/height of the projector resolution. Moreover, the symbols are composed of alternating white and black concentric circles filling in from the outermost white circle <ref type="figure">(Figure 4</ref>). This scheme allows flexible description of various elements given the value of k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Decoding</head><p>The decoding process follows the opposite operation of the pattern generation. The following equation can be obtained after adding โI checker to both sides of Equation 1.</p><p>I combined โ I checker = I code โ I checker โ I checker = I code <ref type="bibr" target="#b1">(2)</ref> This means that the code pattern, which is originated from the perfect submap, can be recovered through the exclusive OR operation applied to the generated combined pattern and the corresponding checkerboard pattern. However, I checker is not known, and also needs to be recovered from the captured combined pattern image. </p><formula xml:id="formula_5">I code = I capture โ I checker ,<label>(3)</label></formula><p>where I capture is a pattern image captured by the camera, I checker is the corresponding unknown checkerboard pattern, and I code is the recovered code pattern from I capture . This section explains how to obtain I checker from I capture and then I code from these two images ( <ref type="figure" target="#fig_3">Figure 5</ref>). Using the decoded I code , we can identify the relationship between a projector, which projects a pattern image, and a camera view, which includes the pattern image. I checker can be obtained from point correspondences that are located in the center of the symbols. Recall that the centers coincide with the corner of the squares in the checker pattern. To find the corners where four squares meet in I checker , a corner detector extracts every possible corner point in I capture . These points form the initial point set P. The Shi-Tomasi corner detection algorithm produces reasonable results for our system <ref type="bibr" target="#b21">[22]</ref>. Then, we utilize an edge image I edge which can be obtained from I capture . Only correspondence points P โ P on intersections of two straight edge lines can be extracted from points P. I checker can be obtained by checking whether 4 correspondence points form a white or black square in I capture . A midpoint between any two diagonal points determines the white or black color of each square in the checkerboard.</p><p>Computation of Equation 3 then generates I code . In practice, erroneous edges, caused by misalignment between generated I checker and I capture , remain in I code . These edges can be removed with a morphological opening operation <ref type="bibr" target="#b6">[7]</ref>. We then compute I contour using I code . Recall that each element c i j in a perfect submap was converted to a symbol in the form of a concentric circle in Section 4.1. Now this element is deduced according to the number of circular contours that enclose each correspondence point. Finally, we can calculate point correspondences between the camera and the projector by comparing the u ร v subarrays of these elements using the codewords of the perfect submap used in I code .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Colored Patterns</head><p>Our approach allows all of the projectors to work simultaneously. In order to differentiate patterns emitted from adjacent projectors, each projector uses one of the RGB color channels different from the colors used by the adjacent projectors. The use of different color channels plays a key role in identifying projectors. Before decoding <ref type="figure">Figure 6</ref>: Homography calculation a captured image, the image from each color channel is converted into a binary image by thresholding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Projector Identification</head><p>The color of the pattern emitted from each projector is assigned using a De Bruijn sequence <ref type="bibr" target="#b8">[9]</ref> whose size of alphabet is 3 and size of substring is 2. These sizes result from 3 RGB channels and 2 captured patterns per image. Two adjacent projectors should not be assigned with the same color because it would make it impossible to differentiate one from the other. Therefore, we exclude all of the repeated consecutive characters from the sequence such as 'RR' or 'GG' or 'BB'. Once the color of the pattern of each projector is determined depending on the sequence, the colored patterns, which are generated using the scheme explained in Section 4.1, are projected on the display surface. We take pictures of the overlapped regions formed by the patterns from two adjacent projectors and use them as inputs. As can be seen in <ref type="figure">Figure 6</ref> view i+1, there may be a case where more than 2 patterns are captured. Therefore, the most dominant two colors and the locations of their areas in the input image are determined using an image moment <ref type="bibr" target="#b7">[8]</ref>. The two colors lead to a correspondent substring in the De Bruijn sequence. The matched substring identifies which projector the pattern is emitted from.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">GEOMETRIC CALIBRATION 5.1 Directed Linear Transform</head><p>By using the correspondences from two patterns that are observed in a single camera view, we compute a homography between the camera and the projector coordinates to create a pseudo-single view and to determine the display region. The homography matrix H i, j is found by using the RANSAC robust estimation method <ref type="bibr" target="#b23">[24]</ref>.</p><formula xml:id="formula_6">H j, j+1 = H i, j+1 H โ1 i, j , H iโ1,i = H โ1 i, j H iโ1, j<label>(4)</label></formula><p>Here, i denotes the index of a camera view while j denotes the index of a projector. The camera view works as a chain to transform the coordinates of one projector to those of the next projector. Inversely, the pattern can be used as a chain to transform between two camera views (Equation 4). The transformation between every projector node and a view node can be expressed as a single homography maxtrix H, while the inverse direction can be expressed as H โ1 , resulting in a form of undirected graph ( <ref type="figure">Figure 6</ref>). Given H, the transformation of a point x can be expressed as x = Hx, while that of a line l = (a, b, c) which satisfies the equation ax + by + c = 0 can be expressed as l = H โ l <ref type="bibr" target="#b10">[11]</ref>.</p><p>Notation All of the estimations are performed based on pseudosingle view coordinates that are approximately fronto-parallel to a single plane <ref type="figure">(Figure 6 view i)</ref>. We conducted the experiments in a 3-multi-planar environments, with consideration for the wide field of view of humans and an ordinary room structure. Coordinates that are transformed into approximately a fronto-parallel view are expressed using subscripts such as L: left plane, C: center plane, and R: right plane. For instance, a point x that is transformed into an approximately fronto-parallel view of the center plane is expressed as x C .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Vanishing Points Estimation</head><p>Vanishing Point The edges between walls, ceiling, and floor provide a clue to extract the necessary geometric information of the space. The spatial geometry is estimated using vanishing points obtained from the set of edges in pseudo-single views. Each wall plane is assumed to be a rectangle, and the horizontal vanishing point, where the extension of the top edge and the bottom edge meet, is represented as ฯ H . The vectors that pass through the top edge and the bottom edge are expressed as ฮฑ H and ฮฒ H respectively, and the angle they form is expressed as ฯ H . In the same manner, the vertical vanishing point is ฯ V , the vectors that pass through the left edge and the right edge are ฮฑ V and ฮฒ V , respectively. The angle between these two vectors is ฯ V . Each wall plane can be defined by their horizontal and vertical vanishing point ฯ and their vector ฮฑ, ฮฒ <ref type="figure">(Figure 8</ref>).  We extract line segments from the input images that contain edges ( <ref type="figure">Figure 6</ref> view i โ 1, view i + 1). Application of a Canny edge detector <ref type="bibr" target="#b2">[3]</ref> extracts edges from where wall planes intersect as well as from patterns. Then, substracting pattern regions from each channel image leaves only the necesary edges that constitute a geometric structure <ref type="figure" target="#fig_5">(Figure 7</ref>).Application of a Hough transform to these edge components produces complete line segments <ref type="bibr" target="#b11">[12]</ref>. The vanishing point ฯ of each plane, and its vectors ฮฑ and ฮฒ can be computed from these line segments and their intersections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Display Area Determination</head><p>This step estimates the largest rectangular display area. Our approach is similar to the method described in Raij and Pollefeys <ref type="bibr" target="#b15">[16]</ref> in terms of utilizing vanishing points. However, our approach is different in that the x-axis of the camera image is not required to be parallel to the ground. Thanks to the vanishing points extracted from the geometry of the environment, our approach can be applicable to multi-planar displays without manually placed fiducial markers <ref type="bibr" target="#b9">[10]</ref> or attached tilt sensor <ref type="bibr" target="#b17">[18]</ref>.</p><p>The display region on each plane is determined using the vanishing point ฯ, vector ฮฑ, and ฮฒ obtained from the previous step. The <ref type="figure">Figure 8</ref>: Display region determination using vanishing points following equations set the biggest possible display region that can be created on each plane <ref type="figure">(Figure 8)</ref>.</p><formula xml:id="formula_7">ฯ ( n, m) = cos โ1 ( n โข m n m ) (5) ฮฑ = argmin xโU ฯ ( x, ฮฒ ), ฮฒ = argmin xโV ฯ ( x, ฮฑ)<label>(6)</label></formula><p>Here, U is a set of vectors, including ฮฑ, that originate from w towards the endpoints of the top edges in each projection image. Any endpoint that lies inside the adjacent projection image or lies outside the plane can be replaced by the intersection formed by the boundaries of the adjacent projection images or those of the plane (The hollow orange dots in <ref type="figure">Figure 8</ref>). Using ฮฒ and the endpoints of the bottom edges of each projection image, V is similarly defined. After evaluating the maximum display region from each plane, the height of the display is set to the height of the smallest region. Then, using the transformation matrix H L,R , the widths of the display on the side walls are compared with each other. We select the smaller one between the two.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RESULTS</head><p>Entire pattern estimation Our method does not require the camera to observe the entire region of the structured light pattern. The minimum pattern region, which is needed to identify one correspondence point, is determined by a subarray size u ร v. In a planar display surface, camera-projector homography can be obtained as long as the camera view contains a pattern region that allows it to compute at least 4 correspondence points <ref type="bibr" target="#b10">[11]</ref>. This property of a perfect submap in a planar surface helps estimate the remaining pattern that is not captured by the camera. We demonstrate this with images that include partial regions of the pattern. <ref type="figure">Figure 9</ref> shows the patterns restored in their entirety from a set of partially captured patterns. This process allows the auto-calibration of a multiprojector display with a handheld camera as opposed to relying on elaborately installed cameras as employed in most of the previous systems <ref type="bibr" target="#b3">[4]</ref>[19] <ref type="bibr" target="#b19">[20]</ref>. Adaptive pattern Appropriate parameters for the perfect submap used to generate a pattern can be obtained according to the resolution of the handheld camera using our flexible pattern scheme. In order to examine the adaptability of the pattern resolution given the resolution of the camera, we tested four different patterns with three different handheld cameras that have the resolution of 5616ร3744 (CANON EOS), 3264ร2488 (APPLE iPhone), and 3264ร1836 (SAMSUNG Galaxy S4), respectively. The perfect submaps which satisfy the necessary conditions described in Section 4 are not unique. For the experiments, we created four different patterns and computed their associated paramters while (a) The camera views with regard to the pattern (b) The pattern with regard to the camera views <ref type="figure">Figure 9</ref>: These images show the relationship between the entire pattern and the camera views which include a partial region of the pattern. Using this relationship, the entire pattern can be estimated from the partially captured patterns.</p><p>maintaining the ratio of the subarray size/perfect submap size to be 1.5โผ3.5%. This ratio ensures the minimum pattern region to be captured within the camera view, which allows the identification of one correspondence point. With this ratio, each pattern fully occupied the camera view ( <ref type="figure" target="#fig_6">Figure 10</ref>), allowing the comparison between our test cameras with differing fields of view.</p><p>An accurate camera-projector homography can be obtained when a large number of point correspondences are available. In turn, the use of a high-resolution pattern image may achieve a high acquirement rate of point correspondences. <ref type="figure" target="#fig_7">Figure 11</ref> shows the recognition rate of each pattern and the number of decoded point correspondences in the captured image. The graphs verify that our assumption regarding the relationship between a high resolution pattern and a high acquirement rate is generally true. However, it is not the case when the resolution of the camera is low compared to that of the pattern image. For instance, the pattern recognition rate of the SAMSUNG GALAXY S4, which has the lowest resolution tested, was significantly lower than that of the other cameras in the case of pattern 4 which has the highest resolution. In this case, pattern 3, which has a lower resolution than pattern 4, is a better choice for the achievement of a high recognition rate and acquirement.  Performance We conducted an experiment in a 3-multi-wall environment with width, height, and length of 4m, 2m, and 4.5m, respectively. Six Viewsonic PJD-7820HD projectors were used, of which two were placed at each wall to cover the human field of view. We experimented with three different handheld devices. The first and second one were a CANON EOS 5D Mark II with CANON EF 24-105mm Lens and an APPLE iPhone 5. A structured light pattern was created using parameters the t a = 2, t s = 6 for the perfect submap. The third one was a SAMSUNG GALAXY S4 (GT-I9505) phone camera, which is of lower quality than the others. The values of the used parameters were t a = 1.5, t s = 7. Note that the time required for the parameter setting was approximately 30 seconds for all of the experiments. The use of a smaller perfect submap, which is represented by smaller values of t a , led to similar results to those produced by others ( <ref type="figure" target="#fig_0">Figure 12)</ref>.</p><p>The computations for the experiments were performed by a computer with an Intel(R) Xeon(R) CPU E5520 @ 2.27GHz with 8 Cores (2 logical cores per each physical core) with OpenMP. It required approximately 30 seconds for the user to capture the overlapping projection areas formed by six casually installed projectors with a handheld device. <ref type="table">Table 1</ref> shows detailed calibration times for a range of images. We measured the average performance over five different calibration results. Given the captured images, it required less than 2 minutes to complete the calibration process such (4 camera calibration image, 6 projectors, 5 captured images) <ref type="table">Table 1</ref>: Computation times (milisecond) averaged from 5 different calibration results with 3 kinds of handheld devices <ref type="figure" target="#fig_2">Figure 13</ref>: Left: small space 60cm width, 50cm height and 70cm length; Right: calibration result as the steps to construct, maintain, or fix a large display like the one shown in <ref type="figure">Figure 1</ref>. Evaluation We projected a regularly spaced grid onto the display surface-a method often used in other related work <ref type="bibr" target="#b4">[5]</ref>[20]-to evaluate the quality of the alignment achieved by our approach. We then compared the quality of our alignment to that of the result produced by Garcia-Dorado and Cooperstock <ref type="bibr" target="#b9">[10]</ref>, which employs fiducial markers and typical time multiplexing structured light patterns known as the Gray code patterns. <ref type="bibr" target="#b0">1</ref>  <ref type="figure" target="#fig_8">Figure 14</ref> shows the result of alignment. The grid images that emitted from the two projectors matched accurately in the overlapped region. The alignment produced a sharp image when a color image is projected on the surface. See feathers of the parrot and grids in the third and fourth rows in <ref type="figure" target="#fig_8">Figure 14</ref>. Small space To prove the versatility of our system, we modeled a very small space with the dimension of 60 cm in width, 50 cm in height, and 70 cm in length. It is impossible for any of the existing methods to perform an automatic calibration for this small space because there is no sufficient distance available for the camera to capture the entire scene inside the setup. In contrast, our method allows a handheld camera to be put inside the setup and be freely rotated while it captures a portion of the scene. <ref type="figure" target="#fig_2">Figure 13</ref> shows the result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION AND FUTURE WORK</head><p>We propose a novel framework that allows multi-projector calibration in a multi-planar space using a handheld camera that is widely prevalent in the public these days. A novel structured light pattern scheme is also proposed to allow users to take advantage of the easily accessible handheld cameras. Compared to existing methods, which have greater constraints on the field of view of usable cameras, our method can achieve an accurate geometric calibration even if only a partial region of the pattern is observed. Moreover, we present a method to find an optimal rectangular display in a multi-planar environment using vanishing points and show its applicability to general small spaces surrounded by walls. This framework is more cost-efficient than previous methods, and only requires users to carry out the simple task of capturing a few images to start the calibration using a handheld camera. Therefore, neither professional knowledge nor help from an expert is necessary for high quality multi-projection calibration. Through various experiments, we verify that our method is useful for constructing and maintaining SID.</p><p>Limitation and Future Work While our paper focuses on a handheld camera driven geometric calibration, we do not deal with photometric calibration. In order to carry out the entire multiprojector calibration procedure with a single handheld camera, we have to maintain a condition where each projector projects a unique single pattern simultaneously. Therefore, when all projectors have the same luminance, our method will work well using alpha blending. In the case where the brightness of the projectors varies significantly, however, our method will show degradation in performance. Our approach may also inaccurately calculate a correspondence during the decoding procedure if the input image is blurry. A blurred image is produced when the user inadvertently moves the camera while taking a photo. We would like to make our method more robust by applying motion de-blurring techniques <ref type="bibr" target="#b22">[23]</ref>. Lastly, our approach requires a pre-process to calculate the intrinsic parameters of the camera. We hope to remove this step by adding a self-calibration module, which utilizes cuboid geometric information from images captured with an un-calibrated camera to account for the camera's intrinsic parameters.</p><p>In order to cover the user's wide field of view, we arranged the projectors in a simple fan configuration. However, it would be interesting to investigate for a more complicated arrangement of projectors (e.g. 2 ร 2 arrangement). This would be possible with the extension of the De Bruijin sequence into the De Bruijin tori. Furthermore, our system allows to capture only a part of the pattern using a handheld camera. As this capture does not have to be exact on a planar surface, we believe that the approach can be used for curved surfaces of blended corners. We will look into this issue in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>This figure shows the overview of our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>code pattern (c) checker pattern (d) combined pattern</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Example of the codification scheme</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Decoding pipelineBy applying this rule to a captured pattern image, we obtain</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Binary image of pattern after applying a dilation operation (d) Edge for geometry and detected line segments</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Edge extraction for geometric information</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :</head><label>10</label><figDesc>(a): Pattern1 (t a = 1.6,t s = 5), (b): Pattern2 (t a = 2,t s = 6), (c): Pattern3 (t a = 2.4,t s = 7), (d): Pattern4 (t a = 2.8,t s = 12); Four different flexible patterns created and their associated parameters used for the generation of the graphs inFigure 11.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 :</head><label>11</label><figDesc>Decoded point correspondences and recognition rate Figure 12: Left: result with CANON EOS 5D Mark II; Right: result with SAMSUNG GALAXY S4; The difference in the quality of the cameras does not influence the calibration results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 14 :</head><label>14</label><figDesc>Left: geometric alignment only; Right: geometric alignment with alpha blending; First and second rows: alignment achieved by multiplexing structured light and fiducial markers<ref type="bibr" target="#b9">[10]</ref>; Third and fourth rows: our results.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We attached the markers very carefully because the alignment result of their approach<ref type="bibr" target="#b9">[10]</ref> depends on the accurate position of the markers. To compare the quality of the alignment, we set 2 projectors to have a sufficient overlapping region on the wall plane.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Design of a monochromatic pattern for a robust structured light coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Albitar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Graebling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doignon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">529</biblScope>
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Camera-based calibration techniques for seamless multiprojector displays. Visualization and Computer Graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="193" to="206" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scalable alignment of large-format multi-projector displays using camera homography trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Visualization&apos;02</title>
		<meeting>the conference on Visualization&apos;02</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="339" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic alignment of high-resolution multi-projector display using an un-calibrated camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Housel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Visualization&apos;00</title>
		<meeting>the conference on Visualization&apos;00</meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="125" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Surround-screen projection-based virtual reality: the design and implementation of the cave</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cruz-Neira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Sandin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Defanti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 20th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="135" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An introduction to morphological image processing. Tutorial texts in optical engineering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Dougherty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Moments and moment invariants in pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zitova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Suk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A survey of full length nonlinear shift register cycle algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fredricksen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="195" to="221" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fully automatic multi-projector calibration with an uncalibrated camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Garcia-Dorado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cooperstock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="29" to="36" />
		</imprint>
	</monogr>
	<note>2011 IEEE Computer Society Conference on</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Robust detection of lines using the progressive probabilistic hough transform. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Galambos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="119" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Aperiodic and semi-periodic perfect maps. Information Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="95" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Structured light using pseudorandom codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Morano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ozturk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Conn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dubin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zietz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nissanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="322" to="327" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Easy calibration of a multi-projector display system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okatani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Deguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Auto-calibration of multi-projector display walls</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on</title>
		<meeting>the 17th International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="14" to="17" />
		</imprint>
	</monogr>
	<note>Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-projector displays using camera-based registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Towles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fuchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visualization&apos;99. Proceedings</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="161" to="522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ilamps: geometrically aware and self-configuring projectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Baar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Beardsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Willwacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Forlines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2006 Courses</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A low-cost projector mosaic with fast registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Baar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">X</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Autocalibration of multiprojector cavelike immersive environments. Visualization and Computer Graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sajadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Majumder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="381" to="393" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A state of the art in structured light patterns for surface profilometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pribanic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Llado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2666" to="2680" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Good features to track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="593" to="600" />
		</imprint>
	</monogr>
	<note>IEEE Computer Society Conference on</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Motion-aware noise filtering for deblurring of noisy and blurry images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Detecting planar homographies in an image pair</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Laganiรฉre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Symposium on</title>
		<meeting>the 2nd International Symposium on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="182" to="187" />
		</imprint>
	</monogr>
	<note>Image and Signal Processing and Analysis</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Three-dimensional imaging system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Yee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Griffin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optical engineering</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2070" to="2075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A flexible new technique for camera calibration. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1330" to="1334" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
