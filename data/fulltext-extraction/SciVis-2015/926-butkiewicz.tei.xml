<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Effectiveness of Structured Textures on Dynamically Changing Terrain-like Surfaces</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Butkiewicz</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">H</forename><surname>Stevens</surname></persName>
						</author>
						<title level="a" type="main">Effectiveness of Structured Textures on Dynamically Changing Terrain-like Surfaces</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TVCG.2015.2467962</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Structured textures</term>
					<term>terrain</term>
					<term>deformation</term>
					<term>dynamic surfaces</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Previous perceptual research and human factors studies have identified several effective methods for texturing 3D surfaces to ensure that their curvature is accurately perceived by viewers. However, most of these studies examined the application of these techniques to static surfaces. This paper explores the effectiveness of applying these techniques to dynamically changing surfaces. When these surfaces change shape, common texturing methods, such as grids and contours, induce a range of different motion cues, which can draw attention and provide information about the size, shape, and rate of change. A human factors study was conducted to evaluate the relative effectiveness of these methods when applied to dynamically changing pseudo-terrain surfaces. The results indicate that, while no technique is most effective for all cases, contour lines generally perform best, and that the pseudocontour lines induced by banded color scales convey the same benefits.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Previous perceptual research has identified some best practices regarding how to texture curved 3D surfaces to effectively convey their shape. <ref type="bibr" target="#b0">[1]</ref>[2] <ref type="bibr" target="#b2">[3]</ref> Among the most effective are contour lines and grid lines. However, most prior research has examined the effectiveness of these techniques as applied to static 3D surfaces, with little focus on 4D surfaces (i.e. 3D surfaces that change over time). In these applications, it is important to not only convey the current shape of the surface, but also the absolute and relative sizes, shapes, and rates of changes occurring. Furthermore, for complex or busy scenes, it is challenging to ensure subtle changes are not overlooked.</p><p>One of the most interesting differences between how a structured texturing technique performs on a static surface versus a dynamic surface is the stability of the derived illustrations as the surface deforms. As surfaces change, texturing techniques that were stable in static illustrations can exhibit erratic behavior. The manners in which these illustrations change as the surface deforms can provide motion cues, which reinforce perception of the surface's changes.</p><p>With regular grid texturing, these cues are the angles at the crossing points becoming acute or obtuse, and the cells between stretching and contracting. With contour lines, the motion cues are existing lines expanding or contracting about the centers of deformation, emanating from protruding deformations and shrinking into collapsing deformations. Additionally, contour line techniques result in the emergence of new lines, which can be extremely effective at drawing the viewer's attention, even to subtle changes.</p><p>Viewing conditions must also be considered when choosing or evaluating structured texturing techniques. The viewing angle can be critical to the success or failure of techniques. For example, a regular grid is unable to provide any shape information when used in a topdown plan view. Lines drawn horizontally (relative to the viewing direction) across the surface provide excellent curvature information, but can only be used in cases where the scene is viewed from a static location. These considerations are crucial when designing for interactive visualization environments where users can navigate freely across all possible viewing angles.</p><p>In an attempt to address these shortcomings (static surfaces and static viewpoints) in the prior research, this paper presents the results of human factors experiments designed to study the relative effectiveness of these structured texturing methods on dynamically changing surfaces. The first experiment examines their efficacy in relative comparison tasks between two similar changes presented sideby-side. The second experiment determines how well each technique grabs the attention of the viewer to ensure subtle changes are noticed in busy scenes. Finally, the third experiment explores how other texturing techniques can be modified by applying what was learned in the first two experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Both grids and contour lines have a long history in map making. Because most maps are top-down plan views, grids generally are only used to convey scale, not shape information. Contour lines, however, excel in plan views. Their use to convey the shape of terrain began in the 18th century, and was widespread by the 19th. When texturing geospatial datasets, one can safely assume most users are experienced at interpreting contours.</p><p>There has been extensive research into the perception of surface shape. While shading itself is a strong cue to surface shape and curvature, it is prone to error, and applying textures has been found to be more effective. <ref type="bibr" target="#b3">[4]</ref> Cutting and Millard <ref type="bibr" target="#b4">[5]</ref> identified three textural visual cues, termed gradients, which are sources of shape and orientation information that change with the visual angle of a surface: perspective, compression, and density.</p><p>Perspective refers to the decreasing size of texture elements (texels) as they recede away from the viewpoint. Compression is the deformation, or change in ratios between the two dimensions of texels. Finally, density refers to the number of texels per image area, which increases with distance.</p><p>Grids excel at providing all three of these cues, due to the inherent consistency and regularity of their texels. The effectiveness of grids as visual cues can be so powerful that they will override motion parallax cues when the conflict with one another <ref type="bibr" target="#b5">[6]</ref>. While contour lines also exhibit these same cues, there can be serious ambiguity as to whether it is a result of a change in distance/orientation of a surface, or merely changing slope.</p><p>For static views of terrain-like surfaces, Sweet and Ware <ref type="bibr" target="#b2">[3]</ref> found grids more effective than contour lines for perceiving surface shape and curvature; except in top-down plan views, where contours were predictably far more effective. As they reported the least error in perceived shape/curvature occurred when viewing angles were 45º to the surface, we chose to use this angle throughout our study.</p><p>Velisavljević and Elder <ref type="bibr" target="#b6">[7]</ref> isolated the roles of texels, scale structures, global symmetries, and regularity with regard to surface texture in the perception of a static surface's tilt/slant, and found that orthogonal linear structures -whether or not they are contained in well-defined texels or spatially irregular patterns -and symmetry within the texture lead to the most accurate judgment of surface tilt/slant. This further supports the good performance of grids as applied to static surfaces. However, it should be noted that parallel linear structures, such as would often be generated by contours, were not amongst the textures evaluated in this study.</p><p>Furthermore, Velisavljević and Elder <ref type="bibr" target="#b6">[7]</ref> found orthogonal linear structures to be "a necessary and sufficient property for accurate surface attitude judgment," lending further support for the use of grid patterns for surface shape perception.</p><p>As for contour lines, a significant benefit of their use for texturing dynamically changing surfaces is the accretion (and deletion) of contour lines as the terrain deforms to varying depths. The abrupt visual onset of well-contrasted stimuli -in this case, contour lineshas been shown to be a strong cue for capturing attention <ref type="bibr" target="#b7">[8]</ref>. Abrupt visual offsets also tend to capture attention, but not quite as strongly as their onset counterparts <ref type="bibr" target="#b8">[9]</ref>. It is expected that visual attention will be better captured by areas of deformation when new contour lines suddenly pop into and out of existence, as opposed to relying solely on texture distortions, such as with the use of grids.</p><p>Our experiments differ from most other shape perception visualization studies, in that we examined performance in dynamic rather than static scenes, with changes in both surfaces and viewpoints. Evidence indicates differences in shape cue perception between static and dynamic environments: Winnemöller et al. <ref type="bibr" target="#b9">[10]</ref> found that mixing shading and outlining of objects in a dynamic environment does not demonstrate the additive effect of shape perception found with mixing shading and texturing of static curved objects by Bülthoff and Mallot <ref type="bibr" target="#b10">[11]</ref>.</p><p>Blake et al. <ref type="bibr" target="#b11">[12]</ref> developed an ideal observer model to estimate shape from texture using statistical information from texture density and texel orientation. Results showed that cue combinations are used by the visual system in a natural setting, whereas previous experiments used less-natural cue conflicts to measure combination tendencies. The scenes in our experiments always use plain shading, and selectively augment it with different texturing conditions, producing a more natural setting in which cue combinations are complementary.</p><p>The movements in the animation of a change can provide detailed shape information, through 3D structure from motion (SfM), which is especially relevant to our experiments in which the scene is constantly rotating. While one tends to consider 3D SfM, and more generally the kinetic depth effect <ref type="bibr" target="#b12">[13]</ref>, as being associated with more rigid motions, Todd <ref type="bibr" target="#b13">[14]</ref> showed that 3D SfM is perceived just as readily by nonrigid motions as it is by rigid motions for curved surfaces, and that parallel projections and polar projections are comparably valid for observing 3D SfM.</p><p>Domini et al. <ref type="bibr" target="#b14">[15]</ref> demonstrated that perceived 3D SfM is not veridical, and proposed a probabilistic model to replace Euclidian and affine models. The probabilistic model does not solve the inverseoptic problem (reconstruction of 3D shape from 2D projection), but does account for veridical judgments and perceptual bias consistent with human performance. They conclude that humans seem to process optic flow in a heuristic fashion that is perhaps influenced by learning.</p><p>Domini et al. <ref type="bibr" target="#b15">[16]</ref> further proposed the Intrinsic Constraint (IC) model for SfM, which postulates that the visual system pools together different depth signals to create a composite estimate that is more strongly associated with the distal 3D shape than any of the estimates from isolated signals.</p><p>There is also relevant research in animated vector fields. Bachthaler and Weiskopf <ref type="bibr" target="#b16">[17]</ref> present a method of illustrating 2D and 2.5D vector flow fields by rotating the input field by π/2 to create orthogonal LIC (line integral convolution) curves, which are then animated. This serves to decouple line direction from the direction of animation; the authors argue that this will allow for better perception of local motion when using high contrast, high frequency orthogonal LIC curves. Whereas the orthogonal LIC method animates lines over a static surface, our study involves static (in that they are tied to set height values) contour lines on animated surfaces whose direction of deformation is orthogonal to those static contours. Though no user studies were conducted to evaluate the perceptual effectiveness of their method, their qualitative findings nonetheless lend some support to our hypothesis that contour lines on a deforming surface will better capture a user's attention than grids (which deform with the surface) or shading alone.</p><p>Interrante et al. <ref type="bibr" target="#b0">[1]</ref> conducted a somewhat similar evaluation of texturing methods applied to a layered transparent surface. Their study compared the relative accuracy of plain shading (no texture), gridded texturing, and principal curvature texturing styles in conveying the shape and depth distance of a transparent surface enveloping a solid, opaque surface. They found that sparse, opaque texturing provided significantly better depth and shape cues than no texturing, but found no significant overarching differences between the two texturing methods. This method was further extended <ref type="bibr" target="#b17">[18]</ref> to use a voxel-based approach employing 3D line integral convolution (LIC) to advect a set of evenly distributed particles and the empty space between them. However, there was no empirical user evaluation of the effectiveness or efficiency of this extended technique relative to others.</p><p>Finally, in our third experiment, we explore modification of a traditional rainbow color scale through banding to induce pseudocontour lines. Though rainbow color scales are still incredibly commonplace (and often the default color scale in commercial visualization software), it has been well documented that they suffer from many inherent problems. Besides the obvious issues with colorblindness, there is a false assumption that our visual system has embedded intuitions regarding spectral sequences <ref type="bibr" target="#b18">[19]</ref>.</p><p>We did not seek to rescue the rainbow color scale from its welldeserved place in the dustbin of visualization, instead we used it simply as a familiar canvas for experimentation. By replacing its smooth gradients with sharply banded distinct colors, we induced pseudo-contour lines. Ideally, these should have the same effects as plain contour lines, with the added benefit that new contours would make a color suddenly appear, helping to identify the change as a new perceptual object more quickly than plain contour lines alone. This hypothesis is supported by the conclusions of Hillstrom et al. <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTAL DESIGN</head><p>The following three experiments were conducted with 15 participants (6 male, 9 female), all undergraduate psychology students paid $20 for completing the study. To avoid learning effects, they participated in all of our experiments in a staggered and randomized order.</p><p>Each participant began the study with a guided training session, which explained the two main tasks. Participants completed as many practice iterations as necessary to ensure that they fully understood the tasks. After training, the actual tasks were presented to the user in blocks, with the option to pause for a break in between each.</p><p>The two main tasks used in these experiments were comparison and detection:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.1</head><p>Comparison Tasks Comparison tasks were used to evaluate the effectiveness of texturing techniques for communicating the relative sizes of changes on dynamically deforming surfaces. In each comparison task, participants were shown a pair of two-second long animations sideby-side, and had to decide which exhibited the more significant change. The two animations played simultaneously and looped automatically for as long as necessary for participants to make a decision.</p><p>We allowed participants to re-watch the animations as many times as they needed for a few reasons: Many of the differences between animations were extremely subtle, purposefully posing a significant challenge. Individuals may also use different strategies in their assessments, e.g. they may watch both animations at once, switch their focus back and forth, or study one multiple times before watching the other. Finally, by tracking how many times participants watched the animations, we may be able to learn more about how quickly they received the necessary information, and which conditions were most challenging. (The implications of allowing re-watching versus singletime viewing is further discussed in Section 7.)</p><p>Comparison blocks consisted of six dynamic terrain models, each exhibiting six changes, for 36 total changes per block. There were three size conditions (small, medium, and large), that determined the width and height of each change, and three difference conditions (30%, 15%, and 7.5%) that determined how significantly the changes in the two animations varied from each other. These values were tuned based on pilot studies to ensure that large differences were easily noticeable, and small differences were close to the just noticeable difference, and thus quite challenging to detect. Examples of these size and difference conditions are shown in <ref type="figure" target="#fig_0">Figure 1</ref> and 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2</head><p>Detection Tasks Detection tasks were used to determine the relative attention-getting abilities of texturing techniques in drawing the gaze of a viewer to changes as they occur. In each detection task, participants watched animations of dynamic terrain models and indicated each time they detected a change occurring.</p><p>Participants were shown an animation of a dynamic pseudo-terrain model which intermittently exhibited random subtle changes over a period of roughly 30 seconds. Changes were one second in duration, with randomized periods of one to four seconds in between. Participants were instructed to indicate every time they noticed a change occurring by clicking a button. In each animation, the terrain models were spinning slowly (three revolutions per minute) around the vertical axis. This was done to keep the entire scene always in motion, as otherwise we found all changes were too easy to detect.</p><p>Detection blocks consisted of 36 changes, spread evenly across three terrain models. To explore effectiveness across a range of sizes, there were three change width conditions (narrow, medium, and wide) and three change height conditions (small, medium, and large). These values were tuned during pilot studies so the largest changes would be easily seen and the smallest would be just barely noticeable. Examples of these width and height conditions are shown in <ref type="figure">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.3</head><p>Stimuli Our pseudo-terrain models were randomly generated by summing multiple Gabor functions with varying amplitudes, wavelengths, and centers. This technique was described and used successfully by Sweet and Ware <ref type="bibr" target="#b2">[3]</ref> in a similar study, and we extended it by increasing the variation to represent a wider range of terrain types (e.g. from smoothly sloping to erratically bumpy).</p><p>Specifically, we extended it with a randomly generated spread value, which determined the maximum distance from the center of the model within which the Gabor functions would be randomly centered. We also randomized amplitude and wavelength value ranges for "major" Gabor functions (those with high amplitudes &amp; long wavelengths, generating large prominent features) and "minor" Gabor functions (smaller amplitudes &amp; short wavelengths, generating smaller, noisier features), and then randomized the number and ratio of the two generated. <ref type="figure">Figure 4</ref> shows some of the terrains possible.</p><p>After generating these base terrain models, changes were introduced as necessary at particular timesteps by applying Gaussian extrusions or depressions in random locations (never too close to the edges or previous changes). The rendering software automatically morphs the terrain models smoothly between timesteps, generating the animations shown to the participants.</p><p>As shown in <ref type="figure" target="#fig_3">Figure 5</ref>, the first two experiments utilize the following texturing techniques as different rendering conditions: plain shading, grids, and contours.</p><p>Plain shading is the control condition, representing simple rendering of the terrain surface using basic directional lighting and Lambertian shading models. It is also the base onto which all the other textures are applied.</p><p>Grids projects a regular square grid orthogonally onto the terrain from above, which causes the grid pattern to stretch and deform on the terrain's curves.</p><p>Contours draws contour lines which connect points of equal height, approximating the effect of a planes with vertical normals slicing through the terrain's surface at regular intervals.    Each of these techniques were implemented within OpenGL fragment shaders, which performed the calculations individually for each pixel, rather than per vertex or through the projection of a texture onto the terrain. This enables higher quality renderings with smoother grid and contour lines, and without artifacts from underlying triangle strips, etc.</p><p>Because grid lines and contours behave quite differently as surfaces deform, there is no particular combination of contour interval and grid cell size that will be consistently equal on a dynamic surface. Ultimately, we decided to choose grid and contour spacing values that would result in roughly the same number of pixels being colored differently than the base plain shading. This helps to ensure both techniques present similar overall texel density.</p><p>As is commonplace in the geospatial use of contour lines, we used both regular "minor" contour lines and slightly thicker "major" contour lines. Specifically, we thickened every 7 th contour line. These major lines serve two important purposes: First, contour lines are inherently countable (both consciously, and unconsciously at a glance), and these major contour lines assist in quicker counting and estimations. Secondly, they provide a level-of-detail effect, showing the basic shape of the terrain, which is then filled in by the minor lines.</p><p>We did not utilize similar hierarchical major grid lines in our work, because they do not provide the same contribution to perception of the terrain's shape as they do in contour lines. Whereas individual contour lines can provide reinforcing shape information regarding a wider surrounding area (e.g. it may encircle a mountain peak), individual grid lines only convey the shape information about their immediate location (within adjacent cells). Instead, the perceptual power of a grid stems from the regularity and density of its individual texels as a whole. Most significantly, major gridlines are necessarily evenly distributed in arbitrary locations, while major contour lines are more heavily distributed on and around prominent features. This is not to say there are no benefits to using major grid lines: They can provide the same speed benefits to counting as contour lines, which is important when judging the 2D sizes of features, especially in top-down plan views. However, our study only introduced variation into the heights of 3D changes, thus they were not likely to assist significantly in the task.</p><p>Terrain models were presented to the user using standard perspective rendering, with the camera pointed at the terrain from a 45º viewing angle relative to the horizontal, as shown in <ref type="figure" target="#fig_4">Figure 6</ref>. Sweet and Ware <ref type="bibr" target="#b2">[3]</ref> identified this oblique viewing angle as optimal for surface perception, producing the smallest errors in surface curvature estimation. Furthermore, at too high of an angle (i.e. topdown or plan-view) projected textures such as grid lines won't show any movement, and too low of an angle risks terrain obscuring itself. The camera distance was as close as was possible while keeping the entire terrain model in view. Standard OpenGL Lambertian shading was used on the surfaces. The scenes were lit using a single directional light source above and at an angle (45º) to the terrain, as shown in <ref type="figure" target="#fig_5">Figure 7</ref>. For comparison tasks, the light source was always oriented 45º about the vertical axis relative to the camera (shown in <ref type="figure" target="#fig_6">Figure 8</ref>), i.e. if the camera was viewing from the south, the light would be coming from the southwest. This lighting setup produces helpful, disambiguating shadows and highlights <ref type="bibr" target="#b20">[21]</ref>, as can be seen in <ref type="figure" target="#fig_3">Figure 5</ref>. For detection tasks, the lighting direction remained static relative to the terrain model, while the camera rotated about the vertical axis.  Subjects viewed the animations on a 27-inch 120Hz LCD monitor with a resolution of 1920x1080. Subjects input their decisions using a standard USB mouse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT ONE</head><p>The goal of the first experiment was to evaluate the effectiveness of grids and contours compared to plain shading for communicating the relative sizes of changes on dynamically deforming surfaces.</p><p>Participants watched side-by-side animations and had to choose which depicted a more significant change.</p><p>We predicted that contours would perform better than grids, because while both convey shape and change information through texel deformation, contours have the added advantage of varying numbers of texels, which can be counted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.1</head><p>Tasks This experiment presented users with blocks of comparison tasks as previously described in Section 3.1. Each block of comparison tasks used a single texturing technique (none/plain shading, grids, or contours). There were two blocks for each technique, for a total of six blocks (216 individual comparison tasks) per participant. The order of techniques was randomized for each participant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>A logistic regression was performed on the data. Logistic regression is a special type of regression analysis that provides a direct probability model to assess the relationship between a binary dependent variable (the response variable) and one or more continuous or categorical independent variables (predictor variables). The model assumes a Bernoulli distribution to the data, as opposed to a normal distribution, and outputs its results as odds ratios. The response variable input to the model was the result of the comparison (correct vs incorrect), and the predictor variables included as main effects were: texture technique, change size, relative difference between the two animations, and how many times the comparison animation was viewed before responding.</p><p>The model was run for each reference level of texturing technique in order to evaluate the contrasts between the three techniques. The same procedure was performed with each of the change size and relative difference variables to assess their contrasts.</p><p>A Hosmer-Lemeshow goodness of fit (H-L) test yielded X2(8) = 5.49 and was insignificant (p &gt; 0.05), indicating a good fit of the model to the observed data. Alternate models, which included interaction effects between texture and change size, texture and relative difference, or change size and relative difference were also evaluated using the likelihood ratio test, but none of these models were found to be significantly better than the model without them.</p><p>As we had predicted, contours performed significantly better than grids for enabling correct comparisons (Wald z = 2.25, p &lt; 0.05), improving the odds of making a correct comparison by 27%. Contour lines also had significantly better odds than plain shading (Wald z = 2.42, p &lt; 0.05), increasing the odds by 29% over shading alone. Grids, on the other hand, did not appear to enhance performance much, as there was only a 2% increase in the odds of making a correct comparison over just shading alone, and this difference was not statistically significant.</p><p>The size of changes had a predictable and often significant effect, with the odds of making correct comparisons generally increasing with larger change sizes and greater relative differences. For example, large changes provided a 40% increase in odds over small changes (Wald z = 3.20, p &lt; 0.01). This is likely due to the increased number of changing pixels and texels that provide helpful information.</p><p>As would be expected, the relative difference between the changes in the two animations was the strongest factor in performance. The odds of correctly comparing large-30% relative differences were 116% higher than those of medium-15% relative differences (Wald z = 6.46, p &lt; 0.0001) and 316% higher than small-7.5% relative differences (Wald z = 12.52, p &lt; 0.0001). Similarly, medium-15% relative differences had 93% higher odds of a correct comparison than small-7.5% relative differences (Wald z = 6.74, p &lt; 0.0001).</p><p>The number of times the looping animations were watched was a significant continuous factor (Wald z = 3.40, p &lt; 0.001). Each repeated viewing increased the odds of making a correct comparison by 12%. The average number of times the animations were viewed before choosing was 2.35 times for plain shading, 2.34 times for grids, and 2.67 times for contours.</p><p>Interestingly, if one considers only the correct responses, the average additional number of times watched were: 0.18 more for plain shading, 0.01 more for grids, and 0.31 more for contours. Thus, although contours required slightly more viewing time than plain shading or grids, that additional decision time corresponded to increased comparison accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENT TWO</head><p>The goal of the second experiment was to determine the relative attention-getting abilities of grids and contours in drawing the gaze of a viewer to changes as they occur.</p><p>Participants watched animations of dynamic terrain models and had to indicate each time they detected a change occurring.</p><p>We predicted that contours would perform significantly better than grids, due to both the induced motion cues and the appearance of new texels. <ref type="figure">Fig. 9</ref>. Error rates for each technique in the comparison tasks of Experiment 1 across multiple change size and relative difference conditions.</p><p>Error bars represent the 95% binomial probability confidence interval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Tasks</head><p>This experiment presented users with blocks of detection tasks as previously described in Section 3.2. Each block of detection tasks used a single texturing technique (none/plain shading, grids, or contours). There were two blocks for each technique, for a total of six blocks (216 individual changes) per participant. The order of techniques was randomized for each participant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.2</head><p>Results A logistic regression model including texturing technique, change width, change height, and the two-way interactions between each of them was analyzed, with the detection of a terrain deformation (detected vs missed) as the dependent response variable. An H-L test was applied to the model, yielding X2(8) = 1.61 and an insignificant (p &gt; 0.05) difference between observed responses and expected responses, indicating a good model fit. The significances of the interaction terms were assessed using a likelihood ratio test against a model without any interaction terms.</p><p>Several interactions were found to be significant, including texture-width (X2(4) = 205.36, p &lt; 0.0001), texture-height (X2(10) = 28.63, p &lt; 0.01), and width-height (X2(10) = 27.16, p &lt; 0.01). The significant reductions in deviance along with the H-L test of the model including the interactions suggests that the effect of texture technique for detecting terrain deformations is a complex interaction that depends significantly on both the width and height of the changes.</p><p>Due to the presence of multiple interaction effects, the conditional effects of texture were analyzed for each of the 18 combinations of change width and height to judge the performance of the three texture conditions. As can be seen in <ref type="figure" target="#fig_0">Figure 10</ref>, the results did not indicate any one technique was consistently best across all conditions.</p><p>We predicted that contours would perform the best, and for changes with larger widths, contours were indeed the best texturing technique for ensuring detection. Here, the advantage of contours over both plain shading and grids was highly significant (p &lt; 0.0001) for all heights, and greatly improved the odds of detecting a change anywhere from 240% to 1067%. However, for medium-width changes, their advantage over plain shading and grids was much slimmer, only reaching statistical significance for extrusions of medium and large height conditions. Finally, for the smallest-width changes, contours were found to be significantly less effective (p &lt; 0.01) than grids for almost all height variations, with grids improving odds by 113% to 355%.</p><p>Interestingly, for these small-width changes, plain shading actually showed the best overall performance, significantly better (p &lt; 0.05) than contours for all heights, improving odds by 90% to 383%, and significantly better (p &lt; 0.05) than grids for the largest heights, improving odds by 89% to 103%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENT THREE</head><p>The goal of the third experiment was to determine if modifying a traditional rainbow color scale to be banded into discrete colors instead of smoothly interpolated would produce any of the benefits of contours observed in Experiments 1 &amp; 2.</p><p>For those same comparison and detection tasks, we predicted that the discretely banded rainbow color scale would perform better than the smoothly interpolated rainbow color scale due to the additional cues from the induced contour lines between colors. Error bars represent the 95% binomial probability confidence interval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Stimuli &amp; Tasks</head><p>We extended our OpenGL fragment shader (described in Section 3.3) to implement the two rendering conditions on a per-pixel basis. Examples of the resulting techniques can be seen in <ref type="figure" target="#fig_0">Figure 11</ref>.</p><p>Rainbow smooth colors each pixel of the terrain surface with a smoothly interpolated color value directly corresponding to its height.</p><p>Rainbow banded colors each pixel of the terrain surface with a discrete color value from a limited set <ref type="bibr" target="#b19">(20)</ref> of easily differentiable colors, resulting in hard discontinuities between the color/height values that approximate contour lines. More specifically, instead of linearly traversing the color scale and picking evenly spaced color values, we unevenly stepped through the possible range of colors, making sure that the color at each step was noticeably different than its neighbors.</p><p>This experiment presented users with both comparison task blocks (see Section 3.1) and detection task blocks (see Section 3.2). Each block used a single texturing technique (rainbow smooth or rainbow banded). There were two blocks for each technique, for a total of four blocks. The order of tasks and techniques was randomized for each participant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results</head><p>The comparison tasks were analyzed with the same logistic regression model as in Experiment 1. However, a likelihood ratio test showed a significant interaction between change size and relative difference (X2(4) = 21.00, p &lt; 0.001), so the interaction term was added to the model; other interactions between texture, change size, and relative difference were evaluated but failed to yield a model that was a better fit than those without the interaction terms. The H-L test of the model yielded X2(8) = 6.41 and was insignificant (p &gt; 0.05), indicating a good fit of the model to the data.</p><p>As we predicted, the banded rainbow texture performed significantly better than the smooth rainbow texture (p &lt; 0.001), improving the odds of making a correct comparison by 46%. A trend similar to Experiment 1 was found with regards to feature change size and relative difference, where an increase in either resulted in improved odds ratios of making a correct comparison. The interaction between these two variables is complex and more difficult to tease apart, but generally points to a synergistic effect where larger feature change sizes and relative differences work together to provide better odds for successfully completing the task than either variable would on its own.</p><p>It is important to note that, due to the limited number of colors, the intervals between the implicit lines of the rainbow banding were significantly wider than the explicit contour lines in the previous experiments. Wider intervals translate to less detail, which thus implies there should be lower accuracy in the comparison tasks. Therefore, it may at first seem surprising that errors rates for the rainbow banded comparisons were lower than the explicit contour equivalents in Experiment 1. However, this is most likely attributable to the added color information, which provides absolute height values that greatly assist comparison tasks.</p><p>The detection tasks were analyzed with a similar logistic regression model as in Experiment 2, this time including only the interaction term between texture and change width (X2(2) = 59.08, p &lt; 0.0001), as texture-height and width-height interactions were not significant for this model. An H-L test yielded X2(8) = 11.30, failing to reach significance (p &gt; 0.05), which suggests a good model fit.</p><p>Results similar to Experiment 2 were observed, with smooth rainbow texture being analogous to plain shading, and banded rainbow texture being analogous to the contours texture. Again, the performance varied significantly with the size/width of the changes:</p><p>For changes with small widths, the smooth rainbow texture performed significantly better (p &lt; 0.01) than the banded rainbow texture for large and small depressions, improving odds of detecting a change by 58%.</p><p>For medium-width changes, the odds of detecting a change when using the banded rainbow texture were 32% better than the smooth rainbow texture, but just as in Experiment 2, this performance gain was too slight to reach significance.</p><p>For changes with larger widths, the banded rainbow texture had a highly significant advantage (p &lt; 0.0001) over the smooth rainbow texture, improving the odds of detecting a change by 297%.</p><p>These results support the pattern suggested by Experiment 2: Plain shading and smooth color transitions allow viewers to more easily notice very small changes, but banded/contour textures vastly outperform smooth/shading for larger changes. Combined with the insignificant performance differences for medium-width changes, the effect implies a transition of texture advantages from plain shading to contoured/banded textures as the size of the changes increase.</p><p>The results of this experiment confirm our hypothesis that, by discretizing traditionally interpolated color scales into easily differentiable bands, the implicit pseudo-contour lines that are generated confer the same benefits as explicit contour lines. <ref type="figure" target="#fig_0">Fig. 12</ref>. Error rates for the comparison tasks in Experiment 3 for different change size and illustration type conditions. Error bars represent the 95% binomial probability confidence interval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION</head><p>While this study did not find any particular technique to be most effective across the board, there were some important observations which can be applied to improve dynamic 4D visualizations:</p><p>In the comparison tasks of the first experiment, contour lines performed strongly over grids and plain shading, indicating they are well suited for conveying the sizes of changes on dynamic surfaces. As a previous study had conversely ranked grids over contours for static surface perception tasks, this finding confirms that, like other visual cues, structured texturing techniques can perform differently in dynamic scenes versus static ones.</p><p>Considering the number of times that subjects watched the looping animations before making correct decisions reveals that, although they rewatched the contour lined surfaces the most, that additional decision making time corresponded to increased accuracy. This may be due to the inherent countability of contour lines, which have a varying number and density of texels, as opposed to techniques such as grids, which have a static number of texels which only deform. This is not to say that the lines were individually counted, but viewers can generally glance at two sets and determine which has more elements without explicitly counting them.</p><p>One limitation of our comparison experiment that warrants further study is how much this ability to rewatch animations affects the results. For example, if participants were instead only allowed to view the animations once, would contour lines lose their edge over grids?</p><p>The results of the second experiment were less straightforward. They suggest that, for detecting subtle changes in a dynamic terrain, the effectiveness of each texturing technique is highly dependent on the size of the changes:</p><p>For very small changes, plain shading's strong advantage over contours and grids was likely due to the lack of increased visual noise that comes from the texels of grids and contours. The advantage grids had over contours here may be explained by the uniform texels of the grids generating less visual noise than the more chaotic contours. Thusly, applying a structured texturing technique is not necessarily always advantageous, as decreasing the signal-to-noise ratio can cause small changes to be missed.</p><p>However, as changes sizes increase, the attention grabbing benefits of contour lines begin to outweigh the increased visual noise: For medium-sized changes, performance was similar for all three texturing conditions, though in a few cases contours did show slight but significant advantages over grids and plain shading. For large changes, contours performed strongly, showing a clear and significant advantage over plain shading and grids in all scenarios.</p><p>The performance of grids and contours for different change sizes is also heavily dependent on the cell sizes and contour intervals. As features shrink to sizes near or below the texel size, these texture techniques lose the ability to portray them. We needed to keep the grid cell sizes and contour intervals static in our study, but in practice they can be adjusted to properly mesh with feature sizes. This can be challenging in geospatial applications, where the cell sizes or contour intervals must stay consistent or subdivide evenly, as they are often used for measuring. For contours in particular, further investigation is warranted into the relationship between line intervals and portrayal of changes of diverse frequencies and sizes.</p><p>Participants were administered a post-study questionnaire to assess their preferences and experiences using the different texturing techniques. The participants were asked to rate the ease of interpretation, accuracy, and appearance/aesthetics of the five illustration techniques on a five-level Likert-type scale. Each participant was also asked to indicate and briefly explain their preferred illustration technique for both comparison and detection tasks. Extra space was provided for the participant to indicate any reasons they disliked a certain technique, as well as a small section for additional comments.</p><p>Overall, participants ranked the rainbow banded technique highest for ease of interpretation, accuracy, and appearance/aesthetics. Between plain shading, grids, and contours, participants on average ranked contours the highest for those three categories as well. For comparison tasks, participants overwhelmingly indicated a preference for the rainbow banded technique, while detection tasks garnered a larger spread of responses, with contours receiving slightly more votes than the other texturing techniques. This lends further evidence to contour and pseudo-contour texturing techniques not just having empirical effectiveness, but also leading to a more desirable user experience when used with dynamic scenes.</p><p>Because dynamic scenes are perceived differently than static scenes, there is still much room for future perceptual research and human factors experimentation regarding optimal visualization methods, particularly into the relative sizes of texels and the changes and terrain features they are applied to.</p><p>The dynamic nature of the scenes might also be extended to explore other aspects common in interactive visualizations. For example, we did not test under conditions of more complex navigation actions, such as zooming or panning.</p><p>Another aspect of change representation that was not studied here was shape recall. It is likely that contours would perform well for qualitative shape recall (e.g. circular, oval, etc.), as they directly follow the perimeters of changes and features. However, grid cells' uniformity may increase quantitative accuracy in shape size recall. Error bars represent the 95% binomial probability confidence interval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>Previous studies reported that grid textures outperformed contour lines in surface perception tasks when applied to static surfaces. However, there was a lack of studies evaluating the performance of these structured textures as applied to dynamically changing surfaces, a distinction that has been shown to be perceptually significant for various visual cues. This study directly addressed this unexplored topic by evaluating grids and contour lines in a dynamic visualization environment. It was hypothesized that due to the different manners in which each technique's texels change during surface deformation, that contours would actually perform better than grids.</p><p>The results of the first experiment confirmed that indeed, when applied to dynamic surfaces, contour lines, in general, provided better visual cues than grids, allowing viewers to more accurately compare and judge which of two similar animations depicted a more significant change occurring.</p><p>The second experiment, which tested how well the techniques ensured viewers noticed subtle changes, was less conclusive: Contours did outperform grids for larger changes, but as change sizes decreased, contours became less effective than grids and even plain shading. This is likely attributable to decreased visual noise in the latter techniques.</p><p>Finally, our third experiment confirmed the observations of the first two, and showed that by discretizing an interpolated color scale to create a banded color scale, the implicit pseudo-contour lines conveyed the same benefits as explicitly added contour lines.</p><p>In conclusion, for dynamic scenes in which terrain models and other similar surfaces change over time, generally contour lines are the best choice of structured texture. They draw attention to changes as they occur, and enable more accurate judgement of relative sizes.</p><p>As dynamic 4D models and simulations are becoming more commonplace, these findings are increasingly applicable to the design of modern geospatial and other scientific data visualizations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The three size conditions of changes used in comparison blocks, shown as both extrusions and depressions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The three difference conditions used in comparison blocks. The leftmost column shows the three original change sizes, while the next three columns show versions that are 7.5%, 15%, and 30% smaller.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>The nine combinations of change width and height conditions used in detection blocks. Top row -narrow width, middle row -medium width, bottom row -wide width. Left column -large height, middle column -medium height, right column -small height. Examples of random pseudo-terrain models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Plain shading (top), grids (middle), contours (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Viewing angle of camera relative to terrain model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Angle of directional light source relative to terrain model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Top-down view of light source angle relative to camera in comparison tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 .Fig. 10 .</head><label>1110</label><figDesc>Smooth (top)  and banded (bottom) rainbow color scales. False-negative error rates for each technique in the detection tasks of Experiment 2 across multiple change width and height conditions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 13 .</head><label>13</label><figDesc>False-negative error rates for each technique in the detection tasks of Experiment 3 across multiple change width and height conditions.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was partially supported by NOAA Grants #NA0NOS-4001153 and #NA10NOS4000073.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Conveying the 3D shape of smoothly curving transparent surfaces via texture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Interrante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Pizer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="98" to="117" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Showing shape with texture: two directions seem better than one</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hagh-Shenas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Interrante</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE 5007: Human Vision and Electronic Imaging VIII</title>
		<meeting>SPIE 5007: Human Vision and Electronic Imaging VIII</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="332" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">View direction, surface orientation and texture orientation for perception of surface shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sweet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ware</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Graphics Interface</title>
		<meeting>of Graphics Interface<address><addrLine>Waterloo, Ontario, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="97" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Perception of surface curvature and direction of illumination from patterns of shading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Todd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mingolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="583" to="595" />
			<date type="published" when="1983-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Three Gradients and the Perception of Flat and Curved Surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Cutting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Millard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Experimental Psychology, General</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="198" to="216" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Visual perception of planar orientation: dominance of static depth cues over motion cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cornilleau-Pérèz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Droulez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Miège</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bourdoncle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1402" to="1412" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Texture properties affecting the accuracy of surface attitude judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Velisavljević</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="2166" to="2191" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Egeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yantis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual Attention: Control, Representation, and Time Course</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="269" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The control of attention by abrupt visual onsets and offsets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="567" to="571" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Using NPR to evaluate perceptual shape cues in dynamic environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Winnemöller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gooch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th international symposium on Non-photorealistic animation and rendering</title>
		<meeting>the 5th international symposium on Non-photorealistic animation and rendering</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="85" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Integration of stereo, shading and texture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Bülthoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Mallot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="119" to="146" />
		</imprint>
		<respStmt>
			<orgName>AI and the Eye</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Shape from Texture: Ideal Observers and Human Psychophysics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Bülthoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sheinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1723" to="1737" />
			<date type="published" when="1993-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The Kinetic Depth Effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>O'connell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="205" to="217" />
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The perception of three-dimensional structure from rigid and nonrigid motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Todd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="97" to="103" />
			<date type="published" when="1984-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3-D structure perceived from dynamic information: a new theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Domini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Caudek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="444" to="449" />
			<date type="published" when="2003-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stereo and motion information are not independently processed by the visual system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Domini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Caudek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tassinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="1707" to="1723" />
			<date type="published" when="2005-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Animation of orthogonal texture patterns for vector field visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bachthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="741" to="755" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Illustrating surface shape in volume data via principal direction-driven 3D line integral convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Interrante</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH &apos;97</title>
		<meeting>SIGGRAPH &apos;97</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="109" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brewer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spectral Schemes: Controversial Color Use on Maps</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="203" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visual motion and attentional capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Hillstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yantis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="399" to="411" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Perception of shape from shading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Ramachandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">331</biblScope>
			<biblScope unit="page" from="163" to="166" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
