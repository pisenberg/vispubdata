<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gaze Stripes: Image-Based Visualization of Eye Tracking Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuno</forename><surname>Kurzhals</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Hlawatsch</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Heimerl</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Burch</surname></persName>
						</author>
						<title level="a" type="main">Gaze Stripes: Image-Based Visualization of Eye Tracking Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TVCG.2015.2468091</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Eye tracking</term>
					<term>time-dependent data</term>
					<term>spatio-temporal visualization</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Fig. 1: Results from the Kite video. In the upper part, individual frames from the video are shown together with gaze points (yellow circles). The lower part shows our gaze stripes approach: the image data around the gaze points are displayed over time.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The analysis of eye tracking data becomes challenging if more information (e.g., sequential viewing behavior) has to be extracted from the data than just the spatial distribution of attention on a visual stimulus. Furthermore, if the data of a large group of participants has to be analyzed, existing visualization techniques usually suffer from visual clutter or provide only a time-and participant-aggregated view of the data. The difficulties increase with the number of participants and the duration of the recorded eye gaze data. Moreover, large areas of the context of the presented stimulus are typically lost due to overplotting with additional graphics or by displaying the stimulus in a different view, which separates context and eye tracking data visualization.</p><p>An even more challenging problem with existing visualization techniques is the investigation of dynamic stimuli. Examples include animated diagrams, videos, or graphical user interfaces. For the analysis of such eye tracking data, defining areas of interest (AOIs) is typically required to extract important information. By providing semantic information about objects or special regions in the stimulus, alternative techniques for the visual and statistical analysis of the participants' viewing behaviors become feasible. However, the annotation process is a tedious preparation step that requires manual input from the analyst. Although computer vision and automatic clustering techniques exist to facilitate this process (see Holmqvist et al. <ref type="bibr" target="#b14">[15]</ref> for a detailed overview of AOI annotation techniques), the manual annotation of AOIs in a frame-by-frame fashion or with dynamically changing bounding shapes is usually still required and time consuming. Furthermore, the insight gained from annotated data strongly depends on the properties of the AOIs (see Section 4.1).</p><p>In this paper, we present a new visualization approach for eye tracking data: gaze stripes. Our approach does not require any AOI definitions, works for static and dynamic stimuli alike, and provides good scalability with respect to the number of participants and the temporal length of gaze sequences. Furthermore, it preserves the temporal order and information of the data. We achieve this by displaying a sequence of thumbnail images from the stimulus content for each participantthe gaze stripe. The sequences are based on the gaze samples along the participants' scanpaths and the thumbnails contain image data of the local context of a stimulus around the gaze points. <ref type="figure">Figure 1</ref> displays an example of our technique applied to the Kite video (see <ref type="table" target="#tab_3">Table 2</ref> for details on the data set). The gaze stripes for the participants are displayed in a stacked manner. On the overview level, patterns and outliers can be detected and a general impression of the scene is provided. For instance, all participants followed the motion of the yellow kite. We get also an impression of the parts of the scene through which the kite moves and how it is oriented. Zooming-in allows a more detailed analysis of the eye tracking data, demonstrated with the close-up images: (a) different participants focused on different parts of the kite; (b) while most of the participants focused on the kite, one participant looked at the person controlling the kite; (c, d) the changing orientation of the kite is clearly visible.</p><p>Besides describing and demonstrating our technique, we present our implementation with different complementary views on the data that can be used by the analyst to enrich the visualization. In addition to discussing the properties of our approach, we also report on the feedback of several analysts working with our implementation.</p><p>Our main contribution is a new technique for investigating eye tracking data from multiple participants for both static and dynamic stimuli. As we demonstrate in this paper, our approach allows a detailed analysis of the data without the need to define AOIs or apply complex algorithms. The visualization is occlusion-free and easy to understand, even for non-experts. Especially for dynamic stimuli without annotation, our approach provides information about the data more efficiently than existing state-of-the-art techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Analyzing eye tracking data by means of visualization has become increasingly popular in the visualization community. Although there are some prominent visualization techniques for eye movement data such as attention maps <ref type="bibr" target="#b36">[37]</ref> or scanpath visualizations such as gaze plots <ref type="bibr" target="#b10">[11]</ref>, our approach has some strengths compared to them: In general, attention maps and gaze plots lack detail information due to aggregation, occlusion, or visual clutter <ref type="bibr" target="#b27">[28]</ref>. Our technique is free of visual overlap, does not occlude the stimulus as other techniques, and does not aggregate the data over time and participants. Moreover, due to the representation with small-sized thumbnails, it still remains scalable in both the time and participant dimension. Another great advantage is the intrinsic support for static and dynamic stimuli alike. Existing techniques are typically designed for static stimuli and need animated variants to make them applicable to video data; however, animation is problematic because it comes with several perceptual and cognitive issues <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>The design of our visualization technique targets good support for time-oriented visualization <ref type="bibr" target="#b0">[1]</ref>: linear time is put along parallel timelines, one for each participant. Here, we apply the concept of timeto-space mapping, i.e., we provide a static visualization (apart from reactions to user interaction) that has several benefits compared to an animated sequence of the same data <ref type="bibr" target="#b34">[35]</ref>.</p><p>Our technique represents the gaze data from multiple participants by stacking individual timelines on top of each other. This approach is visually similar to the work by Andrienko et al. <ref type="bibr" target="#b2">[3]</ref>. They visualize the distances to selected points of interest with color coding. Kurzhals et al. <ref type="bibr" target="#b17">[18]</ref> also apply a stacked timeline visualization for AOI-based eye tracking data from video. Although visually similar, these techniques all depend on annotated eye tracking data. The annotation is usually the most time-consuming preprocessing step. AOIs are either manually annotated or extracted automatically with clustering approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b28">29]</ref>. However, automatic annotation typically lacks a semantic interpretation of the stimulus and therefore, a time-consuming manual annotation is often the only way to process the data to apply the above techniques. The main advantage of our approach is that the definition of AOIs is not required.</p><p>Fixation data can be mapped individually to AOI labels without actually defining boundary shapes. A semi-automatic approach for such an annotation can be found in SemantiCode <ref type="bibr" target="#b24">[25]</ref>, which uses image thumbnails based on fixation positions from a video stimulus to let the user define to which AOI they belong. This information is then applied to create a classification scheme for the remaining fixations in the data. This approach is similar to ours in terms of interpreting gaze data by image thumbnails. However, SemantiCode applies this principle for annotation only and further analysis by statistical or vi-sual techniques is still required to interpret the data. Also, in dynamic stimuli, the changing conditions still require the analyst to perform manual annotations. With our visualization approach, the gaze data can be interpreted directly by the analyst, while automatic processing of the image data can be applied on demand to further support the interpretation of selected time spans.</p><p>In the field of bioinformatics, methods for scalable genomic alignments have been developed. For example, the Sequence Surveyor <ref type="bibr" target="#b1">[2]</ref> or the iHAT <ref type="bibr" target="#b13">[14]</ref> systems rely on a representation loosely related to ours, using color-coded sequences that scale to large data sets in the vertical and horizontal dimensions <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b29">30]</ref>. Although these techniques are visually similar to gaze stripes, our technique does not require any annotation of categories. In fact, we integrate local stimulus information in the form of small images, which is not done by any of the above methods.</p><p>Thumbnail-based visualizations to summarize video data are discussed by Manovich <ref type="bibr" target="#b21">[22]</ref>. The author describes an approach to stack key frames of video game play of a user, as well as of multiple video sequences (see also Takahashi et al. <ref type="bibr" target="#b31">[32]</ref> and Christel and Martin <ref type="bibr" target="#b5">[6]</ref>). In these publications, only complete frames were visualized to summarize the content of a video without the spatio-temporal eye gaze information of multiple participants. We adapt this principle and extend it by a gaze data-driven selection of the sub-scene context to create thumbnails for an arbitrary number of participants that can then be compared with each other. By applying a sequence of thumbnails with an adjustable crop size as in our technique, we are able to generate occlusion-free stripes for the participants' eye gazes while still showing stimulus information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EYE TRACKING VISUALIZATION REQUIREMENTS</head><p>Statistical analysis is a common approach to evaluate recorded eye tracking data. Established metrics <ref type="bibr" target="#b14">[15]</ref> (e.g., fixation durations or saccade lengths) can be applied for descriptive and inferential statistics on data with or without annotation. However, not all analysis problems can be solved by statistical methods alone. Especially, questions considering the semantics and spatio-temporal context of the data (e.g., the order in which regions in the stimulus have been visited) require an additional visual representation of the data.</p><p>In this section, we first review the data dimensions of recorded eye tracking data and the stimulus. Furthermore, we discuss the requirements a visualization of this data should meet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Model</head><p>The visual stimulus used in an eye tracking study can either be static or dynamic. A static stimulus can be described as an image I with dimensions m × n. If we deal with a dynamic stimulus, a sequence of images has to be inspected by the study participants, which can be modeled as I := (I 1 ,...,I T ). T describes the number of samples in the data. T can be determined by the sampling rate of the video or the eye tracker (in our case 60 Hz). Since a detailed gaze stripe can become very long, we allow the analyst to reduce the number of displayed time steps on demand, in order to obtain a better overview. In our examples, T is constant for all participants, in contrast to unsynchronized eye tracking data (e.g., from mobile eye tracking glasses).</p><p>The eye movements of an individual participant are modeled as a sequence of gaze points P := (p 1 ,..., p T ) where each point p t with t ∈ {1,...,T } is either mapped to the local coordinate system of the stimulus, or discarded due to invalid data. The eye movements of N participants can be modeled as</p><formula xml:id="formula_0">L P := {P 1 ,...,P N }.</formula><p>Moreover, we define a bounding box parameter δ for the size of the thumbnail images extracted. The gaze points are used as center points of these bounding boxes. The size of the thumbnails can be adjusted interactively by the analysts. We allow them to vary the size δ of a thumbnail between 40-100 pixels. With the size of 100 pixels, a visual angle of around 2 • of a participant at a distance of 64 cm from the screen can be covered <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26]</ref>. In our presented use cases, we apply the gaze stripes with 100 × 100 pixels thumbnails, if not mentioned otherwise, to preserve the details of the local stimulus context, allowing for easy interpretation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Requirements</head><p>For the visualization of eye tracking data, four main requirements have to be considered (based on the taxonomy by Blascheck et al. <ref type="bibr" target="#b3">[4]</ref>): ditional data dimension is the number of recorded participants that is required to compare between participants and identify groups of similar viewing behavior as well as outliers.</p><formula xml:id="formula_1">• (R1)</formula><p>Depending on the approach, different visualization techniques focus on one or several of these aspects and aggregate or neglect the others. In our comparison (Section 5), we will discuss how standard eye tracking visualizations and our novel approach meet these requirements and compare them to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">GAZE STRIPES</head><p>The visualization with gaze stripes for multiple participants is based on two data sources: the visual stimulus that was investigated by the participants and the spatio-temporal point-based gaze information that was recorded by an eye tracker. This means that we have to face a coupled data analysis problem and the resulting visualization should meet all four requirements mentioned above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Visual Design of Gaze Stripes</head><p>Our visualization approach displays the gaze data of individual participants on separate timelines along the x-axis. The list of all participants' timelines L P is ordered along the y-axis. At initialization, the participants are ordered by their ID. Ordering based on scanpath similarity can be performed by clustering of time spans. For each gaze point p t , a thumbnail image with the local context of the stimulus is cut out and stacked along the timeline <ref type="figure" target="#fig_0">(Figure 2</ref>(c)). Invalid sample points (e.g., due to missing eye detections) are not drawn, leaving an empty field for this time step to keep the data synchronized.</p><p>Our approach maps participants and time in a similar way as scarf plots <ref type="figure" target="#fig_0">(Figure 2</ref>(b)), i.e., time along the x-axis and participants along the y-axis, but it does not require any annotation. Also, the amount of details visible in the scarf plots strongly depends on the defined AOIs. On a coarse scale with 3 AOIs <ref type="figure" target="#fig_0">(Figure 2</ref>(a)), the scarf plots provide only the information that the participants first looked at the sky (time step 1), then at the sea (time step 2). In the gaze stripes, we can directly see that both participants did not just look at the sea, but at the same boat in time step 2. To acquire this information with scarf plots, either a definition of more AOIs is required, or additional visualizations have to be applied. Note that in this example, the gaze plots <ref type="figure" target="#fig_0">(Figure 2</ref>(a)) also convey this information, but do not scale well with the number of participants and scanpath length due to visual clutter, whereas our approach creates no overlap between gaze visualization and stimulus.</p><p>Raw gaze data coordinates as well as filtered data (i.e., fixations) can be analyzed with gaze stripes. <ref type="figure" target="#fig_2">Figure 3</ref> illustrates the differences between gaze stripes with and without filtering of the gaze samples for the Antigenic Shift data set (see <ref type="table" target="#tab_3">Table 2</ref>). For fixation data, microsaccadic eye motions are filtered and identical thumbnails tend to recur more often, which helps to identify local context (Figures 3(a) and 3(b)). However, it could be more difficult to detect certain viewing behavior with filtered data, e.g., if the participant follows objects <ref type="figure">(Figures</ref> 3(c) and 3(d)). If fixation filtering is applied, our approach could be adapted to represent one thumbnail per fixation, but this would impair the comparability of synchronized time steps. For the stimuli used in this paper <ref type="table" target="#tab_3">(Table 2)</ref>, we applied the Tobii fixation filter with standard settings (velocity threshold = 35 pixels/sample; distance threshold = 35 pixels), as recommended by Tobii <ref type="bibr" target="#b33">[34]</ref>.</p><p>Since every sampled data point can be represented on a linear timeline with our visualization, the first requirement (R1) is met. The local context of the stimulus (R3) is preserved without occlusion. Multiple participants (R4) can be aligned and compared easily over time. However, the spatial position (R2) in the global context of the stimu-lus is not represented in the gaze stripes. Therefore, we provide a set of coordinated multiple views on complementary aspects of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Complementary Views</head><p>The interpretation of gaze data in the context of the whole stimulus can be problematic when using only gaze stripes to visualize the data. Since this technique provides only local context information at the gaze position, a link to the stimulus is often required. Therefore, we provide a set of complementary views <ref type="figure" target="#fig_5">(Figure 4</ref>) that can be applied to address this issue and enrich the visualization with insights gained during the analysis process. Besides zooming and panning the gaze stripes, the analyst can select time steps and time spans of single or multiple participants' data by simple mouse dragging. A zoom lens <ref type="figure" target="#fig_5">(Figure 4</ref>(g)) can be activated on demand to enlarge the local neighborhood of the currently hovered thumbnail.</p><p>The global context is displayed in a video player with a bee swarm visualization <ref type="bibr" target="#b3">[4]</ref>. The analyst can select the different annotation items Area Marker, Note, Screenshot, Dispersion Histogram, and Hierarchical Clustering via a context menu. Each item is freely scalable, movable, and individual colors can be applied to support visual grouping of items. All items that refer to single time steps or time spans in the gaze stripes set markers on the timeline in their corresponding color. <ref type="figure" target="#fig_5">Figure 4</ref> displays an example of a time span from the UNO data set (see <ref type="table" target="#tab_3">Table 2</ref>) that comprises the event when the right player has to pick up a new card from the uncovered stack of cards. The figure shows a screenshot created completely with our implementation (except for the enumeration symbols (a)-(g)). All items were applied to describe the viewing behavior of the participants during this event:</p><p>(a) Area Marker: Selected regions of the gaze stripes can be highlighted by a colored frame around the involved thumbnails. Individual participants or groups of participants can be marked for an annotation with the other items.</p><p>(b) Annotation Note: Note items provide the analyst a free-text field to annotate events of special interest or comment on other items. This allows for a detailed description of the data to communicate the visualization.</p><p>(c) Screenshot: Depending on the analyst's selection, screenshots can be created for single time steps or over longer time spans. If only one time step is selected, all participants' gaze positions included in the selection are rendered into the video frame (c1). If a time span is selected, the last video frame is used to provide the scene context and the spatio-temporal development of the participants' scanpaths is depicted by gaze plots (c2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(d) Dispersion Histogram:</head><p>The visual similarity of regions in the stimulus can lead to similar thumbnails in the gaze stripes. Although this is an advantage for some cases, other situations require the analyst to know if the participants were looking synchronously at a particular region, or if their gaze was distributed between similar looking objects (see <ref type="bibr">Section 8)</ref>. Therefore, we provide the intersubject dispersion metric D t <ref type="bibr" target="#b23">[24]</ref>:</p><formula xml:id="formula_2">D t = 1 N N ∑ i=1 g i t,max − g i t (x i , y i ) g i t,max − g i t,avg<label>(1)</label></formula><p>For frame dimensions m × n with x ∈ {1, .., m} and y ∈ {1, .., n}, the gaze density function g t (x, y) is defined as:</p><formula xml:id="formula_3">g t (x, y) = 1 N N ∑ i=1 ϕ i (x, y)<label>(2)</label></formula><p>The gaze positions (x i , y i ) from N participants at time t, are replaced by the Gaussian function:</p><formula xml:id="formula_4">ϕ i (x, y) = e − (x−x i ) 2 2σ 2 + (y−y i ) 2 2σ 2<label>(3)</label></formula><p>In the equations, g t,max and g t,avg denote the maximum and average of g t (x, y) with i denoting that the i-th gaze position was excluded from g i t (x, y). σ = 40 pixels was chosen according to the visual angle at a distance of 64 cm, as described in Section 3.1. Higher values of D t indicate that the participants' gaze data was distributed more widely over the scene, and lower values indicate time spans of attentional synchrony <ref type="bibr" target="#b30">[31]</ref>.</p><p>(e) Hierarchical Clustering: The gaze stripes as main part of the visualization are ordered along the y-axis based on the participants' IDs. Since similar viewing behavior can occur between arbitrary participant IDs, a new ordering of the gaze stripes is required to obtain better visual coherence between neighboring stripes. Therefore, the selected time span can be duplicated and clustered based on the scanpath comparison described in Section 4.3. The result of the hierarchical clustering is then displayed as an item attached to the timeline.</p><p>(f) Video Player: The video player shows the context of the stimulus in combination with a gaze replay of the recorded eye movements. For video stimuli, the animated content is displayed, for static stimuli, only the gaze replay is presented on the image. For the gaze replay, the borders of the thumbnails for each participant are shown in the stimulus.</p><p>(g) Zoom Lens: With the zoom lens, single thumbnails and their neighborhood can be investigated without leaving the overview.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Scanpath Clustering</head><p>To  The cluster hierarchy is generated by an agglomerative hierarchical clustering algorithm <ref type="bibr" target="#b12">[13]</ref>. When invoked on sub-sequences of the gaze paths, it starts by adding each gaze stripe to its own cluster. These clusters are then joined recursively according to their similarity, joining the two most similar clusters during each iteration of the algorithm. The process stops once a single root cluster remains. This approach has been used for the analysis of eye tracking data <ref type="bibr" target="#b17">[18]</ref> in AOI-based analysis with the Levenshtein algorithm <ref type="bibr" target="#b19">[20]</ref> to measure sequence dissimilarity. The Levenshtein algorithm quantifies the edit distance of two sequences by counting the number of exchange, delete, and insert operations necessary to transform one sequence into the other.</p><p>For thumbnail sequences, we use a modified version that, rather than counting the number of exchange operations, considers the costs of each of these operations quantified by the distance between both thumbnails. The thumbnail distance is measured by the correlation of hue and saturation histograms of two images. Only the hue and saturation channels are used to reduce problems with shadows in the scene <ref type="bibr" target="#b37">[38]</ref>. For this, we use OpenCV's <ref type="bibr" target="#b4">[5]</ref> implementation of the </p><formula xml:id="formula_5">ρ H 1 ,H 2 = ∑ n k=1 (H 1 (k) −H 1 )(H 2 (k) −H 2 ) ∑ n k=1 (H 1 (k) −H 1 ) 2 • ∑ n k=1 (H 2 (k) −H 2 ) 2<label>(4)</label></formula><p>Our implementation of Levenshtein's string distance measure uses</p><formula xml:id="formula_6">d = (1 − ρ H 1 ,H 2 )</formula><p>as the distance for two images. We decided to use the OpenCV standard number of bins, 50 for the hue values, and 60 for the saturation values, as we found that they produced the best results. This method for calculating image sequence distance is akin to the one presented by Tan et al. <ref type="bibr" target="#b32">[33]</ref>, which was developed to compare long video sequences. We found histogram correlation to work well as an image distance measure for our data sets, but using the Levenshtein algorithm as a sequence distance measure is flexible enough to accommodate any other image comparison method in case they are better suited for particular data sets. An example result from our clustering algorithm is depicted in <ref type="figure" target="#fig_4">Figure 5</ref>. The clustering was generated for a sequence from the Car Pursuit data in which the camera follows a red car moving from right to left. Suddenly, a white car appears from the left, and some participants shift their attention to it. A clustering of the gaze stripes from the point at which the white car appears is depicted in <ref type="figure" target="#fig_4">Figure 5</ref>(a), while <ref type="figure" target="#fig_4">Figure 5(b)</ref> contains a screenshot of the video from this sequence including the gaze positions of the participants. The clustering shows the different reactions to the sudden appearance of the white car. While the six participants in the top cluster kept their eyes on the red car (P2, P3, P4, P7, P8, P9), the two participants in the lower cluster immediately shifted their gaze to the white car (P5, P6). In the upper cluster, the white car only appears in the thumbnails when it starts occluding the red one. Two of the participants in the center cluster (P0, P1) can be considered as outliers, as they get merged late in the clustering process. This is due to the fact that both participants were keeping their eyes on the side window of the red car rather than on its body, which differs in terms of the color profile.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Implementation</head><p>Our visualization is implemented in C++ with Qt 4.8. Using the internal BSP data structure of Qt for the thumbnails, our technique can be used interactively even with long image sequences after an initial loading phase (test system: Windows 7 64 bit on an Intel Core2Quad Q6600 with 2.40 GHz clock rate and 4 GB RAM). Image-based comparisons were performed with OpenCV 2.49.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">COMPARISON</head><p>We have been witnessing an increasing number of new visualization techniques that help interpret eye tracking data and support statistical results. Recently, Blascheck et al. <ref type="bibr" target="#b3">[4]</ref> have surveyed existing approaches by classifying them into nine categories based on properties of the eye tracking data and the types of visualization techniques.</p><p>There are two main categories for the visualization techniques: The first category are point-based techniques, which work with the recorded eye tracking data directly. AOI-based techniques are the second category and require additional annotation information.</p><p>Our approach falls into the category of point-based visualization techniques since only the gaze coordinates and the stimulus are required to generate the gaze stripes. The most common point-based visualization techniques are attention maps and gaze plots ( <ref type="figure" target="#fig_6">Figure 6</ref>).</p><p>In attention maps, the data is aggregated over time and over participants, showing the spatial distribution (R2) of attention on the stimulus (R3) with hot spots that usually indicate potential AOIs. This aggregation makes the attention map scalable for many participants. In the worst case, the attention map becomes more disperse and hot spots are harder to identify. However, due to aggregation, time (R1) and individual participants (R4) cannot be displayed in a static representation.</p><p>Gaze plots show the sequential nature of participants' scanpaths (R4) on the stimulus (R2, R3). Single gaze points or fixations are depicted by circles and connected based on their temporal order (R1) of appearance. Although all four requirements can be met in principle, gaze plots quickly lead to visual clutter and do not scale well with the number of participants and increasing scanpath lengths.</p><p>Gaze stripes as well as gaze plots maintain the temporal information of the data. However, gaze plots display time only on an ordinal scale, which makes it difficult to analyze and compare time spans. In contrast, gaze stripes offer a high temporal resolution-in our examples the full resolution of the data-on a discrete time scale <ref type="bibr" target="#b0">[1]</ref>, providing an overview and detailed information about specific time spans in the data. Since the content of a dynamic stimulus can vary significantly over time, static gaze plots as well as static attention maps can be reasonably applied only to short time spans of the data, whereas our technique is applicable to the full data set without any such restrictions.</p><p>Furthermore, gaze plots and attention maps are drawn directly on the stimulus to provide global context information. The local context is partially overdrawn, which reduces the provided information. Even semi-transparent overlays can avoid this issue only to a certain degree.</p><p>Both techniques can also be applied to dynamic stimuli by anima- A bee swarm can be described as an animated gaze plot that shows only one time step without any connections to previous data points. As mentioned, these animated approaches can be hard to interpret <ref type="bibr" target="#b34">[35]</ref>, since it is problematic to compare multiple participants with animation. Another point-based visualization that has been applied to eye tracking data is the space-time cube <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21]</ref> (see <ref type="figure" target="#fig_0">Figure 12</ref> for an example). Here, the spatio-temporal data is represented in a static volume. For the representation of participants' data, gaze points as well as scanpath trajectories (i.e., 3D gaze plots) can be displayed in an overview of the data. This approach meets the visualization requirements for time (R1) and space (R2) in terms of the spatial distribution over time. However, the stimulus context (R3) is not directly visible and the issues of gaze plots with visual clutter are still present in 3D, so the comparison of multiple participants becomes tedious (R4). Furthermore, issues with depth perception and perspective impair the interpretation of the data (R2, R3).</p><p>Investigating all other categories from the taxonomy of eye tracking visualizations <ref type="bibr" target="#b3">[4]</ref>, we are left with only three publications that are comparable to ours: two publications by Duchowski et al. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref> and one by Hurter et al. <ref type="bibr" target="#b15">[16]</ref>. All of these papers visualize eye tracking data by aggregating either fixations <ref type="bibr" target="#b8">[9]</ref> (impairing R1) or scanpath data <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16]</ref> (impairing R2) of multiple participants (impairing R4). These techniques aim at the analysis of common participant viewing behavior and therefore the detection of outliers and individual differences between participants is not possible. In contrast, we visualize the data on a per-participant basis where common and individual visual viewing strategies can be identified. Therefore, our visualization technique covers a white spot in the taxonomy of Blascheck et al.</p><p>Especially for video stimuli, AOI-based techniques are often the only way to answer certain analysis questions (e.g., fixation sequences or transition patterns on objects). Scarf plots are a common visualization technique to display scanpaths based on AOI information (see <ref type="figure" target="#fig_0">Figure 2</ref>(b)). The visited AOIs are represented with colored blocks on a timeline. Time (R1) and multiple participants (R4) can be represented very effectively with this approach. Spatial position (R2) and the context (R3) of the stimulus, however, are abstracted to color mapping of AOIs. We adapt this approach with a visually similar technique that differs in the represented property of the data and provides more detail about the stimulus than the scarf plots. Since scarf plots are an established technique, the interpretation of gaze stripes is easy even for inexperienced analysts due to its visual similarity to scarf plots. Therefore, our technique aims to provide comparable insights into the data without annotation. </p><formula xml:id="formula_7">• • • • • • • • • • • • • • • Gaze Plots • • • • • • • • • • • • • • • Space-Time Cubes • • • • • • • • • • • • • • • Scarf Plots • • • • • • • • • • • • • • • Gaze Stripes • • • • • • • • • • • • • • •</formula><p>In summary <ref type="table" target="#tab_2">(Table 1)</ref>, common visualization approaches (e.g., attention maps) that scale well with the data properties (e.g., number of participants) aggregate the data and therefore lack information for certain analysis tasks and do not fulfill the requirements completely. With annotation data available, additional techniques such as scarf plots can be applied, but the spatial context is usually abstracted or neglected. Gaze stripes meet most of the requirements without aggregation or abstraction to ease the interpretation of the data. They provide more context (R3) than scarf plots and space-time cube visualization, the other methods suitable for dynamic stimuli. However, gaze stripes represent space only indirectly with the image content of the thumbnails. For further comparisons, we refer to our use cases (Section 6) and our supplemental video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">APPLICATION TO STATIC AND DYNAMIC STIMULI</head><p>To test the applicability of our approach to real-world eye tracking data sets, we investigated two different types of stimuli: static and dynamic (see <ref type="table" target="#tab_3">Table 2</ref>). Stimuli 1 and 2 are images, recorded with a freeviewing task for 90 seconds on a Tobii T60XL (8 participants). The other stimuli consist of video and eye tracking data from a benchmark data set <ref type="bibr" target="#b16">[17]</ref> (25 participants). In the following, we detail findings for one dynamic (Memory video) and the two static (Antigenic Shift infographics and Arecibo Message article) stimuli as typical use cases. The Memory video is further used for our qualitative user study. <ref type="figure" target="#fig_8">Figures 8 and 9</ref> show results for the Antigenic Shift infographics. In the first example <ref type="figure">(Figure 8</ref>), we present the gaze stripe of a single participant <ref type="figure">(Figure 8(a)</ref>). The data consists of short fixations, resulting in short time spans of repeating thumbnails. This can be an indication for reading text. Zooming-in reveals that the participant was reading ... the annotations in the illustration at the bottom of the infographics at this time. This is hard to see in the gaze plot visualization <ref type="figure">(Figure 8(b)</ref>). The scanpaths of different participants occlude each other in the lower area and the labels hide the image content at the respective positions. Furthermore, it is difficult to recognize the temporal order of gaze points, which is represented only by the numbered labels, whereas this information is directly visible in the gaze stripes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Static Stimuli</head><p>The second example <ref type="figure" target="#fig_8">(Figure 9)</ref> shows gaze stripes for multiple participants. The overview shows that there is no coherent viewing behavior between participants in this static stimulus. All participants look at different parts of the infographics. However, we can see that the labels of the different text boxes in the infographics occur in several gaze stripes for the shown time range. We infer from this that the labels are visual cues for reading the infographics. As opposed to other techniques that show visual anchor points such as attention maps, our technique fully retains sequence information.</p><p>Results for the second static stimulus, the Arecibo Message article, are shown in <ref type="figure">Figure 10</ref>. The overview level already reveals if a participant was reading the text or looking at the image in the article. Enlarging the gaze stripes allows us to see at which part of the image the respective participants were looking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Dynamic Stimuli</head><p>For the analysis of the Memory video, we compare our approach ( <ref type="figure">Figure 11</ref>) with a space-time cube visualization <ref type="figure" target="#fig_0">(Figure 12</ref>) of the same data. The space-time cube visualization was created with ISeeCube <ref type="bibr" target="#b17">[18]</ref>, a visual analytics tool for the analysis of eye tracking data for video stimuli. The gaze data is presented as single points. The temporal dimension is represented along the green axis (from left to right). To resolve issues of depth perception and occlusion, the data is also projected to gray walls in 2D. The color mapping of the data points indicates time spans of high attentional synchrony between the participants in red. With the spatio-temporal overview, the distribution of attention of all participants becomes clearly visible: At the presented time step, all participants focus on the card that the hand flips <ref type="figure" target="#fig_0">(Figure 12(a)</ref>). After this event, the main attention seems to distribute between two cards <ref type="figure" target="#fig_0">(Figure 12(b)</ref>). With this representation, we cannot see directly what the new region is, which participants still looked at the first card at the time step (b), or if some participants switched their attention frequently between the cards. To obtain such information, individual 3D gaze plots and video replays have to be investigated, which quickly leads to visual clutter and a high analysis effort.</p><p>The gaze stripes for this example exhibit strong color variation. This is due to the different pictures on the game cards and the visible <ref type="figure">Fig. 10</ref>: Gaze stripes for three seconds of the Arecibo Message data. It can be determined efficiently when participants switched their attention between the text and the image, or when they moved their gaze along the image. <ref type="figure">Fig. 11</ref>: Results for the Memory video. Flipping a card draws the attention of most participants to this card. This is followed by a more divergent viewing behavior-the participants focus on different cards. <ref type="figure" target="#fig_0">Fig. 12</ref>: Space-time cube visualization of the Memory data set: The spatio-temporal distribution of the data from all participants is clearly visible. However, individual participants cannot be compared. human hand. On the overview level, we can identify different areas in the gaze stripes, which makes it already easy to spot time spans when the participants look at the hand that flips the cards (first zoom in <ref type="figure">Figure 11</ref>). Zooming-in allows us to identify the different objects on the cards and to see the motion of the hand. When the hand flips the first card, the participants follow the motion of the hand (left part of the gaze stripes). We can even get an impression of the motion, a wave-like pattern is visible. After that, the participants shift their focus between the second uncovered card (the black pyramid with the green area on the bottom) and the first uncovered card (two cars with yellow coloring). Following the gaze stripe of a single participant allows us to analyze how long a card was focused and if the participant switched frequently between the cards. Furthermore, we can compare this viewing behavior with other participants. At the end of the shown sequence, the hand flips a card again, which draws the attention of the participants to this event.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">QUALITATIVE USER STUDY</head><p>To collect expert feedback for our technique, we observed five visualization experts from our institute during a free exploration task using our application. In separate sessions, we introduced the technique to the visualization experts (E1-E5). Four of them (E1-E4) were analysts with advanced knowledge of eye tracking analysis. Expert E5 was included in our observation to investigate how an analyst without advanced knowledge in this field applies and interprets our technique. Each session took around 30-45 minutes.</p><p>First, the gaze stripe visualization and all annotation items were introduced with a training data set. All experts were allowed to test all components until they felt comfortable to use the application. Afterwards, the Memory data set was loaded and the experts were asked to freely explore the first 30 seconds of the data and use the available items to annotate time spans in the data that they considered interesting. We did not introduce further restrictions in order to investigate which items are commonly used for what purpose, and what findings are extracted. During their analysis, the think-aloud method was applied to protocol their actions. Their annotations on the gaze stripes were saved for later examination.</p><p>General findings that were commonly extracted by the experts, considering the events of flipping cards and switching attention between cards, have already been discussed in Section 6.2. One finding that was noted by all experts, including E5, was the event after 25 seconds when the information about the first matching pair of cards is available to the participants when the card with the pyramid is shown. In the turn before (see <ref type="figure">Figure 7</ref>(c), seconds 14-20), the matching card was uncovered and the participants should be able to remember its position. However, the experts discovered three different behaviors in this situation: the majority of participants needed around one second to investigate the new card and then switched their attention directly to the covered matching card. The second group of participants stayed on the first card, hesitating to look for the matching card. The last group looked at the wrong card from the previous turn.</p><p>This distribution was mainly recognized by using the video player. Since neither the video nor the gaze stripes (due to the similar texture, see also <ref type="figure" target="#fig_5">Figure 14</ref>) could clearly provide the information which participants followed which behavior, all experts except for E5 created several screenshots with gaze plots of the participants' scanpaths to answer this question. Additionally, E1 applied clustering (see <ref type="figure">Figure</ref> 13(a)) to this time span to identify the participants that switched their attention most quickly. E2 and E4 also applied clustering to time spans when the participants were memorizing the two uncovered cards. In this case, the gaze stripes became most important, since the video replay could not be used efficiently to identify the viewing order of individual participants and the gaze plots became too cluttered to extract this information. Since the images on the cards were easy to distinguish in the gaze stripes, the experts could easily identify the order of the focused cards.</p><p>Although each expert created dispersion histograms for testing purposes, E1, E2 (see <ref type="figure" target="#fig_2">Figure 13</ref>(b)), and E4 applied the histograms re- peatedly to investigate how the dispersion of the participants' gazes changed in the time spans when all cards were covered and one card was flipped. E1 mentioned that the visualization could be improved by permanently showing a dispersion histogram over the complete time span instead of creating it on demand, indicating events of attentional synchrony directly in the overview. Notes were applied by all experts to comment on time spans and their created annotation items. Area markers were only used occasionally by few experts.</p><p>In summary, all experts were able to understand and use the application after some exploratory testing. Even expert E5, who did not have further knowledge of eye tracking analysis, was able to interpret the changing patterns in the overview of the gaze stripes and could identify the most salient event that was annotated by all participants. As a common feedback, the experts preferred to use the gaze stripes along with the video player, combining the advantages of both approaches: the contextual information of the video and the details of the local context with gaze stripes. Therefore, a sequential analysis was the common strategy of all participants after they obtained a first overview with the gaze stripes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>We presented a new approach to visualizing eye tracking data for static and dynamic stimuli from multiple participants. With our gaze stripes, the analyst can obtain a first impression of the data as well as detailed insights without the need to define AOIs. Our use cases show that common patterns of attentional synchrony as well as individual scanpaths become easy to interpret with the combination of local and global stimulus context. In combination with our presented annotation items, eye tracking protocols can be generated to illustrate and support statistical results or help build new hypotheses for further analysis.</p><p>Considering the scalability of our approach, we have to distinguish between the scalability with time spans and the number of participants. <ref type="figure">Figure 10</ref> shows approximately three seconds of the Arecibo Message We consider 120 time steps as an approximate value that can be investigated comfortably on a regular WUXGA screen in detail without panning. By reducing the sampling rate, the visualization can show a larger time span but with the possible loss of spatio-temporal information. Another approach for increasing the scalability with time spans is the aggregation of fixations or image series with similar content into single thumbnails. Since we create rectangular thumbnails, a similar behavior can be assumed for the number of participants that can be investigated without zooming and panning of the view; we estimate this limit around 75 participants. This number of participants is usually sufficient for the majority of eye tracking studies. The scalability with the number of participants could be improved by aggregating similar eye movements of different participants to average scanpaths <ref type="bibr" target="#b11">[12]</ref>. This approach could further reduce the number of horizontal gaze stripes, allowing us to display much more participants than our current implementation is able to. As described in Section 7, if similar objects appear in the stimulus and the analyst has to distinguish between these individual objects, the gaze stripes do not provide enough context to perform this task. <ref type="figure" target="#fig_5">Figure 14</ref> shows such a situation from the Memory data set with 16 cards that have an identical texture when their images are covered. In combination with gaze plots and dispersion histograms, this shortcoming of our technique can be compensated. However, visual similarity can also be of interest to the analyst (e.g., when looking at a cartographic map to examine if participants looked more at land or at water regions). In this case, the former shortcoming of our technique becomes an advantage since such similarities can be efficiently detected without the definition of any AOIs. Therefore, we agree with our expert feedback that combining our approach with additional tools is a good solution for several analysis tasks.</p><p>For future work, we plan to extend our analysis to data from mobile eye tracking. In this application scenario, the individual recordings can vary in numerous ways: certain events may occur asynchronously or only for some of the participants, the environmental conditions such as the lighting can change between participants. This would lead to heterogeneous recording data that will be challenging for an imagebased scanpath visualization. Since our approach does not require the definition of AOIs or post processing of the data, it can also be applied to streaming data, e.g., to monitor the recording of eye tracking data in real-time. It is also of interest to further increase the scalability of our approach. To this end, the aggregation and adaptive sampling methods mentioned above could be applied. In this case, it will be challenging to provide a synchronized view for different participants.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>A scene with three AOIs: the beach, the sea, and the sky. Scanpaths of two participants are represented by a (a) gaze plot, (b) scarf plot, and (c) gaze stripes. With gaze stripes, no AOIs are required to interpret the transitions between the three different regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Direct representation of time: The visualization of the linear time dimension of the data is required to identify sequential viewing behavior. • (R2) Direct representation of space: The spatial dimension of the data is required to interpret the distribution of visual attention on the stimulus. • (R3) Direct representation of context: A representation of the stimulus context is required to interpret what the participants were investigating. • (R4) Direct representation of individual participants: An ad-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>The effect of fixation filtering: (a) Microsaccadic eye motions make it difficult to identify local context, e.g., text blocks. (b) Fixation filtering of the data can improve this. However, if the participant follows objects in the scene (here: arrows), aggregated information affects identifying this behavior ((c) without and (d) with filtering).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>allow analysts to identify structures of selected gaze sequences based on their similarity, we included a hierarchical clustering method based on image similarity. The clustering item (Figures 4(e), 5(a)) depicts the result as a dendrogram allowing for an in-depth analysis of scanpath similarities. The selected sequences form the leaf nodes of the dendrogram, including the IDs of the respective participants.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>(a) Example clustering of a short sequence of gaze stripes from the Car Pursuit data set. Two major clusters are visible: (1) participants focusing on the moving red car, and (2) participants shifting their gaze to the appearing white car. (b) Screenshot from the video of the scene in which the white car suddenly appears from the left.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>Gaze stripes can be enriched by several complementary views that provide the global context of the stimulus. Time spans can be annotated with (a) colored area markers, (b) annotation notes, (c) screenshots, (d) dispersion histograms, and (e) a hierarchical clustering of the participants. (f) A linked video player allows playing back the stimulus with gaze information. (g) An interactive zoom lens facilitates a detailed analysis without leaving the overview level.Pearson correlation coefficient. It quantifies the similarity of two histograms H 1 and H 2 with n bins and mean valuesH 1 andH 2 :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Standard visualizations for eye tracking data without AOIs: Attention maps (a) and gaze plots (b), extracted with Tobii Studio. tion. Animated attention maps usually show smoothed changes in the calculated gaze density field. Animated gaze plots display only a specific time frame of connected circles to reduce visual clutter. For dynamic stimuli, bee swarm visualizations are also applied, showing the current gaze positions of all participants during playback.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :Fig. 8 :</head><label>78</label><figDesc>Images from the data sets of our use cases. (a) The Antigenic Shift infographics [23]. (b) The Wikipedia article about the Arecibo Message [36]. (c) Short sequence from the Memory video. Results for the Antigenic Shift infographics. (a) The participant is reading the different annotations in the figure. (b) This viewing behavior is hard to see in the gaze plot due to occlusion. Frame skip is applied for both methods, considering only every 10-th frame of the data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 :</head><label>9</label><figDesc>Results for the Antigenic Shift infographics. The labels in the infographics are visual cues for the participants. Gaze stripes with a size of 40 × 40 pixels and fixation filtering were used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 13 :</head><label>13</label><figDesc>Excerpts from the annotated gaze stripes of two experts. (a) E1 applied the hierarchical clustering and numerous screenshots to examine interesting events. (b) E2 applied various dispersion histograms and illustrated them with gaze plots.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 14 :</head><label>14</label><figDesc>(a) Similar objects cannot be distinguished with the gaze stripes since only local context of the stimulus is available. (b) In this case, gaze plots help resolve this issue. data set with 60 samples per second. Showing this time span zoomed to full width on a computer screen requires the usage of the zoom lens (see Section 4.2) to investigate individual thumbnails in detail.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The authors are with the University of Stuttgart. E-mail: firstname.lastname@vis.uni-stuttgart.de. Manuscript received 31 Mar. 2015; accepted 1 Aug. 2015; date of publication xx Aug. 2015; date of current version 25 Oct. 2015. For information on obtaining reprints of this article, please send e-mail to: tvcg@computer.org.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Manuscript received 31 Mar. 2015; accepted 1 Aug. 2015; date of publication 20 Aug. 2015; date of current version 25 Oct. 2015. For information on obtaining reprints of this article, please send e-mail to: tvcg@computer.org. Digital Object Identifier no. 10.1109/TVCG.2015.2468091</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparison of standard eye tracking visualizations with our gaze stripes with respect to the requirements R1-R4 and suitability for dynamic stimuli.</figDesc><table><row><cell>Technique</cell><cell>Time (R1)</cell><cell>Space (R2)</cell><cell>Context (R3)</cell><cell>Partici-pants (R4)</cell><cell>Dynamic Stimulus</cell></row><row><cell>Attention Maps</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>List of stimuli</figDesc><table><row><cell>Stimulus</cell><cell>Figure</cell><cell>Information</cell></row><row><cell>Antigenic Shift</cell><cell>7(a)</cell><cell>Infographics by the National Institute of Allergy and Infectious Disease (NIAID) [23] explaining how a flu strain can jump between animal species.</cell></row><row><cell>Arecibo Message</cell><cell>7(b)</cell><cell>A Wikipedia article [36] describing the so-called Arecibo message. The article contains mainly text and a single image of the broadcast message.</cell></row><row><cell>Kite</cell><cell>1</cell><cell>An outdoor scene with a person on a meadow steer-ing a kite.</cell></row><row><cell>UNO</cell><cell>4</cell><cell>Two persons playing the UNO card game; only the hands are visible.</cell></row><row><cell>Car Pursuit</cell><cell>5</cell><cell>A panning camera follows a red car while driving through a roundabout.</cell></row><row><cell></cell><cell></cell><cell>A table with cards of a memory game. A hand flips</cell></row><row><cell>Memory</cell><cell>7(c)</cell><cell>pairs of cards repeatedly until all matching pairs are</cell></row><row><cell></cell><cell></cell><cell>uncovered.</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Michael Wörner for voice acting. This work was funded by the German Research Foundation (DFG) as part of the Priority Program Scalable Visual Analytics (SPP 1335) and SFB/Transregio 161.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Visualization of Time-Oriented Data. Human-Computer Interaction Series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Aigner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Miksch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tominski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sequence Surveyor: Leveraging overview for scalable genomic alignment visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Albers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Dewey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2392" to="2401" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Visual analytics methodology for eye movement studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">V</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2889" to="2898" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">State-of-the-Art of Visualization for Eye Tracking Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blascheck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurzhals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raschke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Euro-Vis -STARs</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="63" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<ptr target="http://opencv.org/" />
	</analytic>
	<monogr>
		<title level="m">The OpenCV Library</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="120" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Information visualization within a digital video library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent Information Systems</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="235" to="257" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gaze-contingent video resolution degradation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Duchowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Mccormick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">3299</biblScope>
			<biblScope unit="page" from="318" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robbins. Scanpath comparison revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Duchowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Driver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jolaoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Ramey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on Eye Tracking Research &amp; Applications</title>
		<meeting>the Symposium on Eye Tracking Research &amp; Applications</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="219" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Aggregate gaze visualization with real-time heatmaps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Duchowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Orero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on Eye Tracking Research &amp; Applications</title>
		<meeting>the Symposium on Eye Tracking Research &amp; Applications</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="13" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Interactive information visualization of a million items</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-D</forename><surname>Fekete</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Plaisant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Symposium on Information Visualization</title>
		<meeting>the IEEE Symposium on Information Visualization</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="117" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Visual scanpath representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">I</forename><surname>Helfman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on Eye Tracking Research &amp; Applications</title>
		<meeting>the Symposium on Eye Tracking Research &amp; Applications</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="203" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Group-wise similarity and classification of aggregate scanpaths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Grindinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Duchowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sawyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on Eye Tracking Research &amp; Applications</title>
		<meeting>the Symposium on Eye Tracking Research &amp; Applications</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="101" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The Elements of Statistical Learning: Data Mining, Inference, and Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Springer Series in Statistics</title>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">iHAT: interactive hierarchical aggregation table for genetic association data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vehlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Battke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nieselt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Suppl</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Holmqvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nyström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Andersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dewhurst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jarodzka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<title level="m">Eye Tracking: A Comprehensive Guide to Methods and Measures</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bundled visualization of dynamic graph and trail data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hurter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ersoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fabrikant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Telea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1141" to="1157" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Benchmark data for evaluating visualization and analysis techniques for eye tracking for video stimuli</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurzhals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Bopp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bässler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ebinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop Beyond Time and Errors: Novel Evaluation Methods for Visualization</title>
		<meeting>the Workshop Beyond Time and Errors: Novel Evaluation Methods for Visualization</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="54" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ISeeCube: Visual analysis of gaze data for video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurzhals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Heimerl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on Eye Tracking Research &amp; Applications</title>
		<meeting>the Symposium on Eye Tracking Research &amp; Applications</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Space-time visual analytics of eye-tracking data for dynamic stimuli</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurzhals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2129" to="2138" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Binary codes capable of correcting deletions, insertions, and reversals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Levenshtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soviet Physics-Doklady</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="707" to="710" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visual exploration of eye movement data using the space-time-cube</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cöltekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-J</forename><surname>Kraak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Geographic Information Science</title>
		<editor>S. I. Fabrikant, T. Reichenbacher, M. van Kreveld, and C. Schlieder</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">6292</biblScope>
			<biblScope unit="page" from="295" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Media visualization: Visual techniques for exploring large media collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Manovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Encyclopedia of Media Studies Volume VI: Media Studies Futures</title>
		<editor>K. Gates</editor>
		<imprint>
			<publisher>Blackwell Publishing Ltd</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="95" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niaid</forename><surname>Antigenic</surname></persName>
		</author>
		<idno>2015/03/18</idno>
		<ptr target="http://www.niaid.nih.gov/topics/flu/research/basic/pages/antigenicshiftillustration.aspx" />
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Effect of compressed offline foveated video on viewing behavior and subjective quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nyström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Holmqvist</surname></persName>
		</author>
		<idno>4:1- 4:14</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Communications, and Applications</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SemantiCode: Using content similarity and database-driven matching to code wearable eyetracker gaze data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Pontillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Kinsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Pelz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on Eye-Tracking Research &amp; Applications</title>
		<meeting>the Symposium on Eye-Tracking Research &amp; Applications</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="267" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Point-of-gaze analysis reveals visual search strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Rajashekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5292</biblScope>
			<biblScope unit="page" from="296" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Effectiveness of animation in trend visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">G</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Stasko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1325" to="1332" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Feature congestion: a measure of display clutter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rosenholtz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mansfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="761" to="770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Robust clustering of eye movement recordings for quantification of visual interest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Decarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on Eye Tracking Research &amp; Applications</title>
		<meeting>the Symposium on Eye Tracking Research &amp; Applications</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Extreme visualization: squeezing a billion records into a million pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attentional synchrony and the influence of viewing task on gaze behavior in static and dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename></persName>
		</author>
		<idno>16:1-16:24</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Video summarization for large sports video archives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nitta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Babaguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Multimedia and Expo</title>
		<meeting>the IEEE International Conference on Multimedia and Expo</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1170" to="1173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A framework for measuring video similarity and its application to video query by example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramadge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Image Processing</title>
		<meeting>the International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="106" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tobii</surname></persName>
		</author>
		<idno>2015/03/18</idno>
		<ptr target="http://www.tobii.com" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Animation: can it facilitate?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tversky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bétrancourt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Studies</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="247" to="262" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Arecibo message</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wikipedia</surname></persName>
		</author>
		<idno>2015/03/18</idno>
		<ptr target="http://en.wikipedia.org/wiki/Arecibo_message" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fixation maps: quantifying eye-movement traces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Wooding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on Eye Tracking Research &amp; Applications</title>
		<meeting>the Symposium on Eye Tracking Research &amp; Applications</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="31" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A hue-saturation histogram difference method to vehicle detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Technology (ICMT), 2011 International Conference on</title>
		<imprint>
			<date type="published" when="2011-07" />
			<biblScope unit="page" from="31" to="34" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
