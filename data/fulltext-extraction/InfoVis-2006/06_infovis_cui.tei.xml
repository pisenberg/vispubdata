<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Measuring Data Abstraction Quality in Multiresolution Visualizations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingguang</forename><surname>Cui</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Matthew</forename><forename type="middle">O</forename><surname>Ward</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Elke</forename><forename type="middle">A</forename><surname>Rundensteiner</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yang</surname></persName>
						</author>
						<title level="a" type="main">Measuring Data Abstraction Quality in Multiresolution Visualizations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T18:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Metrics</term>
					<term>Clustering</term>
					<term>Sampling</term>
					<term>Multiresolution Visualization</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Data abstraction techniques are widely used in multiresolution visualization systems to reduce visual clutter and facilitate analysis from overview to detail. However, analysts are usually unaware of how well the abstracted data represent the original dataset, which can impact the reliability of results gleaned from the abstractions. In this paper, we define two data abstraction quality measures for computing the degree to which the abstraction conveys the original dataset: the Histogram Difference Measure and the Nearest Neighbor Measure. They have been integrated within XmdvTool, a public-domain multiresolution visualization system for multivariate data analysis that supports sampling as well as clustering to simplify data. Several interactive operations are provided, including adjusting the data abstraction level, changing selected regions, and setting the acceptable data abstraction quality level. Conducting these operations, analysts can select an optimal data abstraction level. Also, analysts can compare different abstraction methods using the measures to see how well relative data density and outliers are maintained, and then select an abstraction method that meets the requirement of their analytic tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Very large multivariate datasets are increasingly common in many applications including bioinformatics, social science and data analysis for homeland security. To be effective, visualization tools must be increasingly capable of handling such huge datasets. As the number of data elements increases, two problems arise: displays become cluttered and response time deteriorates. Clutter saturates visualizations, obscures the structure of the visual display, and hinders visual data analysis. Increase in response time makes effective and efficient interactive exploration difficult.</p><p>Many techniques have been proposed to address this scalability problem. They can be classified into two groups: data abstraction techniques in data space and clutter reduction techniques in visual space. We define data abstraction as the process of hiding detail of data while maintaining the essential characteristics of data. Techniques for data abstraction found within information visualization include filtering <ref type="bibr" target="#b1">[2]</ref>, clustering <ref type="bibr" target="#b11">[12]</ref> and sampling <ref type="bibr" target="#b9">[10]</ref>. Clutter reduction assigns more screen space to interesting data elements than to less interesting ones; common techniques include zooming <ref type="bibr" target="#b3">[4]</ref> or distortion <ref type="bibr" target="#b14">[15]</ref>.</p><p>We are faced with a new challenge when working with the abstracted data in place of the original data; analysts should know how well the selected abstraction represents the whole dataset and how reliable the patterns discovered based on this abstraction are. Analysts should be able to compare different abstraction methods, and to select an abstraction level for a specific abstraction method. The measurement of data abstraction quality is one possible solution to resolve the above problems and should be an essential component of multiresolution visual analysis. Bertini and Santucci <ref type="bibr" target="#b6">[7]</ref> present a quality measure for sampling and apply it to finding the optimal sampling level. This measure, however, is limited to sampling. The authors do not consider other types or usages of abstraction measures. Boutin and Hascoet <ref type="bibr" target="#b7">[8]</ref> review and compare various measures for graph clustering, and propose modified onces. Neverthless, these measures are designed specifically for clustering and their extensibility into other types of visualization is not clear.</p><p>In this paper, we develop two examples of abstraction quality measures, namely HDM (Histogram Difference Measure) and NNM (Nearest Neighbor Measure) and show how they can be utilized. The HDM is derived based on the average relative error <ref type="bibr" target="#b2">[3]</ref> of aggregation used in approximate query processing of databases as well as image similarity measures <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25]</ref> used in image retrieval. The NNM is derived based on the nearest neighbor algorithm <ref type="bibr" target="#b10">[11]</ref> used in pattern recognition and an image quality measure <ref type="bibr" target="#b19">[20]</ref> used in image compression. We integrated these measures into several multivariate visualizations, including parallel coordinates, scatterplot matrices and glyphs, employing two dynamic bar charts to display the measures for the selected and the unselected regions of data. Several interactive operations have been designed for operating in this quality space, including adjusting the data abstraction level, changing the selected regions, regenerating the abstraction, and setting a desired quality level. Quality measures are recomputed whenever the above operations are performed. The measures and interactions together form an environment in which analysts can explore multiresolution visualization with abstraction quality information available. Visual analysts can benefit from data abstraction quality measures in several ways. First, these measures give analysts a confidence level in the given abstraction they work with and thus also for any observation made based on the abstracted dataset. This enables them to make more accurate decisions. Second, these measures make analysts aware of the abstraction quality of the dataset. Better yet, interactive mechanisms are available for the analysts to control the abstraction quality. Thus they can find an appropriate abstraction level by trading off the accuracy of representing the data subset and the degree of visual clutter. Third, these measures can be used to compare the effectiveness of different abstraction methods. Analysts can thus select the abstraction method via a compromise between the relative data density, the degree to which outliers are preserved, and response time. For some applications, it may be acceptable to use the abstracted datasets with moderate quality as long as the decisions can be rapidly made, while other applications may require a higher level of confidence in the data utilized.</p><p>The rest of the paper is organized as follows. We review related work in Section 2, and define two abstraction quality measures in Section 3. Sections 4 describes the integration of these measures with multiresolution visualization using sampling and clustering. Sections 5 and 6 present two case studies of the measures in use. Section 7 concludes this paper and discusses future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Several researchers have proposed measures for visual quality, visual clutter and data abstraction. They have employed the measures to control interactive analysis on the visualization. Tufte <ref type="bibr" target="#b26">[27]</ref> describes general measures for visual quality, such as the lie factor (the ratio be-tween size of the effect shown in the graphic and size of the effect in data) and the data density (the ratio between the drawn data entries and the graph area). Bertini et. al. <ref type="bibr" target="#b5">[6]</ref> give a model for measuring visual density and clutter in 2D scatterplots. They develop a clutter measure represented by the percentage of colliding pixels in all possible permutations. Peng et al. <ref type="bibr" target="#b17">[18]</ref> propose visual clutter measures for parallel coordinates, scatterplots matrices, star glyphs and dimensional stacking. They use these measures to compute dimension orders with low visual clutter. Rosenholtz et al. <ref type="bibr" target="#b20">[21]</ref> present a feature congestion measure for displaying clutter based on image retrieval techniques, evaluating similarity and correlation among different features in the visualization. This measure can be used in an automated user interface critiquing tool. The work of Bertini et al. <ref type="bibr" target="#b6">[7]</ref> is closest to ours in that they have designed a quality measure for data abstraction. They partition 2D scatterplots into a grid of blocks, compare data densities of the original dataset and the data densities of samples in each block, and then calculate the percentage of matching blocks to achieve this measure. They use this quality measure to find an optimal sampling level. This measure is similar to the Histogram Difference Measure in our work, but the matching method of blocks is much coarser than our calculation of the histogram difference.</p><p>Sampling refers to the process of selecting and using subsets of observations to estimate some parameters about a population <ref type="bibr" target="#b25">[26]</ref>; techniques include simple random sampling, stratified sampling and quota sampling. Sampling techniques have been well studied in statistics and widely applied in social science. In Computer Science, sampling is used for many tasks, including optimizing queries in databases with approximate information from samples <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b0">1]</ref>. In recent years, faced with increasingly dense visualizations, researchers have begun to explore combining sampling with visualization. Dix and Ellis <ref type="bibr" target="#b9">[10]</ref> demonstrate that random sampling can make the visualization of large datasets more perceptually effective. Their Astral Telescope Visualiser employs a 2D zooming interface to show data with different sampling levels. Bertini and Santucci <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> employ a non-uniform sampling algorithm to select less data in dense areas to reduce clutter, and more data in sparse areas to maintain data characteristics. Rafiei and Curial <ref type="bibr" target="#b18">[19]</ref> apply simple random sampling into network visualization and show that this sampling preserves the common characteristics of the network.</p><p>Clustering refers to the process of partitioning a dataset into groups of objects based on similarity between objects or proximity according to some distance measure <ref type="bibr" target="#b4">[5]</ref>. Each group, called a cluster, consists of objects that are similar among themselves and dissimilar to objects in other groups. Clustering is an aggregation method, since a cluster is regarded as a higher level object that represents all objects it contains. It is widely used because of two reasons: 1) By visualizing cluster attributes rather than the original data, the number of visual elements displayed can be greatly reduced; 2) Clustering itself is a pattern discovering process. Thus visualizing clusters can explicitly reveal hidden patterns to viewers. Many visualization systems have adopted clustering methods to reduce clutter and analyze datasets. Fua et al. <ref type="bibr" target="#b11">[12]</ref> cluster multivariate datasets, and navigate the hierarchy from clustering with a structure-based tool that supports drill-down, roll-up and brushing operations. They present a hierarchical version of parallel coordinates and later extend the work to other multivariate visualizations <ref type="bibr" target="#b27">[28]</ref>. The InfoSky visual explorer <ref type="bibr" target="#b12">[13]</ref> supports interactively exploring large, hierarchically structured document collections based on clustering. Kreuseler et al. <ref type="bibr" target="#b13">[14]</ref> present a scalable framework for information visualization that can compute the clustering and hierarchy dynamically and support different methods to visualize clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">QUALITY MEASURES FOR DATA ABSTRACTION</head><p>Data abstraction is the process of reducing a large dataset into one of moderate size, reducing the detail od data while maintaining dominant characteristics of the original dataset. Some data abstraction methods select a subset of the original dataset as the abstraction, such as sampling and filtering, while other data abstraction methods construct a new, more abstract representation, such as clustering and summarizing. Measurement generally refers to the process of estimating the magnitude of a quantitative property <ref type="bibr" target="#b8">[9]</ref>. Measurement is essential for scientific research; with measurement, researchers can compare different objects and evaluate the effectiveness of programs or processes.</p><p>In this section, we will describe two abstraction quality measures in detail. To facilitate explanation of these measures, we define the Data Abstraction Level (DAL) as the ratio between the size of the abstracted dataset and the original dataset, and Data Abstraction Quality (DAQ) as the degree to which the abstracted dataset represents the original dataset. At a given DAL, the DAQ will vary based on the different abstraction methods used or even on different invocations of a given abstraction operation. A good abstraction method should maximize the data abstraction quality and minimize the variance of data abstraction quality in different invocations. The DAL can be considered as a very coarse data abstraction quality measure. Other data abstraction quality measures will, in general, be better descriptors than the DAL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Histogram Difference Measure</head><p>A histogram is an aggregation method that conveys data distribution. To construct a histogram, the data space is partitioned into many small ranges, with each range corresponding to a bin. The height of a histogram bin is determined by the percentage of data points that fall in the corresponding range. It reveals the data density within each subrange.</p><p>Because a histogram is a common data descriptor and is fast to compute, we propose to use the difference between the normalized histograms of the original dataset and the abstracted dataset as a measure to gauge the DAQ. First we compute two histogram with the same number of bins from the original dataset and the abstracted dataset. If the distributions are skewed, we can use non-uniform bin widths. Bin sizes correspond to the percentage of the total number of data points that fall in the bins. Bin difference is defined as the absolute difference between two bins. Then the histogram difference corresponds to the summation of bin differences between the corresponding bins in the two histograms. HDM (Histogram Difference Measure) is defined as the normalized histogram difference. Its range is from 0 to 1. 0 means in every pair of corresponding bins, at least one is empty, and 1 indicates a perfect match. We express these with the following equations:</p><formula xml:id="formula_0">Pb i = |Po i − Ps i | (1)</formula><p>where Po i is the percentage of data that fall into the i-th bin of the original histogram, Ps i is the percentage of data that fall into the ith bin of the abstracted histogram, and Pb i corresponds to their bin difference;</p><formula xml:id="formula_1">Ph = N ∑ i=1 Pb i = N ∑ i=1 |Po i − Ps i | (2)</formula><p>where Ph is the histogram difference, and N is the number of bins.</p><formula xml:id="formula_2">HDM = 1.0 − Ph MAX(Ph)<label>(3)</label></formula><p>where HDM is the Histogram Difference Measure, and MAX(Ph) is the maximum histogram difference. These equations generate the HDM for one data dimension. The HDM for an N-dimensional dataset is defined as the average of the N 1D HDMs. Recall that the histogram represents a data distribution. Thus the proposed HDM represents the difference between the data distributions in the two datasets. If this difference is very small, the HDM will be near 1. In this case, the abstracted dataset represents the original dataset very well, implying the data abstraction method has a very high quality.</p><p>Thus far, we use the absolute difference between bins to calculate the histogram difference. If we consider a histogram as a vector, then we can treat the histogram difference as the distance between two vectors. Thus many general methods of calculating vector distances can be used to calculate the histogram difference. For example, the equation for Minkowski distance is:</p><formula xml:id="formula_3">Ph = N ∑ i=1 Pb i = ( N ∑ i=1 |Po i − Ps i | p ) 1 p (4)</formula><p>where p is the distance type, p &gt; 0. For p=1, it is the Manhattan distance, which coincides with our definition of histogram difference above. For p=2, it is the Euclidean distance. We first use the Manhattan distance as the histogram difference, because it represents the absolute density difference between two datasets. Different distance methods will be evaluated in our future work. The number of bins in a histogram influences its effectiveness in conveying information. We can compute the bin size by setting a bin width. The default bin width in our work is calculated using this equation: W = 3.49S×N − <ref type="bibr">1 3</ref> , where S is the standard deviation in a given dimension and N is the number of data points. It has been illustrated by <ref type="bibr">Scott [23]</ref> that this generally results in an effective bin size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Validating Histogram Difference Measures</head><p>Aggregate queries refer to queries involving aggregation in database, such as summary, average and maximum. For very large databases, it may be prohibitively expensive to get exact results for aggregate queries. We note that the result of an aggregate query lists aggregate values in each sub-category. This in fact parallels a histogram. Error metrics are needed to measure the accuracy of the estimated query result compared to the actual query result. Babcock et al. <ref type="bibr" target="#b2">[3]</ref> define the average relative error with the following equation:</p><formula xml:id="formula_4">RelErr = 1 n (k + m ∑ j=1 |x j − x j | x j )<label>(5)</label></formula><p>where n is the number of bins, k is the number of empty bins in the estimated histogram, m is equal to n − k, x j is the actual value of the j-th bin, and x j is the estimated value of the j-th bin. This metric is similar to our histogram-based measure except that it uses the percentage of each bin, while our measure uses the percentage of the whole dataset. With this error measure, they demonstrated that dynamic sampling can provide more accurate approximate results than non-adaptive usage of uniform or non-uniform sampling.</p><p>In the image retrieval field, features are extracted from images in order to facilitate searching over images. Image features are an abstraction of the image and often are described as a histogram on image parameters such as color. Similarity measures are needed to compare histograms of image features. Swain and Ballard <ref type="bibr" target="#b24">[25]</ref> first proposed the color indexing method, which compares the color histograms of two images, defined in Equation 6. Niblack et al. <ref type="bibr" target="#b16">[17]</ref> proposed measuring the image similarity with the quadratic distance metric of a histogram using Equation 7.</p><formula xml:id="formula_5">Similarity(H 1 , H 2 ) = N ∑ i=1 |H 1i − H 2i | (6) Similarity(H 1 , H 2 ) = (H 1 − H 2 ) R A(H 1 − H 2 )<label>(7)</label></formula><p>where H 1 and H 2 are two histograms, H 1i is the i-th bin of the first histogram, H 2i is the i-th bin of the second histogram, and A=[a i j ], where a i j indicates the relationship between H 1i and H 1 j . If all a i j (i = j) = 0, then it becomes the Euclidean distance between the two histograms (Equation 8).</p><formula xml:id="formula_6">Similarity(H 1 , H 2 ) = ( N ∑ i=1 |H 1i − H 2i | 2 ) 1 2<label>(8)</label></formula><p>Siggelkow <ref type="bibr" target="#b23">[24]</ref> provided a comprehensive list of image similarity measures used in image retrieval systems. Our proposed histogrambased measure is similar to the first image similarity measure listed by Siggelkow, and is based on the same model as all similarity measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Nearest Neighbor Measure</head><p>As the name implies, nearest neighbor algorithms <ref type="bibr" target="#b10">[11]</ref> search for the object nearest to a given object. They are widely used to classify data into groups in data clustering and pattern recognition. Every object corresponds to a record. We assume that each record in the original dataset has a nearest neighbor in the abstracted dataset, called its representative. The records in the original dataset that are represented by the same record in the abstracted dataset form a cluster. Currently only numeric values are considered. We define the NNM (Nearest Neighbor Measure) as the normalized average of distances between every record in the original dataset to its representative. The following steps show the algorithm to calculate the NNM.</p><p>1. Choose a distance method to calculate the distance between two records. We use the Euclidean distance because it is the most common method for calculating distance between records. Our future work will compare the NNMs with different parameters, including different ways to compute the distance. The equation is:</p><formula xml:id="formula_7">D(x, y) = ∑ N k=1 (x k − y k ) 2 √ N<label>(9)</label></formula><p>where x and y are two arbitrary records, D(x, y) is the normalized distance between x and y, x k and y k are the k-th normalized dimension values of record x and y, respectively, ranging from 0 to 1, N is number of dimensions, and √ N is the maximum distance in the N dimension space. We normalize the distance through dividing by the maximum distance √ N.</p><p>2. Find a representative for each record in the original dataset. For the i-th record in the original dataset, we calculate the distances to all the records in the abstracted dataset, select the one with the smallest distance as the representative, and store this distance. The process is described with the following equation:</p><formula xml:id="formula_8">D i = K min j=1 D(x i , y j )<label>(10)</label></formula><p>where x i is the i-th record in the original dataset, y j is the j-th record in the abstracted dataset, K denotes the number of records in the abstracted dataset, and D i is the distance of the i-th record to its representative.</p><p>3. Normalize the average of the minimum distances into the NNM. We use the following equation to do this:</p><formula xml:id="formula_9">NNM = 1.0 − ∑ M i=1 D i M<label>(11)</label></formula><p>where D i is the normalized distance of the i-th record to its representative, NNM is the Nearest Neighbor Measure, and M denotes the number of records.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Validating Nearest Neighbor Measures</head><p>The Nearest Neighbor Measure employs an algorithm to find a representative for each record, averages the normalized distances between data records and their representatives, and normalizes this value. To support the use of the Nearest Neighbor Measure, we first show that the nearest neighbor algorithm has been successfully used in pattern recognition, and then show that some image quality measures in image compression are derived from the average distance between pairs of image pixels in a method similar to our measure. In pattern recognition, the nearest neighbor algorithm is used to classify phenomena based on observed features <ref type="bibr" target="#b10">[11]</ref>. Phenomena and features are described in a vector. In the training stage, feature vectors are extracted from a set of observed objects. In the testing stage, a vector is extracted from a new phenomena, and the distances from this vector to all feature vectors are computed. The feature vector with the smallest distance is the nearest neighbor. This phenomena is assigned to the class that its nearest neighbor belongs to. We use the same algorithm to find the representative for each data record.</p><p>Image quality measure is essential for image compression. It is not only used to evaluate compression techniques, but also to control the compression process and decide how many bits are allocated to each subband <ref type="bibr" target="#b19">[20]</ref>. Many image quality measures can be derived from the total or average distance between pairs of image pixels. The PSNR (peak signal-to-noise ratio) is the most common image quality measure, derived from MSE (mean squared error) and used in the JPEG 2000 Standard <ref type="bibr" target="#b21">[22]</ref>. It is defined by the following equations:</p><formula xml:id="formula_10">MSE = ∑ N i=1 ∑ M j=1 (F(i, j) −F(i, j)) 2 NM (12)</formula><p>where MSE is the mean squared error, F(i, j) is the pixel value at (i, j) in the original image,F(i, j) is the pixel value at (i, j) in the compressed image, and M and N are the length and height of the image.</p><formula xml:id="formula_11">PSBR = 10 log 10 ( MAX 2 I MSE )<label>(13)</label></formula><p>where PSBR is the peak signal-to-noise ratio and MAX I is the maximum pixel value. As we can see, the NNM employs the same method to compute the average distance between two datasets. The only difference is that they employ different methods to process the average distance to get the measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">INTEGRATING QUALITY MEASURES WITH MULTIRESOLU-TION VISUALIZATION</head><p>In this section, we describe our work on integrating quality measures into XmdvTool to develop effective and abstraction-aware multiresolution visualization. First we describe the interaction tool that we use to display quality measures. Then we present the interactive operations we support for quality measures. Next, we discuss the view continuity problem of sampling, and finally we give an overview of the Structure-Based Brush (SBB) we use to control abstraction parameters in clustering data. Analysts can adjust the DAL of clustering through both the general widget for all abstraction methods and the SBB, while they can only brush the structure formed by clustering through the SBB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Displaying Measures</head><p>XmdvTool supports interactive selection via brushing <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16]</ref> using a rich assortment of tools. The data selected through brushing is called the selected data, while the remaining data are called the unselected data. Analysts can adjust the DAL for the selected data as well as the unselected data. Each view of the data generates several quality measures. We use bar charts to display them. <ref type="figure">Figure 1</ref> shows two such bar charts, the left one conveys the quality measures for the selected data, and the right one conveys the quality measures for the unselected data. <ref type="figure">Fig. 1</ref>. Graphs to display measures and sliding bars to adjust the DAL These charts only illustrate the quality measures at a single DAL. We use 1D plots to illustrate quality measures and their relationship to the DAL. In <ref type="figure" target="#fig_0">Figure 2</ref>, the left and right plot show the quality measures for the selected and unselected regions, respectively. In each plot, the x-axis represents the DAL and the y-axis represents the quality measures. The red and blue line represent the changes of HDM and NNM against the abstraction level, respectively. A vertical line called the DAL handle is drawn to indicate the current abstraction level. The cross points of this vertical line and the plot lines denote the corresponding measures of this abstraction level. The DAL and measures are displayed to the right of the DAL handle. With these plots, analysts can know the quality of the current DAL in the context of the entire quality space. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Interactive Operations</head><p>Several interactive operations are supported in this system. Users can move the slider bar in <ref type="figure">Figure 1</ref> or the DAL handle in <ref type="figure" target="#fig_0">Figure 2</ref> to adjust the data abstraction level. After the DAL has been changed, the system will generate an abstracted dataset and display it in the data visualization. The DALs for selected and unselected data can be adjusted independently. Users can also modify the location of one of the boundaries of the selected region by clicking the left mouse button on or near the boundary and dragging in the desired direction. In addition, the selected region can be moved by choosing a region on the data display, and then adjusting the DAL for the region. This usually means that the user knows the data subset that she wants to explore and wants to take advantage of the scalability of multiresolution visualization. Alternatively a user can first choose a DAL in the current selected region, and then adjust the selected/brushing boundary to enlarge or diminish the size of the region. This usually means that an acceptable data abstraction level had been found, but the area of interest needs to be increased or decreased.</p><p>Analysts can also instruct the system to run the abstraction algorithm again to generate a new abstraction. For example, resampling can help analysts verify patterns that had been discovered in the previous samples. If a pattern still exists after resampling several times, this pattern is most likely a robust one. Furthermore, analysts can compare the abstraction measures from mutiple resampling, and select an abstraction with best quality. Finally, a user can indicate a desired quality level based on one of the measures and let the system decide the appropriate DAL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">View Continuity for Sampling</head><p>When analysts change the DAL, the patterns in the previous sample can be more easily remembered and compared with those in the current sample if view continuity is maintained. This can be accomplished by following the three guidelines below:</p><p>1. When changing from a low DAL to a higher DAL, all the records in the previous sample should be kept.</p><p>2. When changing from a high DAL to lower DAL, all the records in the new sample should come from the previous sample.</p><p>3. When broadening or narrowing the brushing boundary, the system should keep the records from the previous view, and then employ the same rules as above. We follow the above guidelines to maintain view continuity. Analysts still have the option to resample at any time or whenever they change the DAL or the selected region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Widget to Control Cluster-Based Abstraction</head><p>Hierarchical clustering generates a tree of clusters ranging from a single cluster containing the entire dataset to terminal clusters containing one record each. To represent a cluster in multiresolution visualization, one member of the cluster can be selected as a representative or a new record can be constructed to summarize the records in this cluster. This new record becomes the parent of all the records or clusters it contains. By recursively clustering data into related groups, a tree of clusters is formed. The abstracted dataset in clustering is defined as all the items with a specific node depth. This node depth represents the DAL. If the tree is visited using an in-order traversal algorithm, then all the nodes of this tree will be sorted and each node corresponds to a unique position in this order. Brushing is thus achieved via selecting a range of nodes in this order. We employ two handles to control this range. All the nodes in this range form a subtree. Analysts can adjust the abstraction level and visualize selected nodes in more or less detail.   <ref type="figure" target="#fig_1">3</ref> shows the widget to control both the level of abstraction and brushing, referred to as the Structure-Based Brush (SBB) <ref type="bibr" target="#b11">[12]</ref>. The triangular frame depicts the tree (see (a)). The leaf contour (see (c)) depicts the silhouette of the tree. It delineates the approximate shape formed by chaining the leaf nodes. The colored bold contour (see (b)) across the tree delineates the tree cut that represents the abstracted dataset in a specific data abstraction level. Analysts can adjust the DAL by moving this contour. The two movable handles (see (e)) on the base of the triangle are called range handles. The range handles, together with the apex of the triangle, form the selected region in the structure space (see (d)). Analysts can adjust the selected region by moving the left handle, the right handle or both. This interface, while specific to hierarchically clustered data, can support all of the interactions on the abstraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CASE STUDY 1: CHOOSING A DATA ABSTRACTION LEVEL (DAL)</head><p>In this section, we show how to choose an appropriate DAL. At this level, the abstracted dataset should have high data abstraction quality (equal or more than 0.90) and the visualization should have the best visual quality under the constraints of the data abstraction quality. The analytic task is to search for clusters in the OUT5D dataset. This dataset consists of five remote sensing channels: SPOT, Magnetics, Potassium, Thorium and Uranium, with 16384 records. We employ scatterplots to visualize this dataset. <ref type="figure">Figure 4</ref> shows the original dataset. Data points have significant overlaps with each other and so we cannot distinguish relative data density in different regions and have difficulty observing any trends within this dataset. First we make an abstraction with the DAL equal to 0.02. The corresponding HDM is 0.92 and the NNM is 0.93; this abstraction quality meets our requirements. The abstraction quality is positively related to the data abstraction level in general, although small fluctuations may exist. The scatterplot matrix with DAL equal to 0.02 is shown in <ref type="figure">Figure 5;</ref> we can see that a cluster (named Cluster A) exists in the marked scatterplot, but data points in other places are too sparse to observe definitive clustering behavior. Next we will focus on searching for a visualization with the best visual quality.</p><p>We change the DAL to 0.08. As shown in <ref type="figure">Figure 6</ref>, the sparse region in the marked scatterplot illustrates very good visual quality. However, data points in Cluster A are overplotted, thus the actual relative data density in Cluster A is higher than the relative data density we observe. Next we adjust the DAL to 0.04. As shown in <ref type="figure">Figure 7</ref>, the visual quality in the marked scatterplot is very good while the relative data density is maintained, although a small number of data points in Cluster A still overlap with each other. The quality measures of this abstraction are shown in <ref type="figure">Figure 8</ref>. This quality meets our requirement and we terminate our exploration.</p><p>Abstraction quality measures give us confidence in the pattern we discovered. If we only know that the DAL, the ratio between the number of abstracted records and the number of original records, is 0.04, we cannot have much confidence in our discoveries because we know that 96 percent of the data are not shown. However, with the HDM more than 0.95 and the NNM more than 0.96 for both clustering and sampling, we are fairly certain that the abstracted dataset represents the original dataset very well and that the pattern (Cluster A in this case) is very likely valid. In general, we can assign the abstraction quality measures to the discovered pattern to indicate the confidence level of the pattern, which enables analysts to make more accurate decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CASE STUDY 2: COMPARING DATA ABSTRACTION METH-ODS</head><p>In this application, two data abstraction methods, clustering and sampling, are compared using the proposed data abstraction measures embedded within our multiresolution visualization system. We employ the AAUP dataset, which surveys the number, salary and compensation of professors at 1161 institutions. We use parallel coordinates to visualize this dataset. Through this case study, we find that sampling has the advantage of maintaining the relative density of datasets while clustering has the advantage of maintaining the outliers of the dataset.</p><p>First we briefly review some characteristics of the HDM and NNM. The HDM is based on the histogram and minimizes the difference between the distributions of two datasets, so it excels in detecting changes in the relative density of data. The NNM minimizes the distance between the original dataset and the abstracted dataset. Outliers cannot be eliminated during abstraction without the increase of the average distance, because they tend to be far from most of the data records. Thus the NNM method gives high priority to outliers and is good at monitoring the change of outliers. The original dataset is shown in <ref type="figure" target="#fig_5">Figure 9</ref>. On the last dimension, the dense range with low values is marked as A and highlighted with red color; the sparse range is marked as B and drawn with green color. We can see that most of the data records are gathered in range A. We sample the original dataset and tentatively set the DAL to 0.08. <ref type="figure">Figure  10</ref> shows the visualization of this abstraction. <ref type="figure" target="#fig_0">Figure 12a</ref> shows the data abstraction quality of the whole dataset: HDM is 0.90 and NNM is 0.95. We then cluster this dataset, and also set the DAL to 0.08 to  <ref type="figure">Figure 11</ref>, the visual clutter is significantly reduced. <ref type="figure" target="#fig_0">Figure 12b</ref> shows the data abstraction quality of the whole dataset: HDM is 0.66 and NNM is 0.96.</p><p>We can see that the HDM of sampling is much better than the HDM of clustering. Thus we conclude that sampling maintains the relative density of the dataset much better than clustering. This can be explained by the fact that clustering finds one representative for each cluster, no matter how many members the cluster represents. Thus it loses the relative density. This can be verified by the visualizations. We can clearly see that sample-based visualization maintains the relative density, while cluster-based visualization reduces the relative density in range A and enlarges it in range B. On the other hand, the NNM is slightly better than that for sampling. We tested sampling and clustering at other abstraction levels and got similar results. So we verified that clustering maintains the outliers better than sampling. Analysts can consider the importance of maintaining relative density versus outliers for their analytic tasks, observe the HDM or NNM measures, and then select an abstraction method that balances relative density and outliers to meet their goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION AND FUTURE WORK</head><p>In this paper, we have identified data abstraction as a common mechanism for dealing with large-scale data visualization. We designed two data abstraction quality measures to gauge how well the abstracted dataset represents the original dataset: the Histogram Difference Measure (HDM) and the Nearest Neighbor Measure (NNM). We implemented these measures within XmdvTool, which supports both sampling and clustering as abstraction methods. Several interactive operations were developed, including adjusting the data abstraction level, changing selected regions, regenerating the abstraction, and setting the desired abstraction quality. The quality measures indicates the quality of the abstraction and thus also indirectly the quality of any patterns discovered through the abstraction.</p><p>Aided by these measures, analysts can find the most appropriate data abstraction level for a given task, that is, one with a reasonable abstraction quality and acceptable data density. These measures can also be used to compare different data abstraction methods in terms of how well they maintain relative data density and outliers. Thus our framework enables analysts to select abstraction methods that best fit their analytic tasks. We provide two case studies to illustrate the usefulness of these measures and the effectiveness of our proposed interactive tools related to the measures.</p><p>Ideally, the data abstraction quality measures should conform to the data abstraction quality as perceived by analysts. As shown in Section 3, many alternative measures could be devised, including variations for computing the HDM and NNM. In our future work we will compare and evaluate how well the different realizations of HDM and NNM agree with the quality perceived by analysts. We will also explore ideas for new data abstraction measures based on statistical properties of the data, such as mean value and standard deviation. Different abstraction measures may be sensitive to the changes in different dataset features, such as the relative data density and characteristics of outliers. So we will also evaluate the advantages and limitations of abstraction measures in the presence of different dataset features. Peng et al. <ref type="bibr" target="#b17">[18]</ref> designed and implemented several clutter quality measures based on visual clutter into XmdvTool, and we are also integrating data quality attributes within our visualizations. We plan to integrate these three efforts at quality measurement, representation and optimization and provide a quality-aware visualization framework to support the visual analysis process. Through this framework, analysts will be able to interactively explore very large datasets with quality information and refinement techniques available at each stage of the visualization process.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>1D plots of quality measures</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Structure-based brushing tool. (a) The tree frame; (b) Contour corresponding to current level-of-detail; (c) Leaf contour approximates shape of the tree; (d) Structure-based brush; (e) Interactive brush handles; (f) Colormap legend for level-of-detail contour.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure</head><label></label><figDesc>Figure 3 shows the widget to control both the level of abstraction and brushing, referred to as the Structure-Based Brush (SBB) [12]. The triangular frame depicts the tree (see (a)). The leaf contour (see (c)) depicts the silhouette of the tree. It delineates the approximate shape formed by chaining the leaf nodes. The colored bold contour (see (b)) across the tree delineates the tree cut that represents the abstracted dataset in a specific data abstraction level. Analysts can adjust the DAL by moving this contour. The two movable handles (see (e)) on the base of the triangle are called range handles. The range handles, together with the apex of the triangle, form the selected region in the structure space (see (d)). Analysts can adjust the selected region by moving the left handle, the right handle or both. This interface,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Scatterplots of original dataset (DAL=1.00) Scatterplots of abstracted dataset (DAL=0.02)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .Fig. 7 .Fig. 8 .</head><label>678</label><figDesc>Scatterplots of abstracted dataset (DAL=0.08) Scatterplots of abstracted dataset (DAL=0.04) Data abstraction measures</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .</head><label>9</label><figDesc>Parallel Coordinates of AAUP dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 .Fig. 11 .</head><label>1011</label><figDesc>Parallel Coordinates of sampled AAUP dataset (DAL=0.08) Parallel Coordinates of clustered AAUP dataset (DAL=0.08) Fig. 12. a. Quality measures for the abstraction of sampling; b. Quality measures for the abstraction of clustering facilitate comparison. As displayed in</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work was supported under NSF grants IIS-0119276 and IIS-00414380.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Congressional samples for approximate answering of group-by queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Poosala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGMOD International Conference on Management of Data</title>
		<meeting>ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="487" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Visual information seeking using the filmfinder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ahlberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>ACM SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">433</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dynamic sample selection for approximate query processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babcock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGMOD International Conference on Management of Data</title>
		<meeting>ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="539" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pad++: Advances in multiscale interfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bederson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>ACM SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page">315</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Survey of clustering data mining techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Berkhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Accrue Software</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">By chance is not enough: preserving relative density through nonuniform sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bertini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Santucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth International Conference on Information Visualisation (IV&apos;04)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="622" to="629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Quality metrics for 2d scatterplot graphics: automatically reducing visual clutter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bertini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Santucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 4th International Symposium on SmartGraphics</title>
		<meeting>of 4th International Symposium on SmartGraphics</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="77" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cluster validity indices for graph partitioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boutin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hascoet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth International Conference on Information Visualisation (IV&apos;04)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="376" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The Metrology Handbook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bucher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>ASQ Quality Press</publisher>
			<pubPlace>Milwaukee, Wisc.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">By chance -enhancing interaction with large data sets through statistical sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advanced Visual Interfaces</title>
		<meeting>Advanced Visual Interfaces</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Pattern classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stork</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>John Wiley and Sons, Inc</publisher>
		</imprint>
	</monogr>
	<note>2th edition</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Structure-based brushes: A mechanism for navigating hierarchically organized data and information spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rundensteiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="150" to="159" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Evaluating a system for interactive exploration of large, hierarchically structured document repositories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Granitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kienreich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sabol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Klieber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Symposium on Information Visualization</title>
		<meeting>IEEE Symposium on Information Visualization</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="127" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A scalable framework for information visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kreuseler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Symposium on Information Visualization</title>
		<meeting>IEEE Symposium on Information Visualization</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="27" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A review and taxonomy of distortion-oriented presentation techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Apperley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer-Human Interaction</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="126" to="160" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">High dimensional brushing for interactive exploration of multivariate data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Visualization</title>
		<meeting>IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="271" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The QBIC project: querying images by content using color, texture and shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Niblack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Equitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Flickner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Glasman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Petkovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yanker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SPIE Storage and Retrieval for Image and Video Databases</title>
		<meeting>of SPIE Storage and Retrieval for Image and Video Databases</meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="173" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Clutter reduction in multidimensional data visualization using dimension reordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rundensteiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Symposium on Information Visualization</title>
		<meeting>IEEE Symposium on Information Visualization</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Effectively visualizing large networks through sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Visualization</title>
		<meeting>IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="48" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Optimal bit allocation via the generalized BFOS algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Riskin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Information Theory, IT</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="400" to="402" />
			<date type="published" when="1991-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Feature congestion: a measure of display clutter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rosenholtz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mansfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>ACM SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="761" to="770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">JPEG 2000 still image coding versus other standards. SPIE&apos;s 45th annual meeting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Santa-Cruz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Askelof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Christopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Digital Image Processing XXIII</title>
		<imprint>
			<date type="published" when="2000-08" />
			<biblScope unit="volume">4115</biblScope>
			<biblScope unit="page" from="446" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On optimal and data-based histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="605" to="610" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Feature Histograms for Content-Based Image Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Siggelkow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<pubPlace>Freiburg, Germany</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Freiburg, Institute for Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D Thesis</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Color indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Swain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="11" to="32" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thompson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<publisher>John Wiley and Sons, Inc</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>2th edition</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The Visual Display of Quantitative Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tufte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982" />
			<publisher>Graphics Press</publisher>
			<pubPlace>Cheshire, CT</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Hierarchical exploration of large multivariate data sets. Data Visualization: The State of the Art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rundensteiner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="201" to="212" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
