<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Research Report: Visualizing Decision Table Classifiers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><forename type="middle">G</forename><surname>Becker</surname></persName>
							<email>becker@engr.sgi.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Silicon Graphics Inc</orgName>
								<address>
									<addrLine>2011 N. Shoreline Blvd</addrLine>
									<postCode>MS-500, 94043-1389</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Research Report: Visualizing Decision Table Classifiers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T18:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>classifier</term>
					<term>decision table</term>
					<term>dimensional stacking</term>
					<term>trellis displays</term>
					<term>relational data</term>
					<term>data mining</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Decision tables[1], like decision trees[2] or neural nets[3], are classification models used for prediction. They are induced by machine learning algorithms. A decision table consists of a hierarchical table in which each entry in a higher level table gets broken down by the values of a pair of additional attributes to form another table. The structure is similar to dimensional stacking [4]. Presented here is a visualization method that allows a model based on many attributes to be understood even by those unfamiliar with machine learning. Various forms of interaction are used to make this visualization more useful than other static designs.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In supervised classification learning, an induction algorithm produces a model from a labeled set of training data. The resulting classification model is capable of predicting labels for a new set of unlabeled records that have the same attributes as the training data. Each distinct label value is called a class. The word attribute is used here to refer to a column in a relational dataset.</p><p>Many classifier models are applied as a black box. It is not necessary to understand the model in order to use it effectively for prediction. Recently, there has been a realization that great insight can be gained by visualizing the structure of a data mining model. Decision trees were one of the earliest classification models to be represented graphically because of their easy to understand structure <ref type="bibr" target="#b1">[2]</ref>. Simple Bayesian classifiers have also been represented graphically <ref type="bibr" target="#b4">[5]</ref>. Neural networks have an arcane structure which is difficult to visualize. Fortunately, decision tables have a simple structure suitable for creating an understandable display.</p><p>The ability to describe the structure of a classifier in a way that people can easily understand, transforms classifiers from incomprehensible black boxes to tools for knowledge discovery. Classification without an explanation reduces the trust a user has in the system. Spiegelhalter and Knill-Jones <ref type="bibr" target="#b5">[6]</ref> found that physicians would reject a system that gave insufficient explanation even if the model had good accuracy. A human may decide not to use a classifier if he or she realizes that it is based on irrelevant attributes, bad data, or if important factors are being ignored.</p><p>There have been some initial attempts to display decision tables in the form of a General Logic Diagram (GLD) <ref type="bibr" target="#b6">[7]</ref>. In this format each cell in the decision table has a single color to indicate the predicted class.</p><p>Several methods of visualizing multi-dimensional datasets that use several variables to form a hierarchy have been proposed. Leblanc, Ward, and Wittels proposed dimensional stacking <ref type="bibr" target="#b3">[4]</ref> which shows a single colored cell at the lowest level of the hierarchy. Trellis displays <ref type="bibr" target="#b7">[8]</ref>, have expanded this concept by generalizing the representation of the cell at the lowest level to be a plot of any type, such as a scatterplot, surface plot, or line graph.</p><p>A deficiency of these methods is that a static scene is limited to two or three levels of detail before the display gets too large or the cells become too small to be useful.</p><p>Interactive drill-down, drill-up, drill-through, filtering, and animation allow the user to explore in great detail specific regions of interest, while comparing them to other regions that may be higher in the hierarchy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Design Requirements and Solutions</head><p>Decision tables tend to have a very large number of cells at the lowest level. For example, 6 attributes with 8 values per attribute, have potentially 6 8 = 1,679,616 cells. Fortunately, the fact that the data is seldom uniformly distributed over the space reduces the cell count considerably in practice. Even so, the size is usually far too great to deal with interactively. Even building such a scene can take prohibitively long. The solution is to provide levels of detail to the users which they can incrementally drill down on. The graphics are not computed for lower levels until a drill-down request occurs. In general, the design attempts to follow Ben Shneiderman's visual information-seeking mantra: "overview first, zoom and filter, then details-on-demand" <ref type="bibr" target="#b8">[9]</ref>.</p><p>Since the data structure is inherently hierarchical it is easy to start with an overview by showing just the top levels of the hierarchy. Three dimensional manipulation allows flexible navigation for studying regions of interest. Drilling down locally gets the user details on demand, and drill-through can return the actual raw data from which the classifier was induced. Filtering on specific values or ranges of values has also been implemented.</p><p>Each of the cells needs to show a distribution of the classes for the records matching that cell. This distribution is shown using a cake chart, which is similar to a pie chart, but rectangular in shape and has rectangular slices instead of wedge shaped slices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Treatment of Attribute Values</head><p>Attributes must have some set of discrete values for the classifier to be effective. If an attribute is continuous, the inducer uses entropy based discretization so that the distribution of classes in adjacent bins are as different as possible. This discretization is done globally <ref type="bibr" target="#b9">[10]</ref>.</p><p>The ordering of the bins for the continuous attributes is explicit, but for categorical attributes the values can be ordered in three meaningful ways: alphabetically, numerically by record weights, or numerically by correlation with one of the classes to be predicted. In most cases, it was found that the last method provides the most benefit. If a particular class is selected for sorting by label probability, then the selected class is used in determining the ordering. This type of ordering was also found useful in trellis displays <ref type="bibr" target="#b7">[8]</ref> (where it is referred to as main-effects ordering) and in visualizing the structure of a Simple Bayesian Classifier <ref type="bibr" target="#b4">[5]</ref>. By looking at how dramatically the distributions change along an axis, it is possible to see how well that categorical attribute correlates with the label, and also how it interacts with the other attribute mapped to that level of the hierarchy. <ref type="figure" target="#fig_0">Figure 1</ref> shows the interaction between the odor and spore-print color attributes in a mushroom dataset. Note that the values for the more edible mushrooms appear in the lower left because the ordering of nominals has been specified to be by correlation with edibility.</p><p>It is common for relational data to contain NULL (unknown) values. These NULL values are treated quite differently from 0 or some other value. If an attribute has NULL values, then the unknown values are denoted by a question mark (?) in the visualization. The NULL value, if present, always appears as the first value, and does not get sorted with the rest. Viewing of NULLs can be toggled on or off so that they do not interfere with observing trends in the non-null data.</p><p>Nominal values which contain less than a certain percentage of the data may be filtered out in order to simplify the visualization. This is useful because in many datasets there are some nominal attributes with many unique values. The visualization does not perform well if one axis has many more values than its pair at a given level. Usually one is in interested in the values that are most prevalent. A slider is provided to filter values that have less than some small percentage of the total weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation Issues</head><p>Given a training set of labeled records, an induction algorithm creates a classifier. Automatic selection of attributes is at the core of the induction algorithm <ref type="bibr" target="#b10">[11]</ref>. Alternatively, a user may apply their knowledge of the domain to specify some or all of the attributes to use at each level of the hierarchy. Using the tool this way may not build a model with maximal accuracy, but it may be more useful for gaining understanding of one's data. Once the model has been built (usually on a server), the structure is passed to the visualization tool for display. As a result, the visualization application never loads the whole dataset into memory. However, drill-through techniques may be used to retrieve some subset of the original data.</p><p>The structure of the decision table model is stored simply as a relational table, where each row represents an aggregate of all the records for each combination of values of the attributes used. Once loaded into memory, a hierarchy of tables is constructed, where each new table one level higher up in the hierarchy has two fewer attributes. Finally, the top-most level, a single row represents all the data. Besides a column for each attribute, there is a column for the record count (or more generally, sum of record weights), and a column containing a vector of probabilities (each probability gives the proportion of records in each class). For example, given a dataset with four attributes plus a label which has 3 values (or classes), <ref type="table" target="#tab_0">Table 1 shows the base  table, Table 2 shows the next higher table, and finally Table 3</ref>, which has only one row, shows the table at the very top.There is a row for every combination of attribute values that have representative records. As a result, no row can have 0 weight. Each row corresponds to a cake chart in the visualization. The height of the cake chart corresponds to the Weights column, and the slices of the cake are determined by the proportion of those records in each class (i.e. the Probability[] column). The geometry is represented as an Inventor <ref type="bibr" target="#b11">[12]</ref> scene graph.</p><p>There were many complicated issues dealing with layout during implementation. The main ones were: aspect ratios, overlapping text, and reducing the amount of geometry to draw.</p><p>The aspect ration is the ratio of the length to the width. An aspect ratio of one is optimal <ref type="bibr" target="#b7">[8]</ref>, but it is easy to have huge extremes if one axis has many more values than the other, or even if one axis has moderately more values for several levels in a row. A solution is to interchange attributes at levels until the aspect ratio becomes as close to one as possible at every level.</p><p>Ideally one would like to see text labels annotate every cake chart in the scene. Unfortunately, if that were done the text would obscure other objects or be too small to read. A compromise is to show the value labels only if they are at the edge (right and top), or if their is no data in the region to the left or top of the current matrix of cake charts. Occasionally there is still overlap, but in most cases the text is still readable because of the difference in scale and color. The text in the scene has four levels of detail that are controlled automatically based on the user's distance.</p><p>In an early prototype, drill-down was controlled automatically as the user navigated. If the user's viewpoint got close to a cake chart, it was automatically expanded to the next level of detail. Although a nice idea in theory, too much detail was generated in most cases, and the interaction became sluggish. The solution was the custom drill down described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Interactive Features</head><p>Interaction is crucial when you have a large model to be explored and understood. You simply cannot display everything in a static scene and have it be comprehensible.</p><p>When you drill down into a cake chart, the data represented by that cake is re-displayed in a new matrix of cake charts where the attributes assigned to that level of detail are used as the axes. The cake charts sit on top of a gray base. If the base is completely covered with cakes, then every combination of values at this level is represented by the data. Often much of the base is uncovered. This shows regions where no data exists. In a very sparse dataset large regions are empty. Selecting a gray base has the same effect as selecting the cake that was one level up in level of detail. Drilling down on a base causes every cake sitting on top of it to expand to the next level of detail.</p><p>If a cake chart is selected on the left, then the pie chart on the right will show a distribution which matches that shown by the cake. When more than one cake on the left is selected, then the pie on the right will show the probability distribution (with respect to the label) for the set of records defined by the union of selected cakes (see <ref type="figure" target="#fig_3">Figure 4)</ref>. In this way it is not only possible to extract predicted values based on the model, but it is also possible to define a query for drilling through to the underlying data.</p><p>If the classes are nominal they are listed under the pie chart on the right, and are in order of slice size. The class with the largest probability is at the top. As values on the left are selected, this order may change based on the new probabilities. The class that would be predicted given current selections is shown at the top of the list. If the label is a binned attribute the order of the classes is not based on slice size, but on numerical ordering.</p><p>If you place the cursor over a base or cake chart without selecting, text is displayed showing the values of the two attributes at that level of detail. Also shown is the weight of records represented; which is the height of the cake chart (see <ref type="figure" target="#fig_0">Figure 1)</ref>.</p><p>At each level of detail the attribute names are shown to the left and bottom of the array of cake charts, their values are shown to the right and top, respectively (if there is room). If there are an odd total number of attributes, the lowest level shows only one attribute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results</head><p>Described here are some examples where the visualization has been applied. In the first example, our goal is to understand what factors effect mushroom edibility. There are over a dozen attributes in the original data, but it is not necessary to use all of them in order to build an accurate classification model. The decision table classifier whose structure is shown in <ref type="figure" target="#fig_0">Figure 1</ref> uses only four of the attributes. The window on the left shows the top level of detail. Odor and spore-print-color were chosen by the inducer to be at the top level because doing so improves the accuracy of the underlying classifier. Note the dependency between these two attributes. There is only one top  level cake (odor="none" and spore-print-color="white") with more than one class is present. When you drill-down to the next level by clicking on this cake chart the attributes habitat and population are used to resolve the ambiguity (see <ref type="figure" target="#fig_1">Figure 2</ref>). <ref type="figure" target="#fig_2">Figure 3</ref> shows the structure of a decision table classifier induced from census data containing fifteen attributes, and about 50,000 records. Here the income attribute (which has been binned into three ranges) has been selected as the class label. The attributes selected at level one are: relationship and sex; level two has education and occupation; and level three has hrswk (hours worked per week) and age.</p><p>At the top level, relationship and sex have a strong correlation. Surprisingly, there are 3 male wives and one female husband (indicating a possible data quality problem). We can see distinct weight and salary distributions for each combination of sex and relationship. There is a significant difference between distributions for unmarried males versus females (See <ref type="figure" target="#fig_2">figure 3)</ref>. There is a cluster of red cake charts in the lower left of the male matrix that does not exist in the female matrix. The female matrix has obvious spikes at occupation = "admin-clerical" and "other service". No such spikes are visible in the corresponding male matrix. <ref type="figure" target="#fig_3">Figure 4</ref> shows the result of drilling down on the base for male husbands. One can compare the salary distributions for HS-grad husbands who are in sales with those who are executive managers (these two regions are have been selected in <ref type="figure" target="#fig_3">figure 4)</ref>. Although the distribution of age and hours worked is similar, the probability of having an income greater than 60,000 class is 34% for this group of managers, compared with 27% for the salesmen. The records in <ref type="figure" target="#fig_4">figure 5</ref> show the result of drilling through on these two selected regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions and Future Work</head><p>Machine learning can build accurate models to describe data. The knowledge contained in these models is hidden, however, without an effective way to convey it to users. The method of visualizing decision table classifiers presented here takes full advantage of interactive techniques to maximize user control of the model exploration process. With understanding of the underlying model comes a trust the results it yields. The combination of machine learning and data visualization gives a far more powerful tool for understanding than either does independently.</p><p>There are several areas for future research: allow the label to be continuous rather than discrete; automatically choose attributes at each level so that understanding rather than accuracy is maximized; allow the user to interactively rearrange the attributes mapped to each level would help exploration; and use the same idea of trellis displays <ref type="bibr" target="#b7">[8]</ref>, to allow different plots at each cell rather than strictly using a cake chart.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Top level of a decision table induced from a mushroom dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Drill down on the set of mushrooms that have no odor and have spore print color="white".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>A decision table based on census data. Cursor is over male husbands with a high school education and whose occupation is craft−regair.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Close up of the drill down on male husbands. The records shown in figure 5 resulted from drilling−through the 2 selected regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>These records are the results of drilling−through on the 2 selected regions in figure 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 : Level 2</head><label>1</label><figDesc></figDesc><table><row><cell>Attrib1</cell><cell>Attrib2</cell><cell>Attrib3</cell><cell>Attrib4</cell><cell>Weight</cell><cell>Probability[]</cell></row><row><cell>a</cell><cell>10-20</cell><cell>4-9</cell><cell>yes</cell><cell>10</cell><cell>.3, .6, .1</cell></row><row><cell>a</cell><cell>10-20</cell><cell>9-10</cell><cell>yes</cell><cell>34</cell><cell>.61, .37, .02</cell></row><row><cell>a</cell><cell>20-30</cell><cell>9-10</cell><cell>yes</cell><cell>123</cell><cell>.1, .6, .3</cell></row><row><cell>a</cell><cell>20-30</cell><cell>9-10</cell><cell>no</cell><cell>1</cell><cell>1., 0., 0.</cell></row><row><cell>a</cell><cell>20-30</cell><cell>10-18</cell><cell>yes</cell><cell>5</cell><cell>.8, .2, 0</cell></row><row><cell>a</cell><cell>20-30</cell><cell>10-18</cell><cell>no</cell><cell>23</cell><cell>.2 , .3, .5</cell></row><row><cell>:</cell><cell>:</cell><cell>:</cell><cell>:</cell><cell>:</cell><cell>:</cell></row><row><cell>d</cell><cell>40-50</cell><cell>4-9</cell><cell>yes</cell><cell>2</cell><cell>.5, .5, 0.</cell></row><row><cell>d</cell><cell>40-50</cell><cell>9-10</cell><cell>no</cell><cell>7</cell><cell>0., 1., 0.</cell></row><row><cell>d</cell><cell>40-50</cell><cell>10-18</cell><cell>no</cell><cell>9</cell><cell>.33, .33, .33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 : Level 1</head><label>2</label><figDesc></figDesc><table><row><cell>Attrib1</cell><cell>Attrib2</cell><cell>Weight</cell><cell>Probability[]</cell></row><row><cell>a</cell><cell>10-20</cell><cell>44</cell><cell>.61, .34, .05</cell></row><row><cell>a</cell><cell>20-30</cell><cell>850</cell><cell>.23, .41, .36</cell></row><row><cell>a</cell><cell>30-40</cell><cell>230</cell><cell>.35, .2, .45</cell></row><row><cell>a</cell><cell>40-50</cell><cell>56</cell><cell>.12, .28, .60</cell></row><row><cell>:</cell><cell>:</cell><cell>:</cell><cell>:</cell></row><row><cell>d</cell><cell>40-50</cell><cell>120</cell><cell>.17, .31 , .52</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 : Level 0</head><label>3</label><figDesc></figDesc><table><row><cell>Weight</cell><cell>Probability[]</cell></row><row><cell>19,345</cell><cell>.32, .33, .35</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The author would like to thank Ronny Kohavi, Roger Crawfis, and Dan Sommerfield for their helpful suggestions. Dan Sommerfield implemented the decision table induction algorithm. The census and mushroom data are available from http://www.ics.uci.edu/~mlearn/MLRepository.html.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Power of Decision Tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Machine Learning</title>
		<meeting>the European Conference on Machine Learning<address><addrLine>Berlin, Heidelberg, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<biblScope unit="volume">914</biblScope>
			<biblScope unit="page" from="174" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quinlan</surname></persName>
		</author>
		<title level="m">C4.5:Programs For Machine Learning</title>
		<imprint>
			<publisher>Morgan Kaufmann Publishers, Inc</publisher>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Introduction to the Theory of Neural Computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hertz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Palmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<publisher>Addison Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploring N-Dimensional Databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leblanc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wittels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of First IEEE Conference on Visualization (Visualization &apos;90)</title>
		<meeting>First IEEE Conference on Visualization (Visualization &apos;90)</meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="230" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Visualizing the Simple Bayesian Classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sommerfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Integration of Data Mining and Visualization</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
	<note>to appear in Issues in the</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Statistical and Knowledge-based Approaches to Clinical Decision Support Systems, with an Application in Gastroenterology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Spiegelhalter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R Knill-Jones</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society A</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="page" from="35" to="37" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hypothesis-driven constructive induction in AQ17-HCI: A method and experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wnek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Michalski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="139" to="168" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The Visual Design of Trellis Display</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cleveland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Statistical Graphics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="123" to="155" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Designing the User Interface: Strategies for Effective Human-Computer Interaction, Third Edition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Addison-Wesley Publ. Co</publisher>
			<biblScope unit="page">523</biblScope>
			<pubPlace>Reading, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Error-Based and Entropy-Based Discretization of Continuous Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sahami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Second International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="114" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Targeting Business Users with Decision Table Classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sommerfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Knowledge Discovery and Data Mining</title>
		<meeting>Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
	<note>to appear in the</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wernecke</surname></persName>
		</author>
		<title level="m">The Inventor Mentor</title>
		<imprint>
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
