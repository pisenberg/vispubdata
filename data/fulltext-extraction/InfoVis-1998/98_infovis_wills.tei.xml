<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Interactive View for Hierarchical Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">J</forename><surname>Wills</surname></persName>
							<email>gwills@research.bell-labs.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Lucent Technologies (Bell Laboratories)</orgName>
								<address>
									<addrLine>Room 1u334, 1000 E. Warrenville Road</addrLine>
									<postCode>60566</postCode>
									<settlement>Naperville</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Interactive View for Hierarchical Clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T18:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>This paper describes a visualization of a general hierarchical clustering algorithm that allows the user to manipulate the number of classes produced by the clustering method without requiring a radical re-drawing of the clustering tree. The visual method used, a space-filling recursive division of a rectangular area, keeps the items under consideration at the same screen position even while the number of classes is under interactive control. As well as presenting a compact representation of the clustering with different cluster numbers, this method is particularly useful in a linked views environment where additional information can be added to a display to encode other information, without this added level of detail being perturbed when changes are made to the number of clusters.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A common problem in data analysis is forming groups of similar items based on a number of variables describing the items. A classic case is FisherÕs analysis of iris data <ref type="bibr">(Fisher, 1936)</ref> in which measurements of sepal and petal length and width can be used to place the plants into groups that correspond to different species of iris plant. In business settings, it is often important to form customer groups for precision marketing.</p><p>Hierarchical clustering is one of the more common methods employed for categorizing cases based on available information on those cases. The overall goal of any clustering is clear: Using variables that describe the cases, divide the cases into a number of classes such that each class contains members that are similar to each other and dissimilar to members of other classes. This simple goal does not, however, translate to a simple algorithm, and there is a wide body of literature relating different techniques for creating such clusters <ref type="bibr">(Everitt, 1993)</ref>. Probably the most common technique is that of hierarchical clustering <ref type="bibr">(ibid.)</ref> This method of clustering does not require, as some methods do, that the number of resulting clusters be pre-set. Instead, a hierarchical clustering view builds a binary tree in which the original data items are the leaves, and interior nodes represent clusters of items. The interior nodes are also marked with the measure of the dissimilarity between the two sets of child clusters. By cutting the tree at a given level of dissimilarity, the analyst can create clusterings with different numbers of groups without re-running the algorithm. This</p><formula xml:id="formula_0">draft 2 8/30/98</formula><p>property is very important in the exploratory study of large data sets; it allows the analyst to run a potentially very slow algorithm on a large set of data and then examine the resulting artifact in an exploratory fashion. For the purposes of this paper, the method used to perform the hierarchical clustering is irrelevant; we are concerned only with visualizing the resulting tree. <ref type="table">Table 1</ref> shows some sample data which we will use to demonstrate our approach; it consists of data on seven animals; their relative sizes and number of legs. In <ref type="figure">Figure 1</ref> we show a tree resulting from the application of a common hierarchical clustering method, with interior (cluster) nodes annotated with the dissimilarity between their children. We can see that node D has dissimilarity 4.5, while node B has dissimilarity 2.0. This means that man is more similar to kangaroo than snake is to the cat-dog pair. <ref type="table" target="#tab_1">Table 2</ref> shows the results of cutting this tree at different levels of dissimilarity. As we decrease the allowable dissimilarity, we cut the tree lower down and so produce more clusters. With dissimilarity 10 there is only one cluster; with a dissimilarity of 2.5 there are five. <ref type="figure">Figure 1</ref> is the usual way of visualizing the results of a hierarchical clustering algorithm, but it becomes too cumbersome with even moderate sized data sets of over a thousand nodes. A common solution to this problem is not to display the full tree, but only to display the tree as far as the current cut level plus one step (for example, with dissimilarity 5, only the nodes A, B, D, E, F would be shown). This approach is suitable for many applications, but suffers from several problems. First, as the cut level is changed  -none -cat-cow-dog-horse-kangaroo-man-snake 7.5 F cat-dog-kangaroo-man-snake, cow-horse 5 E, F cat-dog-snake, kangaroo-man, cow-horse 2.5 C, D, E, F cat, dog, snake, kangaroo-man, cow-horse interactively, the tree displayed often changes dramatically, growing in some directions and remaining static in others. As well as being distracting, this has the associated problem that the cluster in which a specific data item lies changes as the tree is grown or shrunk and therefore it is hard to locate individual items. A second problem is that even moderately sized trees of 100 leaves are still hard to see, so that clustering is limited to fewer numbers of clusters. Third, cluster size is not obvious. Fourth, it offers no hint of what might happen if the number of clusters is increased. We address these problems by proposing a new method of viewing the results of a hierarchical clustering algorithm motivated by space-filling tree layout methods such as tree-maps. <ref type="bibr" target="#b0">(Schniederman, 1992)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Description of the Method</head><p>A tree-map, as described in <ref type="bibr" target="#b0">Schniederman (1992)</ref> is a method for drawing a tree that makes maximal use of screen space. The basic version takes a specified rectangular area and recursively subdivides it up based on the tree structure. It looks at the first level of the tree and splits up the viewing area horizontally into n rectangles, where n is the number of children of the first node. Each rectangle is allocated an area proportional to the size of the subtree underneath each child node. The algorithm then looks at the next level of the tree and for each node here performs the same algorithm, except it recursively divides the area vertically. The algorithm continues doing this subdivision in alternating directions until either the maximum specified depth is reached or a leaf node is reached. In either case the rectangular area for that node is then drawn with user-specified characteristics such as color, shading and labeling.</p><p>The tree-map recursive subdivision model was looked at for implementing a view on hierarchical clustering, but was found to be lacking in the following areas:</p><p>• The tree-map has no concept of tree dissimilarity; it assumes that the user wants to cut the tree at given depths. This would mean that the clustering associated with dissimilarity 5 in our example could not be represented by the tree-map. The best it could do would be to cut at depth 2 and give the groups cat-dog-snake, kangarooman, cow, horse. This splits up cow and horse, which is wrong as they are very similar to each other. We therefore modify the splitting criteria so that it recursively splits until it reaches a leaf or the node has sufficiently small dissimilarity.</p><p>• The tree-map shows accumulated groups of data items only. We want to see not the accumulated groups, but also the individual data items (so that we can color individual data items differently, rather than just color groups all one color). Therefore we modify the algorithm so that when it decides not to split a node any longer it draws a rectangle for the bounding cluster, and then enters a second mode where it continues to recursively split until it reaches a leaf node, where it draws the node as a glyph in the center of the rectangle that the splitting algorithm has created. draft 4 8/30/98</p><p>• The tree-mapÕs alternating horizontal and vertical splits is good at showing the tree depth, but has a tendency to create very long skinny rectangles with unbalanced trees.</p><p>Since such trees are common in hierarchical clustering and since we have no interest in the depth of a given cluster, just its dissimilarity, we elect to choose to split along the longest axis of the current rectangle, so that the resulting rectangles are more square than they would have been if split along the other axis. We do this without any consideration of the depth of the current node.</p><p>• The tree-map typically shows the complete hierarchy of split nodes, insetting rectangles prior to splitting to do so. Since our interest is not in seeing at what depth the clusters occur, but in seeing the clusters themselves as clearly as possible, we omit this insetting and display. <ref type="figure">Figure 2</ref> shows how the hierarchical clustering view of <ref type="figure">Figure 1</ref> would differ from a treemap version. Note that in the tree-map version it is impossible to split up the group cat-dogsnake without also splitting up cow-horse, so a perfect comparison cannot be made. The first tree-map is shown without insetting; the second is shown with insetting (the default setting). For both views continuing to split deeper does not alter the locations of existing groups, nor does it change the size of the view, but where the tree-map shows the details of the hierarchy more accurately, it does so by presenting a more cluttered view that focuses attention on the tree depths, not the clusters. The tree view shows us that the leaf nodes horse and cow are at a different depth than the others, but that is an unimportant detail. The alternating horizontal-vertical split pattern has also created several long thin bars. Since we know these are leaf nodes, we know that these subtend the same areas, but if we did not know that it would be hard to tell that the cluster ÔsnaÕ is the same size as the cluster ÔkanÕ.</p><p>The following section of pseudo code shows one way in which the hierarchical clustering visualization can be constructed. It would be used by calling the split routine with the tree root node, an initial area into which the view should be drawn, with cluster_drawn set to be true, and with a suitable critical_value as set by the user or some other criteria. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Example</head><p>In this section we show a simple example of the Hierarchical Clustering View being used to examine some data on breakfast cereals. This data set is freely available from the net at statlib (http://lib.stat.cmu.edu) and consists of nutritional information on 76 different cereals. In this section we use two of them, sugar and fiber to cluster the data and we use a third, shelf, to color individual data points. <ref type="figure" target="#fig_1">Figure 3</ref> shows the clustering view for various different dissimilarity values. The line graph at the bottom of each view is a plot of number of clusters (x-axis) against dissimilarity needed to do next split (y-axis). The dissimilarity values were chosen subjectively by dragging the mouse over the line graph and watching the plot change. Looking down the series of screen shots, the property that the configuration of points does not change is obvious. All that is changing is the enclosing boxes that define the groups.</p><p>The steep descent of the line graph shows us that we do not need many clusters in this data set; probably the seven of the first set are sufficient.</p><p>The colors of the data items encode the shelf that the product was found on (black lowest, gray middle and white highest). Notice that one cluster is found only on the bottom shelf. These consist of low sugar, medium fiber cereals; oatmeal, cheerios and several varieties of shredded wheat. Along the left of the view are two groups of one and two items. These are all extremely high fiber bran cereals, placed at adult eye level.</p><p>At the top right is a group where most of the cereals are in the middle shelf; at childrenÕs eye level. Looking at these we see, without much surprise, that these are the high sugar, low fiber group. The supermarketÕs plan is clear: Get the children to grab the sweet cereals and guilt the adults into buying good-foryou high-fiber cereal. <ref type="figure" target="#fig_3">Figure 6</ref> and <ref type="figure" target="#fig_2">Figure 4</ref> show the effect of changing the aspect ratio of the view both by making it more square and more elongated. In either case the algorithm produces good results. Note that in this figure we have set the grayscale intensity of the items to indicate their cluster group. <ref type="figure">Figure 5</ref> shows a scatterplot of the variables used to construct the clustering, with data items colored the same as in the cluster view. The white outlier (100% bran) and the two other bran outliers in off-white are clear. There is not much separation in this data set; the clusters are not very distant from each other. We might have guessed this from the line graph associated with the cluster view; it drops very  steeply then flattens out when the fourth split is made; there are really only three very different groups; the two outlier groups and the body of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Summary</head><p>The hierarchical clustering view proposed here solves several important criteria:</p><p>• It displays individual data items as well as data clusters, making it very useful in an exploratory environment, especially when used in a linked views environment (Eick and <ref type="bibr">Wills, 1995)</ref>. Furthermore, the locations of the data items do not change when the degree of clustering is changed. Regardless of the groups, the data items remain stationary, aiding navigation and data interrogation.</p><p>• It allows the user to try new critical values rapidly and easily and thereby cut the tree into different clusters.</p><p>• It is compact; trees such as in <ref type="figure" target="#fig_2">Figure 4</ref> with 76 leaf nodes and 75 interior nodes would be very expansive in traditional tree drawings, whereas our method allows trees with tens of thousands of leaves to be shown on a standard display.</p><p>• It is adaptable to a wide variety of aspect ratios; even extremely elongated areas pose no problems for it.</p><p>This cluster view has been implemented in a linked views environment <ref type="bibr" target="#b2">(Wills, 1996)</ref> as one of the linked views components. The clustering algorithm used is a combination of a fast mutual nearest neighbors method (Frakes and Baeza-Yates, 1995) and a complete linkage method <ref type="bibr">(Everitt, 1993)</ref>, used when the number of clusters is/becomes small), and it is designed to work on a data tables with a variety of different types of variables (categorical and numerical). Within this framework it is feasible to use this view to examine trees on data sets of size up to half a million or so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">References</head><p>Eick S.J. and <ref type="bibr">Wills G.J. (1995)</ref>  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Clustering Cereals 8/30/98</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Clustering view with square areaFigure 5. Scatterplot of cluster variables</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Clustering view with elongated area</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Clusters at various levels of allowable dissimilarity</figDesc><table><row><cell>draft</cell><cell>3</cell><cell>8/30/98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>High Interaction Graphics. European Journal of Operational Research 1995, 445-459. Everitt, B. S. (1993) Cluster Analysis. 3 rd ed. Halsted Press NY Fisher, R.A. (1936) The use of multiple measures in taxonomic problems. Annals of Eugenics 7, 179-188 Frakes W.B. and Baeza-Yates, R. (1995) Information retrieval : data structures and algorithms</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tree visualization with tree-maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schniederman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A 2D space-filling approach</title>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="1992-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Selection: 524, 288 Ways to say &apos;This is Interesting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Wills</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Visualization Õ96 Proceedings</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
