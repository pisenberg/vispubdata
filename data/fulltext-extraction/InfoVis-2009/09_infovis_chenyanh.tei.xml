<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exemplar-based Visualization of Large Document Corpus</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhua</forename><surname>Chen</surname></persName>
						</author>
						<title level="a" type="main">Exemplar-based Visualization of Large Document Corpus</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T18:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Exemplar</term>
					<term>large-scale document visualization</term>
					<term>multidimensional projection</term>
				</keywords>
			</textClass>
			<abstract>
				<p>With the rapid growth of the World Wide Web and electronic information services, text corpus is becoming available online at an incredible rate. By displaying text data in a logical layout (e.g., color graphs), text visualization presents a direct way to observe the documents as well as understand the relationship between them. In this paper, we propose a novel technique, Exemplarbased Visualization (EV), to visualize an extremely large text corpus. Capitalizing on recent advances in matrix approximation and decomposition, EV presents a probabilistic multidimensional projection model in the low-rank text subspace with a sound objective function. The probability of each document proportion to the topics is obtained through iterative optimization and embedded to a low dimensional space using parameter embedding. By selecting the representative exemplars, we obtain a compact approximation of the data. This makes the visualization highly efficient and flexible. In addition, the selected exemplars neatly summarize the entire data set and greatly reduce the cognitive overload in the visualization, leading to an easier interpretation of large text corpus. Empirically, we demonstrate the superior performance of EV through extensive experiments performed on the publicly available text data sets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>With the rapid growth of the World Wide Web and electronic information services, text corpus is becoming available on-line at an incredible rate. No one has time to read everything, yet in many applications we often have to make critical decisions based on our understanding of large document collections. For example, when a physician prescribes a specific drug, he frequently needs to identify and understand a comprehensive body of published literature describing an association between the drug of interest and an adverse event of interest. Thus, text mining, a technique of deriving high-quality knowledge from text, has recently drawn great attention in the research community. Research topics in text mining include, but not limited to, language identification, document clustering, summarization, text indexing and visualization. In particular, text visualization refers to the technology that displays text data or mining results in a logical layout (e.g., color graphs) so that one can view and analyze documents easily and intuitively. It presents a direct way to observe the documents as well as understand the relationship between them. In addition, visualization allows people to explore the inside logic of the model and offers users a chance to interact with the mining model so that questions can be answered.</p><p>In general, it is convenient to transform document collections into a data matrix <ref type="bibr" target="#b4">[5]</ref>, where the columns represent documents and the row vectors denote keyword counting after pre-processing. Thus, textual data sets have a very high dimensionality. A common way of visualizing text corpus is to map the raw data matrix into a d-dimensional space with d = 1, 2, 3 by employing dimensionality reduction techniques. The objective is to preserve in the projected space the distance relationships among the documents in their original space. Depending on the choice of mapping functions, both linear (e.g., principle component analysis (PCA) <ref type="bibr" target="#b12">[13]</ref>) and nonlinear (e.g., ISOMAP <ref type="bibr" target="#b23">[24]</ref>) dimensionality reduction techniques have been proposed in the literature. Facing the ever-increasing amount of available documents, a major challenge of text visualization is to develop scalable approaches that are able to process tens of thousands of documents. First, from a computational point of view, large text corpus significantly raises the bar on the efficiency of an algorithm. For a collection of more than ten thousand documents, typical data projection methods, such as PCA, will fail to run due to insufficient memory. Second, since all documents are shown at once in the resulting space, overlaps of highly related documents are inevitable. Hierarchical clustering-based methods can partially solve the memory problem and produce a tree structure for document exploration. However, these algorithms run extremely slow. More important, they are not mathematically rigorous due to lacking a well defined objective function. Finally, knowledge or information is usually sparsely encoded in document collections. Thus, main topics of a text corpus are more accurately described by a probabilistic model <ref type="bibr" target="#b9">[10]</ref>. That is, a document is modeled as a mixture of topics, and a topic is modeled based on the probabilities of words.</p><p>In the paper, we propose an Exemplar-based approach to Visualize (EV) extremely large text corpus. Capitalizing on recent advances in matrix approximation and decomposition, our method provides a means to visualize tens of thousands of documents with high accuracy (in retaining neighbor relations), high efficiency (in computation), and high flexibility (through the use of exemplars). Specifically, we first computes a representative text data subspace C and a lowrank approximationX by applying the low-rank matrix approximation method. Next, documents are clustered through the matrix decomposition:X = CWG T , where W is the weight matrix, and G is the cluster indicator matrix. To reduce the clutter in the visualization, the exemplars in each cluster are first visualized through Parameter Embedding (PE) <ref type="bibr" target="#b10">[11]</ref>, providing an overview of the distribution of the entire document collection. When desired, on the clicking of an exemplar, documents in the associated cluster or in a user-selected neighborhood are shown to provide further details. In addition, hierarchical data exploration can also be implemented by recursively applying EV in an area of interest.</p><p>In summary, a novel method is proposed here to visualize large document data sets in the low-rank subspace. From a theoretical perspective, EV presents a probabilistic multidimensional projection model with a sound objective function. Based on the rigorous derivation, the final visualization is obtained through iterative optimization. By selecting the representative rows and columns, EV obtains a compact approximation of the text data. This makes the visualization efficient and flexible. In addition, the selected exemplars neatly summarize the document collection and greatly reduce the cognitive overload in the visualization, leading to an easier interpretation of the text mining results. Through extensive experiments performed on the publicly available text data sets, we demonstrate the superior performance of EV when compared with existing techniques. The remainder of the paper is organized as follows. We first review related work in Section 2.</p><p>Then, we present our algorithm in Section 3. In Section 4, we provide thorough experimental evaluation. Finally, we conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Visualization enables us to browse intuitively through huge amounts of data and thus provides a very powerful tool for expanding the human ability to comprehend high dimensional data. A number of different techniques <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5]</ref> were proposed in the literature for visualizing a large data set, among which multidimensional projection is the most popular one. In document visualization, let X = {x 1 , x 2 , ..x n } ∈ R m×n be a word-document matrix where columns represent the documents and rows denote the words appearing in them. In other words, the documents are treated as vectors with word frequency as their features. Multidimensional projection is to find the embedding of documents Y = {y 1 , y 2 , ..y n } ∈ R d×n in the visualization space, usually d = {1, 2, 3} and minimize |δ (</p><formula xml:id="formula_0">x i , x j ) − D( f (x i ), f (x j ))|, where δ (x i , x j ) is the original dissimilarity distance and D( f (x i ), f (x j ))</formula><p>is the Euclidean distance between the corresponding two points in the projected space, and f : X → Y is a mapping function <ref type="bibr" target="#b22">[23]</ref>.</p><p>In general, multidimensional projection techniques <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b19">20</ref>] can be divided into two major categories based on the function f employed: Linear Projection methods and Non-linear Projection methods. Linear projection creates an orthogonal linear transformation that transforms the data to a new coordinate system such that the new variable is a linear combination of the original variables. Among such techniques, the widely known is PCA <ref type="bibr" target="#b12">[13]</ref>. PCA finds a lowdimensional embedding of the data points that best preserves their variance as measured in the high-dimensional input space.</p><p>However, many data sets contain essential nonlinear structures that are invisible to PCA. For those cases, non-linear projection methods, using information not contained in the covariance matrix, are more appropriate. Several approaches, such as multidimensional scaling (MDS) <ref type="bibr" target="#b3">[4]</ref> and ISOMAP <ref type="bibr" target="#b23">[24]</ref>, have been proposed for reproducing nonlinear higher-dimensional structures on a lower-dimensional display, and they differ in how the distances are weighted and how the functions are optimized. Classical MDS produces a low-dimensional representation of the objects such that the distances (e.g., the Euclidean distance (L2 norm), the manhattan distance (L1, absolute norm), and the maximum norm) among the points in the new space reflect the proximities of the data in the original space. MDS is equivalent to PCA when the distance measure is Euclidean. ISOMAP extends metric MDS by incorporating the geodesic distances defined as the sum of edge weights along the shortest path between two nodes in a weighted graph (e.g., computed using Dijkstra's algorithm). Then, the top d eigenvectors of the geodesic distance matrix are used to represent the coordinates in the new d-dimensional Euclidean space. The most recently developed text visualization systems based on the above traditional projection techniques include Infosky 1 and IN-SPIRE <ref type="bibr" target="#b1">2</ref> .</p><p>Although current multidimensional projection techniques can extract a low-dimensional representation of a document based on the word frequency, most of them take no account of the latent structure in the given data, i.e., topics in the document collection. To this end, Least Square Projection (LSP) <ref type="bibr" target="#b16">[17]</ref> first chooses a set of control points using k-medoids method <ref type="bibr" target="#b0">[1]</ref> based on the number of topics and then obtains the projection through the least square approximation, in which the data are projected following the geometry defined by the control points. Recently, incorporating probabilistic topic models into analyzing documents has attracted great interest in the research community <ref type="bibr" target="#b11">[12]</ref> since it can provide a higher quality (i.e., more meaningful) visualization. In Probabilistic Latent Semantic Analysis (PLSA) <ref type="bibr" target="#b9">[10]</ref>, a topic is modeled as a probability distribution over words, and documents with similar semantics (i.e., topics) are embedded closely even if they do not share any words. The topic proportions estimated by PLSA can be embedded in the Euclidean space by Parametric Embedding (PE) <ref type="bibr" target="#b10">[11]</ref>, which employs a set of topic proportions as the input. Consequently, the documents that tend to be associated with the same topic would be embedded nearby, as would topics that tend to have the similar documents associated with them.</p><p>Unfortunately, all the aforementioned methods are inapplicable to visualize an extremely large-scale text corpus. When dealing with tens of thousands of documents, for example, PCA will fail to run due to insufficient memory and the high computational cost of solving the eigen problem. Similarly, PLSA model is also computationally expensive. Actually, all of the above models have a time complexity greater than O(n). The ever-increasing online document collections present unprecedented challenges for the development of highly scalable methods that can be implemented in a linear polynomial time. Therefore, hierarchical-clustering based visualization methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16]</ref> are proposed to partially solve the memory and computation problem, in which a hierarchical cluster tree is first constructed using a recursive partitioning process, and then the elements of that tree are mapped to the d-dimensional space to create a visual representation. However, these methods are derived intuitively, lacking a mathematically rigorous objective function to minimize f . In addition, all determinations are strictly based on local decisions, and the deterministic nature of the hierarchical techniques prevents reevaluation after points are grouped into a node of tree. Therefore, an incorrect assignment made earlier in the process may not be modified, and the optimal hierarchy has to be found through reconstruction.</p><p>In order to achieve high accuracy with low computational cost for visualizing large-scale data sets, we present a novel method, called Exemplar-based Visualization (EV). In the following, we will derive a theoretically sound algorithm for EV and apply it to visualize large document corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXEMPLAR-BASED VISUALIZATION</head><p>In this section, we first present the EV model and derive the algorithm in Section 3.1. Then, we give some theoretical results in Section 3.2, including the correctness and convergence of the algorithm, time and space complexity analysis, and advantages of EV when compared with other visualization models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Formulation and Algorithm</head><p>The proposed EV model takes a three-step approach to visualize largescale text corpus. First, low rank matrix approximation is employed to select the representative subspaces and generate the compact approximation of the word-document matrix X m×n . Among various matrix approximation methods, near-optimal low-rank approximation has gained increasing popularity in recent years due to its great computational and storage efficiency. The representative ones include Algorithm 844 <ref type="bibr" target="#b1">[2]</ref>, CUR <ref type="bibr" target="#b17">[18]</ref> and CMD <ref type="bibr" target="#b21">[22]</ref>. Typically, a near-optimal low-rank approximation algorithm first selects a set of columns C and a set of rows R as the left and right matrices of the approximation. Then, the middle matrix U is computed by minimizing X − CUR 2 F . Thus, at the end of the first step, we obtain the low-rank approximatioñ X = CUR, the representative subspaces C (data exemplar set) and R (feature set).</p><p>In the second step, we need to obtain the "soft" cluster indicators in the low-rank exemplar subspace, representing the probability of each document proportion to the topics in the topic model <ref type="bibr" target="#b6">[7]</ref>. We formulate this task as an optimization problem,</p><formula xml:id="formula_1">J = min W≥0,G≥0 X − CWG T 2 F (1) = Tr(X TX −X T CWG T − GW T C TX + GW T C T CWG T )</formula><p>where W is the weight matrix and G is the cluster indicator matrix with each element g ih ∈ [0, 1], indicating the probability distribution over topics for a particular document. In the optimization process, we propose an iterative algorithm to get non-negative W and G while fixing arbitrarily signed C andX. The updating rules are obtained by using the auxiliary functions and the optimization theory:</p><formula xml:id="formula_2">W (i,h) ← W (i,h) (A 1 + G) (i,h) + (A 3 − WG T G) (i,h) (A 1 − G) (i,h) + (A 3 + WG T G) (i,h)<label>(2)</label></formula><formula xml:id="formula_3">G (i,h) ← G (i,h) (A 2 + W) (i,h) + (GW T A 3 − W) (i,h) (A 2 − W) (i,h) + (GW T A 3 + W) (i,h)<label>(3)</label></formula><p>where A 1 = C TX , A 2 =X T C and A 3 = C T C. The third step is to use PE <ref type="bibr" target="#b10">[11]</ref> to embed documents into a lowdimensional Euclidean space such that the input probabilities G = p(L h |x i ) (where L is the topic label of a document) are approximated as closely as possible by the embedding-space probabilities p(L h |y i ). The objective is to minimize the difference between input probabilities and the corresponding embedding-space probabilities using a sum of Kullback-Leibler (KL) divergences for each document:</p><formula xml:id="formula_4">∑ n i=1 KL(p(L h |x i ) p(L h |y i )). Minimizing this sum ∑ z h=1 p(L h |y i )</formula><p>is equivalent to minimizing the following sum of KL divergences:</p><formula xml:id="formula_5">E(y i , φ h ) = − n ∑ i=1 z ∑ h=1 p(L h |x i ) log p(L h |y i )<label>(4)</label></formula><p>The unknown parameters, a set of coordinates of documents y i and coordinates of topics φ h in the embedding space, can be obtained with a gradient-based numerical optimization method. The gradients of Equation (4) with respect to y i and φ h are:</p><formula xml:id="formula_6">∂ E ∂ y i = z ∑ h=1 (p(L h |x i ) − p(L h |y i ))(y i − φ h ) (5) ∂ E ∂ φ h = n ∑ i=1 (p(L h |x i ) − p(L h |y i ))(φ h − y i )<label>(6)</label></formula><p>Thus, we can find the locally optimal solution for embedding coordinates y i for each document given φ h . The complete EV algorithm is given in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Exemplar-based Visualization</head><p>INPUT: word-document matrix X ∈ R m×n , selected number of documents and words r, c ∈ Z (a) Let A 1 = C TX , A 2 =X T C and A 3 = C T C, then split each matrix into the positive and negative parts:</p><formula xml:id="formula_7">+ s.t.1 ≤ r ≤ m, 1 ≤ c ≤ n,</formula><formula xml:id="formula_8">A + q = (|A q | + A q )/2; A − q = (|A q | − A q )/2;</formula><p>where q ∈ {1, 2, 3};</p><p>(b)</p><formula xml:id="formula_9">W (i,h) ← W (i,h) (A 1 + G) (i,h) + (A 3 − WG T G) (i,h) (A 1 − G) (i,h) + (A 3 + WG T G) (i,h) G (i,h) ← G (i,h) (A 2 + W) (i,h) + (GW T A 3 − W) (i,h) (A 2 − W) (i,h) + (GW T A 3 + W) (i,h) 4. Normalize cluster indicator G = p(L h |x i ) such that ∑ z h=1 p(L h |x i ) = 1; 5.</formula><p>Use parameter embedding to obtain the embedding-space coordinates y i for each document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Theoretical Analysis</head><p>In this section, we first show that our algorithm is correct and converges under the updating rules given in Equations (2)-(3). In addition, we show the efficiency of EV by analyzing its space and time requirements. Finally, we point out the advantages of EV when compared with other visualization methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Correctness and Convergence of EV</head><p>The correctness and convergence of the EV algorithm can be stated as the following two propositions. Due to the space limit, we give an outline of the proof of the propositions and omit the details. Motivated by <ref type="bibr" target="#b5">[6]</ref>, we plan to render the proof based on optimization theory, auxiliary function and several matrix inequalities. First, following the standard theory of constrained optimization, we fix one variable G and introduce the Lagrangian multipliers λ 1 and λ 2 to minimize the Lagrangian function</p><formula xml:id="formula_10">L(W, G, λ 1 , λ 2 ) = X − CWG T 2 F − Tr(λ 1 W) − Tr(λ 2 G T ).</formula><p>Second, based on the KL complementarity condition, we set the gradient descent of ∂ L ∂ W to be zero while fixing G. Then, we successively update W using Equation <ref type="formula" target="#formula_2">2</ref>until J converges to a local minima. Similarly, given W, we can set ∂ L ∂ G to be zero and update G using Equation (3) until J converges to a local minima. W and G should update alternatively. Third, we construct auxiliary functions to prove that Equation <ref type="formula">1</ref>decreases monotonically under the updating rules. An auxiliary function Z(W t+1 , W t ) should satisfy the two conditions:</p><formula xml:id="formula_11">Z(W t+1 , W t ) ≥ J(W t ), and Z(W t , W t ) = J(W t ) for any W t+1 and W t . We define W t+1 = min W Z(W, W t ), then we obtain the follow- ing equation J(W t ) = Z(W t , W t ) ≥ Z(W t+1 , W t ) ≥ J(W t+1 ). Thus, with a proper auxiliary function, J(W t ) is decreasing monotonically.</formula><p>Similarly, we can also prove J(G t ) is decreasing monotonically under an appropriate auxiliary function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Time and Space Complexity</head><p>To visualize a large data set, efficiency in both space and speed is essential. In the following, we provide detailed analysis on the time and space complexity of EV. To simplify the analysis, we assume n = m and r = c though they are not necessarily equal in the algorithm.</p><p>In Algorithm 1, the near-optimal matrix approximation is very efficient, having time complexity of O(nc 2 ). Details are given in <ref type="bibr" target="#b1">[2]</ref>. In the decomposition step, even thoughX is used in the description of the algorithm, the computation is actually done using the three small matrices, C, U and R. Specifically, we first need to compute A 1 , A 2 and A 3 with the following time,</p><formula xml:id="formula_12">A 1 : c(n × c + c 2 + c × n) A 2 : c(n × c + c 2 + c × n) A 3 : c 2 n</formula><p>Then, we need to compute W and G in Equations (2) and (3). Assuming that the number of iteration t = 1, the time for computing W and G are,</p><formula xml:id="formula_13">W : 2(c 2 z + cz 2 + z 2 n + cnz) G : 2(c 2 z + cz 2 + z 2 n + cnz)</formula><p>Thus, the total time for matrix decomposition is O(c 2 n + (c 2 z + z 2 n + cnz)). In addition, the time complexity of PE is O(nz). Since z min(c, n) and c n, the overall computational complexity is O(n).</p><p>Regarding the space complexity, EV needs 2cn + c 2 units to store C, U and R, and needs cz and nz units for W and G, respectively. In addition, the temporal storage for computing A q and updating W and G require O(cn) units. Since c n, the total space used is O(n).</p><p>In summary, both the time and space complexity of EV are linear, and thus it is highly scalable, suitable for visualizing a very large document collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Advantages of EV</head><p>From a theoretical point of view, EV has the following unique properties for visualizing large-scale text corpus when compared with other visualization methods:</p><p>• Accuracy: EV is a probabilistic multidimensional projection model with a well-defined objective function. Through iterative optimization, it can preserve the proximity in the highdimensional input space and thus provide accurate visualization results.</p><p>• Efficiency: EV has a high computational and spacial efficiency, and thus it is especially useful to visualize large document data. Compared with the time complexity of other visualization approaches, EV has a linear running time. Moreover, EV only needs to compute the non-zero entries of the approximation matrix, which further reduces the computational time for a sparse matrix (e.g., word-document matrix). EV also has the space complexity of O(n) while other algorithms typically require O(n 2 ) storage units.</p><p>• Flexibility: EV decomposes a word-document matrix into three matrices with the representative data subspace C, which contains the exemplar documents from the collection. By choosing the subspace dimensions, EV can visualize text corpus with different granularity, effectively reducing the clutter/overlap in the layout and cognitive overload.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we compared EV with PLSA+PE, LSP, ISOMAP, MDS and PCA for visualizing text data sets. Specifically, we implemented two EV models: EV-844 and EV-CUR, in our experiments. In EV-844, Algorithm 844 <ref type="bibr" target="#b1">[2]</ref> is used to successively select a column or row at a time with the largest norm from text data, resulting in a unique subspace; while EV-CUR uses CUR <ref type="bibr" target="#b17">[18]</ref> to pick the representative samples based on their probability distributions computed by the norms. Note that duplicates may exist in the CUR subspace because the samples with large norms are likely to be selected more than once. In the following, Section 4.1 gives the details of the data sets we used. In Section 4.2, we discussed the quantitative evaluation methods used to report the experimental results. On several public text data sets (including two large ones with 18, 864 and 15, 565 documents, respectively), we demonstrated the superior visualization results by EV in Section 4.3, in which we also compared the computational speed of all the algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Sets</head><p>For the experiments on document visualization, we used the 20Newsgroups data <ref type="bibr" target="#b13">[14]</ref> and 10PubMed data. 20Newsgroups data consists of documents in the 20 Newsgroups corpus. The corpus contains 18, 864 articles categorized into 20 discussion groups <ref type="bibr" target="#b2">3</ref> with a vocabulary size 26, 214. Note that at its full size the data here is too large to be processed by all the algorithms except EV. In order to make the comparison with existing methods, we constructed two subsets of 20Newsgroups through uniform random sampling: 20Newsgroups-I and 20Newsgroups-II, shown in <ref type="table">Table 1</ref>.</p><p>10PubMed data consists of published abstracts in the MEDLINE database 4 from 2000 to 2008, relating to 10 different diseases. We used "MajorTopic" tag along with the disease-related MeSH terms as queries to MEDLINE. <ref type="table">Table 2</ref> shows the 10 document sets (15, 565 documents) retrieved. From all the retrieved abstracts, the common and stop words are removed, and the words are stemmed using Porter's suffix-stripping algorithm <ref type="bibr" target="#b18">[19]</ref>. Finally, we built a word-document matrix of the size 22437 × 15565. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Measurement</head><p>We evaluated the visualization results quantitatively based on the label predication accuracy with the k-nearest neighbor (k-NN) method <ref type="bibr" target="#b7">[8]</ref> in the visualization space. Documents are labeled with discussion groups in the 20Newsgroups data, and with disease names in the 10PubMed data. Majority voting among the training documents in the k neighbors of a test document is used to decide its predicted label. The accuracy generally becomes high when documents with the same label are located together while documents with different labels are located far away from each other in the visualization space. Quantitatively, the accuracy AC(k) is computed as,</p><formula xml:id="formula_14">AC(k) = 1 n n ∑ i=1 I(l i ,l k (y i )),<label>(7)</label></formula><p>where n denotes the total number of documents in the experiment, l i is the ground truth label of the ith document,l k (y i ) is the predicted label by k-NN in the embedding space, and I is the delta function that equals one ifl k (y i ) = l i , and zero otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>First, on the data sets 20Newsgroups-I and 20Newsgroups-II we compared the neighbor-preserving accuracy in two-dimensional visualization generated by EV-844, EV-CUR, PLSA+PE, LSP, ISOMAP, MDS, and PCA. Through uniform random sampling, we created 10 independent evaluation sets for each data set, with given number of topics (3 for 20Newsgroups-I and 20 for 20Newsgroups-II) and documents (100 for 20Newsgroups-I and 50 for 20Newsgroups-II). The average accuracy values are obtained using k-NN over the 10 sets with k = {1, 2, ..., 50}, shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Generally, the AC values obtained by the seven methods are higher for a small number of topics (e.g., z=3 in <ref type="figure" target="#fig_0">Figure 1(a)</ref>) than those with a large number of topics (e.g., z=20 in <ref type="figure" target="#fig_0">Figure 1(b)</ref>). Moreover, the accuracy achieved by the topic models (i.e., EV-844, EV-CUR, PLSA+PE and LSP) is significantly higher than the traditional projection methods (i.e., PCA, MDS and ISOMAP). These results indicate that topic information is very helpful for the data visualization. When visualizing real-world text corpus, particularly the ones collected from the World Wide Web, the number of topics is typically unknown and thus has to be estimated through topic model detection. Some well-known approaches include Bayesian Inference Criteria (BIC) and Minimum Message Length (MML). A detailed discussion of model detection can be found in <ref type="bibr" target="#b14">[15]</ref>. In our experiments, the number of topics for all the topic models is simply set based on the ground truth. Another important observation from <ref type="figure" target="#fig_0">Figure 1</ref> is that EV-844 constantly provides a higher accuracy value than EV-CUR. This is mainly because Algorithm 844 selects unique columns (exemplars) while CUR may choose replicated ones to build the subspace. Thus, we used EV-844 in the rest of our experiments and referred it to EV without special mention. Finally, as shown in <ref type="figure" target="#fig_0">Figure 1(a)</ref>, the two probabilistic topic models (i.e., EV and PLSA+PE) have comparable performance on 20Newsgroups-I. However, as the number of topics increases, EV clearly outperforms PLSA+PE on 20Newsgroups-II in <ref type="figure" target="#fig_0">Figure 1(b)</ref>. These results imply that EV can appropriately embed documents in a two-dimensional Euclidean space while keeping the essential relationship of the documents, especially for a data set with a large number of topics. <ref type="figure">Figures 2 and 3</ref> show the visualization results obtained by EV, PLSA+PE, LSP, ISOMAP, MDS, and PCA on 20Newsgroups-I and 20Newsgroups-II, respectively. Here, each point represents a document, and the different color shapes represent the topic labels. For example, there are three different color shapes in <ref type="figure">Figure 2</ref>, representing three groups of news: black diamond for "comp.sys.ibm.pc", green triangle for "rec.sport.baseball" and red circle for "sci.med". In the EV visualization <ref type="figure">(Figure 2(f)</ref>), documents with the same label are nicely clustered together while documents with different labels tend to be placed far away. In PLSA+PE and LSP (Figures 2(e) and (d)), documents are located slightly more mixed than those in EV. On the other hand, with PCA, MDS and ISOMAP <ref type="figure">(Figures 2(a)</ref>-(c)), documents with different labels are mixed, and thus the AC values of the corresponding layout are very low. These results also imply that the topic As discussed earlier, by choosing the dimension of the subspace, EV can visualize documents with different granularity and enhance the interpretability of the visualization. In <ref type="figure">Figures 2(g</ref>)-(i) and 3(g)-(i), the representative documents selected in the low-rank subspace are embedded in a two-dimensional layout, for 20Newsgroups-I and 20Newsgroups-II, respectively. In <ref type="figure">Figures 2(g</ref>)-(i), we provided a series of visualization for 20Newsgroups-I, from the most abstract view to the visual layout with considerate amount of details as the number of selected exemplars increases from 10 to 40. This result demonstrates that EV can use exemplars to summarize the distribution of the entire document collection. Similarly, <ref type="figure">Figures 3(g</ref>)-(i) illustrate the visualization from abstract to details when the number of exemplars increases from 100 to 400 in 20Newsgroups-II. In these figures, the overlapping in the original layout <ref type="figure">(Figure 3</ref> , where n is the number of documents and s is the condition number in LSP. Our experiments are performed on a machine with Quad 3GHz Intel Core2 processors and 4GB RAM. In order to compare under the same condition, the running time are reported based on a single iteration if an algorithm uses the iterative approach. <ref type="table" target="#tab_4">Table 3</ref> summarizes the computation time in seconds for all six methods with increasing number of documents. From <ref type="table" target="#tab_4">Table  3</ref>, EV clearly is the quickest among the six, followed by PLSA+PE and PCA, while the computing time of LSP, MDS and ISOMAP increases quickly with the number of documents. More important, we observed that some algorithms fail to provide a result within a reasonable time for relatively large document sets. Specifically, ISOMAP is the slowest and cannot give a result when the matrix contains more than 3, 000 documents due to insufficient memory. When we have more than 10, 000 samples, only EV can provide a result within a reasonable computation time, while all other methods fail (indicated by a cross x in the table). Clearly, EV is suitable to visualize large text corpus we are increasingly facing these days thanks to its high computational efficiency.</p><p>We also developed an Exemplar-based Visualization software tool to offer a range of functions of creating visualization with userspecified configuration and thus supporting visual exploration of document data. First, when choose "View All" menu, the system can show all the documents at once for the 20Newsgroups and 10PubMed data sets. In this case, EV is the only one among the six algorithms that can produce a projection in a reasonable time. For example, <ref type="figure">Figure 4</ref>(a) shows visualization by EV for the 18, 864 documents in 20Newsgrups. Again, each point represents a document, and the different color shapes represent the topic labels. Note that it is difficult to see the details because the number of documents is very large, leading to extremely heavy overlapping. If one clicks "View Exemplars" and sets the number of exemplars at 1, 000, <ref type="figure">Figure 4(b)</ref> shows the representative documents selected by EV to summarize the whole document collection. Clearly, the cognitive overload and serious overlapping are greatly reduced. Here, a big color shape indicates the mean coordinate of documents for one group, calculated by µ l = 1 n l ∑ n i=1 I(l i = l))y i , where n l is the number of documents labeled with l. Obviously, documents with the same label are clustered together, and similar documents with closely related labels are placed nearby, such as "comp.graphics', "comp.os.ms.windows.misc' and "comp.windows.x" in the "computer" category, or "rec.autos", "rec.motorcycles", "rec.sport.baseball" and "rec.sport.hockey" in the "recreation" news group. Based on the visualized exemplars, EV provides several additional options for a user to further explore the data set. For example, on the click of "View Clusters", a magnified layout of all corresponding documents in the groups of "comp.graphics", "comp.os.ms.windows.misc" and "comp.windows.x" is given in <ref type="figure">Figure 4(c)</ref>, which provides further details. Similarly, a user can specify a neighborhood (the rectangle in <ref type="figure">Figure 4</ref>(b)), clicking "Zoom In" will generate a magnified view of all or representative documents in the selected area. Also, if desired, further clustering and visualization can be performed in an area of interest, leading to a hierarchical structure for data exploration. <ref type="figure">Figure 5</ref> shows the exemplar-based visualization for the 15, 565 documents in the 10PubMed data set. Exemplars and means of 10PubMed data illustrated in <ref type="figure">Figure 5</ref>(a) help us gain a better understanding on the distribution and relations of these documents. It is clear that documents with same disease are likely to be located closely while documents with different diseases are moved further away. We noticed that there is less overlapping in the 10PubMed data set than in 20Newsgroups. One reason is that the number of topics in 10PubMed is less than in 20Newsgroups while another one is that the abstracts in the literature for various diseases is actually easier to be separated than the documents in different news groups. The average value of AC is about 60% in the 10PubMed data set; it is only approximately 30% in 20Newsgroups. If desired, users can further explore the data set by clusters. In <ref type="figure">Figure 5</ref>(b), documents related to two diseases ("Gout" and "Chickenpox") are shown, where the selected exemplars (100 in total) are emphasized by the bigger black shapes. First, our method provides a clear visualization with little clutter. Second, users can quickly browse the large document collection by reading only the representative documents (exemplars) in each cluster. The actual time required by EV to produce visualization for 20Newsgroups and 10PubMed (with 1, 000 exemplars and 1, 000 iterations) are 30 and 25 minutes, respectively. These results clearly show that EV provides a very powerful tool for visualizing large text data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS AND FUTURE WORK</head><p>In the paper, we propose an Exemplar-based approach to Visualize (EV) extremely large text corpus. In EV, a representative text data subspace is first computed from the low-rank approximation of the original word-document matrix. Then, documents are soft clustered using the matrix decomposition and visualized in the Euclidean embedding space through parameter embedding. By selecting the representative documents, EV can visualize tens of thousands of documents with high accuracy (in retaining neighbor relations), high efficiency (in computation), and high flexility (through the use of exemplars).</p><p>The algorithms discussed in this paper have been fully integrated into a visualization software package, which will be released publicly shortly after the Infovis Conference <ref type="bibr" target="#b4">5</ref> . In the future, we plan to conduct practical user studies to solicit feedbacks so that the software can be improved with more convenient and user-friendly features. We also intend to pursue incorporating topic detection model into our system, making it more appropriate for real-world data visualization. Another direction we are considering for the future work is to develop an interaction tool based on the EV model for the visualization of other types of data. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Accuracy with k-NN in the two-dimensional visualization space with different k: (a) 20Newsgroups-I (3 topics), (b) 20Newsgroups-II (20 topics).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(f)) is greatly reduced, making users easier to understand the relations between news documents. Second, we compared the computational speed of the six visualization methods: EV, PLSA+PE, PCA, LSP, MDS and ISOMAP. From a theoretical perspective, the time complexity of EV is O(n), PLSA+PE and PCA are O(n 2 ), LSP is O( f (n, s)) = O(max{n 3 2 , n √ s}), and MDS and ISOMAP are O(n 3 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Visualization of documents in 20Newsgroups-I (300 documents, 3 topics) by (a)PCA, (b)MDS, (c)ISOMAP, (d)LSP, (e)PLSA+PE, and (f)EV, and visualization of (g)10 exemplars, (h)20 exemplars, (i)40 exemplars by EV. Visualization of documents in 20Newsgroups-II (1000 documents, 20 topics) by (a)PCA, (b)MDS, (c)ISOMAP, (d)LSP, (e)PLSA+PE, and (f)EV, and visualization of (g)100 exemplars, (h)200 exemplars, (i)400 exemplars by EV.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>•</head><label></label><figDesc>Yanhua Chen, Lijun Wang, Ming Dong are with Machine Vision and Pattern Recognition Lab, Department of Computer Science, Wayne State University, Detroit, MI48202, E-mail: {chenyanh, ljwang, mdong}@wayne.edu.</figDesc><table /><note>• Jing Hua is with Graphics and Imaging Lab, Department of Computer Science, Wayne State University, Detroit, MI 48202, E-mail: jinghua@wayne.edu. Manuscript received 31 March 2009; accepted 27 July 2009; posted online 11 October 2009; mailed on 5 October 2009. For information on obtaining reprints of this article, please send email to: tvcg@computer.org .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>number of topics z ∈ Z + s.t. 1 ≤ h ≤ z, and the label set of topics L z</figDesc><table /><note>h=1 OUTPUT: Visualization of documents Y = {y i } ∈ R d×n (1 ≤ i ≤ n) in the embedding space 1. Use a near-optimal low-rank approximation method to get C m×c , U c×r , R r×n and X m×n ; 2. Initialize W and G with non-negative values; 3. Iterate by the following updating rules for each i and h until convergence;</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Proposition 1 (Correctness of EV) Given the object function of Equation (1), the constrained solution satisfies KKT complementary conditions under the updating rules in Equations (2)-(3).</figDesc><table><row><cell>Proposition 2 (Convergence of EV) The object function of Equation</cell></row><row><cell>(1) is monotonically decreasing under the updating rules in Equations</cell></row><row><cell>(2)-(3).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Summary of data subsets from 20Newsgroups used in the experiments. Summary of 10Pubmed data used in the experiments.</figDesc><table><row><cell>Data Name</cell><cell></cell><cell>Groups</cell><cell>No. Docs</cell><cell>Total</cell></row><row><cell></cell><cell></cell><cell></cell><cell>per Group</cell><cell>Docs</cell></row><row><cell cols="2">20Newsgroups-I</cell><cell>{comp.sys.ibm.pc.hardware},</cell><cell>100</cell><cell>300</cell></row><row><cell></cell><cell></cell><cell>{rec.sport.baseball},{sci.med}</cell><cell></cell></row><row><cell cols="2">20Newsgroups-II</cell><cell>all 20 groups</cell><cell>50</cell><cell>1000</cell></row><row><cell></cell><cell></cell><cell>Document Name</cell><cell>No. of Docs</cell></row><row><cell>1</cell><cell></cell><cell>Gout</cell><cell>543</cell></row><row><cell>2</cell><cell></cell><cell>Chickenpox</cell><cell>732</cell></row><row><cell>3</cell><cell></cell><cell>Raynaud Disease</cell><cell>343</cell></row><row><cell>4</cell><cell></cell><cell>Jaundice</cell><cell>503</cell></row><row><cell>5</cell><cell></cell><cell>Hepatitis A</cell><cell>796</cell></row><row><cell>6</cell><cell></cell><cell>Hay Fever</cell><cell>1517</cell></row><row><cell>7</cell><cell></cell><cell>Kidney Calculi</cell><cell>1549</cell></row><row><cell>8</cell><cell cols="2">Age-related Macular Degeneration</cell><cell>3283</cell></row><row><cell>9</cell><cell></cell><cell>Migraine</cell><cell>3703</cell></row><row><cell>10</cell><cell></cell><cell>Otitis</cell><cell>2596</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Comparison of computation time (in seconds) for: EV, PLSA+PE, PCA, LSP, MDS and ISOMAP. A cross x indicates that an algorithm does not provide a result in a reasonable time.</figDesc><table><row><cell>Data size</cell><cell>EV</cell><cell>PLSA+PE</cell><cell>PCA</cell><cell>LSP</cell><cell>MDS</cell><cell>ISOMAP</cell></row><row><cell>n</cell><cell>O(n)</cell><cell>O(n 2 )</cell><cell>O(n 2 )</cell><cell>O( f (n, s))</cell><cell>O(n 3 )</cell><cell>O(n 3 )</cell></row><row><cell>1 × 10 3</cell><cell>0.49</cell><cell>0.42</cell><cell>0.40</cell><cell>15.25</cell><cell>20.48</cell><cell>200.05</cell></row><row><cell>2 × 10 3</cell><cell>0.95</cell><cell>1.50</cell><cell>1.36</cell><cell>30.40</cell><cell>216.62</cell><cell>1611.72</cell></row><row><cell>3 × 10 3</cell><cell>1.43</cell><cell>3.20</cell><cell>2.24</cell><cell>80.62</cell><cell>801.30</cell><cell>x</cell></row><row><cell>4 × 10 3</cell><cell>1.93</cell><cell>5.49</cell><cell>3.78</cell><cell>160.10</cell><cell>1881.00</cell><cell>x</cell></row><row><cell>5 × 10 3</cell><cell>2.55</cell><cell>8.38</cell><cell>x</cell><cell>x</cell><cell>x</cell><cell>x</cell></row><row><cell>1 × 10 4</cell><cell>5.79</cell><cell>x</cell><cell>x</cell><cell>x</cell><cell>x</cell><cell>x</cell></row><row><cell cols="7">models generally provide better visualization layout. Figures 3(a)-(f)</cell></row><row><cell cols="7">show 20-topic news groups visualized by the six methods. Similarly,</cell></row><row><cell cols="7">EV provides the best view since news in similar topics are closer while</cell></row><row><cell cols="5">news of distinct topics are placed further away.</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.infovis-wiki.net/index.php?title=InfoSky 2 http://in-spire.pnl.gov</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://www.cs.uiuc.edu/homes/dengcai2/Data/TextData.html<ref type="bibr" target="#b3">4</ref> http://www.ncbi.nlm.nih.gov/pubmed/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">http://vii.wayne.edu</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research was partially funded by U. S. National Science Foundation under grants IIS-0713315 and CNS-0751045, and by the 21st Century Jobs Fund Award, State of Michigan, under grant 06-1-P1-0193.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>; each color shape represents a disease; and the corresponding big color shape indicates the means of an abstract group. Visualization of (a) 1000 exemplars with their means, (b) two distinct groups of diseases: "Gout" and "Chickenpox" with the selected exemplars (100 in total), emphasized by the bigger black shapes.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Survey of clustering data mining techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Berkhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Accrue Software</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Computing sparse reduced-rank approximations to sparse matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Pulatova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Math. Softw</title>
		<imprint>
			<biblScope unit="volume">844</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="252" to="269" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Algorithm</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Visualizing knowledge domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Boyack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Rev. Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1" to="51" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Multidimensional Scaling. Chapman and Hall/CRC, 2nd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">From visual data exploration to visual data mining: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C F</forename><surname>De Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Levkowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Visualiztion and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="378" to="394" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convex and semi-nonnegative matrix factorizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On the equivalence between non-negative matrix factorization and probabilistic latent semantic indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics and Data Analysis</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3913" to="3927" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Pattern Classification. Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hierarchical parallel coordinates for exploration of large datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Rundensteiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proc. of VIS</title>
		<meeting>of VIS</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Probablistic latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proc. of UAI</title>
		<meeting>of UAI</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="289" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Tenenbaum. Parametric embedding for class visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Iwata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ueda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stromsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="2536" to="2556" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Probabilistic latent semantic visualization: Topic model for visualizing documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Iwata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ueda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proc. of KDD</title>
		<meeting>of KDD</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="363" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Principal Component Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">T</forename><surname>Jolliffe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">News weeder: Learning to filter netnews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="331" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Finite Mixture Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mclachlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>John Wiley &amp; Sons, Inc</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>first edition</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hipp: A novel hierarchical point placement strategy and its application to the exploration of document collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Paulovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Minghim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1229" to="1236" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Least square projection: a fast high-precision multidimensional projection technique and its application to document mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Paulovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Nonato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Minghim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Levkowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Visualiztion and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="564" to="575" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast monte carlo algorithms for matrices iii: Computing a compressed approximate matrix decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Petros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Michael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="184" to="206" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An algorithm for suffix stripping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Visual Data Mining: Techniques and Tools for Data Visualization and Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Soukup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Davidson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>John Wiley &amp; Sons, Inc</publisher>
		</imprint>
	</monogr>
	<note>1st edition</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Less is more: Sparse graph mining with compact matrix decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Analysis and Data Mining</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6" to="22" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On improved projection techniques to support visual exploaration of mutlidimensional data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tejada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Minghim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nonato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">formation Visualization</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="218" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
