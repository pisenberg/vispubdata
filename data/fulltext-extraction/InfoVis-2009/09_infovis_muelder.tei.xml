<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visual Analysis of Inter-Process Communication for Large-Scale Parallel Computing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Muelder</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Gygi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwan-Liu</forename><surname>Ma</surname></persName>
						</author>
						<title level="a" type="main">Visual Analysis of Inter-Process Communication for Large-Scale Parallel Computing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T18:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Information Visualization</term>
					<term>MPI Profiling</term>
					<term>Scalability</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Fig. 1. A scalable MPI visualization. Rather than plotting MPI calls per process, this view plots the duration of the call on the y-axis with a log scale versus time on the x-axis. Patterns such as simultaneous start/end times, clusters and trends of similar calls, and particularly long communications can be seen. The data was collected from running matrix operations from the ScaLAPACK library on 256 nodes of NERSC&apos;s Franklin supercomputer.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Many phenomena are difficult or simply impossible to fully study experimentally due to a lack of reliable methods for measuring or con- troling the experiment. For example, very large-scale phenomena such as supernovae cannot be tested experimentally, and very small-scale phenomena such as interaction of particles at the quantum level cannot be precisely measured. In order to study such phenomena, scientists often use numerical simulations. These simulations can supplement existing partial results or lead to new insights which can guide scientists to specific experiments which would confirm the results. However, to obtain useful results, these simulations usually involve large and complicated calculations, which cannot be processed in a reasonable time on a serial computer. Therefore, a parallel supercomputer must be harnessed for such large-scale high-resolution modeling. For decades, significant investments have been made to advance areas of scientific research by creating larger and more powerful parallel su-percomputers. In recent years, large-scale systems have gone from tera-scale to peta-scale, and are continuing to exa-scale systems, enabling scientists to study complex problems which were previously intractable. Many such systems are being operated by both U.S. Department of Energy (DOE) and National Science Foundation (NSF). DOE's "Scientific Discovery through Advanced Computing" (SciDAC) is researching ways to optimize and utilize large-scale systems, and maintains and funds several of them at different DOE sites, such as the Oak Ridge National Laboratory, Sandia National Laboratories, Lawrence Berkley Laboratory, and Lawrence Livermore National Laboratory <ref type="bibr" target="#b19">[21]</ref>. NSF sponsors several systems of its own though its PetaApps <ref type="bibr" target="#b16">[18]</ref> program, including those at University of Texas, Austin <ref type="bibr">[26]</ref> and University of Illinois at Urbana-Champaign <ref type="bibr">[16]</ref>. These systems involve many thousands of processors networked together to allow for large-scale computational simulations.</p><p>In order to fully utilize such large parallel systems, the algorithms and calculations within the simulation must be carefully parallelized. Most implementations of parallel scientific computing use a message passing paradigm, such as that used by the standard Message Passing Interface (MPI). While some tasks can be embarrassingly parallel, many of the operations necessary for these complex simulations are not trivial to parallelize. For instance, many of the matrix algorithms in First Principle Molecular Dynamics (FPMD) simulations take O(n 3 ) operations to work with O(n 2 ) data <ref type="bibr" target="#b12">[13]</ref>. The result of this is that the more parallel this kind of algorithm is made, the more sparsely data is divided among the processors and hence the more communication is necessary between processes to exchange data. As the number of processors increase, it becomes more effective for optimization to analyze and reduce the communication than to use metrics such as numbers of operations.</p><p>One common way to analyze the communications of such programs is through visualization. Several libraries and tools have been developed to capture MPI events and visualize the captured communication patterns. These tools, while effective at analyzing small systems, often do not scale well to large, massively parallel systems. For instance, one common visualization in most existing tools is a Gantt chart, which lines up the processes vertically and plots the MPI events versus time on the horizontal axis. This technique runs into problems once the number of processes exceeds the number of pixels available on the display. We propose an alternate visual analysis strategy for understanding MPI communications at extreme scales.</p><p>Once large-scale computation involves tens-of-thousands to millions of processes, it becomes less useful to consider every process individually; it makes more sense to consider groups of processes or groups of MPI calls before drilling down to individual processes or MPI events. At the highest level, we consider the system as a whole and see how the overall communications are impacting performance over time. Next, we consider the communications at the level of groups of processes by plotting related communications together regardless of the participating processes. This way, MPI calls can be represented at an abstract level regardless of the number of processes. Finally, individual calls and processes can be singled out from this view. We present a scalable approach to MPI visualization that does this by using a timeline overview in combination with focused views which are abstracted from individual processes. The focused view achieves this by directly mapping the MPI events in a temporal space regardless of process rank and using modulated opacity to show process density, as shown in <ref type="figure">Figure 1</ref>. We also show that with our visualization strategy it becomes possible to understand communication behaviors at a large scale and identify room for performance optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related work</head><p>Software visualization is a fairly broad field. Many visualizations focus on managing software development and repositories <ref type="bibr" target="#b22">[24]</ref>. Star-Gate <ref type="bibr" target="#b15">[17]</ref> is a tool that visualizes both the evolution of the software repository and the communication patterns of the developers involved. Other visualizations focus on visualizing the code itself and aid in the analysis of code dependencies in larger projects. Some use visual-ization to analyze and reverse engineer compiled binary code <ref type="bibr" target="#b23">[25]</ref>. Using visualization to optimize performance has been approached in several ways by existing work. For example, TraceVis <ref type="bibr" target="#b17">[19]</ref> visualizes the execution times of individual CPU instructions, and Bootchart <ref type="bibr" target="#b0">[1]</ref> visualizes the performance of programs involved in the boot process of an operating system. Both of these examples use variants of Gantt charts to present the information. However, these tools focus on serial programming, where parallel issues such as communication delays do not come up.</p><p>The problem of characterizing communication has been studied by many researchers. Network monitoring tools such as EtherApe <ref type="bibr" target="#b3">[4]</ref> and EZEL <ref type="bibr" target="#b25">[28]</ref> show communication patterns well, but they focus on pure network activity and do not incorporate properties particular to distributed communication. The communications between parallel processes and data storage servers has also been researched through analysis of access patterns <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b28">31,</ref><ref type="bibr" target="#b29">32]</ref>. Visualization of communication between software modules such as client-server relationships have also been analyzed through the use of graph based visualizations <ref type="bibr" target="#b30">[33]</ref>. These visual approaches are effective at analyzing network traffic, but by focusing singly on network information, the impact on computation efficiency in a massively parallel computation environment would be difficult to deduce One common set of visualization tools for MPI data is Jumpshot <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b27">30]</ref> and its predecessors (Nupshot <ref type="bibr" target="#b8">[10]</ref> and Upshot <ref type="bibr" target="#b5">[7]</ref>). These tools use the MPI Parallel Environment (MPE) library to intercept the MPI calls in a parallel program. Then they visualize the collected trace with a Gantt chart by plotting process rank versus time using color to represent the MPI calls. ParaGraph <ref type="bibr" target="#b4">[6]</ref> is another, older program that visualizes MPI traces collected with the MPICL library which also uses Gantt charts, among other metrics such as overall summaries and communication graphs. Vampir <ref type="bibr" target="#b14">[15]</ref> is another visual tool which combines Gantt charts and summary views. The Tuning and Analysis Utilities (TAU) <ref type="bibr" target="#b21">[23]</ref> suite of tools is one of the more comprehensive tools. The logging facilities included with it allow for conversion to many of the formats used by other existing tools, such as Jumpshot or Vampir. Its own visualizations include Gantt charts, a communication matrix view, and a call graph, among others. Virtue <ref type="bibr" target="#b20">[22]</ref> is the most unique of the related works listed here in that it is a real-time visualization. This allows the user to monitor the performance of an application while it is running and potentially tune it or interact with it. It also incorporates VR techniques such as a CAVE (Cave Automatic Virtual Environment) to provide a more immersive visualization than most other tools. For other parallel environments, GVUs PVaniM tool <ref type="bibr" target="#b24">[27]</ref> and ATEMPT <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b10">12]</ref> present some detailed views of communication events in a PVM (Parallel Virtual Machine) system. Some software visualizations address the scalability issues of plots such as Gantt charts. The works of Jerding et al. <ref type="bibr" target="#b6">[8]</ref>, Moreta and Telea <ref type="bibr" target="#b13">[14]</ref>, and Cornelissen et al. <ref type="bibr" target="#b2">[3]</ref> use plots similar to Gantt charts to profile program execution traces. However, these works maintain the strict ordering of the charts, and use sub-pixel techniques to handle the scalability and allow for visibility of both large trends and outliers. In contrast, our approach sacrifices the ordering to spatially separate large trends from individual outliers.</p><p>Our approach draws upon several existing visualization techniques. The timeline view consists of a stacked graph representation, and the detailed view is based on techniques such as scatterplots and arc diagrams <ref type="bibr" target="#b26">[29]</ref>. In order to plot a large number of calls simultaneously, we also incorporate existing techniques such as high precision alpha blending and opacity scaling similar to the work by Johansson et al. <ref type="bibr" target="#b7">[9]</ref>.  a timeline view which shows the status of the entire system over the entire run by depicting what fraction of the system is performing what MPI calls over time. From the timeline, a range of time can be selected to focus on. The MPI call view plots MPI calls within this range directly with respect to time, using opacity to handle overplotting issues due to the scale of the data. From the MPI call view, the individual processes can be highlighted to provide specific details to the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Timeline View</head><p>The timeline view depicts a stacked graph of the overall process activity over time. Each stacked area of the graph is associated with an MPI function, and its height represents the fraction of the processes that were calling that function at that time. One result of this is that the height of the remaining space which is empty corresponds to the efficiency of the system as a whole, as that is the fraction of processes not involved in communication at that time. <ref type="figure" target="#fig_1">Figure 2</ref> shows a small portion of the timeline view blown up for clarity, with the MPI functions colored according to the legend in <ref type="figure" target="#fig_2">Figure 3</ref>. The more common functions are colored in unique colors while the less common functions are all grey. The timeline view is also used as an interface to select smaller time ranges to view in more detail, and the selected range is indicated by the semi-transparent box shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">MPI Call View</head><p>The most direct representation of the MPI calls is to render each call from each process with respect to time. Gantt charts do this, but they restrict the y-axis to represent the MPI calls' originating processes. While we retain the use of the x-axis as time, we chose to use the yaxis to represent other properties of the MPI calls. In particular, we found it effective to use the y-axis to represent duration of the MPI calls, particularly on a log scale as the durations vary over several orders of magnitude. The advantage to using duration on the y-axis is that large delays due to communication will be prominently seen at the top of the plot. Since this and other y-axis mappings allow the MPI calls to overlap, we modulate the opacity of the calls, which makes the overall intensity of the visualization represent the density of the MPI calls. The color is mapped to the MPI function being called as in the timeline. For the representation of the calls themselves, we explored several options, including arches, lines, and individual points, examples of which are shown in <ref type="figure" target="#fig_3">Figure 4</ref>.</p><p>Of the representations we use, the arch representation, shown in <ref type="figure" target="#fig_3">Figure 4</ref>(a), is the least scalable, but is probably also the most intuitive. The beginning point of each arch corresponds to the start time of the MPI call, and the end point of the arch corresponds to the end time. The y-position of the apex is proportional to the duration of the call on some scale. <ref type="figure" target="#fig_3">Figure 4</ref>(a) is on a linear scale, and depicts patterns that show dependency relationships such as when many processes are dependent on a previous synchronized MPI call or when one global communication is blocking. These are indicated by sets of MPI calls that either start or end nearly simultaneously.</p><p>The line representation, shown in <ref type="figure" target="#fig_3">Figure 4</ref>(b), is the most similar to traditional Gantt charts. Each line goes from the start of the MPI call to the end of the MPI call. As in the other representations, the yposition is proportional to the duration and, in this example, is on a log scale. This representation is more scalable than the arc representation as it produces less clutter on the screen. This comes at the cost of being able to readily see dependencies, as dependent MPI calls no longer touch. However, patterns of simultaneous starting and stopping of MPI calls are readily visible as vertically linear and logarithmic trends. Also, clear groupings of MPI calls can be seen, corresponding to the originating MPI functions in the code.</p><p>As the duration of the MPI calls are already being encoded in the height, it is redundant to show duration on the x-axis as well. So the final and most scalable representation of MPI calls we implemented uses simple points to plot the duration of the MPI calls versus either the start or end times of the MPI calls, as shown in <ref type="figure" target="#fig_3">Figure 4(c)</ref>. Similar to the line representation, dependency information is not easily visible. However, vertical and logarithmic trends clearly delimit simultaneous function calls and returns. When plotting start times versus duration, the vertical trends show simultaneous start times and the log curves to the left show simultaneous end times, and when plotting end time versus duration it is the other way around, with the log curves going to the right.</p><p>From any of these representations, details of any MPI call can be determined by selecting it with the mouse, at which point all the calls from the selected call's process are highlighted, and details about the selected MPI call are presented to the user textually, as is demonstrated in all three examples in <ref type="figure" target="#fig_3">Figure 4</ref>. MPI functions can also be highlighted by selecting them from the color legend, at which point all calls to that function get highlighted in the call view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Opacity Scaling</head><p>When plotting the MPI calls with our approach, many of them overlap, particularly when they start or end simultaneously. A simple way to resolve this overlap is to make the calls semitransparent and use alpha blending to combine them. However, this very quickly runs into limitations as the number of calls increases, as shown in <ref type="figure">Figure5(a)</ref>. First, the standard 8-bit alpha buffer only allows for a maximum overplotting of 256. And second, in order to show overlap of large numbers of MPI calls, the opacity has to be set so low that outliers are nearly invisible. In order to keep both the opacity of outliers high and the combined opacity of dense overlap from overflowing the alpha buffer, we utilize the opacity scaling techniques of <ref type="bibr" target="#b7">[9]</ref>. In our implementation of this technique, we first render to a high precision density buffer D which keeps track of the total amount of overplot and then to a high precision color buffer C which blends the input color information with opacity inversely proportional to the density information to result in an average color that is fully opaque. We then combine these buffers with a transfer function to render the final pixels P to the screen. We implemented two such functions: a linear map, and a logarithmic map. The linear map (shown in Figure5(b)) is defined as:</p><formula xml:id="formula_0">P x,y = C x,y × o min + (1 − o min ) × D x,y (D max )</formula><p>And the logarithmic map is defined as (shown in Figure5(c)):</p><formula xml:id="formula_1">P x,y = C x,y × o min + (1 − o min ) × log D x,y log (D max )</formula><p>Where o min is a user defined minimum opacity level and D max is the maximum level of overplotting that occurred. By calculating the final opacity in this manner, we guarantee that any outliers will have at least opacity o min , that no overplotting exceeds the maximum opacity and, in the case of the logarithmic map, that the system will be able to handle many orders of magnitude of overplotting.  <ref type="figure">Fig. 5</ref>. Opacity Scaling. Standard opacity accumulation (a) has trouble both with too much overplotting and with outliers being too transparent. Applying a minimum value and scaling the opacity (b) helps, and applying a logarithmic scale (c) helps even more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CASE STUDIES</head><p>Many simulations are performed through the use of large linear algebra calculations. One of the most common tools used to run these calculations is the ScaLAPACK library, which utilizes MPI communications to perform distributed linear algebra calculations. The Qbox FPMD simulation codes, for instance, utilize ScaLAPACK functions intensively. In order to demonstrate our approach of visualizing large parallel MPI traces, we use it to analyze matrix operations which use the ScaLAPACK library and its underlying libraries.</p><p>We captured the MPI communication using the Multi-Processing Environment (MPE) library. This generates a standardized log file in either clog2 or slog2 format, which we can then visualize. The examples shown here were run on NERSC's Franklin, which is a Cray XT4 massively parallel processing system with 38,128 Opteron compute cores and a peak performance of 356 TFlops/sec <ref type="bibr">[5]</ref>. All tests were run with one process per processor, so that no extraneous context switching overhead would be incurred. While tracing adds overhead for writing the log file out at the end of the program, we found that the impact on performance of actual computation was negligible.</p><p>Common Matrix Operations <ref type="figure" target="#fig_6">Figure 6</ref> shows the results of visualizing a series of common matrix operations. The operations chosen are commonly used in scientific calculations. In this example, the operations were run on 256 processes. From the timeline in <ref type="figure" target="#fig_6">Figure 6(a)</ref>, the first thing that is plainly visible is that the program went through several visually distinct stages, each of which correspond to different matrix operations. We visualize each section in more detail, then compare and contrast them.</p><p>The first operation performed was a matrix multiplication, shown in <ref type="figure" target="#fig_6">Figure 6</ref>(b). As indicated by the colorings, the matrix multiplication's communication pattern mostly consists of MPI Send, MPI Recv, and MPI Reduce, with the MPI Recv calls generally taking the longest. The calls are staggered and generally quite short, indicating that the algorithm is already well optimized. It ends with a single large MPI All Reduce which resynchronizes the system.</p><p>Inversion is more complicated than multiplication, involving multiple steps. It starts with an LU decomposition ( <ref type="figure" target="#fig_6">Figure 6(c)</ref>), then uses the resulting triangular matrices to calculate the actual inverse ( <ref type="figure" target="#fig_6">Figure 6(d)</ref>). The LU decomposition consisted almost entirely of MPI Recv calls, with the corresponding MPI Send calls barely visible. Interestingly, there is a very cyclic pattern, alternating between short calls and long calls. The strong synchronicity of the communications in this section is also interesting, and it could indicate potential for optimization either through redistribution of the data or changing the communication methods to be more asynchronous. <ref type="figure" target="#fig_6">Figure 6(d)</ref> shows the completion of the matrix inversion and contains two sub-sections. These sections each start with large calls to MPI Reduce, and there are many shorter calls to MPI Bcast in the first half and MPI Recv in the second half. While the calls to MPI Bcast and MPI Recv are quite numerous, they are generally short and staggered. The real expense here are the MPI Reduce calls, which keeps many processes idle for a long time. The these MPI Reduce calls form a very distinctive pattern where they start synchronously, but their ends follow a logarithmic trend. This pattern could be indicative of a network communication issue. For instance, this pattern could be induced by using a logical tree communication network when the underlying physical network topology is a actually a torus.</p><p>The eigenproblem is an eigenvector/eigenvalue solver and is the single largest matrix operation in this case study, so in <ref type="figure" target="#fig_6">Figure 6</ref>(e), we only show a representative part of it. Most of the MPI calls here also group together into distinct clusters, and they are a mix of MPI Reduce and MPI Recv calls, with the MPI Reduce taking slightly longer. However, they are all very short compared to the single large MPI Bcast which starts at the top of <ref type="figure" target="#fig_6">Figure 6</ref>(e), gradually locks more processes as the program progresses, and does not finish until the middle of <ref type="figure" target="#fig_6">Figure 6</ref>(f), at which point some processes have been idle for more than half of the total computation time. This can be seen in its entirety at the top of <ref type="figure">Figure 1</ref>. <ref type="figure" target="#fig_6">Figure 6</ref>(f) depicts more calculations that were involved in the eigenproblem after the long operation finished, such as MPI All Reduce calls, along with the MPI Reduce and MPI Recv calls which are running very synchronously.</p><p>The sixth section, shown in 6(g) shows a Gram-Schmidt orthogonalization, which is composed of 4 individual matrix operations, one of which is trivially parallelizable and takes nearly no time to complete. The communications in the other three operations look much like the matrix multiplication, with the exception that there are gaps between the operations where the processes synchronized and that there are some MPI All Reduce calls. However, there is one large cluster of calls to MPI Send and MPI Recv near the end which are substantially longer. We determined that this occurred in the middle of the pdtrsm() operation. If this were due to a straggling process or poor load balancing, the calls would end simultaneously. Since they do not, this could indicate a network bottleneck or other system interference. Testing Scalability While <ref type="figure" target="#fig_6">Figure 6</ref> demonstrates our approach on a series of matrix operations on a moderate size system, larger systems should also be considered. In order to investigate the effects of scaling on the visualization, we focus on matrix multiplication, as it is a commonly performed operation. <ref type="figure" target="#fig_7">Figure 7</ref> demonstrates the effects of scaling up a matrix multiplication from a modestly small set of processes (64) up to large numbers of processes (16,384). As the number of processes is scaled up, so is the size of the data it is working on. This keeps the communication effects from completely overwhelming the execution, and vice versa. The first observation that can be made from these timelines is that the proportion of time spent doing the actual calculation decreases as the scale goes up. That is, as more processes are used, it takes longer to finish the initialization process to set up the communication channels and distribute the initial data. By 4,096 processes <ref type="figure" target="#fig_7">(Figure 7(d)</ref>), it already takes more time to initialize the program than to calculate the result. However, this effect would be offset on the more complex programs or larger datasets used in actual simulations, as the shorter computation offered by larger systems marginalizes the cost of initialization. Another observation that can be made is that within the matrix multiplication itself, the more processes there are, the greater the proportion of them that are in the middle of some form of communication at any given time, and thus the lower the efficiency of the system. In particular, by the time we reach 16,384 processes, almost all the time is spent in communication rather than actual computation. Finally, while the communication patterns were fairly cyclic at smaller scales, variances in the communications add up in the larger scales leading to acyclic patterns, as can be seen well in <ref type="figure" target="#fig_7">Figure 7</ref>(c). To understand what goes on within the matrix operation at large scales, we then focus on it in the detail view. <ref type="figure">Figure 8</ref> shows the detail of the matrix multiplication on 16,384 processes shown near the end of <ref type="figure" target="#fig_7">Figure 7(d)</ref>. As this scale, the MPI calls are quite dense, so we use the point representations of plotting either start or end times versus duration separately. <ref type="figure">Figure 8(b)</ref> shows the end points of the communication calls. The first major trend visible in this view is that there are two stages in the operation. While the second half is much the same as in smaller scales such as in <ref type="figure" target="#fig_6">Figure 6</ref>(b), the first half is quite different. It contains MPI calls that took much longer than at the smaller scale. Namely, it begins with MPI Recv calls that take a long time followed by some MPI Reduce calls which took longer than normal. It can be seen that there were still unfinished MPI Comm Create calls, which would explain the perturbation of the matrix multiplication. Thus in this case, to improve the performance of the matrix multiplication itself it would help to optimize the initialization procedures. After that, the matrix multiplication is a fairly dense mix of MPI Send, MPI Recv, and MPI Reduce calls with some of the MPI Recv calls taking distinctly longer than the rest of the MPI calls.</p><p>In order to better understand the normal communication patterns at this scale, we zoom into a small region of the operation where the communication was fairly regular. Even at this scale, the number of communications and thus points on the screen is quite dense. However, some small trends and clusters of communication are visible. One point of interest is how the communication does split into several very distinct layers. The MPI Send calls are still the shortest, as in <ref type="figure" target="#fig_6">Figure 6</ref>(b). Next is a layer of MPI Recv calls which are also fairly short. Above that are the MPI Reduce calls, which take longer to distribute data among all processes involved. Finally, there is the distinct layer of clusters of MPI Recv calls above, which are clearly separated from the rest of the calls.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSIONS</head><p>As massively parallel computer systems are constantly moving to larger scales, it is becoming ever more important to understand how to use these systems efficiently. Access to these systems is often limited, so scientists cannot usually afford to thoroughly analyze their codes during long term and computation intensive simulations. Our approach uses process independent visualization and focus+context techniques to offer more scalability than traditional parallel system visualizations. And by analyzing a common scientific computation library on a modern supercomputer, our results can aid in refining and optimizing the underlying library used by the scientists, which would allow for more efficient use of the limited access time the scientists are allotted on similar large-scale systems. (c) Matrix Inversion -LU decomposition ("pdgetrf()"). In the LU decomposition, communications alternate between distinct groups of short and medium length calls, indicating synchronizations.</p><p>(d) Matrix Inversion -Invert using LU ("pdgetri()"). This operation has two parts, each with a single large MPI Reduce and many smaller, staggered communications. The MPI Reduce calls are potentially inefficient. (e) Eigenproblem ("pdsyevd()"). The longest operation, so only a small section is shown here. Mostly, the communications are quite short, but gradually, processes get stuck in very long, inefficient call to MPI Bcast. (f) Eigenproblem cont.. All the MPI calls from the previous section end synchronously, including the very long MPI Bcast. Communications are sparse in this next section, indicating periods of heavier computation. The patterns themselves are quite synchronized, similar to the LU decomposition patterns in Figure6(c) (g) Gram-Schmidt orthogonalization cont. ("pdsyrk()," "pdsyr()," "pdpotrf()," and "pdtrsm()"). The patterns here are similar to the matrix multiplication patterns in Figure6(b), with two exceptions: the two major synchronization points near the middle which delimit a small section, and abnormally long calls to MPI Send and MPI Recv near the end.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">FUTURE WORK</head><p>While the results we achieved were quite effective at the scale we were dealing with, further extension of this work to greater scales could prove challenging. For instance, we currently load the entire log file into memory before visualizing it. Very large log files would need outof-core access. Support for more log formats would be very beneficial to this end. We support clog and clog2 formats, but extending to slog2 format would aid out-of-core visualization, as it was designed with that intent. Extending the work to include profiling a real simulation would be useful, but the resulting log file would likely be much larger than the ones shown here. This would necessitate not only out-of-core data access, but also a higher level interface than the current timeline, such as one that abstracts the data to the matrix operation level. The data formats we use do not clearly identify the MPI call across processes. If we move to a data format that identifies the calls hierarchically from the function level, the MPI calls could be accurately clustered together, which would allow for a hierarchically based visualization. As our current approach only uses two views, it would be interesting to either add an intermediate level view or a more detailed view based on selections from the MPI call plot. Further understanding could also be achievable by taking into account the topology of the supercomputer itself, or by drilling down to the underlying network traffic. This would allow for detection of network bottlenecks, which our current system cannot explicitly show.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>•</head><label></label><figDesc>Chris Muelder, Francois Gygi, and Kwan-Liu Ma are with University of California, Davis. • E-mails: muelder@cs.ucdavis.edu,fgygi@ucdavis.edu, and ma@cs.ucdavis.edu Manuscript received 31 March 2009; accepted 27 July 2009; posted online 11 October 2009; mailed on 5 October 2009.For information on obtaining reprints of this article, please send email to: tvcg@computer.org .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Section of a timeline of MPI calls. The timeline provides an overview of the activity of the entire system on the y-axis versus time on the x-axis. Each layer corresponds to an MPI function, and the height is the fraction of processes calling that function. Distinct homogenous sections of the timeline correspond to the various matrix operations performed in the program. From the timeline, ranges of data can be selected to view in more detail.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>.Fig. 3 .</head><label>3</label><figDesc>Color Legend. The colors used in all figures. For clarity, the most prominent functions have been colored while the less commonly visible functions have been grayed out.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>MPI Call Plots. Different representations of MPI calls for direct visualization. Arches (a) are easier to visually follow, while lines (b) and start/end points (c) are more scalable with respect to screen space. In each case, one call is selected, with details presented in text form, and with all calls from the same process highlighted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) Standard opacity accumulation (b) Linear opacity map with minimum value (c) Logarithmic opacity map with minimum value</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Matrix Multiplication ("pdgemm()"). This operation seems to be optimized fairly well. While there is much communication going on, the calls are staggered well, keeping overall communication lengths small.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>A series of matrix operations. The timeline shows the entire run of program consisting of a series of matrix operations on 256 processes. The individual matrix operations are visible as distinct sections in the timeline (a). Each section is shown in more detail with call plots (b-g). The ScaLAPACK functions are listed in the parentheses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>(a) 64 Processes, 10,000x1,000 matrices. Measured = 199 GFlop/s. Peak = 589 GFlop/s. Efficiency = 34% (b) 256 Processes, 10,000x2,000 matrices. Measured = 725 GFlop/s. Peak = 2355 GFlop/s. Efficiency = 31% (c) 1,024 Processes, 20,000x2,048 matrices. Measured = 1,263 GFlop/s. Peak = 9,420 GFlop/s. Efficiency=13% (d) 4,096 Processes, 20,480x2,048 matrices. Measured = 1,611 GFlop/s. Peak = 37,683 GFlop/s. Efficiency = 4.2% (e) 16,384 Processes, 20,480x2,048 matrices. Measured = 1,438 GFlop/s. Peak = 150,733 GFlop/s. Efficiency = 0.9% Effect of scale on matrix multiplication. As the number of processes increases, so does the cost of setting up the communication structures before actually executing the matrix operation. Overall time for running the program increases with the number of processes and scale of the data, but the multiplication takes less time per element. The efficiency of the system drops substantially with large numbers of processes.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">A SCALABLE APPROACHAs the number of processes increases, the usefulness of keeping track of individual processes lessens, and it becomes more helpful and more useful to consider the system as a whole or in part before looking at individual processes. However, it is still useful to be able to drill down into the details of the data, so we implemented an interactive focus+context visualization which presents a high level abstraction, a focused view, and details on demand. The high level view consists of</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work is supported in part by the National Science Foundation through grants CCF-0938114, CCF-0808896, OCI-0749227, OCI-0749217, CNS-0551727, and CCF-0811422, and the U.S. Department of Energy through the SciDAC program with Agreement No. DE-FC02-06ER25777. This research used resources of the National Energy Research Scientific Computing Center (NERSC) through the DOE SciDAC program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bootchart</surname></persName>
		</author>
		<ptr target="http://www.bootchart.org/" />
		<title level="m">visualization of system startup processes for optimizing boot times</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An efficient format for nearly constanttime access to arbitrary time intervals in large trace files</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gropp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lusk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Programming</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="155" to="165" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Execution trace analysis through massive sequence and circular bundle views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cornelissen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zaidman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Holten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Moonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Deursen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Van Wijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Syst. Softw</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2252" to="2268" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">EtherApe: A graphical network monitor</title>
		<ptr target="http://etherape.sourceforge.net" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Paragraph: a tool for visualizing performance of parallel programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Heath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second Workshop on Environments and Tools for Parallel Sci. Comput</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="221" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Studying parallel program behavior with upshot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Herrarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lusk</surname></persName>
		</author>
		<idno>ANL-91/15</idno>
		<imprint>
			<date type="published" when="1991" />
			<pubPlace>Argonne National Laboratory</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Visualizing interactions in program executions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Jerding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Stasko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ball</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICSE &apos;97: Proc. of the 19th Intl. Conf. on Software engineering</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="360" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Revealing structure within clustered parallel coordinates displays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ljung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">InfoVis &apos;05: Proc. of the 2005 IEEE Symposium on Information Visualization</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="125" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Performance analysis of MPI programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Karrels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lusk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Workshop on Environments and Tools For Parallel Scientific Computing</title>
		<editor>J. Dongarra and B. Tourancheau</editor>
		<meeting>of the Workshop on Environments and Tools For Parallel Scientific Computing</meeting>
		<imprint>
			<publisher>SIAM Publications</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="195" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Debugging point-to-point communication in MPI and PVM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kranzlmueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Volkert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">1497</biblScope>
			<biblScope unit="page" from="265" to="272" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Debugging massively parallel programs with atempt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kranzlmüller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Volkert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCN Europe 1996: Proc. of the Intl. Conf. and Exhibition on High-Performance Computing and Networking, pages (a) End points (b) Zoomed in focus with start points</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">When zoomed in further, some detailed trends can be seen in the midst of the ocean of communication, but these clusters are generally small, indicating good division of processes</title>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Springer-Verlag</publisher>
			<biblScope unit="page" from="806" to="811" />
			<pubPlace>London, UK</pubPlace>
		</imprint>
	</monogr>
	<note>There is a clear section early in the operation with longer MPI calls caused by interference from the previous operation, as seen in the upper left of (a)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Martin</surname></persName>
		</author>
		<title level="m">Electronic Structure. Basic Theory and Practical Methods in Physics</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multiscale visualization of dynamic software logs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moreta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Telea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EuroVis</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="11" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Nagel</surname></persName>
		</author>
		<ptr target="http://www.vampir-ng.de/index.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stargate: A unified, interactive visualization of software projects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE PacificVis</title>
		<meeting>of IEEE PacificVis</meeting>
		<imprint>
			<date type="published" when="2008-03" />
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<ptr target="http://www.nsf.gov/pubs/2007/nsf07559/nsf07559.htm" />
		<title level="m">Accelerating Discovery in Science and Engineering through Petascale Simulations and Analysis (PetaApps), the National Science Foundation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">TraceVis: An execution visualization tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Roberts</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004-07" />
			<pubPlace>Urbana, IL</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Illinois</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visualization and parallel i/o at extreme scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peterka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Moreland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of DOE SciDAC</title>
		<meeting>of DOE SciDAC</meeting>
		<imprint>
			<date type="published" when="2008-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
		<ptr target="http://www.scidac.gov/" />
	</analytic>
	<monogr>
		<title level="j">Scientific Discovery through Advanced Computing</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Virtue: performance visualization of parallel and distributed applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shaffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whitmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schaeffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="44" to="51" />
			<date type="published" when="1999-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The tau parallel performance system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Shende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Malony</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. High Perform. Comput. Appl</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="287" to="311" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On the use of visualization to support awareness of human activities in software development: a survey and a framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-A</forename><forename type="middle">D</forename><surname>Storey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Čubranić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>German</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SoftVis &apos;05: Proc. of the 2005 ACM symposium on Software visualization</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="193" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An interactive reverse engineering environment for large-scale c++ code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Telea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Voinea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SoftVis &apos;08: Proc. of the 4th ACM symposium on Software visualization</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="67" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">PVaniM: a tool for visualization in network computing environments. j-CPE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Topol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Stasko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sunderam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998-12" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1197" to="1222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ezel: a visual tool for performance assessment of peer-to-peer file-sharing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Voinea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Telea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Van Wijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">InfoVis &apos;04: Proc. of the 2004 IEEE Symposium on Information Visualization</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<title level="m">Arc diagrams: Visualizing structure in strings. InfoVis &apos;02: Proc. of the 2002 IEEE Symposium on Information Visualization</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">110</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">From trace generation to visualization: A performance framework for distributed parallel systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bolmarcich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Snir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wootton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Parpia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lusk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM/IEEE Supercomputing (SC00)</title>
		<meeting>of ACM/IEEE Supercomputing (SC00)</meeting>
		<imprint>
			<date type="published" when="2000-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A study of I/O techniques for parallel visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Parallel Computing</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="183" />
			<date type="published" when="2005-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A parallel visualization pipeline for terascale earthquake simulations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM/IEEE Supercomputing (SC04)</title>
		<meeting>of ACM/IEEE Supercomputing (SC04)</meeting>
		<imprint>
			<date type="published" when="2004-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Analyzing the reliability of communication between software entities using a 3d visualization of clustered graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeckzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kalcklösch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schröder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SoftVis &apos;08: Proc. of the 4th ACM symposium on Software visualization</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="37" to="46" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
