<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Geodesic Distance-weighted Shape Vector Image Diffusion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Jing</forename><surname>Hua</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoqiang</forename><surname>Lai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Dong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianfeng</forename><surname>Gu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Qin</surname></persName>
						</author>
						<title level="a" type="main">Geodesic Distance-weighted Shape Vector Image Diffusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-20T19:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Surface Matching</term>
					<term>Shape Vector Image</term>
					<term>Multiscale Diffusion</term>
					<term>Visualization</term>
				</keywords>
			</textClass>
			<abstract>
				<p>This paper presents a novel and efficient surface matching and visualization framework through the geodesic distanceweighted shape vector image diffusion. Based on conformal geometry, our approach can uniquely map a 3D surface to a canonical rectangular domain and encode the shape characteristics (e.g., mean curvatures and conformal factors) of the surface in the 2D domain to construct a geodesic distance-weighted shape vector image, where the distances between sampling pixels are not uniform but the actual geodesic distances on the manifold. Through the novel geodesic distance-weighted shape vector image diffusion presented in this paper, we can create a multiscale diffusion space, in which the cross-scale extrema can be detected as the robust geometric features for the matching and registration of surfaces. Therefore, statistical analysis and visualization of surface properties across subjects become readily available. The experiments on scanned surface models show that our method is very robust for feature extraction and surface matching even under noise and resolution change. We have also applied the framework on the real 3D human neocortical surfaces, and demonstrated the excellent performance of our approach in statistical analysis and integrated visualization of the multimodality volumetric data over the shape vector image.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>To date, 3D surface matching and data visualization is still a very challenging research problem in many visual data processing and analysis fields. This is partly because surfaces may have highly flexible freeform shape characteristics which are difficult to be captured and used for matching and registration purposes, especially under noisy conditions. One of the fundamental issues in surface matching is the shape representation scheme. In recent years, different shape representations, such as curvature-based representations <ref type="bibr" target="#b24">[25]</ref>, regional point representations <ref type="bibr" target="#b19">[20]</ref>, shape distributions <ref type="bibr" target="#b17">[18]</ref>, etc., have been proposed for 3D surface matching. These representations and matching algorithms are directly dependent on surface meshes, and therefore, are not robust and do not perform well for inter-subject surface registration under such circumstances as noises, resolution changes, and variances.</p><p>Recently, converting the problem of surface analysis to canonical domains has gained increasing attention <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b27">28]</ref>. A family of geometric maps has recently been adopted to create a shape image that does not suffer from aforementioned problems such as noise, etc. This simplifies the shape-analysis problem to a 2D image-analysis problem. When constructing shape images, geometric mapping provides a viable solution to map a 3D surface to a 2D domain and encode the shape information of the surface in the 2D image. According to the conformal geometry theory, each 3D shape can be mapped to a 2D domain through a global optimization and the resulting map is a diffeomorphism <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b7">8]</ref>. Consequently, the 3D shape analysis problem can be simplified to a 2D image analysis problem of the conformal geometric maps. These maps are stable, insensitive to resolution changes and robust to occlusion and noises. Therefore, accurate and efficient 3D shape analysis may be achieved through 2D image analysis. Along this direction, Wang et al. <ref type="bibr" target="#b25">[26]</ref> presented a global correlation-based method for matching least-squares conformal maps, which works for the recognition purpose, but is not effective for exact matching and registration of different surfaces. In general, there is a lack of analysis tools for processing of this type of special shape images, especially based on local features.</p><p>• J. Hua, Z. <ref type="bibr">Lai</ref>  In this paper, we present a novel and efficient surface matching and visualization framework based on the geodesic distance-weighted shape vector image diffusion. Firstly, our framework conformally maps a to-be-analyzed surface to a canonical 2D domain. The surface curvatures and conformal factors are then interpolated and encoded into the rectangular 2D domain, which we call shape vector image in this paper. As the surface curvatures and conformal factors can uniquely define the surface, the vector image composed by curvature and conformal factors can serve as the shape signature. In the shape vector image, the distances between sampling pixels are the actual geodesic distances on the manifold. Since the mapping is independent of the mesh resolution, the resulting shape vector image is robust to different samplings of the surface. In order to extract the most robust and salient features to abstract the shape vector image, we propose to create a multiscale vector-valued diffusion space through our novel geodesic distance-weighted shape vector image diffusion. As a result, analysis of the shape vector image in its diffusion space is similar to the direct diffusion analysis of the 3D model. A valuable point here is that our computation is executed in a regular 2D domain, which is much simpler than in the 3D domain.</p><p>In the diffusion space, we can then extract distinctive features used for matching and analysis. A rich set of scale-aware features can be extracted from the diffusion space representation. Similar to the feature extraction technique in <ref type="bibr" target="#b9">[10]</ref>, our approach detects the extrema across the scales as keypoints. We then calculate the orientation histograms around the keypoints as feature descriptors, which provide distinctive bases for representing the 3D geometry of the original shape. These scale-aware geometric features can directly be used for robust matching and registration against the noises and distortions. Therefore, statistical analysis and visualization of surface properties across subjects become readily available. This is important for many real-world applications. For example, it is very useful for processing inter-subject brain surfaces from medical scans of different subjects since these surfaces exhibit the inherited physiological variances among subjects. We have conducted extensive experiments on scanned real-world surface models and real 3D human neocortical surfaces, through which we demonstrate the excellent performance of our approach in surface matching and registration, statistical analysis, and integrated visualization of the multimodality volumetric data over the shape vector image.</p><p>Our contributions in this paper can be summarized as follows:</p><p>• We present a novel geodesic distance-weighted shape vector image diffusion method. It allows the construction of the multiscale diffusion space and scale-space processing of the shape vector image composed of the intrinsic geometric characteristics, e.g., mean curvatures and conformal factors.</p><p>• By detecting the cross-scale extrema in the diffusion space, we can create a set of distinctive scale-aware geometric features. These robust features are well suitable for surface matching and registration.</p><p>• We apply the framework to the matching of scanned face data and multimodality brain surface data, which demonstrates the excellent performance of the proposed geodesic distance-weighted shape vector image diffusion method.</p><p>• With thin-plate spline deformation of matched features among different subjects, we develop a framework for comparative analysis and visualization of cross-subject multimodality neuroimaging data. It is a powerful tool for analyzing surfaces with multidimensional textures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Prior work</head><p>Geometric maps have been widely applied in the fields of computer graphics and computer vision. Zhang and Hebert used harmonic maps to construct scalar shape images to match the surfaces <ref type="bibr" target="#b28">[29]</ref>. In <ref type="bibr" target="#b26">[27]</ref>, harmonic maps were used to track dynamic 3D surface. However, calculating harmonic maps needs to identify the surface boundary and create the boundary mapping from 3D surfaces to the 2D domain, which becomes unreliable when there are noises and occlusions in the 3D original data. Since the interior feature points are often more robust in the 3D original data, conformal maps, which do not need boundary information, can be a natural choice to overcome the difficulty. Using several feature constraints instead of the boundary condition, conformal maps have many appealing properties <ref type="bibr" target="#b25">[26]</ref>. For example, if the parameterization is conformal, the surface can be uniquely determined by the mean curvatures with area stretching factors defined on the parametric domain. In <ref type="bibr" target="#b6">[7]</ref>, genus zero surface conformal mapping was discussed and it was adopted for the brain surface mapping <ref type="bibr" target="#b30">[31]</ref>. The conformal parameterization can be uniquely determined by two corresponding points. Conformal parameterization depends on the geometry itself, not the triangulation of the surfaces. Hence, conformal mapping is a viable solution for 3D shape image construction. This motivated us to encode a shape vector image using conformal mapping for surface representation. After mapping the 3D surface to the 2D image plane, extracting features from the shape image becomes one of the most important tasks. Generally speaking, the analysis of 2D image is a better understood problem <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b12">13]</ref>. However, the analysis of the shape image that integrates geometric and appearance information has its own special challenges mainly due to the non-uniform sampling and different pixel properties (i.e., geometric characteristics instead of grey-scale intensities). Hence, conventional image analysis techniques may not work well. For example, line segments <ref type="bibr" target="#b4">[5]</ref>, groupings of edges <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15]</ref>, and regions <ref type="bibr" target="#b2">[3]</ref> have all been proposed as features for image matching. Nevertheless, these feature extraction techniques are not reliable for shape images due to the aforementioned special characteristics of shape images and they only work well under certain circumstances. Recently, there has been a great deal of research work on developing more reliable features for conventional images. One of them is to take advantage of corner detectors. Zhang et al. <ref type="bibr" target="#b29">[30]</ref> used the Harris corner detector to identify feature locations. Harris corner detector was also used by Schmid and Mohr <ref type="bibr" target="#b21">[22]</ref> to identify interest points for the object recognition problem. Other approaches have been proposed for appearance-based matching, including eigenspace analysis <ref type="bibr" target="#b13">[14]</ref>, color histograms <ref type="bibr" target="#b23">[24]</ref>, and receptive field histograms <ref type="bibr" target="#b20">[21]</ref>. However, it is difficult to extend them to match variant inter-subject images because of their more global features. Ohba and Ikeuchi <ref type="bibr" target="#b16">[17]</ref> successfully applied the eigenspace approach to cluttered images by using many small local eigen-windows, but this is very time consuming and not practical. Most recently, SIFT, scale-invariant features, has been proposed <ref type="bibr" target="#b9">[10]</ref> for 2D regular images and proven to be the most robust among many other local invariant feature descriptors with respect to different geometrical changes <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b0">1]</ref>. SIFT is based on the scale space theory. Scale-space processing usually convolves an image with Gaussian filters, generating a sequence of images, and then the difference of successive Gaussian-blurred images are calculated to create the Difference of Gaussians (DoG) for further analysis. Since scalespace theory was mainly developed for gray-scale images, the usage of the scale-space processing on the special shape images or shape vector images are under-explored and its performance remains unknown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">CONFORMAL SHAPE VECTOR IMAGE CONSTRUCTION</head><p>A good shape image should be able to fully represent the geometric characteristics of a given surface, and thus serves as a domain for indexing other heterogenous attributes. Thus, a 3D surface can be converted to a multidimensional vector image for effective processing. We employ conformal mapping to accomplish the task. In order to match 3D shapes accurately and efficiently, we develop a 2D representation, shape vector image, using conformal mapping. Given a surface patch M, its conformal image I c can be created using conformal mapping. There is one-to-one correspondence between the vertices in M and the vertices in I c . Based on the shape attributes computed at each vertex of M, attribute values can be interpolated and computed for each pixel of the conformal shape vector image. In practice, we compute the conformal parameterization by a nonlinear optimization method carried out in the tangential space of a sphere as proposed in <ref type="bibr" target="#b6">[7]</ref>.</p><p>Suppose K denotes the simplical complex and there is a piecewise linear embedding l : |K| → R 3 . Then a triangular mesh can be represented as (K, l). For the purpose of implementation, surfaces are usually approximated by triangular meshes. We use p, q to denote the vertices and {p, q} to denote the edge spanned between p and q. The surface and its parametric domain are modeled as piecewise linear functions f and g in accordance with (K, l), respectively. The mean curvature at vertex q is estimated as in <ref type="bibr" target="#b10">[11]</ref> by</p><formula xml:id="formula_0">H(q) = 1 4A ∑ p∈N 1 (q) (cot α p,q + cot β p,q )( f (q) − f (p)) 2 ,<label>(1)</label></formula><p>where α p,q and β p,q are the two opposite angles of edge p, q in the two triangles sharing this edge, and N 1 (q) is the set of 1-ring neighbor vertices of vertex q. A is the area of the associated surface patch (socalled finite volume in Mechanics), which is given by</p><formula xml:id="formula_1">A(q) = 1 8 ∑ p∈N 1 (q) (cot α p,q + cot β p,q ) f (q) − f (p) 2 ,<label>(2)</label></formula><p>under the condition that the triangles in the 1-ring neighbors are nonobtuse. In case of obtuse triangles, refer to <ref type="bibr" target="#b10">[11]</ref> for solutions. Following this path, we define the discrete conformal factor operator as</p><formula xml:id="formula_2">λ (q) = A g (q) A f (q) ,<label>(3)</label></formula><p>where the A f (q) and A g (q) are the averaging areas for each homotopic vertex q on surface f and g, respectively.</p><p>As conformal surface representation S(u, v) is parameterized by conformal parameters (u, v) on a domain D, the conformal factor function, λ (u, v), and mean curvature function, H(u, v), defined on D satisfy the Gauss and Codazzi equation. If λ (u, v) and H(u, v) are given together with the boundary conditions, S(u, v) can be uniquely reconstructed. Since the mean curvature and the conformal factor are two important attributes, we assign these two attributes to I c (u, v) to construct a vector image I, where the pixel attributes are represented by a vector [H, λ ] . Other features such as normal and texture can be considered if necessary. We use barycentric interpolation for sampling I c (u, v) to I. <ref type="figure" target="#fig_1">Fig. 1</ref> shows the Igea surface model with 5002 vertices ( <ref type="figure" target="#fig_1">Fig. 1(a)</ref>) and its corresponding mean curvature channel ( <ref type="figure" target="#fig_1">Fig. 1(b)</ref>) and conformal factor channel ( <ref type="figure" target="#fig_1">Fig. 1(c)</ref>). The composite shape vector image is shown in <ref type="figure" target="#fig_1">Fig. 1(d)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GEODESIC DISTANCE-WEIGHTED DIFFUSION</head><p>Since a surface can be represented as a unique shape vector image composed of conformal factors and curvatures, many algorithms suitable for image computing may be used for feature extraction from this type of images. For the purpose of matching and visualization of crosssubject data, the main task is to find the stable keypoints or regions and their local image features for alignment. Since the shape vector image representation that we propose consists of the mean curvature and area distortion, it provides important signature of the local geometry, which is transformation invariant and suitable for shape matching. This section describes a novel diffusion-based algorithm to extract distinctive features from the shape vector images. Through the geodesic distanceweighted shape vector image diffusion, we can identify the robust keypoints and their scales from the computed diffusion extrema, which are suitable for the matching purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Shape vector image diffusion and diffusion space</head><p>Our shape vector image is a multichannel image. The simplest way to perform the diffusion filtering of the shape vector image is to deal with each channel separately and independently. However, this method leads to an undesirable effect that edges may be formed at different locations for each channel since the curvature and conformal factor channels must take effect simultaneously in order to accurately determine the local geometry. In our framework, we employs a diffusivity g which combines information from all channels. For a vector image I = (I 1 , I 2 , ..., I m ) , the diffusion is performed by</p><formula xml:id="formula_3">∂ I k ∂t = div g∇I k (k = 1, ..., m),<label>(4)</label></formula><p>where div indicates the divergence operator, ∇ is the gradient operator, and</p><formula xml:id="formula_4">g = 1 1+( x l ) 2 .</formula><p>For the case in which g is a constant for a specific channel I k , it reduces to the isotropic heat diffusion equation, ∂ I k ∂t = c I k , where is the Laplacian operators and the solution is Gaussian smoothing. However, Gaussian smoothing has a typical disadvantage, especially for the shape vector image. Gaussian smoothing does not only reduce noise, but also blurs important geometric features such as sharp edges, hence making them harder to be identified.</p><p>To solve the problem, we propose to perform geodesic distanceweighted inhomogeneous linear diffusion of the shape vector image,</p><formula xml:id="formula_5">∂ I k ∂t = div g ∇ f I k 2 ∇I k ,<label>(5)</label></formula><p>where I k is the actual diffused image, f I k is the original image and g is the diffusivity function. For a specific channel P = I k , the numerical solution for Equation 5 can be computed, similar to the 4-nearestneighbors discretization <ref type="bibr" target="#b18">[19]</ref>, as follows,</p><formula xml:id="formula_6">P t+1 i, j = P t i, j + ρ[c N • N P + c S • S P + c E • E P + c W • W P],<label>(6)</label></formula><p>where 0 ≤ ρ ≤ 1/4, N, S, E, W are the subscripts for the North, South, East, and West, t is the scale, i and j are the indices of the image pixel.</p><p>Since the shape vector image encodes geodesic distance information, the symbol is defined as follows with the consideration of geodesic distances:</p><formula xml:id="formula_7">N P i, j = P i−1, j − P i, j GeoD[N] , S P i, j = P i+1, j − P i, j GeoD[S] , E P i, j = P i, j+1 − P i, j GeoD[E] , W P i, j = P i, j−1 − P i, j GeoD[W ] ,<label>(7)</label></formula><p>where</p><formula xml:id="formula_8">GeoD[N] = GeoD([i − 1, j], [i, j]), GeoD[S] = GeoD([i + 1, j], [i, j]), GeoD[E] = GeoD([i, j + 1], [i, j]), GeoD[W ] = GeoD([i, j − 1], [i, j]),</formula><p>and</p><formula xml:id="formula_9">GeoD([pixelA], [pixelB])</formula><p>is the normalized geodesic distance between the pixelA and pixelB on the manifold, which is normalized by dividing the averaged GeoD over the image. And the c is defined as:</p><formula xml:id="formula_10">c N i, j = g(| N f I i, j |), c S i, j = g(| S f I i, j |), c E i, j = g(| E f I i, j |), c W i, j = g(| W f I i, j |),</formula><p>where</p><formula xml:id="formula_11">N f I i, j , S f I i, j , E f I i, j and W f I i, j</formula><p>are computed by Equation 7. Therefore, the final numerical solution is</p><formula xml:id="formula_12">P t+1 i, j = P t i, j + ρ[ c N • N P + c S • S P GeoD NS + c E • E P + c W • W P GeoD EW ],<label>(8)</label></formula><p>where</p><formula xml:id="formula_13">GeoD NS = GeoD[N] + GeoD[S] 2 , GeoD EW = GeoD[E] + GeoD[W ] 2 .</formula><p>Using this numerical solution, we can construct a discrete geometric diffusion space which encodes the surface geometric information.</p><p>Particularly for our shape vector image scheme, our approach sums up the diffusivity of each channel to a common diffusivity. This may be regarded as collecting the contrast information of all channels. Thus, for a shape vector image I = (I 1 , I 2 , ..., I m ) , the vector diffusion is performed by</p><formula xml:id="formula_14">∂ I k ∂t = div g m ∑ n=1 ∇ f I n 2 ∇I k (k = 1, ..., m).<label>(9)</label></formula><p>By solving Equations 9, we obtain the diffusion space,</p><formula xml:id="formula_15">⎛ ⎜ ⎜ ⎝ I 1 I 2 . . . I m ⎞ ⎟ ⎟ ⎠ = I t 0 I t 1 ... I t n = ⎛ ⎜ ⎜ ⎜ ⎝ I 1 t 0 I 1 t 1 ... I 1 t n I 2 t 0 I 2 t 1 ... I 2 t n . . . . . . . . . I m t 0 I m t 1 ... I m t n ⎞ ⎟ ⎟ ⎟ ⎠ ,<label>(10)</label></formula><p>which is a sequence of shape vector images with t as the scale in a matrix format, i.e., each row of the matrix is the sequence images of a specific channel with t as the scale, and each column of the matrix is the vector image at a specific scale t. <ref type="figure" target="#fig_0">Fig. 2</ref> shows the diffused shape vector images of the Igea model with increasing diffusion scale t. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Properties of diffusion space</head><p>Suppose a mapping f : M → I and the diffusion D : I → S, where M is a 3D surface, I is the shape vector image and S is the diffusion space. In this section, we firstly show that D satisfies the criteria of the multiple scale descriptions. Secondly, we show that f • D, together, creates the diffusion space appropriate for scale-space processing of the 3D surface geometry. The diffusion space construction is to create a multiscale "semantically meaningful" descriptions of images. As we know, a scale-space representation must have the property that no spurious detail will be generated passing from finer to coarser scales. This is so-called causality, which means any feature at a coarse level of resolution is required to possess a "cause" at a finer level of resolution although the reverse need not be true. The causality criterion can be established by showing the used diffusion equation satisfies the maximum principle, that is to say, all the maxima of the solution of the equation in space and time belong to the initial condition (i.e., the original image). A proof of the maximum principle for the diffusion equation can be found in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b15">16]</ref>. Therefore, for the diffusion D, the satisfaction of the maximum principle leads to the satisfaction of the causality for the diffusion space. Consequently, D satisfies the criteria of the multiple scale descriptions.</p><p>For our shape vector image, we use the geodesic distance-weighted method, in which the distance can be retrieved by the mapping, f , for the computation of the diffusion to construct a geometric diffusion space. Therefore, constructing and analyzing the geometric diffusion space is similar to analyzing a direct diffusion space of the 3D surface. As a result, f • D is able to construct a multiple scale space and multiscale descriptions for the 3D surface.</p><p>The geodesic distance-weighted anisotropic diffusion has the advantages of preserving and identifying true features as well as preventing dislocated false features in the diffusion space when tak-ing the actual geodesic distance as a-priori information. The adjacent figure shows an illustrative one-dimensional example, where the curve is the actual shape object and the line segment is the 1D shape "image". The sampling vertices (from v 1 to v 6 ) on the curve are mapped to the pixels (from p 1 to p 6 ) on the 1D shape "image" shown at the bottom. As we can see, the sampling vertices of the curve, v 2 and v 3 , have relatively high curvature values while other sampling vertices have low curvatures. Without considering the geodesic distance among the pixels, p 4 's curvature value will be increased while moving from finer to coarser scales. For example, at the immediately next level, p 3 will boost the diffusion of p 4 since its distance to p 4 is considered the same as the distance between p 4 and p 5 (a unit length). The situation will get worse when diffusing further since p 2 will also dramatically affect the diffusion of p 4 . However, v 2 is far away from the diffusion vertex, v 4 . In fact, p 2 is not suppose to have significant influence on p 4 . On the other side, it is also difficult to preserve the high curvature values for p 2 and p 3 . Therefore, the procedure will dislocate keypoints when moving from finer to coarser scales. So the keypoints detected at a coarse scale do not give the correct location in the original image, which will result in instability and incorrect matching. The situation is much improved when considering geodesic distances in the diffusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Keypoint-based shape descriptors</head><p>The maximum principle states that all the maxima of the solution of the equation in space and time belong to the initial condition (i.e., the original image). Therefore, we propose to detect the extrema across the diffusion space as our keypoints since they are most robust points at the specific scales which are able to represent collectly the original image. We sample the diffusion space by computing a sequence of diffused shape vector images at discrete scales, t. For each diffusion scale, we use Equation 9 to calculate its diffused images which can be expressed in a matrix form like Equation 10.</p><p>In order to extract the cross-scale extrema, we compute the Difference of Diffusion (DoD) using the following vector-based equation,</p><formula xml:id="formula_16">DoD t i = I t i+1 − I t i (i = 0, ..., n − 1).<label>(11)</label></formula><p>Consequently, we can obtain, images to its eight neighbors at the same scale t j and nine corresponding neighboring pixels in each of the neighboring scales t j−1 and t j+1 . If the pixel value is the maximum or minimum among all compared pixels, it is selected as a keypoint. This algorithm is carried out through all the channels of the vector images: DoD i , <ref type="figure" target="#fig_1">(i = 1, ..., m)</ref>. The maxima and minima found in all the channels will be considered as the keypoints. <ref type="figure" target="#fig_3">Fig. 3(a)</ref> and (b) show all the detected keypoints on the Igea model. <ref type="figure" target="#fig_3">Fig. 3</ref>(c-e), (g-i) and (k-m) show the detected extrema (shown with points) on the corresponding DoDs at different scales. <ref type="figure" target="#fig_3">Fig. 3(f)</ref>, (j) and (n) show the scale sizes, at which the extrema in (e), (i) and (m), respectively, are detected, with the corresponding sizes of circles. One valuable point is that the detected keypoints have the associated scales computed by the algorithm, which are very important to construct scale-aware feature descriptors. After localizing the keypoints, feature descriptors are built to characterize these points at the scales where they are identified. These descriptors should contain the necessary distinct information for their corresponding keypoints. In our framework, the descriptor is calculated channel by channel.</p><formula xml:id="formula_17">⎛ ⎜ ⎜ ⎝ DoD 1 DoD 2 . . . DoD m ⎞ ⎟ ⎟ ⎠ = ⎛ ⎜ ⎜ ⎜ ⎝ DoD 1 t 0 DoD 1 t 1 ... DoD 1 t n−1 DoD 2 t 0 DoD 2 t 1 ... DoD 2 t n−1 . . . . . . . . . . . . DoD m t 0 DoD m t 1 ... DoD m t n−1 ⎞ ⎟ ⎟ ⎟ ⎠ .<label>(12)</label></formula><p>For each channel, the local gradient-orientation histograms of the same-scale neighboring pixels of a keypoint are used as the key entries of the descriptor. In this work, we construct a keypoint descriptor with 4 × 4 subdescriptors computed from a 16 × 16 sample pixel array, which is shown on the left side of <ref type="figure" target="#fig_5">Fig. 4</ref>. That is to say, a feature descriptor is computed as a set of orientation histograms on 4 × 4 pixel neighborhoods or subregions. The coordinates of the subdescriptors and the gradient orientations are rotated relative to the keypoint orientation (defined by the gradient vector at the keypoint location) so that it can achieve orientation invariance. One of the subdescriptors is shown on the right panel of <ref type="figure" target="#fig_5">Fig. 4</ref>, which gives eight directions of the orientation histogram with the length of each arrow corresponding to the magnitude of that histogram entry. Since the descriptor is computed with a 4 × 4 array of histograms with 8 orientation bins in each, this leads to a feature vector with 4 × 4 × 8 = 128 elements.</p><p>In the case of an m channel vector image, a keypoint has m descriptors which are combined as a vector, des = [des 1 , des 2 ,...,des m ] , where m is the dimension of the vector image. Hence, the descriptor des of a keypoint in the shape vector image is a m × 128 dimensionbased vector. This descriptor will be used for matching, and all the descriptors computed for all the keypoints form a feature descriptor database, which abstracts the original surface with a small number of robust keypoints and their local descriptors. The robust keypoints and constructed local shape descriptors together are well suitable for the matching purpose as demonstrated by our experiments in Section 5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SHAPE MATCHING AND REGISTRATION</head><p>In our framework, shape matching is to match the keypoints and their associated scale-aware local shape descriptors among different objects. Since the keypoints detected from the diffusion space are very reliable feature points presented in the original surface, matching these keypoints with thin-plate spline deformation will lead to accurate registration of the entire surfaces as well.</p><p>Descriptor matching is performed for a keypoint by comparing the distance from its constructed local descriptor to the descriptor of its closest matched point (DIS CN ) with the distance from the keypoint descriptor to the descriptor of its second-closest matched point (DIS SCN ) found on the to-be-matched object. The distance of two descrip-tors, des1 and des2 which are m dimension vectors, is calculated by, </p><formula xml:id="formula_18">DIS = ∑ m i=1 des1 i − des2 i .</formula><p>If this inequality holds, the points are matched; Otherwise, they are not matched. This inequality ensures that only distinctive keypoints having prominent similarity are matched. Since the 3D data can be coarsely aligned easily through affine transformations during the preprocessing, we can use the uniform subdivision grid to speed up the matching. The Euclidean distance bound (ED) of two potentially matched keypoints is calculated and can be used in efficiently finding the closest and the second-closest matched points within grids. After finding all the matched points, registration can be achieved using thin-plate splines deformation with the matched points as point constraints <ref type="bibr" target="#b31">[32]</ref>. In our shape vector image registration, the keypoints (x i , y i ) are taken as landmarks and V = (x i , y i )| i is a set of the matched keypoints on the other shape vector image. After computing the thinplate splines with the above point constraints, the deformation function f (x, y) = [ f x (x, y), f y (x, y)] can be obtained to map each point (x i , y i ) to its homolog (x i , y i ). The other unconstrained areas will follow the deformation. At the end, we can register the two shape vector images (i.e., the two 3D surfaces).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS AND APPLICATIONS</head><p>To evaluate the proposed approach, we have conducted extensive experiments. We have applied our algorithm on real scanned face models and human neocortex surfaces extracted from high-resolution MR scans. The surface matching is demonstrated first, followed by the application of the framework in the multimodality image analysis and visualization. Our system is implemented with C++ for the computationally intensive algorithms and VTK/OpenGL for rendering and visualization. The experiments are conducted on a Dell Precision Workstation T7400, which has a Xeon CPU with Quad Cores and 4GB RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Repeatability under noise</head><p>We have tested the repeatability of keypoints detection with noises. The Igea surface model is added up to 10% Gaussian noise directly on the mesh. The perturbed surfaces with different noise levels are converted to the shape vector images and then the keypoints are detected with our geodesic distance-weighted diffusion method. We compare the repeatability of the detected keypoints with the ones detected without additive noise. The repeatability result is shown in <ref type="figure" target="#fig_6">Fig. 5</ref>. Compared to the repeatability results by regular anisotropic diffusion method and isotropic diffusion method, our method is much more robust under noise. The main reason is that those two methods have instabilities when moving from finer to coarser scales as described in Section 3.2. They are easier to be affected by noise during the diffusion procedure. Therefore, more keypoints originally detected without noises cannot be repeatedly detected across scales under noisy circumstances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Surface matching</head><p>For scanned face models, we create the shape vector images using conformal mapping. Based on this planar parameter domain, we construct the shape vector image by assigning the H and λ values to each corresponding image pixel. Hence, the shape vector image is a two dimensional vector image [I 1 , I 2 ] , where I 1 = H and I 2 = λ . The geodesic distance is computed and encoded as well. After the shape image is generated, we use the vector diffusion to create the DoD matrix, of which each row is a sequence of images in different scales in each channel. By finding the maxima and minima in each row of the matrix as the keypoints, the descriptor is computed for each point. Each descriptor is a 2*128=256 dimension vector and all these descriptors form the descriptor database. The matching algorithm is performed to find the matched points which satisfies the inequality 13. <ref type="figure" target="#fig_7">Fig. 6</ref> shows the matching result of two faces with different expressions from the same human subject. The average matching accuracy of 10 such experiments on 10 different subjects reaches 95% in terms of correct keypoint correspondence. For the neocortex surface, a genus zero surface, conformal mapping is performed to transfer it to a sphere. The sphere can be mapped to a 2D domain through a reparameterization as follows, σ (θ , ϕ) = (cos θ cos ϕ, cos θ sin ϕ, sin ϕ),</p><formula xml:id="formula_20">DoD 1 DoD 2 = DoD 1 t 0 DoD 1 t 1 ... DoD 1 t n−1 DoD 2 t 0 DoD 2 t 1 ... DoD 2 t n−1<label>(14)</label></formula><p>where θ and ϕ are the rows and columns in the 2D domain image. Then, we follow the same procedure as we use for face models to find the matching keypoints. <ref type="figure" target="#fig_8">Fig. 7(a)</ref> shows the matching result of two different subjects. In order to allow readers clearly see the matched points between the two shape images, only 10% of the matched points are shown in the figure. After matching, we use the matched points as landmarks to register the shape vector images using the thin-plate spline technique. We have conducted the evaluation on intersubject matching of 20 brain surfaces. The results are evaluated quantitatively in terms of major landmark (e.g., the central sulcus, the sylvian fissure, the posterior sulcus) overlaps. <ref type="figure" target="#fig_8">Fig. 7(b)</ref> shows one region that we used to test the registration accuracy, where the green color indicates completely correct overlap while the red color indicates mismatched areas among all the subjects. The average mismatch distance error for total 20 different subjects is only 3.98 mm, which outperforms the latest reported results on inter-subject brain surface registration <ref type="bibr" target="#b3">[4]</ref>. To further show the efficacy of our approach, we have compared our approach with the closely-related methods, anisotropic diffusion method and SIFT. Since SIFT can only work on scalar image, we only input the curvature channel to the SIFT processing. The regular anisotropic diffusion method is applied on both the curvature and conformal factor channels. We randomly select a pair of brain surfaces among 20 subjects. Then, our geodesic distance-weighted shape vector image diffusion method, the regular SIFT method, and the anisotropic diffusion method are performed for matching and registration. The comparison results are shown in <ref type="figure" target="#fig_9">Fig. 8</ref>. The main advantage of our method is to introduce geodesic distance into diffusion space. Therefore, it increases the stability of extrema detection as described in Section 3.2 and the robustness of shape descriptors. The experimental results confirm that the keypoints and constructed local shape descriptors together are very robust features well suitable for the matching purpose. The computational time of the geodesic distanceweighted shape vector image diffusion-based feature extraction and matching is recorded for the tested models in <ref type="table">Table 1</ref>. Note that, the geodesic distance information is pre-computed offline and is not included in the recorded time.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Multimodality Analysis</head><p>The developed framework is ideal for cross-subject analysis and visualization of multimodal brain surface properties. In order to facilitate effective analysis of medical imaging data, especially related to the human neocortical surface, a combination of noninvasive anatomical and functional imaging, such as Magnetic Resonance Imaging (MRI), Diffusion Tensor Imaging (DTI) and Positron Emission Tomography (PET), is frequently used. These modalities provide important, complemental information over the cortex regions. During the preprocessing, a brain surface can be extracted from the MRI volume data. The registration of PET and DTI volumes to the same subject's MRI volume can be done with the mutual information registration algorithm provided in Insight Segmentation and Registration Toolkits (ITK). The registration is easy since the data is for the same subject. In order to integrate PET and MRI data, a normal fusion approach is applied in the native space of the registered MRI and PET volumes of each subject. In this analysis, we choose one normal brain as the template in our framework. All other normal or abnormal individuals are registered to the template shape vector image (SVI) using the methods described in Section 3 and 4. <ref type="figure">Fig. 9</ref> shows the idea and flow of our framework. The last two columns in the figure show the maps of the brain surface PET texture and DTI texture, computed from PET and DTI volumes, which are also registered across subjects because their alignments are already registered to their corresponding MRI volumes during the aforementioned preprocessing. Based on the registered SVIs, PET and DTI maps, statistical analysis of PET and DTI across subjects can be achieved. <ref type="figure">Fig. 9</ref>. The multimodality image analysis pipeline. The referenced brain is used as the template SVI (TSVI), and then all other brain SVIs are registered based on this TSVI. Based on the registered shape vector images, multimodality data such as the PET and DTI, can be integrated over the SVI images to perform the multimodality analysis.</p><p>By comparing a patient's PET texture with a group of normal subjects based upon matched SVIs, we can identify abnormal PET regions which significantly vary from the normal distribution. <ref type="figure" target="#fig_1">Fig. 10(a)</ref> shows two detected abnormal regions on the PET shape image. Because we know the mapping and parameterization, we can easily find out the abnormal regions in the actual brain surface. <ref type="figure" target="#fig_1">Fig. 10(b)</ref> shows the corresponding abnormal regions on the brain PET data. The same scheme can be applied to the population-based DTI analysis. During the preprocessing, the cortico-cortical fiber tracts can be extracted using the brute-force fiber tracking method as shown in <ref type="figure" target="#fig_1">Fig. 10(c)</ref>. Then, these fiber connectivity can be converted to the fiber connectivity strength ratio and plotted in the shape vector image domain to form a DTI texture image. The analysis framework can be used to detect the abnormal regions based on statistical comparison of DTI information between a patient and a group of normal subjects. <ref type="figure" target="#fig_1">Fig. 10(d)</ref> shows a DTI fiber connectivity strength image of an abnormal subject and the detected abnormal region as highlighted with a red contour.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we have presented a novel and accurate surface matching method based on the geodesic distance-weighted diffusion of shape vector images. Through the detected stable keypoints and their local shape descriptors in the diffusion space, our method converts a 3D surface matching problem to a 2D shape vector image matching problem. The robust features facilitate the reliable matching and registration as demonstrated by our experiments. The 2D representation allows easier statistical analysis of other modality features directly computed in the matched 2D domain. The applications to medical image analysis and visualization are demonstrated through multimodality data integration in the 2D domain to support more accurate localization of brain disorder regions using population study. There are some remaining research issues in our current framework. In some situation, when a patch of a surface model containing rich, complex features is squeezed into a very small patch of the domain during the conformal mapping, there may not have enough samples to capture the geometric characteristics with the uniform sampling through the entire domain. This issue may be addressed through controlling the distortion during the mapping or adaptive sampling of the domain according to the distortions and feature richness. Our future work will be focused on this development. Another direction for future work is to apply our framework to very high-genus shapes. In this case, a robust, consistent topological cut algorithm needs to be used to cut and map the high-genus shape to a planar domain. Then, the boundaries of the cuts need to be glued together. Interesting research issues may arise from these boundaries and singular points where diffusion behaviors need to be specially handled. Our future research work will look into these issues.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>m and ds 2 n</head><label>2</label><figDesc>In the theorem of differential geometry, a diffeomorphism f : M → N is conformal if and only if, for any surface patch σ m on M, the first fundamental forms of σ m and σ n = f • σ m are proportional. Mathematically, this means that f • ds 2 m = λ ds 2 n , where λ is called the conformal factor, ds 2 are the first fundamental form on M and N. If M and N are surfaces, a diffeomorphism f : M → N is said to be conformal if , whenever f takes two intersecting curves γ m andγ m on M to curves γ n andγ n on N, the angle of intersection of γ m andγ m is equal to the angle of intersection of γ n andγ n . In short, f is conformal if it preserves angles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Shape Vector Image. (a) shows the Igea (5002 vertices) surface and mesh; (b) and (c) show the mean curvature channel and conformal factor channel of the shape vector image representation of the Igea model; (d) is the composite shape vector image including both channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>The diffused shape vector images, consisting both curvature and conformal factor channels, of the Igea model at different diffusion scales, t, computed by the geodesic distance-weighted diffusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 (</head><label>3</label><figDesc>c-e), (g-i), and (k-m) show the computed intermediate curvature channel images of the DoDs across scales. Once DoD vector images have been obtained, keypoints are identified as local minima/maxima of the DoD images across scales t. For each channel DoD i , it is done by comparing each pixel in the DoD t j i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Keypoint detection in the diffusion space. (a) The Igea model with all the detected keypoints at different scales indicated by the points of different colors and sizes. (b) All the detected keypoints shown on the curvature channel of the shape vector image. (c-e), (g-i) and (k-m) show the intermediate curvature channel images of the DoDs across scales t and the detected extrema (shown by points) on the corresponding DoDs at different scales. (f), (j) and (n) show again the extrema detected at (e), (i) and (m), respectively, with the different sizes of circles indicating the sizes of scales at which these extrema are detected.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>A keypoint descriptor is generated by computing the gradient magnitude and orientation at every pixel around the keypoint (16 × 16 sample pixels). These samples are then accumulated into orientation histograms summarizing the contents over 4 × 4 subregions, indicated with thicker framed boxes. The right panel shows one subregion with the length of each arrow corresponding to the sum of the gradient magnitudes along that direction within the region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Repeatability of keypoint features when the Igea model is under different Gaussian noise levels. The left panel shows the Igea models (with the computed curvature colormaps) with 4% and 10% additive Gaussian noise and their corresponding shape vector images. The detected keypoints are shown in the shape vector images. The right panel shows the repeatability of the feature points extracted by our geodesic distance-weighted shape vector image diffusion method. The comparison to the conventional anisotropic and isotropic diffusion methods is demonstrated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Matching of face models with different expressions from the same subject. The left panel shows all the matched keypoints between the two surfaces. The right panel shows the scales of the matched keypoints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Matching of two different subjects' brain surfaces. (a) 10% of matched points are shown using the linked lines. (b) The overlap test on one registered brain region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>The experimental results on matching arbitrary two different brain surfaces randomly selected from 20 subjects. The comparison shows that our method constantly outperforms the regular anisotropic diffusion method and the SIFT method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Population-based PET and DTI image analysis. (a) The regions within the black contours are the detected abnormal regions in the PET texture image; (b) The regions in black are the corresponding abnormalities on the individual's brain cortical surface. (c) shows a 3D rendering of a normal DTI fiber connectivity. (d) shows the abnormal DTI map where the abnormality is contoured in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and M. Dong are with Wayne State University. Email: {jinghua|kevinlai|mdong}@wayne.edu • X. Gu and H. Qin are with Stony Brook University. Email: {gu|qin}@cs.sunysb.edu. • Correspondence to Prof. J. Hua. Manuscript received 31 March 2008; accepted 1 August 2008; posted online 19 October 2008; mailed on 13 October 2008. For information on obtaining reprints of this article, please send e-mailto:tvcg@computer.org.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Once the DIS CN and the DIS SCN are found, the DIS CN and the DIS SCN are compared to decide whether they are matched or not. The judge function for the comparison is threshold × DIS CN &lt;= DIS SCN .</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work is supported in part by the research grants: NSF IIS-0713315, NSF CNS-0751045, NIH 1R01NS058802-01A2, NIH 2R01NS041922-05A1, MTTC 05-135/GR686, MTTC 05-154/GR705, and MEDC 06-1-P1-0193.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">CSIFT: A SIFT descriptor with color invariant characteristics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdel-Hakim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1978" to="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Boostmap: a method for efficient approximate similarity rankings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Athitsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kollios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="268" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recognition using region correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="141" to="162" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generalized surface flows for deformable registration and cortical matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Leahy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Desbrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<meeting>International Conference on Medical Image Computing and Computer-Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="692" to="697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Localizing overlapping parts by searching the interpretation tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grimson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lozano-Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="469" to="482" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Geometry images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gortler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="355" to="361" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Genus zero surface conformal mapping and its application to brain surface mappings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Yau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="949" to="958" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Least squares conformal maps for automatic texture atlas generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petitjean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Maillot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="362" to="371" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Three-dimensional object recognition from single twodimensional images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="355" to="395" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discrete differential geometry operators for triangulated 2-manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Desbrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schroder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Visualization and Mathematics</title>
		<meeting>International Conference on Visualization and Mathematics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="35" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A performance evaluation of local descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="257" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient shape matching using shape contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1832" to="1837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visual learning and recognition of 3-D objects from appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Murase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="24" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large-scale tests of a keyed, appearancebased 3-D object recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Selinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="2469" to="88" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A strong maximum principle for parabolic equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nirenbarg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications on Pure and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="177" />
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Detectability, uniqueness, and reliability of eigen windows for stable verification of partially occluded objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ohba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ikeuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1043" to="1091" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Shape distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Osada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chazelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dobkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="807" to="832" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scale space and edge detection using anisotropic diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="628" to="639" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A new paradigm for recognizing 3-D object shapes from range data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruiz-Correa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1126" to="1133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Object recognition using multidimensional receptive field histogram</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>James</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th European Conference on Computer Vision</title>
		<meeting>the 4th European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="610" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Local grayvalue invriants for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mohr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="530" to="534" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">2D-shape analysis using conformal mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mumford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="350" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Color indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Swain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="32" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Curvature-based representation of objects from range data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vemuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mitiche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="114" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Conformal geometry and its applications on 3D shape matching, recognition and stitching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1209" to="1220" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">High resolution tracking of non-rigid 3D motion of densely sampled data using harmonic maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="388" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adaptive geometry image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y.</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="948" to="960" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Harmonic maps and their applications in surface matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="524" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A robust technique for matching two uncalibrated images through the recovery of the unknown epipolar geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Deriche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Faugeras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="87" to="119" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An approach for intersubject analysis of 3d brain images based on conformal geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Muzik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Image Processing</title>
		<meeting>International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1193" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Non-rigid surface registration using spherical thin-plate splines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Muzik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<meeting>International Conference on Medical Image Computing and Computer-Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="367" to="374" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
