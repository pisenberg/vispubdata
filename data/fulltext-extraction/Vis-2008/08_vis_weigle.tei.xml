<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Comparison of the Perceptual Benefits of Linear Perspective and Physically-Based Illumination for Display of Dense 3D Streamtubes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2008-10-19">19 October 2008</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Weigle</surname></persName>
							<email>weigle@eecs.utk.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">C</forename><surname>Banks</surname></persName>
							<email>dbanks@eecs.utk.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UT/ORNL Joint Institiute for Computational Sciences</orgName>
								<orgName type="department" key="dep2">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">University of Tennessee</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">UT/ORNL Joint Institiute for Computational Sciences</orgName>
								<orgName type="department" key="dep2">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">University of Tennessee</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Comparison of the Perceptual Benefits of Linear Perspective and Physically-Based Illumination for Display of Dense 3D Streamtubes</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2008-10-19">19 October 2008</date>
						</imprint>
					</monogr>
					<note type="submission">received 31 March 2008; accepted 1 August 2008; posted online</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-20T19:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>user study</term>
					<term>volume completion</term>
					<term>3D shape perception</term>
					<term>physically-based illumination</term>
					<term>global illumination</term>
					<term>local illumination</term>
					<term>multi-scale visualization</term>
					<term>flow visualization</term>
					<term>streamtubes</term>
					<term>DT-MRI</term>
					<term>white matter tractography</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Large datasets typically contain coarse features comprised of finer sub-features. Even if the shapes of the small structures are evident in a 3D display, the aggregate shapes they suggest may not be easily inferred. From previous studies in shape perception, the evidence has not been clear whether physically-based illumination confers any advantage over local illumination for understanding scenes that arise in visualization of large data sets that contain features at two distinct scales. In this paper we show that physicallybased illumination can improve the perception for some static scenes of complex 3D geometry from flow fields. We perform humansubjects experiments to quantify the effect of physically-based illumination on participant performance for two tasks: selecting the closer of two streamtubes from a field of tubes, and identifying the shape of the domain of a flow field over different densities of tubes. We find that physically-based illumination influences participant performance as strongly as perspective projection, suggesting that physically-based illumination is indeed a strong cue to the layout of complex scenes. We also find that increasing the density of tubes for the shape identification task improved participant performance under physically-based illumination but not under the traditional hardware-accelerated illumination model.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Large datasets typically contain features at multiple scales. In fact, these large-scale features are generally comprised of small-scale subfeatures. Visualizations of large datasets face the challenge of providing cues to the shape and layout of features across scale. One area of visualization rife with multiscale features is display of 3D flow.</p><p>A desirable goal for visualization of multiscale data is that the user can, within a single image, attend to the full range of feature scales, from large to small. For a flow field sampled by streamtubes the tubes themselves are the small-scale geometric detail, and their emergent structure provides the larger scale. A hardware-accelerated "local" model of illumination (e.g. as implemented in OpenGL) enables perception of the shape of the individual tubes, and perspective and occlusion provide cues to relative depth that can be used in perceiving larger scale features of the flow. Although a full model of light transport clearly provides still more visual information (at least some of which is known to be processed by the human visual system <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8]</ref>), there is no consensus in the visual-perception literature of how significant these cues are for shape inference. While phenomena captured by physical light transport (e.g. shadows and interreflections) clearly depend on the scene layout, it is not clear whether scene perception in turn depends on these phenomena.</p><p>Recently Melek et al. <ref type="bibr" target="#b15">[16]</ref> described a display technique for dense volumes of thread-like data that uses a local illumination model augmented with a shadowing pass; individual fibers near the domain boundaries are clearly visible, and the added shadows provide cues to the density of the fiber mass and cues to the relative depth of individual fibers toward the core of the domain. Although shadows have received significant study in the psychology literature, the majority of psychological research treats shadows as an optional phenomenon, and many higher-order effects (e.g. interreflections) have received only limited study <ref type="bibr" target="#b13">[14]</ref>. We do not intend to parcel out each phenomenological outcome of light transport for individual study; instead we will approach physically-based illumination holistically. These phenomena are inherent in light transport, they have physical meaning, and they are processed by the human visual system. The question we address in this work is whether physically-based illumination (taken as a whole) influences the perception of a scene compared to local illumination, and whether that effect improves task performance.</p><p>In this paper, we show the effect of physically-based illumination on perception for complex 3D flow-field geometry in static scenes. We perform two human-subjects experiments to compare the traditional hardware-accelerated local illumination model with a physically-based illumination model. The first experiment asks participants to identify the closer of two streamtubes selected from a field of tubes. The second experiment asks participants to identify the shape of the flow-field domain; we additionally manipulate the density at which streamtubes sample the flow field. Both experiments include the separate manipulation of the projection model (orthographic or perspective), providing a known perceptual cue as a benchmark for effect size.</p><p>The remainder of this paper is organized as follows. Section 2 discusses related research in visual perception. Section 3 motivates the particular tasks with which we explore the effects of physically-based illumination. Sections 4 and 5 describe our human-subjects experiments and their results. Finally, Section 6 states the conclusions of our work and their significance for visualizations of large datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PREVIOUS WORK 2.1 Perspective Projection</head><p>Within the many visualization communities, the prevalence of linear perspective for data display varies. For instance, in visualizations for medical diagnosis, one might choose not to incorporate perspective projection to facilitate rapid measurement of lesions with physical tools (e.g. calipers). In visualizations of some scientific data, the use of linear perspective may be so ingrained as to not be questioned.</p><p>Studies have compared the utility of 2D (orthographic projection) displays versus 3D (perspective projection) displays. Tory et al. examined the perception of 2D and 3D displays for orientation and relativeposition tasks, finding that 3D displays were best suited for relativeposition tasks <ref type="bibr" target="#b25">[26]</ref>. St. John et al. found that 3D displays provided for better shape-understanding and scene-layout performance <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Shape from Shading</head><p>The perception of shape from shading (or reflectance patterns) in the human visual system has been shown to have a number of biases (e.g. overhead). Ramachandran found that the perception of shape from shading is biased toward a single luminaire <ref type="bibr" target="#b19">[20]</ref>. <ref type="bibr">Langer et al.</ref> found that under diffuse lighting, the human visual system has a bias toward interpreting dark regions as "deep" or recessed <ref type="bibr" target="#b12">[13]</ref>.</p><p>Caniard et al. show that shape from shading under single pointsource local illumination is very sensitive to light source position <ref type="bibr" target="#b3">[4]</ref>. Indeed, more complicated lighting systems have been shown to provide enhanced shape perception of surfaces <ref type="bibr" target="#b9">[10]</ref>. Caniard suggests that improving the physical realism of the renderings, perhaps with global illumination, may help. However, Caniard also points out that some surface materials (i.e. reflectance models) may not benefit perceptually, as they may still present position-dependent reflectance patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Physically-based Illumination</head><p>Local illumination can be understood as a solution to the usual lighttransport equation using vector-valued light <ref type="bibr" target="#b1">[2]</ref>. This enables us to compare the perceptual effects that result from solving a single transport equation using two different initial conditions, rather than comparing algorithms or phenomenological effects like shadows or highlights that result from solving the governing equations.</p><p>Physically-based illumination as a whole has received little study in perception or visualization literature. <ref type="bibr">Madison et al.</ref> found that phenomenological effects of physically-based rendering, specifically cast shadows and diffuse interreflections, provide perceptual cues to the proximity between surfaces <ref type="bibr" target="#b13">[14]</ref>. However, the cast shadow, one of the most obvious phenomenological effects of physically-based illumination, has been the subject of numerous psycho-visual studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Cast Shadows</head><p>It is known that phenomenological shadows are processed by the visual system <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21]</ref>, however there exists conflicting evidence as to whether shadows are actually used as a perceptual cue. Jacobson et al. and Sinha found that preattentive processes are not sensitive to inconsistencies in shadow direction during visual search tasks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24]</ref>. Elder et al. found evidence of rapid discrimination of objects based on shadow properties (e.g. cast versus attached shadows, shadow direction, and shadow displacement) <ref type="bibr" target="#b7">[8]</ref>. Castiello found that shadows incongruent with the casting object or the direction of illumination take longer to identify <ref type="bibr" target="#b4">[5]</ref>, while Bonfiglioli et al. found no evidence of an effect in identifying objects with false shadows <ref type="bibr" target="#b2">[3]</ref>.</p><p>Several studies show that higher-level processes are indeed influenced by shadows. Bonfiglioli et al. found a distractor-like effect (consistent with difficulty identifying object affordances) when grasping for objects with incongruent shadows <ref type="bibr" target="#b2">[3]</ref>. Shadows have also been found to contribute to scene perception. Cavanagh et al. found shadows provide a strong cue to the direction of the light source <ref type="bibr" target="#b6">[7]</ref>. Mamassian et al. and Ni et al. both found shadows to provide perceptual cues to the layout of objects in a scene <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Perception at Multiple Scales</head><p>Scene layout is one area of perception where features at one scale combine to produce features at a larger scale. Numerous cues to relative depth, including the direction of light sources and cast shadows, combine to provide information about spatial relationships within the scene. Thus perception at small scales contributes to perception at a larger scale. Another way in which perception transitions from smallscale features to large is in volume completion. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates 3 types of visual completion. Modal completion is the perceptual "filling-in" of object boundaries with illusory contours (typically due to nearby high-contrast edges). Amodal completion is the perception that incomplete object boundaries are occluded by some intermediate object. Volume completion (c) predicts the perception of volumetric shape that cannot be fully explained by either modal (a) or amodal completion (b) <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. Modal and amodal completion are largely 2D phenomena, and studies have shown that under the right conditions, a stimulus may be perceived as either modally or amodally completing, each case leading to a different interpretation of the stimulus <ref type="bibr" target="#b22">[23]</ref>.</p><p>Tse suggests that the percept of object completion is more complicated than 2D imagery and further demonstrates 3D volumetric shapes  that are readily completed but cannot be explained by either modal or amodal processes alone. The example of <ref type="figure" target="#fig_0">Figure 1</ref>(c), though only one example of volume completion, is of particular interest to our current work as it combines small-scale features (the spheres) into the perception of a larger-scale volumetric shape (the sweep of spheres around the cylinder). We are led to question how the density of the small-scale features affects the perception of volume completion and whether physically-based illumination has any influence on the perception of their aggregate shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MOTIVATION FOR EXPERIMENTAL DESIGN</head><p>This study was motivated in part by recent work to display white matter fiber tracts educed from diffusion tensor magnetic resonance imaging (DT-MRI) of the brain. Often these tracts are constructed as stream tubes from the principal eigenvector field of the tensor-valued data. The current state of the practice is to display the resulting tubes using local illumination, as in <ref type="figure" target="#fig_1">Figure 2</ref>(a). We wondered whether this choice resulted from deliberation or convenience, and whether physicallybased illumination would improve perception of shape. In a clinical setting, a 3D visualization of fiber tracts might be used to answer the following questions. Which bundle (fascicle) does a fiber belong to? Is there a lesion? How is it situated with respect to the fibers? Which fibers will be cut in order to remove a tumor?</p><p>The surgeons and radiologists who viewed 3D visualizations of fibers with physically-based and with local illumination ( <ref type="figure" target="#fig_1">Figure 2</ref>) uniformly agreed that physically-based illumination made the resulting image look "better" or "more real," <ref type="bibr" target="#b0">[1]</ref> but did not express confidence that it would improve speed or quality of diagnosis or surgical planning. It is an open question whether physically-based illumination confers clinically relevant value on the perception of such 3D scenes.</p><p>Conducting user studies with clinically-trained specialists involving clinical data poses a significant problem for generalizing the results.</p><p>These specialists attend to clinical stimuli using specialized cognitive processes that could result in confounding factors. For example, certain cues that are evident to a clinician are the result of years of training: evidence suggests that a radiologist's brain responds differently than a layman's to a medical scan <ref type="bibr" target="#b10">[11]</ref>. For our studies, we designed experiments that would test the response of non-specialists when viewing 3D scenes that are similar to tractography, but that would not require knowledge of neuroanatomy or medicine. We were thus motivated to ask shape-perception questions that a layman could answer rather than questions designed for a medical expert. The two questions we chose are, however, clinically motivated: (1) which tube is closer, and <ref type="formula">2</ref>what is the aggregate shape of neighboring tubes?</p><p>Question 1 is motivated by the clinical task of determining where, in the 3D volume of the brain, a particular fiber is situated. Question 2 is motivated by the task of determining what fascicles are present in a display of fiber tracts.</p><p>We wanted to compare the effect of physically-based illumination to the effect of some known shape-perception cue. We chose linear perspective because in most visualization tools leveraging hardwareaccelerated local illumination, perspective is the primary cue to the relative depth of objects in a scene, however many medical visualiation tools forego perspective to facilitate faster physical measurement of critical structures. Visualization of DT-MRI tractograpy is a sufficiently new endeavor such that no de facto standard has been adopted.</p><p>For question 2, we wished to produce distinct volumetric shapes that might look similar in the absence of perspective projection (i.e. orthographic projection) but distinct in its presence. Therefore, we desired volumetric shapes with similar silhouettes under orthographic projection, but apparent differences under perspective projection. Faceted shapes meet this criterion; we use one such a set of volumetric shapes in Experiment 2, described in (Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT 1</head><p>In the first experiment, we address the impact of physically-based illumination on perception of small-scale features in complex datasets. We do so by asking users to focus their attention on individual streamtubes in a noisy flow field with a dominant orientation of flow.</p><p>Participants are asked to identify the nearer tube (selected from a pair of targets) in a field of distractors. Target A is nearer than target B if any point on target A is nearer than every point on target B. Stimuli differed in the distance between targets, target-to-camera distance, projection model, and illumination model. Distractors provide the shadows and inter-reflections that are the phenomenological contribution of physically-based illumination. We hypothesized that, in the absence of perspective-depth cues, physically-based illumination would improve scene-layout perception (identification of the nearer target). We also hypothesized that, in the presence of perspective, physically-based illumination would improve scene-layout perception.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Methods</head><p>Two tubes in a volume of 50 uniform radius tubes are indicated (by color) as targets. Participants are tasked with identifying the nearer of the two targets. Scenes are rendered with global or local illumination (see <ref type="bibr">Beason et al.</ref> for the exact illumination models <ref type="bibr" target="#b1">[2]</ref>) and with orthographic or perspective projection. The 60x60x60 (tube radii) volume of tubes is placed 45 tube radii from the participant's point of view. A single area light is above, behind, and to the right of the point of view. Analysis will show the influence of these rendering conditions on scene perception as measured by task performance. <ref type="figure" target="#fig_2">Figure 3</ref> shows a scene in 4 of its 8 possible stimulus conditions (not shown are the 4 versions of the scene with the target colors reversed); the green tube is approximately 4 tube radii closer to the viewer. We constructed scenes containing 50 random tubes of uniform radius (r = 1). Each tube centerline follows an integral path through a 3D vector field. The vector field is constructed as the weighted sum of a uniform flow (along the x-axis) summed with 3D Perlin noise (8 octaves, 0.2 persistence) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. Each tube is grown from a random seed point within the flow field via forward and backward Euler integration. A growing tube is terminated if its centerline would pass within 10 radii of another tube, if it has reached the maximum desired length, or if it would pass outside the boundary of the flow field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Stimuli</head><p>Two tubes were pseudo-randomly selected from the 50 tubes in each scene to serve as targets. The selection of target tubes in each scene was constrained such that one target would not occlude the other as seen from the camera position (for both orthographic and perspective projection), and targets were not near the boundary of the flow volume (so each would receive cast shadows from the distractors under physically-based illumination). One selected tube was identified by the color red, and the other selected tube was identified by the color green. All distractor tubes were neutral gray.</p><p>We constructed 16 unique scenes, each from a different random vector field and each with different target tubes. Each target tube has some point with a minimum distance to the point of view (center of projection) of the participant. We call this point the target's closest approach, denoted as CA(X) for a target tube X. Scenes were used once each for both combinations of red/green colorings of the two targets. Scenes were rendered with global or local illumination combined with perspective or orthographic illumination, for a total of 4 rendering conditions. A complete session consisted of 128 stimuli, typically lasting 15 minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Participants</head><p>Participants (N = 5, 1 female) for this study all had corrected-tonormal visual acuity and normal color sensitivity. One participant was left handed. Only one participant had significant experience with human subjects experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Setup and Procedure</head><p>The stimuli were displayed on a 17" LCD monitor at 1280 × 1024 resolution. Participants sat an average of 65 cm away from the monitor, with stimuli subtending approximately 28 • ×23 • of visual angle. Each stimulus would fade in and out (through black) over 100 ms.</p><p>Stimuli were presented in random order. Participants were allowed unlimited time to examine each stimulus, but on average participants responded in less than 5 seconds. Participants indicated their response via the keyboard, pressing 'R' if they perceived the red tube to be closer or 'G' if they perceived the green tube to be closer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>The 5 sessions, at 128 trials each, yielded 640 total trials.  <ref type="figure" target="#fig_4">Figure 4</ref> shows the proportion of correct responses for the different levels of the two display models, projection and illumination, and their 95% confidence intervals. The only statistically significant interaction effect found was between difference in closest approach and projection model (F = 61.59, D f = 15, p 0.001) -as one might expect, the proportion of correct responses does not change statistically significantly as the difference in closest approach between targets increases under orthographic projection, while the proportion increases as the difference increases under perspective.</p><p>A Tukey Honestly Separable Differences (HSD) test comparing the effects of a combined display condition (projection and illumination models together, see <ref type="figure">Figure 5</ref>) shows that participants gave the most correct responses for stimuli using perspective projection with global illumination (84%). Participants gave the fewest correct responses for stimuli using orthographic projection with local illumination (56%), and the participants' responses were not statistically distinguishable from chance (50%) in this case. Global illumination and orthographic projection together (63%) can not be said to be a statistically significant improvement over orthographic projection and local illumination, however performance is clearly better than chance. Perspective projection and local illumination provided further improvement (73%) over global and orthographic, but can not be said to provide statistically significantly better performance.</p><p>We performed ANOVA tests (with the same factors as for participant response) to predict response time. We found statistically significant effects for subject id (F = Percent Correct <ref type="figure">Figure 5</ref>. The interaction effect between projection and illumination models, with 95% confidence intervals. Bars sharing a common color are in the same statistical group according to Tukey HSD. Combining perspective with global illumination produced the best performance.</p><p>der perspective response times decreased statistically significantly as the difference in closest approach increased (from 4.4 s to 2.1 s). A Tukey HSD test comparing the effects of a combined display condition (projection and illumination models together, see <ref type="figure">Figure 6</ref>) shows that participants responded fastest under perspective projection and global illumination (approximately 3.3 s), and slowest under orthographic projection and global illumination (approximately 5.5 s). From <ref type="figure">Figure 6</ref> we see that there is little statistically significant difference between all participant responses or only correct responses. When considering all responses, a Tukey HSD groups performance in two categories determined by projection model. If considering only correct responses, a Tukey HSD can not determine to which group orthographic projection with local illumination belongs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussion</head><p>For correct responses (see <ref type="figure" target="#fig_4">Figure 4)</ref>, we find that illumination model does affect perception -participants gave 9% more correct responses under global illumination (73.8%) than under local illumination (64.7%). However, the difference in correct responses between local and global illumination is smaller than that between orthographic (60%) and perspective projection (78.4%). For this task, projection model is a stronger influence on participant response (an 18% difference). We also find that global illumination and perspective projection reinforce each other, attaining the most correct responses from participants (83.1%, see <ref type="figure">Figure 5</ref>). Note that orthographic projection with local illumination (55.6% correct) is not statistically significantly dif- ferent from chance (p &gt; 0.26); this combination of models provides only occlusion as a strong cue to relative depth between objects. <ref type="figure">Figure 7</ref> shows logistic regression models for participant response for each combined display condition versus the difference in closest approach between the two target tubes. We see that display conditions using perspective reach 100% correct responses (according to the model fit), while conditions using orthographic projection do not. In fact, the combination of local illumination and orthographic projection is not statistically separable from chance over the full range. Global illumination displays consistently outperform local illumination displays over large enough differences in target closest approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENT 2</head><p>In the second experiment, we addressed the impact of physically-based illumination on perception of large-scale features in complex datasets. We did so by asking users to focus their attention on the domain of a random vector field for different streamtube densities.</p><p>Participants were asked to identify the apparent shape of a volume containing uniform-radius tubes. Stimuli differed in apparent volume shape, density of tube packing, projection model, and illumination model. The volumes had the same silhouette under orthographic projection. We hypothesized that as the volume density decreased, participants would find perception of scene layout (identification of the apparent volume) more difficult. We also hypothesized that perspective projection would produce improved task performance over orthographic projection, and that physically-based illumination would produce improved task performance over local illumination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Methods</head><p>Participants were presented a series of scenes of illuminated tubes packed in a volume and tasked with identifying the apparent shape of the volume. Scenes were rendered with global or local illumination, and with orthographic or perspective projection. Scenes also differed in the density of the tube packing. Analysis shows the effect of these parameters on scene perception as measured by task performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Stimuli</head><p>We constructed scenes containing one of the 3 volume shapes packed with random tubes of uniform radius. As in Experiment 1, the tubes were computed as integral paths through a 3D vector field. However, in this experiment the vector field was produced from Perlin noise alone (8 octaves, 0.2 persistence). Integrating the random flow produced tubes that conformed to the shape of the volumes without producing artifacts (clipping, thinning) that would distinguish each volume. Termination conditions for the tubes were similar to Experiment 1 (distance to other tubes, contained within the volume) with the additional constraint that tubes were to be 50 ± 5 radii in arclength (mean  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>49.97)</head><p>. Five random vector fields were produced -one was reserved for training stimuli, and the others were used as trial stimuli.</p><p>As illustrated in <ref type="figure" target="#fig_8">Figure 8</ref>, there were 3 volumes used in Experiment 2. The volumes presented the same silhouette in all scenes when viewed with orthographic projection. However the visible faces of each volume were quite different. The first presented 3 rectangular faces, as if viewing a cube corner-on. The second presented 6 triangular faces, as if viewing an icosahedron edge-on. The third presented a single hexagonal face, as if viewing a hexagonal prism.</p><p>The densities at which the tubes fill each volume were varied among 5 levels. Densities were controlled indirectly by varying the minimum distance between tubes to produce a desired average pixel coverage under orthographic projection. The inter-tube distance ranged from 2 to 7.5 radii, equally spaced over 5 levels.</p><p>Once the desired tube densities were found the scenes were rendered 4 ways, selecting local or global illumination with orthographic or perspective projection <ref type="figure" target="#fig_9">(Figure 9</ref>). This was repeated over the 5 densities ( <ref type="figure" target="#fig_0">Figure 10</ref>). The 4 display conditions combined with 5 density levels, 3 shapes, and 4 random flow fields produced 240 trial stimuli. Volume shapes (with a maximum extent of 56 tube radii) are placed 50 tube radii from the participant's point of view. Two area lights were used. A key light was placed above, behind, and to the right of the virtual camera. A fill light of equal size was placed 60 degrees off the key light (in the plane defined by the center of the key light, the point of view, and the view direction) below, behind, and to the left of the virtual camera. The key light was 8 times brighter than the fill light. <ref type="figure" target="#fig_0">Figure 10</ref>. The volumes shapes in each display condition at each density. The shapes differ by row (top: cube, middle: icosahedron, bottom: hexagonal prism). Each quadrant is rendered with a different display condition, arranged as in <ref type="figure" target="#fig_9">Figure 9</ref>. Density decreases to the right.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Participants</head><p>Participants (N = 12, 4 females) for this study had normal or correctedto-normal visual acuity. Three participants reported being left hand dominant, though only one participant chose to use the computer mouse left-handed. Only one participant had significant experience with human subjects experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Setup and Procedure</head><p>Participants were given a short training exercise (12 stimuli, unique from the trial stimuli) that introduced the experiment graphical user interface (GUI) and task. The 3 volumes were displayed under a pseudorandom subset of the different rendering conditions (local or global illumination, orthographic or perspective projection) and the 5 different densities. Participants were presented 6 of the training stimuli in the exact manner of the real session except that the correct answer was indicated after the participant responded. Participants were allowed to ask questions during the training session.</p><p>The stimuli were displayed on a 17" LCD monitor at 1280 × 1024 resolution. Participants were seated an average of 66 cm away from the monitor, with the stimuli subtending approximately 28 • ×23 • of visual angle. Stimuli were presented in random order. Each stimulus would fade in and out (through gray) over 250 ms, and the stimulus itself remained visible for 2 seconds. After each stimulus, an answer key image was presented depicting the 3 apparent volumes. Participants indicated their response (identifying the shape of the volume depicted in the previous stimulus) by clicking a button in the GUI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>The 12 sessions, at 240 trials each, yielded 2880 total trials. We performed ANOVA for a model including subject id, projection model, illumination model, volume density, and volume shape to predict correct participant response. Subject id (F = 0.83, D f = 11, p &gt; 0.60) was not found to be a statistically significant factor. Projection model (F = 48.10, D f = 1, p 0.001), illumination model (F = 21.67, D f = 1, p 0.001), volume density (F = 7.91, D f = 4, p &lt; 0.001), and volume shape (F = 18.04, D f = 2, p &lt; 0.001) were all found to be statistically significant factors. <ref type="figure" target="#fig_0">Figure 11</ref> shows the proportion of correct participant responses, and their 95% confidence intervals, for illumination and projection models. In terms of correct responses, projection model has a similar effect size (orthographic: 46.6%, perspective: 58.8%) to global illumination (local: 48.6%, global: 56.8%), producing differences of about 10%. Interaction effects were found for illumination model and volume density (F = 4.08, D f = 4, p &lt; 0.01), volume shape and projection model (F = 52.28, D f = 2, p 0.001), volume shape and illumination model (F = 70.66, D f = 2, p 0.001), and volume shape and density (F = 2.66, D f = 8, p &lt; 0.01).</p><p>A Tukey HSD comparing the effects of combined display condition (projection model and illumination model) shows that participants  <ref type="figure" target="#fig_0">Figure 11</ref>. Illumination and projection models were found to have a significant effect on participant response (shown, means and 95% confidence intervals). Perspective (versus orthographic) projection and global (versus local) illumination each enabled better performance.  <ref type="figure" target="#fig_0">Figure 12</ref>. Interaction between illumination and projection models (means and 95% confidence intervals shown; color indicates Tukey HSD groups). Participants perform the best for stimuli rendered with perspective projection. Global illumination with perspective projection enables the most correct participant responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interaction of Display Models</head><p>gave the fewest correct responses for orthographic projection with local illumination at 41% ( <ref type="figure" target="#fig_0">Figure 12</ref>). Participants gave the most correct responses for perspective with global illumination (61.4%), but not statistically significantly more than for perspective with local illumination (56.3%). Orthographic projection with global illumination (52.2%) falls below perspective projection with either local or global illumination but above orthographic projection with local illumination. A Tukey HSD comparing the effects of density shows that participants were statistically best at recognizing volume shapes at the top two densities (each with p &lt; 0.01 versus the lowest 3 densities). The performance difference between the top two densities only approaches, but does not achieve, statistical significance (p &gt; 0.07). Looking at the interaction between illumination model and density, shown in <ref type="figure" target="#fig_0">Figure 13</ref>, we see that performance improves statistically significantly with increased density under global illumination (p &lt; 0.01), but not under local illumination.</p><p>When we consider the volume shapes individually, we see a different picture. The interaction effects between volume shape and the other model factors <ref type="figure" target="#fig_0">(Figure 14)</ref> suggest that different shapes might have different optimal display conditions (within this experiment's parameters). However, if participants frequently respond 'hexagon' when confronted with a stimulus they cannot readily identify, then the data should contain two things: 1) a disproportionately high number of 'hexagon' responses, and 2) a high false positive rate for the same.</p><p>From the confusion matrices for local <ref type="table" target="#tab_2">(Table 1</ref>) and global illumina-  <ref type="figure" target="#fig_0">Figure 14</ref>. Interaction between volume shape and illumination model (means and 95% confidence intervals). Top: under orthographic projection; Bottom: under perspective projection. The hexagonal slab appears to cause a reversal of effect for illumination model. tion ( <ref type="table">Table 2)</ref> we see that the number of 'hexagon' responses is lower in general for global illumination (367 times, or 26%) than for local illumination (730 times, or 51%). Also, the 'hexagon' responses are more concentrated on the hexagonal-prism stimuli under global illumination (155 false positives, or 42%) than for local illumination (416 false positives, or 57%). A similar, although smaller, improvement is found moving from orthographic to perspective projection, with the 'hexagon' false positives improving from 55% with orthographic projection to 49% with perspective. One could surmise that participants judged that stimuli typically looked like hexagonal slabs under local illumination but not so under global illumination.`````````r esponse stimulus cube icosa hexa total <ref type="table" target="#tab_2">cube  224  72  74  370  icosa  86  162  92  340  hexa  170  246  314  730  total  480  480  480</ref> 1440  <ref type="table">Table 2</ref>. Confusion matrix (with row and column totals) for global illumination data. Participants respond 'hexagon' just over 25% of the time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Discussion</head><p>We find that the illumination model does affect perception -participants gave more correct responses under global illumination (57%) than under local illumination (49%). Statistically, we cannot differentiate between the effect sizes for illumination model and projection model (p &gt; 0.12). Global illumination plus perspective projection receives more correct responses (61%) than the other displays. However, unlike Experiment 1, the performance (percent correct) for the combined display condition of global illumination and perspective projection does not provide statistically significantly better performance than the other 3 combined display conditions. We also find that increasing the volume density significantly improves performance under global illumination, but not under local illumination. There is an increase in participant performance under local illumination as density increases, but it is not statistically significant. This effect may be entirely due to the fact that, under global illumination, the tubes near the faces of the volume begin (as density increases) to be illuminated similarly to the solid volume itself -an effect that local illumination cannot produce.</p><p>One surprising result is the apparent inversion of participant performance with the hexagonal-prism stimuli. From <ref type="figure" target="#fig_0">Figure 14</ref>, participants show significantly improved perception of volume shape for the cube and icosahedron when going from local to global illumination under orthographic projection (from about 30% correct -near chance -to about 55% correct) and moderate improvement under perspective projection (a less than 10% increase for the cube and about a 20% increase for the icosahedron). However, the hexagonal prism shows the opposite effect. Additionally, the hexagonal prism shows a decrease in performance (of about 12%) under local illumination as the projection model changes from orthographic to perspective.</p><p>In fact, the confusion matrices factored by illumination model (local: <ref type="table" target="#tab_2">Table 1</ref>, global: <ref type="table">Table 2</ref>) show that while under local illumination participants responded 'hexagon' a statistical majority of the time (730 responses, 51%) and 'icosahedron' a marginal minority of the time (340 responses, 24%), under global illumination they responded 'hexagon' a statistical minority of the time (367 responses, 25%) and 'icosahedron' a marginal majority of the time (556 responses, 39%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>Computer graphics researchers from both academia and industry expend considerable effort to develop physically-based illumination algorithms. This effort is aimed largely at entertainment (e.g., movies and video games) to draw and keep an audience's attention, with less regard to effects of illumination on shape perception. Although it is now possible to implement some global effects in real-time on the GPU <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b8">9]</ref>, fully physically-based illumination without restrictions on material types, luminaires, or error thresholds has not yet been demonstrated using GPUs. Moreover, no major commercial or Open Source packages for scientific or medical visualization include physically-based illumination as a rendering option. Our discussions with scientists, surgeons, radiologists, and vendors of medical imaging systems and scientific visualization software collectively reveal a widespread perception that global illumination does not provide added perceptual value when applied to scientific and medical data, or that the scientific user does not want the option. The work described herein shows that physically-based illumination can improve perception of complex 3D flow-field geometry in some static scenes. Especially in the case of applications where users choose to forego perspective projection (medical diagnosis, for example), the addition of physicallybased illumination may be of value.</p><p>In particular, the data from Experiment 1 support the following claims: (1) the effect-size of physically-based illumination on scene perception approaches that of perspective projection, (2) physicallybased illumination provides improved perception (versus local illumination) of the relative depth of objects in a complex scene, (3) the perceptual effects of perspective projection and physically-based illumination reinforce each other, such that (4) the strongest perceptual cueing (of those cues examined) is achieved by combining perspective projection and physically-based illumination.</p><p>The data from Experiment 2 support these additional claims: (5) the shape of some aggregate forms (such as volumes packed with tubes/fibers) is difficult to perceive under local illumination, and perception is unaffected by changes in density, however (6) the shape of such aggregate forms can be perceived better under physically-based illumination, and perception improves with increased density.</p><p>Although a significant first step, these experiments may not fully address the issues of perception and cognition in our target clinical setting. Our participants are non-specialists, our experimental datasets do not reflect the characteristics of fiber tracts and fascicles of the brain, and our perceptual tasks are quite rudimentary compared to clinical tasks. Additionally, more study is necessary to understand how these results generalize to other shapes and other tasks than tubes and scene layout. One surprising result of the study can be seen in <ref type="figure" target="#fig_0">Figure 14</ref>, which suggests that not all shapes were clearly perceived by the participants. This confusion suggests physically-based illumination may not always provide a benefit. Categorizing the instances where physicallybased illumination is of benefit to perception, those where it is of no consequence, and those (if any) where it interferes would be of great value to the practice and study of 3D visualization.</p><p>This study only compared physically-based illumination to local illumination within the space of lighting models. The motivation for this comparison is that (1) physically-based illumination solves the equations governing light transport, but (2) local illumination is the canonical technology available in all (and the only choice in many) 3D visualization systems. When a scientific user is free to choose either illumination model, the natural question arises whether one model offers a benefit over the other.</p><p>For 3D visualization at interactive speeds, physically-based illumination is slower today. But for static images (such as slides for group meetings, scholarly presentations, or archival journals) where sub-second rendering is not a requirement, our experiments indicate that the audience will be more likely to make correct geometric inferences about some 3D scenes with physically-based illumination.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Types of visual completion. Modal completion (a) enables the perception of a triangle superimposed upon the black disks. Amodal completion (b) enables the perception of an occluded triangle visible through holes in the page. Volume completion (c) is a 3D phenomenon, enabling the perception of a stream of bubbles flowing around the pipe; the stream has apparent boundaries and volumetric shape that can not be explained by modal or amodal completion.(a) local illumination (b) global illumination</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Visualizations of DT-MRI data as fiber tracts and isosurface. The left image was rendered with local illumination. The right image was rendered with physically-based illumination. Although the images are clearly different, users and practitioners of visualization generally express uncertainty that either is better for understanding geometric data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>A scene from Exp. 1 rendered in 4 possible stimulus conditions. Shown are the stimuli produced by crossing projection and illumination models. Not shown are the stimuli with target colors reversed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>The difference in closest approach for a pair of targets A and B, ||CA(A)| − |CA(B)||, tells us how much closer the closer of the two target tubes is to the participant's point of view. The amount ||CA(A)| −| CA(B)|| varied between approximately 1.5 and 50 tube radii. No attempt was made to uniformly sample this range.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Comparing performance (percent correct responses) under the two display models (illumination and projection), with 95% confidence intervals. Red bars indicate the projection model variable; blue bars indicate the illumination model variable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>We performed ANOVA for a model including subject id, difference in target closest approach, target color, projection model, and illumination model to predict correct participant response. Subject id (F = 0.65, D f = 4, p &gt; 0.6) and target color (F = 0.58, D f = 1, p &gt; 0.68) were not found to be statistically significant factors. Difference in target closest approach (F = 32.39, D f = 15, p 0.001), projection model (F = 31.62, D f = 1, p 0.001), and illumination model (F = 8.28, D f = 1, p &lt; 0.01) were all found to be statistically significant factors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>5.52, D f = 4, p 0.001), projection model (F = 35.24, D f = 1, p 0.001), and difference in closest approach (F = 6.75, D f = 15, p &lt; 0.01). The largest main effect, projection model, improves response time by almost 2 seconds (orthographic: 5.451 s, perspective: 3.64 s). The only statistically significant interaction effect was for projection model and difference in closest approach (F = 5.36, D f = 15, p &lt; 0.05); under orthographic projection response times did not vary statistically significantly as the difference in closest approach increased (approximately 5.4 s), but un-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Effect on response time for the interaction of the two display models (illumination and projection) with 95% confidence intervals. The bars indicating correct responses only are colored to indicate groupings found from a Tukey HSD test. Logistic regression models for combined display conditions (illumination and projection) versus the difference in closest approach for the target tubes. Error bars indicate 95% confidence intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>The 3 volume shapes used in Experiment 2 as viewed under orthographic projection (where the volumes have the same silhouette). These shapes are packed (at different densities) with tubes following a noise-flow field to produce scene geometry.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .</head><label>9</label><figDesc>A maximum density (level 5) cube in each of 4 display conditions. The rows differ by projection model; the columns differ by illumination model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Confusion matrix (with row and column totals) for local illumination data. Participants respond 'hexagon' over 50% of the time.````````r</figDesc><table><row><cell>esponse</cell><cell cols="3">stimulus cube icosa hexa</cell><cell>total</cell></row><row><cell>cube</cell><cell>334</cell><cell>102</cell><cell>81</cell><cell>517</cell></row><row><cell>icosa</cell><cell>97</cell><cell>272</cell><cell>187</cell><cell>556</cell></row><row><cell>hexa</cell><cell>49</cell><cell>106</cell><cell>212</cell><cell>367</cell></row><row><cell>total</cell><cell>480</cell><cell>480</cell><cell cols="2">480 1440</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>Support for this work was provided by the University of Tennessee, the ORNL Scientific Computing Group, the UT/ORNL Science Alliance, the UT/ORNL Joint Institute for Computational Sciences, and the Na- School. Images were rendered using "Pane" by K. Beason. 3D scenes were constructed using "FiberKit" by Y. Yagi.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Global illumination of white matter fibers from DT-MRI data. Visualization in Medicine and Life Sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Banks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-F</forename><surname>Westin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Retro-rendering with vector-values light: producing local illumination from the transport equation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Beason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Banks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visualization and Data Analysis</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">SPIE</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Differential effects of cast shadows on perception and action</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bonfiglioli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pavani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Castiello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1291" to="1304" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Distortion in 3d shape estimation with changes in illumination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caniard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fleming</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applied Perception in Graphics and Visualization</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="99" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Implicit processing of shadows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Castiello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="2305" to="2309" />
			<date type="published" when="2001-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Shadows in the brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Castiello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lusher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Disler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cogn. Neurosci</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="862" to="872" />
			<date type="published" when="2003-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Shape from shadows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cavanagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Leclerc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception &amp; Performance</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="27" />
			<date type="published" when="1989-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rapid processing of cast and attached shadows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Trithart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pintilie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1319" to="1338" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Real-time global illumination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fantasylab</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lightkit: A lighting system for effective visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">48</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">What is different about a radiologist&apos;s brain?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Radue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">236</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Why cast shadows are expendable: insensitivity of human observers and the inherent ambiguity of cast shadows in pictorial art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Werner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1369" to="1383" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Depth discrimination from shading under diffuse lighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Langer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bülthoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="649" to="660" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Use of interreflection and shadow for surface contact</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Madison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kersten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shirley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Smits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception and Psychophysics</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="187" to="194" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The perception of cast shadows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mamassian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Knill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kersten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="288" to="295" />
			<date type="published" when="1998-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visualization of fibrous and thread-like data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Melek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mayerich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuksel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Keyser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Perception of scene layout from optical contact, shadows, and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Braunstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1305" to="1318" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An image synthesizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Perlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (Proceedings of SIGGRAPH 1985)</title>
		<imprint>
			<date type="published" when="1985" />
			<biblScope unit="page" from="287" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Perlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (Proceedings of SIG-GRAPH 2002)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="681" to="682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Perceiving shape from shading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramachandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific American</title>
		<imprint>
			<biblScope unit="volume">259</biblScope>
			<biblScope unit="page" from="58" to="65" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The influence of cast shadows on visual search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cavanagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1339" to="1358" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Real-time depth-buffer-based ambient occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sainz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Game Developers Conference</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modal and amodal completion generate different shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="454" to="459" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sinha</surname></persName>
		</author>
		<title level="m">Perceiving illumination inconsistencies. Investigative Ophthalmology and Visual Science</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">1192</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The use of 2d and 3d displays for shape-understanding versus relative-position tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>St</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cowen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Smallman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oonk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Factors</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="98" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Combining 2d and 3d views for orientation and relative position tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Möller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Atkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirkpatrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCHI &apos;04</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Volume completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">U</forename><surname>Tse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="37" to="68" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A contour propagation approach to surface filling-in and volume formation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">U</forename><surname>Tse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology Review</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="115" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
