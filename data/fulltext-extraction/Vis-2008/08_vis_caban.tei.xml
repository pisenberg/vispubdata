<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Texture-based Transfer Functions for Direct Volume Rendering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Jesus</forename><forename type="middle">J</forename><surname>Caban</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Penny</forename><surname>Rheingans</surname></persName>
						</author>
						<title level="a" type="main">Texture-based Transfer Functions for Direct Volume Rendering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-20T19:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>visualization</term>
					<term>statistical analysis</term>
					<term>volume rendering</term>
					<term>data variability</term>
					<term>medical imaging</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Visualization of volumetric data faces the difficult task of finding effective parameters for the transfer functions. Those parameters can determine the effectiveness and accuracy of the visualization. Frequently, volumetric data includes multiple structures and features that need to be differentiated. However, if those features have the same intensity and gradient values, existing transfer functions are limited at effectively illustrating those similar features with different rendering properties. We introduce texture-based transfer functions for direct volume rendering. In our approach, the voxel&apos;s resulting opacity and color are based on local textural properties rather than individual intensity values. For example, if the intensity values of the vessels are similar to those on the boundary of the lungs, our texture-based transfer function will analyze the textural properties in those regions and color them differently even though they have the same intensity values in the volume. The use of texture-based transfer functions has several benefits. First, structures and features with the same intensity and gradient values can be automatically visualized with different rendering properties. Second, segmentation or prior knowledge of the specific features within the volume is not required for classifying these features differently. Third, textural metrics can be combined and/or maximized to capture and better differentiate similar structures. We demonstrate our texture-based transfer function for direct volume rendering with synthetic and real-world medical data to show the strength of our technique.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Direct volume rendering provides a visual representation of threedimensional volumetric data based on functions that map densities to specific colors and opacities. These transfer functions are crucial in the understanding of the overall volumetric data and individual features contained within the volume space. Specification of transfer functions to disambiguate the various materials and structures is a tedious and time-consuming task. In addition, accurately identifying and visually distinguishing various objects in a volume is a challenging task.</p><p>Researchers have used first-and second-order derivative-based functions to more accurately identify boundaries between different materials <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. Kniss et al. <ref type="bibr" target="#b8">[9]</ref> introduced dual domain interaction to facilitate identification of boundaries in the three-dimensional domain using a probe that facilitates manual segmentation of various materials. However, most transfer functions are still attached to the voxel's properties such as intensity, gradient, and curvature. This leads to problems when the volumetric data under consideration contains multiple structures with similar intensity values and geometrical properties.</p><p>Like images, volumes commonly contain characteristic patterns and textures that our visual system can clearly identify. In particular, volumes frequently present very small structures -textons -which combined create different structures and characteristic regions. Studies in human perception and psychological research have shown that textures and patterns play an important role in the overall interpretation and understanding of the underlying structure. Motivated by those studies and the need to better differentiate similar structures within volumetric data, we have designed a texture-based transfer function which uses local textural properties of the data in addition to the individual intensity values.</p><p>We introduce texture-based transfer functions (TbTF) to enhance the visualization of volumetric data by allowing similar structures to be rendered differently. In our technique, a volume is first decomposed into overlapping or non-overlapping regions or subvolumes of interest. For each subvolume, a multi-dimensional texture-based de-</p><p>• Jesus J. Caban is with the Department of Computer Science, University of Maryland (UMBC), E-mail: caban1@cs.umbc.edu.</p><p>• Penny Rheingans is with the Department of Computer Science, University of Maryland (UMBC), E-mail: rheingan@cs.umbc.edu. scriptor is computed which captures the local textural statistical properties. All the statistical descriptors are pre-computed and stored in a vector image. During the raycasting integration process, the vector image is used to look up the voxel's textural properties. Based on those properties, different opacity and color are assigned to each voxel, thus enhancing the visualization and illustration of regions with similar density values. Additionally, we evaluate our techniques by experimenting with synthetic data as well as real-world medical data. Based on our observations, we provide guidelines regarding the texture sizes that more effectively capture local structural details. We also demonstrate the benefits of weighting and combining individual textural properties to provide effective visualization of similar structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>The design of effective transfer functions for direct volume rendering has been a widely researched topic. Early work introduced the use of gradients for improved classification in the raycasting process <ref type="bibr" target="#b10">[11]</ref>. Accurately classifying materials has been a challenging task that was approached using gradient-based multi-dimensional transfer functions <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref>. These techniques were particularly useful in the medical domain where the boundaries of adjacent anatomical parts are not easily distinguishable using only the voxel intensity. Multi-dimensional transfer functions were further extended to accentuate and highlight regions of high curvature <ref type="bibr" target="#b7">[8]</ref>.</p><p>Transfer functions have also been used in the illustrative visualization domain. Hauser et al. <ref type="bibr" target="#b3">[4]</ref> were the first to propose a novel framework to mix illustrative representations with standard volume rendering (direct, MIP, x-ray) to highlight features in the visualization while providing context to the viewer at the same time. Style transfer functions were introduced by Bruckner et al. <ref type="bibr" target="#b0">[1]</ref> to give more control to the visualization designer. These techniques have been mostly used with segmented data.</p><p>There has been some work in the field of analyzing the spatial characteristics around the voxel under consideration to apply some smart classification techniques. Roettger et al. <ref type="bibr" target="#b17">[18]</ref> used the voxel barycenter and the region variance to assist manual specification of colors for similar features in the process of volume rendering. Sato et al. <ref type="bibr" target="#b18">[19]</ref> characterized tissues of interest by explicitly defining rules and filters such as edges, sheets, lines, and blobs for each local structure. The filter characteristics were analyzed using a Gaussian models. Two limitations of this technique were that all tissue classification was reduced to four simple structures and that the smoothing could cause loss of important local structures and patterns. Their classification was fur-ther improved with a multichannel approach but it required complex user interaction for good classification results. Ljung et al. <ref type="bibr" target="#b11">[12]</ref> developed a technique that analyzed the neighborhood using statistical techniques such as range weight.</p><p>In addition, recent work has suggested the use of local texture analysis to better classify volume data. Tzeng et al. proposed a technique to classify volume data through a supervised learning technique that considers neighbors density and gradient values <ref type="bibr" target="#b20">[21]</ref>. The technique was limited by the amount of user iteration required to select regions of interest and by the few neighboring metrics used. Lum et al. <ref type="bibr" target="#b12">[13]</ref> suggested the use of local textures, scale-based filtering, and parallel coordinates to better classify volume data interactively. The effectiveness of that approach was limited by how well different materials can be distinguished and identified from the changes of the parallel coordinates.</p><p>In our work, we use first-, second-, and high-order local statistical texture properties to effectively assign voxels to different opacities and colors. Based on local statistical texture properties and the set of statistical metrics we employ, our texture-based transfer functions can accurately capture differences and similar local properties to better differentiate individual structures. In contrast with most existing techniques which are primarily attached to the voxel's density values, our technique automatically assigns voxels with similar intensity values and different textural properties to individual transfer functions.</p><p>Some of the benefits of our texture-based transfer functions are that different structures which have the same intensity values can be rendered differently by the raycast function. Second, previous knowledge of the specific structures or segmentation of the different structures is not needed. In addition, textural properties and statistical metrics can be combined in a manner that maximizes the differences in structures that are similar in density and even some textural properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TEXTURE ANALYSIS</head><p>Texture analysis is a fundamental technique used in medical imaging and computer vision to identify, characterize, and compare regions with similar properties. Statistical textural analysis measures and captures local image properties which are not necessarily based on intensity properties, they are rather a combination of intensity, local patterns, and local image statistics. It has been demonstrated that the generation of textural properties can be used to enhance classification and characterization of individual image regions <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>Our TbTF uses local textural properties to map a given voxel to a specific opacity and color. Instead of only considering the intensity value of the voxel, the gradient information, or the interpolation between neighboring voxels, our TbTF maps textural properties to specific rendering attributes.</p><p>We use a combination of first-, second-, and high-order statistics to capture textural properties. In particular, we use histogram statistics in conjunction with co-occurrence and run-length matrices to capture structural and geometrical properties.</p><p>First-order statistics are calculated from the probability of observing a particular pixel value at a randomly chosen location in the image. First-order statistics are among the simplest textural measurements that can be extracted from 2D/3D images, specially given that they are obtained from a histogram. A histogram is a simple measurement that maps intensity values into various disjoint bins. Let π(i)(i = 1, 2, ..., n) be the number of voxels whose intensity is i in a given subvolume v k of size m 3 . The probability of intensity i being within the subvolume v k is computed by</p><formula xml:id="formula_0">h(i) = π(i) m 3<label>(1)</label></formula><p>Using such probabilities and the image histogram it is possible to extract several local statistical properties including mean, variance, kurtosis, skewness, and deviations. A number of local statistical properties cannot be captured by firstorder statistics, only with more advanced texture-analysis techniques such as second-or high-order statistics. Second-order statistics measure the likelihood of observing an intensity value i and j at an average distance d = (dx, dy) apart <ref type="bibr" target="#b4">[5]</ref>. Frequently, second-order statistics are computed using co-occurrence matrices as demonstrated by Haralick et al. <ref type="bibr" target="#b2">[3]</ref> Let δ = (r, φ ) denote a vector in the polar coordinate of the subvolume. We can compute the joint probability of the pairs of gray levels that occur at pairs of points separated by δ . That joint probability can be stored on a matrix P(i, j) which contains the probability of observing the pair of gray levels (i, j) occurring at separation δ . In our case, we estimate the co-occurrence matrices by computing different matrices for the angles φ = {0 • , 45 • , 90 • , 135 • } within each axis. To guarantee rotation-invariant properties, we compute the average cooccurrence matrix and then extract statistical properties including energy, contrast, correlation, inertia, entropy, and sum of entropies.</p><p>To better identify and characterize regions with similar properties, we use run-length matrices of higher-order statistics to estimate the number of graylevel runs within the volume <ref type="bibr" target="#b19">[20]</ref>. The run-length matrix P θ (i, k)(i = 1, .., m; k = 1, .., n) represents the frequency that k points with 32-bit quantized gray level i continue in the direction</p><formula xml:id="formula_1">θ ∈ 0 • , 45 • , 90 • , 135</formula><p>• . From a run-length matrix textural statistics such as the amount of short (fine) runs, long (coarse) runs, and the uniformity of such runs can be estimated. In total, our TbTF uses 20 textural metrics within a multidimensional descriptor to differentiate local textural properties. <ref type="table">Table  1</ref> lists all the metrics used by our system. For a mathematical description and formulas, please see <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20]</ref>. In our system, all the textural metrics are pre-computed and stored in a vector image which the raycast function uses during the rendering process to look-up the local textural properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">APPROACH</head><p>Our approach is based on our observation that volumetric data can be described as a collection and specific arrangement of 3D textons. By analyzing the specific textural pattern that each collection of 3D textons creates, we can automatically enhance the visualization and more effectively illustrate structures with similar intensity values.  <ref type="figure" target="#fig_0">Figure 1</ref> shows two synthetic volumes used to test our system. Both 3D datasets have different patterns and structures with the same intensity values which existing transfer functions are unable to automatically differentiate. We can see different structures and patterns by looking at the images in <ref type="figure" target="#fig_0">Figure 1</ref>, however existing volume rendering techniques have limitations and difficulties when it comes to effectively differentiating structures that have the same density values and similar gradients (as seen in <ref type="figure" target="#fig_1">Figure 2</ref>(left)).</p><p>Our texture-based transfer functions (TbTF) work as follows: Given an input volume V , we first partition the 3D image into |P| overlapping or non-overlapping regions. For each region or subvolume v i ∈ P, a set of statistical texture properties are computed and stored in a vector image V ′ . Once the vector image has been generated, the pre-computed statistical properties are used by the raycasting function to look up local structural properties and assign different color and opacity to voxels with similar density but with distinct textural properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Volume Partitioning</head><p>The first step is to partition an input volume into meaningful regions where individual statistical properties can be estimated more accurately. Given an input image V , we first partition the volume into |P| regions or subvolumes of interest. Each region is described by two parameters: (ρ, σ ). The value ρ ≥ 1 denotes the region size and σ ≥ 0 denotes the overlap. For instance, to compute the per-voxel statistical properties of a given volume V using a neighboring window w, the volume partition will be created with ρ = 1 and σ = w. The two primary considerations for estimating the value of ρ and σ are that the region or subvolume has to be large enough to capture statistical textural properties, but it has to be small enough to capture only local properties relevant to surrounding voxels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pre-computing Textural Properties</head><p>After determining the values ρ and σ and generating a specific partition, a multi-dimensional vector containing the structural texture properties is estimated for each region v i ∈ P. A combination of first-, second-, and high-order statistics are used to extract textural properties from each subvolume of interest. Once all the 20 metrics are computed and combined into a multi-dimensional vector they are normalized and stored in a vector image V ′ .</p><p>The volumetric partitioning and the statistical textural analysis are pre-computed and used during the visualization step. Alternatively, the partitioning and texture analysis can be done during the initialization step given that the process takes only a few seconds in cases with relatively large partitions or subvolumes of interest. The performance and timing results are further discussed in section 5.1.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Texture-based transfer function</head><p>After the vector image with all the textural properties has been generated, it is used to enhance volumetric visualization. In our system, during the rendering process, a voxel is represented by its local textural properties which enables the flexibility of enhancing and distinguishing regions with similar density and gradient values, but with different local textural properties.</p><p>We present four specific ways to use our TbTF to enhance visualization and improve the differentiation of individual structures. First, the user can specify a value k with the number of structures to discriminate, then by using the multi-dimensional descriptors, vectors can be automatically classified in k groups. Second, the user has the flexibility of enhancing the visualization by manually or automatically changing individual metrics. Third, the user can select a specific area and based on similarity measurements, all equivalent structures can be highlighted by the transfer function. Finally, we also present a technique which provides the flexibility of selecting two regions and computing individual weights for each metric, thus maximizing the differences between those regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Automatic Classification</head><p>After generating or loading a pre-computed vector image V ′ containing all the textural statistical properties, automatic classification of the data is possible. The user can specify a value k with the number of structures to highlight or display with different rendering properties. Then, the entire collection of vectors or a set of non-neighboring vectors v i ∈ V ′ can be used to classify the data. By using an inequalitybased fast k-means implementation <ref type="bibr" target="#b1">[2]</ref>, the data can be clustered within seconds and rendered differently. Note that during the classification process all selected metrics of each sub-volume v i are uniformly weighted. In Section 4.3.4 we will show how to automatically assign non-uniform weights to each metric.</p><p>Our technique of using unsupervised k-means to automatically classify the data can be seen as a type of segmentation. However, given the flexibility the user has to change the value k during the visualization process and/or selecting specific metrics to use during the unsupervised classification step, we believe that automatic classification is an important benefit our TbTF provides.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Individual Weighted Metrics</head><p>A second technique our system provides is to enhance the visualization of structures with similar intensity values by selecting individual metrics. The user can automatically select and change any of the 20 individual metrics to better discriminate the data. Since every region or voxel is represented by a vector that captures the local textural properties, the user can select individual metrics or a combination of different metrics to highlight specific structures. During the rendering process, for every region or voxel under consideration, if the value of the specific metric(s) under consideration are above the user's threshold, the raycast function will assign the voxel a different opacity, color, and rendering properties.</p><p>Given the large amount of metrics the user can change and the possible complexity of understanding individual metrics, weighted metrics can be difficult for data exploration. However, a quick review of medical imaging and computer-aided diagnosis systems shows that researchers have provided specific textural metrics that are important to highlight particular diseases or anatomy <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17]</ref>. TbTF provides the flexibility of using such information to individually weight different metrics and enable domain experts to better highlight important anatomical structures or patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Similarity Measurements</head><p>Another technique used to enhance the visualization of structures with similar density and gradient values are similarity measurements. We have seen that in high-dimensional volume rendering <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref> 3D widgets have been accepted as techniques to select areas of interest. In our system, the user can employ a 3D widget to select a specific region or area of interest S. The combination and averaging of all the multidimensional descriptors (v i ∈ S) contained within the selected region result in a characteristic descriptor. That multi-dimensional descriptor can then be used to highlight similar structures during the rendering process.</p><p>During the raycasting and accumulation process, if the textural properties of a specific voxel is within a certain distance from the characteristic descriptor, then the region or voxel under consideration receives different opacity values and color properties. It is important to note that this approach does not add any additional complexity to the raycasting process given that it only adds a single lookup to determine if the local statistical properties are within a given threshold. <ref type="figure" target="#fig_1">Figure  2</ref>(right) shows a diagram with our modification to the raycast process.</p><p>The raycasting process results in the creation of a visualization that highlights different structures and displays features with similar density values and distinct textural patterns differently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Non-uniform Weights</head><p>Another way to use our TbTF to enhance structures with similar intensity and gradient values is to non-uniformly assign weights to each textural metric. By using a 3D widget, the user can select two small regions of the structures that need to be differentiated. Then, since the vector image V ′ containing all the textural metrics is always kept in memory, two sets -S 1 and S 2 -can be generating by grouping all the multi-dimensional descriptors of the selected areas. The two sets -S 1 and S 2 -represent two particular classes. All the textural metrics for each class are analyzed and based on their textural metrics, differences are maximized. By using the mutual information between the two classes, the textural metrics that are redundant between the two classes and the features that are the most relevant for each independent class can be identified <ref type="bibr" target="#b13">[14]</ref>. From such results, individual weights can be assigned to each group -thus maximizing their differences. Finally, during the raycast function by comparing the non-uniformly weighted descriptor, two similar structures can be automatically differentiated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>To test the strengths and limitation of our system, we have used synthetic and real-world medical volumes. In addition, we have compared different ρ and σ values to provide specific information regarding the parameters which work more effectively.  Given that most of the areas and structures have the same intensity value, existing volume rendering techniques are limited in how effectively different structures can be illustrated separately and with individual rendering properties. We compared the results of using 1D and our texture-based transfer functions. <ref type="figure" target="#fig_2">Figure 3(right)</ref> shows the results of using ρ = 4, σ = 0, and the automatic classification with k = 3. Note that in our automatic classification the value k has to be n + 1 where n is the number of structures to differentiate (ie. n structures plus noise). From the images we can see that existing transfer functions are limited in automatically differentiating regions with distinct patterns and textural properties.  <ref type="figure" target="#fig_4">Figure 4</ref> shows the results with our second synthetic volume which has both fine structures as well as coarse regions. From the result, we can see that a 1D transfer function is limited in separating structures. In addition, we can see that our TbTF was able to find the structures under different parameters, including the use of different values k to automatically classify the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Synthetic Data</head><p>From our results, we can see that structures with linear patterns at 0 • , 45 • , and 135 • were classified and colored together. This is the result of using co-occurrence and run-length matrices which are invariant to rotation. Note that this property of rotationally invariance is not required. However, given that in most real-world 3D images textures or characteristic patterns can be found within the entire volume space and in very limited situations we can find patterns pointing in the same direction, rotationally invariant features help texture-based classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Texture Size</head><p>Using the same synthetic datasets, we tested our TbTFs with different texture sizes ρ and different overlaps σ to find the specific size which best captures structural pattens and textural properties. <ref type="figure" target="#fig_5">Figure 5</ref> compares the results of generating a per-voxel multi-dimensional descriptor using an overlap σ in the range of <ref type="bibr">[1 − 6]</ref>. From the results of both synthetic datasets that contain coarse and fine patterns, we found that when σ = 4 the individual structures were better differentiated and rendered more accurately.</p><p>Based on our experiments with synthetic data and different overlaps, we can conclude that a region of 5 3 (ρ = 1, σ = 4) voxels is large enough to capture local statistical textural properties, while at the same time was small enough to accurately differentiate surrounding structures. Note that our results are not a definite solution for the texture size, however we found that when the textural statistical properties are computed in regions between four (4) and six (6) voxels, different patterns and structures can be accurately distinguished and separated despite them containing fine or coarse textural patterns. In In both experiments we found that when σ = 4 (i.e area of 5 3 ), the data was classified more accurately. Note that with both small and large σ values, there is significant increase on the amount of error and misclassification. addition, from our experiments we found that there are not significant and visible advantages of computing per-voxel textural metrics (ρ = 1) versus using ρ = 2. This is, ρ = 2 can be used without any significant reduction in accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Timing and Performance</head><p>The vector image containing all the local textural properties can be either pre-computed or generated during the initialization. To demonstrate the performance of our system and the time required to estimate the statistical textural properties, we have experimented with different volumes sizes and different ρ and σ values. In addition, to show the minimum impact that the optional and automatic classification presented in Section 4.3.1 has on the rendering pipeline, we have classified each experiment with k = 3 and k = 4. <ref type="table" target="#tab_2">Table 2</ref> summarizes the results. From the table we can see that the processing time for computing the textural properties increases as ρ decreases and the volume size enlarges given the large number of multi-dimensional descriptors to compute. However, it is important to note that for the suggested value of ρ = 2, σ = 4, and relatively large volumes, the pre-processing time is about thirty seconds. In addition, from <ref type="table" target="#tab_2">Table 2</ref> we can see that when automatic classification is used, the overhead and time introduced to the rendering pipeline is minimum. In particular, in the worst case scenario that all the multi-dimensional vectors are used, only about five seconds are required to automatically classify relatively large volumes. Image generated using Simian <ref type="bibr" target="#b8">[9]</ref> and multi-dimensional transfer functions to emphasize boundaries between adjacent features. (Bottom left) Visualization generated using our TbTF with ρ = 2, σ = 4, and k = 3. (Bottom right) Visualization generated using our TbTF with a slightly modified opacity transfer function, ρ = 4, σ = 0, and k = 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Medical Volumetric Data</head><p>In addition to synthetic datasets, we have tested our TbTFs with actual CT and MRI data. The experiments with real-world data helped us refine our techniques and strengthen them to deal with the challenges that synthetic data cannot provide.</p><p>We first applied our techniques to lung data. <ref type="figure" target="#fig_6">Figure 6</ref> shows our results in comparison with other transfer function techniques. The top row shows volume rendered images using a one-dimensional transfer function. The top left image shows a simple transfer function where all the voxels of the same intensity are colored in green. The density based transfer function was not able to disambiguate features in the data. The top center image highlights the boundaries by using lighting and gradient enhancement techniques. The top right image shows the image obtained by using multi-dimensional transfer functions that highlight surface boundaries and allow dual domain interaction. The bottom images show the results of using our TbTF that leads to automatic identification and assignment of colors to the vessels and surrounding anatomy in the lung. Our results were obtained automatically in contrast to the other three techniques which required significant transfer function manipulation. Also it is important to notice that even with relatively large ρ our technique was able to automatically separate different structures. We also tried our TbTFs on heart data as can be seen in <ref type="figure">Figures  7 and 8</ref>. The heart data was more challenging as visualizing specific anatomical features such as the aorta, the heart, and surroundings structures was extremely hard using standard transfer function or dual-domain interaction. <ref type="figure">Figure 7</ref> shows a region of the heart and the aorta along with the tissue around the vertebrae in the spine. The left image was generated using a one-dimensional transfer function. Opacity-based transfer functions were unable to clearly separate the heart's anatomy from the tissue around the vertebrae due to their similar intensities. Using a multi-dimensional transfer function we were able to separate and color the tissue around the vertebrae separately. However, as can be seen the intensity and gradient-based techniques end up picking some features co-located with the heart causing a green boundary around the heart. In addition, generating this particular visualization took a considerable amount of time even using the advanced feature of dual-domain interaction, primarily because identifying the exact boundary of the heart anatomy and the tissue was extremely hard. The right image was obtained using our TbTF technique which automatically identified the two anatomical structures separately and colored them differently.</p><p>We have also applied our TbTF techniques to more complex scenarios. By using our heart dataset, we experimented with automatically highlighting individual structures from multiple intensity value ranges. Previously, we saw that the tissues surrounding the spine have the same intensity values as the aorta. In addition, the cardiac vessels surrounding the heart have the same intensity values as the heart anatomy. By applying a TbTF to individual intensity ranges we can highlight and differentiate multiple structures at a time. <ref type="figure">Figure 8</ref> shows our results. From the images we can see that the heart was colored in red, adjacent vessels in green, and the tissue surrounding the spine was automatically colored in blue.</p><p>Opacity-and gradient-based transfer functions are limited in differentiating structures when all the voxels have the same intensity and similar gradient values. <ref type="figure" target="#fig_8">Figure 9</ref>(top left) shows a visualization obtained by using intensity-and gradient-based transfer functions. If we just focus on a particular intensity value of that visualization, we can see the large amount of uncertainty and misclassification introduced by standard transfer functions. <ref type="figure" target="#fig_8">Figure 9</ref>(top right) shows the set of voxels with intensity value 118. We can see that always within a specific classification and visualization, there are voxels along a particular density range misclassified. By using our TbTF with ρ = 2, σ = 4, and k = we were able to automatically highlight and render differently each structures as shown in <ref type="figure" target="#fig_8">Figure 9</ref>(bottom).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Weighted Textures</head><p>Statistical textural properties have been proven to be strong approaches to characterize similar 2D/3D structures <ref type="bibr" target="#b14">[15]</ref>. However, there are situations where structures have the same intensity values and share multiple textural properties, thus reducing the benefits of local statistical textural properties. We found such a limitation when applying our TbTF to brain MRI images, particularly when trying to highlight the part of the cerebellum. <ref type="figure" target="#fig_0">Fig. 10</ref>. Images of the gray matter of the brain. It is possible to see that the cerebellum has specific textural properties that can be use to differentiate the gray matter from the cerebellum. <ref type="figure" target="#fig_0">Figure 10</ref> shows a 3D slice of the brain MRI dataset. From the image it is possible to see that there are textural differences between the gray matter and the cerebellum. However, by using a uniformly weighted multi-dimensional descriptor such as the one found in our vector image V ′ , the structures cannot be highlighted correctly. <ref type="figure" target="#fig_0">Fig. 11</ref>. Example of the process followed to maximized differences between two regions. The user selects two regions, a joint-histogram is generated with the metrics of all the voxels or regions of interest, and individual weights for each textural metric are estimated to better highlight different structures.</p><p>In such situations, it is possible to maximize the differences between the structures by assigning individual weights to each textural metric. We have extended our texture-based transfer functions to incorporate weighted textural properties. Our approach works as follows: Given a set of structures that need to be highlighted by our transfer function, small areas within each of them are selected. Then, by combining the statistical textural properties of each of the structures in a joint histogram, joint entropy and mutual information can be estimated. The mutual information measures the amount of information that the structure X conveys about the structure Y .</p><p>Such information can then be used to find the textural metrics which are redundant between the structures and most importantly, find the specific metrics which characterize each individual area under consideration. That information is used to estimate a weighting value for each individual textural metric, thus allowing better differentiation of similar structures. In our system, the technique used to generate weights for each metric is based on mRMR <ref type="bibr" target="#b13">[14]</ref>. <ref type="figure" target="#fig_0">Figure 11</ref> shows the general idea of our weighted texture-based transfer function. Different areas of interest are selected, per-voxel metrics combined, the mutual information computed, and individual weighting values estimated for each textural metric.</p><p>In our experiments with MRI data we found that only second-and high-order statistical properties of the data were among the primary metrics to differentiate the gray matter from the cerebellum. In particular, we found that long-run low-gray, gray-level nonuniform, sum of entropy, and inertia were the most characteristic features with resulting weights of 0.31, 0.22, 0.21, and 0.17 respectively. However, it is important to understand that none of these individual textural metrics were able to accurately highlight the cerebellum, but only a combination and weighting of them resulted in effective visualization of the center region of the cerebellum. <ref type="figure" target="#fig_0">Figure 12</ref> shows the results of partitioning the volumes with ρ = 2, σ = and highlighting individual textural metrics. None of the individual metrics was able to accurately highlight different structures. <ref type="figure" target="#fig_0">Figure 13</ref> compares different volume rendering techniques. The topleft image shows how even multi-dimensional transfer functions are limited in accurately enhancing the cerebellum from the gray matter. In addition, the top-right image shows how the equally weighted TbTFs and automatic classification were not able to effectively illustrate the different structures. Only, the use of weighted texture-based transfer functions were able to enhance and assign different rendering properties of the gray matter and the cerebellum.</p><p>Our results with weighted textures should not be taken as a definite solution to extract the cerebellum. In such situations, a segmentation technique might be more suitable. However, when trying to enhance a region of the cerebellum with high textural differences, local texture analysis and weighted textures will work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we present a novel way to visualize different materials in volumetric data by using texture-based transfer functions. The main advantage of using TbTF is that the user can effectively visualize features with similar voxel properties differently given that the technique is not attached to a specific intensity value, but rather local properties.</p><p>The presented system has some limitations. First, the process of estimating all the textural metrics for each voxel or region of interest could be a time-consuming process for relatively large volumes. That is the primary reason why we suggested that for small values ρ and large volumes, the process of estimating and generating the vector image V ′ should be an offline process. Second, our current implementation and graphical interface shown in <ref type="figure" target="#fig_0">Figure 14</ref> does not use the graphics hardware and texture memory to store the vector image V ′ . In the future, we plan to expand our system so the lookup for the textural properties can be done within the graphics card.</p><p>We had considered multi-resolution texture analysis and multi-scale pyramids to better enhance different structures. However, we found that our current classification process does not benefit from such techniques. First, in a pyramid, by reducing the size of the volume or applying smoothing operation to the volume, small features characteristic of particular patterns are lost, thus reducing the accuracy of the classification process. Second, estimating textural properties in a multi-resolution fashion can reduce the classification given that small volume features might have a very similar descriptor to high-level features. We believe that those two techniques can be used in the future to better classify the data, but special consideration has to be taken.</p><p>In our system, we use a combination of first-, second-and highorder statistics to characterize textural features. We found that the enhancement of different features was best performed when metrics were estimated in a 5 3 region. In particular, we found that for ρ = 2 and σ = 4 the computed textural properties led to more accurate feature identification. Our results regarding the textural parameters that more effectively captures local statistics are based on visual estimates. In the future, we plan to more accurate find the texture size and overlap by using a supervised training and testing system. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Two synthetic datasets used to evaluate our system. Most of the structures within the volumes have the same intensity value limiting how effectively existing rendering techniques can differentiate individual structures. Note that the individual structures and different patterns can be visually distinguished.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>(Left) 2D slices of our synthetic data. Since most of the structures have the same intensity value, existing transfer functions are limited in effectively differentiating them. (Right) Diagram of the modification to the raycasting process to incorporate our TbTFs. As the ray traverses through the volume, each voxel location is identified by the precomputed local textural properties. Thus, enabling effective enhancement of structures with different textural properties.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>(Left) Synthetic data being visualized with standard transfer functions. Since most of the areas have the same intensity value, existing visualization techniques are limited in how effectively individual structures are rendered. (Right) The same synthetic dataset visualized using our TbTF. The center of the volume (pointed by the arrow) can be automatically highlighted by just analyzing local textural properties.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 (</head><label>3</label><figDesc>left) shows one of the synthetic volumes used to test our technique.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>(Top left) Results of a 1D transfer function. (Top right) Results of our TbTF with ρ = 2, σ = 4, and k = 3. (Bottom left) Results of our TbTF with ρ = 2, σ = 4, and k = 4. (Bottom right) Results of our TbTF with ρ = 2, σ = 4, k = 4, noise, and structures in different intensity ranges that do not have to be differentiated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>(Top row) Results of using ρ = 1, k = 3, and changing σ = 1, 2, 3, 4, 5, 6. (Bottom row) Results of using ρ = 1, k = 3, and changing σ = 2, 4, 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Visualizations of the lung dataset using various transfer functionbased techniques. (Top left) Standard 1D transfer function using VTK. (Top center) Volume rendered image with lighting and gradient-based transfer function to accentuates the boundaries of the vessels.(Top right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Images showing a segment of the heart, aorta, and tissue around the spine. (Left) Visualization using a 1D opacity-based transfer function. (Center) Resulting image after applying a multi-dimensional transfer function and carefully picking the the specific materials and boundaries to enhance. (Right) Results of our TbTF with ρ = 2, σ = 4, and k = 3 which highlighted individual structures based on their textural properties. Experiment of applying our TbTF to different intensity ranges simultaneously. (Left) Visualization obtained using standard transfer functions. (Right) Image obtained using our TbTF to automatically highlight structures with similar intensity values and different textural properties. In this example, the heart is colored in red, adjacent vessels are colored in green, and the tissues surrounding the spine are colored in bluish purple.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>(Top left) Image obtained by using a combination of opacityand gradient-based transfer functions. (Top right) By only selecting the voxels with density 118 we can see that existing transfer functions are limited in effectively differentiating individual structures. (Bottom) Local textural properties can be used to improved the accuracy of existing transfer functions. This image was obtained with our TbTF and parameters ρ = 2, σ = 4, and k = 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 12 .</head><label>12</label><figDesc>Results of highlighting individual textural metrics. None of individual textural metrics was able to accurate highlight different structures. (Top left) Energy, (Top right) Sum Entropy, (Bottom left) Absolute Deviation, (Bottom right) Standard Deviation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 13 .</head><label>13</label><figDesc>(Top left) Visualization generated using the dual-domain interaction in Simian. The cerebellum cannot be localized using this technique since it shares intensity and gradient similarities with other regions in the volume, even though it has a unique pattern. (Top right) Results with our uniformly weighted TbTF with ρ = 2, σ = 4, and k = 3. (Bottom) Weighted TbTF applied to the brain data. The cerebellum is clearly visualized in red while the rest of the brain structures are seen in green.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 14 .</head><label>14</label><figDesc>Changing individual metrics is possible with our graphical interface. The user can pick a threshold of individual metrics, then during the raycast function, if the voxel under consideration is within the range, different opacity and/or color properties are assigned.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Volume Size ρ = 4, σ = 2 ρ = 2,σ = 4 ρ Performance of our texture-based transfer functions. The first row of each section shows the time required to pre-compute the textural properties for different volume sizes, ρ and σ values. The other two rows show the time required to automatically classify the pre-computed vectors as explained in Section 4.3.1.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>= 1,σ = 2</cell></row><row><cell>64x64x32</cell><cell>0.15s</cell><cell>0.61s</cell><cell>4.09s</cell></row><row><cell>k = 3</cell><cell>0.01s</cell><cell>0.10s</cell><cell>0.74s</cell></row><row><cell>k = 4</cell><cell>0.02s</cell><cell>0.12s</cell><cell>0.82s</cell></row><row><cell>64x64x64</cell><cell>0.30s</cell><cell>1.21s</cell><cell>8.33s</cell></row><row><cell>k = 3</cell><cell>0.04s</cell><cell>0.18s</cell><cell>1.32s</cell></row><row><cell>k = 4</cell><cell>0.05s</cell><cell>0.20s</cell><cell>1.50s</cell></row><row><cell>128x128x64</cell><cell>0.86s</cell><cell>4.57s</cell><cell>32.43s</cell></row><row><cell>k = 3</cell><cell>0.08s</cell><cell>0.76s</cell><cell>6.25s</cell></row><row><cell>k = 4</cell><cell>0.11s</cell><cell>0.94s</cell><cell>6.02s</cell></row><row><cell>128x128x128</cell><cell>1.33s</cell><cell>8.80s</cell><cell>61.47s</cell></row><row><cell>k = 3</cell><cell>0.14s</cell><cell>1.46s</cell><cell>12.16s</cell></row><row><cell>k = 4</cell><cell>0.20s</cell><cell>1.54s</cell><cell>12.93s</cell></row><row><cell>256x256x128</cell><cell>5.25s</cell><cell>32.75s</cell><cell>233.88s</cell></row><row><cell>k = 3</cell><cell>0.62s</cell><cell>6.36s</cell><cell>25.02s</cell></row><row><cell>k = 4</cell><cell>0.76s</cell><cell>6.50s</cell><cell>27.12s</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>The authors would like to thank the anonymous reviewers for their comments and suggestions. We would like to thank Alark Joshi for his support and help comparing our technique with previous work. This work was supported in part by NSF grant 0121288 and the 2008-2009 IBM PhD Fellowship Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Style transfer functions for illustrative volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bruckner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Gröller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="715" to="724" />
			<date type="published" when="2007-09" />
		</imprint>
	</monogr>
	<note>EuroGraphics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Using the triangle inequality to accelerate kmeans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="147" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Textural features for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<date type="published" when="1973" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="610" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Two-level volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">I</forename><surname>Bischi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Gröller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="242" to="252" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generalized cooccurrence matrix for multispectral texture analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hauta-Kasari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Parkkinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaaskelainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lenz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="785" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Texture analysis for classification of cervix lesions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Craine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1144" to="1149" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semi-automatic generation of transfer functions for direct volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kindlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Durkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference of Volume Visualization</title>
		<meeting>the conference of Volume Visualization</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Curvaturebased transfer functions for direct volume rendering: Methods and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Kindlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Whitaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tasdizen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Möller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization</title>
		<meeting>IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Interactive volume rendering using multi-dimensional transfer functions and direct manipulation widgets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kindlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization</title>
		<meeting>IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="2001-10" />
			<biblScope unit="page" from="255" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Texture analysis in three dimensions as a cue to medical diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Petrou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Medical Imaging Processing and Analysis</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Display of surfaces from volume data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="29" to="37" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Local histograms for design of transfer functions in direct volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ljung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ynnerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lundstrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphic</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1570" to="1579" />
			<date type="published" when="2006-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Interactive multi-scale exploration for volume classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Lum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shearer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Visual Computer</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="622" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Feature selection based on mutual information: criteria of max-dependency, max-relevance, and minredundancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1226" to="1238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Image Processing: Dealing With Texture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Petrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Garcia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Texture Analysis in Machine Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Pietikainen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>World Scientific Publishing Company</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A texture dictionary for human organs tissues&apos; classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raicu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Furst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Channin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. The 8th World Multi-Conference on References: Systemics, Cybernetics and Informatics</title>
		<meeting>The 8th World Multi-Conference on References: Systemics, Cybernetics and Informatics</meeting>
		<imprint>
			<date type="published" when="2004-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatialized transfer functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roettger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EuroGraphics -IEEE VGTC Symposium on Visualization</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="271" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tissue classification based on 3D local intensity structures for volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-F</forename><surname>Westin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bhalerao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nakajima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shiraga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kikinis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="160" to="180" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Texture information in run-length matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1602" to="1609" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A novel interface for higherdimensional classification of volume data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-Y</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Lum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="505" to="512" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
