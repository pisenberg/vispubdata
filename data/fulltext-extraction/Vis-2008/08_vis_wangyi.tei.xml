<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Effects of Video Placement and Spatial Context Presentation on Path Reconstruction Tasks with Contextualized Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Bowman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krum</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enylton</forename><surname>Coelho</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonya</forename><surname>Smith-Jackson</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bailey</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Peck</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swethan</forename><surname>Anand</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Kennedy</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yernar</forename><surname>Abdrazakov</surname></persName>
						</author>
						<title level="a" type="main">Effects of Video Placement and Spatial Context Presentation on Path Reconstruction Tasks with Contextualized Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-20T19:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>contextualized videos</term>
					<term>design factors</term>
					<term>user study</term>
					<term>video placement</term>
					<term>spatial context</term>
					<term>tracking</term>
					<term>path reconstruction</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Many interesting and promising prototypes for visualizing video data have been proposed, including those that combine videos with their spatial context (contextualized videos). However, relatively little work has investigated the fundamental design factors behind these prototypes in order to provide general design guidance. Focusing on real-time video data visualization, we evaluated two important design factors-video placement method and spatial context presentation methodthrough a user study. In addition, we evaluated the effect of spatial knowledge of the environment. Participants&apos; performance was measured through path reconstruction tasks, where the participants followed a target through simulated surveillance videos and marked the target paths on the environment model. We found that embedding videos inside the model enabled realtime strategies and led to faster performance. With the help of contextualized videos, participants not familiar with the real environment achieved similar task performance to participants that worked in that environment. We discuss design implications and provide general design recommendations for traffic and security surveillance system interfaces.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>With the ever-increasing number of video cameras and sensors in modern surveillance systems, human operators are challenged by the huge amount of captured information, which often needs to be processed in a limited time. For example, when a suspicious activity is detected, the operator often needs to follow the suspicious person through multiple videos, understand the situation and develop action plans very quickly. One strategy to support rapid decision making is to reduce the amount of information to be processed by human operators. For this purpose, researchers have been exploring automatic video content analysis technologies <ref type="bibr" target="#b20">[21]</ref> to extract information from the videos. Although significant progress has been made in this direction, the high computational cost of these techniques will limit their usage in real-time situations in the near future. More importantly, even though these techniques can significantly reduce the amount of video information to be analyzed by human operators, it is still human operators who need to resolve the ambiguities in the videos, synthesize a wide range of context information with the videos, and make final decisions. Given this, it is important to design interactive visualizations that can support real-time information synthesis and decision making for video surveillance tasks.</p><p>We use the term Contextualized Videos to refer to interactive visualizations that combine videos with their spatial context <ref type="bibr" target="#b17">[18]</ref>. Generally speaking, contextualized videos can integrate spatiotemporal information and other context with videos. In this paper, we focus on contextualized videos that integrate spatial information, which allows observers to see the activities in the videos at their proper locations. Since the spatial relations are presented in the visualization, some cognitive work, e.g., the spatial mapping between the videos and the recall of spatial context can be offloaded onto the perceptual system <ref type="bibr" target="#b15">[16]</ref>. In a traditional video surveillance system that displays a number of video thumbnails or cameos, the operator must maintain a detailed mental model of the building or site and perform numerous mental mappings in order to understand the activities shown in the videos. Previous research has shown that mental registration of multiple views is a challenging cognitive activity <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>Several contextualized video interfaces have been proposed to improve video understanding <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. But most of them focused on proposing innovative designs instead of investigating the fundamental design factors of contextualized videos. In our prior work, we have characterized several design factors common among all the designs <ref type="bibr" target="#b17">[18]</ref>. A deep understanding of these factors can lead to general guidelines valuable to a wide variety of applications involving contextualized videos, e.g., video surveillance interfaces, firefighting control consoles and teleconferencing interfaces.</p><p>Through a user study, we investigated two important design factors: video placement method and spatial context presentation method. Existing visualization guidelines do not fully address the tradeoffs involved in these designs, so a user study is needed to understand the tradeoffs. A path reconstruction task was used in the study. We chose this task because it requires the participants to synthesize information from multiple videos as well as the context information in a limited time.</p><p>Effective design should be compatible with the needs and capabilities of users. To have a comprehensive understanding of the effect of our technology on different populations, our study involved two groups of participants with differing levels of knowledge of the real environment visualized in our system.</p><p>In summary, we investigated the following research questions in this experiment:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>What type of video placement (embedded within an environment model or externally associated with a model) leads to better performance with path reconstruction tasks?</p><p>• To what extent do 2D and 3D models affect user path reconstruction performance?</p><p>• Does the spatial information help participants with little spatial knowledge to achieve a level of performance that is comparable to participants who are familiar with the environment? We compared a total of six contextualized video designs and provide general design suggestions based on the study results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Sawhney et al. demonstrated the feasibility of projecting multiple videos onto a 3D environment model in their Video Flashlight work <ref type="bibr" target="#b10">[11]</ref>. Sebe et al. presented an "Augmented Virtual Environment" system which integrated multiple videos into a 3D context model <ref type="bibr" target="#b12">[13]</ref>. They detected moving objects inside the video and visualized them as textured dynamic rectangles moving around in the 3D model. Several teleconferencing and CSCW systems have placed live videos into collaborative virtual environments <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12]</ref>. All these research projects plotted interesting points in a multi-dimensional design space <ref type="bibr" target="#b17">[18]</ref> and numerous new designs may be invented in the future. Instead of directly comparing these techniques, we extract two basic factors that need to be considered by all the designs and compare the possible choices through a controlled experiment, in order to provide general design guidance.</p><p>Girgensohn et al. used the 2D layout of videos (Spatial Multi-Video player) to show the spatial proximity of the cameras' field of view <ref type="bibr" target="#b3">[4]</ref>. Their user study showed that this design can improve user performance. They also found that a 2D map with camera glyphs led to a similar performance improvement. We took advantage of Girgensohn's result and investigated whether embedding videos inside a map can further improve path reconstruction performance. We also investigated the effect of spatial context presentation. The path reconstruction task we used requires the participants to reconstruct the target individual's path.</p><p>Recently <ref type="bibr">Ivanov et al.</ref> proposed an interactive prototype that allows users to browse and search the video and sensor data recorded by a surveillance system <ref type="bibr" target="#b5">[6]</ref>. They used multiple views to represent both the spatial and temporal context of the videos. Since the target individual's path is mainly captured by motion sensors, human operators need to manually resolve the ambiguity in the data using a tracklet selection control. This is the most sophisticated contextualized video interface we have seen, but it was mainly designed for non-real-time search and data mining tasks.</p><p>All of the systems mentioned so far used either a 2D map or a 3D model of the spatial environment, but none of them compared the two. Some designs, e.g., Video Flashlight <ref type="bibr" target="#b10">[11]</ref>, required a 3D model to work properly and many others did not. Theoretically, both 2D maps and 3D models have their advantages and disadvantages. We directly compared them in our user study. Soh and Smith-Jackson investigated two map design features, i.e., contour representations and color representations, on a wayfinding task <ref type="bibr" target="#b14">[15]</ref>. The task was quite different from video surveillance tasks. Also, the maps were paper-based 2D maps, but their findings revealed users' high dependence on natural contextual cues to perceive map cues.</p><p>Combining videos with environment models shares some characteristics with the use of multiple views for visualization tasks, because a video and the model show different aspects of the same data. Baldanodo et al. summarized eight guidelines for the design of multiple views for visualization <ref type="bibr" target="#b0">[1]</ref>. These design rules do not always lead to a clear answer on which design to use. Tory et al. investigated how to combine 2D and 3D views for volume visualization and spatial relationship tasks <ref type="bibr" target="#b16">[17]</ref>. However, the problem of embedding videos into a bigger context view has not been explored sufficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DESIGN FACTORS AND THE TRADEOFFS</head><p>Two important design factors must be considered when designing contextualized videos. One factor is how to display videos with regard to the environment model. The other is how to visualize the environment model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Video Placement</head><p>When videos and the environment model are both present in an interface, a decision must be made on how to share the display space between them. Videos and the model can be displayed in separate views, as in the Spatial Multi-Video Player interface <ref type="bibr" target="#b3">[4]</ref>. In this case, some association cues, such as color coding and callout lines, can be used to link the video to its location in the model. We call this type of design associated videos. Videos and the model can also be integrated into a single view, as in Video Flashlight <ref type="bibr" target="#b10">[11]</ref>. In this case, the video content is put in the object space of the environment model. We call this type of design embedded videos.</p><p>Embedding videos inside the model reduces the distance between the videos and their context. According to the Proximity Compatibility Principle <ref type="bibr" target="#b18">[19]</ref>, complex tasks that require integration of video and model information are likely to benefit from this design, because the attentional demands are reduced.</p><p>However, embedded videos have two major issues. One is the view distortion problem: the video may be distorted when projected onto the model or observed from a viewpoint far from the central axis of the camera's view direction. The other is the occlusion problem: the video and the model may occlude each other.</p><p>Thus, a controlled experiment is needed to understand the tradeoffs: are embedded videos still beneficial given the disadvantages? Can users understand a complex situation in real time? These questions are interesting, because attentional resources are in high demand during time-critical situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Presentation</head><p>The environment model is an abstraction of the real world and can be presented in different ways. A 2D floor plan as shown in <ref type="figure">Fig. 1</ref> provides a clear overview of the environment, while a 3D model of the floor as shown in <ref type="figure">Fig. 4</ref> allows dynamic observation from different viewpoints and both exocentric and egocentric views. With an angled view, it provides more depth cues. The disadvantages of the 3D representation are occlusion and display clutter, which can be minimized by existing visualization techniques <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8]</ref>. Previous research on task-view dependency did not lead to a clear answer on which visualization to choose. According to the Gestalt similarity principle and Rule of Consistency <ref type="bibr" target="#b0">[1]</ref>, 3D models have the potential to facilitate mapping between the video and the model, because when observed from the camera's viewpoint (an egocentric view) the 3D model's geometry features match those captured by the video. The matching features can help communicate the location and orientation of the person captured by the video. However, it has also been shown that tasks involving spatial understanding favor more exocentric viewpoints like a 2D map or a top-down overview of the 3D model, while tasks involving navigation and tracking favor more egocentric views <ref type="bibr" target="#b19">[20]</ref>. Since the path reconstruction task involves both tracking and spatial understanding subtasks, the tradeoffs can be better understood through an empirical study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Design</head><p>Our experiment included three independent variables: video placement, spatial context presentation, and users' level of spatial knowledge of the environment. For video placement, two levels were selected: pure associated design and pure embedded design. The associated design used callout lines as well as color coding to link videos to their contexts, as shown in <ref type="figure">Fig. 1</ref> (2D associated design) and <ref type="figure">Fig. 4</ref> (3D associated design). As mentioned in the related work section, there are multiple ways to embed videos into the 3D model. Since the purpose of this study was to understand the tradeoffs between basic factors, we chose to use a simple design as shown in Figs. 2 and 5. In the embedded design, the video plane's size was larger than the actual projection plane in order to make them easier to observe. For the same reason, the 3D camera glyphs were placed higher than their actual height. Embedded designs also allowed users to see the spatial context occluded by the videos by pressing the space key, and switch between camera-aligned video and user-aligned video by pressing the "ALT" key. A camera-aligned video is oriented to face the camera so that the orientation of the person and objects in the video can be easily judged. A user-aligned video (or billboard) is oriented toward the user to facilitate viewing. Figs. 2 and 5 compare the two orientation methods.</p><p>The second independent variable was model presentation method, with two levels: 2D model representation and 3D model representation. The 2D design was a simple floor plan of the building <ref type="figure">(Fig. 1</ref>). The 3D design used an angled view of the model, whose walls and doors were rendered in 3D with shading effects so that the floor and walls could be easily differentiated ( <ref type="figure">Fig. 4)</ref>. <ref type="table">Table 1</ref> lists the four conditions created by combining the first two independent variables. We varied video placement and model presentation between subjects because it was difficult to create multiple path reconstruction tasks with the same level of difficulty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1. The four basic designs compared in the experiment</head><p>The 3D embedded design needed some time to learn because the user had to differentiate two cases (as shown in <ref type="figure" target="#fig_0">Fig. 7</ref>) before mentally mapping the scene from the video to the model. In the first case, the observer and the camera are on the same side of the video plane. Mental mapping required very little effort in this case. In the second case, the observer and the camera are on opposite sides of the video plane. Here, mental mapping was difficult because a mental rotation larger than 90 degrees was needed. We used a "mirrored video" metaphor to help reduce the mental effort. The video could be observed on both sides of the video plane. When viewed from the opposite side as the camera, the video screen should be understood as a large mirror. The scene observed on the video screen is the reflection of what is happening in the model.</p><p>Generally, the video size in the embedded condition was smaller than the associated condition, because two factors constrained the video size: (1) all seven videos and their nearby context had to be shown in one display, and (2) videos should not overlap. Moreover, the videos in the 2D embedded condition were smaller than those in 3D embedded condition, because the angled view in the 3D condition allowed a more compact layout of the spatial context. Thus, small video size is an inherent disadvantage of the 2D embedded design.</p><p>In addition to the four basic conditions, we evaluated two combined designs that showed both associated and embedded videos: a 2D combined design ( <ref type="figure">Fig. 3</ref>) and a 3D autorotation design <ref type="figure">(Fig. 6</ref>). The goal of introducing these two designs is to test the hypothesis that embedded and associated designs can compensate each other when combined. Thus, we wanted to compare the 2D combined design with the 2D embedded and associated designs, and the 3D autorotation design with the 3D embedded and associated designs, but we did not plan to compare the 2D and 3D combined designs with each other.</p><p>The 2D combined design is a simple combination of the 2D associated and 2D embedded designs. In the 3D autorotation design, an autorotation function was added to overcome the occlusion and distortion problems of 3D embedded videos. When the user clicks a video, the model will rotate to align the viewpoint with the camera and zoom in to show more details. In this way, the user can clearly see the video and its nearby environment with one click. Mapping between the selected video and the model becomes easy. The user could click either the associated videos or the videos embedded in the spatial context. We tuned the length of the animation to 3 seconds according to user feedback in a pilot study.</p><p>The third independent variable was users' familiarity with the real environment simulated in our system. The resident participants had a high level of spatial knowledge, acquired through tacit and informal learning by working in the building for at least 12 months. Non-resident participants only acquired their knowledge of the building by looking at the floor plan during a guided tour of the building just prior to the experiment. The non-resident participants were used to simulate new security guards who have just started to work in the building. The boredom of this job leads to high turn-over rate and new workers may not be familiar with the environment. We hypothesized that contextualized videos could help.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Tasks</head><p>The path reconstruction task is designed to simulate a real world case involving situation understanding and decision making under time pressure. We simulated a suspicious person tracking situation, where the user not only needs to follow the suspicious person through multiple videos, but also needs to mark on the model the suspicious person's path by clicking a series of dots with a mouse <ref type="figure">(Fig. 1)</ref>.</p><p>Each user did four path reconstruction trials using one of the six visualization designs. In each trial, the target person walked across the building and appeared in 2-4 videos. A total of seven videos were used for each trial and the rest were distractions, where only distracting actors walked in and out. A real surveillance system often contains more videos. But automatic video analysis technologies will be able to significantly reduce the number of ambiguous videos that need to be closely observed by human operators.</p><p>The video footage was shot beforehand at different locations in the real building. To eliminate learning effects, the camera configurations were totally different in each of the four trials.</p><p>The length of each scene used in the trials varied from 45 seconds to 63 seconds. Each scene had 4-6 actors walking around, but only one of them was the target to be tracked.</p><p>The path reconstruction task contained the following subtasks:</p><p>• Scan multiple videos to detect the target person.</p><p>• Discriminate the target from distracters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Mentally map the target's location in the video to his/her location in the spatial context.</p><p>• Predict the next video(s) in which the target is likely to appear and observe those videos.</p><p>• Connect the target's appearance in multiple videos into a continuous path. Multi-tasking between subtasks was possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Participants</head><p>A total of 36 participants, ages 18 to 50, participated in the experiment. Half were classified as residents and half as nonresidents. Color-blind participants were excluded from the subject pool and no preference was given to gender, ethnicity, or national origin. The 18 resident participants had worked inside the building for at least one year. The other 18 non-resident subjects had never been inside the building before. Each group was further divided into six subgroups of 3 participants each to evaluate one of the six designs in <ref type="table">Table 1</ref>. The resident group contained 8 females and the non-resident group contained 5 females. We controlled the gender distribution between subgroups so that each subgroup of 3 participants contained at most 2 females.</p><p>Although more participants would be desirable, the number of resident users was limited, and some of them were excluded from the pool because of prior experience with the interface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Procedure</head><p>Prior to the experiment, each non-resident user was given a printout of the building's floor plan marked with several waypoints and then led on a short tour of the actual site, which was a floor of our research building. During the tour, we did not direct participants' attention. The only thing we pointed out for the participants were the physical locations of the waypoints labelled on the floor plan. Next, each participant was given a pre-questionnaire followed by the ETS standard Cube Comparisons Test to evaluate the participants' mental rotation ability, which was likely to have a significant impact on overall performance.</p><p>The participant was then seated at a desktop computer running the testbed software. During the training session, we first demonstrated how to use the visualization to track people. Then the participants were given a familiarization trial to learn to use it effectively. We showed participants the correct answer as well as how much precision was required for indicating the path. We asked the participants questions to make sure they correctly understood the designs. The entire training session was controlled to last no longer than 15 minutes.</p><p>Following the training session, the participants completed four experimental trials. We showed the participant a picture of the target individual before each trial began. The participant then observed the videos, followed the target individual, and indicated the path taken by the target by making a series of mouse clicks on the model. The participant could complete this task either while the videos were playing or after they ended. The task time and path was automatically recorded by the testbed. The investigators manually noted the participants' behavior and strategy. <ref type="figure">Fig. 1</ref>. Testbed Interface using the 2D associated design. In the 2D associated design, the camera glyphs indicated the camera's location in the building model and the short line on the camera glyph indicated where the camera was facing. The testbed provided a "Replay" button allowing the user to replay the video multiple times, and a "Confirm" button allowing the user to confirm the path of the target actor and end the session. The elapsed time since the start of the task was shown in the lower-right corner. <ref type="figure">Fig. 2</ref>. 2D embedded design. Left: By default videos were placed to face the camera on a 2D plane. In this camera-facing view, the video content was harder to observe, but mapping the actor's location from the video to the model was easier. Right: When participants pressed the "ALT" key the videos were rotated to an upright position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Markers placed by participants to indicate the target individual's path</head><p>Camera glyphs, the short line indicates where the camera is facing Call-out lines that link videos to the corresponding cameras <ref type="figure">Fig. 3</ref>. 2D combined design that integrated 2D embedded videos and 2D associated videos. <ref type="figure">Fig. 4</ref>. 3D associated design <ref type="figure">Fig. 5</ref>. 3D embedded design. The embedded videos were enlarged to make them easier to observe. Left: By default videos were placed to face the camera. Right: When participants pressed the "ALT" key the videos were rotated to face the user. <ref type="figure">Fig. 6</ref>. 3D combined (autorotation) Design, The embedded videos were enlarged to make them easier to observe. Here, the user has clicked on the yellow video.  During each trial, the participants were able to replay the video clips once they ended, but were not allowed to pause the videos. However, they could click the model and specify the location and orientation of the target at any time. During the pilot study, we found that some scenes were hard to understand in a single viewing. Thus, we allowed replay to give the users another opportunity to finish the task, but we did not want the user to pause and replay at any time, because this would lead to too much variation in user strategy.</p><p>The participants were instructed to complete the task as quickly as possible as long as they felt the result was correct. A maximum of three minutes was allowed to perform the task. In most cases, participants finished the tasks before the time ran out.</p><p>On completion of their experimental trials, the participants were interviewed using two oral questions. The first question asked the participants to explain the path of the target actor in the final trial. The second question asked the participants to give a brief account of any landmarks or cues that assisted them in performing the tasks and to what extent these landmarks or cues assisted them.</p><p>Finally, we asked participants' to rate (on a five-point scale) the design they used with regard to ease of learning, ease of use, enjoyment of use and the usefulness of the visualization in a real video surveillance system. A self-reported mental workload evaluation was administered last using simplified SWAT <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7]</ref>. The participants also commented as to which parts of the tasks seemed to cause the highest mental effort.</p><p>RESULTS AND DISCUSSION</p><p>We measured user task time and the precision of the path as performance criteria. The task performance reflected how precisely and quickly people can map the position and orientation of the target person seen in the videos onto the real environment, as well as how well they can understand the scene as a whole across multiple videos. We graded the precision of the path on a six-point scale, with six indicating the most precise path. Since we could not guarantee the four tasks were of equal difficulty, we normalized the task time before taking the average as that participant's task time so that all four tasks have equal contributions. To normalize the task time, we first calculate a normalization factor for each of the four tasks by dividing a constant with the mean task time across all the participants. Then we multiply each individual's task time with these factors. After the normalization, the average task time was equal for all the four tasks. We had two goals in the experiment. The first goal was to understand the two basic design factors (video placement and spatial context presentation), and the second goal was to see whether the combined designs were better than the pure embedded or pure associated designs. Therefore, the performance data were analyzed with two different between-subject ANOVAs. The performance data of the four basic designs were analyzed with a 2 (video placement: associated, embedded) x 2 (spatial context presentation: 2D, 3D) x 2 (user spatial knowledge: resident, non-resident) factorial ANOVA. Then all six designs were analyzed with a one-way ANOVA without investigating the interaction between factors. Chi-Square tests were used to analyze the subjective rating of the designs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Effect of Video Placement</head><p>The factorial ANOVA showed that video placement had a significant effect on task time (F(1,16)=4.63, p&lt;.05) as shown in <ref type="figure">Fig. 8</ref>, but no significant effect was found on task precision <ref type="figure">(Fig. 9)</ref>. A post hoc analysis using Tukey's HSD test showed that the embedded condition was significantly faster than the associated condition (p&lt;.05). We also compared the two combined designs to the four basic designs using a one-way ANOVA and Tukey's HSD post hoc test, but did not find a significant difference between the conditions (F(5,30)=1.63. p=0.18 for task time and F(5,30)=0.93. p=0.46 for precision). This might be due to the small group size.</p><p>The performance difference between the embedded and associated video designs can be partially explained by the participants' behavior. A correlation analysis showed that the task time was significantly correlated with the user strategy when performing the tasks (p&lt;.05). A Chi-Square test showed that the user strategy was significantly different between the associated and embedded videos (p&lt;.05). Nine out of 12 participants using embedded designs, and ten out of 12 using combined designs, indicated the path in real time while the video was playing the first time, but only four out of 12 participants using associated designs did so. The rest of the participants observed the videos once and indicated the path on the model after the videos ended. We believe embedded videos reduced the time needed to link a video to its model context, and therefore better supported real-time tracking.</p><p>The advantage of the real-time strategy is the user's more precise recall of the target's location when marking it on the model. Otherwise, users have to rely on memory or watch the videos again. However, the real-time strategy also requires the participants to map the target's position to the model in real time. We observed several cases when a user using embedded designs missed an important video because he/she spent too much time on mapping the other video to the model. Nevertheless, precision with the embedded designs was not statistically different than precision with the associated designs.</p><p>The self-reported mental workload with embedded designs was slightly higher than that with associated designs as shown in <ref type="figure">Fig 10  (F(1,16</ref>)=2.10, p=0.17). This trend was consistent among all three components of mental workload: mental effort load, time load and psychological stress. We think this is because the participants using embedded designs performed multiple subtasks simultaneously. The small video size and non-orthogonal view angles in the embedded design may have also increased the mental effort of discriminating a person from a distracter, as was pointed out by several participants. None of the participants using associated video reported video observation to be a problem. A third contribution to increased mental workload might be the more complex interfaces of the embedded designs. We observed that a few participants forgot to use some of the functions (e.g. pressing the space key to see the 3D model occluded by the video) while doing the tasks.</p><p>Chi-Square tests did not show significant differences between the participants' subjective ratings of the video placement designs <ref type="figure">(Fig.  11</ref>). This might be due to the small sample size in our study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Effect of Spatial Context Presentation</head><p>Spatial context presentation was not found to have a significant effect on task time and precision. The insignificant performance difference between the 2D and 3D conditions indicate that 3D walls and angled view of the model are not likely to be the main factors that makes 3D condition a good or bad choice.</p><p>From <ref type="figure">Fig. 8</ref> we observed that 3D designs have higher variability in task time. The reason might be that some participants quickly learned how to use the 3D designs effectively, while others did not fully grasp the 3D designs in such a short time. On the contrary, all the participants understood the 2D designs quickly.</p><p>According to participants' subjective ratings <ref type="figure">(Fig. 11</ref>), 3D designs were slightly harder to use (F(1,16)=3.12, p=0.10). However, the mental workload between the 3D and 2D designs was not found to be significantly different. 3D presentation of the environment might have helped in landmark identification and mapping, but the walls in the 3D view sometimes occluded part of the path, making it more difficult to estimate and mark the path.</p><p>3D embedded and 3D autorotation designs resulted in the highest mental workload and were rated the hardest to use, but the difference was not significant. According to user feedback and our observation, the hardest part of using the 3D embedded design was understanding the actors' real direction in the model when the observer were looking from the opposite direction of the camera's view. In this case, the scene in the video was mirrored as shown in <ref type="figure" target="#fig_0">Fig 7.</ref> Although we explained this to the participants and let them practice before the real task, this still led to confusion for some participants. The hardest part of 3D autorotation was to keep track of the person in the video while the model was rotating. Overall, it seems that the 3D designs we used have more negative effects than positive. Despite the insignificantly higher mental workload, 3D visualizations received a significantly higher score than 2D visualizations (p&lt;0.05 for nonzero correlation) on the "usefulness" question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Interactions of Design Factors</head><p>No significant interaction was found between the video placement and spatial context presentation factors. Thus, the two factors were relatively orthogonal. This might be a reason that we were able to find significant differences between conditions even with a relatively small number of subjects in each subgroup.</p><p>However, a separate analysis revealed one interesting local trend. 2D associated videos generally outscored 2D embedded videos in all four subjective ratings. This means although participants performed better in the 2D embedded condition, they were not as satisfied with the interface. Participants were not comfortable with the unaligned video orientation in the 2D embedded condition. Also, the 2D embedded condition had the smallest videos among all the six designs. Several participants reported that the small videos in 2D embedded condition were hard to observe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Effect of Participants' Spatial Knowledge</head><p>Interestingly, no significant difference was found between resident participants and non-residents on either task time or precision metrics. This indicates that non-resident participants might have benefited greatly from contextualized videos, because they are likely to have trouble performing the tasks without a visualization that links the videos to their spatial context.</p><p>With 3D visualizations, the difference between resident and nonresident participants was greater, in terms of both task time and precision. The advantage for residents was less obvious in 2D visualizations. Resident participants also showed more advantage, in terms of both task time and precision, in combined designs. This implies that resident participants can utilize a more abundant set of landmarks about the building. Thus, we should provide more cues for resident users, instead of oversimplifying the designs.</p><p>This result is consistent with the participants' subjective ratings of "ease of learning" and "ease of use." Non-resident participants clearly preferred 2D visualizations, while resident participants showed less strong preference. This might be caused by the different forms of spatial knowledge in the minds of the two groups. Nonresident participants held a 2D map in hand when they toured around the site, thus the spatial information was likely to be stored in 2D in their minds. Resident participants were not trained with the 2D map before doing the tasks. They already had a comprehensive representation of the site in their minds due to their experience of working in the building. Thus, they could easily map any external form of spatial information into their mental model of the building.</p><p>It is interesting to see that the self-reported workload between the two user groups was very similar. Since the resident participants were more familiar with the building, their mental workload should have been lower. While a difference might be observed with a larger number of participants, nevertheless we think that the mental workload similarity between the two groups indicates that both groups were mainly relying on the external context presentation instead of the one in their minds. We observed several resident participants who wanted to look at each video for some time before each task began. They said "I am trying to understand where is where." In other words, they tried to register the camera location and orientation with their mental map. On the contrary, none of the nonresident participants tried to do so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Interview Results</head><p>In order to understand the difference between the two user groups' spatial memory, we interviewed them after the experiment. The recorded interviews were analyzed to elicit their spatial knowledge.</p><p>The interview consisted of two questions for both types of participants. The first question asked the user to orally describe the path that the target individual took in the last task. The second question asked the user to enumerate what kinds of cues were used in reconstructing the path.</p><p>The interviews were recorded and transcribed. Following the approaches described in <ref type="bibr" target="#b9">[10]</ref>, we counted the number of words in answering the questions. The word-count analysis found that the resident participants had more to say in answering both questions, but no significant difference was found because of the large variation between individuals. It seemed that resident participants would take time and give a detailed account of the path as it would require information retrieval from long-term memory and processing at a conscious level which otherwise would be done sub-consciously in their daily routine. Because most interviews were given by resident experimenters, we cannot rule out the possibility that resident participants have the tendency to have a longer conversation because they know the experimenters.</p><p>Following the word-count analysis, we then analyzed the key words in the answers to determine the participants' frames of reference and landmark references when describing the path. The frame of reference can be either the virtual model or the real world. The landmark references can be either physical attributes, e.g., "the large open area" or "the red camera", or socio-functional attributes, e.g., "David's office" or "the kitchen". While answering the question regarding the actor's path, resident participants referenced the real world more often than non-residents. Residents also referred to more landmarks. While the number of physical landmarks referenced by the two groups was similar, resident participants referred to many more socio-functional landmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Other Findings</head><p>Although the designs were early prototypes, all the designs were rated useful for the path reconstruction tasks, as shown in <ref type="figure">Fig. 9</ref> (a usefulness rating of five means the most useful, while one means not useful at all).</p><p>We also found that participants' spatial rotation ability correlated with task time (p=.09), but there was no obvious correlation with precision. The precision of the path reconstruction task does not solely rely on participants' spatial rotation ability to map individual videos to the model. Even if participants failed to map correctly, they could still estimate a reasonable path between the videos according to the target's sequence of appearance in the videos. Unfortunately, we could not tell when each strategy was used in the experiment. Gender did not show a significant correlation with task performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS AND FUTURE WORK</head><p>We compared the effect of two contextualized video design factors and two knowledge levels of participants on performance in a simulated video surveillance task. Generally, the contextualized video visualizations helped nonresident participants in the path reconstruction task, as their performance was not significantly worse than resident participants. In addition, no interaction was found between the video placement and spatial context presentation design factors.</p><p>Specifically, the experiment indicates the following design recommendations:</p><p>• Embedding videos into the model supports and encourages a real-time strategy. Thus, embedded designs should be used in time-critical situations where replay of the video is not practical.</p><p>• 2D associated videos are relatively simple and easier to learn. Thus, they are more appropriate for short-term use.</p><p>• Users with a high level of spatial knowledge of the real environment can utilize a more abundant set of landmarks and can think of the situation in the real environment. Thus, more cues can be presented in the interfaces for such users. Since all the participants were essentially novices with respect to the prototyped designs, the above findings are mainly useful for nonexpert users who have limited experience with the interface.</p><p>The prototyped designs are created to understand the basic design factors. When putting the prototypes in real use, many usability improvements can be made. For example, a Video Flashlight can be added in addition to the video screen, so that the direction and the coverage area of the camera can be easily perceived. Some participants using the associated designs suggested allowing the labeling of video sequences in which the target appeared.</p><p>This experiment did not show any significant differences in task precision because we instructed the participants to prioritize precision over time, and because the precision metric was not very sensitive. Allowing participants to indicate the path after the video ended often led to the highest possible precision score. We plan to do a follow-up experiment to address this limitation. For example, we may force the participants to specify the path while the videos are playing.</p><p>The setting of this experiment forced the participants to indicate the path in the virtual environment model. But in many situations, they need to navigate in the real environment according to the learned route. We would like to investigate how well users can reconstruct the path at the real site in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 7 .</head><label>7</label><figDesc>The two different observation cases in the 3D embedded and 3D autorotation Designs. Since the user is behind the yellow camera, the yellow video shows exactly what the camera sees. But since the user is on the opposite side of the red video from the red camera, the video canvas should be understood as a mirror reflecting what is happening in the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 8 .Fig. 9 .</head><label>89</label><figDesc>Comparing task time among designs. The numbers above the bars show the mean task time for each design in seconds. Comparing task precision score (0-6 scale) among designs, higher score means higher precision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 10 .Fig. 11 .</head><label>1011</label><figDesc>Subjective mental workload score of different designs Subjective rating of the designs on a 0-5 scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>•</head><label></label><figDesc>Yi Wang, Doug Bowman, Tonya Smith-Jackson, David Bailey, Trevor Kennedy, Sarah Peck, Swethan Anand and Yernar Abdrazakov are with Virginia Tech, E-Mail: {samywang, dbowman, smithjack, baileyd, speck, swethan, tkennedy, yabdraz}@vt.edu.</figDesc><table /><note>• David Krum and Enylton Coelho are with the Robert Bosch Research and Technology Center. E-Mail: {David.Krum, Enylton.Coelho}@us.bosch.com. Manuscript received 31 March 2008; accepted 1 August 2008; posted online 19 October 2008; mailed on 13 October 2008. For information on obtaining reprints of this article, please send e-mail to: tvcg@computer.org.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work was supported by a grant from the Robert Bosch Research and Technology Center.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Guidelines for Using Multiple Views in Information Visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Q W</forename><surname>Baldonado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Woodruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuchinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Working Conference on Advanced Visual Interfaces</title>
		<meeting>Working Conference on Advanced Visual Interfaces</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Alternative Approaches to Analyzing SWAT Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Biers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Masline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Factors Society 31st Annual Meeting</title>
		<meeting>the Human Factors Society 31st Annual Meeting</meeting>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="page" from="63" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Taxonomy of 3D Occlusion Management Techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tsigas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Virtual Reality</title>
		<meeting>IEEE Virtual Reality</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Effects of presenting geographic context on tracking activity between cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Girgensohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wilcox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGCHI</title>
		<meeting>SIGCHI</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1167" to="1176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">CU-SeeMe VR immersive desktop teleconferencing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th ACM International Conference on Multimedia</title>
		<meeting>4th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="199" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Visualizing the History of Living Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kaur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Simplified subjective workload assessment technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Luximon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Goonetilleke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ergonomics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="229" to="243" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Using deformations for browsing volumetric data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Mcguffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tancau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Balakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Visualization</title>
		<meeting>IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">53</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cognitive Load and Mental Rotation: Structuring Orthographic Projection for Learning and Problem Solving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Pillay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Instructional Science</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="91" to="113" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Conversation Analysis: The Study of Talk-in-Interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Psathas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>SAGE Publications, Inc</publisher>
			<pubPlace>Thousand Oaks, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Video Flashlights: Realtime Rendering of Multiple Videos for Immersive Model Visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arpa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samarasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th Eurographics workshop on Rendering</title>
		<meeting>13th Eurographics workshop on Rendering</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="157" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Moving Office: Inhabiting a Dynamic Building</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schnädelbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Penn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Steadman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Benford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Koleva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rodden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM CSCW</title>
		<meeting>ACM CSCW</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="313" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3D Video Surveillance with Augmented Virtual Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">O</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">First ACM SIGMM International Workshop on Video Surveillance</title>
		<imprint>
			<biblScope unit="page" from="107" to="112" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mental Rotation of Three-Dimensional Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">N</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">171</biblScope>
			<biblScope unit="page" from="701" to="703" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Influence of Map Design, Individual Differences, and Environmental Cues on Wayfinding Performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Smith-Jackson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Spatial Cognition and Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="166" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Illuminating the Path: The Research and Development Agenda for Visual Analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Cook</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>IEEE Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visualization Task Performance with 2D, 3D, and Combination Displays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Atkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Moller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="page" from="2" to="13" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Contextualized Videos: Combining Videos with Environment Models to Support Situational Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Coelho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2007-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The proximity compatibility principle: its psychological foundation and relevance to display design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Wickens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Carswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Factors</title>
		<imprint>
			<biblScope unit="issue">37</biblScope>
			<biblScope unit="page" from="473" to="494" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Engineering Psychology and Human Performance, Third Edition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Wickens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Hollands</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Issues in video analytics and surveillance systems: Research / prototyping vs. applications / user requirements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Advanced Video and Signal Based Surveillance</title>
		<imprint>
			<date type="published" when="2007-09" />
			<biblScope unit="page" from="10" to="14" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
