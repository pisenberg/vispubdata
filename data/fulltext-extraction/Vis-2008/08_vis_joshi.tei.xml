<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Novel Interaction Techniques for Neurosurgical Planning and Stereotactic Navigation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2008-10-19">19 October 2008</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alark</forename><surname>Joshi</surname></persName>
							<email>alark.joshi@yale.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Scheinost</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">P</forename><surname>Vives</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><forename type="middle">D</forename><surname>Spencer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Lawrence</forename><forename type="middle">H</forename><surname>Staib</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Xenophon</forename><surname>Papademetris</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">â€¢</forename><surname>Alark</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Diagnostic Radiology</orgName>
								<orgName type="institution">Yale School of Medicine</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Neurosurgery, Yale School of Medicine</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Diagnostic Radiology</orgName>
								<orgName type="department" key="dep2">Department of Biomedical Engineering</orgName>
								<orgName type="department" key="dep3">Department of Electrical Engineering at Yale University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Department of Diagnostic Radiology</orgName>
								<orgName type="department" key="dep2">Department of Biomedical Engineering at Yale University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Novel Interaction Techniques for Neurosurgical Planning and Stereotactic Navigation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2008-10-19">19 October 2008</date>
						</imprint>
					</monogr>
					<note type="submission">received 31 March 2008; accepted 1 August 2008; posted online</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-20T19:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>User interaction, irregular cropping</keywords>
			</textClass>
			<abstract>
				<p>Neurosurgical planning and image guided neurosurgery require the visualization of multimodal data obtained from various functional and structural image modalities, such as Magnetic Resonance Imaging (MRI), Computed Tomography (CT), functional MRI, Single photon emission computed tomography (SPECT) and so on. In the case of epilepsy neurosurgery for example, these images are used to identify brain regions to guide intracranial electrode implantation and resection. Generally, such data is visualized using 2D slices and in some cases using a 3D volume rendering along with the functional imaging results. Visualizing the activation region effectively by still preserving sufficient surrounding brain regions for context is exceedingly important to neurologists and surgeons. We present novel interaction techniques for visualization of multimodal data to facilitate improved exploration and planning for neurosurgery. We extended the line widget from VTK to allow surgeons to control the shape of the region of the brain that they can visually crop away during exploration and surgery. We allow simple spherical, cubical, ellipsoidal and cylindrical (probe aligned cuts) for exploration purposes. In addition we integrate the cropping tool with the image-guided navigation system used for epilepsy neurosurgery. We are currently investigating the use of these new tools in surgical planning and based on further feedback from our neurosurgeons we will integrate them into the setup used for image-guided neurosurgery.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The surgical treatment of epilepsy typically involves first an intracranial electrophysiological study, where electrodes are implanted into the brain for invasive monitoring of seizure electrical activity. Should the first study successfully localize the onset of seizures in an area of the brain that can be resected, then a second resection surgery is performed. Both surgeries are planned based on extensive multimodal structural and functional image acquisitions, such as magnetic resonance imaging (MRI) functional MRI (fMRI), single positron emission computed tomography (SPECT) and magnetic resonance spectroscopic imaging (MRSI).</p><p>All of these imaging acquisitions are coregistered to bring them to a common coordinate system and are used for both surgical planning as well as in the operating room during image-guided navigation. This planning process requires detailed visualization of both the imaging studies and any intracranial electrode reconstructions. These visualizations are also used to guide the actual surgery using imageguided navigation procedures to enable the targeting or avoidance of key anatomical and functional areas. In the case of intracranial electrode reconstructions, these visualizations allow the physicians to see what anatomical structure correspond to the location of the measured electrophysiological activity (the gold standard for seizure focus localization) and allow the surgeon to map the resection procedure accordingly. Increasingly, detailed visualizations are becoming an integral and critical part of neurosurgery.</p><p>In order for the visualization to be useful, the physicians need to be able to visualize brain anatomy without any unimportant structures occluding their clear view of the underlying structures. Up until now, the commercial image-guided navigation systems for neurosurgery use for the most part, simple axis-aligned cropping of 3D volumes in conjunction with 2D slice visualizations to explore and visualize regions of interest in the brain. This can lead to confusing orientations of 3D volumes and cropping which does not show the full extent of the featured data. This is particularly the case when the image data needs to be visualized simultaneously with the intracranial electrode localizations which are by nature non-planar and do not show up well in any orthogonal axis view.</p><p>Neurosurgeons at our hospital currently use an image guided navigation system that coregisters the patients' head position to the preoperatively acquired imaging data. The system then tracks surgical tools and shows their position, in real-time, on the coregistered images allowing for image-guided navigation. While the system mostly uses conventional 2D slices we have extended the system, via a research interface, to allow for the probe to be seen in 3D in conjunction with a volume rendering of the coregistered images. To get to the region, the surgeons use a combination of experience and the navigation system.</p><p>Bioimage Suite <ref type="bibr" target="#b7">[8]</ref>, our open source software program is used heavily in surgical planning and all phases of surgery. All the registration between patient MR data and the functional data are performed in Bioimage Suite.</p><p>We propose novel techniques to visualize regions of interest using cropping tools that allow irregular cropping of the brain anatomy. We have developed four specific cropping techniques that allow for easy interaction and effective visualization of the region of interest without removing too much of the surrounding anatomy. The interaction techniques provided to perform irregular cropping make it easy to use the cropping facility. The first interaction technique that we have proposed allows the user to place a cursor in 3D, which on being moved performs irregular cropping on the brain. The second interaction technique, provides a line widget, the ends of which can be controlled by the user. The region around one end of the line widget is cropped and allows for a line of sight to the region being cropped.</p><p>Our newly developed visualization methods are being integrated into BioImage Suite. A feature of BioImage Suite is its integration with the BrainLAB VectorVision Cranial Image-Guided Navigation system that is used clinically for image-guided neurosurgery via the <ref type="figure">Fig. 1</ref>. A schematic of the overall system. This consists of our own BioImage Suite software (shown on the left as "system"), which interfaces with the commercial BrainLAB VVCranial image-guided navigation platform, using the VVLink research interface integrated into VVCranial. The research interface allows our system to obtain real time tool/instrument coordinates. Our system in turn can stream visualizations to the BrainLAB system -see <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>VectorVision Link (VVLink) research interface (see Papademetris et al <ref type="bibr" target="#b8">[9]</ref>.) VVLink was designed to allow for easy integration of research functionality into the commercial system via a network protocol, whose functionality is briefly outlined in <ref type="figure">Figure 1</ref>. In particular, using VVLink, the research software can obtain real-time surgical tool positions from the VVCranial system. We have already used such a setup in the operating room to help visualize multimodal image acquisitions for over 60 surgeries; in this work we prototype the use of the newly developed techniques in this environment.</p><p>The rest of the paper reads as follows. First we review related work in Section 2. We describe our newly developed methods in Section 3this includes an overview of the overall system (Section 3.1). We then present experimental evaluation results in Section 4, where the visualization is interfaced and controlled using an image-guided navigation setup that is an exact replica of what is currently used in the neurosurgery operating rooms in our institution. Finally we present some concluding remarks in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Cropping has been a way to provide viewers with context by removing irrelevant details. Early research in NPR techniques by Seligmann and Feiner <ref type="bibr" target="#b12">[13]</ref> introduced the notion of drawing the user's attention to a region of interest by cropping out irrelevant outer layers and providing sufficient context around it by irregular cropping. In the field of volume visualization, Weiskopf et al. <ref type="bibr" target="#b15">[16]</ref> introduced the notion of irregular interactive clipping techniques for visualizing volumetric data. They used graphics hardware to accelerate clipping for texturemapping based volume rendering. Their technique allowed for complex geometrical clipping but user interaction and its real-world applicability were not demonstrated in their work. Xie et al. <ref type="bibr" target="#b16">[17]</ref> adapt and extend their techniques specifically for medical data and allow for irregular geometric clipping. Their clipping does not facilitate any specific utility and the controls of the clipping are not explained at all. Additionally, the size of the datasets used in their system are modest as compared to real world datasets that we use for surgical planning and treatment.</p><p>Fischer et al. <ref type="bibr" target="#b2">[3]</ref> proposed the use of augmented reality based systems to allow free form interaction in image guided surgery systems. Pflesser et al. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b5">6]</ref> discussed volume visualization aspects for interactive surgical planning and rehearsal. Gering et al. <ref type="bibr" target="#b3">[4]</ref> tracked a surgical tool and interactively showed two intraoperative MR slices orthogonal to the tool tip. Butz et al. <ref type="bibr" target="#b0">[1]</ref> used probe based surgical treatment planning and rehearsal for treating percutaneous tumors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODS</head><p>In order to allow effective exploration and visualization of medical data, the three-dimensional visualization of that data proves extremely effective. Researchers have proven that humans can obtain an increased spatial understanding using three-dimensional imagery as compared to two-dimensional views of the same <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14]</ref>. This is particularly the case when visualizing the reconstructions of the geometry of intracranial electrodes. The electrodes are often implanted as strips or grids which are deformed during implantation and are not co-planar with any of the standard views (e.g. axial, coronal or sagittal). Correct visualization of the electrodes requires the use of 3D visualizations.</p><p>Most often though the data of interest to surgeons even in threedimensional visualizations is occluded by irrelevant detail and they would like to visualize regions of interest. We present techniques that allow surgeons to interact quickly with patient data for improved exploration and understanding. We have extended the line widget available with the Visualization Toolkit (VTK) <ref type="bibr" target="#b11">[12]</ref> and added functionality to allow surgeons control over the shape and size of the regions that they would like to crop. Our system supports spherical, cuboidal, ellipsoidal and probe-aligned cylindrical cropping shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">System Overview</head><p>We implement our visualization tools within the context of our open source image analysis software package: Bioimage Suite <ref type="bibr" target="#b7">[8]</ref>. This is implemented in a combination of scripting languages (Tcl) and C++ and makes heavy use of VTK <ref type="bibr" target="#b11">[12]</ref>.</p><p>The basic viewer of our system is shown in <ref type="figure" target="#fig_1">Figure 3</ref>. This can be used to visualize images (and surface data) in various 2D and 3D modes. One of the modes includes the visualization of the slices in the familiar Axial, Coronal and Sagittal views in addition to the 3D volume rendering of the same. The cross hairs shown in the 2D images are synchronized to allow users to localize a specific region in 2D. But to visualize the same region in 3D they need to use regular axis-aligned cropping. Axis-aligned cropping can allow for removal of certain uninteresting regions, but it often ends up removing too much of the surrounding region and at some point reduces to a 2D visualization.</p><p>As mentioned in the introduction our system can interface to the  BrainLAB VVCranial Image-Guided Navigation system that is used clinically for image-guided neurosurgery via the VVLink research interface. An outline of the interface is shown in <ref type="figure">Figure 1</ref>. Using VVLink, our system receives real time tool coordinates that are tracked by the BrainLAB system. Visualizations generated on our system are streamed to the BrainLAB system for interaction and visualization purposes, an example is shown in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>The Modified VTK line widget: We first describe the line widget and the various modes and functionalities. The line widget was chosen as an optimal choice after some experiments with various kinds of widgets like probe widget, where the center of the probe defines the center of the cutting region. We found in pilot experiments that our users would frequently lose the handle on the cutting probe as the probe got further into the 3D image.</p><p>The line widget is identified by two end points. One endpoint, which we call the cropping endpoint, defines the center of the cropping region while the other endpoint (called the control endpoint) serves multiple purposes depending on the cropping shape selected. For example, in the case of a cube, the length of the cube is defined by the distance between the two endpoints of the line widget. Furthermore, the distance measurement in millimeters, is of considerable importance to surgeons during planning and surgery. Our line widget computes and displays the distance between the two endpoints interactively. <ref type="figure" target="#fig_2">Figure  4</ref> shows a screenshot of only the line widget with the cropping endpoint (shown in red) and the control endpoint (shown in white). The distance between the two endpoints is shown at the bottom left.</p><p>The location of the cropping endpoint of the line widget can also be controlled using the cross hairs in a typical 3-slice view (coronal, axial sagittal view. In our 3slice+3D Mode shown in <ref type="figure" target="#fig_1">Figure 3</ref>, they can control the location of the cross hairs (red, green, blue lines) in any slice. The synchronized cross hairs allow specialists to localize and closely examine a region in 2D. In this mode, our cropping endpoint is synchronized with the cross hairs. This allows them to visualize the region of interest without removing too much of the detail around it. In our focus + context mode, discussed later, we can also highlight the selected region and show an outline of the surrounding region to provide context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Interactive Irregular Cropping</head><p>In order to allow for exploration of multimodal data in 3D, we developed irregular cropping. The cropping is performed in the compositing stage of the raycasting process. In the raycasting process as the ray traverses through the volume the opacity of the voxel under consideration is determined, based on its location in the volume space.</p><p>Our system allows for interactive cuts of various shapes based on the need of the user at that time. The aspect of interactivity is crucial, since the lack of interactivity hampers the ability to explore the data in 3D and obtain sufficient insight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Spherical Cut</head><p>In order to allow the user to perform a spherical cut into the data, we determine the distance of the ray's current position from the center of the sphere. The sphere radius and center are parameters that can be manipulated by the user. The center of the sphere can be controlled by the cross-hairs location or manually by using the line widget. Along the ray the following equation is used to compute the opacity of the voxel during the raycasting process. If the current position of the ray is within the influence of the sphere, the opacity of that voxel is set to zero.</p><formula xml:id="formula_0">O(P(i)) = 0 dist(P(i) âˆ’ SC) &lt; SR T F(scalar) dist(P(i) âˆ’ SC) &gt; SR</formula><p>In the above equation, P(i) is the current location of the ray and O(P(i)) specifies the opacity of the current voxel location based on the result of the evaluation of the equation. SC stands for the center of the sphere and SR stands for the radius of the sphere. Both the parameters of the sphere (radius and the location of the center) can be controlled by the user.</p><p>Using this modification to the raycasting process, we were able to generate visualizations such as those shown in <ref type="figure">Figure 5</ref>.</p><p>Our line widget allows us to control the position of the center of the spherical cropping region. In order to provide some cues regarding the size of the spherical region that will be cut, we provide a wireframe rendering of the spherical region as shown in the left image in <ref type="figure" target="#fig_3">Figure 6</ref>. The right image shows a volume rendering of the result of the spherical cropping with the wireframe disabled. The region that has been cropped out is rendered using distance color blending <ref type="bibr" target="#b10">[11]</ref> to provide depth and mainly to differentiate the region from the rest of the rendered brain.</p><p>We found in initial experiments with controlling the radius of the sphere by the distance between the endpoints that we were unable to obtain small spherical cut outs without having to fine tune the location of the endpoints. To address that issue, we now have a separate slider bar that controls the radius of the sphere, as shown at the bottom of <ref type="figure" target="#fig_3">Figure 6</ref>. The cropping endpoint specifies the center of the sphere and the slider bar specifies the radius.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Cubical Cut</head><p>In some cases, the removal of a cubical region of the brain is more suitable since the surgeons would like to look at regions that showed fMRI activation without removing too much of the brain, thus allowing examination of surrounding anatomy. To facilitate this, we provide cubical removal of regions. The center of the cube and the length of diagonal of the cube are the parameters that are used. These parameters control the size of the cropping cube. <ref type="figure">Figure 7</ref> shows two such <ref type="figure">Fig. 5</ref>. A spherical region of the brain has been removed allowing us to look into the brain. The location of the center of the sphere can be controlled by our line widget and a slider bar on the user interface provides a user with the ability to vary the radius of the sphere. examples of cubical cuts into the brain. During the raycasting process, the opacity for the voxels that lie within the cube is set to zero, thus allowing for cubical cropping. The following equation is evaluated along the ray,</p><formula xml:id="formula_1">O(P(i)) = ï£± ï£² ï£³ 0 P(i) x âˆ’CC x &lt; CL x and P(i) y âˆ’CC y &lt; CL y and P(i) z âˆ’CC z &lt; CL z T F(scalar) dist(P(i) âˆ’ SC) &gt; SR</formula><p>The line widget is also used to control the parameters of the cube. When the selected crop shape is cubical, the distance between the endpoints of the line widget is set to half the distance of the diagonal. The cropping endpoint specifies the center of the cube and the control endpoint specifies one of the corners of the cube. <ref type="figure">Figure 8</ref> shows a screenshot of the line widget controlling the parameters of the cube. A wireframe of the cube is shown to delineate it from the surrounding brain region. The wireframe rendering of the cube can be disabled to produce renderings such as those in <ref type="figure">Figure 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Cylindrical Cut</head><p>The user can obtain cylindrical cuts in order to explore the data better. A region in the shape of a cylinder is cut out of the brain. During the <ref type="figure">Fig. 7</ref>. The images above demonstrate two cuboidal cuts possible using our system. <ref type="figure">Fig. 8</ref>.</p><p>Line widget used for exploring using cubical cuts. The cropping endpoint of the line widget defines the center of the cube, while the distance between the endpoints defines half the diagonal of the cuboidal region.</p><p>raycasting process, the opacity of the voxels that lie in the cylinder are set to zero. The following equation is evaluated along the ray,</p><formula xml:id="formula_2">O(P(i)) = ï£± ï£´ ï£´ ï£² ï£´ ï£´ ï£³ 0</formula><p>dist(P(i x ) âˆ’CC x ) &lt; CR and dist(P(i y ) âˆ’CC y ) &lt; CR and dist(P(i z ) âˆ’CC z ) &lt; CH T F(scalar) dist(P(i x ) âˆ’CC x ) &gt; CR x and dist(P(i y ) âˆ’CC y ) &gt; CR y and dist(P(i z ) âˆ’CC z ) &gt; CR height where P(i xyz ) are the x, y, z coordinates of the current voxel under consideration. CC stands for the center of the circle of the cylinder, CR stands for the radius of the cylinder and CH stands for the height of the cylinder. <ref type="figure" target="#fig_4">Figure 9</ref> shows a view of the cylindrical cut along the axis of the probe. A cylindrical cut along the axis of the probe/instrument being tracked is of value to neurosurgeons. They want to be able to take a closer look further along the axis that their probe was traversing in the brain. Our cylindrical cut allows them to pick a radius of a cylinder that is consistently aligned with the axis of the probe. The radius of the cylinder can be controlled using a slider bar as shown at the bottom in <ref type="figure" target="#fig_4">Figure 9</ref>. During the raycasting process, the opacity of the voxels that lie in the probe-aligned cylinder are set to zero. The following equation is evaluated along the ray,</p><formula xml:id="formula_3">O(P(i)) = 0 dist(P(i) âˆ’ ProbeVector) &lt; CRadius T F(scalar) dist(P(i) âˆ’ ProbeVector) &gt; CRadius</formula><p>where ProbeVector stands for the probe aligned vector, P(i) stands for the current point in the raycasting process and CRadius stands for the radius of the cylinder that is cropped. <ref type="figure">Figure 10</ref> shows a view of the cylindrical cut along the axis of the probe. A wireframe of the cylindrical outline can be seen in the image. To highlight the region surrounding the cutout region, distance color blending is used. The line widget specifies the axis of the probe. . The top figure shows a cylindrical cut using our system. The bottom image shows the control that allows the user to specify the radius of the cylindrical region being cut. <ref type="figure">Fig. 10</ref>. This image shows a cylindrical region being cropped out of the brain according to the axis defined by the line widget. It will be extremely useful for surgeons to look into the brain along the probe. <ref type="figure">Fig. 11</ref>. The left image shows an ellipsoidal cut with the wireframe and the line widget. The right image shows the ellipsoidal cut without the wireframe and the line widget. The ellipsoidal cut is useful when a cut in a specific direction (x, y, z) needs to be deeper than any other direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Ellipsoidal Cut</head><p>In some cases the spherical cut was found to not be as ideal and an ellipsoidal cut was preferred. An ellipsoidal region can be removed by placing the probe in the location and defining a X-, Y-, Z-radius for the ellipsoid. During the raycasting process, the opacity of the voxels that lie in the ellipsoid are set to zero. This provides a ellipsoidal cut which allows an exploration of internal brain regions. The following equation is evaluated along the ray,  <ref type="figure">Figure 11</ref> shows an ellipsoidal cut into the brain.</p><formula xml:id="formula_4">O(P(i)) = ï£± ï£´ ï£´ ï£² ï£´ ï£´ ï£³ 0 abs(P(i x ) âˆ’ EC</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5">Widget Synchronized with Cross Hairs</head><p>In a second mode, we can enable the synchronization of the 2D cross hair point with the cropping endpoint of the line widget. This gives more specific control to the experts and can reduce the amount of user interaction that they need to remember to effectively use our techniques. <ref type="figure" target="#fig_0">Figure 12</ref> shows a screenshot of the cubical cut being controlled using the 3D location of the cross hairs (see attached video).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Visualization Considerations</head><p>We observed that merely cropping out the region was not sufficient. In some cases, even though a spherical cut was taken, the cut was not immediately observed by experts. To clearly and unambiguously convey depth, we applied proven visualization techniques such as distance color blending <ref type="bibr" target="#b10">[11]</ref>, silhouette lines and experimented with shading to convey depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Distance Color Blending</head><p>Distance color blending is a technique inspired by artists to convey depth cues in a static image <ref type="bibr" target="#b10">[11]</ref>. Cool colors (such as shades of blue, violet) are used to convey depth in an image. The following equation is used to implement it in the raycasting process,</p><formula xml:id="formula_5">c d = (1 âˆ’ k ds d v k de )c v + k ds d v k de c b</formula><p>where c d is the resultant color after distance color blending, k ds and k de are coefficients that determine the extent of the color blending, d v is the ratio of the distance traveled by the ray to the total distance that the ray will eventually travel and c b is the blue color component which is generally set to (0, 0, 0.15) but can be varied to get different effects. In our case, not only was it useful for delineating the region surrounding the cropping region but also helped provide a sense of depth. <ref type="figure" target="#fig_2">Figure 14</ref> shows a comparison of the three types of rendering that our system can perform as of now. Experts preferred the middle image for highlighting the region instead of using distance color blending for the whole image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Focus + Context Visualization</head><p>In some cases, instead of removing a region the reverse is equally useful. The specialists want to be able to focus on a region and want us to be able to render it in full detail, while providing a outline that provides context regarding the location of that region. This can easily be done in our system. Instead of setting the opacity of the voxel that falls in that region to zero, we use a different transfer function to color that region differently. <ref type="figure" target="#fig_1">Figure 13</ref> shows a focus+context style visualization, where the region of interest is colored in green and the surrounding region is shown by a simple outline of the brain.</p><p>As can be seen in <ref type="figure" target="#fig_1">Figure 13</ref>, visualizing the outline affects the perception of depth of the viewer. To address that issue, we visualize This image shows a visualization of the cropping synchronized with the cross hairs. In this case, the lower right image shows the cropped visualization where a cuboidal cut is being made based on location of the cross hairs. <ref type="figure" target="#fig_1">Fig. 13</ref>. Focus + context visualization of the brain. In some cases, it is preferred that the region under consideration is rendered with maximum detail and the surrounding region only provides context.</p><p>This focus + context visualization renders a small, user-selectable region around the brain while rendering the rest of the brain differently to provide context to the viewer.  the surrounding region using a grayscale colormap and the region of interest is still visualized using a green colormap.</p><p>The left image in <ref type="figure" target="#fig_8">Figure 15</ref> shows a visualization of the region of interest being colored in green and the surrounding region colored in greyscale. We found that merely coloring the region separately does not help visualize the detail. We additionally provide depth cues by adding silhouette lines to the visualization by computing a dot product of the gradient with the view vector. The right image in <ref type="figure" target="#fig_8">Figure 15</ref> shows a visualization of the same region with silhouette lines. These lines accentuate the ridges and valleys on the surface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL EVALUATION WITH IMAGE-GUIDED NAVI-GATION SYSTEM</head><p>The experiments described in this section relied on tool positions obtained by using the VVLink interface to connect to our lab BrainLAB VVCranial image guided navigation system. <ref type="figure" target="#fig_3">Figure 16</ref> shows a photograph of our system being used with the VVCranial system. We used a replica of the operating room setup in the lab complete with a set of stereo infrared cameras (NDI Polaris) for tool tracking. A physical rubber model of the brain was used as the "patient". The CT image of the rubberhead was registered to the physical position of the patient model using standard image-to-patient rigid transformations that are part of the VVCranial system.</p><p>Next the CT image of the rubberhead was transferred through VVLink to the research system. We then used affine registration methods to register this CT image to an MRI image acquired from an actual patient to connect the coordinate system of the model to the actual patient data. This enabled us to use the 3D mock setup to perform image-guided navigation (see attached video) in the patient data.</p><p>In addition to the anatomical MRI dataset mentioned, the 3D electrode localizations for this patient as well as pre-operative fMRI, SPECT data used to localize areas of abnormal blood flow, was available. In the following section we present results of using this complete system to visualize (using the methods described in the previous section) this multimodal data in a surgical context. The patient MRI and functional data (fMRI, SPECT) was registered using linear registration techniques <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Interactive Cropping</head><p>The first example (see <ref type="figure">Figure 17</ref>-right) demonstrates the use of the spherical cropping tool to allow for the visualization of the brain anatomy with overlaid polygonal models of the implanted electrode strips. In general, prior to 3D visualization of brain data, the skull of the patient needs to be stripped from the MRI data. This process can be unstable in the case of patient data where additional artifacts are often present resulting in poor visualizations. Our technique eliminates the need for skull stripping and allows surgeons to get a good view of the brain and the electrodes directly from the original data set. The left image in <ref type="figure">Figure 17</ref> shows one such example with a cuboidal cut which allows a clear view of the brain. The right image shows a spherical cut with the embedded electrodes that are now visible. Importantly, the region surrounding the electrodes has been cropped out based on the selected shape. <ref type="figure">Fig. 17</ref>. Instrument synchronized cutting. In this case the position of the surgical navigation pointer (shown as a cone in the figure on the right) is used to control the cropping tool. The left image shows that a cubical region of the brain has been removed based on the position of the probe allowing us to look into the brain without stripping out the skull. The right image shows a spherical cut with electrodes showing through. The brain has been cropped out but the position of the electrodes can be seen. <ref type="figure" target="#fig_3">Fig. 16</ref>. A photograph of our 3D mock operating room setup being used with the VVCranial system to perform image-guided navigation in the patient data. On the right you can see a rubber brain that is registered with the actual patient data which is being displayed on the screen. The probe, in the hand of the user in the photograph, is being tracked by the stereo infrared cameras and its coordinates are communicated realtime to our system. Based on the position and selected cropping shape, the user can interact with the data (see attached video).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Visualizing SPECT Activity</head><p>The results of the SPECT scan are crucial to neurosurgeons during surgical planning and surgery, as they correlate with regions of increase blood flow which are often implicated in seizures. Based on the scans, diagnosis and treatment plans are discussed during the planning phase <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7]</ref>. We use our new techniques to visualize the results of the SPECT studies in their actual 3D location in the brain along with the surrounding brain anatomy without cutting out too much of the brain. The SPECT data is registered with the MR data using rigid linear transformations. <ref type="figure" target="#fig_9">Figure 18</ref> shows a screenshot of a patient with SPECT readings visualized along with the brain. Based on the 3D location of the probe, a spherical region is cropped out of the brain to reveal the SPECT results in 3D, as shown by the red blobs in the brain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Visualizing Diffusion Tensor data</head><p>Diffusion Tensor Imaging (DTI) allows for visualization of fibre tracks in the brain and is used during epilepsy surgery. Using irregular cropping we can potentially look into the brain and observe the fiber tracks during planning and surgery. <ref type="figure" target="#fig_4">Figure 19</ref> shows one such example where the DTI data can be visualized by cropping out a section of the brain. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Visualizing fMRI activation regions</head><p>fMRI activation regions are crucial in the surgical planning phase. In our system, we can visualize fMRI regions in conjunction with the anatomical data. In some cases, cropping out the anatomical regions while preserving the fMRI can be useful in visualizing the activation in that region. We also provide the facility to crop out the fMRI data in addition with the anatomical data. The left image in <ref type="figure" target="#fig_0">Figure 20</ref> shows the ability to visualize functional data after cropping the anatomical region, while the right image shows a cropping of the functional as well as the anatomical data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Expert Evaluation</head><p>Our image analysis system, BioImage Suite, is being currently used for surgical planning as well as during epilepsy neurosurgery (over 60 procedures to date) by leveraging the Vector Vision Link interface. During many procedures, the neurosurgeons expressed the desire/need for an irregular cropping tool. In the current stable software setup, we can use axis-aligned cropping to look at activations or positions of electrodes. The ability to selectively crop out only a small part of the anatomy is key as it allows for both visualization of, for example, the location of intracranial electrodes while preserving the larger context  of their locations are lost when simple cropping of the entire "half" brain is performed along an orthogonal axis plane.</p><p>The development of the tools presented in this paper arose directly out of our experience with generating and using image visualization tools for surgical planning and image guided neurosurgery. The methods presented this paper are being integrated into the next version of the software and will begin to complement our existing methodology in the near future both as an aid to surgical planning and surgery itself.</p><p>The key issue in the evaluation of the system is not performance, since the surgeons are more interested in an accurate understanding of the patient's condition during the planning phase to avoid any unforeseen findings during surgery even if the exploration and discussion phase takes longer than using previously existing tools. During the surgical planning phase, various tools are used to look at the same data and make inferences from the same. Scoring the preference or measuring the time required to complete a task using the irregular cropping technique is not practical, since the irregular cropping is only an additional tool that will be used during the surgical planning and treatment phase for increased understanding of the patient's condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS AND FUTURE WORK</head><p>We present novel interaction techniques for visualizing multimodal medical data for neurosurgical treatment, stereotactic navigation and planning. We have applied our techniques for visualizing multimodal data, where SPECT activations, electrode data and such other readings for the patient can be more effectively explored using our interaction techniques. Such techniques may prove immensely useful during the surgical planning and treatment process. Our easy-to-use line widget provides control over the cropping parameters. The wireframe rendering gives an idea of the region that will be cropped eventually.</p><p>We address specific visualization issues by applying some depth cues such as distance color blending. Additionally, regions of interest can be shown in full detail in a "reverse cropping" scenario, where the focus+context style visualizations help effectively visualize the region.</p><p>We found silhouette lines to be an invaluable shape cue for showing the surface detail.</p><p>Our overall system is already in use for surgical planning and image-guided navigation and the techniques presented in this paper will be integrated into the setup in actual use for image-guided epilepsy neurosurgery in the operating room over the next few months. This methodology (including all source code) will be made available as part of BioImage Suite <ref type="bibr" target="#b7">[8]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Conventional visualizations in image guided surgery using the BrainLAB VV Cranial System. The top-left view is a research visualization of an overlay of cortical electrode reconstructions shown over a volume rendering of the 3D anatomical MRI data. This is generated in our research software and sent in to the BrainLAB system via the VVLink interface -seeFigure 1. The other views are conventional visualizations from the BrainLAB system, including (bottom left) a volume rendered view of a patient CT image showing the intracranial electrodes as bright dots.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>A screenshot of one of the views in BioimageSuite. The top two images and the bottom left image shows the standard two dimensional slices (Axial, Coronal and Sagittal) along with a three-dimensional volume rendering view of the same data. The bottom right image shows a 3D volume rendering of the data. The rectangular region marked in violet on the user interface indicates the synchronized x, y, z location of the cross hairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>This image shows the line widget with the distance between the two endpoints shown at the bottom left and the selected endpoint highlighted in red to let the user know whether the endpoint has been picked. The bounding box of the volume is shown in green.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>The left image shows the line widget being used. In this mode, the size of the sphere is shown by a wireframe rendering of the sphere while the cropping endpoint of the line widget allows for controlling the center of the sphere. The right image shows the visualization without the wireframe and the line widget. The cropped region is colored differently to draw the viewers attention and provide depth cues.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 9</head><label>9</label><figDesc>Fig. 9. The top figure shows a cylindrical cut using our system. The bottom image shows the control that allows the user to specify the radius of the cylindrical region being cut.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>x ) &lt; 0 and abs(P(i y ) âˆ’ EC y ) &lt; 0 and abs(P(i z ) âˆ’ EC z ) &lt; 0 T F(scalar) abs(P(i x ) âˆ’ EC x ) &gt; 0 and abs(P(i y ) âˆ’ EC y ) &gt; 0 and abs(P(i z ) âˆ’ EC z ) &gt; 0where P(i xyz ) stands for the x, y, z coordinates of the voxel V(i) under consideration during the raycasting process. EC xyz stand for the x, y, z coordinates of the center of the ellipse. As per the equation, the voxel's contribution is considered only if it falls outside the ellipsoidal region being cropped off.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 14 .</head><label>14</label><figDesc>Comparison of regular cropping, using distance color blending for cropped region, and using distance color blending for the whole brain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 15 .</head><label>15</label><figDesc>Focus + context visualization without and with silhouette lines on the region of interest. The right image provides more detail regarding the ridges and valleys in the region of interest.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 18 .</head><label>18</label><figDesc>Spherical probe-based exploration of a patient. The image shows a surface rendered SPECT activation regions embedded into the 3D anatomical MRI image. Spherical cropping used to peal away part of the anatomical MRI data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig</head><label></label><figDesc>Fig. 19. Fiber tracks obtained from diffusion tensor imaging can be seen using the spherical cropping tool. Interactive visualization of such functional data can provide useful information in the planning and surgical stages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 20 .</head><label>20</label><figDesc>Cropping anatomical regions while preserving the fMRI activation regions is useful in visualizing them effectively in conjunction with surrounding anatomy. The right image shows an option where the fMRI activation regions too can be cropped using one of the cropping tools.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work was supported in part by the NIH/NIBIB under grants R01EB000473 (Duncan J. PI), R01 EB006494 (Papademetris, X. PI) and R21 EB007770 (Papademetris, X. PI)</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pre-and intra-operative planning and simulation of percutaneous tumor ablation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Butz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Warfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tuncali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sg Silverman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Sonnenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jolesz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kikinis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="317" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Comparison of statistical parametric mapping and spect difference imaging in patients with temporal lobe epilepsy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">G</forename><surname>Zubal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gottschalk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Necochea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stokking</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Studholme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Corsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Slawski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Spencer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Blumenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Epilepsia</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="68" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Intuitive and Lightweight User Interaction for Medical Augmented Reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>StraÃŸer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Vision, Modeling and Visualization (VMV)</title>
		<meeting>Vision, Modeling and Visualization (VMV)</meeting>
		<imprint>
			<date type="published" when="2005-11" />
			<biblScope unit="page" from="375" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An integrated visualization system for surgical planning and guidance using image fusion and interventional imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dt Gering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nabavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kikinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wel Grimson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Everett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Jolesz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Wells</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="809" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">How are three-dimensional objects represented in the brain?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>BÃ¼lthoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Edelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Tarr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Cerebral Cortex</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="247" to="260" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Medical image computing at the institute of mathematics and computer science in medicine, university hospital hamburg-eppendorf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kh HÃ¶hne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="713" to="723" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Seizure localization by ictal and postictal SPECT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Mcnally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Paige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varghese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Spencer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">G</forename><surname>Zubal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Blumenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Epilepsia, volume</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page">115</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Bioimage suite: An integrated medical image analysis suite, section of bioimaging sciences, dept. of diagnostic radiology, yale school of medicine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Papademetris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jackowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rajeevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Constable</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Staib</surname></persName>
		</author>
		<ptr target="http://www.bioimagesuite.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Development of a research interface for image guided intervention: Initial application to epilepsy neurosurgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Papademetris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Vives</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Distasio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Staib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Flossman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Frielinghaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zaveri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Blumenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Constable</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Hetherington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Duckrow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Spencer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Spencer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Duncan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Biomedical Imaging ISBI</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="490" to="493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Volume based planning and rehearsal of surgical interventions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pflesser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Tiede</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>HÃ¶hne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Leuwer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Computer Assisted Radiology and Surgery (CARS)</title>
		<meeting>Computer Assisted Radiology and Surgery (CARS)</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Volume illustration: Non-photorealistic rendering of volume models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rheingans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="253" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visualizing with VTK: A tutorial</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Avila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Graph. Appl</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="20" to="27" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Supporting interactivity in automated 3d illustrations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dd Seligmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IUI &apos;93: Proceedings of the 1st international conference on Intelligent user interfaces</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="37" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Role of learning in three-dimensional form perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">384</biblScope>
			<biblScope unit="issue">6608</biblScope>
			<biblScope unit="page" from="460" to="463" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automated three-dimensional registration of magnetic resonance and positron emission tomography brain images by multiresolution optimisation of voxel similarity measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Studholme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hawkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="35" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Volume clipping via per-fragment operations in texture-based volume visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VIS &apos;02: Proceedings of the conference on Visualization &apos;02</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Interactive volume cutting of medical data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1155" to="1159" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
