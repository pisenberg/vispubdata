<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Two-stage Framework for a Topology-Based Projection and Visualization of Classified Document Collections</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Oesterling</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerik</forename><surname>Scheuermann</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Teresniak</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Heyer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Koch</surname></persName>
							<email>steffen.koch@vis.uni-stuttgart.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Ertl</surname></persName>
							<email>thomas.ertl@vis.uni-stuttgart.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunther</forename><forename type="middle">H</forename><surname>Weber</surname></persName>
							<email>ghweber@lbl.gov</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Leipzig</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Stuttgart</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Lawrence Berkeley National Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">IEEE Symposium on Visual Analytics Science and Technology</orgName>
								<address>
									<addrLine>October 24 -28, Salt Lake City</addrLine>
									<settlement>Utah</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Two-stage Framework for a Topology-Based Projection and Visualization of Classified Document Collections</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H</term>
					<term>5</term>
					<term>2 [INFORMATION INTERFACES AND PRE-SENTATION]: User Interfaces-Theory and methods; I</term>
					<term>5</term>
					<term>3 [Pattern Recognition]: Clustering-Algorithms;</term>
				</keywords>
			</textClass>
			<abstract>
				<p>During the last decades, electronic textual information has become the world&apos;s largest and most important information source. Daily newspapers, books, scientific and governmental publications, blogs and private messages have grown into a wellspring of endless information and knowledge. Since neither existing nor new information can be read in its entirety, we rely increasingly on computers to extract and visualize meaningful or interesting topics and documents from this huge information reservoir. In this paper, we extend, improve and combine existing individual approaches into an overall framework that supports topological analysis of high dimensional document point clouds given by the well-known tf-idf document-term weighting method. We show that traditional distance-based approaches fail in very high dimensional spaces, and we describe an improved two-stage method for topology-based projections from the original high dimensional information space to both two dimensional (2-D) and three dimensional (3-D) visualizations. To demonstrate the accuracy and usability of this framework, we compare it to methods introduced recently and apply it to complex document and patent collections.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The quantity of electronic textual data collected today is growing exponentially, and it is becoming increasingly difficult for humans to identify relevant information without getting lost in an overwhelming amount of information. As a consequence, we are relying more and more on computers to pre-process, classify and visualize coherent parts of massive data reasonably. To help humans navigate this wealth of textual information, researchers have been constantly searching for optimal models to accurately represent complex linguistic relationships. One of these models is the vector space model that represents documents as high dimensional vectors.</p><p>In this paper, we propose a framework that makes it possible to investigate and visualize resulting document point clouds using a topological approach. We do not expect users to be familiar with concepts from topology. Instead, we consider our approach to compete with clustering-based methods since it reveals similar information as density-based clustering. We focus our work on classified documents, instead of finding the classification itself, because we make use of a supervised dimension reduction method that incorporates cluster information. The usage of this supervised projection, in our first stage, aims to achieve a representation of the high dimensional data in a "medium" dimensional space which is still acceptable with respect to the optimization criteria of the dimension reduction method. That is, in this space, the cluster structure is claimed to be preserved as much as possible. In our second stage, we perform a topological analysis to obtain a data layout in 3-D that reflects the structure of the point cloud in the intermediate space.</p><p>This direct analysis minimizes the loss of information caused by directly projecting the data down to two or three dimensions. Instead of directly approximating a point set's topology, we indirectly construct a (density) scalar function and study its topology by means of the join-tree, which gives structural/nesting information of the dense regions. However, the join-tree is not capable of describing various and complex high dimensional features.</p><p>Afterwards, this topological information is reflected by a 3-D visualization, augmented by additional information of input data points. This allows for visualizing both the structure and the data set. While the former mainly leads to coarse structural insights, the latter can be used for labeling or details on demand. Regarding visual analytics aspects, we provide the user, e.g. a journalist, with a framework to obtain a 2-D/ 3-D layout of a set of documents. In this layout, the structure describes the data's decomposition into topics, constituted by documents which are similar. The nesting structure helps to identify topics which are related to each other. To achieve this presentation, we use a topology-based projection that critically depends on a single parameter. Interactive analysis <ref type="bibr" target="#b16">[17]</ref> allows a user to identify iteratively an appropriate parameter value and with it the desired information. Since clustering information leads only to coarse insights, we deem our data layout as an initial point for further exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Text classification, as a mixture of information retrieval, machine learning, and (statistical) language processing, is concerned with building systems that partition an unstructured collection of documents into meaningful groups <ref type="bibr" target="#b19">[20]</ref>. The two main variants are clustering, i.e., finding a latent structure of a previously determined number of groups, and categorization, i.e., structuring the data according to a group structure known in advance. For the latter, supervised approach, different types of learners have been used, including probabilistic "naive Bayesian" methods, decision trees, neural networks and example-based methods. In recent years, support vector machines <ref type="bibr" target="#b11">[12]</ref> and boosting <ref type="bibr" target="#b18">[19]</ref> have been the two dominant learning methods for text classification in computational learning theory and for comparative experiments.</p><p>If represented as vectors, one can gain structural insights into reasonably high dimensional data, by visualizing directly the point cloud using axis-based methods such as scatter plot matrices <ref type="bibr" target="#b0">[1]</ref> or parallel coordinates <ref type="bibr" target="#b9">[10]</ref>. Because these techniques depend on the data's dimensionality, it is often beneficial to project the data to lower dimensional spaces prior to visualization. By defining meaningful criteria, like maximal variance or maximal distance between cluster centroids, projections try to preserve the structure in the projected dimension. These projection methods are either supervised, such as LDA <ref type="bibr" target="#b5">[6]</ref> or OCM <ref type="bibr" target="#b10">[11]</ref>, or unsupervised, such as PCA <ref type="bibr" target="#b12">[13]</ref> or MDS <ref type="bibr" target="#b15">[16]</ref>. Choo et al. <ref type="bibr" target="#b3">[4]</ref> combined the advantages of several projections to minimize the information loss during the transformation. Besides linear projections, non-linear embeddings exist that use additional structural information when determining a layout of the data in lower dimensions: Takahashi et al. <ref type="bibr" target="#b21">[22]</ref> proposed a manifold learning approach to obtain a layout in 3-D that reflects the topology of a high dimensional input scalar function. Their method uses the k-nearest-neighborhood graph to seek the manifold proximity and they define a scalar-based distance measure to determine the closeness of points. To use this method for point cloud analysis, one has to define a suitable scalar function. For clustering purposes, a meaningful scalar function should also be defined in the void part of a data set (i.e., locations without vertices) to ensure separation of dense regions from regions of low density. Oesterling et al. <ref type="bibr" target="#b16">[17]</ref> focus on the construction of a point set's appropriate scalar function, supported by a neighborhood definition by means of the Gabriel graph <ref type="bibr" target="#b6">[7]</ref>, instead of deriving a manifold from the function. As a result, their 3-D data layout reflects the topology of the data's approximated density function, realized by the topological landscape metaphor <ref type="bibr" target="#b23">[24]</ref>, a 3-D terrain which has the same topology as the input data set. Therefore, their topological method does not analyze the topology of the point cloud itself, i.e., they do not try to classify parts of the input data into topological spaces or homeomorphic manifolds. Those things are considered in the field of algebraic or point-set topology and try to determine exact manifolds, represented by the input data. A good survey through this field is given in <ref type="bibr" target="#b1">[2]</ref>. Specifically for document collections, ThemeScape <ref type="bibr" target="#b24">[25]</ref> also uses a terrain metaphor to visualize the data, though it utilizes different underlying models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BACKGROUND</head><p>The original contribution of this work lies in extending, improving and combining several individual approaches into an overall framework for analyzing document collections. To make it easier for the reader to follow the details in the later part of this paper, we introduce the most important concepts in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Linear Discriminant Analysis (LDA)</head><p>For classified, high dimensional data, Choo et al. <ref type="bibr" target="#b3">[4]</ref> described the dimension reduction as a trace optimization problem. Following their nomenclature, the clustered m-dimensional data points a i ∈ R m are given as a data matrix</p><formula xml:id="formula_0">A = [A 1 A 2 • • • A k ], where A i ∈ R m×n i and k ∑ i=1 n i = n.</formula><p>The groups of column vectors in A correspond to the k groups of n clustered m-dimensional input vectors. For these groups N i of column indices of vectors belonging to cluster i, the cluster centroids c <ref type="bibr">(i)</ref> and the global centroid c are given by</p><formula xml:id="formula_1">c (i) = 1 n i ∑ j∈N i a j and c = 1 n n ∑ j=1 a j</formula><p>Using the clusters' vectors together with their centroids and the global centroid, the within-cluster scatter matrix S w and the between-cluster scatter matrix S b are defined as</p><formula xml:id="formula_2">S w = k ∑ i=1 ∑ j∈N i (a j − c (i) )(a j − c (i) ) T S b = k ∑ i=1 n i (c (i) − c)(c (i) − c) T</formula><p>By calculating the trace of these scatter matrices as</p><formula xml:id="formula_3">trace(S w ) = k ∑ i=1 ∑ j∈N i a j − c (i) 2 2 (1) trace(S b ) = k ∑ i=1 n i c (i) − c 2 2<label>(2)</label></formula><p>Choo et al. <ref type="bibr" target="#b3">[4]</ref> specify cluster quality measures by considering the distances between the k cluster centroids and the variance within each cluster, respectively. That is, well-separated clusterings usually will have a large trace(S b ) and a small trace(S w ). Eq.(1) describes trace(S w ) as the squared sum of pairwise distances between a cluster's points and its centroid. Likewise, Eq.(2) describes the pairwise distances between the cluster centroids and the global centroid.</p><p>The fundamental idea of their approach is to consider dimension reduction as a trace optimization that maximizes trace(G T S b G) and minimizes trace(G T S w G) in the reduced dimensional space, using a dimension reducing linear transformation</p><formula xml:id="formula_4">G T ∈ R l×m : x ∈ R m×1 → z = G T x ∈ R lx1</formula><p>projecting an m-dimensional input vector to an l-dimensional space (m &gt; l). It turns out that the solution, G LDA , of the LDA criterion as</p><formula xml:id="formula_5">J b/w = maxtrace((G T S w G) −1 (G T S b G))</formula><p>consists of the column vectors which are the leading generalized eigenvectors u of the generalized eigenvalue problem</p><formula xml:id="formula_6">S b u = λ S w u<label>(3)</label></formula><p>and that LDA preserves the original cluster structure after projecting the m-dimensional input vectors into the l-dimensional space, such that l = k − 1. We refer the interested reader to reference <ref type="bibr" target="#b3">[4]</ref> for further information, as it explains all the relationships clearly and in much more detail. In summary, LDA uses the additional clustering information of the input data to do a supervised projection from the original high dimensional space into an optimal lower dimensional space, i.e., (k − 1)-dimensional, maximizing the intercluster distances and minimizing the intra-cluster distances in the reduced dimensional space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Approximating a Point Cloud's Topology</head><p>Although LDA preserves the clustering structure in the intermediate space in terms of its optimization criteria (defined as a trace optimization problem), the target dimensionality might still be significantly larger than two or three. As a consequence, a subsequent projection will cause a loss of information due to projective overplotting in conventional visualizations. To avoid this second projection error, the intermediate space could be analyzed directly, instead of considering the point cloud in either the original m-dimensional space or in the 2-D/ 3-D space. Oesterling et al. <ref type="bibr" target="#b16">[17]</ref> described a method to analyze a point cloud's structure in higher dimensions. They use the structural insights to achieve a 3-D data layout which reflects the data's structure in the original space. The basic idea is to describe the point cloud's structure indirectly by constructing a scalar function that reflects the data distribution in terms of density. In the context of density-based clustering, they have to evaluate the neighborhood of the given points in order to distinguish regions of both high and low density. Subsequently, they perform topological analysis on the resulting scalar field, utilizing the join-tree <ref type="bibr" target="#b2">[3]</ref> that encodes the (joining-) evolution of contours, i.e., regions of equal density throughout the scalar function. The final visualization reflects that join tree's hierarchical structure, which, in turn, reflects the structure of the point cloud's density distribution. In particular, they perform the following steps: 1) to facilitate the investigation of a point's neighborhood, the input point cloud is connected by the Gabriel graph <ref type="bibr" target="#b6">[7]</ref>, which is a special neighborhood graph 2) utilize the neighborhood graph to perform kernel density estimation at meaningful positions in space. In this case, at the graph vertices, i.e., at the data points (where it is dense), and on the mid-points of the graph's edges, i.e., in the void between two neighbored points (where it is likely not dense)</p><p>3) the join-tree computation is performed on this approximated density distribution to analyze amount, size and (joining-) behavior of the contours. Because contours describe the dense regions, this step is equivalent to determining number, size and hierarchy of clusters 4) make use of the topological landscapes metaphor, proposed by Weber et al. in <ref type="bibr" target="#b23">[24]</ref>, to create a 3-D terrain that has the same topology as the join-tree. In this landscape, the structure of hills corresponds to that of the clusters in the input data set</p><p>We show topological landscapes and variations of this metaphor throughout the next sections. Due to space limitations, we refer to <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b23">[24]</ref> for further information since these papers introduce and explain a number of other concepts unrelated to the contributions of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Volatility</head><p>In the vector space model (vsm) <ref type="bibr" target="#b17">[18]</ref>, text documents are represented as vectors. Each dimension corresponds to a separate term <ref type="bibr" target="#b0">1</ref> and denotes the term's relevance in this particular document. Many different criteria have been proposed to extract only meaningful terms together with their individual significances. A common approach is the if-idf document-term weighting: for each single document, each term's frequency is weighted relatively to the number of other documents containing this term. Instead of considering only term frequencies in a single document or the whole corpus, additional semantic analysis can contribute to a vector's final expressiveness. Teresniak et al. <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b8">9]</ref> proposed an approach to define a term's meaningfulness or topical relevance based on the temporal fluctuation of a term's global context (i.e., how neighboring terms change over time). Utilizing stock-market nomenclature, the authors call this fluctuation of context the term's volatility. Thus, analyzing the variation of a term's context for different time slices can be utilized to detect highly discussed topics and their importance over time. To provide rough outline of the method, the major calculation steps are:</p><p>1) Compute the significant co-occurrences C(t) for each term t in the whole corpus</p><p>2) Compute the significant co-occurrences C T i (t) for each term t in every time slice T i <ref type="bibr" target="#b0">1</ref> We take a term to mean the inflected type of a word, whereas a word is assumed to mean an equivalence class of inflected forms of a base form 3) For every co-occurrence term c t, j ∈ C(t) compute rank c t, j (i), the series of all ranks of c t, j in the context of term t in every time slice T i 4) Compute the coefficient of variation (i.e., the ratio of the standard deviation to the mean) CV (rank c t, j (i)) for every cooccurrence term c t, j ∈ C(t) 5) Compute the term's volatility as the average of these variances:</p><formula xml:id="formula_7">Vol(t) = 1 |C(t)| ∑ j CV (rank c t, j (i))</formula><p>When plotting a term's frequency and volatility over time, both quantities do not necessarily correlate. The basic idea of volatility is to detect a topic (as a change of contexts) and not just heavy usage of high-frequent terms describing it. Although other methods may be used to increase the expressiveness of terms, we chose this model to support the tf-idf measure and omit terms from a document vector that are not volatile enough. For more details and examples comparing frequency and volatility, we refer to <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b8">9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PROBLEMS CHOOSING AN APPROPRIATE DISTANCE METRIC 4.1 Geometric Issues</head><p>It was Richard Bellman who first stated almost fifty years ago that "a malediction has plagued the scientist from the earliest days". While this malediction, basically, concerns the problems caused by increasing the number of independent variables in different fields of application, especially for (metric) spaces, this means an exponential increase of volume with each additional dimension. As a consequence, particularly for distance-based approaches, it has been shown <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b7">8]</ref> that depending on the chosen metric, distances between points either depend on the dimensionality (L 1 norm) or approach a constant (L 2 norm) or zero (L d≥3 norm). That is, the relative difference between the distances to a point's farthest and closest point approaches zero. As a consequence, distances become relatively uniform in higher dimensions and some distancebased relationships such as nearest neighbors become meaningless in those spaces. Of course, if distances become uniform, every distance-based approach is affected by this phenomenon. To illustrate this problem for clustering algorithms, we consider the MEDLINE 2 data set. This data set consists of 1, 250 vectors in a 22, 095-dimensional space, divided into five equally sized clusters. The black graph in <ref type="figure">Figure 1</ref>(a) shows the number of individual distances between any two points. It is clearly visible 3 that the maximum of all distances lies around 2.0 and 98, 79% of the distances are greater than 1.85. The key issue is that both the intercluster distances (green) and the intra-cluster distances (red), which are obtained by considering the given clustering information, show the same behavior. If a data set contains several clusters, however, this graph typically shows two peaks: one for the intra-cluster distances, and a peak representing the average distance between points <ref type="bibr" target="#b20">[21]</ref>. Consequently, because only one peak is present, any purely distance-based approach will have issues with finding the underlying clustering of this data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Semantic Issues</head><p>In addition to geometric issues, other semantic problems contribute to the measured distance between documents. If documents are represented by vectors, meaningful words usually serve as the vectors' dimensions. Although both a word and its significance heavily depend on the chosen algorithm, there will always be some words  <ref type="figure">Figure 1</ref>: Partitioning of the MEDLINE distances between any (black) two points into inter-cluster (green) and intra-cluster (red) distances, done (a) in the original dimensionality and (b) after applying LDA. Now, the red and the green peak are separated.</p><p>in the vector that violate the basic assumption that (dis)similarity between documents is reflected by the distance between their corresponding vectors. Such words, like common and frequent words, can make two documents from different topic areas appear more similar than actually desired (or needed). The situation becomes worse if the distance between two documents is dominated by the contribution of non-discriminating words. It is possible that two document vectors about the same topic consist of equal numbers of discriminating and common words. If the common words are even distinct, they can cause the vectors to disperse unintentionally, thus negatively affecting the clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Solution Approach</head><p>In summary, when we consider a document collection as a point cloud, we are faced with two main problems. The first is the construction of a point cloud where distances between points reflect (dis)similarities between documents. Second, due to the curse of dimensionality, we are most likely not able to distinguish between similarity and dissimilarity, because most of the inter-and intracluster distances are uniform. To alleviate the first problem, language processing is necessary to avoid choosing meaningless words which force documents to unintentionally approach or disperse. For the second problem either a supervised (distance-independent in our case) clustering algorithm or a supervised projection to a lower dimensional space, being less afflicted by the curse of dimensionality, is needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">TWO-STAGE PROJECTION</head><p>Our two-stage approach is related to that proposed in <ref type="bibr" target="#b3">[4]</ref>. Choo et al. describe several two-stage combinations of supervised LDA or OCM and unsupervised PCA to project the original input point cloud into an intermediate space, followed by a second projection down to 2-D. The supervised first stage projects the point cloud preserving its cluster structure and the goal of the second projection is to minimize information loss due to the dimension reduction to 2-D. One of the main contributions of our work is to improve the output of this two-stage approach by substituting the second stage a with direct topological analysis of the intermediate space.</p><p>To motivate this, we first consider the two-stage LDA-LDA projection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Examining the Rank-2 LDA projection</head><p>This projection proposed by Choo et al. <ref type="bibr" target="#b3">[4]</ref> consists of two subsequent LDA projections:  <ref type="table">a   a   a  a a   a a  a  aa  a   a  a   a   a  a  a a   a   a  a  a a   a a  a  a  aa  a  a a   a   a  a  a   a  a  a   a   a   a   a   a   a   a  a  a   a   a   a  a  a   a  a   a  a a a   a   a   a  a  a a  a  a   a   a   a  a  a   a  a  a  g  g  g  g g   g   g g</ref> g gg g g g g g g g g g g gg g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g   <ref type="table">e  ee e e  e  ee e  e  ee  e e e  e e  ee e  e  e e  e  e  e  e  e e  e e  e e  e  ee e  ee  e e  e e  e  e  e e e  e e  e  ee e  e  e e  e  e e  ee  e  e e  e e e  e  e e  e  e  ee e  ee e  ea a  a  a  aaa aa a a  a  a  a  a aa a  a  aa a aa a  a a  a a a  a a a  a  a  a a  a  a  a  a aa   a   a   a   a  a a  a  aa a  aaa a a aa  a a a  a  a  a  aa</ref>   <ref type="table">r  r  r  r  r r  r  rr  r r  r  r r r  r r   r  rr r  r r  r  rrrr  r  r  rrr  r r  r  r rr  rr r  r  r r  r  r  r r  r  rr r  r rrr r  r r  rrrr r  rrr  rr  r  rr  tt  t  tt  t t t  t  t  t  t  tt  t   tt  t  t   t  t t  t tt  tt t  t  t  t t  t  t t  t  t  t t tt  t  t  t t  t t  t   t   t t  t  t  tt  tt  t t  t  t  t  t t  t  tt   t  t   t  t  t t  t  t  t  t  t t  t i  ii  i  i</ref>  <ref type="table">s s  s  s  ss s  ss   s   s  s   s   s  s  ss  ss  s  s s  s  ss  ss s  ss s  s s  s  s  sss  s  s  ss  s s   s  s  s s s  s s  ss s  s s s   s   s  s  s s  ss s s  s  s  s ss s  s  s  s s  ss</ref>   <ref type="table">c  e  ee  e e e e e  ee ee  e  e e  e  e  ee e  e  e e e  e  e e  e e  e  e e e  e e  ee e  ee e  ee  e  ee e e e  e  e ee  ee  e ee e  e  e e  e   ee e  e e  ee e  e  eee e  ee  e e a  a  a a  a a  aa  a  a  a  a a  a  a  a a  a a  a a  a a  aa  a a a  a a  a a a  a a  aa  a a  a  aaa   a   a   a   aa  aa  a a  aa aa  a a  a  a  a  a aa  a  a  a  aa  a  aa  a a  aa a  a  aa</ref>   <ref type="table">r  r  r rr  rr r  r  r  r  r rr  r  rrrr r r  r  r r   r  r  r  rr rr  r r  r  r  r rr  rr  r  r r  r r r  r  r r  r  rr  r r  rrr  r  r r r  r  rr r  r rr r  r r  r   r rr  r r rrr  t  ttt t tt t  t  t  t  t t t   t   tt  t t t  t tt  t  t  t t  t  t  t  t  t  t t  t t  t t  t  tt  t tt t  tt  t   t   t t  t  t t  t  t  t t  ttt  t t tt  tt   t  t   t  tt  t  t  t  t t t   t  t  i</ref>   <ref type="table">e  e e  e  e  ee e e e  e e  e  e  e  e  e  ee e  e e   e  e  e e  e e e  e  e e   e  e ee e  e ee  e  e  ee  ee e e  e e  e  e e  ee e  ee  ee e  e  e e  e  e  e   e  ee ee e  e  ee  e  e e  e aa  a aa a  aaa a a  a aa  a  a a  a a  aa a a   a  a  a a  a a  a  aa a  aa a  a  aa  a a  a a  a  a  a  a  a  aa a  a aa  aa a a  aa a  a  aaa  aa aa a a  a a a  aa a  aa a  m  g  g  g  g  g g g  g  g  g g  g  g  g  g  g  g  g  g  g  g  g  g  g  g  gg  g  g  g g  g  g  g  g  g  g  g  g  g  g  g  g  g  g  g  g  g  g  g  g  g  g  g  g  g  g  g  g  g  g  g  g g  g  g  g  g g  g  g  ggg  g  g  g  g  g  g  r r  r  rr  rr r   r   rr  r r r  r  rrr r  rr   r  rr r  r  r r  r  r  r  rrrr r  r r  r  r  r r  rr  rr r  rr  r r  r r  r  r  r r  rr  r r  r  r  r r  r  r  r r  rr  r r  r  rr  r   r   r  r  tt  t  tt  tt   tt  t  tt t  t  t  t t  t   t  t  t   t  t  t  ttt t  t   t   t   t  t  t  tt t  tt  t</ref>  <ref type="table">s  s  s  s s  s  s s  s  s s  s  ss s  ss s  s  s s  s s  s  s  s s ss s  s  ss ss sss s  ss ss s  s  s s s  s  ss ss s s s  s  s s s  s  s s  s  s s  s  ss   s  s ss  s  ss  sw w  w  w  w  w  w  w  w  w  w  w w  w ww  w  w w  ww  w w w  w   ww  w  w  w  w w  w  w  w  w  w w  w  w  www  w  w  w  ww  w  w  w  w  w  w  w  w w  w  w  w  w w  w w  w  w  w  w  w  w  w w  ww w  ww  w w  w  c  c e  ee  e e e  e e  ee ee  e  e e  e  e  ee e  e  e   e  e  e  e e  e e  e  e e   e  e e  ee e  ee e  ee  e  ee e e e  e  e ee  ee e ee e  e  e e  e  e   e  e  e e  ee e  e  eee e  ee  e e a  a  a a  a a  aa a a  a  a a  a  a  a a  a a  a a  a a  aa  a a a  a a  a a a  a a  a   a  a a  a aaa  a a  a aa  aa  a a  aa aa  a a  a a  a  a aa  a a  a  aa  a  aa  a a  aa a a  aa m  g  g  g  g  g  g  g  g  g  g  g  g  g  g  g  g  g  g  g  g  g  g  g  g  g  gg  g  g g gg  g  g  g  g  g  g  g  g  g  g g  g  g  g  g  g  g  g  g  g  g g  g  g  g  g  g g  g  g  g  g  g  g  g  g  g g  g g  g  g  g  g  g  g  g g  r  r  r rr  rr r   r   r  r  r rr  r  rrr r  r r   r  r rr  r  r  rr rr  r r  r  r  r r r  rr  r  r r  r r r  r  r r  r  rr  r r  rrr r  r r r  r  rr r  r rr r  r r  r  r</ref>      two dimensions. As explained in <ref type="bibr" target="#b3">[4]</ref>, the final projection matrix</p><formula xml:id="formula_8">i i i ii i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i ii i i i i i w w w w w w w w w w w w w ww w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w</formula><formula xml:id="formula_9">i i i i i i i i i i i i i iii i i i i i i i i ii i i i i i i i i i i i i i ii ii ii i i i i i i i i i i i i i i i ii i i i i i i i i i ii i i s s</formula><formula xml:id="formula_10">i i i i i ii i i i i i ii i i ii i i i i i i i iiii i ii i i ii i i i i i i i iii i i i iii i i i i ii i i ii i i i ii i i i i i i i i i i i is s</formula><formula xml:id="formula_11">t t t t t tt t t t t t t t t tt t t t t t t t t t t t t t ttt t t t t t t t i i i ii i i i i i i i i i i i i iii i i i i i i i i i ii ii i i i ii i i i i i i ii ii i i i i i i i i i i i i i i i i i i i i i i i iiii i ii i i s ss</formula><formula xml:id="formula_12">i i i i i ii i i i i ii i i i ii i i i i i i i i i i i i ii i i i i i i i i i i i ii i ii i ii i i i i i i i i i i i i i i i i i i i i i i i i i i e e</formula><formula xml:id="formula_13">i i i i i i i i i i i i i i i iii i i i i i i i i ii i i i i i i i i i i i i i ii ii ii i i i i i i i i i i i i i i i ii i i i i i i i i i ii i i s s</formula><formula xml:id="formula_14">V , V T ∈ R 2×m : x ∈ R m×1 → z = V T x ∈ R 2×1 = [u 1 u 2 ]</formula><p>composing the two single projections, consists of the leading generalized eigenvectors of Eq. (3). Since the Rank-2 LDA and LDA+PCA are claimed to produce the best discriminating and almost identical results, we consider the Rank-2 LDA output, using the REUTERS 2 data set. This document collection consists of 800 vectors in a 11, 941-dimensional space, assigned equally to the following k = 10 clusters (the letters are used in the diagrams):</p><p>earn ('e'), acquisitions ('a'), money-fx ('m'), grain ('g'), crude ('r'), trade ('t'), interest ('i'), ship ('s'), wheat ('w'), and corn ('c')</p><p>The result of the Rank-2 LDA is shown in <ref type="figure" target="#fig_9">Figure 2</ref>(a). As can be seen <ref type="bibr" target="#b2">3</ref> , the clustering is preserved well by the Rank-2 LDA. However, the clusters on the right-hand side and in the top left-hand corner of the scatter plot are overplotted. The pivotal question is why. Overplotting could either be due to data relationships, meaning that clusters are indeed mixed in the original space, or due to overplotting in the second stage. Since LDA preserves the cluster relationship in the l-dimensional space, we can analyze this 9-dimensional intermediate space by looking at a scatter plot matrix. <ref type="figure" target="#fig_9">Figure 2(b)</ref> shows us that the second assumption is true. Examining the point cloud not only from the first two principal components, but considering the 7 th − 9 th dimensions in the scatter plot matrix, it can be seen that both overplotted clusters consist of actually separated clusters in the intermediate vector space. This is not completely surprising, as the second-stage dimension reduction only uses two axes to discriminate the classes which contribute most to the second stage criteria. Nevertheless, due to the lack of any information about the intermediate space the user will most likely tend to (mistakenly) assume that clusters are mixed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Substituting the Second Stage</head><p>To eliminate the drawback of overplotting clusters, we substitute the projection in the second stage with a topology-based projection from the l-dimensional intermediate space to a 3-D topological landscape (which can be easily reduced to a 2-D visualization as described in the next section). Considering the LDA-projected (l = k − 1)-dimensional point cloud, at first the Gabriel graph is calculated to obtain neighborhood information, and a Gaussian-like filter kernel is applied on the graph's vertices and edges. Having this approximated density distribution, we determine its contour tree, or more precisely the join-tree as its representative. Subsequently, this tree serves as the input for the mapping process to achieve the final topological landscape visualization. To justify this substitution of the second stage, we revisit the REUTERS example from above. Although the documents belonging to the (c)orn, (w)heat and (g)rain clusters are semantically related, the scatter plot matrix shows us that the corresponding clusters are still separated. These separated point accumulations lead to several density maxima inside the density distribution, resulting in several hills in the topological landscape. <ref type="figure" target="#fig_10">Figure 3(a)</ref> shows the topological landscape of the REUTERS data set. As can be seen, we are now able to distinguish dense parts of the point cloud which are separated by a region with low density. The hills of the landscape describe the topology of the point cloud's density distribution, i.e., hills correspond to contours evolving between their appearing at a density maximum and their merging at a saddle point. The colors of the hills have no special meaning and are chosen randomly. The small spheres correspond to the actual data points and are placed on the hills that correspond to their clusters. The colors of the spheres reflect their class association, thus corresponding to the coloring in <ref type="figure" target="#fig_9">Figure 2</ref>. As described in <ref type="bibr" target="#b16">[17]</ref>, the visual analysis process is performed by finding an appropriate filter radius. For this purpose, the user examines the landscape in each iteration and determines the filter radius for the next iterative step. The analysis process is finished when the user has extracted the desired clustering information or when the landscape denotes that the filter radius is getting too small. In the latter case, noise starts to produce density attractors or several extrema are found inside one cluster. Concerning the rebalancing, we point out that although this step was originally proposed in <ref type="bibr" target="#b23">[24]</ref> to improve space utilization, for clusterings, the non-rebalanced landscape accurately reflects the spatial relationship between groups of points. As demonstrated in <ref type="figure" target="#fig_10">Figure 3(b)</ref>, the hills (of each hierarchy level) are positioned in a spiral layout around the center hill, which corresponds to the global maximum. That is, the global maximum lies inside the 'm'/'i'-cluster, as this is the densest cluster (i.e., points per area). In the neighborhood to this density maximum, there are two other attractors corresponding to (local) accumulations of 'i'-class and 'm'-class points. Next to these clusters, the 't' cluster has its saddle position and in its neighborhood the accumulation of the 'g'/'w'/'c' clusters resides.</p><p>They form a local group, as they are closer to each other than to the yet found clusters. The same holds for the yellow, cyan, and blue clusters, belonging to the 's'/'r'/'e' and 'a' clusters, respectively. They are locally neighbored, but separated from the other clusters. In this local neighborhood, the yellow and cyan clusters are, once more, closer to each other than to the blue clusters. A comparison of this landscape with <ref type="figure" target="#fig_9">Figure 2</ref>(a) leads to roughly the same neighborhood description, except for all the information that was lost by the second stage projection in <ref type="figure" target="#fig_9">Figure 2</ref>(a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXTENDING THE VISUALIZATION</head><p>Although it is capable of visualizing arbitrary, high dimensional data, the original topological landscape metaphor was only applied to visualize the topology of 3-D scientific scalar fields, given on regularly sampled grids. Using this metaphor to visualize data on a completely unstructured grid, sampled mostly inside the clusters and hardly in between, leads to some perceptual problems:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">3-D to 2-D</head><p>First of all, as the landscape is still three dimensional, it suffers from overlapping of the hills and therefore the benefit of the visualization is view-dependent (especially when data points are positioned at the back of a hill). To alleviate the overlapping of hills and data points, we propose a flattening of the original topological landscape. Using the same construction scheme as in 3-D, we create a flat 2-D landscape by using the join tree's (interpolated) isovalues as additional vertex information instead of considering them as 3-D height values. On this 2-D scalar field, which has the same topology as the input join tree, we apply normal color mapping and isoline extraction to encode the absolute densities. In order to support the advantage of metaphors, we relate this visualization to an atoll by applying a color coding from blue (water) to yellow (beach), then fading from green (grass) into brown (mountains) and finally to white (snowy mountain top). The isolines, which correspond to the original height values, allow for an easier density correlation between the data points. Altogether, this visualization supports the same topological insights, but with far less overlapping in 2-D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Improved Volume distribution</head><p>The second problem concerns the representation of approximated contour volumes, i.e., the size of clusters in our case. As described in <ref type="bibr" target="#b23">[24]</ref>, a metric-based distortion can be applied to the landscape in order to reflect better the real size of hills (contours), which otherwise would only depend on the depth of the branch decomposition and the landscape's construction scheme itself. Therefore, all triangles of a hill are resized according to the hill's corresponding cluster volume. Because this volume is distributed equally to all the triangles of a hill, the centered hill of nested hierarchies gets heavily distorted. This distortion primarily destroys the visual expressiveness of the hill's corresponding cluster in the landscape. To solve this problem, we change the triangles' volume assignment. Instead of dividing a branch's volume by the number of the corresponding triangles, we assign the volume above the first saddle to the eight triangles of the centered hill and the volume beneath the first saddle is assigned equally to all the remaining triangles. Although this distribution could be done more accurately, by considering each volume between each pair of saddles, we believe that this distribution sufficiently points out the volume of the corresponding cluster. <ref type="figure">Figure 4</ref> shows the flattened REUTERS landscape with the size of the hills (islands) corresponding to the clusters' sizes (approximated by the number of points).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Labeling</head><p>We additionally enhance this visualization with a labeling of both hills and small data spheres. Since we are dealing with documents, <ref type="figure">Figure 4</ref>: The flattened and volumetric distorted topological landscape of the REUTERS data set. The height lines and the coloring reflect the original height values (i.e., the absolute density values) from low (blue) to high (white). Furthermore, the distorted islands better reflect the actual cluster volumes.</p><p>we are most likely interested in their titles first. However, showing titles of all documents would result in significant overplotting of labels. Therefore, we have implemented the basic 'excentric labeling' approach, proposed by Fekete et al. <ref type="bibr" target="#b4">[5]</ref>. In this approach the user slides a small focus area over the data set, labeling only those data points that lie inside the small focus window. Labels are positioned around the focus area and a line connects them to their corresponding data entities. Finally, labels are colored according to the class of the data points. If titles are too long, we cut them or use a given short version of the title.</p><p>Following the ideas of tf-idf and the vector space model, clusters are thought to constitute topics. Document vectors belonging to the same topic most likely share similar words with similar significances. Therefore, these document vectors accumulate in the subspace spanned by the dimensions (words) they share. The goal is to provide hills (clusters) with labels corresponding to their topics. Instead of using the documents' class-association, we propose a quick semantic analysis. For documents on a single hill, we identify the most frequent word(s) they share and use the two most frequent words as a hill's topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CLASSIFICATION</head><p>Assuming an unclassified entity would match one of the given classes, it is possible to use our proposed two-stage process for classification purposes: The LDA, as a linear dimension reduction, projects similar high dimensional vectors into the same region in the lower dimensional space. Therefore, if we assume that we already have learned the LDA projection for a given class (based on the classified input data), the nature of LDA ensures that additional similar unclassified vectors are projected into the same lower dimensional target area. While this is an intrinsic feature of the projection itself, we can, however, rely on the fact that similar vectors (classified or not) are comprised by the same contours of the density function in the projected space. Therefore, although it is the first stage projection that ensures a clustering in the lower dimensional space, it is the second stage topological projection that eventually gives us a tool for detecting the clustering and for performing a classification, based on cluster association. From our algorithm's point of view, we assign unclassified data entities the class of their neighbored classified entities as follows: We determine the LDA projection matrix based on the classified data and use this matrix to project both the classified and unclassified data (resulting in accumulations in the lower dimensional space if the vectors are similar). We now have two choices: First, we combine both point clouds to serve as the input for the topological analysis, or we alternatively apply the topological analysis solely to the classified point cloud and approximate the topology of the combined point cloud as follows: We extend the height graph of the input point cloud by an edge between each unclassified data point and its nearest classified neighbor (this equals the finding of the nearest contour). Subsequently, we compute the densities at these edges' mid-points and at the unclassified points (in both cases, based on the classified entities). On this extended height graph, we continue as before. For both scenarios, we find similar classified and unclassified documents on the same branches in the branch decomposition, thus assigning an unclassified entity the label of the most frequent class on the entity's branch (i.e., island in the landscape).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">DISCUSSION AND EXAMPLES</head><p>Analyzing the l-dimensional intermediate space is, of course, more expensive than analyzing a (lossy) 2D space. For detailed runtime complexities we refer to <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b23">[24]</ref> and discuss some common runtime issues instead: First, the initial LDA projection greatly accelerates the topological analysis of the point cloud's density distribution in the intermediate vector space. This acceleration is due to the fact that the reduced dimensionality leads to a less full Gabriel graph, resulting in fewer necessary density evaluations on the graph's edges. Since graph connectivity commonly increases exponentially with each additional dimension, this approach leads to significant difference for high dimensional data sets. Besides this, we can also use the classification ideas from the last section and treat a percentage of the input data as it was unclassified. By randomly using, e.g., only 50% of the input vectors to learn the LDA projection matrix, the remaining 50% of the vectors are thought to be projected correctly, due to their similarity to the training data. Of course, using only a part of the input data greatly accelerates the runtime of the LDA. We will demonstrate the quality of both classification and using only a subset of the data later in this section.</p><p>For demonstration purposes, we apply our method to a New York Times document collection and a patent collection. As mentioned, we refer to the literature for detailed information regarding the runtime of the individual steps. To provide a rough guideline, the topological analysis of the previous REUTERS example and the upcoming examples in this chapter took around four seconds each. Our machine has 8GB memory and we use two 2.6GHz-QuadCore processors to benefit from parallelism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">NYT -Document Collection</head><p>The New York Times Annotated Corpus 4 contains over 1.8 million articles written and published by the New York Times between January 1, 1987 and June 19, 2007 with article meta-data provided by the New York Times Newsroom, the New York Times Indexing Service and the online production staff at nytimes.com. As part of the New York Times' indexing procedures, most articles are manually summarized and tagged by a staff of library scientists.</p><p>For our testing purposes, we consider the year 2001 and extract 50 documents per day. As described in Section 3, we use the tf-idf document-term weighting and the terms' volatility to determine the document vectors. Therefore, considering a single document, all the stop words are pruned and the normal tf-idf weighting scheme is applied to the remaining words. Subsequently, a 30-day sliding window is used to determine a term's volatility for each of the 365 days. Then, we use the variance of each term's volatility series to obtain an ordered series of over-year importance of all volatile terms. Finally, we clip the tf-idf vectors by the words that are not assumed to be sufficiently volatile (based on their position in the ordered list). As classification, we choose random documents corresponding to 10 different tags, up to 250 per group. Altogether, this test case consists of 1, 896 documents (points), described by 46, 393 words (dimensions). <ref type="figure" target="#fig_11">Figure 5</ref> shows examples of the final visualization. As can be seen, the LDA and the subsequent topology- based projection down to 2-D preserve the 10 topical clusters successfully described by the vector space model. The proposed labeling of the hills appropriately reflects the underlying topics, suggested by the document titles and implied by the documents' content. Looking at the visualization, it is important to understand that closeness of islands does not imply that the corresponding topics are related in the original space. This information gets lost during the projection and, therefore, spatial relationships are only encoded in the hierarchy of hills. (i.e., only sub-hills express spatial closeness to their parent hill).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Patent Collection</head><p>Access to patent information is of importance for a variety of interest groups today. Besides many other properties, the majority of information describing the nature of a patent is still conveyed through its textual content, therefore making natural language processing (NLP) a mandatory part of solutions for patent analysis. The sheer mass, complexity, high dimensionality, and heterogeneity of patent data make scalable visual analytics approaches for patent analysis <ref type="bibr" target="#b14">[15]</ref> a hard task. One particularly relevant type of meta data that is available for patent applications is manually assigned classification information. This classification information organizes the vast numbers of patents into predefined classes representing certain technical or functional aspects. Several different schemes for patent classification, such as the International Patent Classification (IPC), Japanese F-terms, and the US classification, exist. Patent offices are interested in automatic classification of new patent applications according to the existing classification schemes.</p><p>In order to evaluate our approach, we tested against the IPC comprising more than 70, 000 classes, hierarchically organized into sections, classes, subclasses, main groups, and sub groups. In the end, our test case consists of 1, 552 randomly selected patents from different IPC hierarchies (up to 200 each): We used patent data 5 from the European Patent Office (EPO). As a preprocessing step, the data has been analyzed and the text content was stored in vectorized form within a search index. From this index the tf-idf values for all dimensions of the term vectors have been computed. First, we examine whether our landscape reflects the nesting structure of the chosen patent hierarchy. <ref type="figure">Figure 6(a)</ref> shows the visualization for our test case. Mainly four groups can be identified: The purple and brown points in the upper-right corner, belonging to H04Q patents, clearly address networking, as the labels (and the document titles) relate to atm, address, message, ip and cell. In fact, the H04Q IPC-hierarchy categorizes patents belonging to "electricity" (H), "electric communication technique"(H04) and "selecting (switches, relays, etc)" (H04Q). Although the group of pink, blue and black points on the right-hand side belongs to completely different IPC-sections <ref type="figure" target="#fig_9">(A61K, C12N, G01N)</ref>, the corresponding patents all concern medical issues in their major field: A -Human Necessities, C -Chemistry, G -Physics. Because they share the medical vocabulary, they still constitute one topic in the vector space model. Finally, the centered hill belongs to the B41C cluster and the green and golden points comprise a cluster related to applications of materials in chemistry and metallurgy (C09D, C09J).</p><p>We use this patent example to demonstrate the classification ideas from section 7. For this purpose, we split the patents into 50% classified training data and 50% unclassified test data, which means that we henceforth ignore their class label. For illustration purposes, however, we remember the test data's class association for coloring in the landscape. After determining the training set's LDA projection matrix, we use it to project both patent sets and use their combination for our topological analysis. <ref type="figure">Figure 6</ref>(b) shows the (not volumetric distorted) landscape. As can be seen, the training data (represented by spheres) and the test data (represented by cones) belonging to particular classes (represented by the color) are all hosted on mainly their own islands. This confirms that due to their shared vocabulary, i.e., their shared dimensions, patents of a specific class are equally handled by the LDA, and their accumulation in the lower dimensional space allows us to topologically find the dense area as one combined cluster. While this allows for a faster LDA computation, the topological encoding by means of the join tree's branch decomposition also offers a way to provide the test data the (main) class of a branch's / hill's training data class. In our example, the LDA projection using all the data took 28s, whereas using only 50% took only 12s. To evaluate classification quality, we determined for each branch (island) how many of the branch's test data entities match the branch's training data class (using the test data's known class in this case). On average, 89.4% of the test data on a branch matches the class of the training data, or more precisely ≈ 76.7% in the noisy region of the medicine archipelago, and ≈ 99.6% on the remaining branches which correspond to clusters being better separated.  <ref type="figure">Figure 6</ref>: (a) 2-D topological landscape of the patent data set. The nesting structure of the islands reflects the IPC hierarchy of the test data set. (b) The same landscape without volumetric distortion. Even by learning the LDA with only 50% of the input data, the remaining patents (the cones) are placed on their correct islands (clusters).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSIONS AND FUTURE WORK</head><p>To cluster document point clouds, we demonstrated the necessity of reflecting similarity by distance and we referred to uniform distances, caused by the curse of dimensionality. We tried to alleviate the first problem by using a term's volatility, as we believe that this approach results in more topically related terms. Concerning the dimensionality, we showed that a supervised approach is necessary in very high dimensional spaces. Therefore, we proposed a twostage framework consisting of a supervised LDA projection down to (k − 1)-D, followed by a direct topological analysis of this intermediate vector space. By doing so, we were able to improve comparable approaches that use a lossy second stage projections. We also extended the visualization in <ref type="bibr" target="#b16">[17]</ref> to facilitate a more precise and less overlapping analysis process in 2-D. For classification purposes, we showed how LDA and the use of the branch decomposition can be used for automatic document classification based on an existing classification. Furthermore, the quality of the classification itself can be verified by examining the distribution of colored points in the landscape. If a single color occurs on several hills, the clustering (and therefore the classification) might be inappropriate. Since the presumed classification is a drawback compared to unsupervised approaches, our future work will include the investigation and support of classification methods (possibly also topologybased). We will also consider other data structures to identify more complex topological features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(i) an LDA projection from the original mdimensional space into an intermediate (k − 1)-dimensional space and (ii) an LDA projection from (k − 1)-dimensional data down to</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 2 :</head><label>2</label><figDesc>(a) Rank-2 LDA of the REUTERS data set. Some clusters are still mixed in the projection (b) part of the 9-D scatter plot matrix, showing the intrinsic separation of the overplotted clusters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 3 :</head><label>3</label><figDesc>(a) topological landscape of the REUTERS data set in the intermediate 9-dimensional space. The topological analysis reveals the separated clusters which were overplotted by the Rank-2 LDA (b) without rebalancing the branch decomposition, the spacial relationship between density attractors can be read off the landscape.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 5 :</head><label>5</label><figDesc>(a) 2-D visualization of the NYT data set. Islands correspond to topics and their sizes fit the clusters' number of documents. (b) The same landscape for a higher filter radius and without volumetric distortion. The small spheres between the islands are assumed to be outliers or noise (i.e., documents with too common vocabulary). (c) documents inside the focus area are labeled with their titles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>'</head><label></label><figDesc>A61K..38/17', 'C12N...1/21', 'H04Q...7/22', 'B41C...1/10', 'C09D..11/00', 'C09J...7/02', 'G01N..33/53', 'H04Q..11/04'</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We use the sparse matrices kindly provided in<ref type="bibr" target="#b3">[4]</ref> 3 all diagrams can be arbitrarily magnified in the electronic version of this paper</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://www.ldc.upenn.edu</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">from 'Text of EP-A documents' and 'Text of EP-B documents'</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We thank Christian Heine for valuable remarks and inspiring discussions. We also thank the anonymous reviewers for their useful comments. This work was supported by a grant from the German Science Foundation (DFG), number SCHE663/4-1 within the strategic research initiative on Scalable Visual Analysis (SPP 1335). </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="table">sss  w  w  w  w w  w  w  w  ww w  w  w  w  w w  w w ww  w  w w  w  w   ww  w  w  w w w  w w  w  w  w  w w  w w  w  w  w  w  w w  w  w  w w w w  w  w  w  w w  w  w w  w  w  w  w  w w w  w  ww  w w w  ww  w  w  w  w  c</ref> </div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Brushing scatterplots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Cleveland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="127" to="142" />
		</imprint>
	</monogr>
	<note type="report_type">Technometrics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Topology and data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the AMS</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Computing contour trees in all dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoeyink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Axen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Geometry</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Two-stage Framework for Visualization of Clustered High Dimensional Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE VAST</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Excentric labeling: dynamic neighborhood labeling for data visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-D</forename><surname>Fekete</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Plaisant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI &apos;99: Proceedings of the SIGCHI conference on Human factors in computing systems</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Introduction to statistical pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Academic Press Professional, Inc</publisher>
			<pubPlace>San Diego, CA, USA</pubPlace>
		</imprint>
	</monogr>
	<note>2nd ed.</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A new statistical approach to geographic variation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Sokal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Systematic Zoology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="259" to="278" />
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">What is the nearest neighbor in high dimensional spaces?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hinneburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards automatic detection and tracking of topic change</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Holz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Teresniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CICLing 2010, Iaşi, LNCS 6008. Springer LNCS</title>
		<editor>A. Gelbukh</editor>
		<meeting>CICLing 2010, Iaşi, LNCS 6008. Springer LNCS</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>accepted for oral presentation</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Parallel coordinates: a tool for visualizing multi-dimensional geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Inselberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dimsdale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VIS &apos;90: Proceedings of the 1st conference on Visualization &apos;90</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="361" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dimensional reduction based on centroids and least squares for efficient processing of text data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Rosen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings for the First SIAM Intl. Workshop on Text Mining</title>
		<meeting>for the First SIAM Intl. Workshop on Text Mining</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Text categorization with support vector machines: Learning with many relevant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">T</forename><surname>Jolliffe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">When is &quot;nearest neighbor&quot; meaningful?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Shaft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">In Int. Conf. on Database Theory</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="217" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Iterative integration of visual insights during patent search and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Giereth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Symp. Visual Analytics Science and Technology</title>
		<meeting>IEEE Symp. Visual Analytics Science and Technology</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="203" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Multidimensional Scaling (Quantitative Applications in the Social Sciences)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Kruskal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wish</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978" />
			<publisher>SAGE Publications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visual analysis of high dimensional point clouds using topological landscapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Oesterling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Heine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jaenicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Scheuermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Pacific Visualization 2010 Proceedings</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Term weighting approaches in automatic text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<pubPlace>Ithaca, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Boostexter: A boosting-based system for text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="135" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Machine learning in automated text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N D</forename><surname>Ricerche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The challenges of clustering high-dimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steinbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ertöz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">New Vistas in Statistical Physics: Applications in Econophysics, Bioinformatics, and Pattern Recogn</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Applying manifold learning to plotting approximate contour trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fujishiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1185" to="1192" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Teresniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Scheuermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Holz</surname></persName>
		</author>
		<title level="m">Visualisierung von Bedeutungsverschiebungen in großen diachronen Dokumentkollektionen. Datenbank-Spektrum</title>
		<imprint>
			<date type="published" when="2009-12" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="33" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Topological landscapes: A terrain metaphor for scientific data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-T</forename><surname>Bremer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pascucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1416" to="1423" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Visualizing the non-visual: spatial analysis and interaction with information from text documents. Information Visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pennock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lantrip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pottier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Crow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Symposium</title>
		<imprint>
			<biblScope unit="page">51</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
