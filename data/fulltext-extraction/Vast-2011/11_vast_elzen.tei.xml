<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BaobabView: Interactive Construction and Analysis of Decision Trees</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stef</forename><surname>Van Den Elzen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Eindhoven University of Technology</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarke</forename><forename type="middle">J</forename><surname>Van Wijk</surname></persName>
							<email>vanwijk@win.tue.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">Eindhoven University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">BaobabView: Interactive Construction and Analysis of Decision Trees</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.1.2 [Information Systems]: User/Machine Systems-Human Information Processing</term>
					<term>H.2.8 [Database Management]: Database Applications-Data Mining</term>
					<term>I.3.6 [Computer Graphics]: Methodology and Techniques-Interaction Techniques</term>
				</keywords>
			</textClass>
			<abstract>
				<p>We present a system for the interactive construction and analysis of decision trees that enables domain experts to bring in domain specific knowledge. We identify different user tasks and corresponding requirements, and develop a system incorporating a tight integration of visualization, interaction and algorithmic support. Domain experts are supported in growing, pruning, optimizing and analysing decision trees. Furthermore, we present a scalable decision tree visualization optimized for exploration. We show the effectiveness of our approach by applying the methods to two use cases. The first case illustrates the advantages of interactive construction, the second case demonstrates the effectiveness of analysis of decision trees and exploration of the structure of the data.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>One of the main research areas in machine learning is classification, a supervised learning method. In a classification problem each item is defined by attribute values and a class label. The goal is to construct a model that predicts the target class label for an item given the attribute values. The model is constructed from a set of records, the training set, for which the attribute values and according class labels are known, such as historic data.</p><p>A decision tree is a model used for classification. It can be represented by a node-link diagram in which each internal node represents a test on an attribute, each link represents the according test value, or range of values, and each leaf node contains a class label. A decision tree is typically constructed by a recursive top-down divide-and-conquer algorithm <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>. The construction starts by considering the entire dataset, which is recursively partitioned into mutually exclusive subsets. The main decisions to be made during the construction are deciding on which attribute and test values to separate the dataset and when to stop splitting. The attribute choice is based on an impurity measure such as Gini Gain <ref type="bibr" target="#b8">[9]</ref>, Information Gain <ref type="bibr" target="#b32">[33]</ref> and Gain Ratio <ref type="bibr" target="#b33">[34]</ref>.</p><p>A decision tree is sensitive to noise and outliers in the dataset, causing the model to overfit the data. Two methods are used to overcome this; stop growing the tree early <ref type="bibr" target="#b32">[33]</ref> or fully grow the tree and then prune the tree by deleting subtrees <ref type="bibr" target="#b34">[35]</ref>. Pruning is typically performed using a proportion of the data, the prune set, which is not used for growing the tree. After construction, the prune set is evaluated on the decision tree and nodes are deleted until accuracy of the prune set no longer increases.</p><p>A decision tree is typically evaluated on a separate test or evaluation set containing records for which the class labels are known. The records are evaluated on the constructed decision tree, probably leading to misclassifications. The accuracy of the decision tree is defined as the proportion of misclassifications on this test set. However, accuracy is not the only quality measure. For a decision tree to be understandable, its complexity should be low, which can be measured by the following metrics <ref type="bibr" target="#b35">[36]</ref>: <ref type="bibr" target="#b0">(1)</ref> the total number of nodes; (2) total number of leaves; (3) tree depth; (4) number of attributes used. Small trees containing few attributes are therefore preferred.</p><p>Traditionally a decision tree is created by setting parameters, running an algorithm and evaluating the constructed tree. The parameters are tweaked and the algorithm is run again. This process is repeated until users are satisfied with the constructed decision tree. Once the decision tree is constructed, it is used to classify data for which the class labels are unknown.</p><p>Often, users constructing a decision tree are very knowledgeable in their field, but possess little knowledge of decision tree construction algorithms. Therefore, users do not know the exact meaning of all parameters and their influence on the constructed decision tree. Furthermore, they have little to no knowledge about the algorithms' inner workings. Because of this lack of knowledge, the decision tree construction is often a trial-and-error process and may be very time-consuming. Even worse, domain experts are not enabled to apply their domain specific knowledge to optimize the decision tree, because the algorithm is used as a black box and does not allow them to steer it. Ankerst et al. <ref type="bibr" target="#b3">[4]</ref> identify three important reasons to include domain knowledge and use visualization:</p><p>• by providing adequate data and knowledge visualizations, the pattern recognition capabilities of the human can be used to increase the effectiveness of decision tree construction;</p><p>• due to their active involvement, users have a deeper understanding of the resulting decision tree; and</p><p>• when obtaining intermediate results from the algorithm, users can provide domain knowledge to focus the further search of the algorithm. Using domain knowledge has been recognized as a promising approach for constraining knowledge discovery and for avoiding overfitting.</p><p>Liu and Salvendy <ref type="bibr" target="#b22">[23]</ref> extend this list further; the interactive construction process: improves the effectiveness of modeling; enhances users understanding of the algorithm; and gives them greater satisfaction with the task.</p><p>Recently the inability to incorporate domain knowledge in automatic decision tree construction is acknowledged in industry, where it is believed that interactivity can close the gap <ref type="bibr" target="#b7">[8]</ref>.</p><p>In this paper we present BaobabView, a tool for interactive construction and analysis of decision trees that enables domain experts to apply their domain specific knowledge. We think our tool provides a double example of a visual analytics approach. We show how a machine learning method can be enhanced using interaction and visualization; we also show how manual construction and analysis can be supported by algorithmic and automated support.</p><p>In Section 2 we identify user tasks and corresponding interaction, visualization and algorithmic requirements. Next we discuss related work in Section 3 and introduce our approach in Section 4. To show the effectiveness of our method, two use cases are discussed in Section 5. We conclude with a summary of our results and identify future work in Section 6.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IEEE Symposium on Visual Analytics Science and Technology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">USER TASKS AND REQUIREMENTS</head><p>Users either want to edit the tree (grow, prune or optimize), use the tree (classification) or analyse it (data exploration). Typically, users often switch between the edit and analysis process. For each task we identify important elements and extract requirements. In our research, we mainly focus on the edit and analysis process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Edit</head><p>The edit process can be divided in three subprocesses; grow, prune and optimize. New leaf nodes in the decision tree are created by splitting already existing leaf nodes. Pruning is achieved by deleting subtrees or by merging two sibling nodes into one node. Optimization is achieved by changing split attributes and tweaking split values.</p><p>Grow process In the grow process users construct a decision tree. First a leaf node is chosen based on the class distribution and node size. Next a split attribute is chosen based on an impurity measure or on the domain knowledge of the user. Splitpoints are defined such that misclassifications are minimized, again based on domain knowledge. Finally, the node is split and the process starts over (see <ref type="figure" target="#fig_1">Figure 1</ref>(a)).</p><p>Optimize process In the optimization process the decision tree is tweaked based on users' domain knowledge. Misclassifications on the training dataset are identified based on the confusion matrix <ref type="bibr" target="#b18">[19]</ref>. Next the involved internal nodes are identified and either the split attribute is changed or the split values are adjusted such that the cost of misclassifications is minimized (see <ref type="figure" target="#fig_1">Figure 1</ref>(b)). Often misclassifications of one class are considered worse than misclassifications of another class. Users tweak the split points based on this domain knowledge. Also, acquiring values for one attribute can be more costly than for other attributes <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>Prune process In the pruning process users generalize the constructed decision tree to prevent overfitting. First misclassifications on the prune set are evaluated with support of the confusion matrix. Next the user merges the involved node or deletes the entire subtree if this increases the accuracy on the prune set (see <ref type="figure" target="#fig_1">Figure 1</ref>(c)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Analysis</head><p>Analysis on the decision tree is performed to gain insight. By having a thorough understanding of the decision tree users also gained insight in the underlying structure of their data. The analysis is furthermore useful to determine whether the decision tree is reliable or not, or to see whether classification of a certain class is likely to be correct or not; trees in which one class label is scattered among many leaves are likely to misclassify this particular class. Furthermore leaves that contain few records are also likely to overfit the data and / or misclassify it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Requirements</head><p>We argue that a tight integration of visualization, interaction and algorithmic support is the key to a powerful system to support all different tasks. For each component, we define requirements in <ref type="table" target="#tab_2">Table 1</ref>. A decision tree visualized using a node-link diagram with involved process elements is shown in <ref type="figure" target="#fig_2">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RELATED WORK</head><p>First a short overview of the state of the art in decision tree visualization is given. Next interactive construction of decision trees is reviewed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Decision Tree Visualization</head><p>The most important visualizations that potentially could be used to represent decision trees are the indentation diagram, node-link diagram, treemap, treering and icicle plot. Treemaps and treerings are never proposed in literature and will not be discussed.</p><p>In the indentation diagram each internal and leaf node are shown textually. The parent-child relation is conveyed by indenting the child with respect to the parent. Ankerst et al. <ref type="bibr" target="#b2">[3]</ref>, Do <ref type="bibr" target="#b10">[11]</ref> and Poulet et al. <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref> use indentation diagrams to visualize decision trees. While this diagram displays each of the decision tree components (internal nodes, leaves, split predicates and class labels), it is poor in conveying the overall structure of the decision tree. If the tree is deep and has many nodes, it is hard to see which nodes are on the same level. Furthermore it is difficult to see the number of leaves, and the context may get lost if users have to scroll because of limited screen space.   The most well known tree visualization is the node-link diagram, used by Han and Cerone <ref type="bibr" target="#b16">[17]</ref>, Ware et al. <ref type="bibr" target="#b40">[41]</ref> and Zhang et al. <ref type="bibr" target="#b43">[44]</ref>. Internal and leaf nodes are represented by node glyphs and each parent-child relationship is represented by a link from parent to child node. In the node-link diagram, compared to the indentation diagram, it is easier to see the number of leafs and also to determine which nodes are on the same level. The split points are displayed on the links, the split attribute on the nodes and each leaf contains a class label (see <ref type="figure" target="#fig_2">Figure 2</ref>). Nguyen et al. <ref type="bibr" target="#b26">[27]</ref> use a 2.5D variation on the node-link diagram. Nodes of interest are drawn on top of blurred nodes which are not of interest. However, none of the proposed methods show the full class distribution at each leaf, and also node size is not conveyed. Barlow and Neville <ref type="bibr" target="#b5">[6]</ref> use multiple views. One view contains a node-link graph and another view shows an icicle plot. Wlodyka et al. <ref type="bibr" target="#b41">[42]</ref> show the percentage of the majority class at each leaf, still no full class distribution is presented. Pham et al. <ref type="bibr" target="#b27">[28]</ref> use a radial node-link diagram with fish-eye zoom interaction method to explore decision trees. None of the node-link variations integrate the data visualization with the tree visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimize</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prune</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Grow Merge</head><p>Ankerst et al. <ref type="bibr" target="#b3">[4]</ref> and Liu and Salvendy <ref type="bibr" target="#b22">[23]</ref> use icicle plots to visualize decision trees. The icicle plots naturally convey node size, which is a big advantage, and in both tree-and data visualization are integrated. Ankerst et al. use pixel-based bar charts <ref type="bibr" target="#b1">[2]</ref>. This is a very compact data visualization and scales well, up to the point where one record is smaller than a pixel, however one cannot see if there are (large) gaps between classes in terms of attribute values. The class distribution can be derived from each bar if the user is capable of estimating the number of records from one class by adding all pixels of one color. This task becomes difficult if class values are scattered among the node and there is no clear separation. Furthermore, this pixel-based technique is mainly applicable to continuous valued attributes. Split values as well as split attributes are not shown in the tree visualization. Liu and Salvendy use mosaic plots <ref type="bibr" target="#b13">[14]</ref> as data visualization, only targeted at categorical data. From these plots, the class distribution at each node can be derived. Also this visualization runs into scaling problems. From smaller nodes the mosaic plot is not visible any more. This problem gets worse as node sizes tend to get smaller and smaller, up to the point where a node glyph is smaller than a pixel.</p><p>Wang et al. <ref type="bibr" target="#b39">[40]</ref> propose to visualize decision trees using a combined icicle plot and matrix view. Each of the individual items are displayed in a similarity matrix view below the icicle plot. From this visualization the node distribution can be derived however, gaps between attribute values are not displayed because records are placed in sequence independent of value. Furthermore, the icicle plot tree visualization does not scale well. Teoh and Ma <ref type="bibr" target="#b37">[38]</ref> propose a variation on the icicle plot in which the current node of interest is shown as a large rectangle. The parent of each node is shown as a smaller rectangle left of each node and children are shown as smaller rectangles below the node. This tree visualization is integrated with data visualization by visualizing the individual records using either star- <ref type="bibr" target="#b25">[26]</ref> or parallel coordinates plots, however, nodes topologically far away from the focus node are displayed too small to meaningfully convey the data visualization.</p><p>Xu et al. <ref type="bibr" target="#b42">[43]</ref> propose to integrate decision trees with parallel coordinates plots, but due to the nature of parallel coordinates plots class distributions are not conveyed clearly, split attributes and split values are implicit.</p><p>Most proposed methods from the literature do not integrate the tree visualization with the data visualization. This has the advantage that both views can be optimized for their sole purpose and one view does not conflict with the other view or clutter it. However, this method also suffers from drawbacks. The first obvious drawback is the need to switch between the views, which may cause users to lose their context. Furthermore, only one node from the tree can be selected to show the data visualization for the records of that node. No overview is present in which both the overall tree structure is conveyed and the according data at the nodes. The few attempts in which the data visualization is well integrated with the tree visualization, are either aimed at continuous attributes or categorical attributes and, except for the method of Ankerst et al., do not seem to scale well.</p><p>Visualization of alternative datasets allows users to see the impact on the performance of the derived model <ref type="bibr" target="#b17">[18]</ref>, however, none of the described methods allow for this <ref type="bibr" target="#b24">[25]</ref>.</p><p>All current approaches are mainly targeted at supporting the grow process. As we argued earlier, the prune, optimize and analyse process are equally important. Some methods allow to prune the decision tree but no visual support is offered. To support users in all tasks, appropriate visualization techniques are developed and described in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Interactive Construction</head><p>Ankerst et al. <ref type="bibr" target="#b2">[3]</ref> enable the interactive construction of decision trees via the use of circle segments <ref type="bibr" target="#b4">[5]</ref>. Later, the circle segments were replaced by pixel-oriented bar charts <ref type="bibr" target="#b3">[4]</ref> and the interaction was expanded in three ways: by letting the computer propose a split to the user, by letting the computer expanding a subtree and by automatically growing the tree. Y. Liu and Salvendy <ref type="bibr" target="#b22">[23]</ref> provide similar interaction mechanics. However, no support for optimizing, pruning or analyzing the decision tree is provided. In Y. Liu and Salvendy <ref type="bibr" target="#b23">[24]</ref> (an extension on <ref type="bibr" target="#b22">[23]</ref>), users are enabled to automatically prune the decision tree after construction. However, the user is again not involved in the pruning process, making it impossible to incorporate domain knowledge. Han and Cerone <ref type="bibr" target="#b16">[17]</ref>   children of a node (pruning), and inspecting a node. However, this method is completely manual, no algorithmic support is provided. Teoh and Ma <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b25">26]</ref> allow for interactive construction of decision trees by painting an area of a star coordinate plot and assigning an according class label to it. This procedure is completely manual without algorithmic support, as well as the method of D. Liu et al. <ref type="bibr" target="#b21">[22]</ref>.</p><p>Ware et al. <ref type="bibr" target="#b40">[41]</ref>, Poulet <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref> and Do <ref type="bibr" target="#b10">[11]</ref> allow for bivariate decision trees by enabling users to respectively draw a split polygon and split line in a scatter plot visualization. In Ware et al. users are not algorithmically supported. Poulet and Do both support users algorithmically by suggesting the best split line.</p><p>Most systems partially meet the defined requirements on interaction, visualization and algorithmic support. However, no algorithmic support is provided to support the pruning process. In addition, algorithmic support on decision tree quality is limited and no system is capable of highlighting classes, highlighting attributes and visualizing alternative datasets to support the analysis process. Also dynamically changing split values or attributes to test alternative models is not supported, once a split attribute and value are chosen they can not be changed without first deleting all child nodes. We believe dynamically changing split points, attributes and datasets to see the effect on the constructed decision tree are essential in the grow, prune, optimize and analysis process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">BAOBABVIEW</head><p>In this section we present the solution we have developed to meet the requirements given and to improve on previous work. We named our tool BaobabView, because some of our tree visualizations (see <ref type="figure" target="#fig_1">Figures 10(b)</ref>, 13 and 14) resemble the characteristic shape of the baobab, an African tree. Also, in ancient times kings, elders and leaders would hold meetings under huge baobabs to discuss important matters. The trees provided shelter, and they believed that the spirit of the baobab would always help them make wise decisions <ref type="bibr" target="#b0">1</ref> .</p><p>We argue that the key for a successful system is tight integration of visualization, interaction and algorithmic support. Visualization is needed to provide insight in data, tree and classifier <ref type="bibr" target="#b0">1</ref> http://www.golimpopo.com/activity-detail baobab-tree 15.html behavior. Interaction is needed to incorporate domain knowledge in the decision tree, when choosing split attributes and split values, and pruning the tree. Furthermore, interaction is needed to enable hypothesis testing, by changing split attributes and values. Finally, algorithmic support is needed to support users in construction and enable decision tree evaluation.</p><p>In BaobabView users are enabled to manually construct a decision tree, automatically grow a selected node or import a complete decision tree, generated by the J48 algorithm of Weka <ref type="bibr" target="#b15">[16]</ref> for exploration or optimization. Users are enabled to optimize and prune a decision tree. Exploration is supported in analysing both the decision tree and underlying structure of the data. Users are enabled to easily switch between tasks. For each task, users are supported by linked views, preset layouts and according interaction techniques. <ref type="figure" target="#fig_3">Figure 3</ref> shows the complete user interface of BaobabView.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Decision Tree View</head><p>The first design decision concerns the visualization of the tree structure. Barlow and Neville <ref type="bibr" target="#b6">[7]</ref> compared node-link diagrams, treemaps, tree-rings and icicle plots on their ease of identification of tree topology; ease of identification of node relationships; ease of identification of leaf sizes; and user preference. From this study the icicle plot and node-link diagram were the most favorable. The node-link diagram has the disadvantage that node size is not demonstrated, therefore an icicle plot would be preferred. However, because we want a tight integration of tree and data visualization, we need the data visualizations to be displayed on the nodes, which inevitable leads to aspect ratio problems when using an icicle plot; nodes deeper in the tree get smaller and smaller until they no longer convey the data visualizations in a meaningful manner.</p><p>By using a node-link diagram as opposed to the icicle plot we are given an extra component, namely the link, which we use to convey node size. The width of each link is set proportional to the number of items that is flowing from the parent node to the child node, e.g., if the incoming link at a node is thick then many items are contained in this node, if the incoming link is very thin then only a few items are present. To visualize the distribution of classes for a link between two nodes, the wide link is split into bands, each colored according to the class and given the according proportional width. To convey the link better and make it aesthetically more  pleasing, the link is drawn as a Bézier curve. Finally, we choose for a top-down node-link diagram as opposed to a radial node-link diagram, because here the positions of the nodes have a semantic meaning, i.e., child nodes can be horizontally ordered on split predicates; child nodes containing records with lower values of the split attribute are positioned to the left of nodes with higher values of the split attribute.</p><p>As a default layout algorithm to position the nodes of the decision tree we use the Sugiyama algorithm <ref type="bibr" target="#b36">[37]</ref> as part of the graphviz <ref type="bibr" target="#b12">[13]</ref> implementation, which performs the steps described by Gansner et al. <ref type="bibr" target="#b14">[15]</ref>.</p><p>Nodes are visualized as rectangles, showing relevant information as stacked elements to users for performing their tasks (see <ref type="figure" target="#fig_5">Figure 4)</ref>. The first two elements are relevant for all nodes, the others primarily for interior nodes. All elements can optionally be hidden, if an aspect is not relevant or if an overview is needed.</p><p>The split predicate is displayed using plain text. Because the predicates are based here on a single attribute, they are easy to understand and we do not need a sophisticated visualization metaphor.</p><p>The class distribution of the set of items at a node is visualized using a horizontal bar. Each class is given an area of the bar, proportional to the class quantity. This visualization has some nice properties. It is space-filling, and in our opinion, from the horizontal bar visualization it is easy to perceive if a node contains multiple classes or consists of only one class. This information is important to decide if we need to split the node further.</p><p>The attribute class values and distributions (data visualization) for a prospective split attribute are visualized using a Streamgraph <ref type="bibr" target="#b9">[10]</ref>. Each class is visualized using a different colored stack in the Streamgraph. In our prototype we experimented with four other data visualizations: dot plots, boxplots, stacked histogram and smoothed stacked histogram (see <ref type="figure" target="#fig_7">Figure 5</ref>). We enable users <ref type="figure">Figure 6</ref>: Ranked and sorted attributes based on gain ratio for the segment dataset from the UCI repository <ref type="bibr" target="#b0">[1]</ref>.</p><p>to visualize any combination of data visualizations, for example, a Streamgraph with overlayed dot plots to identify individual values. We found that the Streamgraph yields the most effective perception of class distribution and quantities. For nominal attributes, we present a stacked histogram to users in which each bar represents one enumerated value.</p><p>Splitpoints are visualized using a dashed line, the split value is displayed in a yellow box at the lower end of the splitline. The Streamgraph data visualization is not very precise, and while the relative data quantities can be perceived (colored areas), the absolute values are not present. We solve this problem by introducing (movable) splitpoints with class histograms at both sides of the splitpoint. The histogram bars are colored according to the classes. The value is displayed inside the bar, denoting the actual number of items at the left and right side of the splitpoint. The true quantities are perceived and the user is enabled to see the effect of the split by moving the splitpoint. Note that when the splitpoint is dragged, the sum of the number of elements in the left and right interval remains constant, and hence the total width of the histogram bars remains constant. This gives a stable and smooth impression; the user can drag the splitpoint and watch the bars move.</p><p>The tree overview provides a contextual overview of the main view and navigation support. Both the main view and the overview support zooming, panning and filtering (collapsing and expanding subtrees).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Attribute View</head><p>The attribute view shows the data visualization of each attribute for a selected node. The attributes are sorted based on a user selected impurity measure (Gini Gain <ref type="bibr" target="#b8">[9]</ref>, Information Gain <ref type="bibr" target="#b32">[33]</ref>, Gain Ratio <ref type="bibr" target="#b33">[34]</ref>). As default impurity measure, Gain Ratio <ref type="bibr" target="#b33">[34]</ref> is used, such that users can directly see the most appropriate candidate attributes for splitting the node. The gain ratio is a value between zero and one; a value of one is assigned if a class can be perfectly separated from the other classes, a value of zero is assigned if the attribute does not allow separating one class from the other classes. The Streamgraphs show the class distributions, and show users which classes can be split off. If we rank and sort the attributes horizontally with highest gain ratio value at the left and worst gain ratio value at the right end, we obtain a visualization as shown in <ref type="figure">Figure 6</ref>. Each attribute is given a thick colored border (user-defined colormap) with hatch pattern indicating the goodness of split for the according attribute; in the example, green and fine-  grained hatch pattern indicates a gain ratio value of 1 whereas red and a coarse-grained hatch pattern represents a gain ratio value of 0. The sorted list in combination with the domain knowledge of users allows them to choose the appropriate attribute for splitting the nodes. Furthermore, users are enabled to inspect a table showing correlation values of all attribute pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Visual Confusion Matrix</head><p>A decision tree can be evaluated on misclassifications by inspecting the confusion matrix. A confusion matrix M is an n × n matrix, where n is the number of classes. It displays the number of correct and incorrect classifications. The horizontal axis denotes the known class labels for each record, the vertical axis denotes the class labels as classified by the decision tree. On the intersection of a row i and a column j the number of classifications M i, j is displayed. The diagonal axis shows all correctly classified items. From this diagonal axis, we can calculate the accuracy of the tree which is displayed in the upper left corner of the matrix. Items that are off diagonal are misclassifications. <ref type="figure" target="#fig_9">Figure 7</ref> shows our visual confusion matrix. Each cell in the visual confusion matrix is given a varying grayscale color according to the quantity M i, j . This color encoding allows one to quickly see if there are cells that contain many misclassified items. Each non-empty cell in the matrix contains two stacked rectangles. The width of the rectangles is proportional to M i, j . The upper rectangle is colored to the known class label and the lower rectangle to the given class label (see <ref type="figure" target="#fig_10">Figure 8</ref>). If the item is misclassified then the stacked rectangles both contain colors from the given class and the known class, enabling users to see if a class is misclassified. Each item on the horizontal axis shows a summary of the according column by a rectangle made up of plac-   ing each item in the column as a sequence. This rectangle represents the recall <ref type="bibr" target="#b18">[19]</ref> of a class. If the recall is high, then the rectangle is uniformly colored; if it is low, it is made up of many different class colors. Similarly, the vertical axis items represent a summary of each row, showing the precision <ref type="bibr" target="#b18">[19]</ref> of a class. Finally, each diagonal item shows the harmonic mean of the precision and recall, known as the f -measure.</p><p>The visual confusion matrices in the user interface (see <ref type="figure" target="#fig_3">Figure 3</ref>) show the classification results for both the train and prune set of the current decision tree and are linked to the main-and the data view. If the mouse is hovered over a cell in the confusion matrix the according nodes in the decision tree, which misclassify these items, are highlighted. If a cell of the confusion matrix is clicked, the actual misclassified records are highlighted in the data view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Interaction</head><p>For each of the user tasks interaction techniques are implemented according to the requirements. Dependent on operations, different tree layouts are useful, which are discussed in the next sections. We implemented many options to control the layout of the tree and the visualization of the nodes, such that a wide variety of designs can be produced and evaluated. For each user task an according preset layout is defined. Users are enabled to easily switch between the different preset layouts. We use animation techniques to guide the user in the transition. Close-up inspection of the tree is facilitated via zoom-pan options and a separate navigation window.</p><p>Grow Users are enabled to first choose a leaf node, next choose the split attribute and finally define split point values for this attribute. If splitpoints are added to a node, the splitpoints are positioned such that class separation according to gain ratio is as high as possible. The user is then enabled to fine tune the splitpoint value. We enable users to select a node and grow the tree automatically further if manual splitting is too challenging.</p><p>In the growing process users quickly need to identify leaf nodes that need to be split further, therefore they are enabled to lay out all leaf nodes on the same level. As an additional feature, users can see the leaf distributions by grouping leaf nodes based on majority class, or use a combination of both layouts. <ref type="figure" target="#fig_12">Figure 9</ref> shows the default and optional layouts.</p><p>Prune At each internal node, the accuracy of the train and prune set that would be the result of pruning the subtree is computed. If the prune set accuracy of the entire tree would be better when the subtree of a node is pruned, the node is increased in size and colored (user-defined colormap) according to which node could be pruned best first, in line with the cost complexity pruning method <ref type="bibr" target="#b8">[9]</ref>. We enable users to prune a subtree or to merge a leaf node with a sibling node. Furthermore, users are enabled to inspect outliers and noise through the inspection of dot plots at each node. By hovering over a dot, the according attribute values are highlighted in the data view.</p><p>For the layout of the nodes in the decision tree, we adapted the Sugiyama algorithm such that edges with greater weight are drawn shorter and more straight. Each edge is given a weight according to the number of items. Nodes with fewer items, likely overfitting the data, are therefore pushed to the side of the tree and are easily identified.</p><p>Optimize Optimization of a decision tree is achieved by changing split attributes or split point values on any node in the decision tree. Nodes that are subject to optimization can be identified using both visual confusion matrices. Furthermore, users are enabled to use their domain knowledge to change the split attribute at a node or fine-tune the split value(s) such that misclassification cost is minimized. Also, users are enabled to apply domain knowledge in terms of attribute cost to optimize the decision tree. Costly attributes used   in the decison tree can be changed to alternative attributes. All optimizations made to the decision tree are propagated to the involved subtrees which are updated in realtime. Additionally, all linked views such as the confusion matrices and attribute views are updated to reflect all changes. This enables users to directly see the effect of different hypotheses. Finally, users can load different datasets and visualize them by the constructed decision tree. This visually shows the effect the decision tree has on different datasets and misclassifications, next to that, overfitting can be detected.</p><p>Analysis When analyzing the overall tree and data, users are mainly interested in the structure. Therefore, we put emphasis on the links and do not show the Streamgraph and splitpoints at each node in the analysis preset layout mode. Furthermore, the class distribution is not shown because it can be derived from the links. Additionally, we do not show the node glyphs but only the split predicate. Each link is drawn as a continuous stream of items, from root to leaf node. <ref type="figure" target="#fig_1">Figure 10</ref> shows the decision tree preset layouts for the grow and analyse process.</p><p>For the preset layout, we use the adapted Sugiyama algorithm with weighted edges. The layer separation height depends on the number of items in the nodes. Links containing more items are given a greater height. In addition, the width of the links and nodes are proportional to their size. To optimize the readability and interpretation of the decision tree we minimize the number of crossing edges by sorting the class sequence at each node. The class ordering is determined by taking the weighted x-position of the leafs for one class and then sorting these.</p><p>Additionally we allow users to focus on interesting classes by highlighting of the involved paths. Important attributes used in the decision tree can be explored by giving each node the according split attribute color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">USE CASES</head><p>In the following sections, two case studies are presented that demonstrate the edit and analyse process. The first use case shows the interactive construction process, including growing, pruning and optimization. The second use case shows the effectiveness and scalability of the continuous color-banded links on complex decision trees for analysis purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Interactive Construction</head><p>In this use case, we construct a decision tree for image segmentation data <ref type="bibr" target="#b0">[1]</ref>. The instances, regions of 3×3 pixels, are drawn randomly from a database of 7 outdoor images. The images were next handsegmented to create a classification for every pixel. The dataset <ref type="figure" target="#fig_1">Figure 11</ref>: Decision tree after 10 splits. Highlighted node is difficult to split manually. Right image shows some Streamgraphs of the alternative attributes that can be used to split the node. Note there is no attribute that can be used to separate the involved classes easily.</p><p>contains 2310 instances, 19 continuous attributes and 7 classes. We divide the dataset into 3 subsets; the first, 40 percent of total dataset, to use as train set in BaobabView; the second, 30 percent, to use as prune set; the third, 30 percent of total dataset, to evaluate the constructed decision tree.</p><p>Within a few minutes we make the first 10 splits, which are fairly straightforward choices. In the splitting process we choose for different attributes and splitpoints than presented to us by the system (the choices the classifier algorithm would take). Often the algorithm tries to split off only a few items, ignoring the main classes to be separated. For example in a node with three classes and distribution D = {2,28,25} the algorithm would split off the class with two items, while we decide it is better to separate the classes with 28 and 25 items. Splitting of the two items does not contribute much to the decision tree and is likely to overfit it.</p><p>Next we are confronted with a node that does not lend itself for separation easily (see <ref type="figure" target="#fig_1">Figure 11</ref>). After inspection of the alternatives in the attribute view, we decide manual splitting this node further is too hard. We therefore choose to automatically grow this node further. Next we decide we are done growing the tree, there are no leaf nodes left that have a highly impure class distribution. Now we want to prune the tree based on the accuracy of the prune set. We select the prune mode and are presented with the nodes that have a higher or equal accuracy on the prune set, if this node's subtree would be pruned. We see that many of the automatically generated branches can be pruned. We prune the tree until there is no accuracy improvement on the prune set. Now we inspect the confusion matrices whether one class is misclassified a lot. This is not the case and we conclude we are done constructing the decision tree, presented in <ref type="figure" target="#fig_1">Figure 12</ref>.</p><p>Our resulting tree has 29 nodes in total and uses only 6 of the 19 attributes. Now we evaluate the constructed decision tree on the evaluation dataset. The accuracy of our constructed tree on the evaluation set is 93.2%. To compare our method to a traditional decision tree construction we construct a decision tree with the J48 algorithm of Weka (including pruning with default settings) on the same datasets and evaluate on the same evaluation dataset. The tree has an accuracy of 94.5% on the evaluation dataset, slightly better than our approach, however the constructed decision tree contains 75 nodes, more than 2.5 times our number of nodes. Furthermore, <ref type="figure" target="#fig_1">Figure 12</ref>: Decision tree in grow mode after automatic growing of hard to separate node (left). Decision tree in prune mode showing nodes bigger that can be pruned according to the prune set accuracy (middle). Final decision tree after pruning in analyse mode (upper right), also shown with nodes colored according to split attributes (lower right).</p><p>there are 12 involved attributes in the decision tree, twice as much as compared to our number of attributes used. Our tree is therefore easier to interpret and because we were actively involved in the construction process, we gained additional knowledge about the data. For example, we observed that the grass, path, cement and brickface classes are relatively easy to separate from the rest of the classes and that the foliage and window classes are difficult to separate from each other. Furthermore, we were enabled to use our domain knowledge by choosing appropriate attributes, tweaking splitpoints and decide which subtrees to prune.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Data Exploration</head><p>For the exploration use case we use medical data to determine the location of primary tumors. The data is provided by the Institute of Oncology of the University Medical Centre in Ljubljana, Yugoslavia <ref type="bibr" target="#b19">[20]</ref> and obtained from the UCI repository <ref type="bibr" target="#b0">[1]</ref>. The data consists of 22 classes and 17 attributes. We choose to import the decision tree, generated by the Weka J48 algorithm (Weka implementation of the C4.5 algorithm of Quinlan <ref type="bibr" target="#b32">[33]</ref>), and use this to explore the underlying structure of the data. The imported tree consists of 154 nodes; 66 internal nodes and 88 leaf nodes.</p><p>We switch to analyse mode such that paths are visualized as continuous streams of items from root to leaf and no node glyphs are present, as described in Section 4.4 (see <ref type="figure" target="#fig_1">Figure 13)</ref>. From the visualization we directly see that lung, breast and, head and neck are easy to distinguish from the rest because they lead to thick streams and do not split much. We see that rectum and stomach tumors are very similar and hard to separate, because the two streams are entangled. The same thing is true for pancreas and gallbladder tumors. Furthermore we notice from both the tree and the visual confusion matrix that stomach tumors are often misclassified as ovary tumors when using this decision tree, therefore extra care should be taken if ovary is determined.</p><p>Next we are interested whether there are any differences in diagnoses for male and female. We prune the entire tree and split the root node on the sex attribute. Subsequently we choose to automatically grow the decision tree from both the constructed male and female child nodes. We see <ref type="figure" target="#fig_1">(Figure 14</ref>) that head and neck, colon, testis, liver and prostate tumors are more diagnosed in males; females are diagnosed with breast, gallbladder, and ovary.</p><p>By highlighting individual class paths we observe that both sexes are diagnosed with pancreas, stomach and lung tumors. The paths of these classes split high in the tree and continue their paths to both the male and female branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>Our aim was to develop a tool for the interactive construction and analysis of decision trees that supports all aspects of this process, and offers a combination of visualization and automated support, such that domain experts are enabled to bring in their domain knowledge and to perform their tasks effectively and efficiently. We approached this as a design problem. Rather than inventing just one new visualization and building on top of that, we first analyzed the activities in detail, derived requirements from these, and next generated and selected solutions for all aspects. A simple but important lesson we learned was that different activities require different visualizations, each showing the most relevant information as clearly as possible. We used a multiple view approach, but also found that a parametrized visualization of the tree itself was useful and effective. This enables the user to select different layouts and different information to be shown, where smooth, animated transitions help to maintain situational awareness.</p><p>We think our approach is an advancement over previous solutions, especially concerning the span of supported tasks (construction, pruning, analysis), the tight integration of automated methods, and the rich variety of visualizations offered. As far as we know, none of the systems proposed in literature offers this set of features. As an example, the work of Ankerst et al. does not support users in the pruning and analysing tasks, does not allow for dynamic changes and visualization of alternative datasets and also, we think that pixel-based bar charts are less clear than Streamgraphs. We have built upon a number of existing methods, such as a node-link diagram to visualize the tree, Streamgraphs to visualize distributions, and the confusion matrix to show information. Our particular combination and integration is novel as such, but also we provide new solutions for a variety of issues. Specifically, we developed a flexible layout method for trees; visual annotations to the confusion matrix; a widget to enable the user to modify split points while giving detailed feedback on the effect of changes; and the use of color-banded edges with variable width to visualize the flow of the data through the tree.</p><p>Finally, we have shown the effectiveness of the tool for different types of use by presenting two use cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Future Work</head><p>In our research, we mainly focused on the edit and analyse user tasks. We think that interactive use of decision trees can be valuable and should be explored in future research, for instance to show users why certain classifications were made. We have shown our tool to three experts in decision trees, and they were very positive and appreciated the wide range of functionality offered and the clear visual feedback. However, the application should be tested more thoroughly by domain experts for real-world cases to identify the value in practice together with a formal user study evaluation. Setting up a controlled, quantitative experiment will not be easy, as we do not aim just for improved accuracy, but also hope to provide users with more insight and confidence in the results, which aspects are difficult to quantify. Finally, formal comparisons of our approach with other decision tree visualization methods should be conducted.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Steps of the (a) grow process; (b) optimize process; and (c) prune process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Decision tree node-link representation with annotated user tasks grow, prune and optimize.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Interface of the interactive decision tree construction software with according proposed decision tree visualization; Based on adapted node-link diagram. Nodes contain important decision tree components. Links are visualized as a stream of items.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>items left of splitpoint Number of class items right of splitpoint</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Proposed solution to visualize a node.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Different data visualization methods (individual or combination) provided to users. (a) Dot plots: Over-plotting is solved by transparency. Outliers can be detected. (b) Boxplots: Showing individual class distributions. However, area of the boxplots is misleading because it does not represent quantity. For example quantities for red and blue only differ 3 items. (c) Stacked histogram: Showing individual class distributions as well as quantities. Interpretation is difficult due to discontinuities in colors. (d) Smoothed histogram: Discontinuities are prevented for easier interpretation. (e) Streamgraph: Reducing wiggles and spikes. In our opinion, the Streamgraph is best suited to see individual class distributions as well as quantities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Visual confusion matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Visual confusion matrix cells explained.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>(a) Default layout. (b) Leaves grouped. (c) Leaves on same level. (d) Leaves grouped on same level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>Additional layouts providing users more insight. (a) Default grow process layout. (b) All leaves are grouped based on majority class, enabling users to see the deepest level of a class in the tree and the number of involved leaves. (c) All leaves on same level. Enabling users to identify which nodes need to be split further, based on the class distribution bars. (d) All leaves grouped on majority class and positioned on the same level. Providing users with insight on leaf distributions. (a) Standard node-link diagram, as preset layout for grow process, showing data visualizations and class distributions at each node. (b) Same tree as Figure 10(a) optimized for analysis. Note that, complementary to the standard node-link diagram, much insight becomes available by the use of color-banded links: bus and van class are easy to separate from the rest of classes. Opel and Saab class are difficult to separate. Opel is misclassified often as Saab using this decision tree (Blue stream ending in green node).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 :</head><label>10</label><figDesc>Tree layouts for editing and analysis processes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 :</head><label>13</label><figDesc>Primary tumor location decision tree imported from Weka, showing easy to separate classes (upper right), hard to separate classes (lower right) and misclassifications (lower left).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 14 :</head><label>14</label><figDesc>Primary tumor location decision tree constructed by manual first-split on sex, next automatically constructed. Showing typical diagnoses for males (lower left) diagnoses for females (lower right) and diagnoses of both sexes (upper right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Requirements to support users in interactive decision tree construction tasks.</figDesc><table><row><cell></cell><cell>Interaction</cell><cell>Visualization</cell><cell>Algorithmic support</cell></row><row><cell></cell><cell cols="2">Users need to be enabled to Show</cell><cell>The system should</cell></row><row><cell>Grow</cell><cell>choose leaf node</cell><cell>class distribution at each leaf node</cell><cell>suggest a leaf node based on class distribution</cell></row><row><cell></cell><cell>choose split attribute</cell><cell>class distribution per attribute</cell><cell>suggest a split attribute based on impurity measure</cell></row><row><cell></cell><cell>define split points</cell><cell>node size</cell><cell>suggest split points for the chosen attribute</cell></row><row><cell></cell><cell>let system grow subtree</cell><cell></cell><cell>Automatically grow a subtree from a chosen node</cell></row><row><cell cols="2">Optimize change split attribute</cell><cell>confusion matrix of training set</cell><cell>determine misclassifications on training set</cell></row><row><cell></cell><cell>change split points</cell><cell>alternative attributes</cell><cell></cell></row><row><cell>Prune</cell><cell>delete subtrees</cell><cell>confusion matrix of prune set</cell><cell>determine misclassifications on prune set</cell></row><row><cell></cell><cell>merge two sibling nodes</cell><cell>prune set accuracy at each node</cell><cell>determine prune set accuracy at each node that</cell></row><row><cell></cell><cell></cell><cell></cell><cell>would be the result of pruning the subtree</cell></row><row><cell>Analyse</cell><cell>highlight class(es)</cell><cell>whether a class is easy to separate from other classes</cell><cell></cell></row><row><cell></cell><cell>highlight attribute(s)</cell><cell>what attributes are involved in the separation process</cell><cell></cell></row><row><cell></cell><cell>choose alternative datasets</cell><cell>whether classes are difficult to separate from each other</cell><cell></cell></row><row><cell></cell><cell></cell><cell>effect on decision tree using alternative datasets</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>support the construction of decision trees in five ways: splitting a node (growing), labelling a leaf node, unlabelling a leaf node, deleting all</figDesc><table><row><cell>Decision tree Main view</cell><cell>Visual confusion matrix train set</cell></row><row><cell>Node</cell><cell></cell></row><row><cell>Preset layouts</cell><cell>Confusion matrices</cell></row><row><cell>Link</cell><cell>magnify view</cell></row><row><cell></cell><cell>(on mouse hover)</cell></row><row><cell>Attribute view</cell><cell></cell></row><row><cell></cell><cell>Tree overview</cell></row><row><cell></cell><cell>Visual confusion</cell></row><row><cell>Attribute legend</cell><cell>matrix prune set</cell></row><row><cell>Alternative datasets</cell><cell></cell></row><row><cell></cell><cell>Data view</cell></row><row><cell>Class legend</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N A</forename><surname>Asuncion</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Visual data mining with pixel-oriented visualization techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ankerst</surname></persName>
		</author>
		<idno>3707 MC 7L-70</idno>
		<imprint>
			<date type="published" when="2001" />
			<pubPlace>Seattle, WA 98124</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>The Boeing Company P</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Visual classification: an interactive approach to decision tree construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ankerst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;99: Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="392" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards an effective cooperation of the user and the computer for classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ankerst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD &apos;00</title>
		<meeting>the sixth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD &apos;00<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="179" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Circle segments: A technique for visually exploring large multidimensional data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ankerst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings on Visualization, Hot Topic Session</title>
		<meeting>on Visualization, Hot Topic Session<address><addrLine>San Francisco, CA, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Case study: Visualization for decision tree analysis in data mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Barlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Neville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Symposium on Information Visualization 2001 (INFOVIS&apos;01)</title>
		<meeting>the IEEE Symposium on Information Visualization 2001 (INFOVIS&apos;01)<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="149" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A comparison of 2-d visualizations of hierarchies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Barlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Neville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Information Visualization</title>
		<meeting><address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">131</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Interactivity closes the gap: Lessons learned in an automotive industry application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blumenstock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lanquillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kempe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hipp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wirth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the 2010 conference on Data Mining for Business Applications</title>
		<meeting>eeding of the 2010 conference on Data Mining for Business Applications<address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="17" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Classification and Regression Trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
			<publisher>Wadsworth and Brooks</publisher>
			<pubPlace>Monterey, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Stacked Graphs -Geometry &amp; Aesthetics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Byron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1245" to="1252" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Towards simple, easy to understand, and interactive decision tree algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-N</forename><surname>Do</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
		<respStmt>
			<orgName>College of Information Technology, Cantho University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Metacost: a general method for making classifiers costsensitive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD &apos;99</title>
		<meeting>the fifth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD &apos;99<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="155" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ellson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Gansner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Koutsofios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>North</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Woodhull</surname></persName>
		</author>
		<title level="m">Graphviz -Open Source Graph Drawing Tools. Graph Drawing</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="483" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Visualizing Categorical Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Friendly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>SAS Publishing</publisher>
		</imprint>
	</monogr>
	<note>1st edition</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A technique for drawing directed graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gansner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Koutsofios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>North</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-P</forename><surname>Vo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="214" to="230" />
			<date type="published" when="1993-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The weka data mining software: an update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Reutemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explor. Newsl</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="10" to="18" />
			<date type="published" when="2009-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Interactive construction of decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cercone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Pacific-Asia Conference on Knowledge Discovery and Data Mining, PAKDD &apos;01</title>
		<meeting>the 5th Pacific-Asia Conference on Knowledge Discovery and Data Mining, PAKDD &apos;01<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="575" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Model visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Johnston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<biblScope unit="page" from="223" to="227" />
			<pubPlace>San Francisco, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Glossary of terms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Provost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="271" to="274" />
			<date type="published" when="1998-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Experiments in automatic learning of medical diagnostic rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kononenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bratko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Roskar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
			<pubPlace>Ljubljana</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Faculty of Electrical Engineering, E. Kardelj University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Test strategies for cost-sensitive decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1055" to="1067" />
			<date type="published" when="2006-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Polycluster: an interactive visualization approach to construct classification rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sprague</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning and Applications</title>
		<meeting>International Conference on Machine Learning and Applications</meeting>
		<imprint>
			<date type="published" when="2004-12" />
			<biblScope unit="page" from="280" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Design and evaluation of visualization support to facilitate decision trees classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Salvendy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Hum.-Comput. Stud</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="95" to="110" />
			<date type="published" when="2007-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Interactive visual decision tree classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Salvendy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th international conference on Humancomputer interaction: interaction platforms and techniques, HCI&apos;07</title>
		<meeting>the 12th international conference on Humancomputer interaction: interaction platforms and techniques, HCI&apos;07<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="92" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Visualization support to better comprehend and improve decision tree classification modelling process: a survey and appraisal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Salvendy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Issues in Ergonomics Science</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="92" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Starclass: Interactive visual classification using star coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Teoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd SIAM International Conference on Data Mining</title>
		<meeting>the 3rd SIAM International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="178" to="185" />
		</imprint>
	</monogr>
	<note>The 3rd SIAM International Conference on Data Mining</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Interactive visualization in mining large decision trees. Knowledge Discovery and Data Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shimodaira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Issues and New Applications</title>
		<imprint>
			<biblScope unit="volume">1805</biblScope>
			<biblScope unit="page" from="345" to="348" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Interactive exploration of decision tree results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-K</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-N</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Poulet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Morin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Applied Stochastic Models and Data Analysis, ASMDA&apos;07</title>
		<meeting><address><addrLine>La Cane, Grce</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="152" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">High dimensional visual data classification. Pixelization Paradigm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Poulet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">4370</biblScope>
			<biblScope unit="page" from="25" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Visual data mining. Towards Effective Visual Data Mining with Cooperative Approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Poulet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="389" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cooperation between automatic algorithms, interactive algorithms and visualization tools for visual data mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Poulet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Recherche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Visual Data Mining @ ECML, the 2nd Int. Workshop on Visual Data Mining, PAKDD &apos;02</title>
		<meeting>Visual Data Mining @ ECML, the 2nd Int. Workshop on Visual Data Mining, PAKDD &apos;02</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Induction of decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="81" to="106" />
			<date type="published" when="1986-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">C4.5: programs for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<pubPlace>San Francisco, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Improved use of continuous attributes in c4.5</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Int. Res</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="77" to="90" />
			<date type="published" when="1996-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Simplifying decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Hum.-Comput. Stud</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="497" to="510" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Top-down induction of decision trees classifiers -a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rokach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Maimon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="476" to="487" />
			<date type="published" when="2005-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Methods for visual understanding of hierarchical system structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Toda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="125" />
			<date type="published" when="1981-02" />
		</imprint>
	</monogr>
	<note>Man and Cybernetics</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Paintingclass: interactive construction, visualization and exploration of decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Teoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD &apos;03</title>
		<meeting>the ninth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD &apos;03<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="667" to="672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Types of cost in inductive concept learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Turney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Concept tree based clustering visualization with shaded similarity matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gasser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining</title>
		<meeting><address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="page">697</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Interactive machine learning: letting users build classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ware</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Studies</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="281" to="292" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Visualization of decision rules -from the cardiologists point of view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wlodyka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mlynarski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ilczuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pilat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kargul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Cardiology</title>
		<imprint>
			<biblScope unit="page" from="645" to="648" />
			<date type="published" when="2008-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Parallel filter: A visual classifier based on parallel coordinates and multivariate data analysis. Advanced Intelligent Computing Theories and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">With Aspects of Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">4682</biblScope>
			<biblScope unit="page" from="1172" to="1183" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Vdm-rs: A visual data mining system for exploring and classifying remotely sensed images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gruenwald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gertz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Geosci</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1827" to="1836" />
			<date type="published" when="2009-09" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
