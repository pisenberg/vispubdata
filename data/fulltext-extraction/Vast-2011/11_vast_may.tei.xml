<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Guiding Feature Subset Selection with an Interactive Visualization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>May</surname></persName>
							<email>thorsten.may@igd.fraunhofer.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Bannach</surname></persName>
							<email>andreas.bannach@igd.fraunhofer.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Davey</surname></persName>
							<email>james.davey@igd.fraunhofer.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Ruppert</surname></persName>
							<email>tobias.ruppert@igd.fraunhofer.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörn</forename><surname>Kohlhammer</surname></persName>
							<email>joern.kohlhammer@igd.fraunhofer.de</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Fraunhofer Institute for Computer Graphics Research</orgName>
								<address>
									<settlement>Darmstadt</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">IEEE Symposium on Visual Analytics Science and Technology October</orgName>
								<address>
									<addrLine>23 -28</addrLine>
									<settlement>Providence, Rhode Island</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Guiding Feature Subset Selection with an Interactive Visualization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>G.3 [Mathematics of Computing]: Probability and Statistics-Contingency Table Analysis</term>
					<term>H.5.2 [Information Interfaces and Presentation]: User Interfaces-Graphical User Interfaces</term>
					<term>I.5.2 [Pattern Recognition]: Design Methodology-Feature Evaluation and Selection</term>
				</keywords>
			</textClass>
			<abstract>
				<p>We propose a method for the semi-automated refinement of the results of feature subset selection algorithms. Feature subset selection is a preliminary step in data analysis which identifies the most useful subset of features (columns) in a data table. So-called filter techniques use statistical ranking measures for the correlation of features. Usually a measure is applied to all entities (rows) of a data table. However, the differing contributions of subsets of data entities are masked by statistical aggregation. Feature and entity subset selection are, thus, highly interdependent. Due to the difficulty in visualizing a high-dimensional data table, most feature subset selection algorithms are applied as a black box at the outset of an analysis. Our visualization technique, SmartStripes, allows users to step into the feature subset selection process. It enables the investigation of dependencies and interdependencies between different feature and entity subsets. A user may even choose to control the iterations manually, taking into account the ranking measures, the contributions of different entity subsets, as well as the semantics of the features.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Bethany is preparing for the analysis of a data table. We will call the columns of the table features and the rows entities. Bethany's data table contains data collected about patients in a clinic and has more than 100 features. Her task is to classify the entities (i.e. patients) with respect to a chosen target feature. In her case, the target feature is an indicator feature for a particular medical condition with the values positive or negative.</p><p>Due to the statistical dependencies between the target feature and the other features in the data table, we can say that the other features contain information about the target feature. Classification makes use of this information to produce a predictor for the values of the target feature. So Bethany's goal is to find a function or rule which takes the other features (e.g. temperature, blood pressure, weight, height, etc.) as input and outputs a prediction for the target feature -i.e. positive or negative.</p><p>It makes no sense for Bethany to use all of the available features for the classification. On the one hand, she must include the features which contain as much information about the target feature as possible (relevance). On the other hand, she must ensure that the included features show as little redundancy as possible. When redundant features are included the weighting of these features is effectively increased. This could lead to good features for the classification being masked by less good features.</p><p>Bethany needs to choose a subset of the available features which is optimized with respect to the above criteria. Due to the sheer number of features, she knows that she will have to resort to using automated methods to make the choice. Without automated methods she might have to test all of the more than 2 100 feature subsets to find the best candidate. Feature subset selection algorithms will help her overcome this problem. The subclass of algorithms she considers is known as filters. These iterative algorithms consist of heuristics, which make use of quality measures to construct candidate subsets.</p><p>The search for useful candidate feature subsets is dependent on the entity subset to which the algorithms are applied. Practically all the automated methods Bethany can draw on carry out their optimization of the feature subset on a fixed, predefined entity subset. This is usually the whole data table. Statistical correlations which only affect subsets of the entities can contribute in very different ways to the results of feature subset selection algorithms.</p><p>The contributing factors to the medical condition being analysed by Bethany need to be evaluated differently for men and women. Without some form of guidance, the feature subset selection algorithm would determine which features are most useful for classification in general. If two different models for prediction are required (one for men and one for women) then it would make more sense to execute the feature subset selection algorithm separately for each subset. This would ensure a better separation of the prediction models in the classification step of the analysis. For this and other trivial distinctions Bethany can easily preselect entity subsets. However, if Bethany were to consider correlations which are not self evident then an apportionment of the data into subsets is generally not possible before feature subset selection. If Bethany was able to apportion the data based on the common information of more than one feature then the classification could be improved. The risk of generating a prediction model which simply represents a compromise between several very different entity subsets would be reduced.</p><p>For better control over feature subset selection in the future, Bethany wishes to visualize those measures of feature redundance and relevance which are used by her chosen algorithm. In particular, she wants to interactively monitor changes in the feature subsets during iterative generation with the help of automated methods. This would allow her to guide the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Overview</head><p>We illustrate our basic idea with a simple example (see <ref type="figure">Figure 1</ref>). The scatter plots show features A 1 and A t (the target feature) of a data table. The analyst is interested in a dependency of A t on A 1 . <ref type="figure">Figure 1</ref>(a) shows the bivariate distribution of A 1 and A t . For small values, A 1 would in fact be sufficient to create a predictor for A t .</p><p>The problem is, that A 1 might be only one of hundreds or even thousands of potential candidate features. In this case, visual inspection of all features is not feasible. Hence, automatic methods are used for the ranking of candidates (see Section 2). Most automatic methods, however, use all entities of a data table to estimate the usefulness of candidate features for prediction. In this case, the strong local dependency of A t on A 1 might go unnoticed: Instead, <ref type="figure">Figure 1</ref>: These scatter plots illustrate our basic idea for finding candidate features A 1 to build a predictor for A t . Most automatic feature selection methods compute dependency measures based on all entities in the data table. Thus, local features can be masked by the global distribution, even if they would make for a good local predictor (a). To monitor automatic feature selection, we use a partition of the data and we show the local details of the global quality measures (b+c+d). (Example modified from <ref type="bibr" target="#b11">[12]</ref>)</p><p>weaker candidates than A 1 may be selected for the feature subset.</p><p>We propose a visual-interactive method to display the quality ranking measures computed with existing automatic techniques. The measures are decomposed to different subsets of the dataset. <ref type="figure">Figure 1</ref>(b) shows a useful partition of A 1 . We compute and show the contribution of every subset to the quality ranking measure (see Section 4.3). The local dependency in subsets A 11 and A 12 would be highlighted. <ref type="figure">Figure 1</ref>(c) shows a less suitable partition of A 1 . We have to note, that the partition has a great influence on the quality of the results. In Section 4.2 we show how the partitions can be computed and how they can be modified interactively.</p><p>In principle, the detailed inspection of the quality measures can be used with any partition of entities. Hence, we allow other features than A 1 or A t to be used for the decomposition. <ref type="figure">Figure 1(d)</ref> shows a partition using a third feature, which can be freely chosen.</p><p>In Section 5 we show, how the visualization can be used for the interactive initialization, monitoring and steering of the automatic feature subset selection methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Contribution</head><p>In this paper we introduce a software program called SmartStripes, which couples automated data analysis with interactive visualization for the purpose of feature subset selection. In particular, Smart-Stripes was developed for use with filter algorithms. These are supported by visualizing the quality measures which are used for the assessment of feature subsets. The quality measures used are normally measures of statistical correlation.</p><p>Measures of correlation are commonly displayed with the help of a correlation matrix <ref type="bibr" target="#b4">[5]</ref>. Correlation matrices display one value for each pair of features -i.e. (N 2 − N)/2 values for N features -in a matrix display. In contrast to correlation matrices, we calculate the correlation measures for interactively selectable and adjustable subsets of data entities. The contribution of each subset to the quality measures is visualized separately. Thus, the construction of feature subsets can be analyzed with a particular focus on entity subsets.</p><p>SmartStripes provides users with the ability to interrupt the filter algorithms at any time. In addition, the user can execute individual iterations manually and examine the quality measures for each iteration. Features can be interactively selected or excluded in each step. A manual initialization, in which the user selects the first features before continuing with the algorithm is also possible.</p><p>In addition to the tight coupling of automated and interactive feature selection, we have defined the following requirements for our software program:</p><p>• The visualization must provide an overview over a large, in principal unlimited, number of features in a data table</p><p>• It must be scalable with respect to the number of features and the number of entities.</p><p>• It must be applicable to a variety of filter algorithms. A prerequisite here is the ability to break down quality measures with respect to any given partition of the table into entity subsets.</p><p>• Experts must be able to bring their knowledge of the data set into the analysis process.</p><p>In the next section we will provide a technical overview of feature subset selection. We will then summarize the academic work related to the visual support of feature subset selection in Section 3. We will provide a detailed description of the SmartStripes approach in Section 4. This will be followed by a description of the SmartStripes work flow in Section 5. Finally, we will conclude our paper and give an outlook for future work in Sections 7 and 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">OVERVIEW OF FEATURE SUBSET SELECTION</head><p>Our contribution is a technique for the evaluation and guidance of feature subset selection for the purposes of data mining. It is therefore based on existing automated processing methods. In the following overview we will first classify the different algorithms. Secondly, we will characterize those classes of algorithms which can be used together with SmartStripes. Thirdly, we will illustrate the points at which we enable an interactive intervention in the feature subset selection process.</p><p>Automated methods for feature subset selection are often used for the exploratory analysis of data tables with a large number of features. The problem can be formulated as the search for an appropriate feature subset, where the search space contains all possible subsets of features (i.e. 2 N − 1 subsets for N features). Even after cleaning the data, the number of possible feature subsets can still be so large that a manual search cannot be carried out systematically. Guyon and Elisseeff <ref type="bibr" target="#b6">[7]</ref> describe the general problem as a search for a minimal subset of features, which together are most useful for the following steps of the analysis. In their opinion, one cannot expect to find a single method which is the best method for all cases.</p><p>Considering their goal, feature subset selection methods are very similar to dimension reduction methods.  <ref type="bibr" target="#b3">[4]</ref> for a survey on these methods). We assume the more general case, in which the relationships between nominal features are also considered.</p><p>The quality of a feature subset can be measured in a number of different ways. Kohavi and John <ref type="bibr" target="#b10">[11]</ref> differentiate between socalled wrapper methods and filter methods.</p><p>Wrapper methods consider the results of data mining models which are constructed using the chosen features. In this case the quality of a generated predictor may be carried over to measure the quality of a selected feature subset. These methods involve a high computational effort, because a new predictor must be constructed for each test of a candidate feature subset.</p><p>Filter methods use statistical measures of quality which can be applied before choosing any particular data mining algorithm. The quality measures provide a generalised description of the level of dependency between the candidate features and a target feature. One can distinguish two cases; the features are evaluated independently of one another (feature ranking) or the measure takes a set of features as input (feature set selection). In the first case, the subset containing the features which were ranked the highest is simply chosen (max-relevance method). In the second case, a heuristic is used in order to improve the feature subset based on an evaluation of its contents. These methods require more computational effort, however they have the advantage that they also take the dependencies inside the feature subset into account. Thus, redundant features can be eliminated from the selection.</p><p>Our method allows the examination and interactive modification of exactly these heuristics, in which a chosen feature subset is changed by adding or removing a feature in each step of the algorithm (see Section 5). More complex heuristics, such as those used in genetic algorithms <ref type="bibr" target="#b20">[21]</ref> are not directly supported by our method.</p><p>Without consideration of the heuristics, filter methods can be distinguished with the help of their quality measures. One of the most common measures for numerical data is Pearson's correlation coefficient (see, for example Sirkin <ref type="bibr" target="#b18">[19]</ref>), which describes the level of linear dependence between two features. Our approach does not make use of numerical coefficients because these cannot be applied to all -in particular not to nominal -features. For nominal features only those quality measures can be used for which the statistical distributions of values are estimated and compared. An example of such a measure is the mutual information measure <ref type="bibr" target="#b6">[7]</ref>. This measure is given by the following formula:</p><formula xml:id="formula_0">I MI (A 1 , A 2 ) = n ∑ j=1 m ∑ i=1 p(A 1i , A 2 j )log p(A 1i , A 2 j ) p(A 1i )p(A 2 j ) :=K(A 1i ,A 2 j )<label>(1)</label></formula><p>I MI denotes a quality measure, describing the mutual dependence of the two features A 1 and A 2 . The subsets A 1i and A 2 j for i = 1, . . . , m and j = 1, . . . , n represent the partitions of A 1 and A 2 respectively (compare with <ref type="figure">Figure 1</ref>). The cardinality of a given subset relative to the size of a dataset is given by p, which is used as an estimator of the discrete density function.</p><p>Brown <ref type="bibr" target="#b1">[2]</ref> introduces a system by which the quality measures of different automated methods can be compared and classified. Brown's general formulation of an important class of feature subset selection methods is of particular relevance for our work. The formula for the quality measure Q of feature A s , in which t denotes the index of the target feature, is as follows:</p><formula xml:id="formula_1">Q(A s ) = I MI (A s , A t ) Relevance Q Rel − β s−1 ∑ i=1 I MI (A s , A i ) Redundance Q Red + γ s−1 ∑ i=1 I MI (A s , A i |A t ) Conditionality Q Cond (2)</formula><p>According to this formulation, most methods only differ in the way that the redundance measure between an evaluated feature A s and the already selected features A i is calculated. The difference is encapsulated in the weights β and γ. Also important is the fact that the formula for the relevance I MI can also be used for the redundance.</p><p>Molina et al. <ref type="bibr" target="#b14">[15]</ref> describe, in addition to entropy measures, a number of other possible quality measures. Almost all of these quality measures are sum or integral formulae whose summands each refer to particular intervals or value subsets of the features considered. This is an important factor for the SmartStripes technique. In many cases the sums can be reordered so that the quality measures are broken down to the level of the summands as shown below. These summands each refer to particular entity subsets. In this way the influence of different partitions of the data table can be compared.</p><p>Kriegel et al. <ref type="bibr" target="#b11">[12]</ref> suggest a systematic approach for the categorization of methods for the clustering of high-dimensional data. They identify local feature relevance as one of four problems which these methods face. This means that different clusters are manifested in different feature subsets. Even though this problem was formulated for clustering methods, it is also relevant for the selection of features for the purposes of classification.</p><p>In conclusion, we have identified three points at which we can connect our visual interactive approach with automated filter methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Breaking down and visualizing the contributions of different</head><p>partitions of the data table with respect to the quality measure.</p><p>Restriction of the feature subset selection algorithm (and the subsequent classification) to particular entity subsets of the data table (see Section 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Selection of quality measures from a pool of measures and</head><p>sorting of the features with respect to the chosen measure (see Section 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Switching between automated and interactive heuristics (mixed initiative)</head><p>. The heuristic can be stopped manually to add or remove features. An initialization of the algorithm is also possible in this way (see Section 5).</p><p>The requirement that numerical, ordinal or nominal data should be handled in the same way restricts, on the one hand, the types of methods which can be used with SmartStripes. On the other hand, one is forced to accept a certain information loss, due to the fact that numerical features need to be discretized. Because all further calculations depend on this discretization, it makes no sense to choose a simple discretization a priori. Neither is it wise to allow the discretization of possibly hundreds of features by hand. We therefore initialize the discretization automatically. In addition, we provide users with the ability to refine this discretization freely (see Sections 4.1 and 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RELATED WORK</head><p>In the previous section we presented a categorization of the automated methods for feature subset selection. In this section the state of the art with respect to visual analytics is presented. Our work is compared to other approaches which combine automated and graphical interactive methods for the selection of features.</p><p>The idea of splitting up quality measures for a detailed inspection has been presented by May et al. in <ref type="bibr" target="#b13">[14]</ref>. However, the idea was illustrated with single quality measure only and it did not cover feature interdependency. One contribution of our approach we use the generic formula 2 to apply this technique for a broader class of feature subset selection methods. In addition, our improved approach covers the mixed initiative steering of the selection heuristics, which is necessary whenever feature interdependency is considered.</p><p>Guo <ref type="bibr" target="#b5">[6]</ref> argues that human intervention is necessary to evaluate and guide the procedure of feature subset selection. To our knowledge, the visualizations most commonly used for interactive feature selection are correlation matrices. As described in Section 1, correlation matrices visualize statistics for all correlations between pairs of features in a given table. The relationships between feature pairs are condensed to a single value. Friendly <ref type="bibr" target="#b4">[5]</ref> presents an in-depth exploration of the design of correlation matrices.</p><p>Visual support of the task of feature subset selection must display information on a coarse level of detail. Highly detailed views are not suitable for feature subset selection, because they do not scale well with the number of features. Hence, many approaches implement an overview &amp; detail strategy, where the overview component is used as a feature selection method. Two examples for using a correlation matrix as an overview are given by MacEachren et al. <ref type="bibr" target="#b12">[13]</ref> and Ingram et al. <ref type="bibr" target="#b7">[8]</ref>. Each presents a framework which includes an interactive feature selection step in its respective analytical process. Ingram et al. focus on the analysis of data sets with numerical features. They use the correlation matrix as a means to supervise filter and dimension reduction operations on the basis of Pearson's correlation coefficient. MacEachren et al. use the correlation matrix as a feature selection tool to steer other methods directly. They use the maximum conditional entropy as a measure of correlation. Like other measures that apply to the distribution of values rather than the values themselves, entropy measures have the advantage of being applicable to nominal, ordinal and discretized numerical features.</p><p>Feature sorting is a major concern in a number of visual approaches to feature selection. Guo <ref type="bibr" target="#b5">[6]</ref> enhances the usability value of the correlation matrix by proposing a sorting scheme for the features. This scheme involves sorting the features with respect to their similarity. Our sorting scheme uses the ranking measures derived from the automatic methods instead of mutual similarity. This makes more sense for our goal of reduced redundancy in the feature subset.</p><p>Approaches which do not use a correlation matrix as an overview component have been presented by Elmqvist et al. <ref type="bibr" target="#b2">[3]</ref> and Yang et al. <ref type="bibr" target="#b21">[22]</ref>. Elmqvist et al. use an ordered scatterplot matrix as an overview for the selection of features and to navigate the space of all 2D axis-aligned scatterplot projections. Scatterplot matrices provide a detailed overview of the data table, but they lose this advantage as the number of features increase. Yang et al. propose a visualization technique, which combines a detailed display of the values of features and the relations between different features. The detailed display shows glyphs which represent univariate or bivariate feature values. The similarity relation is represented by the layout of the glyphs on the canvas. In terms of the number of dimensions that can be displayed and compared, it is one of the most scalable techniques. However, using the similarity between different features or other numerical measures requires appropriate features.</p><p>Yang et al. <ref type="bibr" target="#b22">[23]</ref> present an approach for semi-automatic feature selection. Like our approach it is derived from an automatic method, which can be interactively controlled in order to identify a meaningful and useful feature subset. It computes a agglomerative hierarchical clustering of the features. Their approach, however, relies on the definition of similarity measures for all pairs of features. This requires numerical features to be commensurable. Without preparation it can not be applied to nominal or ordinal features.</p><p>Another example for the integration of automatic and interactive heuristics are presented by Ankerst et al. <ref type="bibr" target="#b0">[1]</ref>. Their goal is the optimization of a decision tree. Similar to our approach, their heuristic combines the optimization of feature partitions (i.e. split-points of the tree) and the search for relevant features. The difference is, that our approach does not prescribe the analytical model for the predictor.</p><p>Johansson et al. <ref type="bibr" target="#b8">[9]</ref> present an approach tackling with a problem related to ours. Competing structures in a dataset are emphasized or masked depending on the quality metrics chosen. To alleviate this problem, a mixture of different statistics may be chosen to the control the dimension reduction process. In a sense, this approach is orthogonal to ours: Instead of treating quality metrics as degrees of freedom, we use different entity subsets to emphasize local dependencies in the data. Piringer et al. <ref type="bibr" target="#b16">[17]</ref> present a concept which also allows the comparison of candidate features based on statistical measures for the uni-and bivariate value distribution. In contrast to most other approaches, their measures are not only applied to the complete dataset, but they are applied to any subset generated by brushing. When considering a brushed subset as a binary partition of the entity set (selected vs. unselected entities), this approach is very similar to ours. While their approach allows more flexible definition of partitions, our approach allows the comparison of more than two partitions.</p><p>Mixed initiative approaches for Visual Analytics have also been suggested for other parts of the analytical process. Kapoor et al.</p><p>[10] present a method to steer the classification process from its end: By changing preferences to accept or avoid specific kinds of errors, they provide a user-centered way to guide an automatic optimization of the classification model. Another user-centered approach is presented by Tan et al. <ref type="bibr" target="#b19">[20]</ref>. They use visualization for the interactive steering of the combination of classifier ensembles.</p><p>In summary, we feel that there is still a gap between detail and overview techniques in multidimensional visual analysis. Overview techniques like correlation matrices provide a space efficient overview due to a high aggregation of values. Detail techniques like scatterplot matrices show the plain data values. While correlation matrices provide a space-efficient overview, we feel that they limit the analyst's options for problem diagnosis and interactive refinement. Scatterplot matrices do not scale well with the number of features. SmartStripes is settled between the coarse overview provided by a correlation matrix and the details visible in scatterplot matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THE SMARTSTRIPES APPROACH</head><p>In this section we will describe the components of the SmartStripes approach in detail. Due to the close coupling of data processing and visualization components, a basic understanding of the background processes is necessary for the user to interact with the visualization effectively. Thus, we will begin our presentation with a description of how we partition the features. This will be followed by a description of the feature partition view, in which the partitions of features can be manipulated by the user. In the third subsection we will describe the decomposition of measures for relevance and redundance in order to increase the granularity of the overview. This will be followed by a description of the dependency view, in which the measures can be visualized and interactively explored. Finally, we will describe how the two views are linked to empower the user in guiding the process of feature selection. A screen shot of the SmartStripes user interface can be seen in <ref type="figure" target="#fig_0">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Partitioning of Features</head><p>We begin by emphasising the distinction between the values of a feature and the entities in the data table. When we talk of the value range of a feature we mean the values that can occur in the feature column. The entities are simply the rows of the data table. A small subset of the value range can correspond to a large entity subset and vice versa. Put in another way, many entities can contain the same value for a particular feature and there may be valid values which do not occur in any entities in the data table. The value ranges considered in this paper consist of simple data types, such as floating point numbers, integers and strings. The same concepts can, however, be applied to more complex data types. Since they will play a major role in the following discussion, we include a general definition of partitions, followed by an elaboration for numerical and nominal value ranges. Given a set A s , a partition of A s is a series of sets A s1 , A s2 , . . . , A sm , such that each A si is a subset of A s , no two sets contain the same elements (A si ∩ A s j is empty if i = j) and the union of all sets make A s (A s1 ∪ A s2 ∪ . . . ∪ A sm = A s ). For numerical value ranges the sets of the partition are intervals. In this case each set continues where its predecessor ends. Manual modifications of the partition involve simply shifting the boundaries of the intervals or unifying pairs of neighbouring intervals. For nominal value ranges the sets of the partition do not have a natural ordering. Manual modifications in this case involve moving values from one set to another, unifying pairs of sets and changing their display order.</p><p>As described in Section 2 numerical features have to be discretized to make them comparable with nominal features. Since we use histograms to represent the value ranges we begin by binning the numerical values. For this step we make use of the method described by Shimazaki and Shinomoto <ref type="bibr" target="#b17">[18]</ref>, which produces a binning optimized for histogram displays. In general, this initial binning is not suitable for the comparison of features using statistical methods. We therefore cluster the bins using the k-medoids algorithm in a second preprocessing step. We chose the k-mediods algorithm because of its simplicity and its tolerance of outliers.</p><p>Automated preprocessing is necessary, especially in cases where a large number of features has to be prepared for analysis. We cannot expect the initial partition to be optimal in all cases. Thus, to remain flexible, we allow for interactive modification of the partitions in the feature partition view. We assume that more advanced methods for the automatic creation of useful partitions -like, for example, clustering or brushing -can be integrated into our approach. We will take this up in future work (see Section 8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Feature Partition View</head><p>The feature partition view (see <ref type="figure" target="#fig_0">Figure 2</ref>(1)) visualizes the value range of each feature in the form of a vertical histogram. The bars of the histograms represent bins (in the case of numerical features) or individual values (in the case of nominal features). The length of the bars represents the number of entities in the corresponding entity subset. In the case of nominal features, the bars can be reordered interactively by drag-and-drop.</p><p>The left (base) axis of each histogram is made up of a series of buttons. The buttons represent the clusters described above (in the case of numerical features) or interactively defined groups of values. By dragging the top or bottom edges of the buttons, cluster or group boundaries can be modified and whole clusters or groups can be added or removed. Clicking on a button will add or remove the corresponding entities from the current analysis. By default, all entities are visible and, thus, all the buttons are activated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Decomposition of sums</head><p>In this section, we show how the quality measure (see Equation 2) of a specific feature A s can be decomposed according to the partitions given. The quality measures, introduced in Section 2, only describe the relationship between features with respect to all entities in the data table. The most important component of the visualization is therefore the detailed representation of the quality measures regarding the partition of the features. The idea is to separate the corresponding sum formulae in an appropriate way into components and to represent their contributions to the overall quality individually.</p><p>Every sub sum represents a subset of entities. To break the quality measure down with respect to different subsets we use the partitions which are defined in the feature partition view. The partitions of different features are almost always different. The entity subsets of each feature partition most often strongly differ regarding other features. Thus, the selection of appropriate feature partitions for defining the sub sums plays an important role. When we combine the equations 1 and 2 we can show which part of the quality measure can be decomposed by which feature: The relevance term Q Rel consists of two nested sums and the nesting can be exchanged:</p><formula xml:id="formula_2">Q Rel (A s ) = m ∑ j=1 n ∑ i=1 K(A si , A t j ) Decomposition by A t = n ∑ i=1 m ∑ j=1 K(A si , A t j ) Decomposition by A s (3)</formula><p>The redundance term Q Red consists of three nested sums, but only the feature A s does appear in all terms.</p><formula xml:id="formula_3">Q Red (A s ) = −β o ∑ k=1 n ∑ j=1 m ∑ i=1 K(A si , A k j ) = m ∑ i=1 −β o ∑ k=1 n ∑ j=1 K(A si , A k j )</formula><p>Decomposition by A s (4) Since we can not expect, that a common partition exist for all features A k , only the feature A s may be used for the decomposition of the redundancy part. For the same reason, the conditional mutual information Q Cond can be decomposed by the partitions of A s or A t . Now we are able to define, which part of the quality measure can be applied to which decomposition. The user will be able to select between two decomposition strategies:</p><p>1. The partition of the reference feature A r is used for all features. The advantage of this strategy is that the subsets of the partition can be compared across all features. The drawback is, that it can be applied to Q Rel (see <ref type="figure">Figure 4</ref>(a)) and Q Cond (see <ref type="figure">Figure 4</ref>(b)) only. In particular it can not be applied to the overall measure Q(A s ).</p><p>2. The quality measure for every feature A s is partitioned with its own partition (see <ref type="figure">Figure 4(c)</ref>). The advantage of this strategy is that it can be applied to the overall measure. The drawback is, that the partitions of different features can not be compared directly, because they may be unrelated.</p><p>Due to the decomposition we retain a number of values in addition to the overall quality measure for every feature A s . Two features may have identical overall quality measures; their detailed measures defined by the partial sums can be different. In some cases a single partial sum contains the only significant share of the overall measure. However, strong dependencies which span only a part of the dataset may be interesting for classification. Local patterns, which might be worth a detailed inspection are displayed in the dependency view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Dependency View</head><p>The dependency view (see <ref type="figure" target="#fig_0">Figure 2</ref>(2)) is the visualization central to our approach. This visualization shows the quality measures, which were calculated with respect to the chosen feature ranking or feature subset selection algorithm. We use a matrix-based layout, in which each column represents a feature. In the first decomposition strategy (see previous subsection), the number and size of the cells in every column is determined by the reference feature and its partition (see <ref type="figure" target="#fig_1">Figure 3)</ref>. In the second decomposition strategy, the number and size of the cells is determined by the feature represented by the column. The height of a cell is proportional to the size of its entity subset; this is also an indication of its influence in the overall quality measure. Aside from the decomposition strategy, the user may also choose to inspect a specific component Q Rel , Q Red or Q Cond , if suitable.</p><p>The color saturation of a cell is defined by the normed value of the partial sum. Low values are shown in white, high dependency values are shown in dark blue. The visualization also shows the overall quality measure as a bar-chart on top of the columns (see <ref type="figure" target="#fig_0">Figure 2(4)</ref>). The quality measure is determined by the feature subset selection method chosen. The height of the bars is normed to the range between the minimum and maximum values. The result is not skewed by norming in this way, since the absolute value of the quality measure is not relevant for the heuristic.</p><p>The reference feature used for the calculation of the statistics is initialized with the target feature. While the target feature always remains the same, the reference feature can be changed during analysis (by right clicking on the respective column). This is useful to check for interdependencies between potential candidates which are not necessarily affected by the target feature. In the next section, we will describe in detail how we support the investigation strategy.</p><p>The dependency view also includes tooltip-visualizations, which show details on the quality measures of entity subsets (see <ref type="figure" target="#fig_0">Figure  2</ref>(5)). Since the quality measure is basically a comparison of two value distributions, we show these distribution with two bar-charts. One bar-chart shows the overall distribution of the feature values (light grey), while the second bar-chart shows the distribution of the values in the regarded entity subset only (dark grey). Because the conditional component Q Cond is defined by three features, we display the distributions as a scatterplot, with the corresponding entity subset marked red (see <ref type="figure">Figure 4</ref>(c))</p><p>The result of the selection process is a subset of the set of features. We establish a cooperation of human and machine to create this set. In our proposed mixed initiative, the human may select features by clicking on the corresponding columns. She may also filter specific entity subsets by clicking in the corresponding columns. Any selection or removal causes an automatic recalculation of the statistics and an update of the quality measures for every feature and every cell. Because of the interdependencies of the features, all quality metrics may change after selection. Hence, in order to find the best features for the next iteration, the feature may need to be reordered by their quality measure. The sorting is triggered by the user, to leave the chance to inspect the data with different quality metrics.</p><p>We consider sorting to be the part of the technique, which makes the visualization scalable in terms of the number of features. The default view covers the left boundary of the virtual screen space and the features will be sorted by their quality from left to right. The features, which are likely to be irrelevant are placed to the right side and will be shown by default. However, those features and their measures can be investigated by scrolling.</p><p>In addition, the dependency view is connected to the feature partition view in both directions. On the one hand changing the partition of a feature in the feature partition view, causes a recalculation of the statistics and the current quality measure. Splitting and joining entity subset has an immediate effect. On the other hand, the histograms can be colored by selecting a cell in the matrix and its corresponding subset. The coloring can be used to find suitable splitting points for manual a partition. Therefore, we calculate the correlation between the value distribution of all entities and the value distribution within the subset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">WORKING WITH SMARTSTRIPES</head><p>In order to work with SmartStripes the user has to define the dependent feature to be used as the target feature for the classification. The dependent feature (or target feature) is the most important reference feature throughout the entire feature subset selection and most calculations will refer to it. In the following we will first describe a straightforward work flow, in which the reference feature is not changed. Secondly, we will describe how we deal with changing the reference feature for the inspection of interdependencies.</p><p>SmartStripes can be used in two basic modes: An automated heuristic mode and a mixed initiative mode. In automated mode, feature subset selection works "as usual": All iterations of the heuristic are calculated in a single run. The process terminates based on the maximum number of features or a threshold of the quality measure. The selected features are highlighted. In this mode, SmartStripes can be used to inspect and modify the final result.</p><p>In a mixed initiative mode, the automated method is used to calculate the measures for one iteration at a time. After one iteration, the calculation stops. The quality measures are displayed in the dependency view to suggest the next choice of a feature. The actual choice, however, is made by the user. The user may consider the overall quality measures, the name and semantics of the features and the detailed quality measures regarding the entity subsets defined by the partition (see 5.1). The selection of a feature starts the next iteration: The quality measures are recalculated and a new feature is suggested for the next selection (see <ref type="figure">Figure 4</ref>(d)+(e)).</p><p>Each selected feature will be added or removed from the subset of selected features depending on its previous state. That way it is possible to combine heuristics with forward and backward propagation. In addition the two modes can be used interchangeably: It is possible to initialize a feature subset in the mixed initiative mode and switch to the automated heuristic after a few iterations. It is also possible to use the automated heuristic as an initialization of a mixed initiative backward propagation.</p><p>Because of this step-by-step approach, the mixed initiative mode allows branching from the straightforward work flow. Sometimes, it is interesting to inspect the interdependency between features other than the target feature in greater detail (see <ref type="figure">Figure 4</ref>(f)). As opposed to a correlation matrix, our technique displays a one-tomany relationship between features. This means the user must change the reference feature (initially identical to the target feature) to check the details of the interdependency. This is especially useful when evaluating features suggested by the automated feature subset selection. For example, the first feature is suggested by the automated method for its large relevance. Before accepting this suggestion it makes sense to check whether the new feature is relevant in its own right or whether it is dependant on two or more features, which represents even more fundamental information.</p><p>Because the interdependencies between features may form a complete graph, searching this graph is not linear. Apart from connecting our technique to a view of such a graph -which is beyond the focus of this paper -the arrangement of the feature columns can help to organize the search. Firstly, sorting by the quality measures is automatic, but it is never triggered automatically. Often, the user needs to keep track of the changes while focussing on specific features. Secondly, the sorting is done separately for three groups of features. The first "group" consists of the dependent feature only. This feature is always displayed at the leftmost column and acts as an anchor-point to get back to the "main-branch" of analysis. Next to the right we arrange the features, which are currently selected for the resulting subset in descending order. The rest of the features are arranged on the right-end side.</p><p>In summary the work flow for the feature selection consists of a main branch, which is pursued whenever the target feature is the reference feature and a number of side branches, which are pursued for the detailed evaluation of potential candidate features for selection or removal. Usually the user re-enters the main branch, whenever the subset has been changed. The quality measures shown in the visualization can be changed at any time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Reading the Dependency View</head><p>At its core, our approach juxtaposes information detailing entity subsets and information describing whole features. In general, the dependency view shows where (automatic) feature subset selection will be sensitive to the selection of entities used for subsequent analysis or vice versa. Wherever the dependency measures are unevenly distributed along a feature column, the exclusion of an entity subset or a different data sample is likely to change results.</p><p>Some visual artifacts stand out and may indicate a need for a detailed inspection. The best candidate features are displayed as an unbroken dark blue column, indicating that the candidate may be useful for the predictor of the complete dataset. However, there may be different degrees of dependency along the same column. In the reference feature based views, every row corresponds to the same subset of the reference feature. Sometimes, rows can be clearly distinguished as a darker area spanning multiple columns (see <ref type="figure">Figures  4(a)</ref> and 2(2)). Their corresponding entity subsets display an unusual distribution in a large number of features. Depending on the strength of the relationship, it makes sense to investigate whether this subset can be predicted with a separate classification model. Subsets, which can be distinguished as dark-blue or white rows can also be used for an optimization of the partition of the reference feature. The best partition would minimize the number of subsets while retaining most of the information in the dataset. By merging subsets, the resulting subset often becomes more similar to the overall distribution, resulting in a weaker dependency measure. Thus subsets indicating a strong dependency may be merged until their corresponding rows become white. Likewise, subsets which appear white may simply be too big to expose any features distinguishing it from the overall distribution. The user may split these subsets to see if the resulting partition reveals more details.</p><p>In the candidate feature based view every column is partitioned like its feature. Usually no rows can be identified in this view, because every feature has its own partition. However, this view allows to spot those feature/entity subset combinations which are most suitable for prediction. Instead of focussing on candidate features that display a "solid" dependency on average, it is necessary to trade them off with features that display a high dependencyyet only in a specific entity subset. Because most quality measures for automatic selection methods move the "solid-on-average" features into focus, we implemented an additional measure which uses the weighted maximum measure per column to compare features (weighted by the relative size of its entity subset). This decreases risk of overlooking potentially interesting local features.</p><p>One caveat is a tendency to over interpret the color-contrasts within different parts of a column. The corresponding subsets do not necessarily represent the best partitions of an optimal model of the data: Subsets (i.e. patterns) that manifest in more than one feature are usually not represented accurately by univariate partitions. However, our technique relies on the fact that it is not necessary to use optimal partitions in order to expose dependencies between features. Except in the cases where the features are truly independent, a less-then-optimal partition can be compensated, if the number of entities is sufficiently high.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATION</head><p>We used real-world datasets like the US public micro census for a first evaluation. Many of the features in this household dataset are redundant. Documented relationships are used as "ground-truth" data. Strong global dependencies can easily be detected with our method and with automated methods. Also, a number of local dependencies which are easily detectable in the visualization could be verified in the documentation. Some of them, like the "abnormal" behaviour of disbanded households might appear trivial in retrospect. Features with a hybrid encoding -basically a numerical feature combined with nominal values like not available -also leave a distinct trace in the visualization. Other dependencies, like a redundant encoding of some household characteristics like household type and working experience would require more experience with the data. We believe that an exposition of quality measures may keep the analyst alert to avoid potential pitfalls.</p><p>In the following we were interested in the possibility of detecting multivariate dependencies. Our technique calculates statistics for bi-and trivariate data; in this sense it can be compared to a projection to low-dimensional space, which most often results in a loss of information. In a preliminary study, we created artificial local dependencies by choosing a random subset of data. One to five features are chosen randomly to create clusters or manifolds in their subspace. The rest of the entities and features contained noisy data. Test persons were required to identify the dependent subspaces in the dataset. In this study, the dependencies were easy to spot with our visualization. However, this may be due to the fact, that plain noise simply is not sufficient to mask the artificial dependencies.</p><p>In a follow-up study we modified subsets of real-world data in the same way. We expected that real-world data is a more efficient distractor, which in fact proved to be true. Naturally, the attention is drawn to the strongest -most often bivariate -relations. In some cases these dependencies were clearly associated with an entity subset of the data. They were eliminated by removing this subset from the calculation. Thus, weaker dependencies were easier to spot. However, users stated that they were not able to effectively eliminate subsets, because of the limited flexibility of the univariate feature partitions. Our lesson from this study is the need for a more flexible definition of the subsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">LIMITATIONS</head><p>An important limitation of virtually all techniques using statistical measures is their instability when applied to a too small number of samples; and SmartStripes is no exception. We have to note that many freely available test data sets with less than a thousand entities are unlikely to produce valid results. After bivariate or even trivariate decomposition of the test statistic, a single term may be based on a few samples only. This is in fact one reason, why feature subset selection methods are rarely used with multivariate dependency measures <ref type="bibr" target="#b15">[16]</ref>.</p><p>Because we require interactive frame rates, the effect of having too many samples has to be considered, too. Fortunately, many of the computations can be cached. The most extensive updates are the change of the target feature and the modification of feature partitions. Without any extensive database support, we are able to process tables with about 4 million entries with interactive performance.</p><p>SmartStripes itself is very flexible, which can both be a chance and a burden to the user. Feature partitions and reference feature may be freely chosen at any time, but usually the user does not consider all these information at once. Thus, the interactive control of the automatic feature set optimization covers only the one aspect of the parameter space for interaction. It may be possible that the optimization of the feature partitions can be done in a similar semiautomatic way. This would narrow the degrees of freedom to a more concise user interface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION &amp; FUTURE WORK</head><p>We presented SmartStripes, a visualization technique for the monitoring and control of automatic filter methods for feature subset selection. The visualization displays the quality measures computed for every feature of a data table and broken down to a selected grouping of data entities in a matrix layout. The statistical aggregations of the quality measures are decomposed to compare the individual contribution of different data groups to the quality measure.</p><p>While feature ranking measures are displayed in existing visualization techniques, SmartStripes also supports feature subset selection methods, in which the inclusion or exclusion of a single feature influences the measures of all other features. The user is able to step into the heuristic to interactively modify the automatic optimization of the feature subset. The subset may be changed by adding or removing features from the current configuration. Automatic and interactive selection can be used alternately. In addition, the user may include or exclude specific data partitions depending on the detailed inspection of the measures.</p><p>All features can be sorted according to their current quality rank as a candidate. While by default the virtual screen displays only the most important candidate features, the user is able to view the whole table by scrolling. Hence, the number of features that can be processed and displayed in a meaningful way is virtually infinite. Compared to other visualizations, SmartStripes presents an intermediate level of aggregation. We propose a trade-off between the high aggregation of a correlation matrix and detailed visualization of actual data values.</p><p>SmartStripes is very strictly intended to be an preliminary overview for analysis, which does not prefer or defer specific types of data or specific types of relations (e.g. similarity). Hence, we deliberately restricted the technique to generic statistical dependency measures. The task of finding specific patterns in the data has to be covered by the following data-mining steps. SmartStripes minimizes the risk to miss an opportunity for the generation of a better feature subset.</p><p>In the future we will extend SmartStripes to include synthetic features. These features are the result of clustering and dimension reduction methods or of data transformations. Because they combine information from multiple features, they potentially make up for stronger feature partitions. However, this requires alternating between the feature selection methods and data-mining methods, which are, of course, interdependent. Both processes could be used for an incremental refinement of the other and our technique can be used to control this loop.  <ref type="figure">Figure 4</ref>: The SmartStripes work flow. Each column is broken down with respect to the value range partition of (a) the target feature, (b) each feature individually, and (c) a chosen reference feature.(d) Shows a first glance at the data. In (e) the data has been sorted by relevance and the first feature chosen. (f) Shows the examination of the relevance and redundance with respect to a reference feature. In (g) A candidate feature subset has been selected.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Overview of the SmartStripes interface. (1) The feature partition view. (2) The dependency view. (3) Value subset labels. (4) Feature labels and feature relevance visualized in the form of a bar chart. The target feature label is black, the hue and saturation of the other labels is determined by their quality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The decomposition of the sums is distributed among the cells of the column.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards an effective cooperation of the user and the computer for classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ankerst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 6th International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="179" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A new perspective for information theoretic feature selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<meeting>the 12th International Conference on Artificial Intelligence and Statistics (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rolling the dice: Multidimensional visual exploration using scatterplot matrix navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dragicevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-D</forename><surname>Fekete</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Transactions on Visualization and Computer Graphics</title>
		<meeting>IEEE Transactions on Visualization and Computer Graphics</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1141" to="1148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A survey of dimension reduction techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fodor</surname></persName>
		</author>
		<idno>UCRL-ID-148494</idno>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Center for Applied Scientific Computing</publisher>
		</imprint>
		<respStmt>
			<orgName>Lawrence Livermore National Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Corrgrams: Exploratory displays for correlation matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Friendly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="316" to="324" />
			<date type="published" when="2002-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Coordinating computational and visual approaches for interactive feature selection and multivariate clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="232" to="246" />
		</imprint>
	</monogr>
	<note>Information Visualization</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An introduction to variable and feature selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elisseeff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Resarch</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1157" to="1182" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dimstiller: Workflows for dimensional analysis and reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ingram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munzner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Irvine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bergner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Möller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th IEEE Conference on Visual Analytics in Science and Technology (VAST)</title>
		<meeting>the 5th IEEE Conference on Visual Analytics in Science and Technology (VAST)<address><addrLine>Florida, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2010-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Interactive dimensionality reduction through user-defined combinations of quality metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johansson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="993" to="1000" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Interactive optimization for steering machine classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on Human factors in computing systems, CHI &apos;10</title>
		<meeting>the 28th international conference on Human factors in computing systems, CHI &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1343" to="1352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Wrappers for feature subset selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="273" to="324" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Clustering high-dimensional data: A survey on subspace clustering, pattern-based clustering, and correlation clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kröger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
		<idno>3:1:1-1:58</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data</title>
		<imprint>
			<date type="published" when="2009-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploring high-d spaces with multiform matrices and small multiples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Maceachren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hardisty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lengerich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Symposium on Information Visualization (InfoVis)</title>
		<meeting>the IEEE Symposium on Information Visualization (InfoVis)</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Smartstripes: Looking under the hood of feature subset selection methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ruppert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Workshop on Visual Analytics</title>
		<editor>S. Miksch and G. Santucci</editor>
		<meeting>the 2nd International Workshop on Visual Analytics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="13" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Feature selection algorithms: a survey and experimental evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Belanche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nebot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="306" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Feature selection based on mutual information criteria of max-dependency, max-relevance, and minredundancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2005-08" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1226" to="1238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Quantifying and comparing features in high-dimensional datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Piringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 12th International Conference Information Visualisation</title>
		<meeting>the 2008 12th International Conference Information Visualisation<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="240" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A method for selecting the bin size of a time histogram</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shimazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shinomoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1503" to="1527" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sirkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics for the Social Sciences</title>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Sage Publication Inc</publisher>
		</imprint>
	</monogr>
	<note>3rd Edition</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ensemblematrix: Interactive visualization to support machine learning with multiple classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Talbot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Human Factors in Computing Systems (CHI)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Feature subset selection using a genetic algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Honavar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Systems and their Applications</title>
		<imprint>
			<date type="published" when="1998-04" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">44</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Value and relation display: Interactive visual exploration of large data sets with hundreds of dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hubball</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">O</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Rundensteiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ribarsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="494" to="507" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visual hierarchical dimension reduction for exploration of high dimensional datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">O</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Rundensteiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the symposium on Data visualisation (VisSym)</title>
		<meeting>the symposium on Data visualisation (VisSym)<address><addrLine>Aire-la-Ville, Switzerland, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Eurographics Association</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="19" to="28" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
