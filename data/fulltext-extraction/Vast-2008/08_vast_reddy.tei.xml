<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generating Hypotheses of Trends in High-Dimensional Data Skeletons</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandan</forename><forename type="middle">K</forename><surname>Reddy</surname></persName>
							<email>reddy@cs.wayne.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Snehal</forename><surname>Pokharkar</surname></persName>
							<email>snehalp@wayne.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam</forename><surname>Tin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ho</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Wayne State University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Wayne State University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<settlement>Bell Labs Alcatel-Lucent</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Generating Hypotheses of Trends in High-Dimensional Data Skeletons</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>G</term>
					<term>4</term>
					<term>1 [Mathematics of Computing]: Mathematical Software-Algorithm design and analysis; I</term>
					<term>5</term>
					<term>3 [Computing Methodologies]: Pattern Recognition-Clustering</term>
				</keywords>
			</textClass>
			<abstract>
				<p>We seek an information-revealing representation for highdimensional data distributions that may contain local trends in certain subspaces. Examples are data that have continuous support in simple shapes with identifiable branches. Such data can be represented by a graph that consists of segments of locally fit principal curves or surfaces summarizing each identifiable branch. We describe a new algorithm to find the optimal paths through such a principal graph. The paths are optimal in the sense that they represent the longest smooth trends through the data set, and jointly they cover the data set entirely with minimum overlap. The algorithm is suitable for hypothesizing trends in high-dimensional data, and can assist exploratory data analysis and visualization.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A fundamental concern in data analysis is to find correlations hidden in large, high-dimensional data sets. To analyze correlations the data must be ordered in a certain way in each notion of variability. With such ordering one can proceed to study how variability in one aspect is associated with those in others. The ordering itself would then describe the general course or direction the distributions extended along i.e a certain trend in the data set.</p><p>Ordering data points in a high-dimensional space is a highly nontrivial problem. Simply arranging the points along each coordinate does not serve the purpose, because the intrinsic variability in the data may not necessarily align with a specific observation dimension. An example is that in image analysis the samples may be represented as intensity per pixel, and the pixels are raster-scanned as a feature vector, which is then considered as a point in a multidimensional space. Yet, the trends of variability in the image, such as the movement of an object, are not necessarily revealed if one follows the intensity changes in a particular pixel. They are observable only as certain associated changes in multiple feature components (data dimensions). Furthermore, the changes may not necessarily involve all of the components. For example, the images may contain background pixels that do not have intensity changes as the object moves. The trends do not necessarily span the entire image collection either, because the changes may occur only in those samples collected in the time window containing the duration of the object's movement.</p><p>In this work, we seek a way to uncover trends in datasets that may be local (i.e., not necessarily global), or may be visible in only certain subspaces. We start with an unsupervised learning procedure to obtain a graph structure that describes the local proximity relationship between data points. We then propose an algorithm to navigate the proximity graph, where each navigation path gives a hypothesis of a possible trend in the dataset. An example is given in <ref type="figure" target="#fig_1">Fig. 1</ref> for data in a two-dimensional space. The focus of our work is a novel graph-transformation-based framework for identifying a critical set of independent patterns (or trends) with the least amount of overlap. A set of continuous models for the trends is then built by fitting a principal curve using the subset of data points that belong to each of these trends.</p><p>It may appear that our procedure would discover a trend on any arbitrary data set even if there is none. In fact, what we are looking for is simply a dominant (or weakly dominant) ordering of the points that may be present in certain subspaces. We conjecture that such an ordering exists in most of the data sets of practical concern. Ideally, this conjecture should be validated by a statistical testing procedure, which is beyond the scope of our current work. In this work, we treat this as an exploratory analysis tool, and caution that one must validate the discovered trend by trying to make sense of the ordering of the associated raw data, or by developing a correlation model with other relevant parameters.</p><p>The trends resulting from this procedure can then be analyzed with standard curve properties such as length, curvature, overlap, etc. Their explicit, continuous models can be used to estimate functional relationships between the trends. Analyzing such trends in data can yield more insights for comparing different variability in the data, such as the effects of observational parameters on the outcome, or correlations between different groups of features. We describe the use of this trend finding algorithm in a visualization task <ref type="bibr" target="#b7">[8]</ref>, which can be a useful starting point for data analysis.</p><p>The main contributions in the proposed method are:</p><p>• Dealing with intersecting principal curves by smoothing the data and increasing the curve detection ability of Principal Curves.</p><p>• Identifying trends in the data using a continuous representation.</p><p>• Obtaining the minimum number of most independent paths that cover the entire dataset.</p><p>• Untangling intersecting paths in high-dimensional spaces.</p><p>• Visualizing trends that summarize the data in continuous form.</p><p>• Hypothesizing correlations amongst various feature trends in the dataset.</p><p>The rest of this paper is organized as follows: Section 2 gives information about the relevant background on the various concepts used in the paper. Section 3 describes the problem formulation and explains the key concepts needed to comprehend our algorithm. Section 4 provides the implementation details. Section 5 gives the experimental results of the proposed algorithm on various synthetic and real-world datasets. Finally, Section 6 concludes our discussion and gives the future research directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IEEE Symposium on Visual Analytics Science and Technology</head><p>October 21 -23, Columbus, Ohio, USA U.S. Government work not protected by U.S. copyright.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">INFERENCE OF PROXIMITY STRUCTURES AND HYPOTHE-</head><p>SIZING TRENDS IN DATA Fitting continuous models to data is a much explored area in statistics. A popular choice is a spline <ref type="bibr" target="#b6">[7]</ref>, which is a special function defined by piecewise polynomial (parametric) curves <ref type="bibr" target="#b2">[3]</ref>, and is often used in applications requiring data interpolation and smoothing of multi-dimensional data. Spline functions for interpolation are normally determined as the minimizers of suitable measures of roughness subject to the interpolation constraints. Smoothing splines may be viewed as generalizations of interpolation splines where the functions are determined to minimize a weighted combination of the average squared approximation error over observed data and the roughness measure. For a number of meaningful definitions of the roughness measure, the spline functions are found to be finite dimensional in nature, which is the primary reason for their utility in computation and representation.</p><p>Principal curve <ref type="bibr" target="#b5">[6]</ref> fitting is another popular technique that has been successfully used for various applications in pattern recognition and machine learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9]</ref>. They give a representation for low-dimensional manifolds in the data that can be exploited further to form the trends. However, there is a difficulty in applying these models to a new dataset without prior knowledge about the embedded data geometry. Because not all the data points may participate in each possible trend, forcing a fit using all the points may degrade the overall goodness of the fit and damage the models' validity. An example case is that if there are two separate trends that are intersecting, a single application of the principal curve algorithm may not be able to directly ascribe the precise number of trends and then find them. One way to avoid this problem is to build the models in small neighborhoods, and then seek to connect and extend the local models.</p><p>To assign points to local groups in close proximity, one may employ various clustering algorithms. Popular choices include the k-means procedure and Gaussian mixture estimation <ref type="bibr" target="#b9">[10]</ref>. However, studies on clustering often stop at assigning the data points to clusters. We argue that an important next step is to understand the relationship at the next level, i.e., the relative positioning and potential relationships between the clusters. Therefore, we introduce the cluster relationship graph, as a representation of the proximity structure in the data. A useful choice for this is a minimum spanning tree that connects the cluster centroids. We then seek meaningful ways to traverse this graph that can best reveal underlying trends. Data in the clusters that are considered to be in a trend are then taken to fit a continuous model. Additional procedures that can be of use in this context include data shrinking <ref type="bibr" target="#b11">[12]</ref> methods. These are analogous to thinning and skeletonization in a 2D context that are popular in the computer vision community and have been applied to problems like character recognition <ref type="bibr" target="#b8">[9]</ref>. These techniques can reduce the noise and remove any outliers that usually do not contribute to the main trend, and hence avoid the interference of these points to model fitting.</p><p>A note of caution related to dimensionality reduction. Dimensionality reduction is an active area of research in exploratory analysis with high-dimensional data <ref type="bibr" target="#b0">[1]</ref>. Proper methods can preserve essential proximity structure in the data. But simplistic schemes do not always yield desirable results. For example, many attempts for visualization of high-dimensional data involve systematically presenting different low-dimensional projections of the data onto a restricted number of coordinates. We caution that such operations may not allow a good view into the true variability in the data. An example is given in <ref type="figure" target="#fig_3">Fig. 2</ref> shows a spiral in 3-D space along with the data generating curve. Two projections of the data (onto x-y plane and onto x-z plane) are also shown. It is almost impossible to obtain the original structure of the data and the parametric form of the data generating curve from these two projections. This demonstrates the significance of fitting the trend models in the original dataset and not in the projected space, unless it can be assured that the proximity structure is preserved in the projection.</p><p>In the discussion that follows, we assume that the k-means procedure is used for clustering. The k-means procedure is known to have problems, such as dependence on initialization and employing a strong assumption on the cluster shapes. For only some of these problems there are remedies. However, we find the simplicity of k-means attractive as a starting procedure in exploratory analysis. Furthermore, in our context it serves mainly as a data compression step. As long as a large enough k is used, the shape assumption has only a local effect. The global shape of the manifold is preserved in the resulting data skeleton, i.e., the Principal Tree that we will describe below. Nevertheless, we expect that our method can work with another more robust clustering procedure, or with the assistance of procedures that give an estimation of the intrinsic dimensionality of the data.   </p><formula xml:id="formula_0">Notation Description X Input dataset D Noise-free dataset k Number of clusters C i Cluster centroids σ Set of C i ...C j en i End Nodes of the MST ρ Set of en i in i Internal Nodes of the MST µ Set of in i is i Intersection Nodes of the MST γ Set of in i L i j</formula><p>Set of edges connecting the Centroids </p><formula xml:id="formula_1">C i and C j G p Principal Tree = ({C i }, {L i j }) ; MST connecting C i s P I End-To-End Paths of G p = (en i , ..., in * , ...,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 6 Overlap(O i j,i j ) -For two End-to-End Paths P i j and P i j , the overlap is the set P</head><formula xml:id="formula_2">i j ∩ P i j . Definition 7 Traversal Graph -is a graph G t =({P I },{O IJ }) (I, J = 1, ..., |P i j |), where {P I }</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>is a set of End-To-End Paths P i j s and {O IJ } is a set of edges representing the overlap between P I and P J . The Vertex weight Φ I associated with each P I is a measure of the length of P I and the Edge weight η IJ associated with each O IJ is a measure of the amount of overlap between P I and P J .</head><p>We illustrate these with an example in <ref type="figure">Fig. 3</ref>(a) that contains a two-dimensional dataset. <ref type="figure">Fig. 3</ref>(b) presents the (centroid) C i s in the dataset, one for each cluster identified. C i s are the single point representations of clusters identified by the k-means procedure. <ref type="figure">Fig. 3(c)</ref> shows the Principal Tree with the C i s and their corresponding L i j s. In <ref type="figure">Fig. 4</ref>(a) the end nodes are a,b,c,d as they are incident to only one (link) L i j in the G p , the internal nodes are w,x,y,z, the intersection nodes are x and y, the path D is a path connecting a,w,x,y,d. The overlap between paths D and E is (x,y). <ref type="figure">Fig. 4</ref>(b) indicates the length weights for each segment of the paths. Every path in <ref type="figure">Fig. 4</ref>(b) is represented as a node in the Traversal Graph given by <ref type="figure">Fig. 4(c)</ref>. The overlap between each path is the edge weight in the graph, as shown.</p><p>One can see that each possible End-To-End path in the original feature space will be represented as a node in the Traversal Graph. There are a total of ( t 2 ) nodes in G t for t End Nodes in the Principal tree. In this paper, we propose an algorithm to extract the most informative End-To-End paths that cover the entire data set. To this end, we will now formalize the notion of the most informative End-To-End paths, and define the optimal graph traversal problem that finds the minimal set of such informative paths contained in the data set.  modified and used here. On the identification of the set of optimal End-To-End Paths, a principal curve is fitted to the data points belonging to each P I to obtain the ϒ in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Optimal Traversal Problem</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Principal Curves</head><p>Definition 8 (Principal Curves) are defined as "self-consistent" smooth curves which pass through the "middle" of a d-dimensional probability distribution or data cloud <ref type="bibr" target="#b5">[6]</ref>.</p><p>Principal curves are non linear summarizations of multidimensional data points represented by a smooth, one dimensional curve. The curve passes through the densest regions of the dataset, or the 'middle' of the dataset, taking the shape according to the distributions of the dataset. Let X be a random vector in ℜ d and assume that n samples of X are available. A principal curve f ⊂ ℜ d is a smooth (C ∞ ) unitspeed curve explicitly ordered by λ ∈ Λ ⊂ ℜ 1 , that passes through the middle of the d-dimensional data described by the probability distribution of X.</p><formula xml:id="formula_3">f (λ ) = f (1) (λ )... f (d) (λ ) T = E[X|λ f (X) = λ ]<label>(1)</label></formula><p>where, the projection index λ f (x) of x is the value of λ for which f(x) is closest to x. If a number of such points are present, the point with the largest value of λ is chosen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Algorithm</head><p>We will now propose the following algorithm to generate trend hypotheses in a dataset:</p><p>1. Noise Elimination -implementing a density based noise removal algorithm to reduce the noise and outliers from the dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">IMPLEMENTATION DETAILS</head><p>The top level algorithm is given as Algorithm 1. In this section, we will elaborate on each of the five main steps in the proposed algorithm. </p><formula xml:id="formula_4">D ← D -X (i) 13:</formula><p>end if 14: end for Computing Centroids -The k-means partitional clustering methodology is chosen here for its better run time efficiency. For a given input of dataset D and a pre-designated number k, the algorithm partitions the dataset into k clusters as the output. We represent the clusters by their centroid C i . Membership for each cluster is determined by maximizing the inter-cluster distances and minimizing the intra-cluster distances between feature vectors (the sum of squared error (SSE)).</p><p>Principal Tree (MST) Generation -The main purpose of this step is to compute a compact representation of the relationship between the clusters. Here we chose the representation to be the minimum spanning tree connecting the cluster centroids. The connection weights to be minimized are the Euclidean distance between the centroids. To compute the MST, we use Kruskal's algorithm which is an efficient greedy algorithm that runs in O(nlogn) time. The output is the Adjacency matrix(A) for the MST or the Principal Tree. All the permutations of pairs of en ids are computed in the matrix Pends by the function Get Nodes, Pends = [en id1 ,en id2 ;en id1 ,en id3 ;...].</p><p>Identifying Independent Paths -This is the core component of our algorithm. The Principal Tree is the skeleton from which the End-To-End Paths P I s are identified. To obtain the mappings of the P I s, their l I s, the pairwise O IJ and the curvature ς I , the algorithm Path Value (incorporating Dijkstra's Algorithm <ref type="bibr" target="#b12">[13]</ref>) is implemented on the input Pends and A . Some of the P I s are completely independent of each other while some intersect with others. Some others overlap. The End-To-End Paths are transformed to a G t with these considerations.</p><p>The curvature factor of a given path is a measure of the cumulative cosine function values at the neighborhood of the intersection points. See <ref type="figure" target="#fig_6">Fig. 5</ref> where path P1 is a curved path compared to path P2 which is straight. The curvature value for a path is a measure of the summation of the cosine values of the angles made by the line segments at the intersection points. In order to compensate for any uncharacteristic deviations in the location of the centroids, we take the cosine measure of angles between two segments on either side of the intersection point. Here, P1 and P2 intersect at point d. Thus, curvature(P1) = cos(bcd)+ cos(cde)+ cos(def) = 0.75. Similarly, curvature(P2) = cos(ihd)+ cos(hdj)+ cos(djk) = 0. Path P1 has a higher curvature value as it has more curvature compared to the straightness of path P2. To obtain the set of P I s for the ϒ, the search is performed for the set of pair-wise combination of P I s which has in them all the C i s covered and are most independent. The main algorithm here is Find Ind Path which selects the P I s using a pairwise weight Γ. All the P I , P J pairs and the corresponding Γ IJ between them are identified as the Path matrix PV =[P I ,P J ,Γ IJ ]. This is the input to this algorithm along with the normalized weight parameters w o , w c and w l which sum to unity (w o + w c + w l = 1). The weight parameters control the relative importance of overlap, curvature, and length in the selection of the final paths. These are user input parameters whose value depends on the nature of trends desired and the type of dataset. Ind is the set of Indicator variables for C I s. For every P I selected, the C I s ∈ P I s are eliminated from Ind until Ind = φ . τ,Ind ← Find Path (P J , τ, Ind) 9: end for 10: return (τ)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 Find Ind</head><p>The main goal of computing the Traversal Graph (G t ) is to obtain the most independent set of paths (τ i ) that will cover all the data points. Our goal is to identify the minimal set of paths that cover all the cluster centroids and have the minimum edge weight (ensuring minimum overlap) between them. The algorithm Find Ind Path generates this minimal set τ. This problem of finding the optimal set of paths is formulated as a min-max optimization problem where the aim is to maximize the vertex weights (hence the inclusion of maximum number of Cluster Centroids) and minimizing the edge weights (thus minimizing the overlap). For a given set of vertices (P I ) and (P J ), the input weight (Γ IJ )to the Find Ind Path algorithm is formulated by the following objective function:</p><formula xml:id="formula_5">Γ IJ = w o * η IJ + w c * (ς I + ς J ) − (w l * (Φ I + Φ J ))<label>(2)</label></formula><p>The overlap parameter is (w o * η IJ ) is minimized here by subtracting the parameter for length ((w l * (Φ I + Φ J ))). Parameters for Algorithm 4 Find Path 1: Input:Vertex P I , set of independent paths τ, indicator matrix for C i s Ind 2: Output: τ, Ind 3: Pseudocode: 4: P i ← Get Centroids(P I ) 5: for j=1:size(P i ) do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>for k=1:size(Ind) do 7:</p><formula xml:id="formula_6">if P i (j) =Ind(k) then 8: Ind(k)←0 9:</formula><p>s ← size(τ) <ref type="bibr" target="#b9">10</ref>:</p><formula xml:id="formula_7">τ(s + 1) ← P i 11: end if 12:</formula><p>end for 13: end for 14: return (τ, Ind) curvature (w c * (ς I + ς J )) is added as well and the weights w o ,w l and w c determine the importance of each parameter in the 'trend' selection process. These weights are user input values and can be varied depending upon the type of trends desired and the nature of the dataset. For example, w l can be dominant in value amongst the weights if lengthy trends are desired. The End-to-End Paths corresponding to this minimal set of nodes are the trends in the data. As mentioned earlier, all the P I , P J pairs and the corresponding Γ IJ between them are described by the Path matrix PV=[P I ,P J ,Γ IJ ]. This and the weights w o ,w c ,w l , are the input to the algorithm whose output is τ.</p><p>Algorithm 3 gives the implementation of Find Ind Path. All pairs (P I ,P J ) are ranked in ascending order as per their Γ. The function Get Centroids obtains the corresponding set of nodes for each P I . The ranked pairs of paths are checked for the C i s that they include. Those C i s are eliminated from the Ind set which is a set of Indicator variables for C i s. The pairs of paths are sequentially checked for any new C i that they may add and the process terminates once all C i s are eliminated, that is, (C i ∈ τ) ∩ σ = φ . This task is performed by the function Find Path (See Algorithm 4).</p><p>Visualizing Trends -Each P I includes a sequence of C i s that represent the clusters of the data. Hence each P I is associated with a set of data points included in those clusters. These data points are identified and a principal curve can be fitted on this subset of the dataset <ref type="bibr" target="#b8">[9]</ref>. This is the representation of a trend in that data set.</p><p>A Note about the Parameters -The procedure as described contains several parameters that require user input. Thus it is not a fully automated procedure yet. Until a systematic way to determine the values of the parameters can be obtained, the procedure is best considered as a tool for exploratory analysis. For example, it can be embedded into a data visualization tool (such as Mirage <ref type="bibr" target="#b7">[8]</ref>), so that a user can iterate on trials of different parameter values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL RESULTS</head><p>All programs were written in MATLAB Version 6.5 and run on pentium Dual Core 2.8 GHz machines. Experiments were performed using both synthetic and high-dimensional real-world datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Synthetic Data sets</head><p>Our algorithm was tested successfully on various synthetic data sets that inherently contain trends. Several data sets were created with intersecting and overlapping paths in them. Our algorithm identified the data points along these paths and was able to separate out different trends in these data sets. <ref type="figure" target="#fig_7">Fig. 6(a)-(d)</ref> shows one such synthetic data set where a sine wave intersects with a cos wave in a four-dimensional space. The data set was created by pseudorandomly generating data points using these intersecting curves. The k-means clustering algorithm identified 18 Cluster centroids which were then used to build the Principal Tree using the Kruskal's algorithm. The weight parameters used in our objective function (see Eq. 2) were: w l = 0.4, w o = 0.4 and w c = 0.2. There were six nodes in the Traversal Graph (G t ) using all the six possible paths constructed from the four end nodes of the MST. For this dataset, our algorithm was able to identify two independent trends, as shown in <ref type="figure" target="#fig_7">Fig. 6(f)</ref>. Finally, principal curves were fitted to the data points belonging to each trend individually to obtain a continuous model for the trend.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Real-world Data sets</head><p>Real-world data sets were obtained from image sequences in the CMU Vision and Autonomous Systems Center's Image Database <ref type="bibr" target="#b0">1</ref> . The motion images were of particular importance because they have numerous motion sequences represented by frames numbered in order of the motion's progression. The movement in the images generated by either the change in the camera placement or its angle, or the actual physical movement of the subject provided a highdimensional data set with a definite ordered change in the data values as the motion sequence progressed. The entirety of the dataset contains all the images came from the same subject. The feature vectors obtained from the original images were of very high dimensions. A lower number of dimensions were obtained by rescaling the images. <ref type="table" target="#tab_5">Table 2</ref> provides details about various data sets used in our experiments. Most of these motion data sets inherently contain some form of a 'trend' in a very high-dimensional feature space representing the intensities at the pixels.</p><p>In order to obtain data sets that are more suitable for testing our algorithm, we increased the complexity of these sequences by adding more image sequences which were obtained by shifting images at a particular location. In other words, we generated more trends in the data by obtaining sequences using simulated camera panning that provided a new image sequence. The image at a certain location in the original sequence was used as the basis for subsequently generating more sequences by shifting the axis, thus generating either overlapping or intersecting image sequences of transfigured or shifted images. <ref type="figure" target="#fig_9">Fig. 7</ref> shows the image data sets containing intersecting and overlapping image sequences overlaid on the original image sequence.</p><p>Artichoke data set contains 100 images of a scene which has a toy dog, a cup and a road sign. By shifting the image 50 of the original sequence, an additional set of 121 images were obtained corresponding to a new intersecting trend in the data. Hence, there are a total of 221 images, each of size 60x64, containing trends that intersect at image number 50. This is shown in <ref type="figure" target="#fig_9">Fig 7(a)</ref> using the images in each sequence/trend.</p><p>Hand data set contains 481 images of a hand holding a rice bowl. We included only the first 200 images in our original sequence. We obtained overlapping sequences by shifting images at two different locations (100 and 150) in the sequence. A total of 120 (60 + 60) images were added to the original sequence and hence a set of overlapping trends was obtained. The image size is 60x64 and the sequences overlap between images 100 and 150, as shown in <ref type="figure" target="#fig_9">Fig 7(b)</ref>.</p><p>House data set contains a total of 111 images of a toy house. We obtained a set of overlapping sequences by shifting images 25 and 75. At each location, 50 images were generated. The image size is 72x48 and the sequences overlap between images 25 and 75.</p><p>SRI data set contains 125 images of a lab scene. A sequence of 49 images were obtained from Image 60 using a similar transformation used in the Artichoke dataset. The image size is 60x64 and the data set contains two trends intersecting at image 60.     The original image sequence and the transfigured image sequence represent two different trends in the data set. It is important to note that the trends were extracted in the original space and are visualized in the reduced 2D space in <ref type="figure" target="#fig_9">Fig. 7</ref>. The original highdimensional feature vectors can be visualized in either 2D or 3D space using traditional manifold learning algorithms <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5]</ref>. However, one cannot visualize more complicated datasets using such algorithms. Our algorithm, on the other hand, can extract the trends (and data corresponding to such trends) individually and then lay out the data ordered by each trend in a low-dimensional space. This gives users a way to inspect the trend-specific variability in the data that could be difficult to un-entangle by full manifold projections.</p><p>In all these data sets, we were able to identify the trends in the data using our algorithm. <ref type="table" target="#tab_6">Table 3</ref> gives the results after applying the algorithm to the datasets. Depending on the form of the data and number of data points, we initialized the k-means algorithm with a certain number of centroids between 40 and 60. The only other input parameters to our algorithm are the set of weights which will have to be chosen so that the output trends will have minimum overlap, maximum length and maximum smoothness. The standard parameter values used in our testing were w l = 0.4, w o = 0.4 and w c = 0.2. In some cases (such as the House data set) a minor tweaking of the parameters was needed in order to obtain the original trends. <ref type="table" target="#tab_6">Table 3</ref> also gives the number of data points belonging to each of the trends. Since the MST and the data assignment are done based on the centroid, a data point (especially in the neighborhood of the intersecting centroids) may belong to more than one trend.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND FUTURE RESEARCH</head><p>In spite of the vast literature in clustering and curve fitting, some real-world problems pose interesting challenges that require development of novel algorithms that can advance one step further from the basic clustering or curve fitting solutions. Our algorithm proposes to find interesting, continuous trends in the datasets. Analyzing these trends can provide useful insight into the nature of the data distributions and existing correlations.</p><p>Future work in this direction is to use the uncovered trends for real world datasets and find interesting associations between trends from different descriptions of the data, such as different subsets of features. Clustering procedures other than k-means can be explored for potential improvements, like cluster sculptor which allows the user to tune the clustering parameters so that the domain knowledge of the user can assist in creating better clusters <ref type="bibr" target="#b10">[11]</ref>. For very high-dimensional datasets, simple Euclidean distances may not be the best metric for clustering. Additional possibilities include an analysis of local intrinsic dimensionality of the data, and using it to guide a weighting on the metric to minimize the effect of noise.</p><p>Another possible extension is to cut the principal tree (MST) at the longest edges to break the data down to more natural clusters, in case there exists such an edge (known as the inconsistent edge in graph-theoretic methods for clustering). This can help avoid forcing a global trend on data that contain natural clusters that are far apart.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Original data with principal curves (b) Possible Paths in the principal curves</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Sample data and the possible trends in the data (a) The original data is being fitted using principal curves which go through the "medial-axis" of the data (b) With four different end nodes (a,b,c and d) and two intersection nodes (x,y). There are six possible end-to-end paths (A,B,C,D,E and F). A = (a,x,y,b), B = (c,x,y,d), C=(a,x,c), D=(a,x,y,d), E=(c,x,y,b), F=(d,y,b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Original data in 3-D (b) Projection of the data onto x-y plane (c) Projection of the data onto x-z plane</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>The original spiral data (a) the generating curve is represented by a solid line. (b) projection of the data onto a 2D x-y plane. (c) Projection of the data onto a 2D x-z plane.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Given a Traversal Graph G t =(P I , O IJ ), a set of weights {Φ I }, {η IJ }, find the minimal set of P I s such that: 1) the P I s in the set have the smallest amount of overlap with each other; 2) the P I s cover all the C i s in the Principal Graph G p .To evaluate the pairs of P I s and obtain the minimal set, a measure Γ given by Eq.(2) is introduced. Kruskal's algorithm<ref type="bibr" target="#b12">[13]</ref> is(a) Sample Data (b) Various Clusters and their Centroids (c) Minimum Spanning Tree on data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>The original data , the cluster identification and the Minimum Spanning Tree of the Cluster Centroids.(a) Possible End-To-End Paths from the MST skeleton. (b) Weights for each segments of the End-To-End Paths (c) Traversal Graph of the End-To-End Paths Graph Transformation: Vertices are End-To-End Paths with weights as the lengths of the End-To-End Paths. Edges are the overlaps between the End-To-End Paths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Curvature of paths</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Example of synthetic data set used to test our algorithm. The first row shows various views of the data across different dimensions. The second row shows the results of various steps in our algorithm. (e) Original data and the cluster centroids obtained using the k-means procedure. (f) Principal Tree which is the MST of the cluster centroids. (g) Two most independent End-to-End Paths identified by our algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Examples of Intersecting and Overlapping trends in Image datasets. Additional image sequences are obtained by simulating the camera panning which is done by shifting the coordinates. (a) Intersecting trends in the Artichoke data set. (b) Overlapping trends containing two intersecting nodes in the Hands image sequence data set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Notations used in this paper.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The degree of independence of the patterns is inversely proportional to the length of overlap between the pair of patterns. The key terms used in the paper are now defined here -Definition 1 Principal Tree -is a graph G p = ({C i }, {L i j }) (i, j = 1, ..., k)where the vertex set {C i } is the set of k Cluster Centroids obtained using a k-means procedure, and the set of edges {L i j } connecting centroids C i and C j is the set of links that jointly form a Minimum Spanning Tree (MST) over the vertex set {C i }. They are ordered sequences of vertices that are of the form (en i , in 1 , in 2 , ..., in m , en j ) (if there are m Internal Nodes). Intersection Nodes (is i ) -are those Internal Nodes in i that are on two or more End-To-End Paths.</figDesc><table><row><cell>en j ) Adjacency matrix for MST Length of P I O Curvature factor of paths A l I w o Weight for overlap w c Weight for curvature w l Weight for length G Definition 4 End-To-End Paths (P i j ) -are those paths in G p that contain exactly two End Nodes en i and en j and the Internal Nodes between them. There are ( t 2 ) paths if there are t End Nodes and all nodes appear just once in each P i j . Definition 5</cell></row></table><note>IJ Overlap between P I and P J = length(P I ∩ P J ) ςIt Traversal Graph =({P I },{O IJ }) ΦI Weight at the Vertex P I = l I ηIJ Edge Weight for O IJ τ Set of Independent P I s ΓIJ Input weight to Find Ind Path algorithm ϒ Trends 3.1 The Notion of Trends Trends (ϒ) are the minimal set of most independent sequence of patterns over the feature space containing the dataset. A pattern is a continuous region in a feature space that is filled with data points to a pre-designated density.Definition 2 End Nodes (en i ) -are those C i with degree 1 in G p . Definition 3 Internal Nodes (in i ) -are those C i with degree ≥ 2 in Gp .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Most real world datasets contain noise and outliers. Standard clustering algorithms like k-means, Hierarchical, etc., are sensitive to these factors. The density-based algorithm (Noise Removal) is used as a data preprocessing step to eliminate the noise and outliers. Outliers are points in the dataset which are characteristically low in density and can hence be identified using density based approaches. Two parameters are used to determine whether a point is in a sparse region: a neighborhood radius Eps and a count M of neighboring data points. Every data point which is not within the neighborhood radius Eps of at least M other data points is classified as a noise point. All such points are eliminated to obtain the output D. Algorithm 2 gives the pseudocode.</figDesc><table><row><cell>Algorithm 1 Finding Trends 1: Input: Dataset X 2: Output: Trends ϒ 3: Algorithm: 4: D ← Noise Removal(X , E ps, M) 5: σ ← k-means(D) 6: A ← Kruskal(σ ) 7: ρ, µ, γ, Pends i, j ← Get Nodes (A) 8: P I ,l I ,o I,J , ς I ← Path Values (Pends, A) 9: τ ← Find Ind Path (PV, w o , w c , w l ) 10: ϒ ← P-Curve(τ) Noise Elimination -Algorithm 2 Noise Removal 1: Input: Dataset X , Threshold value Eps, Min count M 2: Output: Noise-free dataset D 3: Pseudocode: 4: D ← X 5: for i← 1 : size(X ) do 6: Ind = / 0 7: for r ← 1 : size(X ), r =i do 8: Dist← dist[X (i),X (r)] 9: Ind = [Ind ; (Dist ≤ Eps ? X (r) : / 0) 10: end for 11: if size(Ind) ≤ M then 12:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Description of various real-world image sequence data sets used in our experiments.</figDesc><table><row><cell>Dataset</cell><cell>Original Size</cell><cell cols="3">Image Width Image Height No. of Dimensions</cell><cell>Nature of trends</cell></row><row><cell></cell><cell>(No. of Images)</cell><cell>(Pixels)</cell><cell>(Pixels)</cell><cell></cell><cell></cell></row><row><cell>Artichoke</cell><cell>100</cell><cell>60</cell><cell>64</cell><cell>3840</cell><cell>Intersecting</cell></row><row><cell>Hand</cell><cell>200</cell><cell>60</cell><cell>64</cell><cell>3840</cell><cell>Significant overlapping</cell></row><row><cell>House</cell><cell>111</cell><cell>72</cell><cell>48</cell><cell>3456</cell><cell>Significant overlapping</cell></row><row><cell>SRI</cell><cell>125</cell><cell>60</cell><cell>64</cell><cell>3840</cell><cell>Intersecting</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Results of our algorithm on real-world data sets.</figDesc><table><row><cell></cell><cell>Augmented Data Size</cell><cell>No. of</cell><cell>Size of</cell><cell>Size of</cell><cell></cell><cell></cell><cell>Runtime</cell></row><row><cell>Dataset Artichoke</cell><cell cols="6">(with trends) No. of Images Clusters Trend 1 Trend 2 Total Size w o 221 50 105 125 230 0.4 0.2 0.4 w c w l</cell><cell>CPU (sec) 100</cell></row><row><cell>Hand</cell><cell>320</cell><cell>60</cell><cell>166</cell><cell>202</cell><cell>368</cell><cell>0.4 0.2 0.4</cell><cell>235</cell></row><row><cell>House</cell><cell>211</cell><cell>55</cell><cell>139</cell><cell>128</cell><cell>267</cell><cell>0.3 0.2 0.5</cell><cell>89</cell></row><row><cell>SRI</cell><cell>174</cell><cell>40</cell><cell>87</cell><cell>94</cell><cell>181</cell><cell>0.4 0.2 0.4</cell><cell>65</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">OPTIMAL TRAVERSAL OF PRINCIPAL GRAPH FOR GEN-ERATING TREND HYPOTHESESIn this section, we will mathematically formulate the task of generating trend hypotheses as a graph traversal problem. We first give definitions for the notion of trends and a number of graph-theoretic entities needed in the description of the algorithm. We then give the problem statement.Table 1gives the notations followed in this paper.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://vasc.ri.cmu.edu/idb/html/motion/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work is partially funded by the Wayne State University Faculty research award and the DHS Dynamic Data Analysis Center grant.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Enhanced high dimensional data visualization through dimension reduction and attribute arrangement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Artero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>De Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Levkowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Information Visualization</title>
		<meeting>the conference on Information Visualization</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="707" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1373" to="1396" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A Practical Guide to Splines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Boor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978" />
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A unified model for probabilistic principal surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="41" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hessian eigenmaps: Locally linear embedding techniques for high-dimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Grimes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5591" to="5596" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Principal curves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Stuetzle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="502" to="516" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The Elements of Statistical Learning. Data Mining, Inference, and Prediction, chapter Boosting and Additive Trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Interactive tools for pattern discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Pattern Recognition</title>
		<meeting>the 17th International Conference on Pattern Recognition<address><addrLine>Cambridge, U.K.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="509" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Piecewise linear skeletonization using principal curves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krzyzak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="74" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Mixture models: Inference and applications to clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Mclachlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Basford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>Marcel Dekker</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Clustersculptor: A visual analytics tool for high-dimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zelenyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Imre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Visual Analytics Science and Technology (VAST)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="75" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A shrinking-based clustering approach for multidimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1389" to="1403" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Introduction to Algorithms,Second Edition, chapter Minimum Spanning Trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L R</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cormen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">E</forename><surname>Leiserson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>MIT Press and McGraw-Hill</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
