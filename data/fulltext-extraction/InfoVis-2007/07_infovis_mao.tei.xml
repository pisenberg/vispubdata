<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sequential Document Visualization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2007-10-27">27 October 2007</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Mao</surname></persName>
							<email>ymao@ece.purude.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Statistics</orgName>
								<orgName type="department" key="dep2">School of Elec. and Computer Engineering</orgName>
								<orgName type="institution">Purdue University -West Lafayette</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Statistics</orgName>
								<orgName type="department" key="dep2">School of Elec. and Computer Engineering</orgName>
								<orgName type="institution">Purdue University -West Lafayette</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
							<email>jvdillon@ece.purude.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Statistics</orgName>
								<orgName type="department" key="dep2">School of Elec. and Computer Engineering</orgName>
								<orgName type="institution">Purdue University -West Lafayette</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Statistics</orgName>
								<orgName type="department" key="dep2">School of Elec. and Computer Engineering</orgName>
								<orgName type="institution">Purdue University -West Lafayette</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Lebanon</surname></persName>
							<email>lebanon@stat.purude.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Statistics</orgName>
								<orgName type="department" key="dep2">School of Elec. and Computer Engineering</orgName>
								<orgName type="institution">Purdue University -West Lafayette</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Statistics</orgName>
								<orgName type="department" key="dep2">School of Elec. and Computer Engineering</orgName>
								<orgName type="institution">Purdue University -West Lafayette</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Sequential Document Visualization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2007-10-27">27 October 2007</date>
						</imprint>
					</monogr>
					<note type="submission">received 31 March 2007; accepted 1 August 2007</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T18:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Document visualization</term>
					<term>multi-resolution analysis</term>
					<term>local fitting</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Documents and other categorical valued time series are often characterized by the frequencies of short range sequential patterns such as n-grams. This representation converts sequential data of varying lengths to high dimensional histogram vectors which are easily modeled by standard statistical models. Unfortunately, the histogram representation ignores most of the medium and long range sequential dependencies making it unsuitable for visualizing sequential data. We present a novel framework for sequential visualization of discrete categorical time series based on the idea of local statistical modeling. The framework embeds categorical time series as smooth curves in the multinomial simplex summarizing the progression of sequential trends. We discuss several visualization techniques based on the above framework and demonstrate their usefulness for document visualization.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Categorial valued time series y = y 1 ,...,y N , y i ∈ V such as text documents or protein sequences are difficult to visualize due to their discrete categorical nature. This difficulty is especially severe when the set V representing the range of possible nominal or categorical values is large. For example, the set V contains dozens of amino acids in the case of proteins and hundreds of thousands, if not millions, of possible words in the case of text documents. Due to the categorical nature of the data, visualization techniques designed for numeric time series data, e.g. <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b8">9]</ref>, are not applicable. A popular alternative is to use dimensionality reduction techniques such as principal component analysis (PCA) or multidimensional scaling (MDS) (e.g., <ref type="bibr" target="#b3">[4]</ref>) for vectorized data based on the sequence histogram</p><formula xml:id="formula_0">γ hist (y) def = 1 N N ∑ j=1 δ 1,y j ,..., 1 N N ∑ j=1 δ k,y j ∈ R |k|<label>(1)</label></formula><p>where we assume without loss of generality that V = {1,...,k} and The histogram representation may lead in some cases to effective visualization of the semantics characterized by the entire document. However, it lacks the ability to capture and visualize the semantic transition between different parts of the document. This lack of a withindocument locality is particularly problematic when the visualization focus is on visualizing the contents of a single document (or a small number of documents) rather than a large corpus.</p><p>A partial solution to the above problem is to represent y by the histogram of length-n patterns called n-grams <ref type="bibr" target="#b12">[13]</ref>. For example, n-grams for n = 3, also called trigrams, are sequential patterns</p><formula xml:id="formula_1">v 1 , v 2 , v 3 , v 1 , v 2 , v 3 ∈ V</formula><p>and their histogram is simply the vector of normalized counts of length-3 pattern appearances. For example, the relative frequency of 3, 1, 3 in a sequence y is</p><formula xml:id="formula_2">1 N − 3 + 1 N−3+1 ∑ i=1 δ y i ,3 δ y i+1 ,1 δ y i+2 ,3 .</formula><p>Increasing the pattern length n invokes the well known biasvariance tradeoff in statistics. High n has the potential of capturing longer range sequential information. On the other hand, the dimensionality of the n-gram representation increases exponentially with n making the n-gram histogram a poor estimator of the underlying expectation parameters. Reducing n improves the accuracy of the ngram histogram as a statistical estimator, but prevents the representation from capturing long range interactions. Since for text documents N |V |, most words and phrases in V will never occur at the same document. As a result, the pattern length n in n-grams is usually restricted to small sizes such as n = 1, 2, or 3.</p><p>Unfortunately, the n-gram representation does not adequately address the histogram shortcomings associated with the lack of sequential information. Frequencies of short phrases do not reflect long or medium range sequential information, nor do they reflect the positions in the document at which the different phrases occur. It is fair to say that the n-gram representation may be viewed as offering a tokenization of short phrases rather than words which is useful for capturing linguistic ambiguity. It does not capture any sequential information beyond that and remains poorly suited for sequential visualization.</p><p>We present a new framework for sequential visualization of documents and other categorical time series which generalizes n-grams in a robust way and maintains both long and short range sequential information as well as position information. The framework, initially explored in <ref type="bibr" target="#b9">[10]</ref> in the context of classification, is based on the ideas of local smoothing and multi-resolution analysis from non-parametric statistics and signal processing.</p><p>By locally averaging the word histograms at different document locations, we obtain a local version of the global histogram γ hist (y) describing the local word distribution. Viewed geometrically, the local averaging embeds the original sequence as a smooth curve in the space of histograms over V . This curve summarizes the progression of semantic and statistical trends within the document. The smooth nature of the curve enables the use of a wide range of tools from differential geometry and smooth analysis. By varying the amount of local averaging we obtain a family of sequential representations possessing different sequential resolutions or scales. Low resolution representations capture topic trends and shifts while ignoring finer details. High resolution representations capture the fine sequential details but make it difficult to grasp the general trends within the document. The rest of the paper is organized as follows. Section 2 reviews related work and Section 3 presents the locally weighted bag of words framework for smooth embeddings of categorical time series. In Section 4 we explore various visualization techniques based on the new framework and present a publicly available visualization toolkit. We then proceed in Section 5 to describe a user study demonstrating the usefulness of the new visualization techniques, followed by a discussion in Section 6. We focus in this paper on demonstrating our ideas in the context of document visualization. However, both the visualization techniques and the toolkit are also applicable to visualizing biological sequences such as proteins or DNA sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Document visualization has recently gained considerable interest due to the recent increase in the size, number and accessibility of text archives. A partial list of references is <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b1">2]</ref> with additional references available in <ref type="bibr" target="#b19">[20]</ref>. A selection of software systems for visualizing text corpora are IN-SPIRE 1 , Enron-exploration tools 2 , Thomson's refviz 3 , and the Science topic browser <ref type="bibr" target="#b3">4</ref> . Most of the methods mentioned above as well as other ones mentioned in <ref type="bibr" target="#b19">[20]</ref> are designed for non-sequential visualization of a corpus of documents by considering the distance relation between individual documents. Such visualization methods are useful tools in browsing vast textual archives and nicely augment automatic search methods.</p><p>In contrast to the above methods, our approach aims at visualizing the sequential semantic progression within a single document. Related work concerning sequential analysis of a single document includes TextTiling <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> which partitions the data into multi-paragraph segments based on the local word histogram. Similarity scores between local word histograms at different document locations are used to segment the document into "text tiles" which were found to correspond well to subtopic boundaries according to human judgement. Similar ideas are explored in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23]</ref>, in the statistical text segmentation literature e.g. <ref type="bibr" target="#b0">[1]</ref> and in the text summarization literature e.g. <ref type="bibr" target="#b18">[19]</ref>.</p><p>Multi-resolution analysis provides a convenient mechanism for the sequential visualization of documents at several levels of granularity. The modern multi-resolution analysis of data is inspired by the short time Fourier transform and wavelet representation <ref type="bibr" target="#b11">[12]</ref>. An interesting application of multi-resolution wavelet analysis to document browsing is the Topic-Islands technique <ref type="bibr" target="#b13">[14]</ref>. Topic-Islands constructs a digital signal that corresponds to the text document and then proceeds to compute its discrete wavelet transform. Multi-resolution visualization is then carried out by visualizing the energy of the various wavelet coefficients.</p><p>Our approach has several unique properties that distinguish it from the related research described above and other studies. The categorical sequences are represented as smooth curves in the histogram space which may be conveniently studied in several resolutions by varying the degree of smoothing. The representation includes the original categorical sequence y = y 1 ,...,y N and the word histogram γ hist (y) as special cases. Interpolating between these two extremes provides the flexibility of viewing sequential details at the desired resolution in a simple and effective manner.</p><p>The smoothness of the representation enables the use of tools from differential geometry and smooth analysis such as gradient, curvature, differential operators, and phase diagrams. These tools are used to create new visualization techniques for sequential trends in documents. Another interesting aspect of the proposed framework is that it draws interesting connections between document visualization and the considerable literature of local fitting <ref type="bibr" target="#b10">[11]</ref> and functional data analysis <ref type="bibr" target="#b14">[15]</ref> in statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE LOCALLY WEIGHTED BAG OF WORDS FRAMEWORK</head><p>The locally weighted bag of words (lowbow) representation was introduced in <ref type="bibr" target="#b9">[10]</ref> for the purpose of improving text document classification. In this section, we review its definition and some of its important properties before proceeding to discuss its applications to visualization in the next section. The description below applies to 1-gram or word histogram and is a slightly simplified version of the original definition in <ref type="bibr" target="#b9">[10]</ref>. Its extension to the locally weighted n-grams is straightforward.</p><p>Since there is no clear ordinal relationship between words in the vocabulary, it is natural to represent words as basis vectors e i = (0,...,0, 1, 0,...,0) ∈ R |V | rather than integers. Viewed in this way, a document corresponds to a sequence of vectors in R |V | representing its sequential contents. For example, a document y = 2, 2, 1, 1, 2 over a vocabulary V = {1, 2, 3} corresponds to e 2 , e 2 , e 1 , e 1 , e 2 =</p><formula xml:id="formula_3">⎛ ⎝ 0 1 0 ⎞ ⎠ , ⎛ ⎝ 0 1 0 ⎞ ⎠ , ⎛ ⎝ 1 0 0 ⎞ ⎠ , ⎛ ⎝ 1 0 0 ⎞ ⎠ , ⎛ ⎝ 0 1 0 ⎞ ⎠ . (3)</formula><p>The locally weighted bag of words representation assigns local word histograms to different locations in the document. It is obtained from (3) by turning the discrete document location quantity into a continuous quantity {1,...,N} → [0, N] and smoothing the vector sequence (3) over the continuous quantity.</p><p>Definition 1 Given a sequence y = y 1 ,..., y N , y i ∈ V , the continuous-time analog of (3) is the function</p><formula xml:id="formula_4">δ y : [0, N] ×V → [0, 1] δ y (t, j) def = δ j,y t t ∈ [0, N], j ∈ V (4)</formula><p>where t is the smallest integer larger than or equal to t and δ a,b is given by <ref type="bibr" target="#b1">(2)</ref>.</p><p>Locally averaging temporal variations in δ y provides sequential smoothing across the continuous document locations. It is obtained by convolving δ y with a smoothing kernel whose shape determines the amount of smoothing and the resulting sequential resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2 A smoothing kernel is a positive valued function K μ,σ :</head><p>[0, N] → R + parameterized by a location parameter μ ∈ [0, N] and a scale parameter σ &gt; 0. We also assume that K μ,σ (t) is smooth in μ,t and is normalized i.e., K μ,σ (t) dt = 1.</p><p>A standard example for a smoothing kernel K μ,σ (t) is the Gaussian or normal distribution with mean μ and variance σ 2 :</p><formula xml:id="formula_5">K μ,σ (t) = C exp(−(t − μ) 2 /(2σ 2 )) where C ensures normalization over the range [0, N].</formula><p>The parameter μ represents the mode or maximum and σ represents the spread or concentration around μ which determines the amount of local smoothing.  </p><formula xml:id="formula_6">γ (σ ) (z) in P {1,2} is visualized by graphing [γ (σ ) μ (z)] 1 as a function of μ/N. (b) The curve γ (σ ) (w) in P {1,2,3}</formula><p>is visualized by graphing it on the 2-D triangular space representing P {1,2,3} (see <ref type="figure" target="#fig_2">Figure 1</ref>). In both cases σ /N = 1/10 (left) and σ /N = 2/10 (right) were used. Increasing σ causes the curves to be more smooth and to shrink towards the red line (a) or red triangle (b) which denote the histograms or the degenerate curves γ (∞) (z), γ (∞) (w).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 3</head><p>The locally weighted bag of words (lowbow) representation of a sequence y, at time μ and scale σ , is the vector γ</p><formula xml:id="formula_7">(σ ) μ (y) ∈ R |V | [γ (σ ) μ (y)] j def = N 0 δ y (t, j) K μ,σ (t) dt j = 1,...,|V |.<label>(5)</label></formula><p>It is easy to verify that the vector γ μ (y) is a probability distribution over V representing the locally weighted word histogram at the document location μ. Furthermore, it can be shown that γ (σ ) μ (y) is a continuous and smooth function in μ (for details see <ref type="bibr" target="#b9">[10]</ref>). As a result of these two observations, we conclude that the lowbow representation</p><formula xml:id="formula_8">γ (σ ) (y) def = {γ (σ ) μ (y) : μ ∈ [0, N]} of</formula><p>y is in fact a smooth curve in the space P V of histograms or probability distributions over V</p><formula xml:id="formula_9">P V def = θ ∈ R |V | : ∀i θ i ≥ 0, |V | ∑ j=1 θ j = 1 .</formula><p>By increasing σ , more local smoothing is applied and the resulting curve γ (σ ) (y) becomes smoother and more slowly varying. In the limit of σ → ∞, the kernel becomes a constant function ∀μ, lim σ →∞ K μ,σ (t) ≡ 1/N and the lowbow curve contracts into a single point which turns out to be equal to the word histogram</p><formula xml:id="formula_10">γ hist (y) defined in (1) lim σ →∞ γ (σ ) μ (y) = γ hist (y) ∀μ ∈ [0, N].<label>(6)</label></formula><p>On the other hand, reducing σ reduces the amount of sequential smoothing making the lowbow curve more wiggly. At the extreme case of σ → 0 the lowbow curve loses its continuity and reverts to δ y which is equivalent to the original sequence y = y 1 ,...,y N . In general, varying σ changes the spread of K μ,σ around its mode μ thus controlling the desired amount of smoothing or sequential resolution. Choosing a large σ reveals the general sequential behavior of the document while smoothing away the finer details. Choosing a small σ increases the sequential resolution which makes finer details available -but doing so may obscure the more general and important sequential trends. The choice of sequential resolution reflects the bias-variance statistical trade-off and ultimately depends on the precise data analysis goal. As we demonstrate in later sections, in some cases there is a need to analyze a document by considering its lowbow curves at multiple scales or resolutions. We illustrate the construction of the lowbow curve by considering a couple of artificial documents with a small vocabulary: z = 1,1,1,2,2,1,1,1,2,1,1 and w = 1,3,3,3,2,2,1,3,3 over V = {1, 2} and V = {1, 2, 3} respectively. γ (σ ) (z) is a curve in the space of distributions P {1,2} . Since in this case [γ    <ref type="figure" target="#fig_0">Figure 2</ref>(b) visualizes the curve γ (σ ) (w) for the same σ /N values in P {1,2,3} after embedding it in two dimensions (see <ref type="figure" target="#fig_2">Figure 1</ref>). Notice how in both cases increasing σ causes the curve to be smoother and to contract into the red line (for z) or red triangle (for w) which denotes the degenerate curve γ (∞) (z) or γ (∞) (w) corresponding to the bag of words or global histogram γ hist (z), γ hist (w).</p><p>Graphing the smoothed representation γ (σ ) as in Figures 2(a), 2(b) is a convenient technique to visualize sequential variation at a particular resolution. By selecting an appropriate smoothing scale σ , important sequential patterns stand out and are uncluttered by patterns at lower resolutions. For example, in the case of <ref type="figure" target="#fig_0">Figure 2</ref>(a) and word sequence 1,1,1,2,2,1,1,1,2,1,1 selecting a resolution of σ /N = 1/10 reveals the fact that there are actually two local minima representing high local concentration of 2 and one local maximum in between representing high local concentration of 1. Switching to σ /N = 2/10 we obtain only one broad local minimum and the additional details that were revealed before in the case of σ /N = 1/10 disappear.</p><p>The curve γ (σ ) is easily visualized in the case of V = {1, 2} or V = {1, 2, 3} as shown by the 1-D or 2-D figures described above. In the more interesting cases of text or biological sequences, V is a much larger set making γ (σ ) μ (y) a high dimensional vector and γ (σ ) (y) a curve in a high dimensional space. The next section deals with various techniques to visualize such high dimensional cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">VISUALIZATION TECHNIQUES</head><p>As mentioned in the previous section, when</p><formula xml:id="formula_11">|V | is large, γ (σ ) = {γ (σ ) μ : μ ∈ [0, N]</formula><p>} is a smooth curve in a high dimensional space which cannot be directly visualized. In this section we introduce several techniques for visualizing such curves and demonstrate their effectiveness in the context of document visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Visualizing Semantic Variation</head><p>Since the lowbow curve {γ</p><formula xml:id="formula_12">(σ ) μ : μ ∈ [0, N]} ⊂ P V</formula><p>describes the movement of the local word histogram it may be used to visualize subtopic boundaries and other sequential shifts. Portions of the lowbow curve that remain more or less in the same region of P V correspond to a semantically homogenous region within the document. On the other hand, portions of the lowbow curve containing drastic movements in P V correspond to abrupt topic changes.</p><p>The first approach we take to visualize the movement of the lowbow curve is to summarize it using linear differential operators. Linear differential operators L( f ) = ∑ i α i D i f , commonly used in functional data analysis <ref type="bibr" target="#b14">[15]</ref>, represent a function f by a linear combination of its derivatives of varying orders. They are well defined on the lowbow curve due to its smoothness and are well suited to capture the instantaneous behavior of the curve. The simplest differential operator is the gradient vector whose components measure the rate of change in different dimensions. Applied to the lowbow curve, we define it as the following vectoṙ</p><formula xml:id="formula_13">γ (σ ) μ def = d dμ [γ (σ ) μ ] 1 ,..., d dμ [γ (σ ) μ ] |V | ∈ R |V |</formula><p>whose components reflect the instantaneous change in the frequency of different words. The gradient vector is defined separately for each location μ and may be considered as a vector field along the lowbow curve pointing at the direction the curve is currently progressing in. It is easy to visualize the rate at which the lowbow curve moves at different locations μ by plotting the gradient norm γ <ref type="bibr">(σ )</ref> μ as a function of μ at different scales σ . Doing so reveals the presence of topic boundaries as well as areas representing homogenous and heterogenous semantic contents. In contrast to complex tools from the segmentation literature <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19]</ref>, the gradient norm is simple, intuitive, and easy to visualize.</p><p>To illustrate the role of the gradient in document visualization we examine its behavior for documents containing clear and predetermined segments. Following <ref type="bibr" target="#b0">[1]</ref>, we consider a document created by concatenating individual news stories which resembles the continuous transcription of online news. We created it by randomly picking a news-wire chunk containing three successive news articles from the RCV1 collection 5 (document id: 4078, 4079 and 4080). The two internal segment borders occur at μ/N = 0.53 and μ/N = 0.76 with the first story obviously longer than the next two stories. The left panel of <ref type="figure" target="#fig_8">Figure 3</ref> displays the gradient norm as a function of μ/N. The curve has several local maxima, but the largest two local maxima correspond almost precisely to the segment borders at μ/N = 0.53, 0.76. The first three local maxima correspond to internal segment boundaries within the first story. Indeed, the first news story begins with the announcement of Sun International Hotels Ltd.'s acquisition of Griffin Gaming &amp; Entertainment Inc.; it then switches to discuss the influencing factor behind Sun's decision at point μ/N = 0.17 before switching again at μ/N = 0.26 to talk about the benefit of the acquisition to Griffin. The story moves on at μ/N = 0.45 to discuss the deal in detail. As with the different news story boundaries, the internal segment boundaries of the first story closely match the local maxima of the gradient norm.</p><p>As demonstrated above the first derivative of the curve conveys important information for the purpose of detecting areas of fast and slow semantic transition. In a similar way, the second derivativeγ (σ ) contains information concerning the curvature or the rate of change of the curve velocity. Regions in the curve corresponding to high or low <ref type="bibr" target="#b4">5</ref> http://trec.nist.gov/data/reuters/reuters.html second derivative or other linear differential operators could provide additional visual aid to a human observer.</p><p>The curve itself γ (σ ) can be visualized by projecting it into its principal components or via multidimensional scaling. We compute the principal components (PCA) or the multidimensional scaling (MDS) of the curve γ (σ ) based on a finite sample of points from the curve {γ</p><formula xml:id="formula_14">(σ ) μ 1 ,...,γ (σ )</formula><p>μ s }. Such visualization techniques would provide the user with smooth curves in 2-D or 3-D whose behavior reveals local semantic information. Portions of the curve corresponding to similar semantic components will typically contain similar local word histograms and therefore will naturally cluster together in both the high dimensional space P V and in the low dimensional projections obtained by PCA or MDS. <ref type="figure" target="#fig_8">Figure 3</ref> (right) shows the 2D projection of the lowbow curve for the three concatenated RCV1 stories mentioned above. To embed the high dimensional curve in two dimensions we used principal component analysis. The blue crosses indicate the positions of the sampled points in the low dimensional embedding while the red circles correspond to the segment boundaries of the three documents. The sampled points in the figure {γ</p><formula xml:id="formula_15">(σ ) μ 1 ,...,γ (σ )</formula><p>μ s } are naturally grouped into three clusters, indicating the presence of three distinct segments mentioned above. The distance between successive points near the segment boundaries is relatively large which demonstrates the high speed of the lowbow curve at these points. This is easily confirmed by examining the gradient norm in the left panel of <ref type="figure" target="#fig_8">Figure 3</ref>.</p><p>Another interesting visualization technique is to graph the projection of the curve γ (σ ) μ on the top principal components separately as a function of the location μ/N. <ref type="figure">Figure 4</ref> contains such plots for the concatenated document. The 3 panels correspond to the projection on the top three principal components (top panel represents the first component and bottom panel represents the third component). The words appearing to the right (left) of the panels correspond to the words whose projection on the components give the highest (lowest, respectively) values. The plots illustrate how the first principal component of the curve γ <ref type="bibr">(σ )</ref> captures the variation between the first and the third stories and the second principal component of γ <ref type="bibr">(σ )</ref> captures the variation between the second and the other two stories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Visualizing Energy Transfer</head><p>An interesting visualization method that draws inspiration from harmonic motion in physics is to graph the first derivative vs. the second derivative of γ (σ ) μ (y) with respect to μ. The resulting graph, called the phase-plane plot, describes the relationship between the velocity and x 10  <ref type="figure">Fig. 4</ref>. Projection of the curve γ (σ ) on its top three principal components for the three concatenated RCV1 stories. The numbers at the right hand side of a word give the word's position in the concatenated documents, e.g. the word "hotel" only appears in the first document.</p><p>acceleration of the curve γ (σ ) as a function of the location parameter. Harmonic motion in physics, such as the movement of a simple pendulum or a spring behaves according to the differential equation</p><formula xml:id="formula_16">x(t) = cx(t).<label>(7)</label></formula><p>It is easy to see that in this case, velocityẋ(t) and accelerationẍ(t) behave as a sine and a cosine function respectively and drawingẋ(t) vs.ẍ(t) for various t results in the circle displayed in <ref type="figure">Figure 5</ref>  μ (y) projected on the third principal component. The striking resemblance between the two phaseplane plots indicates that the projection of γ (σ ) on its third principal component satisfies the harmonic motion differential equation <ref type="bibr" target="#b6">(7)</ref>. Stated differently, we have that the curve γ (σ ) approximately satisfies a partial differential equation <ref type="bibr" target="#b2">3</ref> is the third principal component of the curve.</p><formula xml:id="formula_17">D 2 γ (σ ) , v 3 = c γ (σ ) , v 3 where v</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Multi-resolution Visualization of Data Content</head><p>We have already seen how the shape or resolution of the curve is influenced by different smoothing scales σ (recall <ref type="figure" target="#fig_0">Figures 2(a) and 2(b)</ref>). In this section we examine the simultaneous visualization of multiple curves γ (σ 1 ) (y),...,γ (σ r ) (y) representing the same document at varying resolutions or scales σ 1 &lt; ••• &lt; σ r . Such visualization provides a simple yet effective way to summarize a document at various levels of detail.</p><p>Our multi-resolution visualization is based on constructing a sequence of documents y (σ 1 ) ,...,y (σ r ) which are reduced resolution versions of y. The words of the reduced resolution document y <ref type="bibr">(σ )</ref> are taken to be the most prominent components of the lowbow curve in the appropriate scale γ (σ ) (y). More specifically, we define the reduced resolution document to be</p><formula xml:id="formula_18">y (σ ) μ def = arg max j∈V [γ (σ ) μ (y)] j .<label>(8)</label></formula><p>The construction γ (σ ) (y) → y <ref type="bibr">(σ )</ref> in (8) may be considered as the approximate inverse to the lowbow mapping y → γ (σ ) (y). In other words, y (σ ) is the optimal estimate of the original document based on the lowbow curve γ (σ ) (y) (perfect reconstruction is impossible since y → γ (σ ) (y) is non-invertible). Increasing σ makes y (σ ) more homogenous as it captures major sequential trends while discarding finer details. Reducing σ makes y (σ ) more precise and detailed retrieving the original document y at the limit σ → 0.</p><p>We demonstrate the above idea by constructing the multi-resolution representation y (σ 1 ) ,...,y (σ r ) corresponding to a recent news story from reuters.com. <ref type="bibr" target="#b5">6</ref> The document was preprocessed by removing stop-words, punctuation marks and numbers, and stemming, resulting in a sequence of 272 lower-cased words. We arbitrarily set the length of the reduced resolution documents to be 51 and compute y <ref type="bibr">(σ )</ref> for σ /N = 0.04, 0.07, 0.5.</p><p>We develop two techniques for visualizing the multi-resolution representation. <ref type="figure">Figure 6</ref>(a) demonstrates the color bar approach which displays the reconstructed documents y (σ 1 ) ,...,y (σ r ) as vertical bars side by side ordered in the same order as the scale values. Each vertical bar is split into colored segments describing the word content of the corresponding y (σ i ) . Each word is assigned a different color and its starting position is indicated by a number at the left side of the color box, along with the word itself at the right side. The second visualization method, displayed in <ref type="figure">Figure 6</ref>(b), is inspired by the source code folding tool in integrated development environments such as Microsoft Visual Studio, Netbeans, or Eclipse. It displays the reconstructed document y (σ i ) as a list of words where each word acts as a button which can be expanded in a tree-like manner into the corresponding portion of the original y.</p><p>Both the color-bar and text folding designs enable the user to traverse the multi-resolution representation y (σ 1 ) ,...,y (σ r ) starting from a high σ representing a coarse summary of the original document y to lower values of σ representing finer temporal details. For the specific example described above, at the large scale σ /N = 0.5 we obtain the keywords of US and Iran which demonstrate the general topic of the article. As σ decreases, more details are added to the reduced resolution document diagram. At σ /N = 0.07, we get the added details concerning oil and gasoline. At the finer level of σ /N = 0.04, we learn additional details such as the involvement of Europe in the news story. Despite being a lossy representation, the reduced resolution document idea represents a convenient way to visualize temporal summaries of y at increasing levels of detail. The text folding design helps to effectively browse a document and quickly locate regions of interest using a tree-like search rather than the slower linear scan. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Visualizing Multiple Documents</head><p>Thus far we were mostly concerned with visualizing a single sequence y. In this section we briefly explore visualizing the relative behavior of sequential trends in multiple sequences. As with other forms of sequential visualization, these differences are usually difficult to identify using the traditional n-gram approach. As an example, we compare the news story used in Section 4.3 with a somewhat similar news story. <ref type="bibr" target="#b6">7</ref> The cosine similarity between the corresponding histograms or γ hist vectors is very high -0.9928. The projection of the two resulting curves γ (σ ) (y 1 ), γ (σ ) (y 2 ) into the first two principal components is displayed in <ref type="figure" target="#fig_12">Figure 7</ref>. Note that in this case, in contrast to previous cases the PCA is computed based on sampled points from two curves rather than just one. The words that correspond most strongly and weakly to each principal component are displayed along the x and y axes. The two curves share similar sequential trends most of the time, with the red curve diverging to a different region towards the end. Note also how both curves move from discussing political events concerning Iran and Britain at the left part of the x axis to discuss the effect on oil price and other economic factors at the right part of the x axis. The blue circles in <ref type="figure" target="#fig_12">Figure 7</ref> indicate two overlapping regions between the documents. Actually, both documents at μ 1 /N 1 ≈ 0.5 and μ 2 /N 2 ≈ 0.91 talk about Iran's daily shipments of oil and at μ 1 /N 1 ≈ 0.95 and μ 2 /N 2 ≈ 0.82 talk about a strike by workers at the French Mediterranean oil terminal. Visualizing the relative sequential trends in more than 2 documents results in a cluttered graph containing many curves. An alternative is to use tools from functional data analysis such as functional principal component analysis and functional canonical correlation analysis <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Interactive Document Visualization Toolkit</head><p>To compare the different techniques and evaluate them we constructed an interactive document visualization and exploration toolkit. The toolkit, displayed in <ref type="figure" target="#fig_13">Figure 8</ref> is implemented in Java and is publicly available <ref type="bibr" target="#b7">8</ref> . It contains all the features described in this paper as well as a few additional ones.</p><p>As shown in <ref type="figure" target="#fig_13">Figure 8</ref>, the right panel of the toolkit displays a user specified text document, either with text folding or without. In its left panel the toolkit displays various graphical summaries of the resulting curve γ (σ ) such as velocity or PCA and MDS projections. The bottom panel contains various buttons that allow the user to change the resolution determined by the scale σ or to change the visualization technique. Some features of the lowbow curve such as the velocity can be visualized in the right panel by coloring the text words with appropriate colors. Other features included in the toolkit are visualization of linear differential operators and 3D graphical exploration of PCA and MDS projections using OpenGL. Document preprocessing steps that need to be completed before using the toolkit are: removing stop-words, punctuation marks and numbers, removing casing, stemming, and constructing a vocabulary. The most computer intensive aspect of the preprocessing is the building of the vocabulary whose complexity depends on the length of the document. For all but extremely long documents, the vocabulary building as well as the rest of the preprocessing is extremely fast. Furthermore, all preprocessing tasks are standard and can be accomplished using a wide variety of publicly available text processing softwares. The complexity of constructing and maintaining the lowbow representation is completely determined by the number of sample points and the number of unique words in the document. More specifically it is equivalent to the number of sampled points times the complexity of compiling and maintaining the bag of words or word histogram.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">USER STUDY</head><p>To evaluate this toolkit we conducted an informal study consisting of 30 users each of whom were asked to use the toolkit to answer three types of questions. The users ranged in age from 18 to 55 years old with 1/3 of the users being female. Slightly more than 2/3 of the users were not native speakers of English with the majority of participants being graduate students from engineering or science departments.</p><p>Before attempting the study, a user was introduced to the toolkit by following a written, self-directed tutorial in which he or she was encouraged to experiment with varying scale values on a test document and observe its effect with respect to the velocity, the PCA projection and the reduced resolution document by text folding. The remaining features were disabled for the sake of brevity.</p><p>In the first of the three tasks, the users were asked to discern topic boundaries in eight passages containing two to four concatenated news stories from RCV1. In the second task, users were presented with four documents and a few topic phrases and were asked to find portions of a document that were best represented by the supplied phrases. The phrases were specifically chosen so that their constituent words either did not appear or rarely appeared within the documents. In the third and final task users were asked to identify five keywords for each of eight documents from a list of keyword candidates. In all three tasks, half of the questions disallowed use of the toolkit while the remaining half encouraged, but did not require, its use. Users were required to finish each task within some time constraint, and indicate, at the end of the task, how useful they found the various visualization techniques in accomplishing the three tasks. The average score received by each feature-task pair is displayed in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>Our initial expectation was to receive high score for velocity in task 1, for PCA and PCA components in task 2, and for text-folding in task 3. Part of the user study results confirm our expectation, as indicated by the scores for (task 1, velocity) and (task 3, text folding) pairs. Surprising aspects of the results in <ref type="table" target="#tab_2">Table 1</ref> are that users found the text folding instructive in all three tasks, and they largely disregarded the PCA and PCA component plots.</p><p>Among all the visualization techniques, the velocity graph and the text folding technique are the most intuitive and simplest to understand. This explains the popularity of these two techniques in the user study. In particular, the text folding allowed users to quickly hone in on semantically meaningful words thus increasing the search efficiency as indicated by its effectiveness for tasks 2 and 3. Text folding is more powerful than a simple 'find' or pattern matching tool since the user does not have to specify all the synonyms of the words he is interested in. For example, in task 3, users using the text folding tool were able to quickly identify keywords relevant to a certain passage even if a keyword did not appear as one of the words in the passage itself. The benefit of the tree-like search in text folding and other multi-resolution methods over linear scan is similar to the complexity advantage of tree search (O(log n)) over a linear search (O(n)).</p><p>The relatively low scores given by users to the PCA and PCA components techniques illustrate their difficulty in grasping fully the PCA concept and effectively utilizing it after only a short introduction and experimentation. The users were given only several minutes before the questions to experiment with the toolkit and most of them could not comprehend its potential right away. Our conclusion is that to fully comprehend and effectively use the less intuitive PCA and PCA components, users need more guidance into how to use it and spend more time (perhaps up to 1 hour) experimenting with it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>The smoothed representation γ (σ ) (y) is a promising new direction for visualizing categorical sequences such as text documents or biological data. By varying the smoothing scale σ , γ (σ ) (y) interpolates between the original sequence γ (0) (y) or y and γ (∞) (y) or γ hist (y). The equivalence between the γ (σ ) (y) representation and smooth curves enables us to use techniques inspired by differential analysis and to use techniques invented in the statistical area of functional data analysis. In contrast to n-gram, γ (σ ) (y) captures topical trends and incorporates long range as well as position information at the required level of sequential detail. On the other hand, the novelty of our idea is orthogonal to n-gram as it is possible to combine the two by constructing smoothed representation over n-grams rather than the word set V .</p><p>We demonstrated a number of useful visualization techniques based on curve derivatives, principal component analysis, multi-dimensional scaling, phase-plane analysis, reduced resolution documents and multi-resolution analysis. Based on our experimentation with the interactive toolkit as well as a user study we conclude that the highly intuitive velocity curves and text folding provide valuable assistance for users in absorbing textual information. Less intuitive techniques provide useful assistance to trained individuals. Other individuals, however, may require some amount of training or guidance.</p><p>Viewing the text sequence next to salient visual cues as in <ref type="figure" target="#fig_13">Figure 8</ref>, users are able to comprehend the text and localize their attention faster and more accurately. By controlling the sequential resolution of the visual cues users can efficiently traverse the document much like a binary search over a tree structure. While unlocking the full potential of the visualization framework may take some experimentation and guidance, it is worth it in the long run. People spend a decent portion of their time comprehending text and effective sequential visualization tools have the potential to make this process quicker and more efficient.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>δ a,b def = 1 a = b 0 a = b . ( 2 )</head><label>2</label><figDesc>While conveniently representing categorical sequences as kdimensional numeric vectors, the histogram representation, also known as bag of words, completely ignores word ordering. For example, assuming V = {1,...,5} we have γ hist( 1, 4, 3, 1, 4) = γ hist ( 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Published 14</head><label>14</label><figDesc>September 2007.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 .</head><label>1</label><figDesc>The set P {1,2,3} ⊂ R 3 may be visualized as a surface in R 3 (left) or as a triangle in R 2 (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>(a) The curve</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(σ ) μ (z)] 2 = 1 − [γ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(σ ) μ (z)] 1 we can visualize the curve γ (σ ) (y) by graphing [γ (σ ) μ (y)] 1 as a function of μ/N.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 2 (</head><label>2</label><figDesc>a) illustrates these graphs for σ /N = 1/10 and σ /N = 2/10. In the second case, γ (σ ) (w) is a curve in the space of distributions P {1,2,3} which is a 2-D triangular subset of R 3 demonstrated in Figure 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 3 .</head><label>3</label><figDesc>Speed γ(σ )    μ (left, σ /N = 0.064) and 2D embedding using PCA (right, σ /N = 0.026) of the lowbow curve representing the three successive RCV1 stories of varying lengths as a function of μ/N. The words appearing to the right and bottom of the right panel correspond to the words whose projection on the principle components give the highest (or lowest) values providing a convenient visual summary for the user.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>(left). The intersections of the circle with the x and y axes represent extreme points of the kinetic and potential energy. The sum of both energies is constant when no external force is present, but the relative proportion of them varies depending on the position x(t) of the harmonic motion. The curve in Figure 5 (right) corresponds to a phase-plane diagram of the first and second derivative of γ (σ )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Phase-plane plot for simple harmonic motion such as a simple pendulum or a spring (left) and for γ (σ ) μ (y) representing two concatenated RCV1 stories (right). Documents mapped from lowbow curves generated with three different values of σ /N. Some words are capitalized in the graph for improved readability. (a) Color bar approach. (b) Text folding approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 7 .</head><label>7</label><figDesc>Two dimensional PCA projection of the lowbow curves representing two similar news stories (see Section 4.4 for more details) using a VARIMAX rotation of PCA (σ /N = 0.04).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 8 .</head><label>8</label><figDesc>Screen shot of the toolkit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Average user scores representing the benefit users received from different visualization techniques with respect to the three types of questions they were asked. A score of 5 indicates the feature was very useful while 1 indicates not at all.</figDesc><table><row><cell>score</cell><cell cols="4">PCA PCA components velocity text folding</cell></row><row><cell cols="2">task 1 1.60</cell><cell>1.63</cell><cell>4.40</cell><cell>3.07</cell></row><row><cell cols="2">task 2 1.63</cell><cell>1.60</cell><cell>1.9333</cell><cell>4.10</cell></row><row><cell cols="2">task 3 1.53</cell><cell>1.80</cell><cell>1.8667</cell><cell>3.40</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://in-spire.pnl.gov 2 http://jheer.org/enron,http://enron.trampolinesystems.com 3 http://www.refviz.com 4 http://www.cs.cmu.edu/∼lemur/science</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">http://today.reuters.com/news/articleinvesting.aspx ?type=hotStocksNews&amp;storyID=2007-03-28T004723Z 01 SP116282 RTRUKOC 0 US-MARKETS-OIL.xml</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">http://web.ics.purdue.edu/∼ymao/lowbow.htm</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>The authors wish to thank Jeff Bilmes for his interesting comments regarding the lowbow representation, and anonymous reviewers for their helpful suggestions on earlier drafts. This work was supported in part by NSF grant DMS-0604486.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Statistical models for text segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Beeferman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="177" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dynamic topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Third International Conference on Machine Learning</title>
		<meeting>the Twenty-Third International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Visualization of text document corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fortuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grobelnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mladenic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Informatica</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="497" to="502" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<title level="m">The Elements of Statistical Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Interactive visualization of multiple query results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Havre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hetzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Perrine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jurrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Information Visualization</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page">105</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Themeriver: Visualizing thematic changes in large document collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Havre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hetzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nowell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="20" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-paragraph segmentation of expository text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association of Computational Linguistics</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Texttiling: Segmenting text into multi-paragraph subtopic passages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="64" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamic query tools for time series data sets, timebox widgets for interactive exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hochheiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Visualization</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sequential document representations and simplicial curves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lebanon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 22nd Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>of the 22nd Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Local Regression and Likelihood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loader</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A Wavelet Tour of Signal Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Foundations of Statistical Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schutze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Topic islands -a wavelet based text visualization system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brewster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foote</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Visualization</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="189" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Functional Data Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ramsay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Silverman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Automatic analysis, theme generation, and summarization of machine-readable texts. Readings in Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="478" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic text decomposition using text segments and text themes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UK Conference on Hypertext</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="53" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Infocrystal: A visual tool for information retrieval &amp; management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Spoerri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information and Knowledge Management</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Summarizing scientific articles: experiments with relevance and rhetorical status</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Teufel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="409" to="445" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Illuminating the Path</title>
		<editor>J. J. Thomas and K. A. Cook</editor>
		<imprint>
			<date type="published" when="2005" />
			<publisher>IEEE Computer Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visualizing email content: portraying relationships from conversational histories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Golder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI conference on Human Factors in computing systems</title>
		<meeting>the SIGCHI conference on Human Factors in computing systems</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="979" to="988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visualizing time-series on spirals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alexa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Symposium on Information Visualization</title>
		<meeting>of IEEE Symposium on Information Visualization</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="7" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visualizing sequential patterns for text mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jurrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Symposium on Information Visualization</title>
		<meeting>of IEEE Symposium on Information Visualization</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
