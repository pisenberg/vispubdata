<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Interactive obstruction-free lensing for volumetric data visualization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Traoré</surname></persName>
							<email>traore.s.michael@enac.fr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Hurter</surname></persName>
							<email>christophe.hurter@enac.fr.</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Telea</surname></persName>
							<email>a.c.telea@rug.nl</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">are with ENAC</orgName>
								<orgName type="institution">French Civil Aviation University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Institute Johan Bernoulli</orgName>
								<orgName type="institution">University of Groningen</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Interactive obstruction-free lensing for volumetric data visualization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Interaction techniques</term>
					<term>focus + context</term>
					<term>volume visualization</term>
					<term>volume rendering</term>
					<term>raycasting</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Occlusion is an issue in volumetric visualization as it prevents direct visualization of the region of interest. While many techniques such as transfer functions, volume segmentation or view distortion have been developed to address this, there is still room for improvement to better support the understanding of objects&apos; vicinity. However, most existing Focus+Context fail to solve partial occlusion in datasets where the target and the occluder are very similar density-wise. For these reasons, we investigate a new technique which maintains the general structure of the investigated volumetric dataset while addressing occlusion issues. With our technique, the user interactively defines an area of interest where an occluded region or object is partially visible. Then our lens starts pushing at its border occluding objects, thus revealing hidden volumetric data. Next, the lens is modified with an extended field of view (fish-eye deformation) to better see the vicinity of the selected region. Finally, the user can freely explore the surroundings of the area under investigation within the lens. To provide real-time exploration, we implemented our lens using a GPU accelerated ray-casting framework to handle ray deformations, local lighting, and local viewpoint manipulation. We illustrate our technique with five application scenarios in baggage inspection, 3D fluid flow visualization, chest radiology, air traffic planning, and DTI fiber exploration.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Direct volume rendering (DVR) is a pervasive visualization technique for displaying 3D scalar fields with applications in engineering, material sciences, and medical imaging sciences. However widely adopted, and able to handle large datasets at interactive rates, DVR inherently suffers from the problem of occlusion: Structures of interest located deep in the volume, called next targets, can be hard to spot and/or explore.</p><p>To aid with this, various techniques have been designed including transfer functions, segmentation, selection, and clipping. Yet, all such techniques have limitations. Global mechanisms, like transfer function editing, can remove both occluders and targets if these have similar densities. In certain applications, carefully designed transfer functions exist and should be used without (significant) modifications to facilitate understanding and user training <ref type="bibr" target="#b50">[51]</ref>. Local mechanisms like segmentation, selection, or clipping are more effective in manipulating data confined to a given spatial region. Yet, many such mechanisms assume that one can easily and accurately select targets to remove them (occluders) or keep them (occluded). This is hard to do when e.g. one does not have direct access to the targets, or when significant 3D interaction is required to select occluder(s).</p><p>A different way to handle occlusion is to use lenses. These are flexible lightweight tools which enable local and temporary modifications of the DVR to reveal targets while keeping the global visualization context <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>. However, efficiently selecting the target and removing all in-between occluders is still challenging. More specifically, most existing occlusion management techniques do not simultaneously meet all following requirements: Rapidly create an unobstructed view of the target (R1), allow a flexible local exploration of the target zone (R2), keep the context in which the target is visually embedded (R3), and handle datasets where the target and occluders cannot be separated by transfer function manipulations (R4).</p><p>In this paper, we increase the flexibility of lenses for DVR exploration to jointly cover all above requirements. We propose a focus-andcontext (F+C) lens that combines a distortion technique, which pushes aside the occluding objects, with a fish-eye field of view, to provide a better perspective on targets. We specifically target the use-case of partially occluded objects, where the user has a glimpse of an interesting structure, buried deep in the data, but only slightly visible from a given viewpoint and transfer-function setting. We allow the user to 'open up' the volume without changing these settings, and reveal the target, by simple point, click, and scroll operations. Next, we provide several F+C modifications of the lighting parameters, transfer function, and geometry in the focus area to better understand the target. Our technique, implemented using a CUDA-based approach, can be easily incorporated in any generic DVR system.</p><p>The paper is structured as follows. Section 2 presents related work in occlusion management, lenses, and deformations for DVR visualization. Section 3 introduces the principle of our lens. Section 4 introduces implementation details. Section 5 presents five application scenarios for our lens in baggage inspection, 3D fluid flow visualization, chest radiology, air traffic planning, and DTI fiber exploration. Section 6 discusses our proposal. Finally, Section 7 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>To start with, let us refine requirements R1, . . . ,R4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R1:</head><p>The technique should rapidly create an unobstructed view of the target, i.e., such a view should be created at interactive frame rates (10 fps or more) with no special pre-processing of the input volume (e.g., segmentation), and with minimal user input (e.g., using simple mouse and/or keyboard-modifier events). All above are needed to ensure that one can freely explore the volume by activating the lens anywhere with minimal effort and seeing its effect in real-time. R2: Allowing a flexible exploration of the target zone means that one can manipulate the zone in the lens in various ways to see how the target is actually embedded in its surrounding context. R3: Keeping the context means that the visualization around the lens does not change significantly from what would be shown there if the lens were not activated. This is needed to maintain the user's mental map before vs after activating the lens. R4: The lens should enable the exploration of datasets where targets cannot be easily separated (isolated) from their surrounding context simply by manipulating parameters of the transfer functions (TF). One such issue is when targets and surrounding zones have similar densities; in this case, using a single global opacity TF could either render everything opaque (thus, it will be hard to visually isolate the target) or highly transparent (thus, the zone around the target will be transparent but so will be the target too).</p><p>Previous work on handling occlusion when exploring volume data can be divided into occlusion management techniques and lenses-anddeformation techniques. We next discuss these and also point out limitations from the perspective of R1, . . . ,R4.  <ref type="figure">Fig. 1</ref>. (a-c) A baggage scan is viewed from different angles. In view (c), a suspicious sharp object is spotted between a set of mugs. (d-f) Filtering densities using a classical 1D opacity transfer function removes progressively more of the occluders (mugs), but also the target. (g) The user applies the lens on the target object (double-click). An animation starts opening the lens, rays are gathered to pass through occluders. Halfway through the animation, the object is magnified, but only the area close to the lens is visible. (h) The fish-eye field of view at the end of the animation scatters rays to fully show the target. (i) The lens is increased to magnify the target (mouse scroll).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Occlusion management</head><p>Many occlusion management approaches have been proposed <ref type="bibr" target="#b12">[13]</ref>. Multiple viewports show the data from different perspectives <ref type="bibr" target="#b48">[49]</ref>. This does not help when the target is strongly occluded from all possible viewpoints (R4). Virtual X-ray methods make targets visible by turning occluders (half-)transparent <ref type="bibr" target="#b5">[6]</ref>. Kruger et al. <ref type="bibr" target="#b26">[27]</ref> proposed ClearView, which interactively focuses on specific areas in the data while preserving context without visual clutter by modulating transparency. Correa and Ma <ref type="bibr" target="#b9">[10]</ref> proposed visibility-driven transfer functions (TFs) to maximize the visibility of selected data-intervals. Designing good TFs is still challenging (R2) in general: For instance, in baggage inspection, a dissimulation strategy is to hide a threat among same-density objects, so TF editing cannot easily remove occluders but keep the target (R4) <ref type="bibr" target="#b45">[46]</ref>. De-occluding a tumor from surrounding similar-density tissue in medical scans is a similar situation <ref type="bibr" target="#b34">[35]</ref>. Rezk-Salama and Kolb <ref type="bibr" target="#b35">[36]</ref> also considered the voxels' occurrence on the cast ray, besides their densities and positions. Hurter et al. <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> removed occluders in 2D images, volumes, and trail-sets by deforming (pushing them away) in a focus area. Occludes are selected based on data value-ranges, thus we have here the same limitation as ClearView (R4). Li et al. <ref type="bibr" target="#b29">[30]</ref> proposed luggage virtual unpacking where targets are cleared by interactively moving away occluders. This, however, alters the context (relative position, connectivity) where occluders occur (R3). Recently, an interactive volumetric data exploration via direct voxel manipulation was proposed <ref type="bibr" target="#b45">[46]</ref>. Extending such approaches in a DVR setting to more complex deformations or changes of the data-in-focus is computationally challenging (R1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Lenses and deformations</head><p>An interactive lens is a lightweight tool to solve localized visualization problems by temporarily altering a selected part of the visual data representation <ref type="bibr" target="#b41">[42]</ref>, and hence provides focus-and-context (F+C) solutions to volumetric data occlusion (R2). Parametrizable lens properties include position, shape, appearance, size, orientation, and selection of included data (focus). The lens shape is usually chosen to meet the requirements of the application and is linked to the lens function. Most lenses are circular <ref type="bibr" target="#b40">[41]</ref> (a design we also choose), rectangular <ref type="bibr" target="#b25">[26]</ref>, or adapt their shape to the data-in-focus <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b38">39]</ref>. The lens position and size can be changed manually by the user, or automatically to guide users toward interesting events in the data <ref type="bibr" target="#b39">[40]</ref> or along a path of interesting events <ref type="bibr" target="#b0">[1]</ref>. Our lens updates automatically its properties once a target has been selected. This allows a smooth transition towards an unobstructed and magnified area of interest.</p><p>Lenses for DVR face spatial selection and occlusion challenges. Magic Lens <ref type="bibr" target="#b47">[48]</ref> addresses these by rendering occlusions with lower opacity and magnifying pre-computed features interactively or automatically in a pre-segmented dataset. This, however, does not provide an (interactive) way to deal with similar-density occluders and tar-gets (R4) and does not propose local exploration of the target context (R2). GlyphLens <ref type="bibr" target="#b44">[45]</ref> removes occluding glyphs by pulling them aside through animation. However, this covers only glyph-based and not DVR visualizations.</p><p>Lenses can create discontinuities between their inside and outside areas. Deformations can be a solution to this. Hsu et al. <ref type="bibr" target="#b16">[17]</ref> create flexible deformations by non-linearly sampled rays to smoothly project objects at multiple levels of detail. However, this is computationally expensive and far from real-time (R1). Exploded views are used to partition a volume into several segments <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b36">37]</ref>. This strongly reduces occlusion but distorts the context (R3) and requires some sort of data pre-segmentation which takes time to compute (R1). Correa et al. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> allow one to manipulate the geometry of a data object. McGuffin et al. <ref type="bibr" target="#b30">[31]</ref> performed deformations using peeling to see hidden parts of the data. Such techniques remove potentially important contextual information surrounding the target, which makes it hard to see how the target is embedded in its context (R2).</p><p>Deformations can reveal specific data features by using a precomputed segmentation. Tong et al. proposed a deforming lens which moves streamlines to observe the inner part of streamline bundles <ref type="bibr" target="#b43">[44]</ref>. Other techniques performed deformations using surgical metaphors <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24]</ref> to show hidden parts of a volume. Such techniques do not offer tools for local manipulation of the viewpoint that allows seeing a target under multiple perspectives (R2) while keeping the global context.</p><p>Using non-straight-line rays is another way to avoid occluders. Cui et al. <ref type="bibr" target="#b10">[11]</ref> propose curved (Bézier) rays to support this. Wu et al. <ref type="bibr" target="#b49">[50]</ref> refine this further to create multi-perspective views. These approaches require a quite careful ray-path planning in advance, so they are not directly aimed at a lens effect (R2). Also, the target should be accessible from the viewpoint via a (possibly curved) path (R4). Also, from the presented examples, although two examples of DVR models are given, it seems these techniques mainly address de-occlusion in large 3D polygonal scenes such as terrain and city models. <ref type="table">Table 1</ref> summarizes the main advantages and limitations of a set of volumetric exploration techniques selected from the ones mentioned above which are close to our proposal. As visible, none of the techniques scores high on all requirements. The table also lists the number of different datasets and/or use-cases these techniques were tested on. We will validate our proposal on a similar number of use-cases (Sec. 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Detailed contributions</head><p>Summarizing the above discussion on the requirements and related work on occlusion management, we propose a new technique which combines high-quality DVR with a fast, versatile, and easy to use, lens to support the interactive exploration of occluded data in volumes. In the classification of view deformations by Carpendale et al. <ref type="bibr" target="#b6">[7]</ref>, we use a nonlinear radial distortion through an interactive lens to remove occluding items and keep the global context while magnifying a partially occluded item. Related to volumetric lens techniques, we frame our contribution as follows: We propose an interactive deforming lens that magnifies and pushes aside occluding objects located in front of a designated focal point which meets the four requirements; the combination of flexible and real-time interactive changing of the focal point, custom bent rays used for DVR, lens deformation, and shading and transfer function in the focus area allow us to provide on the fly a range of perspectives of the targets, without having to change the viewpoint or manipulate complex parameters in multiple linked views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRINCIPLE</head><p>Consider the typical DVR algorithm: Given a scalar volume V ⊂ R 3 → R, each pixel x ∈ I in the DVR image I ⊂ R 2 thereof corresponds to the compositing of sampled data along a ray that passes through V and ends at x. In classical DVR ( <ref type="figure" target="#fig_2">Fig. 2-a)</ref>, such rays are defined by the eye position e and a ray direction unit vector d = (x − e)/ x − e . Consider now a focus point f ∈ I (the lens center) and a lens radius R &gt; 0. We modify all rays passing through the lens (focus/0 area D = {x ∈ I| x − f ≤ R} in order to de-occlude, magnify, and emphasize a target object. Our ray behavior is divided into three steps: (1) Provide a clear view of the target by moving closer to it and by pushing occluders aside. (2) Set a wide field-of-view (fisheye) to better see the target. (3) Modify the parameters of the lens, lighting, and opacity TF in real time to better explore the target. These steps are detailed next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Creating an unobstructed view</head><p>The scenario we address is as follows: Given a volume V , users produce a DVR thereof, using whatever suitable TFs and other parameters are applicable. When examining V from various viewpoints, (at least) one viewpoint (e, d) is found from which some intriguing structure is partially visible in I. We call this structure the target. Users next want to quickly and easily unravel the target. For this, we proceed as follows:</p><p>We first gather all rays passing through the lens pixels (focus area D) to follow the lens' axis vector a = (f − e)/ f − e . As explained above, at the location f of the lens center, we do see an interesting partially occluded target. Hence, by definition, the gathered rays pass through occluders to hit this target, otherwise we would not see it. We control gathering by setting the ray direction passing through x ∈ D to</p><formula xml:id="formula_0">r(x) = (1 − α)a + αd,<label>(1)</label></formula><p>with α ∈ [0, 1]. When α = 0 (default), all rays follow the lens axis a, thus, can best pass through obstacles. When α = 1, rays follow a straight path. Changing α with the mouse wheel smoothly navigates between the lens effect, i.e. opening up a 'hole' in the volume to see the target, and classical DVR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Setting a wide field of view</head><p>Once the rays pass obstacles (Sec. 3.1), we want to scatter them so as to best sample the target. Consider that this target is at some depth t target &gt; 0 within V . After the rays pass the occluders, but before they hit the target, i.e., travel past a distance t min &lt; t target through V , we deflect (scatter) them so as to best sample the target. For this, we set the parametric position of a ray point to</p><formula xml:id="formula_1">p(x,t) = r(x)t + β (x − f)(t − t min )<label>(2)</label></formula><p>for any pixel x ∈ D and any t ≥ t min . Here, β ≥ 0, adjusted via the mouse scroll wheel while pressing the Shift key ( <ref type="figure" target="#fig_2">Fig. 2-c</ref>), controls the ray scattering: Small values magnify a small volume area close to the ray r(x); larger values sample more of the volume behind the lens. Intuitively, this is as if we moved a magnifying lens to a depth t min inside V . Summarizing, after the user finds an interesting but partially occluded target using standard DVR, our lens squeezes rays to pass between occluders and next fans them out to explore the target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Interactive exploration of the target</head><p>To achieve a more effective exploration, we can interactively modify several parameters of the DVR and the lens, as follows.</p><p>Lens radius: The radius R, telling how big is the 'hole' to open up in the volume to see the target, is set via the mouse wheel. The parameters α and β (affecting the gathering and scattering of rays respectively) are set by the mouse wheel and modifier keys. The value t min (depth from which scattering starts) is set using the arrow keys.</p><p>Lens axis: Users can rotate the lens axis a using a virtual trackball activated by the right mouse button. Changing a effectively samples the target from many viewpoints, allowing the user to look 'around' it to see parts which are not visible from the current viewpoint, but without actually changing the viewpoint. This is of high added value, since changing the viewpoint can bring us to a view where the target is fully invisible, so we do not know where precisely to activate the lens anymore. <ref type="figure" target="#fig_4">Figure 4</ref> shows three such local rotations for the baggage dataset introduced in <ref type="figure">Fig. 1</ref>. From these, we see that the star-shaped target is relatively thick.</p><p>Lighting: We modify the volumetric Phong lighting parameters to better explore the target, as follows. Let c = e+t min a be a point at depth t min along the lens axis, and let B(c, R) be a sphere of radius R around this point <ref type="figure" target="#fig_2">(Fig. 2b)</ref>. We call voxels in this sphere 'in focus', and all other voxels in V 'out of focus'. Let φ be the specular term coefficient, set to a high value (default: one). First, for all voxels x ∈ B(c, R), we use a specular coefficient</p><formula xml:id="formula_2">φ (x) = φ (1 − d), where d = x − c /R.</formula><p>For all voxels outside B(c, R), we use φ (x) = 0. Hence, voxels close to the focus point c appear highly specular; further away from c, voxels get less specular, and voxels out of focus are purely diffuse. Secondly, we allow the user to locally rotate the light vector using the same trackball mechanism as for the lens axis rotation. Let l lens be this vector, and let l global be the global light vector used by standard DVR. For all voxels in focus, we use a light vector l(x) = (1 − d)l lens + dl global . As the user rotates l lens , the light direction will visibly change in the middle of the lens, stay constant outside it, and smoothly change in between. The above two mechanisms combined yield the effect of a moving flashlight turning around a shiny target embedded in a constantlylit diffuse scene. <ref type="figure" target="#fig_5">Figure 5</ref> shows these mechanisms for a chest CT dataset containing a deeply buried tumor (the dataset and use-case are described in detail in Sec. 5.3). We see how turning the light highlights small-scale details on the target surface (tumor) without changing the viewpoint or lens location. Moreover, the high specularity in the lens attracts the user's attention to this area; the diffuse lighting outside the lens put less emphasis on the context area.</p><p>Opacity: We modify the opacity transfer function along a similar idea as for lighting ( <ref type="figure" target="#fig_3">Fig. 3</ref>). Let T F global o : R → [0, 1] be the user-chosen opacity function used globally for the volume. Let Γ be a Gaussian pulse of unit height centered at the average density valueρ in B(c, R) and with standard deviation σ . We estimateρ and σ by considering the density ρ at 150 points randomly sampled inside B(c, R). Higher values for the sample count yield visually very similar results for our tested volumes of up to 500 <ref type="bibr" target="#b2">3</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T F global o</head><p>to make most out-of-lens voxels relatively transparent. In that case, T F o will still make voxels in B opaque, thus allowing to see the in-focus structures better. <ref type="figure" target="#fig_5">Figure 5</ref> has been generated this way. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Smooth transitions</head><p>If we bend rays passing through the lens pixels D (Eqns. 1 and 2) and trace rays starting at pixels in I \ D as straight lines, discontinuities appear at the lens borders. We solve this as follows. Let p(x,t) be the voxels along a lens ray starting at screen pixel x, as computed by Eqn. 2. Let p line (x,t) be the voxels along a straight-line ray starting at x, i.e., computed using α = 1 and β = 0 in Eqns. 1 and 2 respectively. For every value t along every such ray, we compute the interpolated raȳ</p><formula xml:id="formula_3">p(x,t) = (1 − f (d))p(x,t) + f (d)p line (x,t),</formula><p>where d is the distance of x to the lens axis (normalized to unit by dividing it by R) and f :</p><formula xml:id="formula_4">[0, 1] → [0, 1]</formula><p>is an interpolation function. Next, we use the rays p(x,t) to compute the DVR by standard composition. This way, rays effectively vary smoothly from their bent versions (close to the lens axis) to straight lines (outside the lens). Setting f (d) = d 2 keeps the interpolation transitions close to the lens border, so most of the lens is dedicated to show the desired fisheye effect.</p><p>Separately, we use a slow-in/slow-out animation <ref type="bibr" target="#b11">[12]</ref> to introduce the lens effect. When activating the lens, we vary α and β from their defaults (α = 1, β = 0, i.e. straight-line classical DVR) to their actual user-set values, compute the volume rendering on-the-fly, and display the resulting images. The effect resembles gradually opening a hole in the volume -see the associated video. The speed increase at the start of the animation helps one to quickly see what is revealed in the lens; the decreasing speed at the end helps seeing where the pushed-away occluders actually go. This also gives some semantic to the moving shapes, allowing one to interpret the motion as a magnification of a target, and to keep the focus on visual entities during this transition. When deactivating the lens, we play back the animation in the opposite sense, which suggests closing the opened hole in the volume.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">IMPLEMENTATION</head><p>We implemented our occlusion-free lens by modifying a standard DVR ray caster, publicly available in NVIDIA CUDA's SDK <ref type="bibr" target="#b32">[33]</ref>. We modified this ray caster to incorporate the new ray definition (Eqns. 1 and 2), the lens effect, and the local per-voxel Phong lighting parameters, all controlled via keyboard and mouse. On a PC with 16 GB RAM and a GeForce GTX TITAN X card, we achieve 15 frames per second for volumes up to 512 3 voxels at a 1900 × 1200 pixels screen resolution. All in all, adding our lens to an existing ray caster should pose no significant implementation problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">APPLICATION SCENARIOS</head><p>We next demonstrate our obstruction-free lens via five use-cases considering scalar density volumes from baggage inspection, 3D flow simulation, radiology, air traffic planning, and diffusion tensor imaging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Baggage inspection: An unusual blunt object</head><p>In airports, security agents deal with volumetric data exploration during baggage inspections. While automatic systems can detect densities of harmful substances such as C-4, TNT, and nitroglycerin, or prohibited articles (threats) like classical firearms and knives, unusual threats are hard to find. Four main concealment strategies exist <ref type="bibr" target="#b45">[46]</ref>:</p><p>Superposition: A threat may be sheltered among dense materials. While possible to see through such a 'shield' using high penetration (enhanced X-ray power) or image processing (contrast improvement), such techniques are not universally available and also require finetuning many parameters, which slows down inspection.</p><p>Location: Objects located in the corners, edges, or in the luggage's frame are very hard to spot.</p><p>Dissociation: One can conceal a threat by spreading its parts in the luggage, e.g, by disassembling a weapon and scattering its parts.</p><p>Lure: A minor threat (lure) like small scissors is clearly visible and catch the security agent's attention who can miss the real threat. Baggage labeled as suspicious by human inspection or automated scan heuristics must be checked by human agents. Besides timeconsuming physical unpacking, one can use 'virtual unpacking' tools that segment the 3D scan by a density-based confidence measure and next move the segmented objects away by animation to reduce occlusion <ref type="bibr" target="#b29">[30]</ref>. Such systems have been patented and used in production <ref type="bibr" target="#b28">[29]</ref>. However, when the automatic segmentation is not optimal, the user must manually change its parameters, repeat the segmentation and animation, which goes back to being time-consuming.</p><p>Consider the baggage scan in <ref type="figure">Fig. 1</ref> (283 × 189 × 344 voxels, dataset obtained from an actual airport scan). Automatic baggage inspection systems will not detect anything suspect here. However, while visually exploring this baggage from different angles <ref type="figure">(Fig. 1a-c)</ref>, we see an object hidden between a set of mugs. To reduce occlusion, a common solution in baggage inspection is to filter materials by density in order to show or hide subsets of the volume. However, for our dataset, the suspect target has almost the same density as the surrounding mugs, so removing the latter also removes the target <ref type="figure">(Fig. 1d-f</ref>). Using the obstruction-free fish-eye lens helps here: Clicking on the sharp detail visible in <ref type="figure">Fig. 1c</ref> first gathers rays so they pass through the low-density zone between the mugs <ref type="figure">(Fig. 1f)</ref>. The animation that opens the lens <ref type="figure">(Fig. 1e-g</ref>) reveals an unobstructed view of the target. However, this shows only a small part of the target. Scattering rays next fully reveals the target <ref type="figure">(Fig. 1h)</ref>. Adjusting the lens size shows a more detailed view of the target <ref type="figure">(Fig. 1i)</ref>. Next, locally turning the viewpoint around the target <ref type="figure" target="#fig_4">(Fig. 4)</ref> allows the agent to decide that the target is a shuriken (Japanese ninja star weapon). Since the object is very thick and blunt (see <ref type="figure" target="#fig_4">Fig. 4</ref>), it is not an actual weapon, thus not a threat.</p><p>We evaluated our lens for this use-case by a user study. Eight airport security specialists were recruited (ages 23 to 43; experience in baggage scanning 8 months to 20 years, average familiarity with 3D tools from no experience to expert, see <ref type="figure" target="#fig_7">Fig. 6</ref>). All attended a 20-minute global demo of the lens operation. Next, they were given each a personal training session for using the tool (5 minutes), in which they were instructed on the mouse and keyboard controls. After this, they were asked to work in pairs to examine the above-mentioned baggage CT dataset to form a decision on the nature of the ninja star possible threat (20 minutes of tool usage per person, after which the pair was changed). The idea behind this is that one person operates the tool while the other poses questions or suggest explorations, much like typical airport security operators work with a scanner. In the end, they all separately filled in a web questionnaire covering several questions and also provided open feedback (questionnaire available in the supplementary material). <ref type="figure" target="#fig_7">Figure. 6</ref> shows the answers. The first question-set (S1) regarded how easy-to-use, generally effective, and effective vs other known tools our lens is for untargeted inspection, i.e., when no suspect target is partially visible. Here and next, other tools denote classical 2D X-ray or 3D CT scans used in baggage scanning that the subjects know. As <ref type="figure" target="#fig_7">Fig. 6c-e</ref> shows, the answers (on a 5-point Likert scale) were predominantly positive: The tool is easy to use, is useful, and is actually more useful than known tools for untargeted exploration. The second question-set (S2) regarded how good our tool is to examine specific targets which are partially visible. Here again, the answers were predominantly positive ( <ref type="figure" target="#fig_7">Fig. 6f-h</ref>). The main appreciated features of our tool are listed in <ref type="figure" target="#fig_7">Fig. 6i</ref>.</p><p>We next summarize the received open feedback. According to the subjects, our tool can provide them a better perception of the items inside the baggage as compared to the classical 2D single-viewpoint X-ray machinery they routinely use. Quotes from the open feedback: "clear added-value compared to all systems I know"; "this tool is a real gain for examining luggage with uniform and/or high densities"; "definitely better than known tools for examining threats I am not familiar with / I have not seen before". However, our tool should not be used for the typical carry-on baggage inspection which has a very small allowed inspection time (15 to 20 seconds). Our tool is much more interesting for inspecting checked-in baggage, where inspection time-windows are up to 3 minutes. The perceived added value for this use-case is also higher: Opening up checked-in baggage for manual inspection is much more complicated and time-consuming than for carry-on baggage. Moreover, the only system for inspecting checked-in baggage that the subjects knew of is a scanner that aims to automatically detect threats via X-ray imagery; this system suffers from false positives, so a manual examination tool like ours could quickly eliminate such false positives, and thus the delays of opening up checked-in baggage. Finally, several subjects suggested that adding a function to display a classical 2D slice view (activated by a key press and aligned with the focal point) would be useful since this would show additional detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Fluid flow: A deep-buried spherical vortex</head><p>Flow visualization using streamlines has a long history <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b31">32]</ref>. For 3D datasets, a key challenge is to balance the streamline density. Low values allow seeing inner regions in the data but can subsample (miss) patterns. High values show more data but create too much occlusion. We next show how our lens can be used to alleviate problems in the latter case. The dataset <ref type="bibr" target="#b15">[16]</ref> captures the simulation of water flow in a basin computed on a grid of 128 × 85 × 42 cells using 4595 streamlines with 183K sample points traced by pseudo-random seeding. We convert this set of 3D curves (polylines) to a scalar volume by using GPUaccelerated kernel density estimation (KDE) <ref type="bibr" target="#b27">[28]</ref>. Similar techniques have been used to compute density maps of 2D trail-sets <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b46">47]</ref>.</p><p>We first explore the density volume (500 3 voxels) using standard DVR <ref type="figure" target="#fig_8">(Fig. 7)</ref>. Note that, given KDE's smoothing effect, streamlines appear as finite-thickness tubes rather than pixel-thin curves. After turning the viewpoint a bit, we notice a dense spherical item deep in the data <ref type="figure" target="#fig_8">(Fig. 7a)</ref>. To see its shape better, we increase opacity; however, this immediately increases occlusion so the item becomes invisible.   Conversely, decreasing opacity to reduce occlusion makes the item almost transparent. Our lens solves the problem: In the initial view <ref type="figure" target="#fig_8">(Fig. 7a)</ref>, we point at the target and turn on the lens. This pushes away the occluding stream bundles, and shows that our item is a set of densely-packed, low-speed, tightly-turning streamlines that create a ball-like vortex <ref type="figure" target="#fig_8">(Fig. 7b</ref>). To make sure our target is spherical, we view it in the lens from different directions, by interactively changing the ray directions in the lens <ref type="figure" target="#fig_8">(Fig. 7c)</ref>. Finally, we can close the lens but keep the target magnified <ref type="figure" target="#fig_8">(Fig. 7d)</ref>. Finding the details of this vortex cannot be done using standard DVR. Interestingly, this vortex has also not been discovered by any of the visualization techniques that used this dataset (according to our knowledge) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b37">38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Chest scan: A hard to see tumor</head><p>In our third use-case, we consider a contrast chest CT scan (512×512× 110 voxels) of an elderly patient with a sizeable lung tumor. The tumor was detected in a CT scan performed after the patient reported acute chest pain. Typical examination of these scans by the pulmonologist and radiologist in charge involves slice-based views. <ref type="figure" target="#fig_9">Figures 8a-c and 8d-f</ref> show two such slice sets (axial, coronal, and sagittal views), produced using typical lung, respectively mediastinal, contrast presets. Although the tumor is visible in all these views, its exact shape, morphology, and connection to the lung walls are hard to assess. Finding such details on the tumor is essential, explained the doctors in charge, to determine the TNM score <ref type="bibr" target="#b3">[4]</ref> and also plan treatment. Using standard DVR makes the tumor and its 3D position partially visible <ref type="figure" target="#fig_9">(Fig. 8f</ref>). Yet, occlusion from the rib cage and other tissues is still present. Using both TF presets and manually changing the TFs in the 3D Slicer tool <ref type="bibr" target="#b24">[25]</ref> used to create the DVR could not help de-occluding the tumor without making it partly transparent. The slice images in <ref type="figure" target="#fig_9">Fig. 8a</ref>-f confirm this by showing that the gray values for the tumor and surrounding skin-and-muscle tissue are very similar. This is due to the fact that the tumor had grown rapidly and started necrotizing, which filled it with fluids, making its density very similar to that of the obstructing (skin and muscle) tissue, explained the pulmonologist. Hence, one cannot remove such occluding tissue in a classical DVR setting by opacity TF manipulation without also removing the tumor. This makes examining this specific tumor harder than for regular cases.</p><p>We next used our lens to examine the tumor. <ref type="figure" target="#fig_5">Fig. 5</ref> shows several sample snapshots. We see that the tumor is significantly more visible when using the lens than when using standard DVR <ref type="figure" target="#fig_9">(Fig. 8d)</ref>, both in terms of removing the occluding tissue and in terms of the tumor's opacity -compare the inset in <ref type="figure" target="#fig_9">Fig. 8d</ref> with the images in <ref type="figure" target="#fig_5">Fig. 5</ref>. Secondly, relighting the tumor from various directions allows one to see small-scale morphological details such as the tumor's surface shape and its connection via protuberances and veins with the lung walls.</p><p>We asked the two medical specialists (pulmonologist and radiologist) in charge to state the potential advantages and/or limitations of our lens as compared to standard slicing and DVR techniques, after a 20-minute usage of the tool. Both specialists have over 10 years of medical experience in treating lung cancer, and routinely use several slicing and DVR tools. They work in a private hospital in Belgium and are not actively associated with medical imaging research. Our identities were hidden from them during the lens evaluation. The provided input can be summarized as follows: The occlusion-free lens is definitely easier and faster to use than classical DVR and/or slicing techniques. It is especially more effective than these to get a quick, first impression of a deep buried anatomical detail. Changing the lens' parameters by direct interaction is as simple as changing window/level functions in a typical slice-based tool, and is definitely simpler than tuning typical DVR parameters to obtain similar results. This 'entices' the user to explore, which is a good aspect. The fact that the lens minimizes viewpoint change (volume rotation), i.e., after a suitable viewpoint was found from which a (small) part of the target is visible, one doesn't need to change this viewpoint, is a strong feature, as 3D viewpoint changes are disruptive and cost time. This is important in a cost-aware environment where specialists have very limited time (about 20 minutes) to assess a CT scan. However, the lens should not replace classical slice-based exploration, which shows small-scale details better. In the context of the current dataset <ref type="figure" target="#fig_9">(Fig. 8)</ref>, the lens was useful to both confirm the TNM score (T3 grade tumor, 6.5 cm in size) found via the 2D slices, but much more so for understanding how and where the tumor is connected to surrounding tissue, which is very hard to do using only 2D slices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Aircraft trajectories: Outliers in the French sky</head><p>We next consider a task from air traffic planning -detecting and studying outliers in large-scale datasets containing tens of thousands of 3D (latitude, longitude, height) trails of aircraft over a given spatiotemporal region <ref type="bibr" target="#b18">[19]</ref>. Such datasets are typically viewed using 2D (latitude, longitude) plots where opacity encodes the spatial density of flights -see <ref type="figure" target="#fig_10">Fig. 9a</ref>, which shows one day of recorded aircraft trajectories over the French airspace. <ref type="figure" target="#fig_10">Fig. 9(b)</ref> shows a detail zoom-in, where we can see an abnormal -that is, far from straight or slightly curved -aircraft trail: A tanker aircraft performed an eight-shaped loop as it was waiting to refuel other aircraft. Revealing such patterns using 2D techniques, e.g. <ref type="bibr" target="#b22">[23]</ref>, is very hard. In particular, it is hard to de-occlude these patterns from the overall context of criss-crossing aircraft trails, even when one knows their 2D spatial location.</p><p>Our lens can help with this task, as follows. We first convert the set of 3D trails to a 500 3 density volume, using KDE as for the streamline use-case (Sec. 5.2). Examining this volume via standard DVR shows an outlier trail at some point in space, see curved patterns in <ref type="figure">Fig. 10a</ref>. Activating the lens on this area and interactively tuning the target depth t min (since we don't know the trail's height) beings the outlier trajectory in focus and pushes away occluding trails <ref type="figure">(Fig. 10a)</ref>. Like in the other examples presented so far, we can quickly change the magnification factor and view direction to better study this trail in context ( <ref type="figure">Fig. 10bd)</ref>. From these images, we easily see that the outlier trail has an eight shape. Revealing this outlier trail using standard 2D visualization techniques <ref type="bibr" target="#b22">[23]</ref> costs several minutes. Doing the same using our lens costs under one minute. Also, comparing Figs. 9b and 10b-d, we argue that the eight-shape of the outlier trail is much more prominent, and thus recognizable, in the latter images (made using our lens) than in the former ones. Last but not least, the 3D DVR approach that our lens enhances explicitly encodes flight height information, so our lens can use it by interactively tuning the depth value t min where the lens is focused. This cannot be done with 2D techniques which ignore the depth dimension.</p><p>We validated our findings with an air traffic data scientist with more than 10 years of experience in air traffic control and planning. She confirmed that this specific eight-shape trail in <ref type="figure" target="#fig_10">Fig. 9(b)</ref> is an actual aircraft which performed waiting loops and acted as a fuel supplier for military aircraft. Other comments included the following: Compared to standard 2D visualization techniques, our tool makes detecting outliers easy since there is no need for complex manipulation to reveal such outlier trails. Also, the user does not have to deal with color and alpha mapping parameter-tuning to make specific outliers emerge. Separately, trail visualization easily creates many occlusions leading to either fully opaque areas or too much local overlap, which both hinder seeing and examining specific trails. Our lens does help such cases by distorting the space to locally remove such occlusions. All in all, in the studied dataset ( <ref type="figure" target="#fig_10">Fig. 9)</ref>, the lens was specifically useful since, for high transparency, one would not detect the outlier trail, while for low transparency, one would get a hint of the outlier's existence, but not see it in detail due to too much occlusion; the lens allows using low transparency, but removes the clutter caused by it to reveal the outlier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Brain fibers: Uncluttering the bridge</head><p>Our last use-case considers the exploration of fiber tracts visualized as streamlines of the major eigenvector of a diffusion tensor imaging (DTI) field. Such datasets have a spatially complex structure which makes them hard to explore <ref type="bibr" target="#b1">[2]</ref>. In particular, fiber tracts are spread volumetrically over the entire extent of the brain, and create tangled patterns inside which it is hard to see much. DVR techniques are often used to render such tracts, one of the advantages being that close fibers get visually 'merged' to reveal spatially coherent structures, an effect which is not possible when fibers are rendered as polylines. However, DVR methods also create more occlusion, thus difficulties in seeing structures deep within the volume.</p><p>We consider an 128×128×51 DTI volume (same dataset as in <ref type="bibr" target="#b13">[14]</ref>). We traced 150352 fibers seeded in, and going over, regions of high fractional anisotropy in this volume. We filtered out fibers shorter than 2mm, yielding a total of 120593 fibers to display (6.4M sample points). Next, we converted this fiber-set to a 512 3 density volume, using KDE with a 3D isotropic kernel of radius 15 voxels, like for the streamline use-case (Sec. 5.2). <ref type="figure">Figure 11a</ref> shows the result, rendered with DVR, with opacity function mapping the fiber density. While terminal fibers are well visible, we cannot see anything inside the volume. Activating the lens in the middle of the volume opens a hole through which a small part of the corpus callosum, the fiber bundle wrapping the bridge that connects the two hemispheres, becomes visible. By slightly decreasing opacity <ref type="figure">(Fig. 11c)</ref>, the corpus callosum gets clearly visible, appearing as a compact structure, due to the KDE blending of neighbor fibers. Annotations are manually added by the examiner to delineate the tumor location. Images constructed using the 3D Slicer tool <ref type="bibr" target="#b24">[25]</ref>.</p><p>Obtaining such a view of the corpus callosum only using DVR would be very hard, since transfer functions would either render separated (non-merged) fibers, or else make the fibers surrounding the structure of interest too thick and occluding. This scenario has the main difference compared to all previous ones. In all earlier cases, the standard DVR of the data (that is, without the lens) showed us a partial small cue of the structure of interest within the volume, and we used the screen-space location of this structure as the focus point where to activate the lens. In this last scenario, there is no point in the original DVR image <ref type="figure">(Fig. 11a</ref>) from which the corpus callosum is even partially visible, due to the high opacity given by the used transfer function. Hence, the user can activate the lens at any desired point to peek inside, and towards the center of, the volume. Given the nature of the data, the structure of interest is quite easily visible from most such viewpoints (see lens inset in <ref type="figure">Fig. 11b</ref>). Once its presence is revealed, the user can next adjust the viewpoint and/or the opacity transfer function to get an optimal view on the target, such as the one shown in <ref type="figure">Fig. 11c</ref>. Summarizing, we can use our lens also in cases when no partial view of a target is available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>Several points of our lens proposal are worth discussing, as follows.</p><p>Lens activation: Our lens can support two types of explorations. First, when the user perceives a part of a target of interest in a classical DVR image, the lens can be used to reveal the target in full detail. This directed exploration supports the task 'show me more information about this item'. The use-cases in Secs. 5.1-5.4 are of this type. Secondly, the user can open up a DVR volume at a 2D location from which no partial detail is visible. This is useful when we know that there is an interesting target buried in the volume even without seeing it (corpus callosum use-case in Sec. 5.5), thus supports the task 'show me the data I know it is somewhere in there', or for free exploration to find unknown patterns in a volume, i.e. for the task 'show me what this volume may hide in it'. In the first exploration type (target not fully occluded), our lens is simple and rapid to use -point, click, and optionally rotate light or viewpoint. In the second exploration type (target fully occluded or not even sure whether an interesting target exists in the data), the lens is equally simple to use, but several tries to select a suitable focus point and lens depth are needed.</p><p>Lens shape: Occluders are pushed away, and deformed, isotropically (Secs. 3.2, 3.4). This simple lens model requires a single parameter, the lens radius R, which makes its usage easy. The deformations evolve smoothly from the lens center (maximal) to outside the lens (no deformation), see Sec. 3.4, which effectively blends the local (in lens) focus with the global (out of lens) context (R3). However, this strongly compresses the deformed occluders close to the lens border, making them hardly visible when the lens is fully active. A possible refinement would be to reduce the deformation of the pushed-away occluders while still pushing them away, thereby improving the F+C effect (R3). However, this would occlude areas outside the lens, basically moving occlusion from inside the lens to outside and close to it. Finding an optimal balance between minimal deformation (so one can recognize the pushed-away occluders) and minimal clutter (so these occluders do not destroy the lens context) is a topic for future work. Separately, deformed rays may intersect with straight rays, thereby sampling the same voxel(s) to different image pixels. We did not observe in our usage any artifacts that can be ascribed to this issue, nor did the other users of our tool. This can be explained by the fact that such ray intersections are relatively few and we use a compositing transfer function, akin to a low-pass filter.</p><p>Parameter setting: Our lens depends on several parameters: the 2D lens center f, lens radius R, lens axis direction a, local light direction l lens , scattering start-distance t min , and gathering and scattering parameters α and β . All these parameters are controlled via a mouse-driven virtual trackball, key modifiers, and the arrow keys (Sec. 3). As the lens works at 15 frames per second, the user can quickly tune the parameters and see their effect (R1). Moreover, all parameters start with good preset values (Sec. 3). A possible refinement would be to pre-segment the target, based on user-given values for f, R, and t min , thereby determining β automatically. However, we believe that manual control of the scattering β is important to allow users to choose their most suitable field-of-view angle. In fact, this flexibility allows a better exploration of the local context (R2).</p><p>Implementation: We implement our lens by modifying the ray trajectories constructed in the inner loop (per-pixel raycasting) of a public DVR raycaster <ref type="bibr" target="#b32">[33]</ref>. Apart from this, we change the per-voxel lighting and transfer function based on the voxel location in the lens and the parameters given by user interaction (Sec. 3.3). Such changes are limited and easily applicable to any (parallel) raycaster. Limitations: As explained, de-occluding a target requires either a small fragment thereof to be visible (if so, de-occlusion is very simple and fast), or requires the user to choose the lens focus and target depth based on other insights (which, as explained, requires more trial-anderror). At a higher level, many lens mechanisms exist in the literature, as discussed in Sec. 2. While we have argued that, to our knowledge, none of them simultaneously supports requirements R1,. . .,R4, comparing such mechanisms with our lens for specific use-cases and datasets is an important test for the end-to-end effectiveness of our proposal. We have not covered this point as obtaining (or replicating) implementations of such lenses is very challenging. This remains an important open point for future work -both for our proposal but also for all other volumetric lens proposals in the literature. In particular, none of the techniques in Tab. 1 were compared side-by-side against other techniques. We have performed three user evaluations involving specialists in airport baggage security (Sec. 5.1), pulmonology (Sec. 5.3), and air traffic control (Sec. 5.4). In all cases, users were not involved in this work, nor with other work of the authors. However, the set-up of these evaluations stays at the level of formative user experiments. To confirm and refine the obtained (positive) findings, more formal user studies are needed, which we plan to cover next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>In this paper, we presented a new fish-eye-like context-and-focus lens that addresses the occlusion problems inherent in scalar volume rendering. The principle of our lens consists in first gathering (squeezing) rays so that they easily pass through occluding densities (given a userspecified opacity transfer function) and next scattering (fanning out) rays to best sample the target of interest. Our lens can be directly applied to any DVR raycaster and scalar volume dataset. Its main constraint is that the user should be able to find a viewpoint from which the target of interest, deep buried in the data, is at least slightly visible. We also present several modifications of the local rendering parameters within the lens (view direction, lighting parameters, opacity transfer function) that aim to both better separate the focus (lens) from the context (volume) and also allow more detailed examining of the target. Our lens is easy to use -all its parameters are controlled via direct mouse-and-keyboard interaction -and can be efficiently implemented atop of a standard GPU ray caster. Our lens is especially useful for highlighting structures of interest which are both deeply embedded in volumetric data and cannot be revealed by standard transfer function manipulations due to similar densities in the occluders and target. We demonstrate these points using five use-cases involving datasets from baggage detection, fluid visualization, air traffic control, and chest radiology, and DTI fiber tracts. Several improvements to our proposal are possible, as follows. First and foremost, heuristics can be sought to link all our free parameters (lens size, focus depth, interpolation between focus and context) directly to the volume data, so the user interaction is minimized and therefore exploration efficiency is increased. Secondly, our lens could be extended to different types of volumetric datasets, such as multivariate (vector, tensor) fields. Last but not least, a formal wider-scale evaluation of how the lens addresses more specific tasks, and how it compares to existing tools for these tasks, such as other lens types, is a goal we aim to pursue next.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>voxels, while requiring (slightly) more computation costs. Then, for voxels in B(c, R), we use an effective opacity transfer function T F o = T F global o +(1−d)Γ. For voxels outside B, we use T F global o(standard DVR). This is useful when the user sets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>small part of the target... Lens on, rays pass through occluders... Obstruction-free lens working. A target is (mostly) hidden by occluders in front of it. (a) Classic DVR shows a small part of the target. (b) Our lens gathers rays to avoid occluders (Sec. 3.1). Once close to the target, rays follow again their initial paths. Yet, only a small part of the target is visible. (c) Scattering rays makes the full target visible (Sec. 3.2). Finally, we adjust the local viewing and lighting directions a, l lens (Sec. 3.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Construction of local transfer function T F o . See Sec. 3.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Performing local rotations in the lens improves visibility of the shape and the thickness of the partially occluded target object (ninja star).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Changing lighting parameters in the lens. (a) Constant global specular coefficient. (b) Specular coefficient high in the lens and low outside. (c-f) Changing the in-lens light vector yields the effect of a flashlight rotating around the target. The ball icons illustrate the local light vector direction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Evaluation of lens-based baggage inspection (Sec. 5.1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Flow volume exploration with two different opacity transfer functions (top and bottom rows). In viewpoint (a), we notice a small high-density spherical item. (b) We apply the lens at that location (double click). (c) The directions of rays in the lens are changed to see the whole target in the lens (right click + mouse drag change direction). (d) The lens is gradually closed while keeping the focus area magnified (shift + scroll).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Lung tumor visualization using slices (a-c) and standard DVR (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 .</head><label>9</label><figDesc>Visualizing one day of aircraft trajectories over France<ref type="bibr" target="#b22">[23]</ref>. (a) Overview of all trails. (b) Zoom, filtering, and color mapping techniques are used to highlight an outlier trajectory of an aircraft performing an eight-shaped loop. Revealing this outlier costs significant user effort.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 .Fig. 11 .</head><label>1011</label><figDesc>Inspecting an abnormal aircraft trail. (a) The abnormal trail is spotted in an all-trails view as it is highly curved while all other trails are relatively straight. Activating the lens at the outlier location (b) and changing the magnification factor (c) reveals the trail's eight-shape. (d) Rotating the viewpoint provides spatial insight on the embedding of the outlier in the surrounding trails. Revealing the corpus callosum in a DVR of a set of DTI tracts. (a) The corpus callosum is not even partially visible in the original DVR image. (b) The structure of interest is quite easily visible from most such viewpoints. (c) The viewpoint and/or the opacity transfer function can be adjusted to get an optimal view on the target.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors acknowledge the support of the French National Agency for Research (Agence Nationale de la Recherche ANR) under the grant ANR-14-CE24-0006-01 project TERANOVA and the SESAR Research and Innovation Action Horizon 2020 under project MOTO (The embodied reMOte TOwer). We also thank dr. Van de Perre and dr. Brozici from the pulmonology and radiology departments at the Heilig Hart Ziekenhuis, Mol, Belgium for their invaluable help with the chest tumor use-case described in Sec. 5.3.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">RouteLens: Easy route following for map applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alvina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Appert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pietriga</surname></persName>
		</author>
		<idno>doi: 10. 1145/2598153.2598200</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Working Conference on Advanced Visual Interfaces (AVI)</title>
		<meeting>Intl. Working Conference on Advanced Visual Interfaces (AVI)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="125" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Diffusion tensor imaging (DTI)-based white matter mapping in brain research: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Assaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pasternak</surname></persName>
		</author>
		<idno type="DOI">10.1007/s12031-007-0029-0</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Molecular Neuroscience</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="61" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Illustrative flow visualization: State of the art, trends and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brambilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Carnecky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Peikert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hauser</surname></persName>
		</author>
		<idno type="DOI">10.2312/conf/EG2012/stars/075-094</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. EuroVis -State of the Art Reports</title>
		<meeting>EuroVis -State of the Art Reports</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">TNM classification of malignant tumours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brierley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gospodarowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wittekind</surname></persName>
		</author>
		<idno type="DOI">10.1016/S1470-2045</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Wiley-Blackwell</publisher>
			<biblScope unit="page" from="30438" to="30440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploded views for volume data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bruckner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Gröller</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2006.140</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1077" to="1084" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive cutaways for comprehensible rendering of polygonal scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<idno type="DOI">10.1145/1457515.1409107</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH Asia)</title>
		<meeting>SIGGRAPH Asia)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Extending distortion viewing from 2D to 3D</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S T</forename><surname>Carpendale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Cowperthwaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>Fracchia</surname></persName>
		</author>
		<idno type="DOI">10.1109/38.595268</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="42" to="51" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Feature aligned volume manipulation for illustration and visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Correa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<idno>doi: 10.1109</idno>
	</analytic>
	<monogr>
		<title level="m">/ Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">144</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Illustrative deformation for data exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Correa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<idno>doi: 10.1109</idno>
	</analytic>
	<monogr>
		<title level="m">/Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2007-11" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">70565</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Visibility histograms and visibility-driven transfer functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Correa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Ma</surname></persName>
		</author>
		<idno>doi: 10.1109</idno>
	</analytic>
	<monogr>
		<title level="m">/Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A curved ray camera for handling occlusions through continuous multiperspective visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hoffmann</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2010.127</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1235" to="1242" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Temporal distortion for animated transitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dragicevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bezerianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-D</forename><surname>Fekete</surname></persName>
		</author>
		<idno type="DOI">10.1145/1978942.1979233</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI &apos;11</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems, CHI &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2009" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A taxonomy of 3D occlusion management for visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tsigas</surname></persName>
		</author>
		<idno>doi: 10.1109</idno>
	</analytic>
	<monogr>
		<title level="m">/Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">59</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploration of the brain&apos;s white matter structure through visual abstraction and multi-scale local fiber tract contraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Everts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Begue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bekker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B T M</forename><surname>Roerdink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Isenberg</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2015.2403323</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="808" to="821" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Depth-dependent halos: Illustrative rendering of dense line data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Everts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bekker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Roerdink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Isenberg</surname></persName>
		</author>
		<idno>doi: 10. 1109/TVCG.2009.138</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1299" to="1306" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Flow field clustering via algebraic multigrid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Griebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Preusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rumpf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Telea</surname></persName>
		</author>
		<idno type="DOI">10.1109/VISUAL.2004.32</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Visualization</title>
		<meeting>IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="35" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A rendering framework for multiscale views of 3D models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Correa</surname></persName>
		</author>
		<idno type="DOI">10.1145/2070781.2024165</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2011-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image-based visualization: Interactive multidimensional data exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hurter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Visualization</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="127" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Interactive image-based information visualization for aircraft trajectory analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hurter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Conversy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gianazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Telea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part C: Emerging Technologies</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="207" to="227" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graph bundling by kernel density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hurter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ersoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Telea</surname></persName>
		</author>
		<idno>doi: 10. 1111/j.1467-8659.2012.03079.x</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3pt1</biblScope>
			<biblScope unit="page" from="865" to="874" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Color tunneling: Interactive exploration and selection in volumetric datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hurter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carpendale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Telea</surname></persName>
		</author>
		<idno type="DOI">10.1109/PacificVis.2014.61</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE PacificVis</title>
		<meeting>IEEE PacificVis</meeting>
		<imprint>
			<date type="published" when="2014-03" />
			<biblScope unit="page" from="225" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">MoleView: An attribute and structurebased semantic lens for large element-based plots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hurter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Telea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ersoy</surname></persName>
		</author>
		<idno>doi: 10. 1109/TVCG.2011.223</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2600" to="2609" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">FromDaDy: Spreading data across views to support iterative exploration of aircraft trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hurter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tissoires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Conversy</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2009.145</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1017" to="1024" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Volume splitting and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<idno>doi: 10.1109</idno>
	</analytic>
	<monogr>
		<title level="m">/Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2007-03" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">48</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">3D Slicer: A platform for subject-specific image analysis, visualization, and clinical support</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kikinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Pieper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vosburgh</surname></persName>
		</author>
		<idno>doi: 10. 1007/978-1-4614-7657-3 19</idno>
	</analytic>
	<monogr>
		<title level="m">Intraoperative Imaging Image-Guided Therapy</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="277" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SignalLens: Focus+context applied to electronic time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kincaid</surname></persName>
		</author>
		<idno>doi: 10.1109</idno>
	</analytic>
	<monogr>
		<title level="m">/Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">193</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ClearView: An interactive context preserving hotspot visualization technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Westermann</surname></persName>
		</author>
		<idno>doi: 10. 1109/TVCG.2006.124</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="941" to="948" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">FFTEB: Edge bundling of huge graphs by the Fast Fourier Transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lhuillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hurter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Telea</surname></persName>
		</author>
		<idno type="DOI">10.1109/PACIFICVIS.2017.8031594</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE PacificVis</title>
		<meeting>IEEE PacificVis</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Luggage visualization and virtual unpacking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Paladini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WO patent WO2013142072A3</title>
		<imprint>
			<publisher>Siemens Corp</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Luggage visualization and virtual unpacking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Paladini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kohlberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bahlmann</surname></persName>
		</author>
		<idno type="DOI">10.1145/2425296.2425325</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. WASA SIGGRAPH Asia Workshop</title>
		<meeting>WASA SIGGRAPH Asia Workshop</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="161" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Using deformations for browsing volumetric data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Mcguffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tancau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Balakrishnan</surname></persName>
		</author>
		<idno type="DOI">10.1109/VISUAL.2003.1250400</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Visualization</title>
		<meeting>IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="401" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Flow visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Merzkirch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inc</forename><surname>Nvidia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sdk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samples</surname></persName>
		</author>
		<ptr target="http://www.nvidia.com/object/cuda_get_samples_3.html" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">JellyLens: Contentaware adaptive lenses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pindat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pietriga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puech</surname></persName>
		</author>
		<idno type="DOI">10.1145/2380116.2380150</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Symp. on User Interface Software and Technology (UIST)</title>
		<meeting>ACM Symp. on User Interface Software and Technology (UIST)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="261" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A survey of perceptually motivated 3d visualization of medical image data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Preim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Isenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ropinski</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.12927</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="501" to="525" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Opacity peeling for direct volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rezk-Salama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolb</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-8659.2006.00979.x</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="597" to="606" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Integrating expanding annotations with a 3D explosion probe</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sonnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carpendale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Strothotte</surname></persName>
		</author>
		<idno type="DOI">10.1145/989863.989871</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Working Conference on Advanced Visual Interfaces, AVI &apos;04</title>
		<meeting>the Working Conference on Advanced Visual Interfaces, AVI &apos;04<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="63" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Simplified representation of vector fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Telea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Van Wijk</surname></persName>
		</author>
		<idno type="DOI">10.1109/VISUAL.1999.809865</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Visualization</title>
		<meeting>IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="263" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Smart lenses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thiede</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schumann</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-85412-816</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Symp. on Smart Graphics</title>
		<meeting>Intl. Symp. on Smart Graphics</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="178" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Event-based concepts for user-driven visualization. Information Visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tominski</surname></persName>
		</author>
		<idno type="DOI">10.1057/ivs.2009.32</idno>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="65" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fisheye tree views and lenses for graph visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tominski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Abello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Van Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schumann</surname></persName>
		</author>
		<idno type="DOI">10.1109/IV.2006.54</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Conf. on Information Visualisation (IV)</title>
		<meeting>Intl. Conf. on Information Visualisation (IV)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Interactive lenses for visualization: An extended survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tominski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gladisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dachselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schumann</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.12871</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="173" to="200" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Stackingbased visualization of trajectory attribute data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tominski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Andrienko</surname></persName>
		</author>
		<idno>doi: 10. 1109</idno>
	</analytic>
	<monogr>
		<title level="m">/Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">265</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">View-dependent streamline deformation and exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Wong</surname></persName>
		</author>
		<idno>doi: 10.1109</idno>
	</analytic>
	<monogr>
		<title level="m">/Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">2502583</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Glyphlens: View-dependent occlusion management in the interactive glyph visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Shen</surname></persName>
		</author>
		<idno>doi: 10. 1109</idno>
	</analytic>
	<monogr>
		<title level="m">/Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">2599049</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Interactive exploration of 3d scanned baggage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Traoré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hurter</surname></persName>
		</author>
		<idno type="DOI">10.1109/MCG.2017.10</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="33" />
			<date type="published" when="2017-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">CUBu: Universal realtime bundling for large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Der Zwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Codreanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Telea</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2016.2515611</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2550" to="2563" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The magic volume lens: an interactive focus+context technique for volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kaufman</surname></persName>
		</author>
		<idno type="DOI">10.1109/VISUAL.2005.1532818</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Visualization</title>
		<meeting>IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="367" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Guidelines for using multiple views in information visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Q</forename><surname>Wang Baldonado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Woodruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuchinsky</surname></persName>
		</author>
		<idno>doi: 10. 1145/345513.345271</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Working Conf. on Advanced Visual Interfaces (AVI)</title>
		<meeting>Working Conf. on Advanced Visual Interfaces (AVI)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multiperspective focus+context visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Popescu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2015.2443804</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1555" to="1567" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Interactive transfer function design based on editing direct volume rendered images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2007.1051</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1027" to="1040" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
