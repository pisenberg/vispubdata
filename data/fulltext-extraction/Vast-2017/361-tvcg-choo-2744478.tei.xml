<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename><forename type="middle">D</forename><surname>Park</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland in College Park</orgName>
								<address>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Elmqvist</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland in College Park</orgName>
								<address>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Korea University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename><forename type="middle">N</forename><surname>Diakopoulos</surname></persName>
							<email>nicholas.diakopoulos@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Northwestern University</orgName>
								<address>
									<settlement>Evanston</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TVCG.2017.2744478</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Here the user creates a new unipolar concept <ref type="bibr" target="#b0">(1)</ref> by adding initial keywords related to 'tidal flooding' <ref type="bibr" target="#b1">(2)</ref>. The system recommends related words along with their semantic groupings <ref type="bibr" target="#b2">(3)</ref>, also shown in a scatterplot <ref type="bibr" target="#b3">(4)</ref>, revealing word-and cluster-level relationships. Irrelevant words can be specified to improve recommendation quality <ref type="bibr" target="#b4">(5)</ref>. Concepts <ref type="bibr" target="#b8">(9)</ref> can then be used to rank document corpora <ref type="bibr" target="#b9">(10)</ref>. Document scores can be visualized in a scatterplot based on concepts such as 'tidal flooding' and 'money' <ref type="bibr" target="#b6">(7)</ref>. Users can further refine concepts based on results <ref type="bibr" target="#b7">(8)</ref>.</p><p>Abstract-Central to many text analysis methods is the notion of a concept: a set of semantically related keywords characterizing a specific object, phenomenon, or theme. Advances in word embedding allow building a concept from a small set of seed terms. However, naive application of such techniques may result in false positive errors because of the polysemy of natural language. To mitigate this problem, we present a visual analytics system called ConceptVector that guides a user in building such concepts and then using them to analyze documents. Document-analysis case studies with real-world datasets demonstrate the fine-grained analysis provided by ConceptVector. To support the elaborate modeling of concepts, we introduce a bipolar concept model and support for specifying irrelevant words. We validate the interactive lexicon building interface by a user study and expert reviews. Quantitative evaluation shows that the bipolar lexicon generated with our methods is comparable to human-generated ones.</p><p>Index Terms-Text analytics, visual analytics, word embedding, text summarization, text classification, concepts</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>We live in a world that routinely produces more textual data on a daily basis than can be comfortably viewed-let alone analyzed-by a single person in virtually any given domain: finance, journalism, medicine, politics, and business, to name just a few. As a result, automatic text analysis methods, such as sentiment analysis <ref type="bibr" target="#b33">[34]</ref>, document summarization <ref type="bibr" target="#b3">[4]</ref>, and probabilistic topic modeling <ref type="bibr" target="#b2">[3]</ref> are becoming increasingly important. Central in most of these methods is the focus on textual concepts, defined as a set of semantically related keywords describing a particular object, phenomenon, or theme. For example, sentiment analysis can be viewed as analyzing documents according to two concepts: positive and negative sentiment. Similarly, the topics derived in topic modeling can be thought of as document-driven concepts. The benefit of this unified view is that concepts, once created, can then be shared and reused many times, similarly to widely applicable lexicon sets such as Linguistic Inquiry and Word Count (LIWC) <ref type="bibr" target="#b36">[37]</ref> or General Inquirer (GI) <ref type="bibr" target="#b38">[39]</ref>. Generally, building a lexicon for a particular concept requires significant human effort, and thus only a limited number of human-generated concepts have been available, usually with a small number of keywords contained in each. Recently, Fast et al. <ref type="bibr" target="#b13">[14]</ref> proposed a technique called Empath that uses state-of-the-art word embedding <ref type="bibr" target="#b30">[31]</ref> to efficiently build a semantically meaningful lexicon for a concept. Given userprovided keywords, such as 'bleed' and 'punch,' Empath automatically generates semantically related keywords (e.g., 'violence'). This enables user-driven document analysis from diverse aspects. For example, they found that deceptive languages in fake reviews tend to use stronger and exaggerated words while real reviews often use spatial words to describe their experiences in concrete detail.</p><p>However, we claim that without considering the document context and keyword usage patterns in it, blindly applying a pre-built lexicon for document analysis can easily lead to a misunderstanding of the content. For instance, when we compared Twitter messages from the U.S. 2016 presidential candidates by using a built-in lexicon provided by Empath, we found that Donald Trump used twice as many keywords in an 'alcohol'-related lexicon than the other candidate. A close inspection of the usage pattern of this lexicon revealed the single word lightweight to be a dominant keyword. Lightweight is colloquially used for a person who cannot withstand an alcoholic drink, and hence had found its way into the lexicon for 'alcohol.' However, Trump used this keyword to mock people as less influential or important; therefore, in this corpus, this keyword is not related to the concept 'alcohol.' This example shows the difficulty in applying a lexicon to document analysis in a custom domain because of different usages of keywords in their context.</p><p>Motivated by this challenge, we present a visual analytics system called CONCEPTVECTOR 1 that seamlessly integrates a user-driven lexicon-building process with customized document analysis in a highly efficient and flexible manner. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, users can easily create a lexicon for a particular concept by selecting system-recommended keywords and by adding new keywords of their choice. A user can also explicitly tag recommended words that are unrelated to the concept under consideration as irrelevant, clarifying the meaning by weakening the overall relevance of those words. In addition, ConceptVector supports the definition and construction of bipolar concepts (e.g., positive vs. negative sentiments, liberal vs. conservative political orientation, and Trekkie vs. Star Wars fans) that can be modeled by providing two sets of seed words corresponding to the two polarities. ConceptVector also allows users to analyze any document corpus with respect to any desired concept, such as product reviews based on sentiment, blog posts based on political orientation, or trade articles based on business sectors. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, the document corpus analysis process is tightly integrated with the concept-building process described above so that users can customize concepts during document analysis.</p><p>Our quantitative evaluation validates the proposed bipolar conceptbuilding model by comparing automatically generated rankings with a small number of seed words to the human-labeled rankings of words associated with the concept 'happiness' <ref type="bibr" target="#b10">[11]</ref>. We also present a user study to evaluate the interactive concept-building process, where we compared the performance of the lexicon-building process against using an online thesaurus (Thesaurus.com) and the WordNet <ref type="bibr" target="#b32">[33]</ref> lexical database. We also provide usage scenarios demonstrating the concept-based document analysis process. <ref type="bibr" target="#b0">1</ref> http://www.conceptvector.org/ In summary, the contributions of our work include the following:</p><p>• A visual analytics system called CONCEPTVECTOR where users can interactively build and refine a lexicon for custom concepts and analyze a document corpus using them in a seamless manner;</p><p>• Models for user-steerable word-to-concept similarities to handle irrelevant keywords as well as bipolar concepts; and</p><p>• Quantitative results comparing the capabilities of our word-toconcept similarities to human-labeled ones; and</p><p>• Results from a user study comparing concept generation performance using ConceptVector to Thesaurus.com and WordNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Numerous previous studies have attempted to scale up human capability to make sense of a text corpora. ConceptVector is a visual analytics system that uses word-level semantics using a lexicon for concepts. In this section, we discuss current research related to our work from three perspectives: (1) manual approaches for constructing word relationships and hierarchies, (2) automatic word-embedding approaches, and (3) visual analytics approaches for word-level content analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Building Word Relationships and Hierarchies</head><p>Manually building a lexicon with coherent semantics has long been an active area of research. LIWC <ref type="bibr" target="#b36">[37]</ref> is an example of a manually built lexicon that characterizes various concepts. The General Inquirer 2 is a comparable line of research that builds lexica in diverse concepts. Beyond building a lexicon for a particular purpose, researchers have also developed sophisticated structures that store relationships and hierarchies of words. Unlike these methods, which rely on a small number of experts to compose a lexicon, the Hedonometer project <ref type="bibr" target="#b10">[11]</ref> employed crowdsourcing to build a lexicon for sentiment ranking. One benefit of this approach is its large-sized lexicon, containing the ranked list of 7,000 words in terms of the degree of happiness.</p><p>Although these manually built databases, which store relationships and hierarchies of words, provide high-quality information for various natural language understanding and text analysis tasks, the main problem is the significant human effort needed to create and validate them. This makes it difficult for users to efficiently create a lexicon for their own purpose. Because of this high cost, only a limited number of widely applicable concepts can be built, and building a domain-specific custom lexicon has not been well-supported. This has motivated a slew of automatic methods to craft a lexicon of custom concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Word Embedding</head><p>Word embedding computes semantically meaningful vector representations of words in a high-dimensional space. Compared to traditional methods of representing a word as a vector, such as the bag-of-words representation <ref type="bibr" target="#b28">[29]</ref> or latent semantic indexing <ref type="bibr" target="#b8">[9]</ref>, recent word embedding methods such as word2vec <ref type="bibr" target="#b30">[31]</ref> and GloVe <ref type="bibr" target="#b37">[38]</ref> have two noteworthy advantages in terms of high-level semantics: meaningful nearest neighbors and linear substructures <ref type="bibr" target="#b37">[38]</ref>. Regarding the first, these techniques satisfactorily capture semantically related words as the nearest neighbors of a particular word in a vector space. As for linear substructures, the vector obtained by subtracting two words in a vector space often yields semantics that contrast the words. For instance, if we subtract a word vector 'queen' from 'king' and then add 'girl,' the resulting vector corresponds to 'boy.' This stems from the fact that the vector from 'king' to 'queen' and from 'boy' to 'girl' are similar, commonly representing the notion of gender (from male to female).</p><p>Since such word embedding techniques have shown their advantages in numerous tasks in natural language processing and information retrieval, advanced word embedding techniques have recently been actively studied. Ling et al. proposed the use of multidimensional transformation matrices to flexibly capture different semantics of a single word <ref type="bibr" target="#b26">[27]</ref> leading to better representations for part-of-speech tagging tasks. Similarly, assigning more weight to a particular word than other words in a sentence produced better word embeddings by extending the continuous bag-of-words model <ref type="bibr" target="#b27">[28]</ref>. The weights are computed by an attention model, yielding better performance than neural network models <ref type="bibr" target="#b0">[1]</ref>. Tian et al. integrated an expectation-maximization (EM) algorithm with the continuous skip-gram model to handle the polysemy problem <ref type="bibr" target="#b41">[42]</ref>. For example, the word 'bank' can have multiple vector representations corresponding to 'a place related to money' and 'a place where water runs,' respectively. Besides transforming word-level embeddings, several efforts extended this technique to document-level embeddings that yielded good performance in information retrieval tasks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24]</ref>. Other notable recent studies applied the technique to machine translation <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32]</ref>. Additionally, the skip-gram idea of word2vec has been applied in generating the embeddings of entities in other domains, e.g., bibliographic items in scientific literature <ref type="bibr" target="#b1">[2]</ref> and nodes in network analysis <ref type="bibr" target="#b14">[15]</ref>. Finally, and most relevant to this work, Fast et al. <ref type="bibr" target="#b13">[14]</ref> showed that word embedding can be used to expedite lexicon-building so that users can easily create their own concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Word-Level Content Analysis</head><p>The use of a coherent set of keywords for characterizing a particular concept has wide applicability in various document analysis tasks. For instance, the problem of sentiment analysis has been tackled by identifying a set of keywords expressing the positive (or the negative) sentiment, possibly with different degree values, and this is also known as a lexicon-based sentiment analysis <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b39">40]</ref>. In topic modeling, such as latent Dirichlet allocation (LDA) <ref type="bibr" target="#b2">[3]</ref>, a topic represents a set of semantically related keywords found in a document corpus, e.g., sports-or science-related topics, generated from a large amount of news articles. Recent studies by Kim et. al <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21]</ref> are particularly notable because they introduced a continuous embedding space similar to concepts as considered in this paper, although they only covered emotion-related concepts.</p><p>Topic modeling has also been actively employed in visual analytics approaches for document analysis. TIARA <ref type="bibr" target="#b43">[44]</ref> is one of the first systems that integrated LDA with interactive visualization. This system visualizes the topical changes of documents over time in a streamgraph view reminiscent of ThemeRiver <ref type="bibr" target="#b15">[16]</ref>. Other studies, such as Paral-lelTopics <ref type="bibr" target="#b11">[12]</ref> and TextFlow <ref type="bibr" target="#b7">[8]</ref>, also focused on visualizing topical changes over time in document data by using different visualization techniques, such as parallel coordinates and custom glyphs, respectively. In most of these studies, the key information for understanding the visualized topics is a set of dominant keywords associated with each topic. However, the number of topics can be as large as several hundreds or thousands <ref type="bibr" target="#b40">[41]</ref>. This makes manual interpretation of topic characterization or topic labeling a main bottleneck for the topic modeling. To facilitate this task, Termite <ref type="bibr" target="#b6">[7]</ref> provides an interactive visualization with which a user can explore topics in terms of their dominant keywords, as well as the overlapping patterns of keywords among different topics. In addition, various interactive capabilities that can steer the topic modeling process in a user-driven manner have been studied. iVisClustering <ref type="bibr" target="#b25">[26]</ref> allows a user to perform a user-driven topic modeling process by interactively constructing topic hierarchies and changing keyword weights of a topic. Chang et al. introduced a interactive clustering system based on knowledge-graph embeddings <ref type="bibr" target="#b4">[5]</ref>. More recently, non-negative matrix factorization <ref type="bibr" target="#b24">[25]</ref> has been proposed as an alternative topic modeling method that can flexibly support user needs such as splitting and merging topics, creating a new topic via particular keywords, and supporting user-driven topic discovery <ref type="bibr" target="#b5">[6]</ref>.</p><p>Our ConceptVector work in this study has much in common with topic modeling: both try to summarize documents, and both express words and documents as high-dimensional vectors. However, they differ in whether humans or the document corpus itself drive the latent semantics behind each dimension. Topic modeling, therefore, is bettersuited for finding hidden underlying topic clusters, while ConceptVector provides better interpretability and transferability. In this sense, topic modeling and ConceptVector are complementary.</p><p>Lexicon-based document analysis has also been applied in various application domains. For instance, Kwon et al. <ref type="bibr" target="#b22">[23]</ref> used a manually built lexicon to identify online health community postings that share personal medical experiences. In most of these previous studies, document analysis relied on lexicons of properly chosen words that were created for a specific purpose. The ConceptVector system aims to help users easily create such lexicons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MOTIVATION: CONCEPT-BASED DOCUMENT ANALYSIS</head><p>Here we describe two real-world examples where concept-based document analysis was performed by using Empath and Jupyter Notebook. <ref type="bibr" target="#b2">3</ref> First, we show how concepts can reveal the underlying differences in two document sets, such as tweets from Hillary Clinton and from Donald Trump, highlighting the importance of integrating the lexiconbuilding process with its refinement during the document analysis. Second, we demonstrate how NASDAQ 100 companies can be clustered using the differences in concepts and how each cluster can be interpreted using tweets mentioning them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Tweets by U.S. 2016 Presidential Candidates</head><p>Empath <ref type="bibr" target="#b13">[14]</ref> provides prebuilt lexica of various concepts that can be used to compare two document groups. Using these 194 prebuilt concepts provided by Empath, we analyzed two sets of tweets composed by Hillary Clinton and Donald Trump 4 respectively, each of which contains about 3,000 tweets. shows the most dominant keywords corresponding to each concept. While some keywords make sense, e.g., 'wow' in the 'surprise' category, less meaningful words exist in other categories. For example, Trump was shown to talk more about the 'plant' concept because of the term 'bush,' which in fact indicates Jeff Bush. 'crooked' in the 'ugliness' concept means 'deformed', whereas Trump is using it in his catchphrase 'Crooked Hilary' to mean 'not straightforward; dishonest.' Besides, another strong concept 'hipster' emerged because of the use of the term 'looking,' while 'swearing terms' emerged because of the use of 'bad.' In Hillary's case, the 'sexual' concept appeared owing to the use of 'violence,' which did not make much sense. After removing these words from the corresponding concepts, these concepts no longer show significant differences between the two.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Tweets from NASDAQ 100 Companies</head><p>Concepts can be also used to extract meaningful features from documents. Given tweets about NASDAQ 100 companies, <ref type="bibr" target="#b5">6</ref> our goal in this work was to find meaningful clusters and their distinct characteristics by using concepts as features. That is, for a set of tweets belonging to each company, we obtained its 194-dimensional feature vector by computing the occurrence count of words contained in each of the 194 prebuilt concepts. Afterwards, we performed k-means clustering and 2D embedding via principal component analysis (PCA) <ref type="bibr" target="#b17">[18]</ref>.</p><p>The results <ref type="figure" target="#fig_4">(Figure 4</ref>(a)) reveal that words from the company name affect the results, e.g., 'cooking' and 'restaurant' categories for Dish Network Corporation. Companies containing 'technology' in their names form a cluster because of similar reasons. After removing these words from the lexicon of the corresponding concept and recomputing feature vectors, the clustering results are shown to be more reasonable <ref type="figure" target="#fig_4">(Figure 4</ref> and 'hot.' Companies with their tweets containing negative sentiments such as 'ridicules,' 'neglect,' 'kill,' or 'hate' are clustered together. This example shows that document analysis using concepts as a feature extractor is useful, but that existing systems such as Empath lack the integrated support for concept construction and refinement, as well as interactive concept-based analysis itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCEPTVECTOR IN ACTION</head><p>To address the limitations of using prebuilt lexica, ConceptVector aims at facilitating user-driven concept building as well as the subsequent concept-based document analysis in a seamless manner.</p><p>While the previous examples started with prebuilt lexica, we now present how ConceptVector can be used to build custom concepts in the task of journalistic curation of user comments on online news. Moderation of online comments can follow various approaches, and often includes mechanisms to remove uncivil, profane, or otherwise inflammatory comments. That is, however, not our focus here; instead we consider the approach championed by the New York Times, in which editorially interesting and insightful comments are selected and highlighted on the site as "NYT Picks" comments. Below we present a scenario showing how an expert community moderator from an organization such as the New York Times could leverage the capabilities of ConceptVector to define and deploy those concepts useful for finding and selecting "NYT Picks" comments.</p><p>It is helpful to understand the general editorial attitude and approach-the persona-of an online news moderator. Prior research has enumerated several dimensions of editorial interest for finding high-quality comments including factors such as comment relevance, argument quality, novelty, and personal experience <ref type="bibr" target="#b9">[10]</ref>. Importantly, different articles or subcommunities on a site demand different approaches to moderation and the application of different editorial criteria <ref type="bibr" target="#b34">[35]</ref>. Diversity is a dimension of utmost importance to comment moderators; it is a difficult task to select high-quality comments that also reflect the diversity of voices available in a comment stream. Con-ceptVector is well-suited to enabling such diverse selection because of its capabilities to allow moderators to develop content-specific or even article-specific concepts to apply to different contexts, and to see how comments are scored when applying that concept.</p><p>Let us follow Laurie, a hypothetical comment moderator at the New York Times who is trying to moderate comments on several different articles. Her task is to pinpoint diverse but representative comments to highlight on the site as "NYT Picks."</p><p>The article she is examining is entitled "Seas Are Rising at Fastest Rate in Last 28 Centuries," which has over 1,200 comments when she logs on. <ref type="bibr" target="#b6">7</ref> She is really not looking forward to moderating the comments for this article, because an article like this always brings out the global warming skeptics who can cause quite a ruckus. The article is specifically about the idea of 'tidal flooding,' i.e., the notion that coastal areas will be flooded more often as sea levels rise. Using ConceptVector, she first wants to develop a tightly defined concept on this specific idea of 'tidal flooding' so that she can find comments maximally relevant to the article.</p><p>Laurie creates a unipolar concept for 'tidal flooding' by typing in its relevant keywords, starting with the words 'tidal' and 'flooding.' She then sees related words as recommendations in the scatterplot that help her flesh out the concept by adding related terms such as 'flood,' 'floods,' 'tide,' and 'tides,' as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. She examines the clusters of other terms generated, and decides to avoid words related to specific instances of tidal flooding, such as 'katrina,' or those associated with storms and hurricanes, such as 'storm,' 'raging,' or 'swell.' She wants to keep this a general-purpose concept. Moving on to the second phase, she applies the concept to the comments on the article and immediately notices other key terms, e.g., 'storm,' highlighted as yellow in the retrieved comments. She then adds them to the relevant keyword set of the concept using the integrated concept editor.</p><p>Based on her understanding of media framing, Laurie knows that people often discuss complex issues in terms of specific frames relating to definitions, causal interpretations, moral evaluations, and solutions <ref type="bibr" target="#b12">[13]</ref>, as well as using topical perspectives like economic, political, or scientific. She decides to find a comment to highlight that deals with tidal flooding from the perspective of economic implications. Similar to how she developed the unipolar concept for 'tidal flooding,' she develops another unipolar concept relating to economic implications. She starts with 'economic,' and the scatterplot of recommended words leads her to add related terms such as 'economy' and 'economies,' as well as some of the negative implications that she wants to include, such as 'crisis,' 'impact,' 'turmoil,' and 'instability.' Her economic concept is thus tuned towards negative economic impacts that could arise.</p><p>To apply a combinations of these two concepts, Laurie checks the distribution showing all comments plotted against the relevance scores to each of the two concepts ( <ref type="figure" target="#fig_5">Figure 5</ref>). Here she maps the 'tidal Another cluster is shown to be formed because of the common word 'technology' in their names. After excluding them in the initial lexicon, more meaningful clusters are revealed. For example, Marriott and TripAdvisor form a cluster because of words in 'tourism,' 'vacation,' and 'sleep' concepts (olive green with a black border). Companies with negative sentiments such as 'ridicules,' 'neglect,' 'kill,' and 'hate' were also clustered together (bright red dots with red border). flooding' concept on the x-axis and the 'economy' concept on the yaxis. She then brushes on the scatterplot to find comments containing both concepts, and these comments are filtered into the ranked list. She finds an insightful comment she likes that perfectly combines the two concepts, discussing coastal flooding in terms of impacts to the economy as exposed through the insurance industry. She marks the comment as a "NYT Pick" and it gets highlighted on the site.</p><p>She then begins to read those comments with high scores from the top of the list and quickly finds an insightful one indicating that some of the coastal flooding in Virginia has actually been shown to be a result of subsidence of land. Laurie thinks that highlighting this will deepen the discussion online by pointing out the diverse factors that society needs to grapple with as it confronts global warming. Therefore, she marks this comment as an "NYT Pick" as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">THE CONCEPTVECTOR SYSTEM</head><p>Motivated by the limitations of using prebuilt lexica for concept-based document analysis, we designed ConceptVector as a visual analytics system that tightly integrates concept building and refinement with direct support for concept-based document analysis. In detail, our design rationale behind ConceptVector is as follows:</p><p>D1 Supporting diverse user needs in concept building. Users may have diverse meanings in mind for defining their concepts. Thus, users should be able to construct the lexicon of a concept from scratch and/or refine a prebuilt one to suit to their exact requirements.</p><p>D2 Supporting integrated analysis of iterative lexicon refinement and concept-based document analysis. As seen from our motivational examples (Section 3), even carefully curated lexica need to be adjusted depending on a document corpus. Thus, the concept-based document analytics system should provide interactive refinement capabilities of a lexicon as well as dynamic document analysis based on the updated lexicon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D3</head><p>Revealing lexicon word context in documents. The system should allow users to understand how the words in a lexicon are used in documents in terms of their context.</p><p>In this section, we explain how our front-end interfaces and the back-end computational modules support these tasks, and associate each component with design guidelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Front-end Visual Interface</head><p>Based on our design rationale, the text analytics process in ConceptVector is composed of two iterative processes: concept building and document analysis <ref type="figure" target="#fig_1">(Figure 2)</ref>. We introduce the two views that allow the user to interactively build concepts and analyze documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Concept Building View</head><p>As shown in the left pane of <ref type="figure" target="#fig_1">Figure 2</ref>, the concept building process allows a user to interactively build the keyword sets describing a user's intended concept. <ref type="figure" target="#fig_0">Figure 1</ref> shows a screenshot of our front-end interface that was taken during this process when the user was building the 'tidal flooding' concept.</p><p>We define two types of concepts: bipolar and unipolar. Bipolar concepts have two nontrivial polarities, e.g., positive vs. negative sentiments, happiness vs. unhappiness, etc., while unipolar concepts have a single polarity, e.g., work-related (or not), biology-related (or not), etc. To support both concept types, ConceptVector models a particular concept using three different sets of keywords: positive, negative, and irrelevant (D1). In the case of unipolar concepts, the positive keyword set contains those keywords relevant to a concept of interest, while the negative set is an empty set. For both types, the irrelevant keyword set includes the words marked explicitly as irrelevant by the user.</p><p>The user starts building a concept by adding seed keywords to describe the concept. ConceptVector then recommends keywords that are potentially relevant to the seed keywords for each positive and negative keyword set, and performs k-means clustering, where we set k as 5, based on their word embeddings. Keyword clusters are presented to the user <ref type="figure" target="#fig_0">(Figure 1(3)</ref>), along with their 2D embedding view, computed by t-distributed stochastic neighbor embedding (t-SNE) <ref type="bibr" target="#b42">[43]</ref>  <ref type="figure" target="#fig_0">(Figure 1(4)</ref>). Checking these recommendation results, the user can either expand the initial keyword set by (1) adding individual words, (2) adding a keyword cluster of them, or (3) move words to the irrelevant set by marking them as irrelevant (D1). This iterative concept building continues until the user is satisfied with the constructed keyword set.</p><p>As relevant (or irrelevant) keywords often appear together in a single cluster, processing words at the cluster level makes the concept building process much more efficient than without clustering (D1). For example, if a user enters 'happy' as the only keyword for a concept, irrelevant words such as 'everyone,' 'anyway,' 'yes,' and 'anymore' are recommended as a single cluster, while semantically relevant words such as 'glad,' 'good,' and 'thrilled' form another cluster. When the semantic distinction among words is not clear, users can tag individual words in the cluster. The t-SNE embedding space has very strong neighboring effects <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b37">38]</ref>, placing similar words closely to each other, and hence the 2D embedding view shows the distribution among user-initiated keywords and recommended ones. Users can enter/remove keywords in the t-SNE view as well (D1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Concept-Based Document Analysis View</head><p>The concept-based document analysis view, shown in the right pane of <ref type="figure" target="#fig_0">Figure 1</ref>, allows the user to analyze a document corpus with respect to the constructed concepts. See Section 4 for a detailed description.</p><p>Given a single or multiple user-selected concepts, ConceptVector computes the relevance scores of documents for each concept and retrieves/ranks those documents with high score values <ref type="figure" target="#fig_0">(Figure 1(10)</ref>), which would be meaningful to the user who created/selected the corresponding concept. To help the user understand why these documents have high scores, the significantly contributing keywords are highlighted in yellow color (D3). Please note that our relevance scoring algorithm is not limited to the keywords registered in the positive/negative/irrelevant sets, but that other keywords potentially relevant to the concepts are considered as well. We will describe the algorithm further in the following section.</p><p>Additionally, ConceptVector provides two different views: a temporal view showing the concept strength over time, and a scatterplot showing the distribution of documents according to the relevance scores for the two different concepts, e.g., 'tidal flooding' vs. 'economy' concepts ( <ref type="figure" target="#fig_5">Figure 5</ref>). According to the Jänicke et. al., extraction, evolution, and clustering are the three main tasks in visual text analysis <ref type="bibr" target="#b16">[17]</ref>. The temporal view supports the temporal tracking of the topic signal evolution, while the scatterplot allows mapping/clustering documents in semantic space. Users can assign user-defined concepts as axes of the scatterplot to explore the distribution of the semantic meaning of documents (D2). Note here that we use a modified version of a scatterplot, where both dimensions are binned and dots are scaled to fill the assigned space <ref type="bibr" target="#b35">[36]</ref>. This improves the visibility of outliers and densely overplotted areas. In these views, the user can brush over a time axis or data items to filter data in the ranked retrieval results.</p><p>During the process, the user may add additional words to the relevant and the irrelevant keyword sets of the concept (D2). For example, when applying the 'tidal flooding' concept shown in <ref type="figure" target="#fig_0">Figure 1</ref> to a document corpus, the word 'disaster' was highlighted owing to its high relevance score to the concept. Since this word is not related to the 'tidal flooding' concept, the user can add it to the irrelevant keyword set to revise the concept and update the ranking of documents accordingly. This interaction allows in-situ concept refinement.</p><p>Note that the two analysis tasks of concept building and document analysis are not separate but tightly connected in ConceptVector, so that the user can fluidly switch between concept building/refinement and document analysis based on concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Back-end Relevance Scoring Model</head><p>ConceptVector is built upon the vector representations of words generated by word embedding techniques such as word2vec <ref type="bibr" target="#b30">[31]</ref> or GloVe <ref type="bibr" target="#b37">[38]</ref>. In this step, the training corpus for word embeddings could be a generic one such as Wikipedia articles or a corpus within a particular domain, so that the trained vectors can better reflect the semantics of the domain. ConceptVector currently adopts pretrained vector embedding using Wikipedia articles by GloVe. <ref type="bibr" target="#b7">8</ref> ConceptVector represents a concept C as the three set of keywords: the positive, the negative, and the irrelevant ones-L p , L n , and L i , respectively. Given a word or a document, ConceptVector computes its relevance scores to the concept, based on the probability of a given word belonging to each of L p , L n , and L i using a kernel density estimation (KDE) method.</p><p>In detail, let us denote q as the vector representation of a query word, l as that of the keyword contained in the keyword set L, where L can be one of L p , L n , or L i . We define the probability of q belonging to L as</p><formula xml:id="formula_0">p(q|L) = 1 |L| ∑ x∈L k (q, l) ,<label>(1)</label></formula><p>where k (q, l) represents a kernel function computing the similarity value between the two word vectors q and l. That is, Eq. (1) computes the average similarity values between q and each word l contained in a particular keyword set L. The reason for using a kernel function instead of a simple similarity measure such as cosine similarity is because this provides not only a user-controllable, flexible similarity measure but also a principled probabilistic framework of incorporating multiple similarities of q with L p , L n , and L i , as will be described later.</p><p>The choice of the kernel function k (q, l) can vary, but in ConceptVector, we adopted a Gaussian kernel defined as</p><formula xml:id="formula_1">k (q, l) = 1 √ 2πσ 2 exp − q − l 2 2 σ 2 ,</formula><p>where σ 2 is the bandwidth parameter that determines how quickly the similarity decreases as the L 2 distance increases. A small bandwidth value gives a high similarity only on the words exactly contained in L, which is suitable when L contains many words and a user does not want to consider other words outside L as relevant to the concept. A large bandwidth, on the other hand, will consider many of the outside words as relevant to L, which is useful when a user wants to define the concept in a broad and flexible manner, not just limited to those words contained in L.</p><p>Viewing p(q|L), which is computed by Eq. (1), as the likelihood in a Bayesian context, we can define the prior probability p(L) and the posterior probability p(L|q), respectively, as</p><formula xml:id="formula_2">p(L) = |L| L p + |L n | + |L i |</formula><p>, and</p><formula xml:id="formula_3">p(L|q) = p(L) • p(q|L) p(L p ) • p(q|L p ) + p(L n ) • p(q|L n ) + p(L i ) • p(q|L i )</formula><p>.</p><p>Using these, the final relevance score r (q, C) of a query word q to the concept C is computed as</p><formula xml:id="formula_4">r (q, C) = (1 − p (L = L i |q)) • p(L p ) • p(q|L p ) − p(L n ) • p(q|L n )</formula><p>Basically, r (q, C) computes the differences between the joint probabilities p(q, L p ) and p(q, L n ), ranging between −1 and +1, and furthermore, as p (L = L i |q) increases, r (q, C) becomes close to zero, indicating irrelevance to the concept.</p><p>In the case of a unipolar concept, the relevance score is computed in the exact same manner by setting L n = / 0. These bipolar scores and unipolar scores are used for recommendation of relevant words.</p><p>Finally, the relevance score of a document to a particular concept is computed by simply taking the average relevance score among all the words contained in a document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Implementation Details</head><p>ConceptVector was implemented as a web-based application using D3 and AngularJS. We employed the New York Times online article comments as our corpus; naturally, the approach can be applied to any document corpus. We selected articles with more than 300 comments from the most popular articles during the period August to September 2016. Articles and comments were collected using the NYT API. <ref type="bibr" target="#b8">9</ref> The back-end computational modules were implemented using Python with the Flask framework. <ref type="bibr" target="#b9">10</ref> The key computation shown in Eq. (1) for recommending relevant words requires computing the one-to-all distances for all words in the current keyword set (either positive, negative, or irrelevant). Computing a single one-to-all distance repeatedly due to frequent user interaction may slow down the overall process. We instead compute the one-to-all distance incrementally with a cache that contains recently computed pairs. This is possible because the user incrementally adds a single word at a time to the keyword set. To this end, a least recently used cache of size 10,000 word pairs was employed, resulting in a speed-up of efficient user interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATION</head><p>Visual analytics systems comprise many interconnected components, and this complicates their overall evaluation. Here we separate the visual interface and the back-end computation and evaluate them individually with a user study and a quantitative evaluation, respectively. For the front-end, we focus on the effectiveness of the concept-building view because document analysis requires analysts with domain knowledge and is subjective to inter-analyst differences. For the back-end, we validate the effectiveness of supporting the process of building bipolar concepts. Although we did not evaluate a unipolar case, we generally expect the same level of effectiveness since the process of building unipolar concepts is similar yet simpler than the process of building bipolar concepts. Finally, we also include results from an expert review comparing ConceptVector to Empath <ref type="bibr" target="#b13">[14]</ref> to show ConceptVector's performance in relation to the state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Evaluation of Concept Building</head><p>We conducted a user study to evaluate how users generate lexica with ConceptVector compared to WordNet <ref type="bibr" target="#b32">[33]</ref>  <ref type="bibr" target="#b10">11</ref> and Thesaurus.com <ref type="bibr" target="#b11">12</ref> as baselines. WordNet is known for its large-scale lexical database, and Thesaurus.com is an online thesaurus containing exhaustive synonyms and antonyms for the English language. We employed the following performance metrics: (i) the completion time for building concepts, and (ii) the quality of the resulting concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Methodology</head><p>We recruited 15 graduate students (1 female and 14 males) majoring in computer science to participate in the study. All participants reported high computer skills.</p><p>Each study session lasted 15-25 minutes and involved three systems: ConceptVector, WordNet, and Thesaurus.com. Before starting the session, a test administrator briefly explained how to use the systems and allowed the participant to spend enough time to familiarize themselves. Participants were then asked to build a lexicon for three concepts: 'family,' 'body,' and 'money,' which we selected as relatively neutral and easily comprehensible by all participants. Each participant was randomly assigned to a system for each concept so that at the end of the study they had used all three conditions. Each concept-building task was capped at three minutes. All three systems, including Con-ceptVector, were accessed by their official websites. We recorded both the lexicon each participant created as well as the number of keywords in it as a dependent variable.</p><p>As the ground truth lexicon for each concept, we selected three dictionaries from Linguistic Inquiry and Word Count (LIWC) 2007 <ref type="bibr" target="#b36">[37]</ref>. The ground truth lexicon sizes of the three concepts are 65 words for 'family,' 180 for 'body,' and 173 for'money.' We adopted widely used information retrieval evaluation metrics, precision and recall, where precision is the fraction of correct answers over the total number of answers given, and recall is the fraction of retrieved correct answers out of all correct ones. The null hypothesis assumes that the difference of methods does not affect the precision, recall, or average number of words in the resulting lexicon. <ref type="table" target="#tab_1">Table 1</ref> shows precision, recall, and average total words generated for the three methods. ConceptVector achieved the highest scores in all three metrics, indicating that the user-created lexicon using Con-ceptVector is the most accurate and most time-efficient. We further analyzed the effect of employing ConceptVector using mixed linear model analysis, where the fixed effect is the choice of methods (Con-ceptVector, WordNet, and Thesaurus.com) and the random effect is the choice of specific concepts ('family,' 'money,' and 'body'). <ref type="figure" target="#fig_6">Figure 6</ref> shows boxplots for precision, recall, and total words generated. We used a pairwise Tukey HSD method to test statistical significance between different methods. There was a significant performance boost of employing ConceptVector on recall (F(2, 40) = 5.25, p = .0094). Pairwise Tukey HSD between ConceptVector and the other methods showed significant differences (p &lt; .05). There was also a significant main effect for technique T on precision (F(2, 40) = 5.22, p = .0096). Pairwise comparisons with a Tukey HSD showed significant differences (p &lt; .05) between ConceptVector and Thesaurus.com. Finally, there was a significant main effect for technique T on the number of total words generated (F(2, 40) = 5.40, p = .0084). Pairwise comparisons with a Tukey HSD showed significant differences (p &lt; .05) for ConceptVector and WordNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Results</head><p>Recall rates in all three systems are relatively low compared to the high precision rates. This is mainly because the size of the ground truth lexicon is much larger than the average size of the lexicon a person can create within a short period of time (three minutes in our case). As seen in <ref type="table" target="#tab_1">Table 1</ref>, the average size of the created lexicon was around 8 to 15 depending on the system. Since the current experimental design does not consider polysemy or subtle nuance differences, this experiment could be improved further by employing more sophisticated ground truth data instead of the current ones obtained from LIWC. For example, the 'family' concept may diverge in terms of its subtler meanings to different people. On the one hand, it may correspond mainly to the members of a family such as 'mother,' 'grandfather,' and 'son.' On the other hand, it may correspond to emotional words such as 'love,' 'rest,' and 'nursing.' Since our ground truth lexicon from LIWC was mostly composed of the keywords from the first case, the user-generated keywords from the second case were treated as false positive words. We expect that ConceptVector will perform better if we find ground truth data that enable us to measure  <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b39">40]</ref> 0.0084 these richer relations, and this will be one of our future directions. Furthermore, regardless of which type of concept a user had in mind, ConceptVector properly supported the concept-building process by recommending suitable keywords for different cases. This indicates the flexibility and the affordance that ConceptVector offers compared to other, more rigid, systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Quantitative Evaluation of Bipolar Concepts</head><p>We validate the bipolar concept model supported by ConceptVector to address the following two questions:(1) Does our proposed approach generate relevance scores comparable to human judgments? and (2) How many input words are required to properly model concepts? To answer these questions, we conducted a quantitative analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Experiment Setup</head><p>Validation of a lexicon requires ground truth. For unipolar concepts, the prior work from Fast et al. compared the result with "golden standard dictionaries" such as LIWC and GI <ref type="bibr" target="#b13">[14]</ref>. While many lexica for unipolar concepts have been developed, bipolar lexica are rare. In this study, we adopted a keyword database available from the Hedonometer project <ref type="bibr" target="#b12">13</ref>  <ref type="bibr" target="#b10">[11]</ref>. This database contains a ranked list of 10,200 keywords in terms of their relevance to the concept of 'happiness,' where the ranking was determined by crowdsourcing. The word ranking begins with the happiest word and ends with the saddest word. From this database, we selected 9,600 words from the intersection of the Hedonometer ranking and the vocabulary set from the Wikipedia corpus 14 used to train our word embedding model. From the Wikipedia corpus, we removed 71,697 documents that no longer exist, and used the resulting 171,729 articles. We then removed the words containing nonalphanumerical characters as well as those appearing less than ten times in the entire document corpus, resulting in 142,275 keywords in total. The goal of our experiments was basically to evaluate how well the ranking of words computed by our back-end algorithm matches with the ground truth ranking, given a subset of top and bottom k words as positive and negative sets, respectively, to form a concept. As the methods to generate word vector representations, we used two different word embedding techniques-word2vec <ref type="bibr" target="#b30">[31]</ref> and GloVe <ref type="bibr" target="#b37">[38]</ref>-as well as a baseline method, latent semantic indexing (LSI) <ref type="bibr" target="#b8">[9]</ref>. Additionally, in each vector space, we compared our KDE-based algorithm against logistic regression for computing the word-to-concept relevance score and the associated word ranking. As an evaluation measure, we computed Spearman's rank correlation coefficient between the ranking of the ground truth and that from each different case. <ref type="figure">Figure 7</ref> shows the Spearman's rank correlation coefficients obtained for various word embedding and relevance scoring methods by varying the value of k in the top k and the bottom k keywords used to train each model. In general, given a small number of input keywords less than 200, the algorithm was shown to generate a reasonably good rank correlation of more than 0.4. In addition, as we increase k, the rank correlation increases in all cases, indicating that more information helps the model learn the intended concept (happiness in this case). Between the two word embedding methods and LSI, the former showed a rapidly increasing performance even with a small number, e.g., around 100, of keywords necessary for training. Between our KDE-based scoring method and logistic regression, the former outperformed the latter <ref type="bibr" target="#b12">13</ref> http://hedonometer.org/ 14 https://cs.fit.edu/ mmahoney/compression/textdata.html <ref type="figure">Fig. 7</ref>. Spearman's rank correlation coefficient results with respect to the number of keywords used for training. KDE stands for kernel density estimation, LogiReg for logistic regression, W2V for word2vec <ref type="bibr" target="#b30">[31]</ref>, LSI for latent semantic indexing, and GloVe for GloVe <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Comparison Results</head><p>method when the size of the keyword set is sufficient, e.g., more than 300. Furthermore, the performance of our KDE-based method consistently increases by a large margin compared to competing methods.</p><p>Among different word embedding techniques, the GloVe model followed by the KDE-based method achieved the best rank correlation performance of around 0.8. Word2vec performed relatively well, but it was inferior to GloVe in our task. On the other hand, traditional methods such as LSI do not perform well in this task, showing a rank correlation of 0.45 even with large k values. Finally, the overall performance gain due to the increase of the embedding dimensions was not significant.</p><p>Our experiment involves only bipolar concepts (no unipolar ones), and we did not examine the effect of an irrelevant keyword set. In this case, logistic regression may not be applicable at all. In addition, we found that the ground truth ranking is not always correct, especially among the mid-ranked unclear words. However, the results presented here highlight the potential superiority of our proposed KDE-based scoring approach combined with GloVe, and in the next section, we present results from an expert review to show the effectiveness of the system when used in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Expert Review</head><p>To evaluate the visual interface of ConceptVector in depth, we engaged two experts, one in text analytics (P1) and one in visual analytics (P2), to provide qualitative feedback on ConceptVector compared to the Empath system. Both were postdoctoral researchers, the former of which conducts research on social networks, crisis analytics, and credibility in social media, while the latter studies the healthcare domain that frequently involves text mining and visualization (e.g., electronic medical records). We began with a 15-minute tutorial introducing our system and Empath. Afterwards, the experts built their own custom concepts using both systems. Those concepts were used in the analysis of New York Times comments. During the process, we gathered the feedback on both the model and the visual interface of the system. The online version of Empath was used as a reference. <ref type="bibr" target="#b14">15</ref> Both P1 and P2 agreed that the recommended keywords given by ConceptVector, which are shown in a semantically meaningful grouping in a scatterplot, were easier to grasp than the simple list given by Empath. The scatterplot helped them digest the generated words by providing a high-level overview (P2) or chunking the words into semantically homogeneous groups (P1). It was especially useful in the early stage of concept building, because irrelevant words formed a separate group in many cases, allowing the user to spot them easily and mark them as irrelevant. The word clusters <ref type="figure" target="#fig_0">(Figure 1(3)</ref>) were good for reading words quickly (P2), and was used during most of the concept-building process (P1). Furthermore, the t-SNE view <ref type="figure" target="#fig_0">(Figure 1(4)</ref>) provided an additional benefit of showing the similarities between words (P1) and the relationships between the input terms and the recommended terms (P2). For example, whether the input words form tight clusters or not gives a visual clue as to whether the generated concept is consistent (P1). At the same time, P1 noticed that an input term was actually an outlier compared to other input terms forming a packed cluster. After examining this word, he removed it because it had a very broad meaning and thus dilutes the clarity of the concept. Both experts noted that the difference between the corpora used to train word embedding affects the concept quality. Empath used modern amateur fiction data, but ConceptVector used the Wikipedia dataset. For example, when P1 used 'politics,' 'voting,' and 'elections' as seed terms in Empath, the generated words contained several words such as 'shipping' and 'readers' which did not really make sense. According to P1, Empath also generated more 'high school'-related words. This does not necessarily mean that one system is better but rather that using word embedding trained by a corpus suitable for target corpus to analyze is important. After building a concept about 'grievance,' P1 noted "The recommended words for the grievance concept is different from what I saw on social media. That is, many legalese and lengthy words related to grievance were recommended, but very unlikely to show up on social media." P2 suggested using ConceptVector as a tool to evaluate multiple versions of word embedding models during iterative model development.</p><p>P1 and P2 both agreed that comparing Empath and ConceptVector is challenging, because the main focus of Empath is not its user interface. P1 thought the visual interface in ConceptVector was useful to explore the semantic space. Being able to look around and select words that are not originally shown to him helped to expand the lexicon. P2 pointed out that the document analysis feature of Empath is more of a blackbox and felt uncomfortable with trusting the result. For example, when analyzing the Wikipedia page about 'Ramen,' the 'friends' category was ranked as the 6th, but it is not clear which words in the friends category were counted.</p><p>P1 noted that the word-highlighting feature of ConceptVector allows for the easy spotting of false positives, but detecting false negatives is not currently supported. P2 appreciated the concept score scatterplot <ref type="figure" target="#fig_0">(Figure 1(7)</ref>) that showed the distribution of comments with respect to custom concepts as axes. It revealed outliers and enabled filtering of comments based on semantic contents. After using ConceptVector, P2 said that it could be useful to build a concept for drugs by adding related symptoms and using a positive/negative sentiment as another axis to visualize the sentiment for a particular drug. P2 also liked that the concept dictionary can be refined by trial and error.</p><p>P1 expressed concern about fundamental limitations of both systems. Both systems use word embedding based on the assumption that word co-occurrence statistics reflect semantic similarities, which might not be always true in real-world text analysis. P1 pointed out that while the color coding of words to highlight newly recommended words is an improvement over Empath, it was still difficult to follow the word changes according to the input terms. P2 liked the bipolar concepts feature because it helps in building more sophisticated concepts. As an alternative design, P2 suggested showing the words interpolating positive and negative terms. Those interpolated words will reveal the validity of a concept, as suggested in Axisketcher <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION</head><p>ConceptVector is a novel approach for text analysis that falls somewhere between sentiment analysis performed using manually constructed dictionaries, and topic modeling performed by automatic algorithms. This unique position brings new benefits as well as limitations.</p><p>In general, when achieving a particular analytic goal, an interesting tradeoff between quality and efficiency can be considered. That is, human efforts secure the quality of the outcome, while automated approaches can significantly boost the efficiency of our efforts. For concept building, purely manual approaches such as LIWC and Hedonometer can be viewed as extreme cases, where the task relies completely on human effort. Thus, the resulting dictionary is of high quality, but it is achieved by an inefficient, costly process without automation. On the other hand, purely automated approaches such as topic modeling, which generate multiple sets of semantically coherent words, maximize the efficiency of the task, but the quality of the outcome cannot be controlled by the user. Human labor is still needed to interpret the results that such fully automatic approaches generate.</p><p>In this sense, our approach in the ConceptVector system can be viewed as a balanced-or hybrid-case, where both efficiency and interpretability are achieved via a synergetic blending of both human efforts and automated machine computations. That is, our main steps of adding and removing keywords to construct a particular concept are all confirmed by humans, and in this manner, a high quality outcome is maintained. However, our system significantly accelerates these human-guided processes by crucial automated approaches, including word recommendation based on word embedding, followed by word grouping and visual presentation. Also, after users build a specification, this specification is used to build the concept model, which calculates the relevance scores of all words with this particular concept. In this respect, our system represents an illustrative example for properly achieving human-machine collaborations. As it happens, this is also precisely in line with the visual analytics philosophy, where automatic algorithms and visual interfaces create synergies .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION AND FUTURE WORK</head><p>Current text analytics methods are either based on manually crafted human-generated dictionaries or require the user to interpret a complex, confusing, and sometimes nonsensical topic model generated by the computer. In this paper we proposed ConceptVector, a novel text analytics system that takes an visual analytics approach to document analysis by allowing the user to iteratively define concepts with the aid of automatic recommendations provided using word embeddings. The resulting concepts can be used for concept-based document analysis, where each document is scored depending on how many words related to these concepts it contains. We crystallized the generalizable lessons as design guidelines about how visual analytics can help concept-based document analysis. We compared our interface for generating lexica with existing databases and found that ConceptVector enabled users to generate concepts more effectively using the new system than when using existing databases. We proposed an advanced model for concept generation that can incorporate irrelevant words input and negative words input for bipolar concepts. We also evaluated our model by comparing its performance with a crowdsourced dictionary for validity. Finally, we compared ConceptVector to Empath in an expert review.</p><p>The text analysis provided by ConceptVector enables several novel concept-based document analysis, such as richer sentiment analysis than previous approaches, and such capabilities can be useful for data journalism or social media analysis. There are many limitations that ConceptVector does not solve. Among these, the selection/integration of multiple heterogeneous training data according to the target corpus and the automatic disambiguation of multiple meanings of words according to the context are promising avenues of future research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>ConceptVector supports interactive construction of lexicon-based concepts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Workflow of ConceptVector, involving human-and machine-side tasks in a collaborative manner. See Section 5 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 (</head><label>3</label><figDesc>a) shows the top ten categories statistically significantly different from each other (p &lt; .01). For example, Trump mentioned more terms in the 'ugliness' (13.9 odds), 'swearing terms' (6.7 odds), and 'surprise' (5.8 odds) categories, whereas Hillary used more in the 'sexual' (4.97 odds), 'eating' (4.6 odds), and 'home' (4.2 odds) categories. Interestingly, Trump used more casual language while Hillary's tweets contained words related to 'anger' and 'disgust.'<ref type="bibr" target="#b4">5</ref> However, further examination reveals numerous false positives.Figure 3(b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>(b)). For example, Marriott and TripAdvisor form a single cluster owing to the high frequency of words in 'tourism,' 'warmth,' 'sleep,' and 'vacation' mainly because of the use of the words 'hotel' Top 10 categories between Trump and Hillary (b) Detailed analysis of top words in each category Comparison of tweet messages from Hillary Clinton and from Donald Trump during the U.S. 2016 presidential election.The odd ratios of the top 10 categories show differences between the two candidates in (a). The analysis on actual keywords contributing to their corresponding category scores reveals limitations of using the prebuilt lexicon in (b). Red dotted categories do not make sense, because an irrelevant top word is counted dominantly. For example, keywords such as 'bush' in the 'plant' category and 'looking' in the 'hipster' category are not relevant to their categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>PCA 2D projection of NASDAQ 100 companies with their k-means clustering labels color-coded, where the feature vector of each company is computed from its tweets' word count in each of 194 concepts. The clustering using the prebuilt lexica shows some outliers (a), where further investigation of contributing words shows that the company name itself acts trivially as strong signals, such as 'dish' in Dish Network Corporation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Distribution of comments across the 'tidal flooding' (X-axis) and the 'economy' (Y-axis) concepts. A comment that has scored relatively high on both concepts is selected (orange box). The content of the corresponding comment within this dataset is shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Boxplots comparing the three methods in terms of precision, recall, and the size of resulting lexicon. ConceptVector shows the best result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Manuscript received 31 Mar. 2017; accepted 1 Aug. 2017. Date of publication 28 Aug. 2017; date of current version 1 Oct. 2017. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org, and reference the Digital Object Identifier below. Digital Object Identifier no. 10.1109/TVCG.2017.2744478 ConceptVector: Text Visual Analytics via Interactive Lexicon Building using Word Embedding Deokgun Park, Seungyeon Kim, Jurim Lee, Jaegul Choo, Nicholas Diakopoulos, and Niklas Elmqvist, Senior Member, IEEE</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Precision, recall, and average number of keywords per concept for three methods constructing user-defined concepts. The values in parentheses indicate the standard deviation. See Section 6.1.2 for details.</figDesc><table><row><cell>Metrics</cell><cell>ConceptVector</cell><cell>Thesaurus</cell><cell>WordNet [33]</cell><cell>F value</cell><cell>Pr &gt; F</cell></row><row><cell>Precision</cell><cell>0.6363 (0.1701)</cell><cell>0.3099 (0.3773)</cell><cell>0.3794 (0.3637)</cell><cell>5.22 [2, 40]</cell><cell>0.0096</cell></row><row><cell>Recall</cell><cell>0.0789 (0.0308)</cell><cell>0.0333 (0.0385)</cell><cell>0.0275 (0.0242)</cell><cell>5.25 [2, 40]</cell><cell>0.0094</cell></row><row><cell>Average word count</cell><cell>15.6667 (7.4536)</cell><cell>13.8000 (6.0685)</cell><cell>8.2667 (3.3360)</cell><cell>5.40</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://www.wjh.harvard.edu/ inquirer/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://conceptvector.org/#/twitter 4 https://www.kaggle.com/benhamner/clinton-trump-tweets 5 http://graphics.wsj.com/clinton-trump-twitter/ 6 http://www.followthehashtag.com/datasets/nasdaq-100-companies-freetwitter-dataset/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">http://www.nytimes.com/2016/02/23/science/sea-level-rise-globalwarming-climate-change.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">http://nlp.stanford.edu/projects/glove/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">http://developer.nytimes.com/ 10 http://flask.pocoo.org/ 11 http://wordnetweb.princeton.edu/perl/webwn 12 http://www.thesaurus.com/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>Research reported in this publication was partially supported by NIH grant R01GM114267 and the National Research Foundation of Korea (NRF) grant funded by the Korean government (MSIP) (No. NRF-2016R1C1B2015924). Any opinions, findings, and conclusions or recommendations expressed in this article are those of the authors and do not necessarily reflect the views of the funding agencies.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">cite2vec: Citationdriven document exploration via word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcdonough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Seversky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="691" to="700" />
			<date type="published" when="2017-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The use of MMR, diversity-based reranking for reordering documents and producing summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Research and Development in Information Retrieval</title>
		<meeting>the ACM Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="335" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">AppGrouper: Knowledge-based interactive clustering tool for app search results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Intelligent User Interfaces</title>
		<meeting>the International Conference on Intelligent User Interfaces</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="348" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">UTOPIAN: User-driven topic modeling based on interactive nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Termite: Visualization techniques for assessing textual topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Advanced Visual Interfaces</title>
		<meeting>the ACM Conference on Advanced Visual Interfaces</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="74" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">TextFlow: Towards better understanding of evolving topics in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2412" to="2421" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Picking the NYT Picks: Editorial criteria and automation in the curation of online news comments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Diakopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISOJ Journal</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Temporal patterns of happiness and information in a global social network: Hedonometrics and twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Dodds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">M</forename><surname>Kloumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Bliss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Danforth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">26752</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ParallelTopics: A probabilistic approach to exploring document collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ribarsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Visual Analytics Science and Technology</title>
		<meeting>the IEEE Conference on Visual Analytics Science and Technology</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="231" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Framing: Toward clarification of a fractured paradigm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Entman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Communication</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="51" to="58" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Empath: Understanding topic signals in large-scale text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Human Factors in Computing Systems</title>
		<meeting>the ACM Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4647" to="4657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the ACM Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ThemeRiver: visualizing thematic changes in large document collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Havre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hetzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nowell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="20" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visual text analysis in digital humanities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jänicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Franzini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Cheema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Scheuermann</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.12873</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="page">2016</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Principal Component Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">T</forename><surname>Jolliffe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Estimating temporal dynamics of human emotions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lebanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="168" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Local context sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lebanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2260" to="2266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Beyond sentiment: The manifold of human emotions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lebanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="360" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Axisketcher: Interactive nonlinear axis mapping of visualizations through user drawings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Endert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="230" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">VisOHC: Designing visual analytics for online health communities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="80" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning the parts of objects by non-negative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">401</biblScope>
			<biblScope unit="page" from="788" to="791" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">iVisClustering: An interactive visual document clustering via topic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kihm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3pt3</biblScope>
			<biblScope unit="page" from="1155" to="1164" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Two/too simple adaptations of Word2vec for syntax problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Trancoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1299" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Not all contexts are created equal: Better word representations with variable attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fermandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1367" to="1372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Introduction to Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schutze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>abs/1309.4168</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Advances in Neural Information Processing Systems</title>
		<meeting>the Conference on Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">WordNet: A lexical database for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Supporting comment moderators in identifying high quality online news comments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sachar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Diakopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Human Factors in Computing Systems</title>
		<meeting>the ACM Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1114" to="1125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Gatherplots: Extended scatterplots for categorical data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
		<idno>HCIL-2016-10</idno>
		<imprint>
			<date type="published" when="2016" />
			<pubPlace>College Park</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Maryland</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Linguistic inquiry and word count: LIWC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Pennebaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Booth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Lawrence Erlbaum Associates</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">The General Inquirer: A Computer Approach to Content Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Dunphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Ogilvie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Lexiconbased methods for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Taboada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brooke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tofiloski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Voll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stede</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="267" to="307" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Database of NIH grants using machine-learned categories and graphical clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Talley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Herr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">I</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Leenders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="443" to="444" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A probabilistic model for learning multi-prototype word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics</title>
		<meeting>the International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">TIARA: a visual exploratory text analytic system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the ACM Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="153" to="162" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
