<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TVCG.2017.2744684</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>natural language</term>
					<term>interaction</term>
					<term>language pragmatics</term>
					<term>visual analytics</term>
					<term>ambiguity</term>
					<term>feedback</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Interactive visual data analysis is most productive when users can focus on answering the questions they have about their data, rather than focusing on how to operate the interface to the analysis tool. One viable approach to engaging users in interactive conversations with their data is a natural language interface to visualizations. These interfaces have the potential to be both more expressive and more accessible than other interaction paradigms. We explore how principles from language pragmatics can be applied to the flow of visual analytical conversations, using natural language as an input modality. We evaluate the effectiveness of pragmatics support in our system Evizeon, and present design considerations for conversation interfaces to visual analytics tools.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Starting with an initial utterance (a), our system Evizeon supports references to properties in a visualization (b), within the text (c), and through multi-modal interaction (d). Other forms of interaction include support for compound queries (e) and lexical cohesion (f), where a user may use semantically similar words to describe attributes in the visualization, such as 'deadliest' for the attribute 'fatalities.'</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A well accepted principle in visual analytics is the need to support interactive exploration and iterative view refinement. A single static visualization is rarely sufficient except in the simplest of investigative tasks. The user often needs to interact with their data, iteratively evolving both the questions and the visualization design. Our research explores natural language interaction as a complementary input modality to traditional mouse and touch based interaction for visual analytics.</p><p>Direct manipulation is an effective interaction technique when one can easily point to the objects of interest (e.g., lassoing a cluster of points). However, mouse and touch interaction can be inefficient when the interface requires many steps to complete a task. Moreover, this form of interaction gets in the way when users cannot translate their data related questions into data attributes or visual variables to manipulate <ref type="bibr" target="#b15">[19]</ref>. In contrast, natural language (NL) interaction can offer numerous advantages in terms of ease of use, convenience, and accessibility to novice users, facilitating the flow of analysis for novices and experts alike <ref type="bibr" target="#b15">[19,</ref><ref type="bibr" target="#b31">35]</ref>. These make compelling arguments to investigate NL as a complementary interaction modality for visual analytics.</p><p>However, NL interaction techniques for visualization are in their infancy, and existing tools (e.g., DataTone <ref type="bibr" target="#b13">[17]</ref>, Articulate <ref type="bibr" target="#b22">[26,</ref><ref type="bibr" target="#b33">37]</ref>) largely follow a single query -response paradigm, with some facility to correct system misunderstandings. While promising, these systems do not really support a "cycle of visual analysis"; an interface that requires isolated independent queries cannot be expected to support the fluid iterative exploration and refinement that we expect in visual analytics. Our work introduces new techniques for NL interaction, based on a conversational interaction model. <ref type="figure" target="#fig_0">Figure 1</ref> shows results of natural language interaction using a conversational approach, where the user has a back-and-forth exchange with our system. The first query in <ref type="figure" target="#fig_0">Figure 1a</ref>, "measles in the uk" causes all charts to highlight or filter to cases of measles in the United Kingdom. Here 'uk' is not in the dataset, but is interpreted as an abbreviated place name and automatically matched to the data value 'United Kingdom' using a geographic corpus. The user then types "show me the orange spike" <ref type="figure" target="#fig_0">(Figure 1b)</ref>; the system understands that this is a reference to the visual properties of the line chart and adds detail information to the spike in the line. In <ref type="figure" target="#fig_0">Figure 1c</ref>, the system interprets "mumps over there" as containing a reference to the previous location and a different value in the disease attribute. It retains the filter on 'United Kingdom' but updates disease from 'measles' to 'mumps'. "Epidemics here" <ref type="figure" target="#fig_0">(Figure 1d</ref>) is a reference to marks selected on the map with a mouse, so epidemic diseases in that selected region are highlighted. In <ref type="figure" target="#fig_0">Figure 1e</ref>, "measles epidemic in malawi congo angola" illustrates a conjunctive query involving multiple search criteria. Finally, the user asks for "deadliest" <ref type="figure" target="#fig_0">(Figure 1f</ref>). Here the word 'deadliest', even though not present anywhere in the dataset, is matched to top 10% of fatalities using a semantic similarity match with external knowledge corpora. Throughout this exchange, the user has been able to build on their prior queries and adapt the current system state, rather than starting over each time with a fully qualified input statement.</p><p>The interaction sequence in <ref type="figure" target="#fig_0">Figure 1</ref> involves a back-and-forth information exchange akin to human conversations. In human conversation, a turn is identified as a basic unit of dialog, denoted as an utterance <ref type="bibr" target="#b17">[21]</ref>. Human utterances are very often incomplete or imprecise, relying on the listener to interpret using their contextual knowledge (speaker, topic, time, location, past utterances, etc.). These tendencies carry over into interactions with a visualization, where it is known that people use ambiguous language and partial specification, and may refer to items in their past statements <ref type="bibr" target="#b13">[17,</ref><ref type="bibr" target="#b15">19,</ref><ref type="bibr" target="#b31">35]</ref>.</p><p>The importance of supporting idiosyncrasies of human language became blatantly apparent to us in our work on Eviza <ref type="bibr" target="#b31">[35]</ref>, an early NL interface to visualizations. Based on our user study, plus an analysis of over 3000 example input utterances, we identified the need to support synonyms, compound queries, dependencies on prior queries, pronouns, references to visual properties like mark size and color, and multimodal input. Eviza supported only a few of these language characteristics and only with a very simple model. For example, it supported synonyms only through stemming or partial string matching. Similarly, people appreciated being able to enter follow-on queries, but system behavior in these situations was often unexpected, pointing to a need to better understand when the system should remember information from past queries and when it should start fresh. An effective NL system needs to interpret input in the context of the current system state and the user's recent interactions. Furthermore, it must consider all possible contexts that the user might intend and enable the user to correct misinterpretations. In visual analytics, relevant context primarily consists of semantic properties of the data set (attributes and values currently in play) and characteristics of the visualization (visual properties and encodings). Our work exploits an understanding of human conversations as a basis for algorithms that can infer such contextual information. Pragmatics is a term used in linguistics to determine reference of utterances through context of use <ref type="bibr" target="#b16">[20]</ref>. To realize a pragmatic language for interacting with visual analytics, we utilize and extend a model commonly used in linguistic conversational structure <ref type="bibr" target="#b17">[21]</ref>. We leverage relationships between entities and analytical actions that exist between utterances in conversation.</p><p>Inspired by Pokémon's etymology for name origins <ref type="bibr" target="#b3">[5]</ref>, we call our new system Evizeon, with 'eon' meaning evolution. Like Eviza, Evizeon enables natural language interaction with visualizations. However, Evizeon introduces a series of techniques to support conversational pragmatics, deeply enriching the interaction experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Contributions</head><p>Realizing interactive conversations like in <ref type="figure" target="#fig_0">Figure 1</ref> requires understanding language pragmatics and adapting conversational interfaces to data analytics. Unlike general search interfaces, visual analytics tools can take advantage of their knowledge of data attributes, values, and data related expressions to do a better job of inferring a user's meaning. Towards this end, the contributions of our paper are as follows:</p><p>• We introduce a theoretical framework based on pragmatics that can improve NL interaction with visual analytics. We propose an extension to the centering approach employed in pragmatics theory, to support inter-sentential transitional states of continuing, retaining, and shifting the context of the data attributes in play. • We demonstrate techniques for deducing the grammatical and lexical structure of utterances and their context. Based on our framework, we support various pragmatic forms of NL interaction with visual analytics. These include understanding incomplete utterances; referring to entities within the utterances and visualization properties; supporting long, compound utterances; identifying synonyms and related concepts; and 'repairing' responses to previous utterances. • We provide appropriate visualization responses either within an existing visualization or when necessary, by creating new visualizations. We support pragmatic ambiguity through targeted textual feedback and ambiguity widgets. • We then validate the usefulness of language pragmatics in visual analytic conversations with two user studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Designing natural language interfaces can be challenging as they need to successfully interpret unconstrained input <ref type="bibr" target="#b34">[38]</ref>. Such systems often use deep expert modeling to extract information necessary for an appropriate interpretation. When users stray outside the supported domain, the system must still be able to respond to a broad range of user inputs. This requires a system to ask for user clarification when faced with unexpected input and to learn from these clarifications. The Persona project was one such interface, but with a limited task domain of selecting music from a database <ref type="bibr" target="#b6">[10]</ref>. There has also been a body of research focusing on conversational interfaces, deducing human intent through gaze, turn-taking and dialog structure <ref type="bibr" target="#b5">[9,</ref><ref type="bibr" target="#b9">13,</ref><ref type="bibr" target="#b12">16]</ref>. More recently, NL interfaces for data analysis have emerged as a promising new way of performing analytics. This approach is promising in maintaining conversational flow, as users may be able to express their questions more easily in natural language rather than translating them to system commands. Existing commercial systems <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b2">4,</ref><ref type="bibr" target="#b4">8]</ref> have fundamental limitations. Most return a minimally interactive visualization in response to queries, meaning the answer needs to be exactly correct rather than approximate. Many require experts to perform modeling before the systems are effective. None are richly integrated with a self-service analysis tool in a manner that allows NL interactions to become part of a richer visual cycle of analysis. Research systems have similar limitations. RIA explored geo-referenced data on a map with simple queries <ref type="bibr" target="#b36">[40]</ref>. Articulate generated visualizations based on simple queries with limited pragmatics and feedback <ref type="bibr" target="#b22">[26]</ref>.</p><p>Studies show that systems where users are expected to always employ syntactically and semantically complete utterances can often be frustrating <ref type="bibr" target="#b8">[12]</ref>. DataTone <ref type="bibr" target="#b13">[17]</ref> improved analysis flow by guessing the user's intent, producing a chart according to that guess, and then providing ambiguity widgets through which the user could adjust settings if the system's guess was incorrect. Eviza <ref type="bibr" target="#b31">[35]</ref> was a first step towards supporting simple pragmatics in analytical interaction. The system used contextual inferencing for supporting pragmatics, wherein context established by the preceding dialog is used to create a complete utterance <ref type="bibr" target="#b29">[33]</ref>. A related system, Analyza <ref type="bibr" target="#b11">[15]</ref>, similarly enabled follow-up data queries, but without a visualization focus. These systems recognized the importance of providing feedback on how the system interprets queries and enabling users to correct misunderstandings.</p><p>While the use of pragmatics helps with analytical flow, investigations into this approach so far have been very preliminary. To be truly interactive, these systems need richer support for understanding queries based on syntactic and semantic language structure, particularly tied to the analytical properties of the questions. We also need better criteria for deciding when to remember information from prior queries, and enhanced flexibility for users to correct poor system choices. In this work, we explore how a pragmatics-based approach can enable flexible interactions with data that support the flow of visual analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRAGMATICS</head><p>Visual analysis is most effective when users can focus on their data rather than the analysis tool's interface. Pragmatics is particularly important for visual analysis flow, where questions often emerge from previous questions and insights. Eviza supported pragmatics using a simple finite state machine (FSM) with the states defined by a weighted probabilistic distribution over the parse paths <ref type="bibr" target="#b31">[35]</ref>. The resolution of phrasal segments occured using previous parse paths to maximize likelihood estimation. Studies showed that this approach is rather naïve and we need a more systematic way of understanding when context from previous utterances inform user intent.</p><p>In this work, we apply principles of pragmatics by modeling the interaction behavior as a conversation. Conversations are more than mere sequences of utterances. For a sequence of utterances to be a conversation, it must exhibit coherence. Coherence is a semantic property of conversation, based on the interpretation of each individual utterance relative to the interpretation of other utterances <ref type="bibr" target="#b35">[39]</ref>.</p><p>In order to correctly interpret a set of utterances, we utilize and extend a model commonly used for discourse structure called conversational centering <ref type="bibr" target="#b17">[21]</ref>. In this model, utterances are divided into constituent discourse segments, embedding relationships that may hold between two segments. A center refers to those entities serving to link that utterance to other utterances in the discourse. Consider a discourse segment DS with utterances U 1 ...U m . Each utterance U n (1≤n&lt;m) in DS is assigned a set of forward-looking centers, C f (U n , DS) referring to the current focus of the conversation; each utterance other than the segment's initial utterance, is assigned a set of backward-looking centers, C b (U n , DS). The set of backward-looking centers of a new utterance U n+1 is C b (U n+1 , DS), which is equal to the forward-looking centers of U n (i.e., C f (U n , DS)). In the context of visual analytic conversations, forward and backward-looking centers consist of data attributes and values, visual properties, and analytical actions (e.g., filter, highlight).</p><p>Each discourse segment exhibits both global coherence i.e., the global context of the entire conversation, usually referring to a topic or subject of the conversation, and local coherence i.e., coherence amongst the utterances within that conversation. The global coherence of the analytical conversation is updated when all the data entities and their pronomial referring expressions shift to a new set of entities. Local coherence refers to inferring a sequence of utterances within a local context through transitional states of continuing, retaining, and replacing between C f (U n , DS) and C b (U n , DS). We extend this conversational centering theory for visual analytical conversation by introducing a set of rules for each of these local coherence constructs.</p><p>Given an utterance U n , Evizeon responds by executing a series of analytical functions derived from the forward-looking centers C f (U n , DS).</p><p>Here, an analytical function F(X, op, v) consists of a variable X which can be an attribute or a visualization property, an operator op, and a value v (usually a constant). For example, when the user says "measles in the uk," the system creates two functions namely F CAT(diseases, ==, measles) and F CAT(country, ==, uk). As the user provides a new utterance U n+1 , the system first creates a set of temporary centers C temp (U n+1 , DS) from U n+1 without considering any previous context. We then apply the following set of rules to create a set of forwardlooking centers, C f (U n+1 , DS) based on some set operations between C b (U n+1 , DS) and C temp (U n+1 , DS). These forward-looking centers are then used by Evizeon to respond to the user utterance:</p><p>Continue: This is a transition that continues the context from the backward-looking center to the forward-looking one. Hence, C b (U n+1 , DS)∈C f (U n+1 , DS), along with other entities.</p><p>This transition occurs when a variable X is in C temp (U n+1 ) but not in C b (U n+1 , DS). In this case, the system performs the following union operation:</p><formula xml:id="formula_0">C f (U n+1 , DS)=C b (U n+1 , DS) ∪C temp (U n+1 , DS).</formula><p>Retain: This transition retains the context from the backwardlooking center in the forward-looking one without adding additional entities to the forward-looking one.</p><formula xml:id="formula_1">C b (U n+1 , DS)=C f (U n+1 , DS).</formula><p>This transition triggers when the variable X is in</p><formula xml:id="formula_2">C b (U n+1 , DS) but not in C temp (U n+1 , DS).</formula><p>Shift: In this transition, the context shifts from the previous one, with</p><formula xml:id="formula_3">C f (U n+1 , DS) =C b (U n+1 , DS).</formula><p>This transition occurs when the variable X is in both C b (U n+1 , DS) and C temp (U n+1 , DS) but the corresponding values are different. In this case, the system replaces all the backward-centers C b (U n+1 , DS) containing X with C temp (U n+1 , DS). This transition also occurs when a filter constraint is removed; e.g., removing a widget for measles shifts the disease variable from measles to all diseases.</p><p>Referring to <ref type="figure" target="#fig_0">Figure 1</ref>, we illustrate the use of these different types of transition rules in this analytical conversation snippet between a user, Sara and our system, Evizeon: measles epidemic in malawi congo angola. </p><formula xml:id="formula_4">SHIFT C b ={mumps}, C f ={measles, epidemic} SHIFT C b ={uk}, C f ={malawi,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SYSTEM OVERVIEW</head><p>Evizeon, similar to Eviza, has an autocompletion module informed by the grammar containing both predefined rules as well as rules dynamically added based on the data attributes in the visualization. Evizeon's pragmatics module is significantly extended from Eviza's simple FSM to keep track of transition states between utterances. This module responds by executing a series of analytical functions derived from the forward-looking centers of the pragmatic model. These analytical functions are executed by communicating with a data manager that reads in the requested data from the data files.</p><p>To enhance the support for pragmatics, Evizeon introduces two modules -the visualization manager and dashboard controller. The former manages access to visualization properties including marks, colors, axes, and text. It enables query references to existing visualizations and determines when to generate new visualizations. The latter decides how each view should respond to a query by analyzing the attributes and their visual encodings. The event manager handles consistency across the presentation elements in the visualization when there is a change in state upon execution of the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">FORMS OF PRAGMATIC INTERACTION</head><p>Conversation centering posits that utterances display connectedness between them. The manner in which these utterances link up with each other to form a conversation is cohesion. Cohesion comes about as a result of the combination of both lexical and grammatical structures in the constituent phrases. Identifying phrase structure is a logical starting point to resolve that utterance into one or more analytical functions applied to the visualization. A probabilistic grammar is applied to provide a structural description of the input queries, similar to the approach in Eviza <ref type="bibr" target="#b31">[35]</ref>. We deduce additional syntactic structure by employing a Part-Of-Speech Tagger <ref type="bibr" target="#b24">[28]</ref>. Entities from the parsed output are resolved to corresponding categorical and ordered data attributes <ref type="bibr" target="#b28">[32]</ref>. By applying our framework on conversation structure, we demonstrate various forms of pragmatics in analytical conversation <ref type="bibr" target="#b18">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ellipsis</head><p>Incomplete utterances are common in conversation. Ellipses are syntactically incomplete sentence fragments that exclude one or more linguistic elements. Often, these utterances cannot be understood in isolation, but rather with previously established context. Eviza supports ellipsis by resolving incomplete utterances based on maximum likelihood estimation of the probabilities of the previous parse states <ref type="bibr" target="#b31">[35]</ref>. This model is rather simplistic and does not incorporate any logic for determining when to retain or replace parse states. Evizeon applies rules based on the centering model (Section 3) to determine context for the incomplete utterance. The conversation below shows how an incomplete utterance "townhomes" is understood in the context of the previous utterance, and is shown in <ref type="figure" target="#fig_2">Figure 2</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Referencing</head><p>Referring expressions help unify text and create economy, preventing unnecessary repetition. Halliday and Hassan state that referencing is a conversation form, which instead of being interpreted semantically in its own right, makes reference to something else for its interpretation <ref type="bibr" target="#b18">[22]</ref>. When the interpretation is within the text, this is known as anaphoric referencing. In visual analytics interaction, the reference pertains to data attributes and analytical functions. The conversation below shows how John references the year '2015' when asking for prices in the year 2014. The system computes the date for 'previous' using a temporal function DATECALC <ref type="figure" target="#fig_4">(Figure 3</ref>  We first identify the anaphora in the utterance, such as 'that', 'those', 'them', 'ones', 'previous', 'next.' We then look at the phrasal chunk containing that reference to identify what entities it is referring to. Next, we search through the backward-looking centers C b (U n , DS) to find such entities and replace the anaphoric reference with these entities. After an anaphoric resolution is performed, we apply the rules for updating the forward-looking centers as described in Section 3. For instance, in the above conversation Evizeon identifies that 'previous' is followed by 'year', therefore it finds the value of year in C b (U n , DS). Consider another example, "Show fremont, queen anne, and ballard" followed by "condos in those districts"; here those is refers to some values (i.e., fremont, queen anne, and ballard) of the attribute neighborhood as indicated by the word districts.</p><formula xml:id="formula_5">previous year. SHIFT C b ={2015}, C f ={DATECALC('year', -1, C b ={2015})</formula><p>Note that the references may not always refer to values of a data attribute; they may refer to actions that need to be executed by the system. For instance, consider the utterance "filter out ballard" followed by "do that to fremont." Here, that is not immediately followed by any noun but immediately preceded by a verb word 'do' from which we look at the action mentioned in the previous utterance i.e., 'filter out.'</p><p>Another form of referencing lies outside the text, and in the context of the visualization. Here, the forward-looking center C f references context within the visualization as opposed to text in the backwardlooking center C b . This form of indirect referencing is of two types: (1) A deictic reference refers to some object in the environment, usually by pointing. We support deictic references by enabling multimodal interaction (mouse + speech/text), as shown in <ref type="figure" target="#fig_0">Figure 1d.</ref> (2) A visualization property reference uses properties in the visualization such as color, shape, or text in labels, axes and titles. References to textual properties is supported through computing string edit distances between the input tokens and the labels [2]. For supporting references to mark colors, we use the Berlin &amp; Kay <ref type="bibr" target="#b7">[11]</ref> 11 basic color terms and their associated values. If one of these color terms is in the utterance, we compute a Euclidean color distance in CIELAB <ref type="bibr" target="#b32">[36]</ref> to match the term to the closest color value in the marks. Mapping these references to actual mark types is further disambiguated by rules in the grammar that semantically describe the charts. For example, in <ref type="figure" target="#fig_0">Figure 1b</ref>, Evizeon searches for the closest color to 'orange' in the line chart identified by the token 'spike.' If a referred mark is present in multiple visualizations, the results are shown in all the corresponding ones.</p><p>Eviza only supports deictic referencing as its grammar and FSM do not resolve attributes between forward and backward-looking centers. Conjunctions in utterances communicate a range of relations between phrase fragments called conjuncts. In a conversation, people tend to iteratively build a compound query by adding multiple conjuncts as seen in <ref type="figure" target="#fig_0">Figure 1d</ref>. They often avoid explicit use of conjunctions and connectors, such as 'and', 'or', 'also' between the phrases <ref type="bibr" target="#b21">[25]</ref>. For instance, consider this conversation construct for "houses in Ballard under 600k last summer" <ref type="figure" target="#fig_5">(Figure 4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Conjunctions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Linearization</head><p>Finding implicit data coherence among the conjuncts, can be a challenging task. In the example above, all the conjuncts refer to the same entity 'houses in Ballard.' However, there are cases where conjuncts map to different entities, for instance "houses in Ballard under 600k condos in South Lake Union." The system needs to determine how individual conjuncts resolve to the same or different entities. We devised a rule-based technique that takes a potentially long utterance with possibly implicit conjunctions and translates into a set of analytical functions chained together by logical operators. Evizeon then executes these analytical functions in response to the user utterance.</p><p>Multiple conjuncts within these compound utterances need to be resolved to correctly invoke one or more corresponding analytical functions; A process called linearization <ref type="bibr" target="#b20">[24]</ref>. As mentioned earlier, an analytical function F(X, op, v) consists of a variable X (e.g., an attribute), an operator op, and a value v. Each attribute can be of two types: categorical and ordered <ref type="bibr" target="#b28">[32]</ref>. The ordered data type is further categorized into ordinal and quantitative. The linearization process considers the types of attributes and operators to combine analytical functions using logical operators (i.e., ∧, ∨) as described below:</p><p>Applying the ∨ operator: When two or more adjacent conjuncts share an attribute and if that attribute's data type is categorical, then these conjuncts are connected by ∨. Similarly, if that shared attribute is ordered and the function's operator is ==, we apply ∨. Notice that in such cases, ∨ is logically appropriate because applying ∧ would not match to any item in the data table.</p><p>For example, if the utterance is "show me condos and townhomes," then the system generates the following combination of analytical functions: (F CAT(homeType, ==, condo) ∨ F Cat(homeType, ==, townhome)). Here, both 'condo' and 'townhome' belong to the same categorical attribute, i.e., homeType. Applying ∧ operator would not make sense here because a particular house (item) cannot be both 'condo' and 'townhome' at the same time. Similarly, if the user utters "2 3 bedroom houses", the system generates (F ORDINAL(bed, ==, 2) ∨ F ORDINAL(bed, ==, 3)).</p><p>The ∨ operator is also appropriate if attribute type is ordered and involves the condition X&lt;v 1 and X&gt;v 2 , where v 1 &lt;v 2 . For instance, if the utterance is "before 2013 and after 2014", then the ∨ operator will be used between the two conjuncts. Again, here applying the ∧ operator would result in matching no item in the data table.</p><p>Applying the ∧ operator: The ∧ operator is appropriate if attribute type is ordered and involves the condition X&gt;v 1 and X&lt;v 2 , where v 1 &lt;v 2 .</p><p>For example, "houses over 400k and under 700k" resolves to (F NUMERIC(price, &gt;, 400000) ∧ F NUMERIC(price, &lt;, 700000)). "Beds between 2 to 4" resolves to (F ORDINAL(beds, &gt;=, 2) ∧ F NUMERIC(beds, &lt;=, 4)). Notice that applying ∨ operator would result in matching to all items in the data table, which would not make sense.</p><p>Finally, the ∧ operator is applied when there is no common attribute between two conjuncts. For example, the utterance "price under 600k with 2 beds" resolves to (F ORDINAL(beds, ==, 2) ∧ F NUMERIC(price, &lt;=, 600000)).</p><p>In order to generate the analytical function representation of the whole utterance, we traverse the corresponding parse tree (generated by the parser described in <ref type="bibr" target="#b31">[35]</ref>) in post-order and apply the above two rules iteratively on the phrases as illustrated in <ref type="figure" target="#fig_6">Figure 5</ref>. Here, the system takes the utterance "condos under 600K townhomes under 1M" as input, and iteratively applies the above rules to generate the chain of analytical functions. The previous three types of pragmatics -ellipsis, referencing, and conjunction, provide grammatical cohesion to the conversation. In addition to these grammatical constructs, people often find ways for expressing concepts through related word meanings, i.e., senses in conversation, a term called lexical cohesion <ref type="bibr" target="#b27">[31]</ref>. Eviza supports basic lexical cohesion such as spelling, stemming, plurality variations (e.g., 'profit' and 'profits') and synonyms (e.g., 'country' and 'nation'),</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Lexical Cohesion</head><p>Often word senses are related to each other within a semantic context <ref type="bibr" target="#b26">[30]</ref> (e.g., 'violence' and 'crime'). We identify attribute word senses by employing the word2vec model containing learned vector representations of large text corpora <ref type="bibr" target="#b25">[29]</ref>. We compute word vectors using a recurrent neural network <ref type="bibr" target="#b19">[23]</ref>. The semantic relatedness S rel between a word w i in a given utterance and a data attribute d j , is the maximum value of a score computed as follows:</p><formula xml:id="formula_6">S rel (w i , d j )=max m,n λ cos(v w i , v d j ) + (1 − λ ) 1 dist(S i,m , S j,n )<label>(1)</label></formula><p>where dist(S i,m , s j,n ) is the Wu-Palmer distance <ref type="bibr" target="#b37">[41]</ref> between the two senses S i,m , S j,n . v w i , v d j are the vector representations of w i and that d j respectively. λ is a weighting factor applied to a pairwise cosine distance between the vectors. Other natural language queries that users might ask are "show me the cheapest houses near Ballard" or "where are the mansions in South Lake Union?" We not only have to compute semantic relatedness between these terms and data attributes, but also compute the type of analytical function associated with each term. We consider the corresponding dictionary definitions <ref type="bibr" target="#b0">[1]</ref> as additional features to these word vectors, and check if the definitions contain quantitative adjectives such as 'less', 'more', 'low', 'high.' Appropriate analytical func- tions are then mapped to these adjectives. For example, <ref type="figure" target="#fig_7">Figure 6</ref> shows 'most expensive' mapping to Top-N(sale price), 'cheapest' to Bottom-N(sale price). Similarly, <ref type="figure" target="#fig_0">Figure 1f</ref> shows 'deadliest' mapped to the Top-N values of the attribute 'fatalities.'</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Repair Utterances</head><p>In a natural conversational flow, it is quite common for people to correct or clarify a previous utterance. In addition to widgets, we also support the use of follow-up repair utterances to 'repair' a potentially ambiguous utterance or to change the default behavior of how the results are presented to the user. For instance, if the user would like to update the default behavior of the system, such as highlighting for selection, she can use utterances like "no, filter instead", or she can update attributes (e.g., "get rid of condo" or "change from condo to townhomes") as shown in <ref type="figure" target="#fig_8">Figure 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RESPONSE AND FEEDBACK</head><p>To support a conversation, the visualizations need to provide cohesive and relevant responses to various utterances. Sometimes the system needs to respond by changing the visual encoding of existing visualizations, while in other cases it is necessary to create a new chart to support the visual analytical conversation more effectively. In addition to appropriate visualization responses, it is critical to help the user understand how the system has interpreted her utterance by producing appropriate feedback and allowing her to rectify the interpretation if necessary. We devised a feedback mechanism that helps the user to interpret the system's response and subsequently modify the actions made by Evizeon through some interface controls.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Responses Within Existing Visualizations</head><p>In a traditional dashboard, users can interact by selecting items or attributes in a visualization that are highlighted to provide immediate visual feedback <ref type="bibr" target="#b28">[32]</ref>. Simultaneously, other charts are updated by highlighting/filtering out items. In a natural language interface, instead of making explicit selection by mouse/keyboard, the user mentions different attributes and values, making it a non-trivial task of deciding how each view within a dashboard should respond to the utterance. Previous natural language interfaces (e.g., Eviza <ref type="bibr" target="#b31">[35]</ref>) have not explored this problem as they have only focused on a single visualization.</p><p>To decide how the views (V ) in a dashboard should respond to the utterance, our approach is as follows. If the items in the results set retrieved by applying the analytical functions are directly encoded in a chart, then Evizeon highlights these items. In <ref type="figure" target="#fig_0">Figure 1a</ref>, the map chart highlights the items that match the criteria "measles in the uk." However, for a secondary chart that applies further data transformation on the result set (e.g., the line chart and two bar charts), the following rules are applied: The system first creates a list of all attributes {X 1 , X 2 , ...X m } from the forward looking centers C f (U n+1 , DS). It then invokes the visualization manager to determine if any of these attributes are encoded as dimensions of that visualization V directly (i.e., without using any aggregate functions such as count or average). If that is the case, it highlights the marks related to the corresponding criteria. For example, in <ref type="figure" target="#fig_0">Figure 1a</ref> , Evizeon highlights the series in the line chart and the bar in the barchart representing 'measles.' However, the bar chart on impact (lower right) cannot highlight any mark because it does not encode any attribute in {X 1 , X 2 , ...X m }. Therefore, it filters out the results that do not match the criteria "measles in the uk" and updates the chart accordingly. Note that users can change the default behavior by explicitly expressing the choice about whether to filter vs. highlight (e.g., 'exclude', 'remove', 'filter only').</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Creating New Visualizations</head><p>During visual analysis flow, there may be situations where the existing visualization cannot meet the evolving information needs of the user. This scenario could arise when a particular data attribute cannot be encoded effectively in the existing visualization (e.g., time values in a map). We hypothesize that in such as cases, creating a new visualization as a response could be useful to the user. We draw inspiration from work that connects the visualization and language specification <ref type="bibr" target="#b23">[27,</ref><ref type="bibr" target="#b30">34]</ref>. Our current implementation supports the creation of four different types of visualizations (i.e., bar chart, line chart, map chart, and scatterplot). <ref type="figure" target="#fig_9">Figure 8</ref> shows how a dashboard is progressively constructed based on the input utterances.</p><p>The underlying algorithm for creating or changing an existing visualization works in following steps: First, the system determines if the creation of a new visualization or change of an existing one is necessary. Evizeon analyzes the attributes specified in the forward-looking centers C f (U n+1 , DS), and searches for any current visualization that encodes these data properties. If there is no match with the specification of existing visualizations, the system generates the corresponding new specification consisting of attributes and aggregation types. We employ a simplified version of the automatic presentation algorithm described in <ref type="bibr" target="#b23">[27]</ref> to decide the type of chart generated based on this specification. Finally, the new chart is positioned using a two-dimensional grid-based layout algorithm, automatically coordinated with other views. This updated dashboard responds to subsequent utterances through actions like highlighting or filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Ambiguity Handling</head><p>The interactive dialog provides new challenges for natural language understanding systems. One of the most critical challenges is simply determining the intent of the utterance. These resolutions are expressed in the form of widgets and feedback to help the user understand the system's intent and the provenance of how the utterance was interpreted. By manipulating these widgets and viewing the feedback of what results are shown in the visualization, the user can instantiate a follow-up repair utterance to override or clarify the system decisions made.</p><p>Widgets are identified from the analytical functions derived from an utterance. An important design consideration here is how can we organize and present the widgets in an intuitive way so that the user can understand how the system interprets her utterance and subsequently modify the interpretation using these widgets. For this purpose, we take the original utterance and order the widgets in the same sequence as the corresponding query terms. We achieve this by using the library Sparklificator that facilitates the placement of small word-scale visualization within text in a compact way <ref type="bibr" target="#b14">[18]</ref>. In addition, we provide a set of interactions to users including the ability to manipulate and remove a widget to modify the query and resolve ambiguous ones. <ref type="figure">Figure 9</ref> shows how Evizeon presents the widgets for the utterance "condo near Ballard under 1.2M." Here the first term 'condo' was resolved to the widget representing the criteria 'HOME TYPE equals Condo/coop'. Then, the second widget conveys the fuzzy distance represented by 'near Ballard.' Finally, since 'under 1.2M' does not explicitly mention any attribute, the system determines whether the value 1200000 is within the range of minimum and maximum values of any numeric attribute in the data. If such an attribute exists (LAST SALE PRICE in this case), the system conveys that to the user and then allows her to change the attribute using the drop-down menu.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Textual Feedback</head><p>In addition to ambiguity handling, Evizeon also provides feedback to modify the text when it fails to understand the query. For instance, if the system cannot successfully parse the given utterance, it attempts to automatically correct the misspelled terms by comparing the tokens with the attributes in the current dataset using fuzzy string matching [2]. In cases of partially recognized queries, the system prunes the unrecognized terms and shows results based on the recognized tokens. <ref type="figure" target="#fig_0">Figure 10</ref> presents examples of different possible cases and the corresponding feedback generated by Evizeon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">EVALUATION</head><p>We conducted two user studies to: (1) verify the usefulness of our pragmatics and feedback, (2) identify missing behavior, and (3) explore individual differences. Study 1 examined transition types -continuing, retaining, and shifting. Study 2 was observational and aimed to see how people would utilize the pragmatics mechanisms in practice.</p><p>We chose an observational study to explore pragmatics challenges in an open-ended way. We considered a comparative experiment; however, our prior study of Eviza <ref type="bibr" target="#b31">[35]</ref> had already revealed its limited pragmatics support: it could not handle conjunctions, its model for ellipsis was simplistic, and its lack of feedback was highly confusing. Pilot studies had shown that Eviza could not handle an open-ended multi-step conversation, which is why we only tested controlled independent tasks. Comparing Eviza with Evizeon would only highlight the lack of pragmatics in the former system rather than leading to new insights. The goal of the Evizeon studies was to evaluate usefulness of the new pragmatics features built around conversation centering. <ref type="figure">Fig. 9</ref>. Given the utterance "condo near Ballard under 1.2M", the system conveys how it interprets this query by presenting three different widgets. <ref type="figure" target="#fig_0">Fig. 10</ref>. Examples of feedback implemented in Evizeon: A) The utterance was understood and results were found, B) Not successfully understood, but the system suggested an alternative query, C) The utterance was partially understood and the unrecognized terms were highlighted, D) The utterance was understood but no results were found. E) The utterance was not understood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1">Participants</head><p>We recruited 21 volunteers from Tableau (17 males, 4 females, age 24-65+). All were fluent in English (18 native speakers) and all but 5 regularly used NL interfaces such as Siri and Alexa. Twelve participated in Study 1 and 9 in Study 2, randomly assigned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2">Tasks</head><p>Each participant used one of two dashboards (housing sales or disease outbreaks). Studies 1 and 2 differed in the assigned task:</p><p>Study 1 -Target Criteria Tasks: Here we provided target data values; participants manipulated the visualization (through NL or ambiguity widget input) to reveal those data. Tasks were grouped into sets with related criteria (3 sets of 4 tasks), to force participants through the various transition types. We created four task types: (1) Add: new attribute(s) are added to the context (continuing), (2) Change: value(s) of existing attribute(s) are changed (shifting), (3) Remove: some attribute(s) are removed, retaining others, and (4) Mixed: a mixture of (1-3). Each set started with a new query (following a reset) and each of the remaining tasks transitioned the criteria through one of the task types. To avoid priming participants with specific wording, criteria were presented visually (see <ref type="bibr">[6,</ref><ref type="bibr">7]</ref> and supplemental material).</p><p>Study 2 -Open-Ended Tasks: Here our goal was to qualitatively observe how people would use pragmatics in an unscripted interaction. Participants imagined a fictional home-buying scenario and identified suitable neighborhoods (housing) or explored the data (outbreaks).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.3">Procedure and Apparatus</head><p>We began with a short introduction and demo of possible interactions. Participants were instructed to phrase their queries however felt most natural, to think aloud, and to tell us whenever the system did something unexpected. We discussed reactions to system behavior throughout the session and then concluded with a semi-structured interview. Three participants completed the study in person and the remainder via videoconference with a shared screen. All sessions took approximately 30 minutes and were recorded. Queries were typed rather than spoken.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Results</head><p>Overall, participants were very positive about NL interaction and identified many benefits. Evizeon allowed participants to focus on their questions rather than how to express them, could react without fully understanding ("I love this because. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1">Study 1 -Target Criteria Tasks</head><p>We observed two distinct strategies that participants took to transition their context in the target criteria tasks:</p><p>Edit-in-place: In this strategy, the user repeatedly edits the queries in place with fully qualified utterances. An example sequence of utterances for this strategy might be: (1) "single family houses in fremont," (2) "single family houses in fremont and ballard under 900k," (3) "single family houses under 900k."</p><p>Replace: In this strategy, the user replaces their queries with terse utterances, often involving ellipsis, while continuing the same train of thought. Example sequence: (1) "single family houses in fremont," (2) "and ballard," (3) "under 900k," (4) "all neighborhoods." <ref type="table">Table 1</ref>. Percent of participants in edit-in-place and replace groups who used each interaction type as a primary strategy. Replace participants typed completely new text more often and used more interaction types including ellipsis. Widget manipulation was similar across the two groups. <ref type="table">Table 1</ref> illustrates how the interaction strategies differed. Only P16 used referencing (specifically, anaphoric); perhaps it was not obvious that it was possible. Both groups occasionally used lexical cohesion (e.g., "SFH" for single family residential or "sold in" for last sale date). Evizeon generally worked well for Add and Change tasks: pragmatics behaved as expected and widgets provided effective feedback. Participants had the most problems on Remove tasks; Evizeon neglected to support text-based removal of items from the context so participants often used widgets as a backup.</p><p>Most interesting were the different expectations on how widgets and text should interact, especially on removal tasks. Edit-in-place participants all expected the widgets and the text query to be exactly in sync, both fully documenting the global context state. Their textbased approach to removal was to delete the text for unwanted items (e.g., phrase 3 in the edit-in-place example above). Evizeon's failure to remove constraints following such a query was confusing to these participants. Moreover, they also expected that deleting a widget would remove the corresponding text. The lack of synchrony sometimes led to errors; for example, P13 removed a [fatalities = 0] widget, then edited his query text for a different purpose, but the [fatalities = 0] constraint spuriously returned because its text was still present.</p><p>Participants using the replace strategy had difficulty conveying whether their queries should be interpreted as new versus continuing the current context. For example, "sometimes you want it to remember and sometimes you don't...And you're very sure about which one you want it to be, but there's nothing about your question that indicates why it would be so" [P14]. P15 often explicitly noted a continuation by starting with "now." Others used keywords to indicate broadening of the context. For example, when moving from measles to any diseases in the same place, P16 typed, "all diseases there" (also an example of anaphoric referencing). P16 said that her phrase length should be interpreted as a differentiator, saying, "it was not in sync with me about when it should drop old stuff and when it should keep old stuff. If I had a more complete sentence, then I meant to wipe the slate clean."</p><p>Of 12 participants in Study 1, 8 used the edit-in-place strategy and 3 used replace. The remaining participant reset the view to the default state between most tasks. Low usage of replace might suggest that users rarely relied on previous commands, making global and local coherence unimportant; however, Study 2 showed the opposite effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2">Study 2 -Open-Ended Tasks</head><p>Open-ended tasks demonstrated that the pragmatics functionality was critical. We observed distinct discourse segments containing sets of related utterances for each line of inquiry and 35% of utterances depended on earlier ones in the segment. Maximum segment length ranged from 5 − 18 (avg.=8.6) and 8 out of 9 participants used utterances that required system memory. Maximum memory depth (prior steps necessary to capture all relevant context) ranged from 0 − 15 (avg.=4.4). References were rarely used (5 instances). Comments included ("Ha, that's awesome...it is able to take just these additional little snippets" [P9] and "I did not think it would understand the word expensive..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.that blows my mind a little bit!" [P9]).</head><p>Notably, no participant used a pure edit-in-place strategy. Four used replace, 2 had an indiscernible strategy, and the remaining 3 used a mixture of replace and edit-in-place. For example, P19 frequently used short follow-on queries to continue a train of thought, but longer sentences (often without an explicit reset) to start a new line of inquiry.</p><p>Study 2 also revealed several shortcomings. Overall, 33% of utterances were unsuccessful (i.e., Evizeon did not correctly understand the request). Of these cases, 39% were successfully repaired with a follow-on utterance, 45% were abandoned, and for the remaining 16% the user tried again, eventually leading to success or abandonment.</p><p>Unsuccessful queries most often requested unsupported analytics functionality. The most common case required computing an aggregation (e.g., "most expensive neighborhood" [P9] or "year with the most measles cases" [P22]). Similarly, some users focused on temporal or spatial analytics, for which we had minimal support (e.g., "cases near Paris" [P19]). Interestingly, our outbreaks dataset contained only country and coordinate information, but users automatically assumed Evizeon would understand continent and city names.</p><p>Domain-specific language was a challenge; e.g., the system could not relate disease prevalence to the numeric attribute cases. P18 frequently used the word fatalities (a data attribute) without any numeric constraints because the word itself implies a non-zero state (e.g., "countries with fatalities" versus the more complete "countries with fatalities &gt; 0"). P9 asked to teach the system new words; he particularly wanted to ask Evizeon about Seattle's "crustiest" neighborhoods.</p><p>We also encountered some grammar errors. For housing data, people have a wide variety of ways to express constraints around bed and bathrooms. Evizeon dealt with many of these but missed a few (e.g., "with 4 beds or more" [P10]). Another problem were modifier words. Filter and filter out have the same meaning but filter to means the opposite. Evizeon detected only the word filter, ignoring the modifier. Finally, Evizeon could not deal with meaningless fluff words and sometimes defaulted to a generic error, so that it was not whether the syntax was incorrect or the system lacked the requested functionality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">DISCUSSION AND FUTURE WORK</head><p>Our results suggest that pragmatic natural language interactions are a promising approach for engaging people in the flow of visual analysis <ref type="bibr" target="#b10">[14]</ref>. Even P7, who was an SQL expert, stated that she preferred NL interaction because she could just focus on her questions rather than how to express them. We were pleased to see 3 participants really get into the task; these participants began telling us all their insights about the data rather than their experience with the interface.</p><p>Presence of both edit-in-place and replace strategies presents a design challenge. Edit-in-place depends on continual presence of an editable phrase representing the full context, which would get in the way for the replace strategy. Future work could explore interfaces that adapt to the strategy. The difference in strategies between the two studies also merits investigation. We believe the reliance on edit-in-place in Study 1 was caused by the task intervention, which controlled transition types but represented an artificial analysis experience. We conjecture that the more nuanced mixture of edit-in-place and replace strategies in Study 2 is more representative of real world practice, and that terse utterances would be more prevalent with voice and mobile interaction. We also suggest that future work examine a broader user population including participants not familiar with NL interfaces.</p><p>Our rules for understanding how to transition the context appear to be generally correct, but incomplete for global coherence. Observations suggest that there may be additional cues we can exploit to automatically detect a change in intention between segments. Short queries or keywords like now may indicate a follow-on request whereas long queries may represent a new line of inquiry. Similarity to the prior input could also be considered; e.g., a query identical to the prior query minus some words should be interpreted as relaxing those criteria.</p><p>Beyond improving coordination between the input text and widgets, feedback could be enhanced with mixed-initiative approaches, where the user provides information to help the system learn a personalized context. For instance, if the user says "Show me houses near my kid's school" and the system fails to understand "kids's school," the user could point out that location in the map. We would also like to enable disambiguation of intentions with respect to quantitative metrics. For example,"cheapest" could mean a single item, the lowest N items, the lowest N%, or below some unspecified value like 300K. Other forms of interaction could also indicate intent. For example, if a user clicks away a newly presented visualization, the system should consider this as part of the conversational experience.</p><p>Interestingly, the ability to understand natural language seems to set an expectation of general intelligence; as stated by P14, "I wanted it to do analysis for me...find something interesting...teach me something." Spatial and temporal analytics were the most frequently requested features (e.g., temporal trend analysis and spatial aggregation at different levels of detail). Narrative summarization of the visualization in the textual feedback could further benefit the analytical process.</p><p>Several interesting research directions could further explore the synergies between pragmatics, discourse and visual analytical interaction. Personalized pragmatics derived from historical user interactions and context could provide richer models for intent and flow. Facilitating domain knowledge and advanced analytical functions could help broaden the repertoire of utterances supported. Analytical tools on mobile devices open up opportunities to explore pragmatic interactions unique to spoken dialog. Finally, our system's ability to successfully parse user utterances is mainly limited by hand-crafted grammar rules. A promising direction would be to automatically learn probabilistic context free grammars from a large corpus of visual analytics conversations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSION</head><p>We present Evizeon, a system that implements various types of pragmatic interaction with visual analytics. Such interaction requires understanding the pragmatics of human language and intent, and adapting the context to the domain of data analytics. We extend the centering approach employed in pragmatics theory, to support transitional states of data attributes, values, and data related expressions unique to visual analytical flow. We demonstrate that such a system is useful when it not only parses the linguistic structure of the utterances, but also effectively addresses inevitable ambiguity through repair utterances, feedback and ambiguity widgets. While human-computer interaction by means of natural language may never match the nuances of human-human interaction, we believe that there are many promising research directions to get us closer to that goal; one conversation at a time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Example results of various forms of natural language interactions with a dashboard using pragmatic conversation principles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>SARA: measles in the uk. C f ={measles, uk} EVIZEON: [Applies categorical and spatial filters showing measles in the UK.] SARA: show me the orange spike. CONTINUE C b ={measles, uk}, C f ={measles, uk, orange spike} EVIZEON: [Highlights spike in the line for measles in the line chart.] SARA: mumps over there. RETAIN C b ={uk}, C f ={uk} SHIFT C b ={measles, orange spike}, C f ={mumps} EVIZEON: [Retains spatial filter for UK, updates categorical filter to mumps, removes highlighted spike in the line for measles.] SARA:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Results of an ellipsis implementation in Evizeon. Here, the incomplete utterance "townhomes" is parsed with context from the previous utterance to show townhomes under $1M in the Ballard neighborhood.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Anaphoric reference: the user refers to '2014' as "previous year."</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Evizeon showing the results of an utterance with implicit conjunctions between various attributes for houses in Ballard.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>A parse tree output from Evizeon's parser, showing how the system iteratively connects the analytical functions of adjacent nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>An implementation of lexical cohesion in Evizeon, where 'most expensive' is mapped to the analytical function Top-N(sale price) of houses, and 'cheapest' is mapped to Bottom-N(sale price). Price ranges annotated in yellow for clarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Evizeon parses the repair utterance "remove condos" and updates the results from the previous figure, filtering out condos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Evizeon generates new visualizations as part of a coordinated dashboard in response to various input utterances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>..it didn't fully understand my query, but what it did get, it actually answered my question!" [P9]), helped them learn ("I have actually exposed and discovered the syntax of this dataset" [P9]), and could save time ("literally with 3 questions you've built me a dashboard...you've saved me like 150 clicks right there!" [P19]). They all appreciated the conversational model, "it allowed me to stay in my workflow and ask additional questions" [P21] and the ambiguity widgets, "it gives me a way of checking that it got what I want, and...a second way of interacting" [P16].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Manuscript received 31 Mar. 2017; accepted 1 Aug. 2017. Date of publication 28 Aug. 2017; date of current version 1 Oct. 2017. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org, and reference the Digital Object Identifier below. Digital Object Identifier no. 10.1109/TVCG.2017.2744684 Applying Pragmatics Principles for Interaction with Visual Analytics Enamul Hoque, Vidya Setlur, Melanie Tory, and Isaac Dykeman</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>• Enamul Hoque is with Stanford University. This work was done when Enamul was an intern at Tableau Research and a student at the University of British Columbia. E-mail: enamul@cs.ubc.ca. • Vidya Setlur is with Tableau Research. E-mail: vsetlur@tableau.com. • Melanie Tory is with Tableau Research. E-mail: mtory@tableau.com.</figDesc><table /><note>• Isaac Dykeman is with Rice University. This work was done when Isaac was an intern at Tableau Research. E-mail: ijdykeman@gmail.com.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>congo, angola} EVIZEON: [Applies categorical filter for measles epidemics, applies new spatial filter on Malawi, Congo, and Angola, replacing UK.]</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>The omitted criteria 'less than 1M in Ballard' are retained and the value 'townhomes' replaces 'houses.' ={1M, ballard}, C f ={1M, ballard} SHIFT C b ={houses}, C f ={townhomes} EVIZEON: [Retains numerical and spatial filter for Ballard, applies categorical filter on home type to show only townhomes.]</figDesc><table><row><cell>JOHN:</cell><cell>houses less than 1M in Ballard.</cell></row><row><cell></cell><cell>C f ={houses, ballard, 1M}</cell></row><row><cell cols="2">EVIZEON: [Applies numerical and spatial filters showing houses under</cell></row><row><cell></cell><cell>$1M in Ballard.]</cell></row><row><cell>JOHN:</cell><cell>townhomes.</cell></row><row><cell></cell><cell>RETAIN C b</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Ballard under 600k last summer. CONTINUE C b ={houses, ballard}, C f ={houses, ballard, &lt;600k, last summer} EVIZEON: [Further refines the current visualization by applying a numerical filter on house price and a temporal filter to show the past summer.]</figDesc><table><row><cell></cell><cell>):</cell></row><row><cell>SARA:</cell><cell>houses in Ballard. C f ={houses, ballard}</cell></row><row><cell cols="2">EVIZEON: [Applies categorical and spatial filters showing houses</cell></row><row><cell></cell><cell>in Ballard.]</cell></row><row><cell>SARA:</cell><cell>houses in</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="http://www.dictionary.com" />
		<title level="m">Dictionary</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Analytics</forename><surname>Ibm Watson</surname></persName>
		</author>
		<ptr target="http://www.ibm.com/analytics/watson-analytics/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q &amp; A</forename><surname>Microsoft</surname></persName>
		</author>
		<ptr target="https://powerbi.microsoft.com/en-us/documentation/powerbi-service-q-and-a/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pokémon</forename><surname>Etymology</surname></persName>
		</author>
		<ptr target="https://pokemondb.net/etymology" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<ptr target="http://www.thoughtspot.com/" />
		<title level="m">ThoughtSpot</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mixing scripted interaction with task-oriented language processing in a conversational interface</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ball</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Intelligent User Interfaces, IUI 1999</title>
		<meeting>the 4th International Conference on Intelligent User Interfaces, IUI 1999<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="101" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Lifelike Computer Characters: The Persona Project at Microsoft Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kurlander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pugh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Skelly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>AAAI Press</publisher>
			<biblScope unit="page" from="191" to="222" />
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Basic Color Terms: their Universality and Evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Berlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1969" />
			<publisher>University of California Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The xcalibur project, a natural language interface to expert systems and data bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Boggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Mauldin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">G</forename><surname>Anick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Eighth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Embodiment in conversational interfaces: Rea</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cassell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bickmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Billinghurst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Vilhjálmsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI 1999</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems, CHI 1999<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="520" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Flow: The Psychology of Optimal Experience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Csikszentmihalyi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991-03" />
			<publisher>Harper Perennial</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Analyza: Exploring data with conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dhamdhere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Mccurley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nahmias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Intelligent User Interfaces</title>
		<meeting>the 22nd International Conference on Intelligent User Interfaces</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="493" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Between linguistic attention and gaze fixations inmultimodal conversational interfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ferreira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 International Conference on Multimodal Interfaces</title>
		<meeting>the 2009 International Conference on Multimodal Interfaces<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="143" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Datatone: Managing ambiguity in natural language interfaces for data visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dontcheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Karahalios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual ACM Symposium on User Interface Software Technology</title>
		<meeting>the 28th Annual ACM Symposium on User Interface Software Technology<address><addrLine>UIST; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="489" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploring the placement and design of word-scale visualizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Willett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-D</forename><surname>Fekete</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2291" to="2300" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">How information visualization novices construct visualizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Grammel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-A</forename><surname>Storey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="943" to="952" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Pragmatics and Natural Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Green</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Routledge</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention, intentions, and the structure of discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Grosz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Sidner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="175" to="204" />
			<date type="published" when="1986-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Halliday</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cohesion in English. Longman</title>
		<imprint>
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to understand phrases by embedding the dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="17" to="30" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Handbook of Pragmatics. Blackwell Handbooks in Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ward</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The classification of coherence relations and their linguistic markers: An exploration of two languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Knott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sanders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Pragmatics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="135" to="175" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards a dialogue system that supports rich visualizations of data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aurisano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Di</forename><surname>Eugenio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leigh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="304" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Show me: Automatic presentation for visual analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mackinlay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stolte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1144" />
			<date type="published" when="2007-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mc-Closky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL) System Demonstrations</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems, NIPS 2013</title>
		<meeting>the 26th International Conference on Neural Information Processing Systems, NIPS 2013<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Wordnet: A lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Lexical cohesion computed by thesaural relations as an indicator of the structure of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="48" />
			<date type="published" when="1991-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Visualization Analysis and Design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munzner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Reinhart</surname></persName>
		</author>
		<title level="m">Pragmatics and Linguistics: An Analysis of Sentence Topics. IU Linguistics Club publications. Reproduced by the Indiana University Linguistics Club</title>
		<imprint>
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Declarative interaction design for data visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Satyanarayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wongsuphasawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology</title>
		<meeting>the 27th Annual ACM Symposium on User Interface Software and Technology<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="669" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Eviza: A natural language interface for visual analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Setlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Battersby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gossweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual Symposium on User Interface Software and Technology</title>
		<meeting>the 29th Annual Symposium on User Interface Software and Technology<address><addrLine>UIST; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="365" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The CIEDE2000 color-difference formula: implementation notes, supplementary test data, and mathematical observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Color research and application</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="21" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Articulate: A semi-automated model for translating natural language queries into meaningful visualizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leigh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Smart Graphics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="184" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Creating conversational interfaces for interactive software agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trower</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Extended Abstracts on Human Factors in Computing Systems, CHI EA 1997</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="198" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Text and Context: Explorations in the Semantics and Pragmatics of Discourse. Longman Linguistics Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Van Dijk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977" />
			<publisher>Addison-Wesley Longman Limited</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An optimization-based approach to dynamic visual context management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Information Visualization</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Verbs semantics and lexical selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 32nd Annual Meeting on Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="133" to="138" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
