<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TreePOD: Sensitivity-Aware Selection of Pareto-Optimal Decision Trees</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mühlbacher</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Vienna</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">University of Vienna</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenz</forename><surname>Linhardt</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Vienna</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Möller</surname></persName>
							<email>torsten.moeller@univie.ac.at</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Vienna</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Piringer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Vienna</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">University of Vienna</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename><forename type="middle">Lorenz</forename><surname>Linhardt</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Vienna</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename><surname>Torsten Möller</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Vienna</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TreePOD: Sensitivity-Aware Selection of Pareto-Optimal Decision Trees</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TVCG.2017.2745158</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Model selection</term>
					<term>classification trees</term>
					<term>visual parameter search</term>
					<term>sensitivity analysis</term>
					<term>Pareto optimality</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Balancing accuracy gains with other objectives such as interpretability is a key challenge when building decision trees. However, this process is difficult to automate because it involves know-how about the domain as well as the purpose of the model. This paper presents TreePOD, a new approach for sensitivity-aware model selection along trade-offs. TreePOD is based on exploring a large set of candidate trees generated by sampling the parameters of tree construction algorithms. Based on this set, visualizations of quantitative and qualitative tree aspects provide a comprehensive overview of possible tree characteristics. Along trade-offs between two objectives, TreePOD provides efficient selection guidance by focusing on Pareto-optimal tree candidates. TreePOD also conveys the sensitivities of tree characteristics on variations of selected parameters by extending the tree generation process with a full-factorial sampling. We demonstrate how TreePOD supports a variety of tasks involved in decision tree selection and describe its integration in a holistic workflow for building and selecting decision trees. For evaluation, we illustrate a case study for predicting critical power grid states, and we report qualitative feedback from domain experts in the energy sector. This feedback suggests that TreePOD enables users with and without statistical background a confident and efficient identification of suitable decision trees.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Decision trees are a common technique for statistical classification. Hierarchical decision rules model classes of a categorical variable depending on numerical or categorical independent variables, called features. The decision rules are typically inferred from training data for which the classes are known, which is referred to as supervised learning <ref type="bibr" target="#b13">[14]</ref>. Frequent types of rules include thresholds on numerical features and class membership vectors on categorical features. In contrast to other types of classification models such as neural networks, a key advantage of decision trees is the ability of humans to understand how the model works. Experts in many fields such as medical diagnosis, image processing, or fraud detection therefore appreciate decision trees for their interpretability <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19]</ref>. In addition to classifying new data instances, the understandable model structure also supports explaining class dependencies for hypothesis generation and reporting.</p><p>The process of building decision trees involves multiple trade-offs. As for other model types, the most well-known trade-off is that between over-and underfitting the data for robust generalization (biasvariance trade-off). Automated techniques exist which adjust the model complexity accordingly, e.g., by using different data for growing and pruning the tree <ref type="bibr" target="#b13">[14]</ref>. In addition to accuracy, however, aspects regarding model interpretability by humans are often equally important for decision trees. Model interpretability has received much attention recently <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19]</ref> and is a multi-faceted goal by itself. Simple trees with limited depth and comprising only few decision rules based on a small number of features are typically easier to understand. Moreover, decision trees intended for human decision makers benefit from nice, round thresholds <ref type="bibr" target="#b14">[15]</ref> (e.g., x ≤ 100 instead of x ≤ 99.475).</p><p>Balancing accuracy gains, interpretability and other objectives such as feature acquisition costs <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b44">45]</ref> is a key challenge when building decision trees. However, this process is difficult to automate because it involves know-how about the domain as well as the purpose of the model and often requires a qualitative assessment of the deci-sion tree by domain experts. Even with a deep understanding of the learning algorithm, obtaining a decision tree that satisfies all objectives takes substantial time for trial-and-error <ref type="bibr" target="#b31">[32]</ref>. Aggravating the challenge, many domain experts do not have a background in statistical learning <ref type="bibr" target="#b45">[46]</ref>, but still need to build decision trees which meet their objectives while reflecting their domain knowledge.</p><p>This paper proposes TreePOD, a new Visual Analytics technique for decision tree identification which addresses these challenges. Inspired by work on visual parameter space exploration <ref type="bibr" target="#b39">[40]</ref> and in line with recent work in statistics <ref type="bibr" target="#b48">[49]</ref> , our approach is based on exploring a large set of tree candidates. A key goal is to support a global-to-local strategy for model selection (G1) that initially provides the user with a comprehensive overview of possible tree characteristics. A second goal is to address users with and without deep statistical background (G2). For this reason, TreePOD takes a result-oriented approach which focuses on characteristics of generated trees such as prediction accuracy, complexity, and interpretability. Details of the machine learning process (e.g., training parameters) are hidden by default and exposed only at request. In order to foster a quick identification of suitable trees (G3), TreePOD supports an effective quantitative and qualitative comparison of model alternatives. In order to further increase the user confidence in the selected model (G4), TreePOD visualizes the sensitivity of tree candidates on variations of generation parameters. Based on TreePOD as the main contribution of this paper, additional contributions include:</p><p>• An outlined workflow for decision tree selection.</p><p>• A case study to address a real-world problem in the energy sector.</p><p>• Qualitative feedback of domain experts from the energy sector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Research in statistical learning has devised many automated algorithms for building decision trees, e.g., CART <ref type="bibr" target="#b5">[6]</ref>, C4.5 <ref type="bibr" target="#b36">[37]</ref>, and CHAID <ref type="bibr" target="#b15">[16]</ref>. Many of these algorithms use entropy minimization to choose features and split positions when growing the tree. After the growing phase, automated approaches can be used to ensure the generalizability of the model, e.g., by pruning and cross validation <ref type="bibr" target="#b13">[14]</ref>. Decision trees have also been extended to ensemble learning techniques such as random forests. Such approaches may further increase the accuracy at the cost of incurring significantly higher complexity compared to single trees. Gleicher <ref type="bibr" target="#b11">[12]</ref> notes that accuracy is not the only concern and mentions efficiency, generalizability, robustness, conciseness, verifiability, self-consistency, and comprehensibility as some other qualities that model designers must consider. Gleicher also stresses that these properties form trade-offs where the . Selection of decision trees explaining marital status in the UCI Census Income 1994 dataset <ref type="bibr" target="#b20">[21]</ref>. (a) Candidate trees are generated by sampling the parameters of decision tree algorithms. Linked visualizations guide the selection from this set by providing (b) a summary of tree candidates and parameter variations, (c) a sensitivity-aware overview of the trade-off between the conflicting objectives accuracy and number of nodes, (d) a qualitative comparison of Pareto-optimal trees, and (e) details of a selected decision tree. (f) Applying controlled parameter variations to every tree conveys the effect of parameter changes on tree characteristics, e.g., how rounding of decision boundaries affects accuracy (g). Users can extend the set of candidate trees at any time, (h) and validate trees based on data using linked views.</p><p>proper balance depends on the context and needs. An increasing number of automated approaches take comprehensibility into account as an important goal. Jung et al. <ref type="bibr" target="#b14">[15]</ref>, for example, perform rounding of model coefficients in logistic regression classifiers in order to make them easier for humans to interpret. Lakkarju et al. <ref type="bibr" target="#b18">[19]</ref> include metrics for interpretability in the objective function for model selection. In many cases, however, assessing comprehensibility requires a qualitative inspection by domain experts.</p><p>In contrast to such automated approaches, visualization research has focused on cooperative approaches for decision tree construction which enable users to incorporate their domain knowledge in the generation process. Ankerst et al. <ref type="bibr" target="#b2">[3]</ref> let the user evaluate intermediate results of the construction algorithm to specify constraints. This enables the computer to automatically create patterns satisfying these constraints. Van den Elzen and van Wijk <ref type="bibr" target="#b45">[46]</ref> support an iterative refinement of a tree during the growing, optimization, and pruning phases. This process is based on BaobabView, a technique for visualizing decision trees which combines advantages of other methods such as node-link diagrams <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b47">48]</ref> and icicle plots <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">23]</ref>. All these cooperative approaches may improve comprehensibility and user confidence in the model. A study by Liu and Salvendy <ref type="bibr" target="#b22">[23]</ref> shows that resulting trees have relatively high classification accuracies and small sizes. However, focusing on the iterative refinement of single trees may not lead to the global optimum. Moreover, such approaches do not communicate the overall achieveability of modeling objectives and may require statistical know-how and significant time by the user.</p><p>In order to provide a global coverage of possible tree characteristics, some automated approaches obtain multiple decision trees as result. Zhao <ref type="bibr" target="#b49">[50]</ref> creates Pareto optimal decision trees to capture the trade-off between different types of misclassification errors. Likewise, Czajkowski and Kretowski <ref type="bibr" target="#b8">[9]</ref> use an evolutionary algorithm to generate multiple decision trees which are Pareto optimal for contradictory metrics such as accuracy and the number of nodes. These approaches focus on generating an appropriate set of decision trees, not on exploring this set to facilitate the model selection by a human expert. Czajkowski and Kretowski stress that the comprehensibility of the generated Pareto front is a main issue for future work.</p><p>In visualization, an increasing number of systems provide global exploration strategies of parameter spaces <ref type="bibr" target="#b39">[40]</ref>, e.g., in simulation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">35]</ref> and image analysis <ref type="bibr" target="#b42">[43]</ref>. In many cases, the goal is to identify input parameter values which optimize the output in some sense. Assessing the output often involves both quantitative metrics and qualitative judgments of complex results, for example segmented image data <ref type="bibr" target="#b42">[43]</ref>. Statistical model selection is a closely related problem. Understanding the relation between abstract generation parameters and the resulting model is typically non-intuitive and model selection is usually based on multiple quantitative and qualitative criteria. Related work for exploring model spaces include subspace clustering <ref type="bibr" target="#b27">[28]</ref>, neural networks <ref type="bibr" target="#b25">[26]</ref>, and association rules <ref type="bibr" target="#b7">[8]</ref>.</p><p>In the context of decision trees, we regard the work by Padua et al. <ref type="bibr" target="#b31">[32]</ref> as most similar. Their system supports the analysis of a large set of candidate trees generated by sampling the parameter space of decision tree algorithms. Linked views visualize this parameter space as well as metrics of the resulting trees and thus enable to relate inputs to outputs by interaction. The trees are shown as node-link diagrams and small icicle plots that convey the structure but not the accuracy. This system provides a global overview of tree characteristics (G1) and guides statistical experts towards useful training parameters. However, their work does not explicitly recognize trade-offs between objectives (G3) and does not visualize their sensitivity on changes of generation parameters or the evaluation data.The analysis focuses on an existing set of trees and does not address the integration in an interactive workflow for decision tree building. Moreover, by exposing many details about generation parameters, the system is primarily designed for users with statistical background which contradicts goal (G2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OVERVIEW OF TREEPOD</head><p>TreePOD is a new Visual Analytics technique for sensitivity-aware model selection. The key idea is to create a large set of candidate trees that can be explored with respect to objectives such as prediction accuracy, or interpretability. To this end, the parameter space of tree construction algorithms is sampled to create a diverse set of trees <ref type="figure" target="#fig_0">(Fig. 1a</ref>, Sec. 4). Visualizing the candidate set at different levels of detail in multiple coordinated views <ref type="bibr" target="#b38">[39]</ref> enables a global-to-local strategy for model selection <ref type="bibr" target="#b39">[40]</ref> (Sec. 5): A summary panel displays a concise description of the candidate set, and provides various ways of focusing on candidate subsets <ref type="figure" target="#fig_0">(Fig. 1b)</ref>. A quantitative overview shows achievable values for pairs of objectives, and guides selection along tradeoffs by identifying the Pareto front, i.e., the set of Pareto-optimal trees <ref type="figure" target="#fig_0">(Fig. 1c)</ref>. Tree maps at the bottom visualize accuracy and complexity of the Pareto-optimal trees in a compact form <ref type="figure" target="#fig_0">(Fig. 1d)</ref>. A detail panel shows the currently selected tree and its characteristics ( <ref type="figure" target="#fig_0">Fig. 1e)</ref>.</p><p>To investigate local sensitivities of tree characteristics to parameter changes, users can specify a controlled variation of parameters <ref type="figure" target="#fig_0">(Fig. 1f</ref>, Sec. 6). Visualizing these variations shows how characteristics of single trees, multiple trees, or entire Pareto-fronts are affected by constraints such as rounded decision rules <ref type="figure" target="#fig_0">(Fig. 1g</ref>). Section 6.3 describes how this approach to sensitivity-aware trade-off exploration supports a variety of model selection tasks.</p><p>While TreePOD focuses on analyzing and choosing from an existing set of candidates, we also outline its integration in a workflow for building decision trees (Sec. 7). Key steps in this workflow include the incremental extension of the candidate set based on insights from exploration ( <ref type="figure" target="#fig_0">Fig. 1h</ref>), and the validation of trees based on data ( <ref type="figure" target="#fig_0">Fig. 1i</ref>).</p><p>As a guiding example illustrating TreePOD, consider the following fictional scenario: Jane, an analyst working for the ministry of social affairs, aims to predict the multi-class attribute Marital Status in the UCI "Census Income Dataset 1994" <ref type="bibr" target="#b20">[21]</ref>. Features comprise 12 demographic attributes like Age, Sex, Income, Occupation, Native Country, and many others <ref type="bibr" target="#b0">1</ref> . Her goal is to obtain an accurate and concise set of rules suitable for reporting or policy-making.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">GENERATION OF CANDIDATE TREES</head><p>A prerequisite for model selection is the availability of good candidate models. Automatic decision tree algorithms help to identify tree candidates efficiently. Based on a specification of various parameters, they produce a decision tree for pre-classified data by heuristic optimization in two distinct phases: 1) Training: Given a subset of training data and training parameters, the algorithm generates an initial tree description. Training parameters include a set of candidate features and a selection criterion that defines a feature selection strategy (e.g., maximizing information gain <ref type="bibr" target="#b36">[37]</ref>, Gini impurity <ref type="bibr" target="#b5">[6]</ref> or gain ratio <ref type="bibr" target="#b37">[38]</ref>). Other parameters include numerical termination criteria for the build process such as a maximal tree depth or a minimal leaf size needed for further splits.</p><p>2) Post-processing: In the optional second phase, post-processing such as pruning to avoid overfitting <ref type="bibr" target="#b13">[14]</ref>, or rounding of numerical decision borders to increase interpretability <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b14">15]</ref> may be applied.</p><p>Training and post-processing involve numerical, categorical and set-typed parameters. For easier readability, we subsequently use parameter value as an umbrella term for all types of parameters. Choosing parameter values that result in desirable trees is non-trivial and typically requires substantial effort <ref type="bibr" target="#b31">[32]</ref>. Instead of forcing the parameter space upon the user, TreePOD constructs a diverse set of candidates by sampling various parameters in a stochastic or pseudo-random fashion. This may include drawing feature subsets, drawing the maximal tree depth from a range (e.g., <ref type="bibr">[1,..,10]</ref>), or randomly choosing a tree pruning method. As a key benefit, stochastic assignment of parameters helps creating diverse and unbiased candidates, which increases the probability of reaching the global optimum during exploration. It also reduces the need to specify parameter values prior to exploration.</p><p>Users can also manually assign parameters to incorporate knowledge about algorithms <ref type="bibr" target="#b26">[27]</ref> or previously obtained insights. This includes setting parameters to a fixed value for all trees (e.g., max depth = 6), as well as manual adjustment of sampling ranges (e.g., max depth ∈ <ref type="bibr">[1,..,6]</ref>). However, we provide reasonable defaults for all sets and ranges to keep the mandatory user input to a minimum. Data subsets for growing, pruning, and evaluation can also be manually specified, but are otherwise automatically determined by splitting the available data into random parts of equal size.</p><p>TreePOD also supports various common pruning techniques <ref type="bibr" target="#b13">[14]</ref>. As the simplest method, we support collapsing sub-trees if all leaves within produce the same classification. Pruning can also be deactivated to allow for a more detailed analysis of achievable accuracy.</p><p>In the guiding example, Jane wants to know how well small models can perform. She generates 300 decision trees by sampling <ref type="bibr" target="#b0">(1)</ref> the maximal tree depth between 1 and 6, (2) the minimal leaf size required for further splits, <ref type="bibr" target="#b2">(3)</ref> as well as subsets of the 12 available features to obtain different explanations. This generates 300 candidates that are evaluated for an exploration of their results (see <ref type="figure">Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">GUIDED EXPLORATION OF PARETO-OPTIMAL TREES</head><p>This section describes interactive visualizations of the tree candidates at different levels of detail. The goal is to support the selection of suitable trees based on quantitative and qualitative characteristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Candidate summary panel</head><p>At the coarsest level, TreePOD provides a concise summary of all tree candidates (see <ref type="figure" target="#fig_0">Fig. 1b</ref>). This view describes how the set of candidates is successively refined by the user during exploration. Users may define generation parameter filters, for example to focus on trees based on particular feature subsets or rounding thresholds. Tree candidates may also be filtered based on their result metrics such as accuracy (see Section 5.2). The current set of filters is summarized in this view. Furthermore, the panel states the number of Pareto-optimal trees regarding two objective metrics, which is used as central guidance concept in TreePOD. These concepts will be introduced in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Quantitative trade-off overview</head><p>The model selection process typically involves quantitative metrics. The metrics in our implementation refer to three types of objectives:</p><p>(1) Accuracy, as measured by the F1 score (aka F-measure) <ref type="bibr" target="#b50">[51]</ref>. We provide per-class scores (e.g., F1 "Married") as well as the overall score by computing the weighted average of F1 across classes (denoted Accuracy [F1 score]).</p><p>(2) Complexity, optionally expressed as either the total number of nodes, the number of leaves, the maximum tree depth, the number of used attributes, or the total feature cost.</p><p>(3) Interpretability in terms of human-friendly numbers, computed as the average num. of significant digits in numerical rules <ref type="bibr" target="#b30">[31]</ref>.</p><p>We do not intend to make a case for any particular metric. The concepts of TreePOD could be applied to other metrics as well.</p><p>For an effective quantitative overview of the tree candidates, Tree-POD displays two user-specified metrics in a 2D scatter plot (e.g., Accuracy vs. Nr. of used attributes in <ref type="figure">Fig. 2a</ref>). This provides an overview of the candidates in terms of quantitative characteristics and may reveal patterns such as discontinuities or clusters caused by distinct parameter settings, e.g., the inclusion of important features.</p><p>Not all candidates are equally relevant for model selection. For example, among all trees of the same size in <ref type="figure">Fig. 2b</ref>, some are substantially more accurate than others. An established concept in multicriteria decision making is Pareto optimality <ref type="bibr" target="#b17">[18]</ref>. In general, a solution is considered Pareto-optimal if no other solution exists that is better for some criteria without being worse for others. The set of all Pareto optimal solutions is called Pareto front. In our case, this front comprises all candidate models which are Pareto optimal regarding the two objectives mapped to the axes of the scatter plot.</p><p>Pareto-optimal candidates are highlighted using an increased point size and connected with a line to visualize the Pareto front (see <ref type="figure">Fig. 2</ref>). Drawing the front as an interpolated line rather than step-wise is a potentially too optimistic approximation of the real Pareto front. However, we decided to tolerate this as the selection relies on the discrete set of candidates rather than on the continuous shape of the Pareto front. Visually, drawing interpolated lines enables to compare slopes across neighbouring segments. Very steep and very shallow segments indicate transitions that provide high gain of one objective for low additional cost of the other, guiding users towards possible "sweet spots". Any tree can be selected by a click, making it focal. In the scatter plot, this focal tree is highlighted by a black circle around the point <ref type="figure">(Fig. 2</ref>). Linked views focus on it as well, for example, to show the tree description and parameters which led to that result (see Sec. 5.4).</p><p>The view also enables to define range filters for objective values by dragging handles inwards from the plot borders. In <ref type="figure">Fig. 2</ref>, all trees using more than 5 attributes are excluded as indicated by a semitransparent gray area. Filters persist when changing objectives, which allows investigating a filtered set of candidates with respect to other objectives. This supports a global-to-local workflow for model selection, where the considered set of trees is iteratively refined (G1). Filtered points are not considered when computing the Pareto fronts, but are still displayed in a lower intensity as context. Additionally, a textual representation is shown in the candidate summary (see <ref type="figure">Fig. 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Qualitative comparison along the Pareto front</head><p>The quantitative overview described in the previous section provides effective guidance to trees with high objective values. However, summary metrics hide multiple sources of ambiguity that may be relevant to the decision maker. For example, a high overall accuracy of models can be the result of well-explaining features, or of highly skewed base rates <ref type="bibr" target="#b50">[51]</ref>. Likewise, a single accuracy metric does not inform about the distribution of accuracy among the classes.</p><p>To visualize such qualitative aspects along a trade-off, we encode the set of Pareto-optimal candidates using small tree maps <ref type="bibr" target="#b40">[41]</ref> (see <ref type="figure" target="#fig_1">Fig. 3</ref>). Their sequence represents a linear traversal of the 2D Pareto front, i.e. one objective improves while the other deteriorates from left to right. This arrangement facilitates switching to the next more accurate or next simpler Pareto-optimal tree for an efficient browsing of candidates. Clicking a plot makes the corresponding tree focal.</p><p>Each partition in a tree-map corresponds to a leaf node, with a relative size proportional to the percentage of data instances classified by that leaf. This enables an effective perception of complexity for the corresponding decision tree (see <ref type="figure" target="#fig_1">Fig. 3</ref>).</p><p>Inspired by perception-based approaches to classification [2, 3, 20], we encode the class distribution within a leaf by a quasi-random placement of pixels according to the class frequencies. The emerging pattern enables an intuitive perception of purity and, for high-purity leaves, easy identification of the predominant class. The selected plot in <ref type="figure" target="#fig_1">Figure 3</ref>, for example, indicates a first split that isolates Married persons very well (mostly blue leaf). The other leaves are much less pure. Discriminability of hue depends on the size of coherent areas <ref type="bibr" target="#b28">[29]</ref> and thus on the separability of a data set. We found that, in practice, 5-7 classes can be effectively discriminated also for small pure leaves. For noisy leaves, discrimination of single pixels is typically less important than the overall perception of entropy, which is directly supported by the encoding. This encoding has the advantage that both over-and underfitted trees result in high-frequency patterns. Simple and accurate trees, however, contain large, homogeneous regions. This provides effective qualitative guidance along the trade-off.</p><p>Our approach to pixel-based encoding of class distribution is inspired by work of Ankerst et al. <ref type="bibr" target="#b2">[3]</ref>, but differs with respect to two major aspects: first, their approach shows all levels of the tree next to each other, visualizing the purity gained by every split. Our approach focuses on the leaves to enable an efficient comparison of accuracy and complexity across multiple trees. Second, their pixel arrangement is spatially linked to data items. Our pixel placement is random, which avoids visual structure within the leaves that distracts from the perception of tree complexity. Details on the topology and splits of the tree are shown in a linked visualization (see Sec. 5.4). Inspecting the tree maps in <ref type="figure" target="#fig_0">Fig. 1d</ref>, Jane discovers that the more complex Pareto-optimal candidates are refinements of a few simpler ones. She also perceives "Widows" as least frequent Marital Status (green).</p><p>a b c <ref type="figure">Figure 4</ref>. Details for a selected tree evaluation. A node is hovered to focus on the explanation of Widowed persons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Details for a selected model</head><p>Additional views of TreePOD show further details of the focal tree: 1) Structural aspects of the tree: when decision trees are used for explanatory purposes, inspecting the rules is essential. This includes the names of the used features, as well as their depth in the tree as a notion of their importance. Moreover, the exact split values are often important for explanation or hypothesis generation. The rule definition is also essential for qualitative judgments of interpretability based on domain knowledge. Another structural aspect of trees refers to the topology, e.g., distinguishing deep from wide trees.</p><p>To visualize these aspects, we use a node-link diagram inspired by BaobabView <ref type="bibr" target="#b45">[46]</ref>. Each node contains its rule definition as text. The width of a link leading into a node is proportional to the number of data items that it applies to. Within links, space is subdivided into stacked, colored bands that convey the proportion of each class <ref type="bibr" target="#b45">[46]</ref>. Since we want to emphasize the significance of paths and leaves, we reduce the visual footprint of other aspects. For example, we encode a leaf's decision as a colored triangle glyph instead of coloring the whole leaf, as this would result in large salient areas that distract from the significance of the links. For the same reason, we show detail information for nodes only on demand: when hovering a node, all nodes between it and the root show horizontally stacked bars conveying the gain of purity along the path (see <ref type="figure">Fig. 4</ref>). Hovering class labels in a coloring legend visually emphasizes leaves yielding that class.</p><p>As an indicator for decision confidence, we add a bubble to each leaf node, using the same pixel-based purity encoding as the plots in Sec. 5.3. Their size is proportional to the number of classified data items. Apart from making leaf nodes more salient, these bubbles facilitate visual correspondence of leaves with the tree map visualization.</p><p>2) Quantitative properties of the tree: The quantitative metrics listed in the beginning of Section 5 can be inspected in a list. In particular, this includes metrics currently not shown in the trade-off visualizations. As a familiar encoding of accuracy per class, we also provide a confusion matrix. A column-wise encoding of relative frequencies using a linear gray-scale informs the user about systematic misclassifications. On demand, users can switch to a row-wise relative encoding to focus on recall rather than precision. Absolute numbers are stated per cell to support comparisons in any case.</p><p>As TreePOD generates its tree candidates by parameter sampling, the particular parameter values that led to a tree can be interesting and are shown on demand. We hide this list by default to focus on the resulting trees, rather than the machine learning process (G2).</p><p>Inspecting the details of Pareto-optimal trees, Jane discovers "Age" as an important feature that is often used for the first split, mostly followed by "Sex", and "Income". "Age" seems to be important for the classification of Widow(er)s. The confusion matrix for the focal tree, however, reveals that less than half of all Widow(er)s are classified as such (bottom row in <ref type="figure">Fig.4b</ref>). She also discovers that the rule definitions are often not based on whole numbers, such as "Age &gt; 27.5".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">SENSITIVITY ANALYSIS OF TRADE-OFFS</head><p>Confidence in model selection is a multi-faceted topic. The visualizations described in the previous section provide no direct support for investigating how changes of the parameters involved in training, post-processing, and evaluation would affect the trees. This section describes extensions to the tree generation process and the visualization which enable an effective sensitivity analysis of parameter variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Generating tree families for effective comparison</head><p>Stochastic parameter sampling as described in Sec. 4 efficiently generates a diverse set of alternatives to choose from. However, these samples are usually too diverse to support a focused sensitivity analysis. As a solution, we extend the stochastic generation process by a controlled variation of one or more user-specified parameters, which are subsequently referred to as variation parameters. In contrast to other parameters, variation parameters are varied in a full-factorial manner and define a tree candidate for every possible combination of values. For each stochastic sample of the other parameters (Sec. 4), the controlled variation thus defines a family of trees. All members of one family are referred to as sibling trees. They only differ by the values of one or more variation parameters. For illustration, consider the variation of one parameter in the guiding example: For her report, Jane prefers rules based on simple integer numbers, e.g. "Age &gt; 28" rather than "Age &gt; 27.5". She wonders if even multiples of 10 are sufficiently accurate. Thus, she varies the postprocessing parameter "Round to significant digits" in three steps: {"no rounding", "max. 2 significant digits", and "max. 1 significant digit"}. As a result, 3 variations are created for each of the 300 stochastic samples, which differ by the performed rounding. The new number of candidates is 900, comprising 300 families of 3 trees each.</p><p>This two-step generation process ensures the existence of unbiased alternatives, and enables an effective assessment how a single tree, or the candidate set as a whole changes under controlled variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Sensitivity visualization</head><p>By default, the visualizations do not treat siblings differently from other possible candidates. As a result, one common Pareto-Front is computed, and shown in the quantitative and qualitative views.</p><p>TreePOD supports filtering the candidate set by variation parameters. In the candidate summary panel (Sec. 5.1), all values for each variation parameter are listed using labeled dot markers (see <ref type="figure" target="#fig_0">Fig. 1b</ref>). Clicking on a dot marker filters the set of visible tree candidates to those of the respective value. An additional marker labeled "any" does not filter on that parameter. Filters for multiple variation parameters are combined by a logical "AND". We refer to the vector of all current variation parameter values as the variation focus. Changing the variation focus updates the set of tree candidates which also updates the Pareto front. The corresponding sibling of the previous focal tree becomes the new focal tree, which also updates the detail visualizations.</p><p>For a sensitivity analysis regarding a specific variation parameter, the user may click on its name in the summary panel (e.g., "Round to significant digits" in <ref type="figure" target="#fig_0">Figure 1</ref>). The scatter plot then supports comparing the impact of parameter changes at three levels of locality.</p><p>1) Point-wise sensitivity of the focal tree. As the most local level, the scatter plot displays the siblings of the current focal tree as colored points. Inspired by previous work on sensitivity analysis <ref type="bibr" target="#b3">[4]</ref>, ordinal variations are connected by lines and encoded using different levels of luminance in the order of variation. For example, the turquoise points in <ref type="figure" target="#fig_3">Fig. 5f</ref> show how the focal tree changes for increasing maximal tree depths. For variation parameters without inherent order such as the pruning method, all siblings are connected to the focal tree. In this case, hue is used to discriminate the values. Our implementation attempts to use different hue sets for encoding data classes and variation values. This avoids color scheme overlaps if the numbers of classes and compared parameter values are low, which is a frequent case.</p><p>2) Point-wise sensitivity of Pareto-optimal trees. As a less local level, point-wise sensitivities can be shown for all currently visible Pareto-optimal trees. This enables to investigate how the sensitivity changes along the Pareto front. For example, <ref type="figure" target="#fig_3">Fig. 5d</ref> shows that evaluating trees for validation data leads to a stronger accuracy loss for complex trees than for simple ones.</p><p>3) Sensitivity analysis of the Pareto front. As the most global level of sensitivity visualization, the Pareto front itself is shown for each variation step. Each front is computed individually based on the candidates for the corresponding value of the investigated variation parameter. This enables a direct comparison of achievable trade-offs. In <ref type="figure" target="#fig_0">Fig. 1g</ref>, for example, the turquoise fronts indicate how the trade-off between accuracy and size changes for various rounding thresholds. The color scheme is the same as for point-wise sensitivity encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Application to sub-tasks of model selection</head><p>The process of model selection comprises a number of sub-tasks which can be addressed by TreePOD. We identified four groups of tasks.</p><p>1) Sensitivity-aware selection of tree generation parameters This group of tasks refers to studying the global effect of changing tree generation parameters. The focus of interest is typically on the achievable model characteristics and not on individual trees. Therefore, visualizing the entire front is typically the most suitable level of locality in this case. Typical goals include refining parameter ranges for the stochastic variation or assessing their stability for increased confidence. Specific examples for this group of tasks are:</p><p>Assessing the benefits of feature inclusion: Using features with high explanatory power is essential for a good fit, but some features may be expensive to obtain. Sometimes, these costs can be quantified, e.g., expensive medical tests <ref type="bibr" target="#b21">[22]</ref>. Other times, they are subjective, such as side-effects of medical tests <ref type="bibr" target="#b44">[45]</ref>. The latter are often only vaguely known and harder to compare across features. To support both types of costs, TreePOD enables a qualitative comparison of feature inclusion by varying whether a user-specified subset of the features is included. As an example, <ref type="figure" target="#fig_3">Fig. 5a</ref> shows the achievable Pareto fronts when including Income-related features in explaining Marital Status, or not. A reason to omit them could be a generally high number of missing values, when collecting such data from surveys.</p><p>Assessing accuracy loss due to decision border rounding: Rounding numerical decision thresholds in a post-processing step increases a tree's usefulness in human-oriented application contexts <ref type="bibr" target="#b14">[15]</ref>. However, this typically decreases accuracy. Varying number rounding parameters, e.g., to n significant digits, supports the user in deciding how much accuracy should be sacrificed (see <ref type="figure" target="#fig_3">Fig. 5b</ref>).</p><p>Further examples refer to the variation of generation strategies, such as the feature selection criterion or the pruning method. For both parameters, several methods exist but no single one is considered generally superior <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b29">30]</ref>. Visualizing the variation of Pareto fronts helps to understand the effect of different methods for the given dataset.</p><p>2) Assessment of model stability From a statistical point of view, a weakness of decision trees refers to their high variance compared to other model types <ref type="bibr" target="#b13">[14]</ref>. Slight changes in the training data may lead to substantially different model definitions. TreePOD supports an assessment of model stability by controlled variation of training data subsets. In this case, siblings refer to trees trained for the same parameters, but based on different data. When using meaningful data categories as subsets, encoding the Pareto fronts allows to identify categories for which classification is easier than for others. For example, the scatter plot in <ref type="figure" target="#fig_3">Fig. 5c</ref> shows that Marital Status is harder to predict for some ethnicities than for others.</p><p>3) Sensitivity of accuracy to changed evaluation data Comparing model accuracy across different validation data subsets is a common approach for assessing generalizability to new data <ref type="bibr" target="#b13">[14]</ref>. To enable such assessments, TreePOD supports a user-defined variation of the evaluation data subset analogous to the variation of generation parameters. In this case, siblings represent evaluations of the same tree for different data subsets. Showing these siblings for individual trees conveys how accuracy changes for different subsets, which supports the selection of robust models. Particular examples include:</p><p>Comparing training and validation data: Comparing tree evaluations for different training and validation data subsets provides guidance along the bias-variance trade-off. <ref type="figure" target="#fig_3">Fig. 5d</ref>, for example, shows a steadily increasing training accuracy, while the accuracy for validation data decreases for deeper trees due to over-fitting <ref type="bibr" target="#b13">[14]</ref>. This provides effective guidance for selecting an adequate model complexity.</p><p>Comparing accuracy for data categories: Using meaningful data categories as evaluation subsets allows to identify a potential bias of the models, e.g. towards the most prevalent categories in the training  data. <ref type="figure" target="#fig_3">Fig. 5e</ref>, for example, illustrates a variation of the evaluation data for different ethnic groups. The largest ethnic group of data records in the training data refers to "White" persons and also obtains more accurate classification than most others. 4) Building confidence in a selected tree TreePOD supports studying variations of a single tree to obtain confidence in its superiority. The point-wise sensitivity encoding is suitable for this purpose.</p><p>Assessing gain of refinement: By varying the termination criteria of tree construction (e.g. max. depth, or min. leaf size), TreePOD supports visualizing the benefits incurred by every split level. Reflecting the step-wise nature of the greedy construction process, the resulting line graph visualizes the construction history, to provide guidance for selecting an adequate depth. In <ref type="figure" target="#fig_3">Fig. 5f</ref>, for example, the variation of the maximal tree depth shows how the focal tree is not Paretooptimal at first, but becomes part of the front after five refinement levels. Adding a split level increases the accuracy of the tree further, while 3 more levels yield a significant decrease for the validation data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">WORKFLOW INTEGRATION</head><p>Building decision trees involves multiple steps <ref type="bibr" target="#b45">[46]</ref>. The previous sections focused on the description of TreePOD for analyzing and choosing among an existing set of candidate trees. This section outlines the integration of TreePOD in a workflow for building decision trees. The subsequent steps are roughly ordered by their sequence in a typical workflow. However, our implementation does not enforce a particular order and permits most of them at any time.</p><p>Selecting training and validation data: Selecting plausible input data is typically a first step. In our implementation, users may interactively brush multivariate views of the data such as scatter plots, parallel coordinates, and time series plots to define data subsets for training and validation (see Sec. 8). Interactive data selection is useful, e.g., to exclude artifacts such as outliers based on domain knowledge. Alternatively, the system automatically defines disjunctive data sets for training and validation by random sampling of the input data.</p><p>Definition of initial tree candidates: On demand, TreePOD allows adjusting the variation strategy per generation parameter, i.e., fixed, stochastically sampled, or subject to a controlled variation (see <ref type="figure" target="#fig_4">Fig. 6</ref>). As the parameters have default values for sampling, users may also simply press a "Train" button to start without specifying parameters.</p><p>Global stochastic refinement of tree candidates: Initially, 300 stochastic variations are generated by default. Users may adjust this number depending on, e.g., the size of the training data and the number of features. At any time, users may then press a button titled "Show me more" to generate additional stochastic samples. For each of them, the same controlled variations are applied as for the initial set of trees. The set of Pareto-optimal trees will be re-evaluated for this new set, updating all views. This type of global augmentation of tree candidates is useful if the initial sampling turns out to be too sparse overall.</p><p>Local stochastic refinement of tree candidates: For a more focused, result-oriented refinement, users can create variants of the selected focal tree. Pressing a button titled "Show me more like this" will create new samples by stochastically varying the generation parameters such that they are similar to those of the focal tree, e.g., lying within narrow intervals for quantitative parameters. Repeating this for different Pareto optimal candidates allows steering the refinement of the front, and ensuring that interesting regions obtain enough samples. Alternatively, the user may inspect the particular parameter values for generating the focal tree. Users can then vary specific parameters while keeping all others fixed. For example, this enables to explicitly trigger the creation of additional hierarchy levels for a tree.</p><p>Extending the controlled variation: Users may specify or extend controlled variations of parameters at any time, e.g., if they identify interesting aspects for sensitivity analysis only after an initial inspection of the candidate trees. Each update of variation parameters is applied globally to all trees. This may generate new members of tree families or modify existing ones, e.g., if the controlled variation affects parameters which have previously been sampled stochastically.</p><p>Subjective validation of classification results: Clicking on any node of the focal tree as well as on rows and columns of the confusion matrix highlights the corresponding subset of training and validation data in the linked multivariate views. This supports a subjective validation of the classification results in the context of the actual data. In particular, this step may reveal if misclassifications are evenly distributed over the data or accumulate for, e.g., specific periods in case of time-dependent data or particular regions in case of spatial data. Sometimes, such findings may indicate structural breaks or insufficient quality for subsets of the data. Users can decide to exclude such subsets and re-run the generation for all models.</p><p>Extending the feature set: Detecting data subsets with many misclassifications may also inform domain experts about potentially missing features or may suggest the derivation of new features based on existing ones (e.g., decision boundaries defined by the interaction of multiple features). Derived features may, for example, be created in external computing environments and imported afterwards, e.g., from CSV files. Users may then either re-run the training for all tree candidates, or add the extended features as additional controlled variation.</p><p>Generation of sub-trees: It is sometimes helpful to focus the generation process on a particular sub-tree while considering other parts of the tree as given, e.g., if certain subsets of the data are more complex to model than others (Sec. 9 illustrates an example). In this case, users can specify a particular node of the focal tree as temporary root. This generates a set of candidates for this sub-tree using the same approaches for stochastic sampling and controlled variation as for the entire trees. Only these candidates are considered in this type of subtree mode. By default, only the data corresponding to the temporary root is considered for computation and visualization, and the result metrics refer to the sub-trees only. However, the structural tree still shows the position of the focal sub-tree within the entire tree as context (see <ref type="figure" target="#fig_4">Fig. 6</ref>). Upon leaving the sub-tree mode, the user may either add the focal sub-tree or all Pareto-optimal sub-trees as variants of the initiating focal tree to the overall set of tree candidates.</p><p>Local pruning of the focal tree: As the counterpart to growing sub-trees, users may also manually prune all nodes below a selected node of the focal tree. In contrast to automated pruning which is performed for all tree candidates, this type of local pruning is only applicable to the focal tree. The pruned tree is added to the set of candidates as a variation of the initiating focal tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">IMPLEMENTATION</head><p>TreePOD has been implemented as a part of Visplore, a system for visual exploration of multivariate datasets. Visplore provides multiple linked views such as scatter plots, time series plots, and views for data categorization. Data subsets defined by brushing these views can be used in TreePOD as described in Sec. 7. The system is implemented in C++ and uses OpenGL for rendering. A multi-threading architecture <ref type="bibr" target="#b33">[34]</ref> is used to maintain interactivity during computations.</p><p>For the identification of decision trees, we integrated the CART implementation of the open source library OpenCV <ref type="bibr" target="#b35">[36]</ref>. Post-processing operations such as rounding are implemented on top of the tree definitions produced by OpenCV. In most cases, OpenCV was fast enough to generate large numbers of trees in a few seconds. Specifically, generating 300 trees for a data set of 32541 data records and 12 features took on average 5 seconds on a Desktop PC with Intel i7-2600k CPU at 3.4 Ghz and 16GB RAM. From a technical point of view, the ability to generate large numbers of trees rapidly is a key prerequisite for our approach and specifically the interactive workflow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">EVALUATION</head><p>For evaluating TreePOD and the described workflow, we collaborated with four domain experts working for a transmission system operator and two experts from an IT service provider in the energy market. All of them have been active in this domain for multiple years. They are confronted with classification problems on a regular basis, e.g., for predicting market situations or for building treed prediction models of time series data. Nevertheless, all of them characterize themselves as having little background in statistical learning and very limited expertise with decision tree algorithms in particular. They used to address classification problems based on insights from static diagrams, intuition, and trial-and-error using common statistics software.</p><p>The evaluation took place in three workshops. In a first workshop, we introduced them for one hour to TreePOD by illustrating it based on three energy-related classification problems which were familiar to them from previous projects. They were allowed to ask questions at any time. Based on what they saw, the experts decided on a real-world classification problem as case study for a next workshop.</p><p>In this second workshop about one month later, we addressed that particular model selection problem (Sec. 9.1) after a brief recap of TreePOD. We strictly followed their instructions, but operated the software prototype ourselves. Two main reasons were limited time of the experts for familiarizing with all features, and the goal to keep them focused on aspects of the process rather than the implementation. Conducting the described case study took approximately one hour.</p><p>In a third workshop four months later, two of the experts used a deployed version of TreePOD to address a different model selection problem (Sec. 9.2.). This time, the experts controlled the system themselves, while we observed their actions and their workflow.</p><p>After each workshop, we asked the experts for their feedback using the rose-bud-thorn method <ref type="bibr" target="#b23">[24]</ref> for another hour (Sec. 9.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Case study: prediction of imminent power shortages</head><p>The key task of power grid operators is to balance demand and supply of electricity. Volatile power sources such as wind farms, or fluctuations of energy prices may lead to spontaneous shortages or abundances in networks. Once such critical situations are in progress, they are expensive to fix. Recognizing their imminence in advance for early intervention can thus reduce financial costs significantly.</p><p>In a joint analysis session using TreePOD, domain experts identified decision trees predicting imminent critical situations. The target variable is a categorical time series with two classes "critical in 15min", and "ok in 15min", observed over 1 month (≈ 260,000 records). Features comprise: (1) the DELTA between power supply and demand, (2) the used proportion of a limited RESERVE of balance energy, (3) various transformations of DELTA and RESERVE such as sliding averages over the past 10min (e.g. DELTA 10), first derivatives that express the TENDENCY of change, (4) 39 POWERPLANT production time series, and (6) categories such as MINUTE and HOUR.</p><p>For illustration, <ref type="figure" target="#fig_4">Fig. 6a</ref> shows examples of imminent critical situations, where RESERVE 10 is at its limit. The purpose of the model is to alert human decision makers rather than to replace them. In addition to high accuracy, having a small set of interpretable rules is thus considered highly important by the experts.</p><p>The experts initially select the first and second half of the observed time period as training and validation data. For generating an initial set of tree candidates, the experts stochastically vary the used termination criterion and the subset of input features to obtain 100 samples (see <ref type="figure" target="#fig_4">Fig. 6b</ref>). As variation parameter, the degree of rounding is varied in 4 steps. This results in 100 x 4 = 400 candidates.</p><p>The experts set accuracy and the number of nodes as objectives in the scatter plot. All tree maps of Pareto-optimal candidates show large, pure blue regions <ref type="figure" target="#fig_4">(Fig. 6c</ref>). Inspecting detail views reveals that critical situations are hardly ever imminent when |RESERVE 10| is below 76% of its limit <ref type="figure" target="#fig_4">(Fig. 6d)</ref>. This is the first split of all Pareto optimal candidates. While this matches the expectation of the experts, the particular threshold value is relevant information for them. Classifying the remaining data, however, is more complex as shown by the noise at the margins of the increasingly complex tree maps. In order to focus the further analysis on explaining this remaining variance, the experts enter the sub-tree generation mode for the |RESERVE 10| ≥ 76% node. This creates a separate batch of 400 sub-tree candidates.</p><p>The visualizations of the Pareto-front now show 11 Pareto-optimal sub-tree candidates <ref type="figure" target="#fig_4">(Fig. 6e,f)</ref>. In the scatter plot, the colored Pareto fronts for the varied degrees of rounding show that enforcing 3 or 2 significant digits does not incur a significant accuracy loss for smaller trees, while rounding to 1 digit does <ref type="figure" target="#fig_4">(Fig. 6e)</ref>. After inspecting the trees in detail, the experts decide for 2 significant digits.</p><p>Browsing the Pareto-optimal candidates reveals that the feature TENDENCY RESERVE is used for the first split by most sub-trees. This makes sense for the experts, as this feature indicates an increase (positive values) or decline (negative) of available balance energy.</p><p>By inspecting the Pareto front in the scatter plot, the experts soon decide for a sub-tree with two splits and an accuracy of approximately 0.73 <ref type="figure" target="#fig_4">(Fig. 6e)</ref>. While the next simple candidate with a single split is much less accurate, significant gains of accuracy conversely require a much larger number of splits which contradicts the requirement for simplicity. The experts inspect further details for this selected focal tree <ref type="figure" target="#fig_4">(Fig. 6f,g</ref>). They are surprised that the second split by MINUTE has a threshold of 52, which they wish to investigate further. For this purpose, we configured an additional view of our system beyond TreePOD for the experts. Specifically, stacked bars show the proportion of critical situations per minute within the hour cycle. A click on the MINUTE-based split node in TreePOD updates the stacked bars to show only the corresponding data <ref type="figure" target="#fig_4">(Fig. 6h)</ref>. This visualization confirms the adequacy of the split and also indicates a similarly blue region at the beginning of each hour. Based on this cyclic pattern, the experts hypothesize that the temporal proximity to the full hour might be an even more suitable feature than MINUTE. A composite brush for (MINUTE &gt;52 OR MINUTE &lt; 5) enables to express HOUR CHANGE as a new binary input feature for TreePOD.</p><p>The experts specify an additional controlled variation regarding the inclusion of this feature. The point-wise sensitivity of the focal tree confirms an accuracy gain of approximately 2% for the corresponding sibling. This sibling also belongs to the updated set of Pareto optimal candidates and thus becomes the new focal tree <ref type="figure" target="#fig_4">(Fig.6i)</ref>.</p><p>The experts are already very satisfied with this tree. As a final check, they want to validate its generalizability. A controlled variation of the evaluation data confirms the tree's accuracy for both training and validation data due to its relative simplicity <ref type="figure" target="#fig_4">(Fig.6j</ref>). More complex tree candidates are much less accurate for the validation data.</p><p>At the end of our joint session, the experts were very confident of having selected the most appropriate tree for their purpose. As a next step, they plan to test the performance in operation for a few weeks and eventually update the tree using TreePOD based on recent data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Evaluation workshop with users of TreePOD</head><p>The third workshop took place four months later. After a brief recap, two of the experts controlled the system themselves for approximately one hour each, in individual sessions. The goal was to identify reasons for short-term changes of power production schedules, denoted as a categorical time series REDISPATCH (yes/no) over three months ing conditions of the network and the market, as well as temporal categories. This section describes how TreePOD was used by the experts. Screenshots of their insights can be found in the supplemental material. Feedback of the users is part of Sec. 9.3.</p><p>For an initial definition of tree candidates, the first user studied the dialog's options in depth first. She then started with sampling only the termination criteria, but provided all features to every tree as fixed assignment. The only difference between the resulting trees was their degree of refinement, allowing her to assess the benefits of splits. Surprised by the use of feature EXCHANGE 1 for the first split, she investigated alternative first splits by varying the features, while allowing just one split (max depth = 2). Browsing these trees revealed that no single split allowed splitting off a significant number of redispatch cases, and that the selection of EXCHANGE 1 as the first-split feature was justified. She then resorted to the default settings, and created a new batch of 100 trees based on sampling the features and termination criteria. Surprised by the high variation among the candidates, she repeatedly used the "Show me more like this" button to create more samples near the Pareto-front. She then spent some time browsing the fronts. A linked time series view highlighting periods classified as "REDISPATCH: yes" by the focal tree allowed her to compare the recognized redispatches across trees. Watching this view while browsing, she identified trees explaining the previously unexplained redispatch cases. This was a new way of exploration we had not tried before. She finally concluded that the redispatch periods during the first two months can be classified well using trees of moderate depth (≤ 4), which she considered useful for reporting. Trees that also explain the periods of the last month, however, require significantly more nodes.</p><p>The second user defined an initial batch of 500 candidates by sampling the features and termination criteria. Browsing the Paretooptimal trees enabled him a quick identification of important features, as well as a preferable tree depth (max 4-5) for reporting. Like the first user, he was curious about alternative explanations without the dominant feature EXCHANGE 1. Thus, he extended the candidates by controlled variation of omitting vs. providing this feature to the trees. He discovered that a related feature NET 1 is often selected as a substitute, resulting in trees with comparable accuracy. He then used the same linked time series view as the first user while browsing the trees. He hypothesized that the cause for redispatch periods might have changed after the second month. Thus, he decided to split the data sets based on this possible structural break, and trained trees for each part individually. He discovered that trees for the third month did not use EXCHANGE 1, but rather four other features, confirming his hypothesis. Finally, controlled variation of border rounding showed him that rounding to 3 or even 2 significant digits incurs little accuracy loss for most trees, which he appreciated for his report.</p><p>In conclusion, both experts were satisfied with the explanations they found, and considered them useful for their reports.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">Qualitative Feedback</head><p>The six domain experts stressed the importance of building classification models as part of their jobs. Some models need to be updated frequently due to rapid changes in the energy sector. Consequently, the time they can spend on tuning single models is limited (G3). Moreover, they believe that many domain experts in their field lack a deep statistical knowledge (G2). For all six experts, model accuracy and complexity are typically the most important aspects. Other requirements such as feature acquisition costs and model plausibility need to be considered as well, but are often hard to quantify. Thus, they appreciated that the controlled variation allowed them to compare discrete sets of model variants without the need for quantification.</p><p>The reaction of the experts to TreePOD was very positive overall. All of them praised the possibility of getting a fast overview of possible model characteristics as a huge step forward in comparison to their current practice (G1). In particular, all experts considered the knowledge about the variability of model characteristics and achievability of model objectives as significant gain of confidence (G4). The result-driven approach was embraced as very understandable. The detail visualizations of the model were considered crucial both for understanding the approach as such and for supporting a qualitative model assessment. In general, all experts claimed to have understood Tree-POD within the first workshop to a degree which enabled them to think about applications to own classification problems. We specifically asked them if they consider the controlled variation as beneficial without deep algorithmic knowledge. Four experts answered that important variation options do not require such knowledge in their opinion, e.g., the set of input features or rounding levels. Two of the domain experts also considered the variation of other generation parameters as helpful for non-experts in statistics to develop an intuition of their impact.</p><p>When asked about specific visualizations, five experts considered the tree maps as important intermediate level of complexity between the abstract scatter plot and the detailed structural visualization. They considered their linear order as an intuitive guidance through the candidates. However, all experts agreed that the scatter plot is crucial as an overview and for conveying the shape of the Pareto front, e.g., for an efficient perception of jumps and sparsely sampled regions.</p><p>As a shortcoming, two experts questioned the restriction to binary trees, i.e., each intermediate node having two children. Despite advantages of binary trees from a statistical point of view <ref type="bibr" target="#b13">[14]</ref>, they considered more general trees as easier to understand and to communicate, e.g., when subsequent splits refer to the same feature.</p><p>The experts who used TreePOD themselves found the default sampling parameters combined with the "Show me more like this" button highly enabling for users without statistical background. However, they considered the number of 20 added samples with every press of this button inadequately small. One expert considered a time-based specification a better alternative, e.g., sampling for 1-2 seconds. One expert suggested adding dedicated buttons to trigger important variations more easily, e.g., "create rounded variations", or "omit feature". When defining filters on result metrics, one expert suggested drawing the achievable Pareto front for the filtered trees as context. Concerning the bubble encoding of leaf nodes (see <ref type="figure">Fig. 4a</ref>), the users found purity better conveyed by the stacked bars and bands between nodes. However, one user said their correspondence to the tree maps helped to understand the latter visualization, which was unfamiliar at first.</p><p>The other experts also contributed numerous ideas for further extensions. One expert stressed that upper hierarchy levels should be definable from the outside in order to represent given (political) rules and classification schemes. Another expert requested a sensitivity analysis for decision thresholds of particular nodes. As a very interesting idea, one expert suggested using TreePOD to explain user-defined data subsets. For example, after brushing an anomalous period of energy production in a time series view, TreePOD could explain this period by other time series such as meteorological conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">DISCUSSION AND FUTURE WORK</head><p>TreePOD fosters a shift in the strategy for tuning the generation parameters of decision trees. Fully automated tree generation often results in a cumbersome trial-and-error parameter search <ref type="bibr" target="#b31">[32]</ref>. Most previous work for cooperative decision tree construction <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b45">46]</ref> follow a local-to-global strategy for investigating the parameters <ref type="bibr" target="#b39">[40]</ref>. These approaches can be classified as white-box integration of visualization and mining <ref type="bibr" target="#b4">[5]</ref>. In contrast, TreePOD can be considered a black-box type of integration. A key advantage is to hide details of the generation process from users unless on explicit request (G2). Moreover, Tree-POD encourages a global-to-local search strategy which starts with an overview of possible characteristics for reducing the risk of missing the global optimum (G1). TreePOD still supports a cooperative creation, but on a global scale rather than by focusing on a single tree. Specifically, controlled variations are applied to the entire set of candidates which enables a comparison of the effect across trees for higher user confidence (G4). However, this concept does not exclude local refinements of selected trees if explicitly requested by users (see Sec. 7).</p><p>TreePOD closely follows the Visual Analytics Mantra <ref type="bibr" target="#b16">[17]</ref>: To analyze first, TreePOD generates a comprehensive set of decision candidates and computes quality metrics for them. TreePOD shows the important by focusing the selection on Pareto-optimal tree candidates. Users may zoom and filter by quality metrics. Adding tree candidates enables to analyze further for inspecting sensitivities regarding controlled variations of the tree generation parameters as well as for refining the sampling towards desirable tree properties. Additional views provide details on demand for a selected tree.</p><p>An important design decision of TreePOD is to restrict the number of Pareto objectives to two. This limitation has several significant advantages for keeping the approach understandable by users. For visualization, the simple representation as poly-line permits an intuitive comparison of variations of the entire front. For interaction, the linear order of tree candidates along the trade-off enables an intuitive switch from one tree to the next more accurate or more simple Pareto-optimal tree. For guidance in general, the set of Pareto optimal tree candidates is typically much smaller for two objectives than for three or more objectives, which avoids overwhelming the user with too many alternatives (G3). Moreover, feedback by domain experts suggests that the trade-off between accuracy and complexity is typically the most important consideration, even if additional objectives such as feature acquisition cost exist. Additional objectives can be considered by filtering trees with undesirable values as a common approach to address multi-criteria decision problems <ref type="bibr" target="#b43">[44]</ref>. Nevertheless, experimenting with visualization approaches for higher-dimensional multi-criteria decision making <ref type="bibr" target="#b32">[33]</ref> is relevant as future work.</p><p>Regarding other scalability aspects, the use of hue restricts the number of target classes to approximately ten for perceptual reasons <ref type="bibr" target="#b46">[47]</ref>. Even more so, as color is also used for encoding the variation. We also experimented with showing variations of multiple parameters simultaneously, but rejected this feature due to generating too complex visualizations in many cases. On the other hand, the visual complexity of TreePOD does not depend on the size and dimensionality of the training or validation data. As a practical limit of the data size, however, the approach strongly benefits from short training times of trees in order to generate a sufficiently dense sampling overall and of the Pareto front in particular. The quantitative overview scales well for large numbers of trees, considering that the most relevant information is the location and shape of the Pareto front. Conversely, a sparse sampling will in general obtain a very inaccurate approximation of the real Pareto front. While local refinements of the sampling help to mitigate this problem (Sec. 7), integrating advanced approaches for constructing the Pareto front <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b49">50]</ref> are an important aspect of future work.</p><p>As a next step, we plan to conduct a long-term study based on deploying TreePOD to target users from multiple application domains. Moreover, we intend to extend the approach in order to further utilize information contained in the generated set of candidate trees. For example, analyzing the frequency and the context in which particular features are selected could provide useful information about their importance. Finally, we believe that core concepts of TreePOD are transferable to other types of models. Model selection is typically a multicriteria problem. In addition to accuracy, objectives regarding, e.g., comprehensibility and feature acquisition cost apply to many types of models <ref type="bibr" target="#b11">[12]</ref>, e.g., regression polynomials. We thus plan to evaluate in how far the concepts of TreePOD regarding sampling, guidance, and variation also support the selection process for other types of models by replacing decision tree-specific result metrics and visualizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">CONCLUSION</head><p>This paper described TreePOD, a new approach for sensitivity-aware selection of decision trees in the presence of multiple objectives. Besides accuracy, especially the need for comprehensible models is increasing <ref type="bibr" target="#b18">[19]</ref>. To address this need, TreePOD fosters a global-to-local strategy for model selection in order to guide also non-experts in statistical modeling towards a confident selection of suitable trees.</p><p>Based on TreePOD, we described a holistic workflow for decision tree selection which combines aspects from white-box and black-box integration of visualization and data mining <ref type="bibr" target="#b4">[5]</ref>. A case study conducted in pair-analysis with domain experts illustrated the ability of TreePOD to solve a relevant problem in the energy sector, and confirmed that non-experts in statistics were able to efficiently identify a suitable decision tree with high confidence. TreePOD is applicable to classification problems independent of the application domain. As one possible direction of future work, we believe that TreePOD is conceptually transferable to other types of models for increasing the efficiency and confidence in the selection process.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc>Figure 1. Selection of decision trees explaining marital status in the UCI Census Income 1994 dataset [21]. (a) Candidate trees are generated by sampling the parameters of decision tree algorithms. Linked visualizations guide the selection from this set by providing (b) a summary of tree candidates and parameter variations, (c) a sensitivity-aware overview of the trade-off between the conflicting objectives accuracy and number of nodes, (d) a qualitative comparison of Pareto-optimal trees, and (e) details of a selected decision tree. (f) Applying controlled parameter variations to every tree conveys the effect of parameter changes on tree characteristics, e.g., how rounding of decision boundaries affects accuracy (g). Users can extend the set of candidate trees at any time, (h) and validate trees based on data using linked views.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Pixel-based treemaps convey qualitative aspects of accuracy and complexity along a Pareto front.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Various applications of a systematic variation of parameters in the generation, post-processing, and evaluation of decision trees. The sensitivity is shown for different levels of locality: the entirePareto front (a, b, c, e), all Pareto optimal tree candidates (d), and a single tree (f).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>(2521 records). Features include 23 numerical time series represent-Predicting critical situations in power grid operation. Based on pre-classified data (a), varying decision tree generation parameters (b) obtains an ordered list of Pareto optimal model candidates (c). Inspecting the structure suggests an important first split (d). The Pareto fronts for different degrees of threshold roundness recommend a rounding degree of 2 (e) and a sub-tree candidate (f, g). The distribution of classification accuracy suggests adding the proximity to full hours as feature (h). The resulting tree (i) is better and generalizes for the validation data (j).</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For better demonstration, we intentionally exclude the highly correlated feature Relationship Status, as this would yield trivial rules like Marital Status is 'Married' if Relationship Status is: Wife</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research is funded by the COMET K1 program -Competence Centers for Excellent Technologies (854174) by BMVIT, BMWFW, Styria, Styrian Business Promotion Agency -SFG and Vienna Business Agency. This work has also been supported by the Austrian Funding Agency (FFG) within the scope of the K-project DEXHELPP (843550). The COMET Programme is managed by FFG. Thanks go to all collaborators from the energy sector, and to C. Arbesser, O. Rafelsberger, S. Pajer, and Torsten Möller's group for valuable comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Visual analytics decision support environment for epidemic modeling and response evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Afzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Maciejewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Visual Analytics in Science and Technology (VAST)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="191" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Visual classification: An interactive approach to decision tree construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ankerst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth ACM SIGKDD International Conf. on Knowledge Discovery and Data Mining, KDD &apos;99</title>
		<meeting>the Fifth ACM SIGKDD International Conf. on Knowledge Discovery and Data Mining, KDD &apos;99<address><addrLine>NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="392" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards an effective cooperation of the user and the computer for classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ankerst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth ACM SIGKDD International Conf. on Knowledge Discovery and Data Mining, KDD &apos;00</title>
		<meeting>the Sixth ACM SIGKDD International Conf. on Knowledge Discovery and Data Mining, KDD &apos;00<address><addrLine>NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="179" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Uncertainty-aware exploration of continuous parameter spaces using multivariate prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Piringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Filzmoser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gröller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="911" to="920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Investigating and reflecting on the integration of automatic data analysis and visualization in knowledge discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bertini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lalanne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="9" to="18" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Classification and Regression Trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>CRC Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Result-driven exploration of simulation parameter spaces for visual effects design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bruckner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Möller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1467" to="1475" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Interactive visualization of association rules model using SOM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Castillo-Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Villegas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the XV International Conference on Human Computer Interaction</title>
		<meeting>the XV International Conference on Human Computer Interaction</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">104</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A multi-objective evolutionary approach to pareto optimal model trees. a preliminary study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Czajkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kretowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theory and Practice of Natural Computing</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="85" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Metacost: A general method for making classifiers costsensitive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 5th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="155" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A comparative analysis of methods for pruning decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Esposito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Malerba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Semeraro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="476" to="491" />
			<date type="published" when="1997-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A framework for considering comprehensibility in modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Big Data</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="75" to="88" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Interactive construction of decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cercone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Asia Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="575" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<title level="m">The Elements of Statistical Learning</title>
		<imprint>
			<publisher>Springer New York Inc</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Second Edition</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Simple rules for complex decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Concannon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Goldstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.04690</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An exploratory technique for investigating large quantities of categorical data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Kass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied statistics</title>
		<imprint>
			<biblScope unit="page" from="119" to="127" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visual analytics: Scope and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mansmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneidewind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ziegler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual Data Mining</title>
		<editor>S. Simoff, M. Böhlen, and A. Mazeika</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="76" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Multiple criteria decision making from early history to the 21st century</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Köksalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wallenius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zionts</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>World Scientific</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Interpretable decision sets: A joint framework for description and prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lakkaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1675" to="1684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Revised Selected Papers. Image Processing, Computer Vision, Pattern Recognition, and Graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lévy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pixelization Paradigm: Visual Information Expert Workshop</title>
		<meeting><address><addrLine>Paris</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lichman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Test strategies for cost-sensitive decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">X</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1055" to="1067" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Design and evaluation of visualization support to facilitate decision trees classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Salvendy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Studies</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="95" to="110" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luma Institute</surname></persName>
		</author>
		<title level="m">Vision Statement: A Taxonomy of Innovation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Interactive visual steering--rapid visual prototyping of a common rail injection system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Matković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gračanin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jelović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1699" to="1706" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Visualization for enhancing the data mining process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Meneses</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">G</forename><surname>Grinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Aerospace/Defense Sensing, Simulation, and Controls</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="126" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Opening the black box: Strategies for increased user involvement in existing algorithm implementations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mühlbacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Piringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gratzl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sedlmair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Streit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1643" to="1652" />
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Morpheus: interactive exploration of subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Assent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krieger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Seidl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1089" to="1092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Visualization analysis and design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munzner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Automatic construction of decision trees from data: A multi-disciplinary survey. Data mining and knowledge discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Murthy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="345" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Evaluation of decision trees: a multi-criteria approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-M</forename><surname>Osei-Bryson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and Operations Research</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1933" to="1945" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Interactive exploration of parameter space in data mining: Comprehending the predictive quality of large decision tree collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Padua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schulze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Matković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Delrieux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="99" to="113" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Weightlifter: Visual weight space exploration for multicriteria decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pajer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Streit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Torsney-Weir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Spechtenhauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Möller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Piringer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="611" to="620" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A multi-threading architecture to support interactive visual exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Piringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tominski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Muigg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1113" to="1120" />
			<date type="published" when="2009-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ensemble-vis: A framework for the statistical visualization of ensemble data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Potter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-T</forename><surname>Bremer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doutriaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pascucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Data Mining Workshops</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Real-time computer vision with opencv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pulli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baksheev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kornyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Eruhimov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comm. of the ACM</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="61" to="69" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">C4.5: Programs for Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<pubPlace>San Francisco, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improved use of continuous attributes in c4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="77" to="90" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">State of the art: Coordinated &amp; multiple views in exploratory visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Fifth Int. Conf. on Coordinated and Multiple Views in Exploratory Visualization</title>
		<meeting>of the Fifth Int. Conf. on Coordinated and Multiple Views in Exploratory Visualization</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="61" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Visual parameter space analysis: A conceptual framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sedlmair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Heinzl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bruckner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Piringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Möller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2161" to="2170" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Tree visualization with tree-maps: 2-d space-filling approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="92" to="99" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An extension of wilkinson&apos;s algorithm for positioning tick labels on axes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Talbot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Trans. Visualization &amp; Comp. Graphics (Proc. InfoVis)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Tuner: Principled parameter finding for image segmentation algorithms using visual response surface exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Torsney-Weir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Möller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-C</forename><surname>Hege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Verbavatz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1892" to="1901" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Visualization for decision making under uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Torsney-Weir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sedlmair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Möller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Visualization for Decision Making under Uncertainty (VDMU)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Types of cost in inductive concept learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Turney</surname></persName>
		</author>
		<idno>cs.LG/0212034</idno>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Baobabview: Interactive construction and analysis of decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Van Den Elzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Van Wijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Visual Analytics Science and Technology (VAST 2011)</title>
		<meeting>of the IEEE Conf. on Visual Analytics Science and Technology (VAST 2011)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Information Visualization: Perception for Design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ware</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Interactive machine learning: letting users build classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ware</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Studies</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="281" to="292" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Visualizing statistical models: Removing the blindfold. Statistical Analysis and Data Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wickham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The ASA Data Science Journal</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="203" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A multi-objective genetic programming approach to developing pareto optimal decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Decision Support Systems</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Knowledge discovery and data mining: challenges and realities. Premier reference source. Information Science Reference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Davidson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
