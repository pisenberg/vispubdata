<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ACTIVIS: Visual Exploration of Industry-Scale Deep Neural Network Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsuk</forename><surname>Kahng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><forename type="middle">Y</forename><surname>Andrews</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Kalro</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Polo</roleName><forename type="first">Duen</forename><surname>Horng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chau</surname></persName>
						</author>
						<title level="a" type="main">ACTIVIS: Visual Exploration of Industry-Scale Deep Neural Network Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TVCG.2017.2744718</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Visual analytics</term>
					<term>deep learning</term>
					<term>machine learning</term>
					<term>information visualization</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Fig. 1. ACTIVIS integrates several coordinated views to support exploration of complex deep neural network models, at both instanceand subset-level. 1. Our user Susan starts exploring the model architecture, through its computation graph overview (at A). Selecting a data node (in yellow) displays its neuron activations (at B). 2. The neuron activation matrix view shows the activations for instances and instance subsets; the projected view displays the 2-D projection of instance activations. 3. From the instance selection panel (at C), she explores individual instances and their classification results. 4. Adding instances to the matrix view enables comparison of activation patterns across instances, subsets, and classes, revealing causes for misclassification.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep learning has led to major breakthroughs in various domains, such as computer vision, natural language processing, and healthcare. Many technology companies, like Facebook, have been increasingly adopting deep learning models for their products <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b10">11]</ref>. While powerful deep neural network models have significantly improved prediction accuracy, understanding these models remains a challenge. Deep learning models</p><p>â€¢ Minsuk <ref type="bibr">Kahng and Duen Horng (Polo)</ref>  are more difficult to interpret than most existing machine learning models, because they capture nonlinear hidden structures of data using a huge number of parameters. Therefore, in practice, people often use them as "black boxes", which could be detrimental because when the models do not perform satisfactorily, users would not understand the causes or know how to fix them <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b32">33]</ref>. Despite the recent increasing interest in developing visual tools to help users interpret deep learning models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38]</ref>, the complexity and wide variety of models deployed in industry, and the large-scale datasets that they use, pose unique challenges that are inadequately addressed by existing work. For example, deep learning tasks in industry often involve different types of data, including text and numerical data; however most existing visualization research targets image datasets <ref type="bibr" target="#b37">[38]</ref>. Furthermore, in designing interpretation tools for real-world use and deployment at technology companies, it is a high priority that the tools be flexible and generalizable to the wide variety of models and datasets that the companies use for their many products and services. These observations motivate us to design and develop a visualization tool for interpreting industry-scale deep neural network models, one that can work with a wide range of models, and can be readily deployed on Facebook's machine learning platform.</p><p>Through participatory design with researchers, data scientists, and engineers at Facebook, we have identified common analysis strategies that they use to interpret machine learning models. Specifically, we learned that both instanceand subset-based exploration approaches are common and effective. Instance-based exploration (e.g., how individual instances contribute to a model's accuracy) have demonstrated success in a number of machine learning tasks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b28">29]</ref>. As individual instances are familiar to users, exploring by instances accelerates model understanding. Another effective strategy is to leverage input features or instance subsets specified by users <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23]</ref>. Slicing results by features helps reveal relationships between data attributes and machine learning algorithms' outputs <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>. Subset-based exploration is especially beneficial when dealing with huge datasets in industry, which may consist of millions or billions of data points. Interpreting model results at a higher, more abstract level helps drive down computation time, and help user develop general sense about the models.</p><p>Our tool, called ACTIVIS, aims to support both interpretation strategies for visualization and comparison of multiple instances and subsets. ACTIVIS is an interactive visualization system for deep neural network models that (1) unifies instance-and subset-level inspections, (2) tightly integrates overview of complex models and localized inspection, and (3) scales to a variety of industry-scale datasets and models. ACTIVIS visualizes how neurons are activated by user-specified instances or instance subsets, to help users understand how a model derives its predictions. Users can freely define subsets with raw data attributes, transformed features, and output results, enabling model inspection from multiple angles. While many existing deep learning visualization tools support instance-based exploration <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38]</ref>, ACTIVIS is the first tool that simultaneously supports instance-and subset-based exploration of the deep neural network models. In addition, to help users get a high-level overview of the model, ACTIVIS provides a graph-based representation of the model architecture, from which the user can drill down to perform localized inspection of activations at each model layer (node).</p><p>Illustrative scenario. To illustrate how ACTIVIS works in practice, consider our user Susan who is training a word-level convolutional neural network (CNN) model <ref type="bibr" target="#b18">[19]</ref> to classify question sentences into one of six categories (e.g., whether a question asks about numeric values, as in "what is the diameter of a golf ball?"). Her dataset is part of the TREC question answering data collections <ref type="bibr" target="#b0">1</ref>  <ref type="bibr" target="#b24">[25]</ref>.</p><p>Susan is new to using this CNN model, so she decides to start by using its default training parameters. After training completes, she launches ACTIVIS, which runs in a web browser. ACTIVIS provides an overview of the model by displaying its architecture as a computation graph <ref type="figure">(Fig. 1A, top</ref>), summarizing the model structure. By exploring the graph, Susan learns about the kind of operations (e.g., convolution) that are performed, and how they are combined in the model.</p><p>Based on her experience working with other deep learning models, she knows that a model's performance is strongly correlated with its last hidden layer, thus it would be informative to analyze that layer. In ACTIVIS, a layer is represented as a rounded rectangular node (highlighted in yellow, in <ref type="figure">Fig. 1A, bottom)</ref>.</p><p>Susan clicks the node for the last hidden layer, and ACTIVIS displays the layer's neuron activation in a panel ( <ref type="figure">Fig. 1B)</ref>: the neuron activation matrix view on the left shows how neurons (shown as columns) respond to instances from different classes (rows); and the projected view on the right shows the 2-D projection of instance activations.</p><p>In the matrix view, stronger neuron activations are shown in darker gray. Susan sees that the activation patterns for the six classes (rows) are quite visually distinctive, which may indicate satisfactory classification. However, in the projected view, instances from different classes are not clearly separated, which suggests some degree of misclassification.</p><p>To examine the misclassified instances and to investigate why they are mislabeled, Susan brings up the instance selection panel <ref type="figure">(Fig. 1C</ref>).</p><p>1 http://cogcomp.cs.illinois.edu/Data/QA/QC/ The classification results for the NUMber class alarm Susan, as many instances in that class are misclassified (shown in right column). She examines their associated question text by mouse-overing them, which shows the text in popup tooltips. She wants to compare the activation patterns of the correctly classified instances with those of the misclassified. So she adds two correct instances (#38, #47) and two misclassified instances (#120, #126) to the neuron activation matrix view -indeed, their activation patterns are very different ( <ref type="figure" target="#fig_2">Fig. 1.4)</ref>.</p><p>Taking a closer look at the instance selection panel, Susan sees that many instances have blue borders, meaning they are misclassified as DESCription. Inspecting the instances' text reveals that they often begin with "What is", which is typical for questions asking for descriptions, though they are also common for other question types, as in "What is the diameter of a golf ball?" which is a numeric question <ref type="figure" target="#fig_1">(Fig. 1.3)</ref>.</p><p>To understand the extent to which instances starting with "What is" are generally misclassified by the model, Susan creates an instance subset for them, and ACTIVIS adds this subset as a new row in the neuron activation matrix view. Susan cannot discern any visual patterns from the subset's seemingly scattered, random neuron activations, suggesting that the model may not yet have learned effective ways to distinguish between the different intents of "What is" questions. Based on this finding, she proceeds to train more models with different parameters (e.g., consider longer n-grams) to better classify these questions.</p><p>ACTIVIS integrates multiple coordinated views to enable Susan to work with complex models, and to flexibly explore them at instanceand subset-level, helping her discover and narrow in to specific issues.</p><p>Deployment. ACTIVIS has been deployed on the machine learning platform at Facebook. A developer can visualize a deep learning model using ACTIVIS by adding only a few lines of code, which instructs the model's training process to generate data needed for ACTIVIS. ACTIVIS users at Facebook (e.g., data scientists) can then train models and use ACTIVIS via FBLearner Flow <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12]</ref>, Facebook's internal machine learning web interface, without writing any additional code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACTIVIS's main contributions include:</head><p>â€¢ A novel visual representation that unifies instance-and subset-level inspections of neuron activations, which facilitates comparison of activation patterns for multiple instances and instance subsets. Users can flexibly specify subsets using input features, labels, or any intermediate outcomes in a machine learning pipeline (Sect. 4.2).</p><p>â€¢ An interface that tightly integrates an overview of graph-structured complex models and local inspection of neuron activations, allowing users to explore the model at different levels of abstraction (Sect. 4.3).</p><p>â€¢ A deployed system scaling to large datasets and models (Sect. 4.4).</p><p>â€¢ Case studies with Facebook engineers and data scientists that highlight how ACTIVIS helps them with their work, and usage scenarios that describe how ACTIVIS may work with different models (Sect. 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Machine Learning Interpretation through Visualization</head><p>As the complexity of machine learning algorithms increases, many researchers have recognized the importance of model interpretation and developed interactive tools to help users better understand them <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b36">37]</ref>. While overall model accuracy can be used to select models, users often want to understand why and when a model would perform better than others, so that they can trust the model and know how to further improve it. In developing interpretation tools, revealing relationships between data and models is one of the the most important design goals <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>. Below we present two important analytics strategies that existing works adopt to help users understand how data respond to machine learning models. Instance-based exploration. A widely-used approach to understanding complex algorithms is by tracking how an example (i.e., training or test instance) behaves inside the models. Kulesza et al. <ref type="bibr" target="#b22">[23]</ref> presented an interactive system that explains how models made predictions for each instance. Amershi et al. <ref type="bibr" target="#b2">[3]</ref> developed ModelTracker, a visualization tool that shows the distribution of instance scores for binary classification tasks and allows users to examine each instance individually. The researchers from the same group recently extended their work for multi-classification tasks <ref type="bibr" target="#b31">[32]</ref>. While the above-mentioned tools were designed for model-agnostic, there are also tools designed specifically for neural network models <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b33">34]</ref>. These tools enable users to pick an instance and feed it to the models and show how the parameters of the models change. We will describe them in more detail shortly, in Sect. 2.2.</p><p>Feature-and subset-based exploration. While instance-based exploration is helpful for tracking how models respond to individual examples, feature-or subset-based exploration enables users to better understand the relationships between data and models, as machine learning features make it possible for instances to be grouped and sliced in multiple ways. Researchers have utilized features to visually describe how the models captured the structure of datasets <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref>. Kulesza et al. <ref type="bibr" target="#b22">[23]</ref> used the importance weight of each feature in the Naive Bayes algorithm, and Krause et al. <ref type="bibr" target="#b20">[21]</ref> used partial dependence to show the relationships between features and results. To enable users to analyze results not only by predefined features, researchers have developed tools that enable users to specify instance subsets. Specifying groups can be a good first step for analyzing machine learning results <ref type="bibr" target="#b21">[22]</ref>, as it provides users with an effective way for analyzing complex multidimensional data. In particular, people in the medical domain often perform similar processes, called cohort construction, and Krause et al. <ref type="bibr" target="#b21">[22]</ref> developed an interactive tool that helps this process. McMahan et al. <ref type="bibr" target="#b27">[28]</ref> presented their internal tool that allows users to visually compare the performance differences between models by subsets. MLCube <ref type="bibr" target="#b16">[17]</ref> enabled users to interactively explore and define instance subsets using both raw data attributes and transformed features, and compute evaluation metrics over the subsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Interactive Visualization of Deep Learning Models</head><p>Deep learning has become very popular, largely thanks to the state-ofthe-art performance achieved by convolutional neural network models, commonly used for analyzing image datasets in computer vision. Since deep neural network models typically consist of many parameters, researchers have recognized deep learning interpretation as an important research area. A common approach is to show filters or activations for each neural network layer. This helps users understand what the models have learned in the hidden structure throughout the layers.</p><p>Interactive visualization tools. A number of interactive tools have been developed to effectively visualize the activation information. Tzeng and Ma <ref type="bibr" target="#b35">[36]</ref> was one of the first visualization tools designed for neural network models. While it did not target deep networks, it represented each neuron as a node and visualized a given instance's activations. This idea has been extended to the case of deep neural networks. Karpathy <ref type="bibr" target="#b17">[18]</ref> visualized the activations for each layer of a neural network on his website. Harley <ref type="bibr" target="#b13">[14]</ref> developed an interactive prototype that shows activations for a given instance. Smilkov et al. <ref type="bibr" target="#b33">[34]</ref> developed an interactive prototype for educational purposes, called TensorFlow Playground, which visualized training parameters to help users explore how models process a given instance to make predictions. However, these tools do not scale to large dataset or the complex models commonly used in industry.</p><p>Towards scalable visualization systems. CNNVis <ref type="bibr" target="#b25">[26]</ref> is an interactive visual analytics system designed for convolutional networks. It modeled neurons as a directed graph and utilized several techniques to make it scalable. For example, it uses hierarchical clustering to group neurons and uses bi-directional edge bundling to summarize edges among neurons. They also compute average activations for instances from the same class. However, users cannot feed instances into the system, to perform instance-based analysis which is an effective strategy for understanding machine learning models.</p><p>Another way of handling large number of neurons is to employ dimensionality reduction techniques. By projecting a high-dimensional vector into two-dimensional space, we can better represent the highdimensional nature of deep neural network models. Rauber et al. <ref type="bibr" target="#b30">[31]</ref> studied how 2-D projected view of instance activations and neuron filters can help users better understand neural network models. Google's Embedding Projector <ref type="bibr" target="#b34">[35]</ref> tool, which is integrated into their Tensorflow deep learning framework <ref type="bibr" target="#b0">[1]</ref>, provides an interactive 3-D projection with some additional features (e.g., similar instance search). ReVACNN <ref type="bibr" target="#b9">[10]</ref> is an interactive visual analytics system that uses dimensionality reduction for convolutional networks. While CNNVis <ref type="bibr" target="#b25">[26]</ref> uses clustering to handle large number of neurons, ReVACNN shows both individual neurons and a 2-D projection embedded space (through t-SNE). The individual neuron view helps users explore how individual neurons respond to a user-selected instance; the projected view can help them get a visual summary of instance activations. However, these two views work independently. It is difficult for users to combine their analyses, or compare multiple instances' neuron activations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ANALYTICS NEEDS FOR INDUSTRY-SCALE PROBLEMS</head><p>The ACTIVIS project started in April 2016. Since its inception, we have conducted participatory design sessions with over 15 Facebook engineers, researchers, and data scientists across multiple teams to learn about their visual analytics needs. Together, we collaboratively design and develop ACTIVIS and iteratively improve it.</p><p>In Sect. 3.1, we describe the workflow of how machine learning models are typically trained and used at Facebook, and how results are interpreted. This discussion provides the background information and context for which visualization tools may help improve deep learning model interpretation.</p><p>In Sect. 3.2, we summarize our main findings from our participatory design sessions to highlight six key design challenges that stem from Facebook's needs to work with large-scale datasets, complex deep learning model architectures, and diverse analytics needs. These challenges have been inadequately addressed by current deep learning visualization tools, and they motivate and shape our design goals for ACTIVIS, which we will describe in Sect. 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Background: Machine Learning Practice at Facebook</head><p>Facebook uses machine learning for some of their products. Researchers, engineers, and data scientists from different teams at Facebook perform a wide range of machine learning tasks.</p><p>We first describe how Facebook's machine learning platform helps users train models and interpret their results. Then, we present findings from our discussion with machine learning users and their common analytics patterns in interpreting machine learning models. These findings guide our discovery of design challenges that ACTIVIS aims to address.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">FBLearner Flow: Facebook's Machine Learning Platform</head><p>To help engineers, including non-experts of machine learning, to more easily reuse algorithms in different products and manage experiments with ease, Facebook built a unified machine learning platform called FBLearner Flow <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12]</ref>. It supports many machine learning workflows. Users can easily train models and see their results using the FBLearner Flow interface without writing any code. For example, users can train a model by picking a relevant workflow from a collection of existing workflows and specifying several input parameters for the selected workflow (e.g., location of training dataset, learning parameters). The FBLearner Flow interface is particularly helpful for users who want to use existing machine learning models for their datasets without knowing their internal details.</p><p>Once the training process is done, the interface provides high-level information to aid result analysis (e.g., precision, accuracy). To help users interpret the results from additional multiple aspects, several other statistics are available in the interface (e.g., partial dependence plots). Users can inspect models' internal details via interactive visualization (e.g., for decision trees) <ref type="bibr" target="#b3">[4]</ref>. As deep neural network models gain popularity, developing visualization for their interpretation is a natural step for FBLearner Flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Analytics Patterns for Interpretation</head><p>To better understand how machine learning users at Facebook interpret model results, and how we may design ACTIVIS to better support their analysis, we conducted participatory design sessions with over 15 engineers and data scientists who regularly work with machine learning and deep neural network models. At the high level, we learned that instance-and subset-based strategies are both common and effective, echoing findings from existing research.</p><p>Instance-based analysis. One natural way for users at Facebook to understand complex models is by tracking how an individual example (i.e., training or test instance) behaves inside the models; users often have their own collection of example instances, for which they know their characteristics and ground truth labels. Instance-level exploration is especially useful when an instance is easy to interpret. For example, an instance consisting of text only is much easier to understand than an instance consisting of thousands of numerical features extracted from an end user's data.</p><p>Subset-based analysis. Instance-based analysis, however, is insufficient for all cases. Inspecting instances individually can be tedious, and sometimes hinder insight discovery, such as when instances are associated with many hard-to-interpret numerical features. We learned that some Facebook researchers find subset-based analysis to be more helpful for their work. For example, suppose an instance represents an article that consists of many numerical features extracted from its attributes (e.g., length, popularity). Some users would like to understand how the models behave at higher-level categorization (e.g., by topic, publication date). In addition, some users have curated instance subsets. Understanding model behavior through such familiar subsets promotes their understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Design Challenges</head><p>Besides reaffirming the importance of two analysis strategies discussed above, and the need to support them simultaneously in ACTIVIS, we have identified additional design challenges through the participatory design sessions. We summarize them into six key design challenges. Thus far, they have not been adequately addressed by existing deep learning visualization tools. And they shape the main design goals of ACTIVIS, which we will describe in Sect. 4.1.</p><p>We have labeled the six challenges C1 -C6 and have grouped them into three categories with the labels data, model, and analytics, which indicate the causes for which the challenges arise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C1. Diverse input sources and formats DATA</head><p>While deep learning has become popular because of its superior performance for image data, it has also been applied to many different data formats, including text and numerical features <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19]</ref>. Furthermore, a single model may jointly use multiple types of data at a time. For example, to classify a Facebook post, a model may jointly leverage its textual content, attached photos, and user information, each of which may be associated with many data attributes <ref type="bibr" target="#b1">[2]</ref>. Working with such variety of data sources and formats opens up many opportunities for model interpretation; for example, we may be able to more easily categorize instances using their associated numerical features that can be more readily understood, instead of going the harder route of using imagebased features.</p><p>C2. High data volume DATA Facebook, like many other companies, has a large amount of data. The size of training data often reaches billions of rows and thousands of features. This sheer size of data render many existing visualization tools unusable as they are often designed to visualize the whole dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C3. Complex model architecture MODEL</head><p>Many existing visualization tools for deep learning models often assume simple linear architectures where data linearly flow from the input layer to the output layer (e.g., a series of convolution and max-pooling layer in AlexNet) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b37">38]</ref>. However, most practical model architectures deployed in industry are very complex <ref type="bibr" target="#b10">[11]</ref>; they are often deep and wide, consisting of many layers, neurons, and operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C4. A great variety of models MODEL</head><p>Researchers and engineers at Facebook develop and evaluate mod-els for products every day. It is important for visualization tools to be generalizable so they can work with many different kinds of models. A visualization system would likely be impractical to use or to deploy if a small change to a model requires significant changes made to existing code or special case handling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C5. Diverse subset definitions ANALYTICS</head><p>When performing subset-based analysis, users may want to define subsets in many different ways. Since there are a large number of input formats and input features, there are numerous ways to specify subsets. Instead of providing a fixed set of ways to define subsets, it is desirable to make this process flexible so that users can flexibly define subsets that are relevant to their tasks and goals.</p><p>C6. Simultaneous need for performing instance-and subset-level analysis ANALYTICS Instance-and subset-based are complementary analytics strategies, and it is important to support both at the same time. Instancebased analysis helps users track how an individual instance behaves in the models, but it is tedious to inspect many instances one by one. By specifying subsets and enabling their comparison with individual instances, users can learn how the models respond to many different slices of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ACTIVIS: VISUAL EXPLORATION OF NEURAL NETWORKS</head><p>Through the design challenges we identified (in Sect. 3.2) in our participatory design sessions with researchers, engineers, and data scientists at Facebook, we design and develop ACTIVIS, a novel interactive visual tool for exploring a wide range of industry-scale deep neural network models. In this section, we first present three main design goals distilled from our conversations with Facebook participants (Sect. 4.1). Then, for each design goal, we elaborate on how ACTIVIS achieves it through its system design and visual exploration features (Sects. 4.2-4.4). We label the three design goals G1 -G3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Design Goals</head><p>G1. Unifying instance-and subset-based analysis to facilitate comparison of multiple instance activations. From our participatory design sessions, we learned that both instance-and subset-based analysis are useful and complementary. We aim to support subset-level exploration by enabling users to flexibly define instance subsets for different data types (C1, C5), e.g., a set of documents that contain a specific word. Subset-based analysis also allows users to explore datasets at higher-level abstraction, scaling to billion-scale data or larger (C2). Furthermore, we would like to unify instance-and subset-level inspections to facilitate comparison of multiple instances and groups of instances in a single view (C6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G2</head><p>. Tight integration of overview of model architecture and localized inspection of activations. Industry-scale deep neural network models are often very complex, consisting of many operations (C3). Visualizing every detail and activation value for all intermediate layers can overwhelm users. Therefore, we aim to present the architecture of the models as a starting point of exploration, and let users switch to the detailed inspection of activations.</p><p>G3. Scaling to industry-scale datasets and models through flexible system design. For ACTIVIS to work with many different large-scale models and datasets used in practice, it is important for the system to be flexible and scalable. We aim to support as many different kinds of data types and classification models as what FBLearner currently does (e.g., image, text, numerical) (C1, C4). We would like to achieve this by developing a flexible, modularized system that allows developers to use ACTIVIS for their models with simple API functions, while addressing visual and computational scalability challenges through a multipronged approach (C2, C3). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Exploring Neuron Activations by Instance Subsets</head><p>Drawing inspiration from existing visualizations <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b37">38]</ref>, AC-TIVIS supports the visualization for individual instances. However, it is difficult for users to spot interesting patterns and insights if he can only visualize one instance at a time. For example, consider a hidden layer consisting of 100 neurons. The neuron activations for an instance is a 100-dimension vector consisting of 100 numerical values, where each element in the vector does not have any specific meaning. Instead, if multiple vectors of activation values are presented together, the user may more readily derive meaning by comparing them. For example, users may find that some dimensions may respond more strongly to certain instances, or some dimensions are negatively correlated with certain classes. A challenge in supporting the comparison of multiple instances stems from the sheer size of data instances; it is impossible to present activations for all instances. To tackle this challenge, we enable users to define instance subsets. Then we compute the average activations for instances within the subsets. The vector of average activations for a subset can then be placed next to the vectors of other instances or subsets for comparison.</p><p>The neuron activation matrix, shown at <ref type="figure" target="#fig_0">Fig. 2B</ref>.1, illustrates this concept of comparing multiple instances and instance subsets, using the TREC question classification dataset 2 <ref type="bibr" target="#b24">[25]</ref>. The dataset consists of 5,500 question sentences and each sentence is labeled by one of six categories (e.g., is a question asking about location?). <ref type="figure" target="#fig_0">Fig. 2B</ref> shows the activations for the last hidden layer of the word-level CNN model <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19]</ref>. Each row represents either an instance or a subset of instances. For example, the first row represents a subset of instances <ref type="bibr" target="#b1">2</ref> http://cogcomp.cs.illinois.edu/Data/QA/QC/ whose true class is 'DESC' (descriptions). Each column represents a neuron. Each cell (circle) is a neuron activation value for a subset. A darker circle indicates stronger activation. This matrix view exposes the hidden relationships between neurons and data. For instance, a user may find out a certain neuron is highly activated by instances whose true class is 'LOC'.</p><p>Flexible subset definition. In ACTIVIS, users can flexibly define instance subsets. A subset can be specified using multiple properties of the instances, in many different ways. Example properties include raw data attributes, labels, features, textual content, output scores, and predicted label. Our datasets consist of instances with many features and a combination of different types of data. Flexible subset definition enables users to analyze models from different angles. For example, for instances representing text documents, the user may create a subset for documents that contains a specific phrase. For instances containing numerical features, users can specify conditions, using operations similar to relational selections in databases (e.g., age &gt; 20, topic = 'sports'). By default, a subset is created for each class (e.g., a subset for the 'DESC' class).</p><p>Sorting to reveal patterns. The difficulty in recognizing patterns increases with the number of neurons. ACTIVIS allows users to sort neurons (i.e., columns) by their activation values. For example, in <ref type="figure" target="#fig_1">Fig. 3</ref>, the neurons are sorted based on the average activation values for the class 'LOC'. Sorting facilitates activation comparison and helps reveal patterns, such as spotting instances that are positively correlated with their true class in terms of the activation pattern (e.g., instances #94 and #30 correlate with the 'LOC' class in <ref type="figure" target="#fig_1">Fig. 3</ref>).</p><p>2-D projection of activations. To help users visually examine instance subsets, ACTIVIS provides a 2-D projected view of instance activations. Projection of high-dimensional data into 2-D space has been  considered an effective exploration approach <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35]</ref>. ACTIVIS performs t-distributed stochastic neighbor embedding (t-SNE) <ref type="bibr" target="#b26">[27]</ref> of instance activations. <ref type="figure" target="#fig_0">Fig. 2B.2</ref> shows an example where each dot in the view represents an instance (colored by its true class), and instances with similar activation values are placed closer together by t-SNE.</p><p>The projected view complements with the neuron activation matrix view <ref type="figure" target="#fig_0">(Fig. 2B.1</ref>). Hovering over a subset's row in the matrix would highlight the subset's instances in the projected view, allowing the user to see how instances within the subsets are distributed. In the projected view, hovering over an instance would display its activations; clicking that instance will add it to the matrix view as a new row.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Interface: Tight Integration of Model, Instances, and Activation Visualization</head><p>The above visual representation of activations is the core of our visual analytics system. To help users interactively specify where to start their exploration of a large model, we designed and developed an integrated system interface. As depicted in <ref type="figure" target="#fig_0">Fig. 2</ref>, the interface consists of multiple panels. We describe each of them below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A: Overview of Model Architecture</head><p>Deep learning models often consist of many operations, which makes it difficult for users to fully understand their structure. We aim to provide an overview of the model architecture to users, so they can first make sense of the models, before moving on to parts of the models that they are interested in. Deep neural network models are often represented as computation graphs (DAGs) (as in many deep learning frameworks like Caffe2 3 , TensorFlow <ref type="bibr" target="#b0">[1]</ref>, and Theano <ref type="bibr" target="#b5">[6]</ref>). The frameworks provide a set of operators (e.g., convolution, matrix multiplication, concatenation) to build machine learning programs, and model developers (who create new machine learning workflows for FBLearner Flow) write the programs using these building blocks. Presenting this graph to users would help them first understand the structure of the models and find interesting layers to explore the detailed activations. There are several possible ways in visualizing computation graphs. One approach is to represent operators as nodes and variables as edges. This approach has gained popularity, thanks to its adoption by Tensor-Flow. Another way is to consider both an operator and a variable as a single node. Then the graph becomes a bipartite graph: the direct neighbors of an operator node are always variable nodes; the neighbors of a variable node are always operator nodes. Both approaches have their pros and cons. While the first approach can have a compact representation by reducing the number of nodes, the second one, a classical way to represent programs and diagrams, makes it easier to track data. For ACTIVIS, it would be better to make variable nodes easy to locate as we present activations for a selected variable. Therefore, we decided to represent the graph using the second approach.</p><p>The visualization of the computation graph is shown on the top panel ( <ref type="figure" target="#fig_0">Fig. 2A)</ref>. The direction of data flow is from left (input) to right (output). Each node represents either an operator (dark rectangle) or tensor (circle). To explore this medium-sized graph (often &gt;100 nodes), users can zoom and pan the graph using a mouse. When users hover over a node, its full name is shown, and when they click it, its corresponding activation is shown in the neuron activation panel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B: Activation for Selected Node</head><p>When users select a node of interest from the computation graph, the corresponding neuron activation panel <ref type="figure" target="#fig_0">(Fig. 2B)</ref> will be added to the bottom of the computation graph panel. The neuron activation panel has three subpanels: (0) the names of the selected node and its neighbors, (1) the neuron activation matrix view, and (2) the projected view. The left subpanel shows the name of the selected variable node and its neighbors. Users can hover over a node to highlight where it is located in the computation graph on the top. The neuron matrix view <ref type="figure" target="#fig_0">(Fig. 2B.1</ref>) and projected view <ref type="figure" target="#fig_0">(Fig. 2B.2)</ref> show instance activations for the selected node. Note that we described these views in Sect. 4.2.</p><p>Users can select multiple nodes and visually compare their activation patterns. <ref type="figure" target="#fig_3">Fig. 5</ref> illustrates that users can visually explore how models learned the hidden structure of data through multiple layers. The figure shows three layers, from top to bottom: the second-to-last hidden layer which concatenates multiple maxpool layers <ref type="bibr" target="#b18">[19]</ref>, the last hidden layer, and the output layer. As shown in the figure, the layer's projected views show that as data flow through the network, from input (top) to output (bottom), neuron activation patterns gradually become more discernible and clustered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C: Instance Selection</head><p>The instance selection panel helps users get an overview of instances with their prediction results and determine which ones should be added to the neuron activation view for further exploration and comparison.</p><p>The panel is located at the right side on the interface. It visually summarizes prediction results. Each square represents an instance. Instances are vertically grouped based on their true label. Within a true label (row group), the left column shows correctly classified instances, sorted by their prediction scores in descending order (from top to bottom, and left to right within each row). The right column shows misclassified instances. An instance's fill color represents its true label, its border color the predicted label. When the user hovers over an instance, a tooltip will display basic information about the instance (e.g., textual content, prediction scores).</p><p>The panel also helps users determine which instances can be added to the activation view for further exploration. By hovering over one of the instance boxes, users can see the instance's activations. A new row is added to the activation view presenting the activation values for the selected instance. When users' mouse leaves the box, the added row disappears. To make a row persistent, users can simply click the box. In a similar fashion, users can add many rows by clicking the instance boxes. Then, they can compare activations for multiple instances and also compare those for instances with those for groups of instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Deploying ACTIVIS: Scaling to Industry-scale Datasets and Models</head><p>We have deployed ACTIVIS on Facebook's machine learning platform. Developers who want to use ACTIVIS for their model can easily do so by adding only a few lines of code, which instructs their models' training process to generate information needed for ACTIVIS's visualization. Once model training has completed, the FBLearner Flow interface provides the user with a link to ACTIVIS to visualize and explore the model. The link opens in a new web browser window. ACTIVIS is designed to work with classification tasks that use deep neural network models. As complex models and large datasets are commonly used at Facebook, it is important that ACTIVIS be scalable and flexible, so that engineers can easily adopt ACTIVIS for their models. This section describes our approaches to building and deploying ACTIVIS on FBLearner, Facebook's machine learning platform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Generalizing to Different Models and Data Types</head><p>One of our main goals is to support as many different kinds of data types and models as what FBLearner currently does (e.g., images, text, numerical). The key challenge is to enable existing deployed models to generate data needed for ACTIVIS with as little modification as possible. Without careful thinking, we would have to add a large amount of model-specific code, to enable ACTIVIS to work with different models. To tackle this challenge, we modularize the data generation process and define API functions for model developers so that they can simply call them in their code, to activate ACTIVIS for their models. In practice, for a developer to use ACTIVIS for a model, only three function calls are needed to be added (i.e., calling the preprocess, process, and postprocess methods). For example, developers can specify a list of variable nodes that users can explore, as an argument of the preprocess function (described in detail in Sect. 4.4.2). Furthermore, developers can leverage user-defined functions to specify how subsets are defined in ACTIVIS, a capability particularly helpful for the more abstract, unstructured data types, such as image and audio. For example, developers may leverage the output of an object recognition algorithm that detects objects (e.g., cats, dogs) to define image subsets (e.g., subset of images that contain dogs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Scaling to Large Data and Models</head><p>ACTIVIS addresses visual and computational scalability challenges through multiple complementary approaches. Some of them were introduced in earlier sections (e.g., Sect. 4.2), such as ActiVis's overarching subset-based analysis, and the simultaneous use of neuron matrix (for individual neuron inspection) and projected view (in case of many neurons). We elaborate on some of our other key ideas below.</p><p>Selective precomputation for variable nodes of interest. Industry-scale models often consist of a large number operations (i.e., variable nodes), up to hundreds. Although any variable node can be visualized in the activation visualization, if we compute activations for all of them, it will require significant computation time and space for storing the data. We learned from our discussion with experts and design sessions with potential users that it is typical for only a few variable nodes in a model to be of particular interest (e.g., last hidden layer in CNN). Therefore, instead of generating activations for all variable nodes, we let model developers specify their own default set of variable nodes. The model developers can simply specify them as an argument of the preprocess method. To explore variable nodes not included in the default set, a user can add them by specifying the variable nodes in the FBLearner Flow interface. Such nodes will then be available in the computation graph (highlighted in yellow).</p><p>User-guided sampling and visual instance selection. For billionscale datasets, it is undesirable to display all data points in the instance selection panel. Furthermore, we learned from our design sessions that researchers and engineers are primarily interested in a small number of representative examples, such as "test cases" that they have curated (e.g., instances that should be labeled as Class 'LOC' by all wellperforming models). To meet such needs, by default, we present a sample of instances in the interface (around 1,000), which meet the practical needs of most Facebook engineers. In addition, users may also guide the sampling to include arbitrary examples that they specify (e.g., their test cases).</p><p>Computing neuron activation matrix for large datasets. The main computational challenge of ACTIVIS is in computing the neuron activation matrix over large datasets. Here, we describe our scalable approach whose time complexity is linear in the number of data instances. We first create a matrix S (#instances Ã— #subsets) that describes all instance-to-subset mappings. Once a model predicts labels for instances, it produces an activation matrix A (#instances Ã— #neurons) for each variable node. By multiplying these two matrices (i.e., S T A), followed by normalization, we obtain a matrix containing all subsets' average neuron activation values, which are visualized in the neuron matrix view. As the number of instances dominates, the above computation's time complexity is linear in the number of instances. In practice, this computation roughly takes the same amount of time as testing a model. We have tested ACTIVIS with many datasets (e.g., one with 5 million training instances). ACTIVIS can now scale to any data sizes that FBLearner supports (e.g., billion-scale or larger).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Implementation Details</head><p>The visualization and interactions are implemented mainly with React.js. <ref type="bibr" target="#b3">4</ref> We additionally use a few D3.js V4 components. <ref type="bibr" target="#b4">5</ref> The computation graph is visualized using Dagre, <ref type="bibr" target="#b5">6</ref> a JavaScript library for rendering directed graphs. All the backend code is implemented in Python (including scikit-learn 7 for t-SNE) and the activation data generated from backend are passed to the interface using the JSON format. <ref type="figure">Fig. 6</ref>. Version 1 of ACTIVIS, showing an instance's neuron activation strengths, encoded using color intensity. A main drawback of this design was that users could only see the activations for a single instance at a time. Activation comparison across multiple instances was not possible. <ref type="figure">Fig. 7</ref>. Version 2 of ACTIVIS, which unified instance-and subset-level activation visualization. This design was too visually overwhelming and did not scale to complex models, as it allocated a matrix block for each operator; a complex model could have close to a hundred operators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">INFORMED DESIGN THROUGH ITERATIONS</head><p>The current design of ACTIVIS is the result of twelve months of investigation and development effort through many iterations.</p><p>Unifying instances and subsets to facilitate comparison of multiple instances. The first version of ACTIVIS, depicted in <ref type="figure">Fig. 6</ref>, visualizes activations for all layers (each column group represents a single layer). A main drawback of this design is that users can only see the activations for a single instance at a time; they cannot compare multiple instances' activations. While, for the subsets, we use an approach similar to ACTIVIS's design (each dot represents the average values for the subset), we encode activations for a given instance using background color (here, in green). This means that the visualization cannot support activation comparison across multiple instances. This finding prompted us to unify the treatment for instances and subsets to enable comparison across them. <ref type="figure">Fig. 7</ref> shows our next design iteration that implements this idea.</p><p>Separating program and data to handle complex models. Although the updated version <ref type="figure">(Fig. 7)</ref> shows activations for multiple instances, which helps users explore more information at once, it becomes visually too overwhelming when visualizing large, complex models. Some engineers expressed concern that this design might not generalize well to different models. Also, engineers are often interested in only a few variable nodes, rather than looking at many variable nodes. Therefore, we decided to separate the visualization of the model architecture and the activations for a specific variable node.</p><p>Presenting 2-D projection of instances. One researcher suggested that ACTIVIS should provide more detail for each neuron, in addition to average activations. Our first solution was to present statistics (e.g., variance) and distributions for each neuron. However, some researchers cautioned that this approach could be misleading, because these summaries might not fully capture high-dimensional activation patterns. This prompted us to add the projected view (t-SNE), which enabled users to better explore the high-dimensional patterns (see <ref type="figure" target="#fig_2">Fig. 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CASE STUDIES &amp; USAGE SCENARIOS</head><p>To better understand how ACTIVIS may help Facebook machine learning users with their interpretation of deep neural network models, we recruited three Facebook engineers and data scientists to use the latest version of ACTIVIS to explore text classification models relevant to their work. We summarize key observations from these studies to highlight ACTIVIS's benefits (Sect. 6.1). Then, based on observations and feedback from these users and others who participated in our earlier participatory design sessions, we present example usage scenarios for ranking models to illustrate how ACTIVIS would generalize (Sect. 6.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Case Studies: Exploring Text Classification Models with ACTIVIS 6.1.1 Participants and Study Protocol</head><p>We recruited three Facebook engineers and data scientists to use our tools (their names substituted for privacy):</p><p>Bob is a software engineer who has expertise in natural language processing. He is experimenting with applying text classification models to some Facebook experiences, such as for detecting intents from a text snippet, like understanding when the user may want to go somewhere <ref type="bibr" target="#b1">[2]</ref>. For example, suppose a user writes "I need a ride", Bob may want the models to discover if the user needs transportation to reach the destination. He is interested in selecting the best models based on experimenting with many parameters and a few different models, as in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>Dave is a relatively new software engineer. Like Bob, he is also working with text classification models for user intent detection, but unlike Bob, he is more interested in preparing training datasets from large collections of databases.</p><p>Carol is a data scientist who holds a Ph.D. in the area of natural language processing. Unlike Bob and Dave, she is working with many different machine learning tasks, focusing on textual data.</p><p>We had a 60-minute session with each of the three participants. For the first 20 minutes, we asked them a few questions about their typical workflows, and how they train models and interpret results. Then we introduced them to ACTIVIS by describing its components. The participants used their own datasets and models, available from FBLearner Flow. After the introduction, the participants used ACTIVIS while thinking aloud. They also gave us feedback on how we could further improve ACTIVIS. We recorded audio during the entire session and video for the last part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Key Observations</head><p>We summarize our key observations from interacting with the three participants into the following three themes, each highlighting how our tool helped them with the analysis.</p><p>Spot-checking models with user-defined instances and subsets. ACTIVIS supports flexible subset definition. This feature was developed based on the common model development pattern where practitioners often curate "test cases" that they are familiar with, and for which they know their associated labels. For example, a text snippet "Let's take a cab" should be classified as a positive class of detecting transportation-related intent. Both Bob and Dave indeed found this feature useful (i.e., they also had their own "test cases"), and they appreciated the ability to specify and use their own cases. This would help them better understand whether their models are working well, by comparing the activation patterns of their own instances with those of other instances in the positive or negative classes. Bob's usage of ACTIVIS and comments echo and support the need for subset-level visualization and exploration, currently inadequately supported by existing tools.</p><p>Graph overview as a crucial entry point to model exploration. From our early participatory design sessions, we learned that AC-TIVIS's graph overview was important for practitioners who work with complex models whose tasks only require them to focus on specific components of the models. Bob, who works with many different variations of text classification models, has known that the model he works with mainly uses convolution operations and was curious to see how the convolution works in detail. When he launched ACTIVIS, he first examined the model architecture around the convolution operators using the computation graph panel. He appreciated that he could see how model training parameters are used in the model, which helped him develop better understanding of the internal working mechanism of the models. For example, he found how and where padding are used in the models by exploring the graph <ref type="bibr" target="#b6">[7]</ref>. After he got a better sense about how the model function around the convolution operators, he examined the activation patterns of the convolution output layer. This example shows that the graph overview is important for understanding complex architectures and locating parts that are relevant to the user's tasks. In other words, the graph serves as an important entry point of Bob's analysis. Existing tools assuming user familiarity with models may not hold in real-world large-scale deployment scenarios.</p><p>Visual exploration of activation patterns for evaluating model performances and for debugging hints. One of the main components of ACTIVIS is the visual representation of activations that helps users easily recognize patterns and anomalies. As Carol interacted with the visualization, she gleaned a number of new insights, and a few hints for how to debug deep learning models in general. She interactively selected many different instances and added them to the neuron activation matrix to see how they activated neurons. She found out that the activation patterns for some instances are unexpectedly similar, even though the textual content of the instances seem very different. Also, she spotted that some neurons were not activated at all. She hypothesized that the model could be further improved by changing some of the training parameters, so she decided to modify them to improve the model. While the neuron activation panel helps Carol find models that can be further improved, Bob found some interesting patterns from the activation patterns for the convolution output layer. He quickly found out that some particular words are highly activated while some other words, which he thought can be highly activated, do not respond much. This helped him identify words that are potentially more effective for classification. The examples above demonstrate the power of visual exploration. ACTIVIS helps users recognize patterns by interacting with instances and instance subsets they are familiar with.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Usage Scenario: Exploring Ranking Models</head><p>As there are many potential uses for ACTIVIS at Facebook, we also discussed with a number of researchers and engineers at different teams to understand how they may adopt ACTIVIS. Below, we present a usage scenario of ACTIVIS for exploring ranking models, based on our discussion. We note the scenario strongly resembles others that we have discussed so far; this is encouraging because enabling ACTIVIS to generalize across teams and models is one of our main goals.</p><p>Alice is a research scientist working with ranking models, one of the important machine learning tasks in industry. The ranking models can be used to recommend relevant content to users by analyzing a large number of numerical features extracted from databases <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15]</ref>. Alice is experimenting with deep neural network models to evaluate how these models work for a number of ranking tasks. She often performs subsetbased analysis when examining model performance, such as defining subsets based on categories of page content. Subset-based analysis is essential for Alice, because she works with very large amount of training data (billions of data points, thousands of features). ACTIVIS's instance-based exploration feature is not yet helpful for Alice, since she is still familiarizing herself with the data and has not identified instances that she would like to use for spot-checking the model. In ACTIVIS, Alice is free to use either or both of instance-and subsetbased exploration. For new, unfamiliar datasets, Alice finds it much easier to start her analysis from the high level, then drill down into subsets, using attributes or features.</p><p>Alice has trained a fully-connected deep neural network model with some default parameters. When she launches ACTIVIS, she first examines the output layer to see how the activation patterns for the positive and negative classes may be different. To her surprise, they look similar. Furthermore, by inspecting the neuron activation matrix view, she realizes that many neurons are not activated at all -their activation values are close to 0. This signals that the model may be using more neurons than necessary. So, she decided to train additional models with different parameter combinations (e.g., reduce neurons) to relieve the above issue.</p><p>The performances of some models indeed improve. Happy with this improvement, Alice moves on to perform deeper analysis of the trained models. She first creates a number of instance subsets by using features. She utilizes 50 top features known to be important for ranking. For categorical features, she defines a subset for each category value. For numerical features, she quantizes them into a small number of subsets based on the feature value distribution. ACTIVIS's neuron activation matrix view visualizes how the subsets that Alice has defined are activating the neurons. Maximizing the matrix view to take up the entire screen (and minimizing the computation graph view), Alice visually explores the activation matrix and identifies a number of informative, distinguishing activation patterns. For example, one neuron is highly activated for a single subset, and much less so for other subsets, suggesting that neuron's potential predictive power. With ACTIVIS, Alice can train models that perform well and understand how the models capture the structure of datasets by examining the relationships between features and neurons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION AND FUTURE WORK</head><p>Visualizing gradients. Examining gradients is one of the effective ways to explore deep learning models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18]</ref>. It is straightforward to extend ACTIVIS to visualize gradients by replacing activations with gradients. While activation represents forward data flow from input to output layers, gradient represents backward flow. Gradients would help developers to locate neurons or datasets where the models do not perform well.</p><p>Real-time subset definition. For ACTIVIS to work with a new subset, it needs to load the dataset into RAM to check which instances satisfy the subset's conditions. Currently, it is not of high priority for the above process to be performed in real time, because users often have pre-determined subsets to explore. We plan to integrate dynamic filtering and searching capabilities, to speed up both subset definition and instance selection.</p><p>Automatic discovery of interesting subsets. With ACTIVIS, users can flexibly specify subsets in infinitely many ways. One of the engineers commented that ACTIVIS could help suggest interesting subsets for exploration, based on heuristics or measures. For example, for text datasets, such a subset could include phrases whose activation patterns are very similar or different to those for a given instance or class.</p><p>Supporting input-dependent models. An interesting research direction is to extend ACTIVIS to support models that contain variable nodes whose number of neurons changes depending on the input (e.g., the number of words in a document), and to study the relationships between neurons and subsets for such cases.</p><p>Understanding how ACTIVIS informs model training. We plan to conduct a longitudinal study to better understand ACTIVIS's impact on Facebook's machine learning workflows, such as how ACTIVIS may inform the model training process. For example, a sparse neuron matrix may indicate that a model is using more neurons than needed, which could inform engineers on their decisions for hyperparameter tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>We presented ACTIVIS, a visual analytics system for deep neural network models. We conducted participatory design session with over 15 researchers and engineers across many teams at Facebook to identify key design challenges, and based on them, we distilled three main design goals: (1) unifying instance-and subset-level exploration; (2) tight integration of model architecture and localized activation inspection; and (3) scaling to industry-scale data and models. ACTIVIS has been deployed on Facebook's machine learning platform. We presented case studies with Facebook engineers and data scientists, and usage scenarios of how ACTIVIS may be used with different applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>ACTIVIS integrates multiple coordinated views. A. The computation graph summarizes the model architecture. B. The neuron activation panel's matrix view displays activations for instances, subsets, and classes (at B1), and its projected view shows a 2-D t-SNE projection of the instance activations (at B2). C. The instance selection panel displays instances and their classification results; correctly classified instances shown on the left, misclassified on the right. Clicking an instance adds it to the neuron activation matrix view. The dataset used is from the public TREC question answering data collections<ref type="bibr" target="#b24">[25]</ref>. The trained model is a word-level convolutional model based on<ref type="bibr" target="#b18">[19]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Sorting neurons (columns) by their average activation values for the LOC (location) class helps users more easily spot instances whose activation patterns are positively correlated with that of the class, e.g., instances #94 and #30 (see green arrows).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Hovering over an instance subset (e.g., for the NUMber class) highlights its instances (purple dots) in the t-SNE projected view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Users can simultaneously visualize and compare multiple layers' activations. Shown here, from top to bottom, are: the second-to-last hidden layer, the last hidden layer, and the output layer. Their projected views show that as instances flow through the network from input (top) to output (bottom), their activation patterns gradually become more discernible and clustered (in projected view).</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://caffe2.ai/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://facebook.github.io/react/ 5 https://d3js.org/ 6 https://github.com/cpettitt/dagre 7 http://scikit-learn.org/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Facebook Applied Machine Learning Group, especially Yangqing Jia, Andrew Tulloch, Liang Xiong, and Zhao Tan for their advice and feedback. This work is partly supported by the NSF Graduate Research Fellowship Program under Grant No. DGE-1650044.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>ManÃ©</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>ViÃ©gas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<title level="m">TensorFlow: Large-scale machine learning on heterogeneous distributed systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Introducing DeepText: Facebook&apos;s text understanding engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdulkader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lakshmiratan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://code.facebook.com/posts/181565595577955/introducing-deeptext-facebook-s-text-understanding-engine/" />
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2017" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ModelTracker: Redesigning performance analysis tools for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chickering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Suh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI)</title>
		<meeting>the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="337" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Productionizing machine learning pipelines at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kalro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mehanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sidorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ML Systems Workshop at the 33rd International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Serving a billion personalized news feeds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Backstrom</surname></persName>
		</author>
		<ptr target="https://youtu.be/Xpx5RYNTQvg" />
	</analytic>
	<monogr>
		<title level="m">12th International Workshop on Mining and Learning with Graphs at the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Theano: A CPU and GPU math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Python for Scientific Computing Conference (SciPy)</title>
		<meeting>the Python for Scientific Computing Conference (SciPy)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Implementing a CNN for text classification in TensorFlow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Britz</surname></persName>
		</author>
		<ptr target="http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow" />
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">FeatureInsight: Visual support for error-driven feature ideation in text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Visual Analytics Science and Technology (VAST)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">iVisClassifier: An interactive visual analytics system for classification based on supervised dimension reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kihm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Visual Analytics Science and Technology (VAST)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ReVACNN: Steering convolutional neural network via real-time visual analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Future of Interactive Learning Machines Workshop at the 30th Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep neural networks for YouTube recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sargin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Conference on Recommender Systems</title>
		<meeting>the 10th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Introducing FBLearner Flow: Facebook&apos;s AI backbone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dunn</surname></persName>
		</author>
		<ptr target="https://code.facebook.com/posts/1072626246134461/introducing-fblearner-flow-facebook-s-ai-backbone/" />
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2017" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Explainers: Expert explorations with crafted projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2042" to="2051" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An interactive node-link visualization of convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Symposium on Visual Computing</title>
		<meeting>the 11th International Symposium on Visual Computing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="867" to="877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Practical lessons from predicting clicks on ads at Facebook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Atallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bowers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Q</forename><surname>Candela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Data Mining for Online Advertising</title>
		<meeting>the 8th International Workshop on Data Mining for Online Advertising</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01759</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visual exploration of machine learning results using data cube analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H P</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Human-In-the-Loop Data Analytics at the ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the Workshop on Human-In-the-Loop Data Analytics at the ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Convnetjs</surname></persName>
		</author>
		<ptr target="http://cs.stanford.edu/people/karpathy/convnetjs/" />
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2017" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Infuse: Interactive feature selection for predictive modeling of high dimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bertini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1614" to="1623" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Interacting with predictions: Visual inspection of black-box machine learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2016 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5686" to="5697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Supporting iterative cohort construction with visual temporal queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stavropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="100" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Principles of explanatory debugging to personalize interactive machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stumpf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Intelligent User Interfaces (IUI)</title>
		<meeting>the 20th International Conference on Intelligent User Interfaces (IUI)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="126" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Why-oriented end-user debugging of naive Bayes text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stumpf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Burnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Oberst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Interactive Intelligent Systems (TiiS)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning question classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Computational Linguistics</title>
		<meeting>the 19th International Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards better analysis of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="100" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ad click prediction: A view from the trenches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Davydov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Golovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chikkerur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Hrafnkelsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kubica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1222" to="1230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gestalt: Integrated support for implementation and analysis in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bancroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fogarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Landay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology (UIST)</title>
		<meeting>the 23nd Annual ACM Symposium on User Interface Software and Technology (UIST)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="37" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Investigating statistical machine learning as a tool for software development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fogarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Landay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Harrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="667" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visualizing the hidden activity of artificial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Fadel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Falcao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Telea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="101" to="110" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Squares: Supporting interactive performance analysis for multiclass classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="70" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Why should I trust you?: Explaining the predictions of any classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Directmanipulation visualization of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Smilkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Viegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Visualization for Deep Learning at the 33rd International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Embedding Projector: Interactive visualization and interpretation of embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Smilkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nicholson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>ViÃ©gas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Interpretable Machine Learning in Complex Systems at the 30th Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Opening the black box: Data driven visualization of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-Y</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="383" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">BaobabView: Interactive construction and analysis of decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Van Den Elzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Van Wijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Visual Analytics Science and Technology (VAST)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Understanding neural networks through deep visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Visualization for Deep Learning at the 33rd International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
